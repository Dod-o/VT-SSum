{
    "id": "me2sqtdoqqgqyos5tlh6b427l5ggjlpx",
    "title": "Bayesian or Frequentist, Which Are You?",
    "info": {
        "author": [
            "Michael I. Jordan, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_jordan_bfway/",
    "segmentation": [
        [
            "Alright, so I'm happy to be here, thanks.",
            "Invited, I looked at the list of all the other speakers and I notice there was a lot of Bayesians on the list and so I thought I'd better give a frequentist counterpart if there had been a lot of frequent list, I would have given a Bayesian counterpart.",
            "I'm just sort of a contrarian it hard.",
            "So that's not entirely the goal of the talk, so the goal of talk is to sort of.",
            "It's a summer school to teach a little bit about Bayesian frequentist distinction.",
            "Emphasize a little more.",
            "The frequentist 'cause I think you've probably had more Bayesian during the week.",
            "And emphasizing the essential unity of these two classes of ideas that have been debated for about 400 years now.",
            "So I'm not going to ask this question, but let me I am going to ask the question, so let me you've been through two weeks of mainly Bayesian lectures.",
            "How many of you think that you're a Bayesian?",
            "Would you call yourself a Bayesian?",
            "Alright, I'd say about third of the room.",
            "How many of you are frequentist?",
            "Alright, then, about three of you see?",
            "So that's that's kind of weird.",
            "How many of you are both?",
            "Another third of the room.",
            "How many of you are neither?",
            "Alright, so I'll ask that same question maybe at the end of the lectures and see if they can see if things have shifted at all.",
            "OK, so this this topic that most."
        ],
        [
            "We're interested in statistical inference has been around for quite a long time.",
            "And there are two sort of main perspectives that have stood the test of time.",
            "The Bayesian frequencies I'm going to give you a little argument in a couple of slides from a decision theoretic point of view of why these are really the only two real competitors.",
            "Um?",
            "Kind of essentially become lot because loss functions have two arguments.",
            "There's kind of only two ways to go.",
            "And it's really important to kind of keep throughout your whole career.",
            "Balancing these two things back and forth.",
            "Anybody who decided there one of the other early in their life and it will only only that, I think, is sort of missing the point.",
            "These are kind of things to ponder and muse and understand the relationships and their deep.",
            "So Bayesian perspective is a conditional perspective, so inferences should be made conditional on the current data.",
            "So you just observe some data, hold that fixed and do everything conditional on that.",
            "Don't worry bout other data you could have gotten.",
            "That's the.",
            "The conditional Bayesian perspective.",
            "Now I find myself often being a Bayesian applied projects when I'm working with a domain expert and we have a lot of time to work with each other.",
            "So picture like lots of biology projects, you'll have someone who knows a great deal and you want to try to understand what they're thinking about the problem.",
            "What do they know, what's the what's the prior?",
            "Also, what is the loss function so we often can't priors, but you really also need to consider the loss function as well, so how do they care about so you have time to elicit all those sorts of things?",
            "It's often really very appropriate to be a Bayesian.",
            "And one way to think about what a Bayesian perspective is, it's the optimist.",
            "So I'm approaching a problem as a statistician.",
            "I want to get I want to get knowledge out.",
            "I'm going to inference out from data.",
            "So let's we have the sophisticated tools.",
            "Let's be optimistic and assuming get as much knowledge as possible by working hard to get a good prior and get a loss function and get a good model.",
            "And iterating, so that's the optimist.",
            "The frequentist perspective is an unconditional perspective.",
            "You don't think about conditioning necessarily on the current date.",
            "All you could, but the way you evaluate a frequentist procedure or evaluate procedure from this point of view is that you consider unconditional averages, so you should get good answers in repeated use.",
            "So repeated use means you're going to look at multiple datasets and you're going to take averages unconditionally over the multiple datasets.",
            "Don't condition on one single data set.",
            "You look at multiple datasets you should get.",
            "And you talk about unconditional performance over all those possible datasets.",
            "That's what it means to be a frequentist.",
            "Now, I also think this is a very natural perspective for lots of situations, and I often find myself in a frequentist in particular.",
            "I don't have alot of time to sit down with a domain expert, we just have a very quick sort of project.",
            "We develop a very simple tool that aims at some inference directly.",
            "In some sense an hope that I can prove something about her or someone else has already proved something about that.",
            "I'm not going to wrong by using this, so in particular, if you're going to write a piece of software that's going to be used by many people with many datasets, you really ought to give a frequentist guarantee on that you ought to be able to say that piece of software is going to work well on multiple datasets now, inside the software.",
            "It may be Bayesian, it may condition, but you want to be able to say whatever you condition on multiple different datasets.",
            "You should do well.",
            "OK, now someone sort of worked all that out in advance.",
            "In some sense that there are some theorems that say that Bayesian inference is has good frequentist properties under various situations.",
            "So in some sense you don't have to worry about it that much in kind of classical situations, parametric situations and so on.",
            "Nonparametric is a different story, but in general, if you're not going to necessarily start with additional Bayesian procedure you like some other procedure I'm going to beat the median or something didn't necessarily have a Bayesian justification by like it.",
            "It's a procedure, then I that's my software.",
            "I want to prove that it has good frequentist properties, meaning that.",
            "Large fraction of the time it will give the answer you expect to get on all kinds of datasets.",
            "OK, so I find that a hard argument too.",
            "Defined difficulties with.",
            "I mean it really, you know if you're writing software, you should be a frequentist.",
            "Now the prequels perspective also is that of a pessimist.",
            "So instead of being an optimist, what we think about it?",
            "If you're a frequent, is that we're right on a model, we're going to develop a procedure.",
            "An almost certainly it's going to be a simplification of reality.",
            "Reality is really complicated.",
            "We're going to simplify drastically often, and so we may get the wrong answer and let's protect ourselves not to get the wrong answer too often.",
            "OK, so in medical domains alot of people tend to be frequentist because they want to protect themselves and get about doing something wrong something stupid.",
            "So it's the pessimistic.",
            "So Frequentism is definitely dominate statistics in the last 100 years.",
            "Bayesian World is certainly present, and but smaller, and I think most sessions were trained, is frequent and still tend to approach the problem of frequency POV.",
            "It's just this sort of pessimism just sort of ground in.",
            "There's lots of reasons for that.",
            "There are lots of situations where people have a bad inferences and you know the pessimistic prosperity is important to ingest."
        ],
        [
            "OK, now I'm often asked I go back and forth between the quote unquote machine world and learning world in the sticks world by statisticians.",
            "What is this thing called?",
            "Machine learning and I don't really believe it's a new field per say.",
            "I believe it's a contribution to the general problem with fiscal inference and decision making.",
            "It really is a set of themes.",
            "It's not one kind of field, but it's an it's a loose Confederation of theme.",
            "So reinforcement learning, clustering, classification, graphical models.",
            "I mean, what do they have to do with each other?",
            "You know not.",
            "That much they just happened to be themes that people have found interesting and useful, and they connect to each other, usually by some sort of statistical argument of some kind.",
            "So you know statisticians I talked about the believe though it is not, they don't actually pretend to think they've discovered a new field.",
            "It really is just sort of physical inference.",
            "OK, that's at least that's good.",
            "But it's sort of different flavor in some ways, and one of them is there's a lot of focus on prediction.",
            "If you just could make a good prediction here how you did it, that's often the machine learning spirit, and on kind of completely other side of things, not a prediction, but just do exploratory data analysis.",
            "Find Cool features is kind of a typical machine learning thing to do.",
            "That's exploratory data analysis.",
            "You're trying to understand something about your data.",
            "And so sessions, you know, neither had those are both things I understand, and that's reason why it's part of statistics too.",
            "But a lot of statistics about what's called coverage.",
            "I want you to tell me something in confidence.",
            "In some prediction you have.",
            "OK, and I want you to tell me you found some structure.",
            "You've done some supporting.",
            "You found some structure.",
            "What's actually probably that's real, or what's the probability that that's garbage?",
            "And so that kind of goes on the terminology of coverage.",
            "I want you to frequentist concept, typically to guarantee that if you run your procedure over and again you will kind of do the right thing i.e.",
            "Cover your error bars will cover the truth the right fraction of time or your things you have discovered will be noise.",
            "The right fraction of time.",
            "So that is kind of, you know, say well, OK they still have it kind of matured enough yet to really understand that this is an important issue to be worried about error bars in a frequent since the coverage national coverage.",
            "The other thing that you will note from machine learning people is there's a lot of focus on methodology, so every develops a new model than a new method to fit their model.",
            "That's the focus of things, not so much about kind of classical inferential topics.",
            "And then the evaluation is usually not theoretical, but empirical, so that's kind of good, and statisticians, I think, appreciate that there is a kind of a dollop of empirical process.",
            "Theory is often called statistical learning theory, but it's really empirical process theory, and it's when they turn to theory they turn this big hammer and they don't have the all the other little Hammers that are sitting around that are available.",
            "Mainly asking products, so there's lots of parent nonparametric statistics and machine learning.",
            "In fact, very few people do parametrics, but surprisingly to establish and there's hardly any assassin product.",
            "That's the main tool to statisticians.",
            "All these asymptotics and everyone is using these these big empirical process type Hammers, so that's a bit of a surprise, and the other somewhat surprising fact is that the field is somehow sometimes frequented, sometimes Bayesian, and it's not clear when when one is going to come up with the others come up and it kind of is a coexistence.",
            "You have a lot of people doing just purely Bayesian stuff.",
            "A lot of people do infrequent and stuff.",
            "Sometimes I can call himself frequentist.",
            "And there's no interplay between those two things, so there's kind of two parallel streams that go forward, and occasionally you'll say something well, I can give it a Bayesian interpretation of your procedure.",
            "Here's a prior that matches it or something, but very little actual kind of real interplay, and the usual statistical sense.",
            "OK, so you guys are more machine learning people, so this slide may have meant that much to you, but I think it is important to understand how you understood by."
        ],
        [
            "The outside world.",
            "OK, so I promised a decision theoretic perspective.",
            "Decision theory goes back to Walden to others in the 40s.",
            "And although some of the number of papers that have decision theoretic content to them has dropped over the years, it was the thing to do in the 50s and 60s.",
            "It's still definitely present and I think many people, including myself, view it as an extremely useful perspective to bring to bear and thinking about fundamentals, instead instill inference.",
            "So you know decision theory perspective says you have some data X.",
            "Let's consider a family of probability models indexed by a parameter Theta, and I put parameter in quotes because it doesn't mean a finite dimensional parameter, it just means an index of a class of probability distributions if that in that class is infinite, as it often is like a function space, then Theta indexes all the functions in your function space.",
            "OK, uncountable number of things, so people often pick up a decision theory book and they say to themselves just parametric stuff isn't relevant to me and that's completely wrong.",
            "OK, completely wrong.",
            "So Theta index is a function class or or measure space.",
            "Anything that you want.",
            "OK, so that's our family probability models with index and now having gotten data, we're going to find a procedure of some kind, maybe decision tree, maybe a support vector machine, maybe a graphical model that will take that data and produce some sort of a decision and estimate function.",
            "A decision of some sort.",
            "Alright, and now given that decision an given the underlying probability generated index by Theta, you define a loss function.",
            "OK, how much how bad you're going to feel if the truth was one thing and you made a decision of the form Delta X. OK, and I think almost any statistician would be happy at this point.",
            "This is just this is all fine.",
            "This is what you want to do.",
            "You need to write down a loss function.",
            "Kinda valuate how how good a procedure is doing, and now we're going to multiple procedures that could be D1, which is your decision tree in a D2 which is my support vector machine and so on.",
            "And like to compare these say which is best either in this situation or in many situations hopefully.",
            "So how can I do that?",
            "Well, the loss function is now telling me it's my measure of how good I'm doing alright, But the problem is the loss function isn't just a number I want to get a number to compare two things.",
            "You know, that procedure has loss 3.5 and that one has lost 4.2 that you know that ones better.",
            "But it's not just a number 'cause there's two unknowns there, X is random.",
            "So it's kind of unknown this about X and Theta is unknown.",
            "I don't know the probability solution underlying the data.",
            "So I've got two pieces of things that are unknown.",
            "How can I actually optimize over loss functions?",
            "Choose the right Delta.",
            "OK, so that's kind of the core problem with the decision."
        ],
        [
            "James to face alright.",
            "Well, you got two arguments of that function, so there's going to be 2 perspectives on how to get rid of that unknown knus you start with the Delta X or you start with the Theta.",
            "And those two perspectives are called frequentist and Bayesian.",
            "And that's my argument as to why there are two of them, because there's only two arguments to loss functions.",
            "OK. Alright, so this time of the frequencies went first, so the frequentist looks at the loss functions as well.",
            "You got two unknowns there.",
            "Let's start with the X one.",
            "Try to do something about turning that random X into a.",
            "Into a number somehow.",
            "Well, if it's random and we want a number, we need to take this expectation.",
            "So let's take an expectation of this quantity.",
            "Let's take an expression of the export.",
            "OK, well in expectation with respect to what probability distribution.",
            "Well, let's use the same Theta to take our expectation as is in the loss function.",
            "OK, so I don't need to know the truth operator, I just say whatever the truth is, I will take the expectation that I'll do this for all Theta, so I'll look at all possible truths.",
            "So I'm going to get a risk function.",
            "The frequentist risk here is like function of Theta.",
            "I've picked which data are priority is the truth.",
            "OK, but this quantity here.",
            "This E sub Theta is the expectation under the distribution indexed by Theta annexation over X.",
            "The X goes away here and we just get on the left hand side and that's a key.",
            "So if you need a definition of frequentist this is probably as good as any.",
            "In fact this is kind of 1 message.",
            "I want to give me a lot of people in machine learning.",
            "Are frequentist.",
            "A lot of the work is at least frequentist but you never see the word frequentists and you never people talk about the definition of what it means to be frequentist.",
            "Well here it is.",
            "This is.",
            "Here's one, it it means you take an expectation.",
            "For six data I will start to X right?",
            "In doing that, what you're being not gazing at that point you are taking expectation with respect to other X is over the entire sample space.",
            "Not the ex that you saw, but other extras you might see.",
            "OK, so that's what it means.",
            "We frequently you're looking other possible data you could have gotten the unconditional perspective.",
            "It's exactly so whenever you write down E in an equation and it's an easy averaging over X, you just veered away from the Bayesian.",
            "You've gone towards the frequencies route.",
            "Hope that's clear.",
            "OK, so that's what the frequentist does get started and now they have this function R Theta and they say, well, I don't know what the truth is.",
            "It could be any Theta and now I need still convert this thing to a single number.",
            "How do I convert that to a single number?",
            "OK, and now there's many possibilities and there's been huge literature.",
            "50 years of kind of work on ways to think about turning this into a single number, so the one that you probably heard the most about is mini Max, or you'll take the.",
            "The maximum of this frequentist risk overall Theta, and then find the procedure that has the minimum of the maximum risk and a lot of good work has come out of the mini Max perspective.",
            "That's one way to take get rid of that apart by taking the maximum overall Theta, then taking the minimum over that.",
            "So that's many Max.",
            "But there are other ways to do this.",
            "You could take subclasses of procedures so Delta X could be in a subclass, maybe all unbiased estimators or some other invariant invariant estimators.",
            "So on in that subclass this are afeta might actually have a.",
            "A simple characterization you can actually get out a single number over some subclass, and there are many other kind of ways of trying to approach that.",
            "Another thing you could try to do here would be to average over Theta, right?",
            "But when you average over Theta, you got average over some distribution on Theta, and at that point you become Bayesian.",
            "OK, you're trying trying something about a distribution Theta, so the frequency.",
            "Are you willing to do that in mathematics?",
            "'cause you can get out some mathematical results, but in practice, or trying to avoid doing that, the Bayesian, on the other hand welcomes the opportunity innovate over Theta, 'cause they think it's fine to put a distribution on Theta.",
            "And they're going to do that, and since they have one sitting around, they might as well average this quantity, not over suspect X.",
            "Both Spectra Theta and so here this E over here is a different one.",
            "This E is a conditional expectation given the data X additional perspective, an expectation over the data part of the loss function.",
            "OK, so this is still not a function of X, right?",
            "But that's less troublesome now because the axis soon to be conditioned on it's known it's fixed, so it's just now a single number, and that's called the Bayesian risk.",
            "So we have the frequentist risk and the Bayesian risk.",
            "OK, Bayesian should be interested in this because having to find this now they can optimize over Delta.",
            "And they can find the right procedure.",
            "You know it should it should I report the.",
            "The conditional mean the posterior mean or the conditional median or so and so forth.",
            "Well, it depends on which loss function have here and by optimizing models you can.",
            "You can pick that out.",
            "So if you haven't picked out a loss function then you just report the whole posterior.",
            "But if you have a loss then optimizing this equation will tell you which procedure to use.",
            "Alright, so you don't see enough of that kind of work in the machine learning literature of choosing the loss and talking about this particular risk function, and then this one you see a lot because this is what still learning people people work with and all other frequentists this frequentist risk OK, so the frequentist goes to the left the base and goes to the right and now you can sort of ask what happens if I keep going so I could take this quant."
        ],
        [
            "Here and I can act like a Bayesian average over Theta.",
            "I could take this quantity here and I can act like a frequentist.",
            "An average over X and neither camp would be very happy with you right?",
            "But what would happen at that point when you get a single number?",
            "Because now both things have been averaged over and will that number differ on this branch in this branch?",
            "Yeah.",
            "Just curious about the difference in notation for expectations on the right hand side.",
            "Here in this condition on X. Yeah, this is just the conditional expectation of this random quantity which random start to its condition.",
            "This is a constant, just a random quantity spectator, so this is the expectation with respect to Theta conditional next or over here this is the usual frequentist notation.",
            "They don't want to treat data as a random variable, so you put it as an index.",
            "So this is just the expectation of that particular property.",
            "Distribution is not conditional.",
            "Expectation is unconditional anyway.",
            "You get the same number by doing the two calculations.",
            "Yeah, what's the theorem that tells you get the same?",
            "Number.",
            "It's called Fubini's theorem.",
            "OK, just iterated expectations.",
            "We can switch that to expectations.",
            "OK, and what number do you get when you do this to expectations?",
            "It's called the Bayes risk, so you may have heard the Bayes risk out there and you might think you have to be Bayesian to use the Bayes risk, and that's wrong.",
            "That's not right.",
            "OK, the Bayes risk is gotten by either path of frequentist or Bayesian path.",
            "OK, so that's a little bit of decision theory.",
            "Hope that was interesting.",
            "Let's talk about.",
            "The issue."
        ],
        [
            "Of coherence in calibration to me.",
            "This is very helpful way to understand some of the starts in some other relationships between Bayesian and frequentist ideas.",
            "Let me just actually, before I do that, I said the word relationship.",
            "So if you go back to the previous slide, decision theory has been really the home of a lot of relationships between Bayesian and frequentist ideas.",
            "So in particular if you try to find optimal frequentist procedures and defined in various ways, there's something called complete class theorems that tell you that they are either Bayesian procedures or limits of Bayesian procedures.",
            "So from a frequentist POV, you often want to use Bayesian procedures 'cause you know.",
            "That they give you the class of optimal procedures.",
            "And of course the frequency doesn't necessarily use that theorem practice because you don't know with respect to prior and you're completely unknown, but prior.",
            "But you know mathematically that's a fact, so that's one class of connections between Bayesian or frequentist.",
            "OK, so coherence and calibration is these two words are used a lot to describe kind of bias perspectives on inference.",
            "David Draper is written a lot about these particular words, and a lot of other people as well.",
            "So there are two important goals for statistical inference.",
            "Coherence means coherence that you give out the same answer kind of number question.",
            "I got the same answer no matter what question you ask me and you can't find any incoherence among my multiple answers.",
            "Something like that.",
            "And calibration is.",
            "Means something like if I give you a number out, then that number means something that if you ask me to do a procedure multiple times an I claim that 95% of the time give the right answer within 95% of the time you better give me out the right answer, that's calibration.",
            "OK, so Bayesian work has focused on coherence, while frequently work hasn't been too worried about cars.",
            "I think it's pretty fair statement, so Bayesians get kind of coherence for free 'cause they have a joint probability underline everything, and that's the source of the coherence, and they love to bash frequentist because they find places where Frequentists work is not coherent.",
            "An lots and lots of papers written about that and the frequencies are not so worried about that they're interested in a particular inference problem at one particular time, and doing the best, finding the loss function that targets that lost that problem, and you know you can't be coherent all the time, you know.",
            "That's just life somehow is kind of.",
            "In my truck one soldiers you know I I'm not.",
            "I'm sorry not go here.",
            "You know well coherent is maybe the wrong word.",
            "I'm not going here in the mornings.",
            "But consistent is perhaps another word, but that's another technical meeting, so I'm not consistent here.",
            "I'll tell you one thing in one day, and then I'll tell you something else six months later.",
            "And that's just life.",
            "Now the other hand, frequentist workers tended to focus on calibration, so calibration again kind of, is like you know, social coverage that.",
            "The kind of the numeric values you associate with your procedure really do come out in practice, and Bayesians haven't been too worried about calibration.",
            "OK, now that's kind of a bit of a problem with the Bayesian perspective.",
            "You start writing out a bunch of priors.",
            "You write down procedure running your data, and that's it, you're done.",
            "Alright, what guarantees that you give me?",
            "And could you tell me you know that you did that procedure multiple times?",
            "It would come out to have the guarantee your guarantees turned out to be true.",
            "Alright, well Bayesians don't tend to worry about that.",
            "You know that much enough now.",
            "Good bayesians.",
            "Most Bayesian statistics are actually a little bit frequentist too, and so they will often look at a little bit of a frequency analysis of what they're doing and compare the coverage, for example, of their Bayesian procedure.",
            "Anyway, if you don't do that, if you just if you're a pure Bayesian, then you certainly get coherence, but you know you don't necessarily.",
            "Calibration, on the other hand, if you're appear frequentist, then you just worried about your calibration.",
            "You can be calibrated and completely useless, so 95% of the time you give out error bars that are appointed zero 5% of the time you got error bars that cover everything and so that the average works out to be some kind of confidence interval.",
            "But on any given set of data you give out.",
            "A useless answer.",
            "And so you can be completely re calibrating their hand if you're completely coherent, you can be completely coherent and completely wrong.",
            "It's clear, coherent to give out the answer one to whatever question problem you ask me.",
            "OK, but OK. And so most statisticians find some kind of a blend.",
            "A natural way to proceed because they tend to achieve both coherence and calibration.",
            "So in some sense, given you the answer my question of my title, I think many statisticians, not every single one, but many of them find that they are both a little bit busy, little bit frequentist and these things can be made into conflict.",
            "There are ways of focusing on calibration coherence and showing that one perspective doesn't achieve it, but they really do act complementary an 8 each other.",
            "And it's a little bit like wave particle duality is one way to think about it.",
            "Sort of.",
            "Waves and particles are both there.",
            "They're both could always be true.",
            "There's something right about both of them, but they don't quite really workout together as well as they should.",
            "I think it's true about Bayesian frequentist too.",
            "They're both right in some way, though.",
            "Both around forever, and one's not going to vanish.",
            "But they don't quite, you know, merge entirely.",
            "They do fight each other in various ways, in particular in testing and model selection problems, and I think they'll probably be eventually be some more of a resolution there even, but it's going to take awhile.",
            "Alright, so a few more comments about the kind of sociology really, so the frequentist world is this hodgepodge of people.",
            "You can do any kind of technique as long as you give me an analysis, that's frequentism, so it's just a big big field Bayesian a little smaller and it's really got 2 main subdivisions so."
        ],
        [
            "Have subjective Bayesian.",
            "You have objective basis is simplifying, but these are the kind of two main schools of Bayesian.",
            "Subjective Bayesian is believes that the prior comes from a person or maybe a small group of people.",
            "So the goal is to work with that person, a domain expert and you want to figure out what's the prior that person has in their head an what loss function do they have in their head?",
            "Also, an in the Model 2 is somehow I had to come from the domain expert, so you've got to also figure out what the model is illicit to model.",
            "OK, and the subjective Bayesian argument is that if you got out bad answers from your Bayesian procedure, it's just you didn't work hard enough to get the prior in the loss function in the model.",
            "Should work harder.",
            "Alright, and put that way, it's sort of hard to argue with.",
            "If I spent a million years and got the right prior, you know something I would get out the right inference.",
            "So what kind of if you just once you have the prior loss and the model you're done, they use Bayes rule and so on.",
            "There's not much else to talk about, So what kind of research did you is a subjective Bayesian.",
            "Well you do a lot of things you saw here this last two weeks.",
            "I think a lot of the work you probably saw was effectively subjective Bayesian, even though I'm not sure those words were not used.",
            "What does it mean when you have opened up new kinds of models, right?",
            "Why?",
            "Well, because I'm a subjective Bayesian, I'll go on face some new problem I'd like have a library of models I might bring to bear on the problem, so some of some of people worked on models.",
            "I'll have a big library, so that's one thing you could do.",
            "The other thing you can do is that Bayesian obeys rule.",
            "You have to integrate.",
            "You gotta get that denominator.",
            "So better developed lots of procedures for integration, 'cause that's gonna be hard to do, so a lot of algorithms work goes into subjective Bayesian research on integration."
        ],
        [
            "And then I think you probably don't talk as much about, but if you're going to be really subjective Bayesian, you really ought to worry about how to get those priors.",
            "It's not that easy and you better workout techniques for eliciting and assessing priors from individuals.",
            "There's a whole literature on that, and a lot of Bayesian neural network machine learning people don't focus on that nearly enough, so we really are going to be amazing.",
            "You better worry about how to do that.",
            "Anyway, those are the kind of some of the main areas of research.",
            "There are others, but those are the main, so there's not a lot of focus on analysis of did my procedure work and so on.",
            "That's really what frequentist do, because you know if you have the right inputs, the Bayesian outputs would be a good one.",
            "Alright, so again, you can't really argue about that from philosophical point of view, it's coherent, it's pretty, it's it's nice, but in practice there are really lots and lots of problems and the main one is that lots of work with really complicated models are hierarchies.",
            "There is multivariate quantities, there's matrices, so and so forth, and all of those bring new parameters into the problem.",
            "You know whenever you wish our distribution, you got a whole matrix of parameters set in there.",
            "You gotta put a distribution on that.",
            "Alright, well, OK, that's hard.",
            "And so the more complicated model gets, the more parameters than it's not going to take a long, long time to get a domain expert to kind of say, well, my prior on that wish heart thing this is this.",
            "And Moreover, if you've got long list of parameters, it's really the joint distribution.",
            "All the parameters you better be assessing.",
            "That's why you're supposed to get right.",
            "That's kind of becomes really hopeless.",
            "How do you do that?",
            "Well, they start making independence assumptions.",
            "You start throwing them in, because if I say, well, that's independent.",
            "Now I can think separately about this and think about it in the human domain, expert can get and start thinking about it.",
            "Now you're leaving, you know, based on the floor a little bit 'cause you're not really assessing the right prior.",
            "Simply for computational reasons, you often start writing a list of independence assumptions.",
            "At that point, you may have left kind of optimality behind.",
            "And now a subtle question issue.",
            "But just as important as the others is that it's really hard to get domain experts to assess the tail behavior whenever you're working with real valued quantities, which of course most of our models are as you go higher up in the hierarchy, they start to become real numbers.",
            "You want the probability of some discrete thing with the probability is a real number.",
            "You gotta put her on that so it has tail behave you have to worry about what tail behavior.",
            "You know I can.",
            "I can get my mother and talk about the mean and the standard deviation of something, but she can't tell me about whether it's Laplace, tails or or T tails or whatever, right?",
            "And I don't think many of you could either.",
            "I don't think I could.",
            "It's really hard to assess those things, and it's also hard to kind of learn them.",
            "Quote unquote, hard to get out of here.",
            "Does that matter?",
            "Well, some Bayesian models.",
            "It doesn't matter that much, but in lots of these emails that really does matter a lot.",
            "In fact, in some cases that determines the entire output of your procedure.",
            "And this is a really serious issue, so you will often hear people talking about Bayes factors in marginal likelihoods.",
            "How do you solve model selection problems?",
            "Bayesian marginal likelihood is the knee jerk answer calculate that.",
            "Well, the marginal likelihood is the integral of the likelihood under the prior.",
            "And so the integral under posterior, which tends to sharpen up and tail behavior doesn't matter.",
            "It's under the prior to the tails, are there.",
            "And your integral is if you have very fat tails, is going to be turned by your tails.",
            "That's in the prior that your assumption.",
            "OK. And so the marginal likelihood can be, you know, hugely determine you know affected by the particular assumptions.",
            "Bayes factors similarly based factors are ratios of marginal likelihoods, and so one way you might try to go say, well, use improper priors.",
            "You know, try to make them flat, so I'm not putting very many assumptions under in right as I think you may know marginal likelihoods.",
            "If you're in the prior and you have an improper prior, it has an arbitrary constant.",
            "And those things will tend to divide out when you calculate things like posteriors, but in marginal likelihoods and base factors they don't.",
            "You have a ratio of arbitrary constants, and so the base factor is meaningless in that case.",
            "Alright, so these are really serious issues and there's a lot of physical literature on this.",
            "There's things like intrinsic Bayes factors, an fractional Bayes factors, and various kinds of ways to approach this, but if you don't try to at least think about those things you know it's not.",
            "It is not the solution, it's not the hammer that solves the model selection problem.",
            "OK, so tail behaviors big issue nonparametric.",
            "So a lot of us.",
            "In fact when I wear a Bayesian hat as a researcher, I'm very interested in Nonparametrics and have been for about a decade.",
            "Think it's great, it's awkward for subjective Bayes because it's really complicated and nonparametric Bayes model is hard to think about.",
            "It sees stick breaking things and infinite objects and so on.",
            "What's my subjective prior on those things?",
            "And so a lot of subjective Bayesians in fact are not very happy with the nonparametric based movement.",
            "OK.",
            "So you know that may eventually get worked out, but it is currently an issue.",
            "OK, so that's kind of some of the problems that arise and I belabor them.",
            "Perhaps a little bit, because I really think there is a time, a tendency to sort of, say, the basic research is so easy and systematic.",
            "You know.",
            "How could anyone do anything else?",
            "Well, these are real issues that come up in real life.",
            "And then last one is more philosophical, which is just that a lot of frequentists don't like sort of subjective bayesians.",
            "Sort of telling that can't use a certain method, so I like the support vector machine 'cause it works.",
            "Right, I go in lots of applied situations.",
            "I will roll it out and it will work really well in someone's path and everyone is happy.",
            "I get paid and the company makes money.",
            "What's wrong with that?",
            "Alright, well it doesn't have a Bayesian interpretation, at least the obvious one.",
            "You might really build a work really hard to find one, but it's pretty.",
            "It doesn't have one.",
            "It didn't seem right.",
            "Well, do I have to wait around for someone to show me?",
            "It's amazing to use it?",
            "No, I could just use it because I can write it down.",
            "It works and then I could actually do some theory that shows that it has a frequentist justification, right?",
            "And there's lots of some simple simple kind of nonparametric testing situations where I just got a column of numbers here in a column.",
            "Here I want to say those two columns are different.",
            "Right, well there's these simple things that you just sort them and you you find its column one were higher in the list in column two.",
            "Develop a statistic that measures that and then prove that that will work on repeated usages.",
            "It's a perfectly good kind of approach to testing.",
            "I'm not.",
            "I'm not supposed to use that 'cause that's not Bayesian, it just sort of doesn't feel right.",
            "OK, so I hope by bash subjective Bayes enough that you'll be interested in some other things, yeah?",
            "Behavior in factors.",
            "So you're saying that, oh, this is only the exterior that we have to integrate over?",
            "Then it would be better because that is sharp enough.",
            "And I mean, yeah, it matters if you're integrating the light and then they're likely does shut up, so it doesn't matter whether it's you know it's the integral of the product of two things.",
            "Yeah, no, it does sharpen up Sir.",
            "No, it does sharpen up, but it doesn't.",
            "It's the rate at which things sharpen up and you've got to pick your tail behavior of a certain rate to compete with that.",
            "And you've got to do that effectively.",
            "Are two models in the numerator and denominator, and it's sort of getting all those roles right rates to line up is, which is hard.",
            "Yes, or maybe a little likelihood sharpen up, you're right, but it's important.",
            "A lot of people sort of think well problems go away when you integrate into posterior, which is true.",
            "But the whole point of margarine is prior, so you've got the tail behavior still has to be taken into account."
        ],
        [
            "OK, so objective Bayes you know.",
            "I really like objective Bayes.",
            "There's a whole conference on this that I went to the last couple of years and you know, I think it's like oh oh B oh now it will type that in and you'll see the objective based conference.",
            "So this is a great perspective.",
            "It really is a bridge between frequencies and Bayesian ideas.",
            "It's trying to find ways to set priors that aren't subjective.",
            "Maybe no human would have come up with them, but some sense there are sensible.",
            "They would give you protect you from making bad inferences and Moreover and really complicated models.",
            "It would maybe give you a way to set priors automatically that you don't have to have a human go looking at every the long list of parameters you have.",
            "Alright, so it's been a lot of work on this.",
            "Probably the best existing class of techniques are called reference priors, and the whole talk on its own.",
            "But what they do is they set up a variational problem where they maximize the notion of divergent between the prior and the posterior with respect to the prior.",
            "OK, so the distance between the prime the poster in some sense is the likelihood.",
            "So if you maximize that distance, you're making the likelihood do most of the work in the property as little work as possible.",
            "So that's a well posed variational problem and you can solve it in many situations and you get out of prior to this.",
            "Prayer has been gone from a domain expert.",
            "It's not normal piece of mathematics that tries to protect you about having an over an influential prior and many situation you get out improper priors by doing this procedure.",
            "Jeffreys priors and so on, but not all situations you get out proper priors in some situations as well.",
            "Anyway, this is an ongoing research project.",
            "It's very interesting.",
            "I would hope they would be lectures about reference priors here, but.",
            "Probably work.",
            "And so objective Bayesians, you know how did you pick the reference price is a good idea?",
            "Well, sounds like a good idea, but how did you actually kind of show it was a good idea?",
            "And Moreover you know there's many other kind of approaches to priors, which how do you choose between procedures or principles for choosing priors?",
            "Being subjective, so you use frequentist ideas, you will often sort of show that your Bayesian procedure choosing priors in some way has good frequentist properties and people kind of agree that's not a bad way to kind of get a guidance, so consistency properties are sometimes used.",
            "Admissibility properties are widely used, that's another frequentist idea that are to try to get good principles for choosing priors.",
            "OK, so I like this framework.",
            "It's a great area to work in, but the kind of downside is it can be challenging work within complex models.",
            "OK, so you have to kind of do the mathematics to get out your prior and often.",
            "That's really hard to do, and so that's an ongoing research project to kind of do that, and I'd say for a simple models this is often worked out and this is an off the shelf solution, but for a lot of the models that many of you will be interested in, it's not off the shelf, so you have to do a lot of work to use."
        ],
        [
            "Objective based ideas alright, but I just want you to be aware that this is a counterpart to subjective Bayes.",
            "OK so Lastly.",
            "The frequentist perspective so.",
            "So the frequency project is very is very Catholic.",
            "Procedures can come from anywhere.",
            "They don't have to be derived from a probability model of conditional and or they have to be derived from a probability model.",
            "So nonparametric testing just kind of sensible sets of test statistics and show that they worked support vector machine boosting or kind of things that weren't drive from a probability model.",
            "Also I'd like to mention things like methods based on 1st order logic, right?",
            "So you can have a data set data comes in, you have a big first order logic machine and outcomes some answer right and?",
            "That's good, nothing wrong with that.",
            "And as a frequentist I would want to sit down and say, well, is that a good procedure?",
            "Is that just 'cause it's logic doesn't mean it's necessarily, but it might be good, but I can analyze it from a physical point of view, and that's what frequentism does.",
            "They would say OK. Is that procedure on repeated datasets give me an answer which is good in some notion of loss?",
            "Right, and so I often get in argument people in kind of the more AI side of machine learning, saying, well, there's statistical machine learning, and there's the arrest of machine learning.",
            "And I would say what is that was that other object was that the other partition logical sort of stuff and I said, well, you know it's it's.",
            "These are completely agreeable perspectives.",
            "You can take your logical thing.",
            "I can evaluate it statistically and they said, well, OK, that's fine, but no Bayesian that's different from logical and I just sort of start find these distinctions a little bit unhelpful at times.",
            "So this frequency perspective particular is just an analysis tool and it can analyze all kinds of things.",
            "So I think a machine learning and statistical is the inferential problem of taking in data and getting out knowledge and.",
            "And frequentist perspective is very much part of that.",
            "OK, so if you can get your methods from anywhere, you know I can write down, you know, Mike Jordan Cilius method ever.",
            "And you want to be able to rule that out.",
            "So what frequencies mostly do is they develop techniques of analysis.",
            "That is your rule, out stupid methods into rank the reasonable methods.",
            "So it tends to focus more on analysis and methods.",
            "But I did want to mention one and passing one general frequentist method.",
            "It's the bootstrap.",
            "It's kind of as automatic as Bayesian procedures can be used on all kinds of problems.",
            "It's just a general methodology that's very frequently so.",
            "The bootstrap is that you take your original data set and you re sample it multiple times, and in doing so you're looking at alternative datasets.",
            "You're exactly being a frequentist from mythological POV now.",
            "Not so much an analysis point of view.",
            "Of course.",
            "Then there's analysis to show that procedure has good frequentist properties itself, but it's very interesting broad class of techniques.",
            "OK, so."
        ],
        [
            "So.",
            "Oh, I think this is one more slide on kind of introduction and I'm going to move on to some more concrete stuff.",
            "So what do you do is a frequentist?",
            "What kind of activities do you do?",
            "Well, you also write down models.",
            "You develop procedures and all that, but more the kind of the analysis side of the story is kind of hierarchy of mathematical things.",
            "You do.",
            "First of all, you may try to prove consistency that if there is a correct answer, you'll converge to that no matter what that correct answer was.",
            "So that's often kind of fairly straightforward and not that informative, more informative thing to do is to get rates of conversions towards rates of convergence.",
            "02 procedures are both good.",
            "They're both consistent, but maybe one of them has a faster convergence rate in terms of number of data points.",
            "I might want to prefer that procedure alot.",
            "Alot of work is on that and more hard, but also very important is to try to get sampling distributions that as the number of data points gets large.",
            "Perhaps I converted some nice distribution like a Koshi or normal or something like that, and then I can use that distribution I can get.",
            "Error bars so I can get error bars by finding out the sampling distribution.",
            "OK, so there is certainly work on consistency and animal literature.",
            "There's some on rates and there's very little long sampling distributions, so classical frequentist statistics.",
            "We focus on parametric statistics in the 40s and 50s, but since then it's mainly been nonparametric.",
            "Really, there's a lot of nonparametric testing and there's tons of other kind of person function estimation.",
            "And all these large P small in problems where these are going to Infinity.",
            "The number of parameters is going to Infinity as well as the number of data points.",
            "And so on.",
            "So often you'll see people say, well, classical statistics was parametric and so on.",
            "But you know it's just not.",
            "The tools were developed to be general Nonparametrics is probably part of the story, one of the most general tools, its empirical process theory, empirical process theory talks about convergence of objects uniformly, so you find consistency rating distribution uniformly on various spaces, function spaces, parameter spaces, measure spaces, and so on, so forth.",
            "So statistical learning theory is really a part of that, it's it's a particular area of empirical process theory that focuses on 01 loss.",
            "But the tools there, Rademacher, and all that were developed in empirical process, their whole books on this.",
            "So if you're interested in theory, this tool is available.",
            "It's used to prove things about the bootstrap.",
            "It's used things to prove things about them, estimators and so on, so forth.",
            "A lot of frequency analysis using this big heavy tool.",
            "And there are lots of other tools that are simpler, but that ones always always available.",
            "OK, I'm going to take a little positive there.",
            "Any first many questions and then just kind of stretching, pause and then the rest of my presentation today and the next time.",
            "Are going to be some little vignettes on research that I've been involved in.",
            "That is all frequentist and try to give you a better flavor.",
            "What quiz activity really is like?",
            "What kind of problems are set them up and see that there's some challenges there and see how to overcome them.",
            "Sewing machine learning methods, but then analyze frequently POV and try to carry all the way through to the end.",
            "So I think I'll probably the rest of this talk.",
            "They talk about experimental design and then these things.",
            "Three things will be for the next presentation, so any questions on sort of the philosophical stuff first.",
            "Yeah.",
            "Boots that would.",
            "Yeah, I mean.",
            "One of my current favorite books are just kind of statistics in general is add vendor barks asymptotically statistics.",
            "Add takes a Catholic view, has got Bayesian and frequentist arguments throughout it.",
            "It's probably more frequently overall, but it's it's got Bayesian theorems as well.",
            "Jim Burger if you don't.",
            "If you not been introduced to James Burger yet, you should be.",
            "He's got a great book on statistical decision theory.",
            "The first addition of it was frequently in the second issue was Bayesian, and it's kind of good to read both of them.",
            "I he anyway and then the second issue as well, there's just.",
            "There's a lot of merging of frequentist and Bayesian ideas, and I just think reading his book and his papers in general is a very good educational experience.",
            "Yeah.",
            "Is also subjective.",
            "Is objective based also subjective?",
            "Well, you know?",
            "Yeah sure, in the sense that I've written down a big complicated model and some of the parameters I'm going to possibly able to elicit subjectively.",
            "And there's a whole bunch of others that are often called nuisance parameters or whatever that I don't want to or can't listen should actively.",
            "I'll try to use objective Bayesian methods for those so you know, most Bayesians do this.",
            "Actually in real life they will sit down and say something with this parameter.",
            "I sort of believe is in this range for this or that reason.",
            "And there's this scale factor.",
            "I have no idea what it should be.",
            "Let's put a Jeffreys prior on that now.",
            "That's kind of ad hoc thing to do, and there's a lot of ad hoc re, but that's kind of your in effect blending objective based in subjective base.",
            "So I think objective base is kind of big.",
            "Tend to that incorporates subjective as well as objective ideas.",
            "Yeah.",
            "I'd like to make it interesting that yes.",
            "OK, no, the diapers are not functions of the data in reference priors.",
            "This is your as you're sitting there.",
            "You haven't seen any data yet at all, and you're thinking about what prior should I use?",
            "You're free to envision datasets you could get.",
            "You haven't seen any data yet, but you're free to sit and imagine possible datasets you could get.",
            "OK. And the vergence function in the reference prior is that expectation over possible datasets you could get.",
            "So in fact it has a little frequentist kind of mathematical character, but it's perfectly Bayesian.",
            "Babies are freedom to dream about datasets and not just one data set.",
            "Yeah.",
            "And if you have to imagine the possible datasets that you that you're going to get, are you then?",
            "Solving the problem you want to solve.",
            "I mean, don't you need for being respected your posterior in order to know which data set?",
            "How do you get your posterior prior to get your posterior we're getting prior from you know, but I'm just saying like if you say I need to use the, imagine the datasets that I'm going to see, but isn't that the whole point that isn't that knowing your posterior?",
            "So if you don't know your posterior, how could you do that?",
            "No, no, I've got I. I wrote down a probability model in beginning, everybody agrees.",
            "You sort of have to start there.",
            "OK. And now I can imagine datasets under that probability model.",
            "So that defines my probability measure and I can go from there and now take averages with respect to that, and that's what the reference parts does.",
            "Just you just need the likelihood.",
            "And given likelihood you do this.",
            "Hope this divergences maximization thing that gives you a prior and now you see a real data set you put your prior together real, like you're likely on that data set you observed and do a posterior.",
            "It's perfectly Bayesian.",
            "Statistical reference does depend on the size of data set that you're expecting too, yeah?",
            "Depends on that.",
            "It's sort of an experimental design flavor, and you know that which is which is arguably a good thing.",
            "You kind of want to think about how your data will be gathered.",
            "There's a very good Jose Bernardo was the first person to talk about this in great detail, and he's got a lot of papers talking about Huawei experiment design.",
            "It should be taken into account in Bayesian arguments.",
            "A lot of the likelihood principle shouldn't, and that is sort of a misleading argument.",
            "OK, so if you want to read more about this, read some Jose's papers.",
            "Yeah, so it seems that.",
            "The introductory imagine your potential dates that you could have 20 proprietors kind of similar to that, so you imagine your privacy or updates that you might have and try and model that in the prior and then in the objective framework you trying imagine data.",
            "Now your data set your measure on X space, so the data is in X.",
            "You know we're trying to use that somehow.",
            "Get some Theta.",
            "OK, so that didn't help you.",
            "Imagine datasets in X to get something on Theta, right?",
            "Whether you're doing you're simply trying to find a prior that under these datasets you're imagining, quote, unquote.",
            "Has little impact on your posterior as possible.",
            "And it's pretty neat that you actually just write that down.",
            "Write down the KL diversions with respect to prior and posterior.",
            "You average that over X.",
            "That's where the averaging is coming.",
            "You don't know that yes, you can average over all these possible dates that everyone you get the mutual information.",
            "And you solve that problem and out pops it prior, and it often is a Jeffreys prior, and it has a lot of nice properties.",
            "I should go over that side.",
            "There's a lot of questions here, yeah?",
            "Nothing like this.",
            "Use their answers to rank different estimators, yeah?",
            "Anything you can come up with our boundaries on the quantitative, just not the only thing you can come up with.",
            "The balance.",
            "That's not true.",
            "Lot of physical or any people only use bounds.",
            "'cause somehow the I think it's more of the CSR strategy.",
            "It has to be bound.",
            "But most of the decisions use some products of all kinds expansions that aren't bounds that are hopefully tight.",
            "Is it necessary that optimizing those is going to give you the best procedure?",
            "But no.",
            "I mean, analysis is kind of always grains of salt.",
            "You know there's you did an analysis and you got an answer, and there's a little bit of error.",
            "There's a Taylor 3rd order term you've neglected, and so on, but that's what mathematics is about.",
            "Sort of getting an understanding, and maybe you know not being quite exactly right, but hopefully get a guide and then explore it further.",
            "Yeah, there's no.",
            "You know.",
            "I always rank that procedure over that.",
            "There's always a little bit to this kind of understanding, the setting and the consequences of the analysis tools available for evaluating methods that give out probabilistic predictions and soft predictions.",
            "Point estimate.",
            "I mean, that's what frequent is mainly focused on its coverage.",
            "Not the not just the point estimates.",
            "You should not cut.",
            "Yeah, your loss function can for example be a log loss of predicted distribution.",
            "You can study that it's frequentist convergence in terms of some procedure.",
            "It's commonly done.",
            "Yeah, absolutely maybe one more question then I want.",
            "I do have a second part of the talk that I want to get too.",
            "So yeah, sometimes I worry about it being recommended for general tool.",
            "Is that when you often get two data points exactly same place now something my mother finger who like noise level and do something crazy so thankful Bayesian model works.",
            "Yeah, so there's all kinds of bootstrap literature and there's the simplest bootstrap.",
            "As you know, you know some some issues that that occasionally come up, but there's a general technique called resampling methods, and there's a lot of worry about that.",
            "In fact, you can prove their situations with Bootstrap is not consistent.",
            "OK, but there are correction.",
            "There are better bootstraps which are consistent.",
            "Alright, there's a whole literature.",
            "There's a book on resampling Joe Romano and others have been very nice book on on that whole technology.",
            "So alright, let's take a short.",
            "I guess it's a 2 minute break and let me have time to get through the rest of my talk.",
            "OK, those are all great questions and I was glad trigger all of them.",
            "I really wouldn't mind spending the rest of the time just talking about those things.",
            "But you know, I do a similar cover.",
            "So while I'm around the rest of the day, if you want to chat more about those things, I'm happy to talk."
        ],
        [
            "Anyone about those?",
            "So as I said, what I'm going to do, the rest of my presentation today.",
            "Tomorrow is go through some vignettes of particular problems and show you get a little flavor of how you do frequent analysis.",
            "OK, this first vignette is about Los Funk."
        ],
        [
            "In classification and about experimental design.",
            "So the main paper this was based on came out last year.",
            "Like my college join long again, Ann, Martin Wainwright and there are two 2 backup papers that also play some role in development of these ideas.",
            "So this has to do with things like boosting support vector machine and sort of classification algorithms.",
            "And there's they all kind of came out.",
            "Separately and then there was some realization.",
            "There's a lot of unity and the ideas and there was frequent analysis that came out.",
            "You know, for example, show that they were consistent, so boosting eventually was shown to be consistent.",
            "It was not clear in the beginning and support vector machine and so on.",
            "So we're going to face a hard class of problems.",
            "We're going to not just classification, but also experimental design simultaneously with classification.",
            "And now we're going to ask, are things like the boosting loss, the SVM loss, and so on and so forth still consistent even though you're doing this harder problem and that I wouldn't know how to answer that question in less, I did this analysis an having done the analysis, then you actually learn that actually turns out some classes of these loss functions, leading City, and some of them don't, so it's kind of a little bit surprised I'll leave you to kind of guess which of them do in which other, don't you?"
        ],
        [
            "And what the setup of the problem is.",
            "So the way we got the original involved in this was a practical problem that some people at the Intel Lab in Berkeley had one of these early sensor networks, and they ask us to solve a classification problem here, which is that they had a bunch of sensors on a grid.",
            "This were up on the ceiling, and there was a little robot moving around and it had a light source on it and they wanted the robot had gone into a particular region of the room or not, so the region was some green region.",
            "It could be convex or not, they just want to know whether or not these are highly noisy.",
            "Sensors, and so it was kind of hard classification problem to solve, but Moreover the more interesting part was that these sensors there's one of them over there.",
            "Have these little batteries and they if they transmit data all the time, the batteries run out immediately, so they need to transmit only a little bit of data like one or two bits for time slice.",
            "So you can't transmit the real valued voltage you're sensing of the light.",
            "You need to transmit a quantized version of that, and the question became how do you quantize what's the what's the optimal way to quantize?",
            "Given that my problem is one of classification.",
            "If it was just data compression, I know how to quantize, but now I'm trying to quantize for the purposes of classification.",
            "How do I do that?"
        ],
        [
            "Alright, so here's the abstraction of the problem, so there's a bunch of observables X one through XS that are often real valued quantities.",
            "But since they've actually gone through a analog to digital processor, they've actually become quantized.",
            "But the cardinality of the quantization is really big, so M is really, really large.",
            "Alright, we're going to quantize those with Quantizers Q1 through QS, and these are distributed, so this is this is a different spatial locations, right?",
            "And so this Q here doesn't get to see any other ex is this is a local calculation, so this quantizer then spits out a Z1, which is the quantized version of X1, and so on.",
            "And this one it really is quantized in its cardinality, is much much smaller than M and then the central.",
            "These then are transmitted to a central site over the radio and the central site hits a discriminant function to the Z values and tries to predict.",
            "Yes or no.",
            "You're in the green region and of course you were in the green.",
            "Are you or not?",
            "And depending on the value of that hypothesis, you get different distributions on your light sort sensors.",
            "That problem is called decentralized detection.",
            "It actually exists in the literature before we got involved with it in the 80s was a hot topic in electrical engineering."
        ],
        [
            "Signal processing.",
            "OK, so the general set up is going to be XY pairs.",
            "Let's assume their ID for simplicity.",
            "The wise are going to be 01.",
            "We're going to have a quantizer now.",
            "That takes the original covariate vector X and turns that into a quantized version Z.",
            "And it would be quantization space in which Q lies in an Q is the space is some sort of random mapping, so it for our analysis that needs to be set of random mappings.",
            "But in practice, we would often implemented as a deterministic mapping.",
            "OK, so Q is a random mapping all.",
            "Right now we're going to stick analysis of this kind of object and what kind of object is this?",
            "Well, this is known as an experimental design, an offer mazina something like an analysis of variance table.",
            "You know a person comes in and they went to sell 1, three or cell 45.",
            "That's experimental design, but it's really a broader the broad mathematical problem is really just a kind of a mapping from some space Zita.",
            "Some specs to Z.",
            "And the mapping random.",
            "In fact, in the analysis variance it is you come in here it's a randomized experiment you're assigned to some selling or some random assignment.",
            "So the Z space here would be the cell of the analysis of variance table.",
            "And that's a discrete variable.",
            "Right index comes in its description of the person who gets put in one of these cells.",
            "So that's just one example of a mapping Q.",
            "But then there are many others.",
            "So to set in generality you just allow you to be in some space and then you have the space be characterized in various different ways.",
            "So I will use the language of experimental design.",
            "Quantization is a special case of that analysis variance as a special case, and so on.",
            "If you prefer one of those.",
            "Special cases think in those terms.",
            "Now that's half the problem is, is the thermal design, but the other half of the problem is the discriminant function I the classifier.",
            "So we have this family of classifiers that lie in some family big gamma, and it's probably going to be a large family, nonparametric family and our problem is to define the decision like in our decision theoretic framework to know decision decision was two parts, it's to choose the quantizer Q and the choose the discriminant function.",
            "That's the output is this tuple.",
            "Alright, and what's our loss function?",
            "Well, the risk is going to be the probability made an error, so that's why they want the quantized discriminant function value is not equal to the correct label that is A10 loss function.",
            "If I take the expectation that the probability that they're not equal.",
            "So this is the risk function as a function of Q and gamma, so it's different notation, but it is fits in my decision theoretic framework we talked about earlier.",
            "OK, so."
        ],
        [
            "There are many applications of this.",
            "OK, so if you look at the existing literature, there's sort of help on two sides of the equation, but not on both of them simultaneously.",
            "So the classical signal processing literature define this problem.",
            "Decentralized detection, and it assumes that everything is known except for the quantizer.",
            "OK, everything means that all the probability solutions are known.",
            "The class conditional probability distributions, the class prior probabilities and so on.",
            "So all that's not known is Q.",
            "Might and so how do you find Q and something a little drawing?",
            "I didn't do it on the slides here.",
            "I guess I turn on that light over here.",
            "I didn't come on.",
            "did I do that wrong?",
            "No, I didn't work.",
            "So in my original space, there's the X space over here.",
            "You know they had Class 1.",
            "And class 0.",
            "Maybe looking like that and it's sort of hard to discriminate boundary among these two things, so I might want to use a mapping queue which goes over to a space Z, which pushes them as far apart as possible.",
            "Right, that would be a good choice of Q, and if I did a bad choice of Q it would smash them together even further.",
            "Alright, so all I gotta do is measure in some ways the divergences among probability distributions and then optimize Q with respect to the versions.",
            "What version should you use?",
            "Alright, well you're trying to maximize divergences here.",
            "It's often we talk about minimum divergences, a maximum divergent problem, and so these guys said, well, what are some divergences you can maximize?",
            "And so they wrote down lots of kind of function functionals on probability solutions, and they found that some of them were easy to maximize.",
            "Some of them were not, and so they picked those, and that's what they did.",
            "So hell, injure badura a whole bunch of other kind of came out of that literature and became famous in other fields.",
            "And they were set up because of this problem of diversions maximization.",
            "So lots of radar has been done this way.",
            "You know, it's a big literature where people have just picked a diversion, say, hell injure turn off or something.",
            "And then it's a function of probability solution.",
            "But you assume those positions are known.",
            "And so just write down the expected vergence maximized spectacu and then pop out that Q back to the user and so then you put that into your radar on the radar quantizes.",
            "In that way it's called signal set selection.",
            "Alright, so that's the story and I would view this as a basically a heuristic literature.",
            "It could turn the lights off now that I can have control control.",
            "So it's heuristic.",
            "What do I do?",
            "Two sets of switches.",
            "Alright, so it's basically using a plugin and then not really worrying about how well it performs.",
            "You put it in and you don't then try to evaluate how well that does.",
            "Alright, so the simple machine learning literature in their hands as focused on problems where the whole problem is to find the discriminant function and not worry at all about the experimental design.",
            "So you assume that that's known and you try to find that OK, and the way it's done is by defining a surrogate loss function, boost indiscretions, Porter machines are all based on surrogate loss functions, and this is kind of it's more rigorous.",
            "There's a decision theoretic flavor, there's consistent results and so on so forth, but isn't really facing the whole problem, which is to find the Q and the gamma simultaneously.",
            "OK, so let's build up a little machinery.",
            "Let's talk about the."
        ],
        [
            "F divergences these are the guys that have been discussed by the signal processing literature and of course they then appear in many others as well and kind of part.",
            "Part of this talk is going to somehow unify these things.",
            "It's just a list of things in your mind.",
            "There's gonna be relationships here.",
            "So in this talk about discrete random variables, just for simplicity is alright.",
            "So instead of integrals, but you can do this with continuous as well.",
            "So you define the F diversions between two measures mu and \u03c0, as F of the likelihood ratio, and the integrate, or you average with respect to the pie.",
            "OK, so that looks kind of like Hilda vergence, and in fact, if F is chosen to be you log U, you get KL diversions.",
            "Alright, but if F is chosen to be absolute value minus one, for example, then you get out the variational distance, which is here just the L1 distance on measures F can be any continuous convex function, so these are particular examples, But here's another one.",
            "And if you plug that one in, you get out the Ellinger distance, and this goes on for several pages.",
            "So you plug in any continuous convex F, and you look at you now define the new ally, Sylvie or F. Vergence."
        ],
        [
            "Right, so why did these guys use the app divergences while they were somehow intuitively appealing, but there was a little bit of kind of underlying theory behind that choice.",
            "It's not entirely satisfactory.",
            "In fact, it's not really satisfying at all, but it.",
            "It's a good starting place.",
            "And the theorem was due to David Blackwell in 1951.",
            "Classical paper.",
            "Well worth reading had a big impact on economics.",
            "And so his theorem stated the following.",
            "If a procedure, a IE, some kind of an estimator has a smaller F divergent than a procedure be for some particular choice of F in your F diversions.",
            "OK, then there exists some set of prior probabilities.",
            "These are the class probabilities that you're in one class or the other, so some set of problems in the class is such that procedure A has a smaller probability of error than procedure be.",
            "Well, that's what you care about.",
            "That's the risk.",
            "So we've now just learn the procedure as a smaller risk than procedure be.",
            "That's good, we should now choose proper procedure A and we were told that by looking after Vergence is.",
            "So now I have to watch it gives us some information risk, right?",
            "Well, this is just an existence statement, though it says there exists some set of problems we don't know what the problems are.",
            "For which that after versions gives us this ranking.",
            "And we don't know in our particular problem which after versions to use.",
            "OK, so it's not that help when it's not at all helpful in practice, but it does at least suggest that after versions are not unreasonable objects to be looking at, if you're trying to minimize the probability of error, and that's a good thing, because minimizing the probability error is course nonconvex problem, the risk the 01 loss is non convex.",
            "And so you try to find some other kind of function you can optimize, and these things are convex and therefore you might try them by this theorem.",
            "So that's what people did.",
            "There's famous papers, Kayla, the 1967 and so on, choosing particular divergences and just kind of in some sense, hoping that the priors now we're right for that afterwards on the particular problem.",
            "OK. Alright, now there are some supporting arguments for asymptotically these divergences also arise in other ways.",
            "In fact, the original callback leader, Vergence, arose by analysis of hypothesis testing.",
            "It characterizes the power function in hypothesis testing, where two classes are staying at a fixed distance apart.",
            "As the number of data points gets large and similarly turn off, distance arise when you do the same analysis in the Bayesian setting, we have priors longer classes, so these divergences were kind of in some ways talking about probability of error directly in this case, but it's an asymptotically arguments hypothesis testing.",
            "So anyway, it's still a heuristic literature.",
            "Honey, let's turn to the other side of the coin.",
            "How to choose the discriminant function, and so you kind of know know this stuff.",
            "This is a machine learning kind of 101.",
            "You choose a loss."
        ],
        [
            "That measures the distance between your class labeling your discriminate you were going to start with.",
            "01 loss is kind of the real loss we're trying to optimize, and in the binary case you can write that as the indicator function of when the labels disagree.",
            "OK, the labels disagree.",
            "In fact, now I'm using Y is 1 -- 1 and the discriminant function outputs also one or minus one or a real number in general.",
            "So if they disagree in their sign, that's bad and you pay a loss of one in that case.",
            "Otherwise you pay loss of 0.",
            "OK, so mainly focuses on the discriminant function.",
            "Now we know also from this point of view, it's intractable.",
            "Minimize your loss affected this argument as well.",
            "So instead what people have done is they pick these surrogate loss functions which are convex up."
        ],
        [
            "Ground 01 loss, so hopefully you've all seen this picture.",
            "Here is the 01 loss expressed in terms of this margin value, either product of the Y and the gamma of Z, and if you disagree, you're on this side.",
            "You pay with loss of 1, otherwise you pay loss of zero, and so it's intractable.",
            "To optimize this instead, people look at these upper bounds and the blue line is the is the support vector machine, the hinge loss, the red one.",
            "Here is logistic, the green one I think is the boosting loss the exponential loss, which is what.",
            "What gives you boosting?",
            "And there's a whole bunch of others that are that you know this.",
            "This page could be littered with examples.",
            "OK, so all those different procedures aren't that different from, you know, optimization point of view.",
            "You just write down these just optimize over these particular choices curves and then you hopefully try to prove something about them.",
            "What can you prove about them?"
        ],
        [
            "Well, let's try to set up a little bit of the theory so.",
            "And we're doing optimization here.",
            "This is sometimes in statistics is called M estimation.",
            "M estimators.",
            "You write that a function often is called a contrast instead of loss that distinguish between the thing you're trying to analyze.",
            "This is a procedure, not analysis.",
            "But you're optimizing, and that's called em estimation, and the machine learning that is often called empirical risk mitigation system is the same idea.",
            "Alright, so we have this idea.",
            "Training data we write down and M estimation functional, which is this object here which just takes our contrast function or loss function.",
            "So the exponential loss or the hinge loss, we sum it up over our datasets and we call that function.",
            "We'd like to optimize either, so it's an empirical risk if you will.",
            "OK. Alright, so here are some theory for this object.",
            "This is a paper that my colleagues and I worked on that gives you necessary and sufficient conditions.",
            "There have been work on sufficient conditions.",
            "This gives a full treatment of these surrogate loss functions.",
            "It gives both necessary and sufficient conditions for consistency, so we're trying to say if you use these loss functions, did you get the same answer?",
            "The end as if you had optimized 01 loss?",
            "That would be a satisfying story and all those guys do that.",
            "And here.",
            "Here's here's a theory that tells you that that is the case.",
            "Alright."
        ],
        [
            "So first of all, not any arbitrary fee can be used.",
            "It has to satisfy some properties.",
            "In particular, we had a very weak condition called classification calibration, which is essentially a form of Fisher consistency.",
            "And here's the equation that defines it.",
            "Let me not spend a lot of time on this, But basically this says that if you disagree with the right answer, you pay a bigger loss that's bigger than if you don't disagree with the right answer.",
            "OK, so this sort of says that things kind of tilt up to the left that on the left side where you're making an error you have a bigger loss than on the right hand side where you're not making an error.",
            "OK, so it turns out to be necessary and sufficient for what's called Bayes.",
            "Consistent, it's not a Bayesian notion as I hope you remember.",
            "I talk about Bayes risk earlier in the lecture.",
            "This is consistency in the sense of 01 loss.",
            "OK, so we will now define a surrogate loss function to be something that is classification calibrated.",
            "That's the definition of this object here.",
            "It satisfies this property.",
            "All right now, the one that you can forget this definition because in the convex case where Phi is convex function, then it's clear if and only if its differentiable at zero and it has a negative derivative at 0.",
            "So all those curves and don't turn the light 'cause it's not worth it, all those curves.",
            "Tilted like this at the origin, they had a strictly negative derivative here and.",
            "And that's all you need for classification calibration.",
            "So it just matters what happens around the origin.",
            "OK, so that's just kind of set up for the rest of this talk.",
            "That's what we mean by a surrogate loss funk."
        ],
        [
            "And it turns out that those surrogate loss functions defined in this kind of machine learning literature turned out to have a very nice relationship.",
            "They have divergences sort of surprising but true.",
            "So there's going to be constructive and many to one correspondence between surrogate loss functions and afterwards they go back and forth in two directions and haven't done that.",
            "There's about 2 space that we can work on.",
            "You can be instant loss functions, and you can go instead and work in the space of divergences or vice versa.",
            "And by doing that, we're able to define a notion of equivalence among loss functions.",
            "So two loss functions will be equivalent if they map roughly over into the same after vergence.",
            "Not quite, that's not exactly right.",
            "There's kind of range of divergences, but we will define a notion of equivalence, and with that notion of equivalence, and it could be extremely easy to prove things like that.",
            "This procedure is consistent, and this is not.",
            "So it's kind of a nice characterization of a space of loss functions, space of loss functions has some structures, not just a list of loss function that has a lot of structure to it, and we have a theory that shows that explicates that structure."
        ],
        [
            "OK so I got about 10 minutes right?",
            "Is that actually correct?",
            "Is that give or take five?",
            "I start a little late, but I'm trying to figure out where I'm going to be in the middle of this talk by the end of this, so I'm going to figure out where to how far to go.",
            "OK, so let's just set up a little bit of notation.",
            "This is kind of dull, but necessary notation, so remember, the risk function is a function of the expectations.",
            "The frequentist expectation of the loss and then the data is the Y in the Z.",
            "It's a tuple, and then the parameter which was Theta back in the original slides does not become a tuple as well, it's both.",
            "It's both the discriminate in the quantizer.",
            "For simplicity, it would be nice to work with conditional distributions of Z, given why the unnormalized conditionals, i.e.",
            "The joints so musian, piussi.",
            "Are these class conditional densities unnormalized?",
            "So P little P&Q are the priors he was 1 -- P?",
            "And we integrate out the X, which is the unobserved covariates, and so this is not just a function of Z, so we take the class conditional distribution, integrate that out under the quantizer, we get a distribution on Z. OK, so using this notation you can now represent the fee risk.",
            "The expectation here is of course over the two values that why can take on and so if we just do that expectation over why we get a new Anna Pie and we get a minus from Y equal to minus one and a one where Y is equal to 1, add those two up.",
            "That's the expectation over Y and then we do the expectation over Z where the mu in the Z are the measures.",
            "OK, so that's just a representation of the risks.",
            "So this kind of looks a little bit of a convex flavor.",
            "The fee tilts in One Direction and fee of minus tilt in the other direction.",
            "This is a convex combination of that kind of, roughly speaking."
        ],
        [
            "Some convexity properties already emerging.",
            "OK, so now as a frequentist you're free to do something what is called profiling.",
            "I have a function of two arguments and.",
            "How do I get rid of one of the arguments so I can optimize structure?",
            "The other argument alright?",
            "Well if you're a Bayesian, the kind of thing you want to do is to integrate out one of the arguments, but we don't.",
            "How do you integrate out the discriminant function?",
            "I don't know how to do that, but I can optimize it out.",
            "That's called profiling statistics to optimize out one argument so you can focus on the other ones, called profiling.",
            "You often with profile likelihood for example.",
            "Alright, so let's profile this risk function by by optimizing out the discriminant function and we get a function.",
            "Now just of Q and then we could use that as a function too for choosing Q. OK, so let's do that now for some examples.",
            "So if we choose 01 loss.",
            "You can easily figure out you can do this optimization and the answer is just that it's a difference of sign of the difference of the two measures, which makes sense if you're doing 0, unless you want to pick the guy that had the bigger measure.",
            "So you plug that back in to this optimized you will get this function here and I've just done that in a couple of steps here.",
            "If I plug it in, you just have to do a little calculation.",
            "If your classification you will know how to do these calculations.",
            "If not, get out of paper afterwards.",
            "It's really easy, you plug back into 01 loss.",
            "You get this this minimum.",
            "Right makes intuitive sense.",
            "You should pay the loss of the worst of the smaller class an if you just didn't know the absolute value is equal to this minimum.",
            "It just really easy to see and then this thing here is 1 minus the variational distance, just by definition.",
            "OK, alright, so it turns out this profiled risk.",
            "Happened to be the negative of a of a vergence that's kind of interesting, so if you use this divergents effectively, what you were doing was working with the Profile 01 loss.",
            "That's kind of interesting, right?",
            "OK, so we did that calculation and we thought well, does that hold more generally for other kind of losses other than 01 loss?",
            "And turns out it did turn out.",
            "It's a really fun exercise to do this for all kinds of losses.",
            "If you start with the hinge loss.",
            "For example, an you profile out the discriminant function, you will get one minus the variational distance, so it turned out there."
        ],
        [
            "There are two different losses mapped into the same after vergence kind of interesting, so it's not that we thought it might be 1 to one.",
            "This immediately proved it's not 1 to one relationship.",
            "What about if you start the exponential loss?",
            "The boosting loss?",
            "Well, it turned out.",
            "Then you got the hell injure distance, and that's a nice lecture size for you to do.",
            "It's kind of surprising that the exponential function goes into square root.",
            "If you start with the logistic loss, you got out something looking like the Cal.",
            "This is kale divergent, symmetrized.",
            "It's called the capacity discrimination and so on and so forth.",
            "So all the losses we could write down, they all turned into F divergences, and so we wondered, is there a general theory behind?"
        ],
        [
            "Yes, alright, it turns out there is, yeah.",
            "Let's talk about that.",
            "I'm bout to finish and I want to get that would take me a little bit of diversion.",
            "So let me talk to you about that a little bit later.",
            "So it turns out that there is a general relationship here that this class of surrogate loss functions Maps over in the class of after vergence is for every surrogate loss function, there is a corresponding F divergent, and for every after vergence there is a class of loss functions and these partition the space an exhaust.",
            "The space I've sort of glossed functions, so it's a complete characterization of the space of loss functions in terms of F divergences."
        ],
        [
            "OK, and so I'm going to give you a little flavor of how that's proved.",
            "It's a.",
            "It's a theorem in annals paper.",
            "I mentioned the keytool underline.",
            "It is my favorite tool of convex analysis called conjugate Duality.",
            "Unifies lots and lots of things.",
            "So just to remind you what conjugate duality is is if you take a lower semi continuous convex function F. The convex dual is defined as the Supreme of a linear functional minus the original function.",
            "That's necessarily a convex function, so the star means conjugate dual.",
            "So we're going to work with F star of negative beta for technical reasons, so this PSI of beta is the thing to remember.",
            "It's the conjugate dual function up to this flipping of the sign."
        ],
        [
            "OK, I think on the next slide I have the theorem.",
            "Yeah, so let me just take a minute to explain the theorem.",
            "So this is the theorem that shows that we have this.",
            "This relationship between divergent and losses so One Direction is pretty straightforward for any certain margin based surrogate loss function.",
            "There is an after version such that when you profile out.",
            "The loss function.",
            "The discriminant function, you get the negative and after versions for some convex function F. So that goes in One Direction.",
            "Moreover, in going in that direction, it turns out that for fee that is continuous that you get some nice properties of that conjugate dual function.",
            "These are sort of technical, it's decreasing and convex.",
            "It kind of it has a fixed point property and there's kind of a cascade property kind of fixed point like property here.",
            "So let's not worry about the technical details.",
            "It's just that turns out even if you have this, we condition on fee.",
            "That's continuous, which all the ones in practice are that you get out these kind of mathematical convexity properties of the conjugate dual.",
            "Now the other direction is really interesting, one, which is that if F is a lower scimitars convex function that satisfies these conditions, when you take its conjugate dual.",
            "OK, then there exists a loss function that induces that after version.",
            "So this goes in the backward direction and shows that after Vergence is also characterized loss functions.",
            "OK, the forward direction is actually trivial.",
            "To prove this one page and the backwards direction requires a lot of convex analysis, isn't it?"
        ],
        [
            "Definitely non trivial, so here's the easy direction.",
            "Even though the election I can take you through it, it's pretty easy.",
            "Here's the risk function.",
            "Remember, I wrote that down a little while ago.",
            "Now let's profile that.",
            "Let's optimize out the discriminant function, right?",
            "Well, when you when you optimize this out over gamma, we're going to do this for each Z, and so we can move it inside and replace gamma Z with just a number.",
            "So we get this expression inside.",
            "OK, and now let's just pull out Pi of Z and now we're left with this expression here.",
            "OK, just dividing and multiplying, dividing by \u03c0's and pulling it out.",
            "Alright, and now if we look at this object right here, this is a function of the likelihood ratio, and Moreover this is a convex function.",
            "Right?",
            "'cause it's here it is.",
            "I just wrote it down here.",
            "It's just there's a linear, a bunch of linear family of linear function.",
            "You take the infimum that gives you a concave function, you flip the sign, you get a convex function.",
            "So we've identified the F. It's just this function and this thing is now \u03c0 of F of the likelihood ratio.",
            "It is enough to vergence so all these little examples we were doing were all just examples of that.",
            "So that's really very very easy."
        ],
        [
            "The other direction is hard, but there is a constructive consequences of it, which is that when you go through that proof at some point, you identify the loss function.",
            "You can write it down, it has a certain form for Alpha which is equal to 0.",
            "It's just this fixed point.",
            "For Alpha bigger than zero, it is the conjugate dual function of a free function G. This is like a degree of freedom.",
            "You can choose any G you want that is increasing continuously convex.",
            "I'll give us some examples here in the next slide, so sigh of that function and then when Alpha is negative you just get the G function itself, so it turns out that's I had this this form as part of the proof, so this kind of gives a little structure of the possible loss functions.",
            "You can get an also gives you points out to where the freedom and since this Jeep."
        ],
        [
            "Auction.",
            "Now, so you can now do this for some examples.",
            "So if you start with the hell injure distance, remember that is the F divergent where F is equal to the square root function.",
            "Alright, if you now just take the conjugate dual of that, that's a little easy action.",
            "Just a piece of calculus.",
            "To do that.",
            "You'll get this function.",
            "That's the conjugate.",
            "Do love the square root function and take the minus of that, and now that's the PSI function.",
            "I've calculated it and now I can choose a bunch of GS and plug them into PSI and and I get out loss functions and so if I use G equal to E to EU minus one, that's a particular choice of G. Then I get out the red curve, which is the exponential loss.",
            "So we've now we've got in the opposite direction.",
            "We started her distance and we recovered the boosting loss.",
            "But also I could choose other.",
            "Geez if I chose G equal to U squared or is equal to U, those those are continuous convex.",
            "Then I get these other loss functions.",
            "The reading great curve which are equivalent to the boosting laws in that they map into the same divergent function.",
            "OK, yes so.",
            "Version and this gives you constructive proof to go backwards and find correct entire class.",
            "It defines the entire class in exactly the characterization.",
            "It just turns out it's not as strong as we need for statistical theory.",
            "This theory actually defines even a little broader class.",
            "You take a step, divergences, you brought it out a little bit, and you go backwards to get all losses that map into that and those things turn out to be universally equivalent.",
            "So you already headed in the right direction, but it turns out that for reasons I'll get into, it's a little broader than just one.",
            "After versions defines the kind of the whole statistical story."
        ],
        [
            "Here's the variational distance.",
            "If you liked the hinge loss you have started with the red curve there, the underlined F divergences is based on the variational, and as we saw earlier slide, you can write that as the using F is equal to the men.",
            "If you take the conjugate tool of that you get out this function here kind of got the hinge kind of look to it and if you plug in then different choices of G you get out these different curves including the hinge loss.",
            "And some other curves which are equivalent to the hinge loss in terms of giving you the same variational distance as the after."
        ],
        [
            "And callback levler you can kind of play the same story and so on.",
            "So I am."
        ],
        [
            "About ready to run out of time.",
            "Let me see.",
            "Let me just page through a couple of things so I can see where I am and then 'cause I'm going to stop here.",
            "OK, I think I'm just let's see base consistency and the universal equivalent story in the theorem.",
            "There's three theorems here.",
            "I'm not going to get to 3, which is my favorite one, but I'm going to.",
            "I'm going to do.",
            "Just let's see where was I?",
            "I think I'm just gonna set it up and I'm going to stop there so.",
            "Alright, so I've given you a theorem that relates after verses in losses and now I'm going to anticipate how what you are going to have this theorem so 01 loss is the goal of classification, so let's start with that and we now map into into diversion space and we get the variational distance with F chosen this way.",
            "OK, now that's plug it in right here.",
            "Let's now consider a broader class of F divergences defined by taking this kind of fine expansion of the original after versions of the men of you one.",
            "This kind of a bigger class of after vergence is.",
            "Alright, and now let's get all those F divergences.",
            "So if I could draw a little picture here and this will return next time.",
            "So we've started with with a particular loss.",
            "We care about the 01 loss here.",
            "We mapped over the space of F divergences there and we got the variational distance, and then we're going to broaden out a little bit and get a class of kind of all the affine combinations sort of thing of that one.",
            "And then we're going backwards and get a broader class of loss functions.",
            "All the ones that map into here.",
            "It's kind of composed of several subsets.",
            "There's a whole bunch of them over here.",
            "Alright, it will turn out that all of these over here have the same statistical properties in 01 loss, right?",
            "So you get consistency immediately for all of them.",
            "Alright, anything outside of that does not.",
            "It's there, not universal equivalence.",
            "We can prove lack of consistency and consistency, so you don't see that yet.",
            "'cause you don't know why I picked this particular fine thing, but you'll see that in theorem two or three.",
            "But anyway, let's just assume that that's a reasonable thing to do.",
            "We do that and immediately that's going to tell us about Bayes consistency and will also give us the converse, which that only these flosses yield based consistency.",
            "OK, so I've set up the story, I guess the slides will be available.",
            "You can look ahead if you want to read the annals paper.",
            "You have nothing else to do with your time.",
            "You can see these theorems and see how they proved right.",
            "Let me stop and see if there are questions.",
            "This is half of a story.",
            "Yeah.",
            "I OK so.",
            "So this talk is about classification.",
            "OK, so we're centered around 01 loss and we're developing a kind of a theory for that.",
            "There's a whole other sort of story to talk about, other loss, other so kind of the focus of the law.",
            "So if I was doing regression, I would develop a different parallel story here when I get to the end of this sequence of theorems, I will be able to see a little bit more about that.",
            "Alright, let's just focus on binary classification for now.",
            "Yes.",
            "Would it also make sense about in this space so early in the summer school one of the lecture was saying that there's more?",
            "More widely use of kernel methods in response to find distances between.",
            "Yeah no.",
            "So we worked on distance between distribution defined by kernels and in fact one of the applications of this will be to that at the end of this talk.",
            "So will talk about kernel space characterizations and essentially comes about by having a function space over which you optimize and that functions based kernel space.",
            "It's just one particular choice.",
            "Alright, so I'll just one way to get a kind of rich class to optimize over so it's not a fundamental part of the story, but it's just kind of convenient part of the story, yeah?",
            "University at all the estimators within that we couldn't even for the purpose of this particular efficiency or OK, let me tell you what in one sentence, what it is and and you'll see it next time.",
            "Universal equivalence means that.",
            "If I if two procedures are ranked the same by the F divergent, then there they have the same rank the same by the loss, so it's kind of the.",
            "The Blackwell theorem that I mentioned earlier but extended to the whole class of divergences and loss functions.",
            "But any given data for any given probability solutions universal in the sense of any probability distribution.",
            "That's right.",
            "So we'll be able to say if you're not universally equivalent to 01 loss, there must exist.",
            "Some problem will get it.",
            "You'll get a different answer than the 01 loss would give you.",
            "Right, so that's it's not a satisfactory loss function to be using 'cause you can get it wrong.",
            "Sorry, no, no, this is not an apology theory.",
            "Let me see if someone else has a question and I'll come back to you.",
            "OK, go ahead.",
            "I'm not familiar with F divergences, but in general, can you give us a sense what is so special about the mathematical properties of F divergent such that it works for F divergences but not other convexity?",
            "That F function is a convex functional likely ratio.",
            "That's the key.",
            "Yeah.",
            "Losses that.",
            "Is there a best, most great question?",
            "So no, this theory is silent on that.",
            "It just identifies the class and then return it to you.",
            "And I say now you choose among that class according to other principles, say complex in particular, or sparseness or some other principle that goes beyond decision theory.",
            "But I think that's a good thing to do is I'm going to narrow down my class and say all these are good and now you can bring another principle to bear on choosing a moment.",
            "So I don't want to tell you that you have to use that one.",
            "That's got the frequently spirit.",
            "It's rather to say that if you want to carve it down further, you better bring another principle.",
            "Bear computational complexity.",
            "Sparsity, some other principle.",
            "Yeah.",
            "Derivation, there was an unpractical assumption in the sense that you could optimize for each when you do your profiling to optimize over.",
            "Yeah.",
            "No, that's not an assumption, but you're right, it's nonparametric in the sense that it's over all possible gamma.",
            "That step wouldn't be true if it was restricted, its overall measurable functions gamma.",
            "Which is which is.",
            "That's right, so eventual theorem of consistency is going to have to be in a sieve of some kind where we're actually going up to a rich nonparametric class.",
            "But that's the spirit of this kind of whole story is to do this for things like support groups supposed to converge for all possible generating distributions.",
            "Alright, since the popcorn is stop popping now and we can, we're done."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm happy to be here, thanks.",
                    "label": 0
                },
                {
                    "sent": "Invited, I looked at the list of all the other speakers and I notice there was a lot of Bayesians on the list and so I thought I'd better give a frequentist counterpart if there had been a lot of frequent list, I would have given a Bayesian counterpart.",
                    "label": 0
                },
                {
                    "sent": "I'm just sort of a contrarian it hard.",
                    "label": 0
                },
                {
                    "sent": "So that's not entirely the goal of the talk, so the goal of talk is to sort of.",
                    "label": 0
                },
                {
                    "sent": "It's a summer school to teach a little bit about Bayesian frequentist distinction.",
                    "label": 0
                },
                {
                    "sent": "Emphasize a little more.",
                    "label": 0
                },
                {
                    "sent": "The frequentist 'cause I think you've probably had more Bayesian during the week.",
                    "label": 0
                },
                {
                    "sent": "And emphasizing the essential unity of these two classes of ideas that have been debated for about 400 years now.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to ask this question, but let me I am going to ask the question, so let me you've been through two weeks of mainly Bayesian lectures.",
                    "label": 0
                },
                {
                    "sent": "How many of you think that you're a Bayesian?",
                    "label": 0
                },
                {
                    "sent": "Would you call yourself a Bayesian?",
                    "label": 1
                },
                {
                    "sent": "Alright, I'd say about third of the room.",
                    "label": 0
                },
                {
                    "sent": "How many of you are frequentist?",
                    "label": 0
                },
                {
                    "sent": "Alright, then, about three of you see?",
                    "label": 0
                },
                {
                    "sent": "So that's that's kind of weird.",
                    "label": 0
                },
                {
                    "sent": "How many of you are both?",
                    "label": 0
                },
                {
                    "sent": "Another third of the room.",
                    "label": 0
                },
                {
                    "sent": "How many of you are neither?",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'll ask that same question maybe at the end of the lectures and see if they can see if things have shifted at all.",
                    "label": 0
                },
                {
                    "sent": "OK, so this this topic that most.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're interested in statistical inference has been around for quite a long time.",
                    "label": 0
                },
                {
                    "sent": "And there are two sort of main perspectives that have stood the test of time.",
                    "label": 0
                },
                {
                    "sent": "The Bayesian frequencies I'm going to give you a little argument in a couple of slides from a decision theoretic point of view of why these are really the only two real competitors.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Kind of essentially become lot because loss functions have two arguments.",
                    "label": 0
                },
                {
                    "sent": "There's kind of only two ways to go.",
                    "label": 0
                },
                {
                    "sent": "And it's really important to kind of keep throughout your whole career.",
                    "label": 0
                },
                {
                    "sent": "Balancing these two things back and forth.",
                    "label": 0
                },
                {
                    "sent": "Anybody who decided there one of the other early in their life and it will only only that, I think, is sort of missing the point.",
                    "label": 0
                },
                {
                    "sent": "These are kind of things to ponder and muse and understand the relationships and their deep.",
                    "label": 0
                },
                {
                    "sent": "So Bayesian perspective is a conditional perspective, so inferences should be made conditional on the current data.",
                    "label": 1
                },
                {
                    "sent": "So you just observe some data, hold that fixed and do everything conditional on that.",
                    "label": 0
                },
                {
                    "sent": "Don't worry bout other data you could have gotten.",
                    "label": 0
                },
                {
                    "sent": "That's the.",
                    "label": 0
                },
                {
                    "sent": "The conditional Bayesian perspective.",
                    "label": 0
                },
                {
                    "sent": "Now I find myself often being a Bayesian applied projects when I'm working with a domain expert and we have a lot of time to work with each other.",
                    "label": 0
                },
                {
                    "sent": "So picture like lots of biology projects, you'll have someone who knows a great deal and you want to try to understand what they're thinking about the problem.",
                    "label": 0
                },
                {
                    "sent": "What do they know, what's the what's the prior?",
                    "label": 0
                },
                {
                    "sent": "Also, what is the loss function so we often can't priors, but you really also need to consider the loss function as well, so how do they care about so you have time to elicit all those sorts of things?",
                    "label": 0
                },
                {
                    "sent": "It's often really very appropriate to be a Bayesian.",
                    "label": 0
                },
                {
                    "sent": "And one way to think about what a Bayesian perspective is, it's the optimist.",
                    "label": 0
                },
                {
                    "sent": "So I'm approaching a problem as a statistician.",
                    "label": 0
                },
                {
                    "sent": "I want to get I want to get knowledge out.",
                    "label": 0
                },
                {
                    "sent": "I'm going to inference out from data.",
                    "label": 0
                },
                {
                    "sent": "So let's we have the sophisticated tools.",
                    "label": 0
                },
                {
                    "sent": "Let's be optimistic and assuming get as much knowledge as possible by working hard to get a good prior and get a loss function and get a good model.",
                    "label": 0
                },
                {
                    "sent": "And iterating, so that's the optimist.",
                    "label": 1
                },
                {
                    "sent": "The frequentist perspective is an unconditional perspective.",
                    "label": 0
                },
                {
                    "sent": "You don't think about conditioning necessarily on the current date.",
                    "label": 1
                },
                {
                    "sent": "All you could, but the way you evaluate a frequentist procedure or evaluate procedure from this point of view is that you consider unconditional averages, so you should get good answers in repeated use.",
                    "label": 0
                },
                {
                    "sent": "So repeated use means you're going to look at multiple datasets and you're going to take averages unconditionally over the multiple datasets.",
                    "label": 0
                },
                {
                    "sent": "Don't condition on one single data set.",
                    "label": 0
                },
                {
                    "sent": "You look at multiple datasets you should get.",
                    "label": 0
                },
                {
                    "sent": "And you talk about unconditional performance over all those possible datasets.",
                    "label": 0
                },
                {
                    "sent": "That's what it means to be a frequentist.",
                    "label": 0
                },
                {
                    "sent": "Now, I also think this is a very natural perspective for lots of situations, and I often find myself in a frequentist in particular.",
                    "label": 1
                },
                {
                    "sent": "I don't have alot of time to sit down with a domain expert, we just have a very quick sort of project.",
                    "label": 0
                },
                {
                    "sent": "We develop a very simple tool that aims at some inference directly.",
                    "label": 0
                },
                {
                    "sent": "In some sense an hope that I can prove something about her or someone else has already proved something about that.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to wrong by using this, so in particular, if you're going to write a piece of software that's going to be used by many people with many datasets, you really ought to give a frequentist guarantee on that you ought to be able to say that piece of software is going to work well on multiple datasets now, inside the software.",
                    "label": 0
                },
                {
                    "sent": "It may be Bayesian, it may condition, but you want to be able to say whatever you condition on multiple different datasets.",
                    "label": 0
                },
                {
                    "sent": "You should do well.",
                    "label": 0
                },
                {
                    "sent": "OK, now someone sort of worked all that out in advance.",
                    "label": 0
                },
                {
                    "sent": "In some sense that there are some theorems that say that Bayesian inference is has good frequentist properties under various situations.",
                    "label": 0
                },
                {
                    "sent": "So in some sense you don't have to worry about it that much in kind of classical situations, parametric situations and so on.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric is a different story, but in general, if you're not going to necessarily start with additional Bayesian procedure you like some other procedure I'm going to beat the median or something didn't necessarily have a Bayesian justification by like it.",
                    "label": 0
                },
                {
                    "sent": "It's a procedure, then I that's my software.",
                    "label": 0
                },
                {
                    "sent": "I want to prove that it has good frequentist properties, meaning that.",
                    "label": 0
                },
                {
                    "sent": "Large fraction of the time it will give the answer you expect to get on all kinds of datasets.",
                    "label": 0
                },
                {
                    "sent": "OK, so I find that a hard argument too.",
                    "label": 0
                },
                {
                    "sent": "Defined difficulties with.",
                    "label": 0
                },
                {
                    "sent": "I mean it really, you know if you're writing software, you should be a frequentist.",
                    "label": 0
                },
                {
                    "sent": "Now the prequels perspective also is that of a pessimist.",
                    "label": 0
                },
                {
                    "sent": "So instead of being an optimist, what we think about it?",
                    "label": 0
                },
                {
                    "sent": "If you're a frequent, is that we're right on a model, we're going to develop a procedure.",
                    "label": 1
                },
                {
                    "sent": "An almost certainly it's going to be a simplification of reality.",
                    "label": 0
                },
                {
                    "sent": "Reality is really complicated.",
                    "label": 0
                },
                {
                    "sent": "We're going to simplify drastically often, and so we may get the wrong answer and let's protect ourselves not to get the wrong answer too often.",
                    "label": 0
                },
                {
                    "sent": "OK, so in medical domains alot of people tend to be frequentist because they want to protect themselves and get about doing something wrong something stupid.",
                    "label": 0
                },
                {
                    "sent": "So it's the pessimistic.",
                    "label": 0
                },
                {
                    "sent": "So Frequentism is definitely dominate statistics in the last 100 years.",
                    "label": 0
                },
                {
                    "sent": "Bayesian World is certainly present, and but smaller, and I think most sessions were trained, is frequent and still tend to approach the problem of frequency POV.",
                    "label": 0
                },
                {
                    "sent": "It's just this sort of pessimism just sort of ground in.",
                    "label": 0
                },
                {
                    "sent": "There's lots of reasons for that.",
                    "label": 0
                },
                {
                    "sent": "There are lots of situations where people have a bad inferences and you know the pessimistic prosperity is important to ingest.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now I'm often asked I go back and forth between the quote unquote machine world and learning world in the sticks world by statisticians.",
                    "label": 0
                },
                {
                    "sent": "What is this thing called?",
                    "label": 0
                },
                {
                    "sent": "Machine learning and I don't really believe it's a new field per say.",
                    "label": 0
                },
                {
                    "sent": "I believe it's a contribution to the general problem with fiscal inference and decision making.",
                    "label": 0
                },
                {
                    "sent": "It really is a set of themes.",
                    "label": 0
                },
                {
                    "sent": "It's not one kind of field, but it's an it's a loose Confederation of theme.",
                    "label": 1
                },
                {
                    "sent": "So reinforcement learning, clustering, classification, graphical models.",
                    "label": 0
                },
                {
                    "sent": "I mean, what do they have to do with each other?",
                    "label": 0
                },
                {
                    "sent": "You know not.",
                    "label": 0
                },
                {
                    "sent": "That much they just happened to be themes that people have found interesting and useful, and they connect to each other, usually by some sort of statistical argument of some kind.",
                    "label": 0
                },
                {
                    "sent": "So you know statisticians I talked about the believe though it is not, they don't actually pretend to think they've discovered a new field.",
                    "label": 0
                },
                {
                    "sent": "It really is just sort of physical inference.",
                    "label": 0
                },
                {
                    "sent": "OK, that's at least that's good.",
                    "label": 0
                },
                {
                    "sent": "But it's sort of different flavor in some ways, and one of them is there's a lot of focus on prediction.",
                    "label": 0
                },
                {
                    "sent": "If you just could make a good prediction here how you did it, that's often the machine learning spirit, and on kind of completely other side of things, not a prediction, but just do exploratory data analysis.",
                    "label": 0
                },
                {
                    "sent": "Find Cool features is kind of a typical machine learning thing to do.",
                    "label": 0
                },
                {
                    "sent": "That's exploratory data analysis.",
                    "label": 0
                },
                {
                    "sent": "You're trying to understand something about your data.",
                    "label": 0
                },
                {
                    "sent": "And so sessions, you know, neither had those are both things I understand, and that's reason why it's part of statistics too.",
                    "label": 0
                },
                {
                    "sent": "But a lot of statistics about what's called coverage.",
                    "label": 0
                },
                {
                    "sent": "I want you to tell me something in confidence.",
                    "label": 0
                },
                {
                    "sent": "In some prediction you have.",
                    "label": 0
                },
                {
                    "sent": "OK, and I want you to tell me you found some structure.",
                    "label": 0
                },
                {
                    "sent": "You've done some supporting.",
                    "label": 0
                },
                {
                    "sent": "You found some structure.",
                    "label": 0
                },
                {
                    "sent": "What's actually probably that's real, or what's the probability that that's garbage?",
                    "label": 0
                },
                {
                    "sent": "And so that kind of goes on the terminology of coverage.",
                    "label": 0
                },
                {
                    "sent": "I want you to frequentist concept, typically to guarantee that if you run your procedure over and again you will kind of do the right thing i.e.",
                    "label": 0
                },
                {
                    "sent": "Cover your error bars will cover the truth the right fraction of time or your things you have discovered will be noise.",
                    "label": 0
                },
                {
                    "sent": "The right fraction of time.",
                    "label": 0
                },
                {
                    "sent": "So that is kind of, you know, say well, OK they still have it kind of matured enough yet to really understand that this is an important issue to be worried about error bars in a frequent since the coverage national coverage.",
                    "label": 0
                },
                {
                    "sent": "The other thing that you will note from machine learning people is there's a lot of focus on methodology, so every develops a new model than a new method to fit their model.",
                    "label": 0
                },
                {
                    "sent": "That's the focus of things, not so much about kind of classical inferential topics.",
                    "label": 0
                },
                {
                    "sent": "And then the evaluation is usually not theoretical, but empirical, so that's kind of good, and statisticians, I think, appreciate that there is a kind of a dollop of empirical process.",
                    "label": 1
                },
                {
                    "sent": "Theory is often called statistical learning theory, but it's really empirical process theory, and it's when they turn to theory they turn this big hammer and they don't have the all the other little Hammers that are sitting around that are available.",
                    "label": 0
                },
                {
                    "sent": "Mainly asking products, so there's lots of parent nonparametric statistics and machine learning.",
                    "label": 0
                },
                {
                    "sent": "In fact, very few people do parametrics, but surprisingly to establish and there's hardly any assassin product.",
                    "label": 0
                },
                {
                    "sent": "That's the main tool to statisticians.",
                    "label": 0
                },
                {
                    "sent": "All these asymptotics and everyone is using these these big empirical process type Hammers, so that's a bit of a surprise, and the other somewhat surprising fact is that the field is somehow sometimes frequented, sometimes Bayesian, and it's not clear when when one is going to come up with the others come up and it kind of is a coexistence.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of people doing just purely Bayesian stuff.",
                    "label": 0
                },
                {
                    "sent": "A lot of people do infrequent and stuff.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I can call himself frequentist.",
                    "label": 0
                },
                {
                    "sent": "And there's no interplay between those two things, so there's kind of two parallel streams that go forward, and occasionally you'll say something well, I can give it a Bayesian interpretation of your procedure.",
                    "label": 0
                },
                {
                    "sent": "Here's a prior that matches it or something, but very little actual kind of real interplay, and the usual statistical sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so you guys are more machine learning people, so this slide may have meant that much to you, but I think it is important to understand how you understood by.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The outside world.",
                    "label": 0
                },
                {
                    "sent": "OK, so I promised a decision theoretic perspective.",
                    "label": 0
                },
                {
                    "sent": "Decision theory goes back to Walden to others in the 40s.",
                    "label": 0
                },
                {
                    "sent": "And although some of the number of papers that have decision theoretic content to them has dropped over the years, it was the thing to do in the 50s and 60s.",
                    "label": 0
                },
                {
                    "sent": "It's still definitely present and I think many people, including myself, view it as an extremely useful perspective to bring to bear and thinking about fundamentals, instead instill inference.",
                    "label": 0
                },
                {
                    "sent": "So you know decision theory perspective says you have some data X.",
                    "label": 0
                },
                {
                    "sent": "Let's consider a family of probability models indexed by a parameter Theta, and I put parameter in quotes because it doesn't mean a finite dimensional parameter, it just means an index of a class of probability distributions if that in that class is infinite, as it often is like a function space, then Theta indexes all the functions in your function space.",
                    "label": 1
                },
                {
                    "sent": "OK, uncountable number of things, so people often pick up a decision theory book and they say to themselves just parametric stuff isn't relevant to me and that's completely wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, completely wrong.",
                    "label": 0
                },
                {
                    "sent": "So Theta index is a function class or or measure space.",
                    "label": 0
                },
                {
                    "sent": "Anything that you want.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's our family probability models with index and now having gotten data, we're going to find a procedure of some kind, maybe decision tree, maybe a support vector machine, maybe a graphical model that will take that data and produce some sort of a decision and estimate function.",
                    "label": 1
                },
                {
                    "sent": "A decision of some sort.",
                    "label": 1
                },
                {
                    "sent": "Alright, and now given that decision an given the underlying probability generated index by Theta, you define a loss function.",
                    "label": 0
                },
                {
                    "sent": "OK, how much how bad you're going to feel if the truth was one thing and you made a decision of the form Delta X. OK, and I think almost any statistician would be happy at this point.",
                    "label": 0
                },
                {
                    "sent": "This is just this is all fine.",
                    "label": 0
                },
                {
                    "sent": "This is what you want to do.",
                    "label": 0
                },
                {
                    "sent": "You need to write down a loss function.",
                    "label": 0
                },
                {
                    "sent": "Kinda valuate how how good a procedure is doing, and now we're going to multiple procedures that could be D1, which is your decision tree in a D2 which is my support vector machine and so on.",
                    "label": 0
                },
                {
                    "sent": "And like to compare these say which is best either in this situation or in many situations hopefully.",
                    "label": 0
                },
                {
                    "sent": "So how can I do that?",
                    "label": 0
                },
                {
                    "sent": "Well, the loss function is now telling me it's my measure of how good I'm doing alright, But the problem is the loss function isn't just a number I want to get a number to compare two things.",
                    "label": 0
                },
                {
                    "sent": "You know, that procedure has loss 3.5 and that one has lost 4.2 that you know that ones better.",
                    "label": 0
                },
                {
                    "sent": "But it's not just a number 'cause there's two unknowns there, X is random.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of unknown this about X and Theta is unknown.",
                    "label": 0
                },
                {
                    "sent": "I don't know the probability solution underlying the data.",
                    "label": 0
                },
                {
                    "sent": "So I've got two pieces of things that are unknown.",
                    "label": 0
                },
                {
                    "sent": "How can I actually optimize over loss functions?",
                    "label": 0
                },
                {
                    "sent": "Choose the right Delta.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's kind of the core problem with the decision.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "James to face alright.",
                    "label": 0
                },
                {
                    "sent": "Well, you got two arguments of that function, so there's going to be 2 perspectives on how to get rid of that unknown knus you start with the Delta X or you start with the Theta.",
                    "label": 0
                },
                {
                    "sent": "And those two perspectives are called frequentist and Bayesian.",
                    "label": 0
                },
                {
                    "sent": "And that's my argument as to why there are two of them, because there's only two arguments to loss functions.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, so this time of the frequencies went first, so the frequentist looks at the loss functions as well.",
                    "label": 0
                },
                {
                    "sent": "You got two unknowns there.",
                    "label": 0
                },
                {
                    "sent": "Let's start with the X one.",
                    "label": 0
                },
                {
                    "sent": "Try to do something about turning that random X into a.",
                    "label": 0
                },
                {
                    "sent": "Into a number somehow.",
                    "label": 0
                },
                {
                    "sent": "Well, if it's random and we want a number, we need to take this expectation.",
                    "label": 0
                },
                {
                    "sent": "So let's take an expectation of this quantity.",
                    "label": 0
                },
                {
                    "sent": "Let's take an expression of the export.",
                    "label": 0
                },
                {
                    "sent": "OK, well in expectation with respect to what probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, let's use the same Theta to take our expectation as is in the loss function.",
                    "label": 1
                },
                {
                    "sent": "OK, so I don't need to know the truth operator, I just say whatever the truth is, I will take the expectation that I'll do this for all Theta, so I'll look at all possible truths.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to get a risk function.",
                    "label": 0
                },
                {
                    "sent": "The frequentist risk here is like function of Theta.",
                    "label": 0
                },
                {
                    "sent": "I've picked which data are priority is the truth.",
                    "label": 0
                },
                {
                    "sent": "OK, but this quantity here.",
                    "label": 0
                },
                {
                    "sent": "This E sub Theta is the expectation under the distribution indexed by Theta annexation over X.",
                    "label": 0
                },
                {
                    "sent": "The X goes away here and we just get on the left hand side and that's a key.",
                    "label": 0
                },
                {
                    "sent": "So if you need a definition of frequentist this is probably as good as any.",
                    "label": 0
                },
                {
                    "sent": "In fact this is kind of 1 message.",
                    "label": 0
                },
                {
                    "sent": "I want to give me a lot of people in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Are frequentist.",
                    "label": 0
                },
                {
                    "sent": "A lot of the work is at least frequentist but you never see the word frequentists and you never people talk about the definition of what it means to be frequentist.",
                    "label": 0
                },
                {
                    "sent": "Well here it is.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Here's one, it it means you take an expectation.",
                    "label": 0
                },
                {
                    "sent": "For six data I will start to X right?",
                    "label": 0
                },
                {
                    "sent": "In doing that, what you're being not gazing at that point you are taking expectation with respect to other X is over the entire sample space.",
                    "label": 0
                },
                {
                    "sent": "Not the ex that you saw, but other extras you might see.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what it means.",
                    "label": 0
                },
                {
                    "sent": "We frequently you're looking other possible data you could have gotten the unconditional perspective.",
                    "label": 0
                },
                {
                    "sent": "It's exactly so whenever you write down E in an equation and it's an easy averaging over X, you just veered away from the Bayesian.",
                    "label": 0
                },
                {
                    "sent": "You've gone towards the frequencies route.",
                    "label": 0
                },
                {
                    "sent": "Hope that's clear.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what the frequentist does get started and now they have this function R Theta and they say, well, I don't know what the truth is.",
                    "label": 0
                },
                {
                    "sent": "It could be any Theta and now I need still convert this thing to a single number.",
                    "label": 0
                },
                {
                    "sent": "How do I convert that to a single number?",
                    "label": 0
                },
                {
                    "sent": "OK, and now there's many possibilities and there's been huge literature.",
                    "label": 0
                },
                {
                    "sent": "50 years of kind of work on ways to think about turning this into a single number, so the one that you probably heard the most about is mini Max, or you'll take the.",
                    "label": 0
                },
                {
                    "sent": "The maximum of this frequentist risk overall Theta, and then find the procedure that has the minimum of the maximum risk and a lot of good work has come out of the mini Max perspective.",
                    "label": 0
                },
                {
                    "sent": "That's one way to take get rid of that apart by taking the maximum overall Theta, then taking the minimum over that.",
                    "label": 0
                },
                {
                    "sent": "So that's many Max.",
                    "label": 0
                },
                {
                    "sent": "But there are other ways to do this.",
                    "label": 0
                },
                {
                    "sent": "You could take subclasses of procedures so Delta X could be in a subclass, maybe all unbiased estimators or some other invariant invariant estimators.",
                    "label": 0
                },
                {
                    "sent": "So on in that subclass this are afeta might actually have a.",
                    "label": 0
                },
                {
                    "sent": "A simple characterization you can actually get out a single number over some subclass, and there are many other kind of ways of trying to approach that.",
                    "label": 0
                },
                {
                    "sent": "Another thing you could try to do here would be to average over Theta, right?",
                    "label": 0
                },
                {
                    "sent": "But when you average over Theta, you got average over some distribution on Theta, and at that point you become Bayesian.",
                    "label": 0
                },
                {
                    "sent": "OK, you're trying trying something about a distribution Theta, so the frequency.",
                    "label": 0
                },
                {
                    "sent": "Are you willing to do that in mathematics?",
                    "label": 0
                },
                {
                    "sent": "'cause you can get out some mathematical results, but in practice, or trying to avoid doing that, the Bayesian, on the other hand welcomes the opportunity innovate over Theta, 'cause they think it's fine to put a distribution on Theta.",
                    "label": 0
                },
                {
                    "sent": "And they're going to do that, and since they have one sitting around, they might as well average this quantity, not over suspect X.",
                    "label": 0
                },
                {
                    "sent": "Both Spectra Theta and so here this E over here is a different one.",
                    "label": 0
                },
                {
                    "sent": "This E is a conditional expectation given the data X additional perspective, an expectation over the data part of the loss function.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is still not a function of X, right?",
                    "label": 0
                },
                {
                    "sent": "But that's less troublesome now because the axis soon to be conditioned on it's known it's fixed, so it's just now a single number, and that's called the Bayesian risk.",
                    "label": 0
                },
                {
                    "sent": "So we have the frequentist risk and the Bayesian risk.",
                    "label": 0
                },
                {
                    "sent": "OK, Bayesian should be interested in this because having to find this now they can optimize over Delta.",
                    "label": 0
                },
                {
                    "sent": "And they can find the right procedure.",
                    "label": 0
                },
                {
                    "sent": "You know it should it should I report the.",
                    "label": 0
                },
                {
                    "sent": "The conditional mean the posterior mean or the conditional median or so and so forth.",
                    "label": 0
                },
                {
                    "sent": "Well, it depends on which loss function have here and by optimizing models you can.",
                    "label": 0
                },
                {
                    "sent": "You can pick that out.",
                    "label": 1
                },
                {
                    "sent": "So if you haven't picked out a loss function then you just report the whole posterior.",
                    "label": 0
                },
                {
                    "sent": "But if you have a loss then optimizing this equation will tell you which procedure to use.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you don't see enough of that kind of work in the machine learning literature of choosing the loss and talking about this particular risk function, and then this one you see a lot because this is what still learning people people work with and all other frequentists this frequentist risk OK, so the frequentist goes to the left the base and goes to the right and now you can sort of ask what happens if I keep going so I could take this quant.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here and I can act like a Bayesian average over Theta.",
                    "label": 0
                },
                {
                    "sent": "I could take this quantity here and I can act like a frequentist.",
                    "label": 0
                },
                {
                    "sent": "An average over X and neither camp would be very happy with you right?",
                    "label": 0
                },
                {
                    "sent": "But what would happen at that point when you get a single number?",
                    "label": 0
                },
                {
                    "sent": "Because now both things have been averaged over and will that number differ on this branch in this branch?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Just curious about the difference in notation for expectations on the right hand side.",
                    "label": 1
                },
                {
                    "sent": "Here in this condition on X. Yeah, this is just the conditional expectation of this random quantity which random start to its condition.",
                    "label": 0
                },
                {
                    "sent": "This is a constant, just a random quantity spectator, so this is the expectation with respect to Theta conditional next or over here this is the usual frequentist notation.",
                    "label": 0
                },
                {
                    "sent": "They don't want to treat data as a random variable, so you put it as an index.",
                    "label": 0
                },
                {
                    "sent": "So this is just the expectation of that particular property.",
                    "label": 0
                },
                {
                    "sent": "Distribution is not conditional.",
                    "label": 0
                },
                {
                    "sent": "Expectation is unconditional anyway.",
                    "label": 0
                },
                {
                    "sent": "You get the same number by doing the two calculations.",
                    "label": 0
                },
                {
                    "sent": "Yeah, what's the theorem that tells you get the same?",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                },
                {
                    "sent": "It's called Fubini's theorem.",
                    "label": 0
                },
                {
                    "sent": "OK, just iterated expectations.",
                    "label": 0
                },
                {
                    "sent": "We can switch that to expectations.",
                    "label": 0
                },
                {
                    "sent": "OK, and what number do you get when you do this to expectations?",
                    "label": 0
                },
                {
                    "sent": "It's called the Bayes risk, so you may have heard the Bayes risk out there and you might think you have to be Bayesian to use the Bayes risk, and that's wrong.",
                    "label": 1
                },
                {
                    "sent": "That's not right.",
                    "label": 0
                },
                {
                    "sent": "OK, the Bayes risk is gotten by either path of frequentist or Bayesian path.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a little bit of decision theory.",
                    "label": 0
                },
                {
                    "sent": "Hope that was interesting.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about.",
                    "label": 0
                },
                {
                    "sent": "The issue.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of coherence in calibration to me.",
                    "label": 0
                },
                {
                    "sent": "This is very helpful way to understand some of the starts in some other relationships between Bayesian and frequentist ideas.",
                    "label": 0
                },
                {
                    "sent": "Let me just actually, before I do that, I said the word relationship.",
                    "label": 0
                },
                {
                    "sent": "So if you go back to the previous slide, decision theory has been really the home of a lot of relationships between Bayesian and frequentist ideas.",
                    "label": 0
                },
                {
                    "sent": "So in particular if you try to find optimal frequentist procedures and defined in various ways, there's something called complete class theorems that tell you that they are either Bayesian procedures or limits of Bayesian procedures.",
                    "label": 0
                },
                {
                    "sent": "So from a frequentist POV, you often want to use Bayesian procedures 'cause you know.",
                    "label": 0
                },
                {
                    "sent": "That they give you the class of optimal procedures.",
                    "label": 0
                },
                {
                    "sent": "And of course the frequency doesn't necessarily use that theorem practice because you don't know with respect to prior and you're completely unknown, but prior.",
                    "label": 0
                },
                {
                    "sent": "But you know mathematically that's a fact, so that's one class of connections between Bayesian or frequentist.",
                    "label": 0
                },
                {
                    "sent": "OK, so coherence and calibration is these two words are used a lot to describe kind of bias perspectives on inference.",
                    "label": 0
                },
                {
                    "sent": "David Draper is written a lot about these particular words, and a lot of other people as well.",
                    "label": 0
                },
                {
                    "sent": "So there are two important goals for statistical inference.",
                    "label": 1
                },
                {
                    "sent": "Coherence means coherence that you give out the same answer kind of number question.",
                    "label": 0
                },
                {
                    "sent": "I got the same answer no matter what question you ask me and you can't find any incoherence among my multiple answers.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "And calibration is.",
                    "label": 0
                },
                {
                    "sent": "Means something like if I give you a number out, then that number means something that if you ask me to do a procedure multiple times an I claim that 95% of the time give the right answer within 95% of the time you better give me out the right answer, that's calibration.",
                    "label": 0
                },
                {
                    "sent": "OK, so Bayesian work has focused on coherence, while frequently work hasn't been too worried about cars.",
                    "label": 1
                },
                {
                    "sent": "I think it's pretty fair statement, so Bayesians get kind of coherence for free 'cause they have a joint probability underline everything, and that's the source of the coherence, and they love to bash frequentist because they find places where Frequentists work is not coherent.",
                    "label": 0
                },
                {
                    "sent": "An lots and lots of papers written about that and the frequencies are not so worried about that they're interested in a particular inference problem at one particular time, and doing the best, finding the loss function that targets that lost that problem, and you know you can't be coherent all the time, you know.",
                    "label": 0
                },
                {
                    "sent": "That's just life somehow is kind of.",
                    "label": 0
                },
                {
                    "sent": "In my truck one soldiers you know I I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry not go here.",
                    "label": 0
                },
                {
                    "sent": "You know well coherent is maybe the wrong word.",
                    "label": 0
                },
                {
                    "sent": "I'm not going here in the mornings.",
                    "label": 0
                },
                {
                    "sent": "But consistent is perhaps another word, but that's another technical meeting, so I'm not consistent here.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you one thing in one day, and then I'll tell you something else six months later.",
                    "label": 0
                },
                {
                    "sent": "And that's just life.",
                    "label": 0
                },
                {
                    "sent": "Now the other hand, frequentist workers tended to focus on calibration, so calibration again kind of, is like you know, social coverage that.",
                    "label": 1
                },
                {
                    "sent": "The kind of the numeric values you associate with your procedure really do come out in practice, and Bayesians haven't been too worried about calibration.",
                    "label": 0
                },
                {
                    "sent": "OK, now that's kind of a bit of a problem with the Bayesian perspective.",
                    "label": 0
                },
                {
                    "sent": "You start writing out a bunch of priors.",
                    "label": 0
                },
                {
                    "sent": "You write down procedure running your data, and that's it, you're done.",
                    "label": 0
                },
                {
                    "sent": "Alright, what guarantees that you give me?",
                    "label": 0
                },
                {
                    "sent": "And could you tell me you know that you did that procedure multiple times?",
                    "label": 0
                },
                {
                    "sent": "It would come out to have the guarantee your guarantees turned out to be true.",
                    "label": 0
                },
                {
                    "sent": "Alright, well Bayesians don't tend to worry about that.",
                    "label": 0
                },
                {
                    "sent": "You know that much enough now.",
                    "label": 0
                },
                {
                    "sent": "Good bayesians.",
                    "label": 0
                },
                {
                    "sent": "Most Bayesian statistics are actually a little bit frequentist too, and so they will often look at a little bit of a frequency analysis of what they're doing and compare the coverage, for example, of their Bayesian procedure.",
                    "label": 0
                },
                {
                    "sent": "Anyway, if you don't do that, if you just if you're a pure Bayesian, then you certainly get coherence, but you know you don't necessarily.",
                    "label": 0
                },
                {
                    "sent": "Calibration, on the other hand, if you're appear frequentist, then you just worried about your calibration.",
                    "label": 0
                },
                {
                    "sent": "You can be calibrated and completely useless, so 95% of the time you give out error bars that are appointed zero 5% of the time you got error bars that cover everything and so that the average works out to be some kind of confidence interval.",
                    "label": 0
                },
                {
                    "sent": "But on any given set of data you give out.",
                    "label": 1
                },
                {
                    "sent": "A useless answer.",
                    "label": 0
                },
                {
                    "sent": "And so you can be completely re calibrating their hand if you're completely coherent, you can be completely coherent and completely wrong.",
                    "label": 0
                },
                {
                    "sent": "It's clear, coherent to give out the answer one to whatever question problem you ask me.",
                    "label": 0
                },
                {
                    "sent": "OK, but OK. And so most statisticians find some kind of a blend.",
                    "label": 1
                },
                {
                    "sent": "A natural way to proceed because they tend to achieve both coherence and calibration.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, given you the answer my question of my title, I think many statisticians, not every single one, but many of them find that they are both a little bit busy, little bit frequentist and these things can be made into conflict.",
                    "label": 0
                },
                {
                    "sent": "There are ways of focusing on calibration coherence and showing that one perspective doesn't achieve it, but they really do act complementary an 8 each other.",
                    "label": 0
                },
                {
                    "sent": "And it's a little bit like wave particle duality is one way to think about it.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "Waves and particles are both there.",
                    "label": 0
                },
                {
                    "sent": "They're both could always be true.",
                    "label": 0
                },
                {
                    "sent": "There's something right about both of them, but they don't quite really workout together as well as they should.",
                    "label": 0
                },
                {
                    "sent": "I think it's true about Bayesian frequentist too.",
                    "label": 0
                },
                {
                    "sent": "They're both right in some way, though.",
                    "label": 0
                },
                {
                    "sent": "Both around forever, and one's not going to vanish.",
                    "label": 0
                },
                {
                    "sent": "But they don't quite, you know, merge entirely.",
                    "label": 0
                },
                {
                    "sent": "They do fight each other in various ways, in particular in testing and model selection problems, and I think they'll probably be eventually be some more of a resolution there even, but it's going to take awhile.",
                    "label": 0
                },
                {
                    "sent": "Alright, so a few more comments about the kind of sociology really, so the frequentist world is this hodgepodge of people.",
                    "label": 0
                },
                {
                    "sent": "You can do any kind of technique as long as you give me an analysis, that's frequentism, so it's just a big big field Bayesian a little smaller and it's really got 2 main subdivisions so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have subjective Bayesian.",
                    "label": 0
                },
                {
                    "sent": "You have objective basis is simplifying, but these are the kind of two main schools of Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Subjective Bayesian is believes that the prior comes from a person or maybe a small group of people.",
                    "label": 0
                },
                {
                    "sent": "So the goal is to work with that person, a domain expert and you want to figure out what's the prior that person has in their head an what loss function do they have in their head?",
                    "label": 0
                },
                {
                    "sent": "Also, an in the Model 2 is somehow I had to come from the domain expert, so you've got to also figure out what the model is illicit to model.",
                    "label": 1
                },
                {
                    "sent": "OK, and the subjective Bayesian argument is that if you got out bad answers from your Bayesian procedure, it's just you didn't work hard enough to get the prior in the loss function in the model.",
                    "label": 1
                },
                {
                    "sent": "Should work harder.",
                    "label": 0
                },
                {
                    "sent": "Alright, and put that way, it's sort of hard to argue with.",
                    "label": 0
                },
                {
                    "sent": "If I spent a million years and got the right prior, you know something I would get out the right inference.",
                    "label": 1
                },
                {
                    "sent": "So what kind of if you just once you have the prior loss and the model you're done, they use Bayes rule and so on.",
                    "label": 0
                },
                {
                    "sent": "There's not much else to talk about, So what kind of research did you is a subjective Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Well you do a lot of things you saw here this last two weeks.",
                    "label": 0
                },
                {
                    "sent": "I think a lot of the work you probably saw was effectively subjective Bayesian, even though I'm not sure those words were not used.",
                    "label": 1
                },
                {
                    "sent": "What does it mean when you have opened up new kinds of models, right?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Well, because I'm a subjective Bayesian, I'll go on face some new problem I'd like have a library of models I might bring to bear on the problem, so some of some of people worked on models.",
                    "label": 0
                },
                {
                    "sent": "I'll have a big library, so that's one thing you could do.",
                    "label": 0
                },
                {
                    "sent": "The other thing you can do is that Bayesian obeys rule.",
                    "label": 0
                },
                {
                    "sent": "You have to integrate.",
                    "label": 0
                },
                {
                    "sent": "You gotta get that denominator.",
                    "label": 1
                },
                {
                    "sent": "So better developed lots of procedures for integration, 'cause that's gonna be hard to do, so a lot of algorithms work goes into subjective Bayesian research on integration.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then I think you probably don't talk as much about, but if you're going to be really subjective Bayesian, you really ought to worry about how to get those priors.",
                    "label": 0
                },
                {
                    "sent": "It's not that easy and you better workout techniques for eliciting and assessing priors from individuals.",
                    "label": 0
                },
                {
                    "sent": "There's a whole literature on that, and a lot of Bayesian neural network machine learning people don't focus on that nearly enough, so we really are going to be amazing.",
                    "label": 0
                },
                {
                    "sent": "You better worry about how to do that.",
                    "label": 0
                },
                {
                    "sent": "Anyway, those are the kind of some of the main areas of research.",
                    "label": 0
                },
                {
                    "sent": "There are others, but those are the main, so there's not a lot of focus on analysis of did my procedure work and so on.",
                    "label": 0
                },
                {
                    "sent": "That's really what frequentist do, because you know if you have the right inputs, the Bayesian outputs would be a good one.",
                    "label": 0
                },
                {
                    "sent": "Alright, so again, you can't really argue about that from philosophical point of view, it's coherent, it's pretty, it's it's nice, but in practice there are really lots and lots of problems and the main one is that lots of work with really complicated models are hierarchies.",
                    "label": 1
                },
                {
                    "sent": "There is multivariate quantities, there's matrices, so and so forth, and all of those bring new parameters into the problem.",
                    "label": 0
                },
                {
                    "sent": "You know whenever you wish our distribution, you got a whole matrix of parameters set in there.",
                    "label": 0
                },
                {
                    "sent": "You gotta put a distribution on that.",
                    "label": 0
                },
                {
                    "sent": "Alright, well, OK, that's hard.",
                    "label": 0
                },
                {
                    "sent": "And so the more complicated model gets, the more parameters than it's not going to take a long, long time to get a domain expert to kind of say, well, my prior on that wish heart thing this is this.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, if you've got long list of parameters, it's really the joint distribution.",
                    "label": 0
                },
                {
                    "sent": "All the parameters you better be assessing.",
                    "label": 0
                },
                {
                    "sent": "That's why you're supposed to get right.",
                    "label": 0
                },
                {
                    "sent": "That's kind of becomes really hopeless.",
                    "label": 0
                },
                {
                    "sent": "How do you do that?",
                    "label": 1
                },
                {
                    "sent": "Well, they start making independence assumptions.",
                    "label": 0
                },
                {
                    "sent": "You start throwing them in, because if I say, well, that's independent.",
                    "label": 0
                },
                {
                    "sent": "Now I can think separately about this and think about it in the human domain, expert can get and start thinking about it.",
                    "label": 1
                },
                {
                    "sent": "Now you're leaving, you know, based on the floor a little bit 'cause you're not really assessing the right prior.",
                    "label": 0
                },
                {
                    "sent": "Simply for computational reasons, you often start writing a list of independence assumptions.",
                    "label": 0
                },
                {
                    "sent": "At that point, you may have left kind of optimality behind.",
                    "label": 0
                },
                {
                    "sent": "And now a subtle question issue.",
                    "label": 0
                },
                {
                    "sent": "But just as important as the others is that it's really hard to get domain experts to assess the tail behavior whenever you're working with real valued quantities, which of course most of our models are as you go higher up in the hierarchy, they start to become real numbers.",
                    "label": 0
                },
                {
                    "sent": "You want the probability of some discrete thing with the probability is a real number.",
                    "label": 0
                },
                {
                    "sent": "You gotta put her on that so it has tail behave you have to worry about what tail behavior.",
                    "label": 0
                },
                {
                    "sent": "You know I can.",
                    "label": 0
                },
                {
                    "sent": "I can get my mother and talk about the mean and the standard deviation of something, but she can't tell me about whether it's Laplace, tails or or T tails or whatever, right?",
                    "label": 0
                },
                {
                    "sent": "And I don't think many of you could either.",
                    "label": 0
                },
                {
                    "sent": "I don't think I could.",
                    "label": 0
                },
                {
                    "sent": "It's really hard to assess those things, and it's also hard to kind of learn them.",
                    "label": 0
                },
                {
                    "sent": "Quote unquote, hard to get out of here.",
                    "label": 0
                },
                {
                    "sent": "Does that matter?",
                    "label": 0
                },
                {
                    "sent": "Well, some Bayesian models.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter that much, but in lots of these emails that really does matter a lot.",
                    "label": 0
                },
                {
                    "sent": "In fact, in some cases that determines the entire output of your procedure.",
                    "label": 0
                },
                {
                    "sent": "And this is a really serious issue, so you will often hear people talking about Bayes factors in marginal likelihoods.",
                    "label": 0
                },
                {
                    "sent": "How do you solve model selection problems?",
                    "label": 0
                },
                {
                    "sent": "Bayesian marginal likelihood is the knee jerk answer calculate that.",
                    "label": 0
                },
                {
                    "sent": "Well, the marginal likelihood is the integral of the likelihood under the prior.",
                    "label": 0
                },
                {
                    "sent": "And so the integral under posterior, which tends to sharpen up and tail behavior doesn't matter.",
                    "label": 1
                },
                {
                    "sent": "It's under the prior to the tails, are there.",
                    "label": 0
                },
                {
                    "sent": "And your integral is if you have very fat tails, is going to be turned by your tails.",
                    "label": 0
                },
                {
                    "sent": "That's in the prior that your assumption.",
                    "label": 0
                },
                {
                    "sent": "OK. And so the marginal likelihood can be, you know, hugely determine you know affected by the particular assumptions.",
                    "label": 0
                },
                {
                    "sent": "Bayes factors similarly based factors are ratios of marginal likelihoods, and so one way you might try to go say, well, use improper priors.",
                    "label": 1
                },
                {
                    "sent": "You know, try to make them flat, so I'm not putting very many assumptions under in right as I think you may know marginal likelihoods.",
                    "label": 0
                },
                {
                    "sent": "If you're in the prior and you have an improper prior, it has an arbitrary constant.",
                    "label": 1
                },
                {
                    "sent": "And those things will tend to divide out when you calculate things like posteriors, but in marginal likelihoods and base factors they don't.",
                    "label": 0
                },
                {
                    "sent": "You have a ratio of arbitrary constants, and so the base factor is meaningless in that case.",
                    "label": 0
                },
                {
                    "sent": "Alright, so these are really serious issues and there's a lot of physical literature on this.",
                    "label": 0
                },
                {
                    "sent": "There's things like intrinsic Bayes factors, an fractional Bayes factors, and various kinds of ways to approach this, but if you don't try to at least think about those things you know it's not.",
                    "label": 0
                },
                {
                    "sent": "It is not the solution, it's not the hammer that solves the model selection problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so tail behaviors big issue nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So a lot of us.",
                    "label": 0
                },
                {
                    "sent": "In fact when I wear a Bayesian hat as a researcher, I'm very interested in Nonparametrics and have been for about a decade.",
                    "label": 0
                },
                {
                    "sent": "Think it's great, it's awkward for subjective Bayes because it's really complicated and nonparametric Bayes model is hard to think about.",
                    "label": 1
                },
                {
                    "sent": "It sees stick breaking things and infinite objects and so on.",
                    "label": 0
                },
                {
                    "sent": "What's my subjective prior on those things?",
                    "label": 0
                },
                {
                    "sent": "And so a lot of subjective Bayesians in fact are not very happy with the nonparametric based movement.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you know that may eventually get worked out, but it is currently an issue.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's kind of some of the problems that arise and I belabor them.",
                    "label": 0
                },
                {
                    "sent": "Perhaps a little bit, because I really think there is a time, a tendency to sort of, say, the basic research is so easy and systematic.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "How could anyone do anything else?",
                    "label": 0
                },
                {
                    "sent": "Well, these are real issues that come up in real life.",
                    "label": 0
                },
                {
                    "sent": "And then last one is more philosophical, which is just that a lot of frequentists don't like sort of subjective bayesians.",
                    "label": 0
                },
                {
                    "sent": "Sort of telling that can't use a certain method, so I like the support vector machine 'cause it works.",
                    "label": 0
                },
                {
                    "sent": "Right, I go in lots of applied situations.",
                    "label": 0
                },
                {
                    "sent": "I will roll it out and it will work really well in someone's path and everyone is happy.",
                    "label": 0
                },
                {
                    "sent": "I get paid and the company makes money.",
                    "label": 0
                },
                {
                    "sent": "What's wrong with that?",
                    "label": 0
                },
                {
                    "sent": "Alright, well it doesn't have a Bayesian interpretation, at least the obvious one.",
                    "label": 0
                },
                {
                    "sent": "You might really build a work really hard to find one, but it's pretty.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have one.",
                    "label": 0
                },
                {
                    "sent": "It didn't seem right.",
                    "label": 0
                },
                {
                    "sent": "Well, do I have to wait around for someone to show me?",
                    "label": 0
                },
                {
                    "sent": "It's amazing to use it?",
                    "label": 0
                },
                {
                    "sent": "No, I could just use it because I can write it down.",
                    "label": 0
                },
                {
                    "sent": "It works and then I could actually do some theory that shows that it has a frequentist justification, right?",
                    "label": 0
                },
                {
                    "sent": "And there's lots of some simple simple kind of nonparametric testing situations where I just got a column of numbers here in a column.",
                    "label": 0
                },
                {
                    "sent": "Here I want to say those two columns are different.",
                    "label": 0
                },
                {
                    "sent": "Right, well there's these simple things that you just sort them and you you find its column one were higher in the list in column two.",
                    "label": 0
                },
                {
                    "sent": "Develop a statistic that measures that and then prove that that will work on repeated usages.",
                    "label": 0
                },
                {
                    "sent": "It's a perfectly good kind of approach to testing.",
                    "label": 0
                },
                {
                    "sent": "I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm not supposed to use that 'cause that's not Bayesian, it just sort of doesn't feel right.",
                    "label": 0
                },
                {
                    "sent": "OK, so I hope by bash subjective Bayes enough that you'll be interested in some other things, yeah?",
                    "label": 0
                },
                {
                    "sent": "Behavior in factors.",
                    "label": 0
                },
                {
                    "sent": "So you're saying that, oh, this is only the exterior that we have to integrate over?",
                    "label": 0
                },
                {
                    "sent": "Then it would be better because that is sharp enough.",
                    "label": 0
                },
                {
                    "sent": "And I mean, yeah, it matters if you're integrating the light and then they're likely does shut up, so it doesn't matter whether it's you know it's the integral of the product of two things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, no, it does sharpen up Sir.",
                    "label": 0
                },
                {
                    "sent": "No, it does sharpen up, but it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's the rate at which things sharpen up and you've got to pick your tail behavior of a certain rate to compete with that.",
                    "label": 0
                },
                {
                    "sent": "And you've got to do that effectively.",
                    "label": 0
                },
                {
                    "sent": "Are two models in the numerator and denominator, and it's sort of getting all those roles right rates to line up is, which is hard.",
                    "label": 0
                },
                {
                    "sent": "Yes, or maybe a little likelihood sharpen up, you're right, but it's important.",
                    "label": 0
                },
                {
                    "sent": "A lot of people sort of think well problems go away when you integrate into posterior, which is true.",
                    "label": 0
                },
                {
                    "sent": "But the whole point of margarine is prior, so you've got the tail behavior still has to be taken into account.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so objective Bayes you know.",
                    "label": 0
                },
                {
                    "sent": "I really like objective Bayes.",
                    "label": 1
                },
                {
                    "sent": "There's a whole conference on this that I went to the last couple of years and you know, I think it's like oh oh B oh now it will type that in and you'll see the objective based conference.",
                    "label": 0
                },
                {
                    "sent": "So this is a great perspective.",
                    "label": 0
                },
                {
                    "sent": "It really is a bridge between frequencies and Bayesian ideas.",
                    "label": 0
                },
                {
                    "sent": "It's trying to find ways to set priors that aren't subjective.",
                    "label": 0
                },
                {
                    "sent": "Maybe no human would have come up with them, but some sense there are sensible.",
                    "label": 0
                },
                {
                    "sent": "They would give you protect you from making bad inferences and Moreover and really complicated models.",
                    "label": 0
                },
                {
                    "sent": "It would maybe give you a way to set priors automatically that you don't have to have a human go looking at every the long list of parameters you have.",
                    "label": 0
                },
                {
                    "sent": "Alright, so it's been a lot of work on this.",
                    "label": 0
                },
                {
                    "sent": "Probably the best existing class of techniques are called reference priors, and the whole talk on its own.",
                    "label": 0
                },
                {
                    "sent": "But what they do is they set up a variational problem where they maximize the notion of divergent between the prior and the posterior with respect to the prior.",
                    "label": 1
                },
                {
                    "sent": "OK, so the distance between the prime the poster in some sense is the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So if you maximize that distance, you're making the likelihood do most of the work in the property as little work as possible.",
                    "label": 0
                },
                {
                    "sent": "So that's a well posed variational problem and you can solve it in many situations and you get out of prior to this.",
                    "label": 0
                },
                {
                    "sent": "Prayer has been gone from a domain expert.",
                    "label": 0
                },
                {
                    "sent": "It's not normal piece of mathematics that tries to protect you about having an over an influential prior and many situation you get out improper priors by doing this procedure.",
                    "label": 0
                },
                {
                    "sent": "Jeffreys priors and so on, but not all situations you get out proper priors in some situations as well.",
                    "label": 0
                },
                {
                    "sent": "Anyway, this is an ongoing research project.",
                    "label": 0
                },
                {
                    "sent": "It's very interesting.",
                    "label": 0
                },
                {
                    "sent": "I would hope they would be lectures about reference priors here, but.",
                    "label": 0
                },
                {
                    "sent": "Probably work.",
                    "label": 0
                },
                {
                    "sent": "And so objective Bayesians, you know how did you pick the reference price is a good idea?",
                    "label": 0
                },
                {
                    "sent": "Well, sounds like a good idea, but how did you actually kind of show it was a good idea?",
                    "label": 0
                },
                {
                    "sent": "And Moreover you know there's many other kind of approaches to priors, which how do you choose between procedures or principles for choosing priors?",
                    "label": 0
                },
                {
                    "sent": "Being subjective, so you use frequentist ideas, you will often sort of show that your Bayesian procedure choosing priors in some way has good frequentist properties and people kind of agree that's not a bad way to kind of get a guidance, so consistency properties are sometimes used.",
                    "label": 1
                },
                {
                    "sent": "Admissibility properties are widely used, that's another frequentist idea that are to try to get good principles for choosing priors.",
                    "label": 0
                },
                {
                    "sent": "OK, so I like this framework.",
                    "label": 0
                },
                {
                    "sent": "It's a great area to work in, but the kind of downside is it can be challenging work within complex models.",
                    "label": 1
                },
                {
                    "sent": "OK, so you have to kind of do the mathematics to get out your prior and often.",
                    "label": 0
                },
                {
                    "sent": "That's really hard to do, and so that's an ongoing research project to kind of do that, and I'd say for a simple models this is often worked out and this is an off the shelf solution, but for a lot of the models that many of you will be interested in, it's not off the shelf, so you have to do a lot of work to use.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Objective based ideas alright, but I just want you to be aware that this is a counterpart to subjective Bayes.",
                    "label": 0
                },
                {
                    "sent": "OK so Lastly.",
                    "label": 0
                },
                {
                    "sent": "The frequentist perspective so.",
                    "label": 0
                },
                {
                    "sent": "So the frequency project is very is very Catholic.",
                    "label": 0
                },
                {
                    "sent": "Procedures can come from anywhere.",
                    "label": 1
                },
                {
                    "sent": "They don't have to be derived from a probability model of conditional and or they have to be derived from a probability model.",
                    "label": 1
                },
                {
                    "sent": "So nonparametric testing just kind of sensible sets of test statistics and show that they worked support vector machine boosting or kind of things that weren't drive from a probability model.",
                    "label": 0
                },
                {
                    "sent": "Also I'd like to mention things like methods based on 1st order logic, right?",
                    "label": 0
                },
                {
                    "sent": "So you can have a data set data comes in, you have a big first order logic machine and outcomes some answer right and?",
                    "label": 0
                },
                {
                    "sent": "That's good, nothing wrong with that.",
                    "label": 0
                },
                {
                    "sent": "And as a frequentist I would want to sit down and say, well, is that a good procedure?",
                    "label": 0
                },
                {
                    "sent": "Is that just 'cause it's logic doesn't mean it's necessarily, but it might be good, but I can analyze it from a physical point of view, and that's what frequentism does.",
                    "label": 0
                },
                {
                    "sent": "They would say OK. Is that procedure on repeated datasets give me an answer which is good in some notion of loss?",
                    "label": 0
                },
                {
                    "sent": "Right, and so I often get in argument people in kind of the more AI side of machine learning, saying, well, there's statistical machine learning, and there's the arrest of machine learning.",
                    "label": 0
                },
                {
                    "sent": "And I would say what is that was that other object was that the other partition logical sort of stuff and I said, well, you know it's it's.",
                    "label": 0
                },
                {
                    "sent": "These are completely agreeable perspectives.",
                    "label": 0
                },
                {
                    "sent": "You can take your logical thing.",
                    "label": 0
                },
                {
                    "sent": "I can evaluate it statistically and they said, well, OK, that's fine, but no Bayesian that's different from logical and I just sort of start find these distinctions a little bit unhelpful at times.",
                    "label": 0
                },
                {
                    "sent": "So this frequency perspective particular is just an analysis tool and it can analyze all kinds of things.",
                    "label": 0
                },
                {
                    "sent": "So I think a machine learning and statistical is the inferential problem of taking in data and getting out knowledge and.",
                    "label": 0
                },
                {
                    "sent": "And frequentist perspective is very much part of that.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you can get your methods from anywhere, you know I can write down, you know, Mike Jordan Cilius method ever.",
                    "label": 1
                },
                {
                    "sent": "And you want to be able to rule that out.",
                    "label": 1
                },
                {
                    "sent": "So what frequencies mostly do is they develop techniques of analysis.",
                    "label": 0
                },
                {
                    "sent": "That is your rule, out stupid methods into rank the reasonable methods.",
                    "label": 1
                },
                {
                    "sent": "So it tends to focus more on analysis and methods.",
                    "label": 0
                },
                {
                    "sent": "But I did want to mention one and passing one general frequentist method.",
                    "label": 0
                },
                {
                    "sent": "It's the bootstrap.",
                    "label": 0
                },
                {
                    "sent": "It's kind of as automatic as Bayesian procedures can be used on all kinds of problems.",
                    "label": 0
                },
                {
                    "sent": "It's just a general methodology that's very frequently so.",
                    "label": 0
                },
                {
                    "sent": "The bootstrap is that you take your original data set and you re sample it multiple times, and in doing so you're looking at alternative datasets.",
                    "label": 0
                },
                {
                    "sent": "You're exactly being a frequentist from mythological POV now.",
                    "label": 0
                },
                {
                    "sent": "Not so much an analysis point of view.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Then there's analysis to show that procedure has good frequentist properties itself, but it's very interesting broad class of techniques.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Oh, I think this is one more slide on kind of introduction and I'm going to move on to some more concrete stuff.",
                    "label": 0
                },
                {
                    "sent": "So what do you do is a frequentist?",
                    "label": 0
                },
                {
                    "sent": "What kind of activities do you do?",
                    "label": 0
                },
                {
                    "sent": "Well, you also write down models.",
                    "label": 0
                },
                {
                    "sent": "You develop procedures and all that, but more the kind of the analysis side of the story is kind of hierarchy of mathematical things.",
                    "label": 0
                },
                {
                    "sent": "You do.",
                    "label": 0
                },
                {
                    "sent": "First of all, you may try to prove consistency that if there is a correct answer, you'll converge to that no matter what that correct answer was.",
                    "label": 0
                },
                {
                    "sent": "So that's often kind of fairly straightforward and not that informative, more informative thing to do is to get rates of conversions towards rates of convergence.",
                    "label": 0
                },
                {
                    "sent": "02 procedures are both good.",
                    "label": 0
                },
                {
                    "sent": "They're both consistent, but maybe one of them has a faster convergence rate in terms of number of data points.",
                    "label": 0
                },
                {
                    "sent": "I might want to prefer that procedure alot.",
                    "label": 0
                },
                {
                    "sent": "Alot of work is on that and more hard, but also very important is to try to get sampling distributions that as the number of data points gets large.",
                    "label": 0
                },
                {
                    "sent": "Perhaps I converted some nice distribution like a Koshi or normal or something like that, and then I can use that distribution I can get.",
                    "label": 0
                },
                {
                    "sent": "Error bars so I can get error bars by finding out the sampling distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is certainly work on consistency and animal literature.",
                    "label": 0
                },
                {
                    "sent": "There's some on rates and there's very little long sampling distributions, so classical frequentist statistics.",
                    "label": 1
                },
                {
                    "sent": "We focus on parametric statistics in the 40s and 50s, but since then it's mainly been nonparametric.",
                    "label": 1
                },
                {
                    "sent": "Really, there's a lot of nonparametric testing and there's tons of other kind of person function estimation.",
                    "label": 0
                },
                {
                    "sent": "And all these large P small in problems where these are going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "The number of parameters is going to Infinity as well as the number of data points.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So often you'll see people say, well, classical statistics was parametric and so on.",
                    "label": 0
                },
                {
                    "sent": "But you know it's just not.",
                    "label": 0
                },
                {
                    "sent": "The tools were developed to be general Nonparametrics is probably part of the story, one of the most general tools, its empirical process theory, empirical process theory talks about convergence of objects uniformly, so you find consistency rating distribution uniformly on various spaces, function spaces, parameter spaces, measure spaces, and so on, so forth.",
                    "label": 1
                },
                {
                    "sent": "So statistical learning theory is really a part of that, it's it's a particular area of empirical process theory that focuses on 01 loss.",
                    "label": 0
                },
                {
                    "sent": "But the tools there, Rademacher, and all that were developed in empirical process, their whole books on this.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in theory, this tool is available.",
                    "label": 0
                },
                {
                    "sent": "It's used to prove things about the bootstrap.",
                    "label": 0
                },
                {
                    "sent": "It's used things to prove things about them, estimators and so on, so forth.",
                    "label": 0
                },
                {
                    "sent": "A lot of frequency analysis using this big heavy tool.",
                    "label": 0
                },
                {
                    "sent": "And there are lots of other tools that are simpler, but that ones always always available.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to take a little positive there.",
                    "label": 0
                },
                {
                    "sent": "Any first many questions and then just kind of stretching, pause and then the rest of my presentation today and the next time.",
                    "label": 0
                },
                {
                    "sent": "Are going to be some little vignettes on research that I've been involved in.",
                    "label": 0
                },
                {
                    "sent": "That is all frequentist and try to give you a better flavor.",
                    "label": 0
                },
                {
                    "sent": "What quiz activity really is like?",
                    "label": 0
                },
                {
                    "sent": "What kind of problems are set them up and see that there's some challenges there and see how to overcome them.",
                    "label": 0
                },
                {
                    "sent": "Sewing machine learning methods, but then analyze frequently POV and try to carry all the way through to the end.",
                    "label": 0
                },
                {
                    "sent": "So I think I'll probably the rest of this talk.",
                    "label": 0
                },
                {
                    "sent": "They talk about experimental design and then these things.",
                    "label": 0
                },
                {
                    "sent": "Three things will be for the next presentation, so any questions on sort of the philosophical stuff first.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Boots that would.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "One of my current favorite books are just kind of statistics in general is add vendor barks asymptotically statistics.",
                    "label": 0
                },
                {
                    "sent": "Add takes a Catholic view, has got Bayesian and frequentist arguments throughout it.",
                    "label": 0
                },
                {
                    "sent": "It's probably more frequently overall, but it's it's got Bayesian theorems as well.",
                    "label": 0
                },
                {
                    "sent": "Jim Burger if you don't.",
                    "label": 0
                },
                {
                    "sent": "If you not been introduced to James Burger yet, you should be.",
                    "label": 0
                },
                {
                    "sent": "He's got a great book on statistical decision theory.",
                    "label": 0
                },
                {
                    "sent": "The first addition of it was frequently in the second issue was Bayesian, and it's kind of good to read both of them.",
                    "label": 0
                },
                {
                    "sent": "I he anyway and then the second issue as well, there's just.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of merging of frequentist and Bayesian ideas, and I just think reading his book and his papers in general is a very good educational experience.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Is also subjective.",
                    "label": 0
                },
                {
                    "sent": "Is objective based also subjective?",
                    "label": 0
                },
                {
                    "sent": "Well, you know?",
                    "label": 0
                },
                {
                    "sent": "Yeah sure, in the sense that I've written down a big complicated model and some of the parameters I'm going to possibly able to elicit subjectively.",
                    "label": 0
                },
                {
                    "sent": "And there's a whole bunch of others that are often called nuisance parameters or whatever that I don't want to or can't listen should actively.",
                    "label": 0
                },
                {
                    "sent": "I'll try to use objective Bayesian methods for those so you know, most Bayesians do this.",
                    "label": 0
                },
                {
                    "sent": "Actually in real life they will sit down and say something with this parameter.",
                    "label": 0
                },
                {
                    "sent": "I sort of believe is in this range for this or that reason.",
                    "label": 0
                },
                {
                    "sent": "And there's this scale factor.",
                    "label": 0
                },
                {
                    "sent": "I have no idea what it should be.",
                    "label": 0
                },
                {
                    "sent": "Let's put a Jeffreys prior on that now.",
                    "label": 0
                },
                {
                    "sent": "That's kind of ad hoc thing to do, and there's a lot of ad hoc re, but that's kind of your in effect blending objective based in subjective base.",
                    "label": 0
                },
                {
                    "sent": "So I think objective base is kind of big.",
                    "label": 0
                },
                {
                    "sent": "Tend to that incorporates subjective as well as objective ideas.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I'd like to make it interesting that yes.",
                    "label": 0
                },
                {
                    "sent": "OK, no, the diapers are not functions of the data in reference priors.",
                    "label": 0
                },
                {
                    "sent": "This is your as you're sitting there.",
                    "label": 0
                },
                {
                    "sent": "You haven't seen any data yet at all, and you're thinking about what prior should I use?",
                    "label": 0
                },
                {
                    "sent": "You're free to envision datasets you could get.",
                    "label": 0
                },
                {
                    "sent": "You haven't seen any data yet, but you're free to sit and imagine possible datasets you could get.",
                    "label": 0
                },
                {
                    "sent": "OK. And the vergence function in the reference prior is that expectation over possible datasets you could get.",
                    "label": 0
                },
                {
                    "sent": "So in fact it has a little frequentist kind of mathematical character, but it's perfectly Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Babies are freedom to dream about datasets and not just one data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And if you have to imagine the possible datasets that you that you're going to get, are you then?",
                    "label": 0
                },
                {
                    "sent": "Solving the problem you want to solve.",
                    "label": 0
                },
                {
                    "sent": "I mean, don't you need for being respected your posterior in order to know which data set?",
                    "label": 0
                },
                {
                    "sent": "How do you get your posterior prior to get your posterior we're getting prior from you know, but I'm just saying like if you say I need to use the, imagine the datasets that I'm going to see, but isn't that the whole point that isn't that knowing your posterior?",
                    "label": 0
                },
                {
                    "sent": "So if you don't know your posterior, how could you do that?",
                    "label": 0
                },
                {
                    "sent": "No, no, I've got I. I wrote down a probability model in beginning, everybody agrees.",
                    "label": 0
                },
                {
                    "sent": "You sort of have to start there.",
                    "label": 0
                },
                {
                    "sent": "OK. And now I can imagine datasets under that probability model.",
                    "label": 0
                },
                {
                    "sent": "So that defines my probability measure and I can go from there and now take averages with respect to that, and that's what the reference parts does.",
                    "label": 0
                },
                {
                    "sent": "Just you just need the likelihood.",
                    "label": 0
                },
                {
                    "sent": "And given likelihood you do this.",
                    "label": 0
                },
                {
                    "sent": "Hope this divergences maximization thing that gives you a prior and now you see a real data set you put your prior together real, like you're likely on that data set you observed and do a posterior.",
                    "label": 0
                },
                {
                    "sent": "It's perfectly Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Statistical reference does depend on the size of data set that you're expecting too, yeah?",
                    "label": 0
                },
                {
                    "sent": "Depends on that.",
                    "label": 0
                },
                {
                    "sent": "It's sort of an experimental design flavor, and you know that which is which is arguably a good thing.",
                    "label": 0
                },
                {
                    "sent": "You kind of want to think about how your data will be gathered.",
                    "label": 0
                },
                {
                    "sent": "There's a very good Jose Bernardo was the first person to talk about this in great detail, and he's got a lot of papers talking about Huawei experiment design.",
                    "label": 0
                },
                {
                    "sent": "It should be taken into account in Bayesian arguments.",
                    "label": 0
                },
                {
                    "sent": "A lot of the likelihood principle shouldn't, and that is sort of a misleading argument.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to read more about this, read some Jose's papers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it seems that.",
                    "label": 0
                },
                {
                    "sent": "The introductory imagine your potential dates that you could have 20 proprietors kind of similar to that, so you imagine your privacy or updates that you might have and try and model that in the prior and then in the objective framework you trying imagine data.",
                    "label": 0
                },
                {
                    "sent": "Now your data set your measure on X space, so the data is in X.",
                    "label": 0
                },
                {
                    "sent": "You know we're trying to use that somehow.",
                    "label": 0
                },
                {
                    "sent": "Get some Theta.",
                    "label": 0
                },
                {
                    "sent": "OK, so that didn't help you.",
                    "label": 0
                },
                {
                    "sent": "Imagine datasets in X to get something on Theta, right?",
                    "label": 0
                },
                {
                    "sent": "Whether you're doing you're simply trying to find a prior that under these datasets you're imagining, quote, unquote.",
                    "label": 0
                },
                {
                    "sent": "Has little impact on your posterior as possible.",
                    "label": 0
                },
                {
                    "sent": "And it's pretty neat that you actually just write that down.",
                    "label": 0
                },
                {
                    "sent": "Write down the KL diversions with respect to prior and posterior.",
                    "label": 0
                },
                {
                    "sent": "You average that over X.",
                    "label": 0
                },
                {
                    "sent": "That's where the averaging is coming.",
                    "label": 0
                },
                {
                    "sent": "You don't know that yes, you can average over all these possible dates that everyone you get the mutual information.",
                    "label": 0
                },
                {
                    "sent": "And you solve that problem and out pops it prior, and it often is a Jeffreys prior, and it has a lot of nice properties.",
                    "label": 0
                },
                {
                    "sent": "I should go over that side.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of questions here, yeah?",
                    "label": 0
                },
                {
                    "sent": "Nothing like this.",
                    "label": 0
                },
                {
                    "sent": "Use their answers to rank different estimators, yeah?",
                    "label": 0
                },
                {
                    "sent": "Anything you can come up with our boundaries on the quantitative, just not the only thing you can come up with.",
                    "label": 0
                },
                {
                    "sent": "The balance.",
                    "label": 0
                },
                {
                    "sent": "That's not true.",
                    "label": 0
                },
                {
                    "sent": "Lot of physical or any people only use bounds.",
                    "label": 0
                },
                {
                    "sent": "'cause somehow the I think it's more of the CSR strategy.",
                    "label": 0
                },
                {
                    "sent": "It has to be bound.",
                    "label": 0
                },
                {
                    "sent": "But most of the decisions use some products of all kinds expansions that aren't bounds that are hopefully tight.",
                    "label": 0
                },
                {
                    "sent": "Is it necessary that optimizing those is going to give you the best procedure?",
                    "label": 0
                },
                {
                    "sent": "But no.",
                    "label": 0
                },
                {
                    "sent": "I mean, analysis is kind of always grains of salt.",
                    "label": 0
                },
                {
                    "sent": "You know there's you did an analysis and you got an answer, and there's a little bit of error.",
                    "label": 0
                },
                {
                    "sent": "There's a Taylor 3rd order term you've neglected, and so on, but that's what mathematics is about.",
                    "label": 0
                },
                {
                    "sent": "Sort of getting an understanding, and maybe you know not being quite exactly right, but hopefully get a guide and then explore it further.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's no.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "I always rank that procedure over that.",
                    "label": 0
                },
                {
                    "sent": "There's always a little bit to this kind of understanding, the setting and the consequences of the analysis tools available for evaluating methods that give out probabilistic predictions and soft predictions.",
                    "label": 0
                },
                {
                    "sent": "Point estimate.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's what frequent is mainly focused on its coverage.",
                    "label": 0
                },
                {
                    "sent": "Not the not just the point estimates.",
                    "label": 0
                },
                {
                    "sent": "You should not cut.",
                    "label": 0
                },
                {
                    "sent": "Yeah, your loss function can for example be a log loss of predicted distribution.",
                    "label": 0
                },
                {
                    "sent": "You can study that it's frequentist convergence in terms of some procedure.",
                    "label": 0
                },
                {
                    "sent": "It's commonly done.",
                    "label": 0
                },
                {
                    "sent": "Yeah, absolutely maybe one more question then I want.",
                    "label": 0
                },
                {
                    "sent": "I do have a second part of the talk that I want to get too.",
                    "label": 0
                },
                {
                    "sent": "So yeah, sometimes I worry about it being recommended for general tool.",
                    "label": 0
                },
                {
                    "sent": "Is that when you often get two data points exactly same place now something my mother finger who like noise level and do something crazy so thankful Bayesian model works.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's all kinds of bootstrap literature and there's the simplest bootstrap.",
                    "label": 0
                },
                {
                    "sent": "As you know, you know some some issues that that occasionally come up, but there's a general technique called resampling methods, and there's a lot of worry about that.",
                    "label": 0
                },
                {
                    "sent": "In fact, you can prove their situations with Bootstrap is not consistent.",
                    "label": 0
                },
                {
                    "sent": "OK, but there are correction.",
                    "label": 0
                },
                {
                    "sent": "There are better bootstraps which are consistent.",
                    "label": 0
                },
                {
                    "sent": "Alright, there's a whole literature.",
                    "label": 0
                },
                {
                    "sent": "There's a book on resampling Joe Romano and others have been very nice book on on that whole technology.",
                    "label": 0
                },
                {
                    "sent": "So alright, let's take a short.",
                    "label": 0
                },
                {
                    "sent": "I guess it's a 2 minute break and let me have time to get through the rest of my talk.",
                    "label": 0
                },
                {
                    "sent": "OK, those are all great questions and I was glad trigger all of them.",
                    "label": 0
                },
                {
                    "sent": "I really wouldn't mind spending the rest of the time just talking about those things.",
                    "label": 0
                },
                {
                    "sent": "But you know, I do a similar cover.",
                    "label": 0
                },
                {
                    "sent": "So while I'm around the rest of the day, if you want to chat more about those things, I'm happy to talk.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyone about those?",
                    "label": 0
                },
                {
                    "sent": "So as I said, what I'm going to do, the rest of my presentation today.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow is go through some vignettes of particular problems and show you get a little flavor of how you do frequent analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, this first vignette is about Los Funk.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In classification and about experimental design.",
                    "label": 1
                },
                {
                    "sent": "So the main paper this was based on came out last year.",
                    "label": 0
                },
                {
                    "sent": "Like my college join long again, Ann, Martin Wainwright and there are two 2 backup papers that also play some role in development of these ideas.",
                    "label": 0
                },
                {
                    "sent": "So this has to do with things like boosting support vector machine and sort of classification algorithms.",
                    "label": 0
                },
                {
                    "sent": "And there's they all kind of came out.",
                    "label": 0
                },
                {
                    "sent": "Separately and then there was some realization.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of unity and the ideas and there was frequent analysis that came out.",
                    "label": 0
                },
                {
                    "sent": "You know, for example, show that they were consistent, so boosting eventually was shown to be consistent.",
                    "label": 0
                },
                {
                    "sent": "It was not clear in the beginning and support vector machine and so on.",
                    "label": 0
                },
                {
                    "sent": "So we're going to face a hard class of problems.",
                    "label": 0
                },
                {
                    "sent": "We're going to not just classification, but also experimental design simultaneously with classification.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to ask, are things like the boosting loss, the SVM loss, and so on and so forth still consistent even though you're doing this harder problem and that I wouldn't know how to answer that question in less, I did this analysis an having done the analysis, then you actually learn that actually turns out some classes of these loss functions, leading City, and some of them don't, so it's kind of a little bit surprised I'll leave you to kind of guess which of them do in which other, don't you?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what the setup of the problem is.",
                    "label": 0
                },
                {
                    "sent": "So the way we got the original involved in this was a practical problem that some people at the Intel Lab in Berkeley had one of these early sensor networks, and they ask us to solve a classification problem here, which is that they had a bunch of sensors on a grid.",
                    "label": 0
                },
                {
                    "sent": "This were up on the ceiling, and there was a little robot moving around and it had a light source on it and they wanted the robot had gone into a particular region of the room or not, so the region was some green region.",
                    "label": 1
                },
                {
                    "sent": "It could be convex or not, they just want to know whether or not these are highly noisy.",
                    "label": 0
                },
                {
                    "sent": "Sensors, and so it was kind of hard classification problem to solve, but Moreover the more interesting part was that these sensors there's one of them over there.",
                    "label": 0
                },
                {
                    "sent": "Have these little batteries and they if they transmit data all the time, the batteries run out immediately, so they need to transmit only a little bit of data like one or two bits for time slice.",
                    "label": 1
                },
                {
                    "sent": "So you can't transmit the real valued voltage you're sensing of the light.",
                    "label": 0
                },
                {
                    "sent": "You need to transmit a quantized version of that, and the question became how do you quantize what's the what's the optimal way to quantize?",
                    "label": 0
                },
                {
                    "sent": "Given that my problem is one of classification.",
                    "label": 0
                },
                {
                    "sent": "If it was just data compression, I know how to quantize, but now I'm trying to quantize for the purposes of classification.",
                    "label": 0
                },
                {
                    "sent": "How do I do that?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so here's the abstraction of the problem, so there's a bunch of observables X one through XS that are often real valued quantities.",
                    "label": 0
                },
                {
                    "sent": "But since they've actually gone through a analog to digital processor, they've actually become quantized.",
                    "label": 0
                },
                {
                    "sent": "But the cardinality of the quantization is really big, so M is really, really large.",
                    "label": 0
                },
                {
                    "sent": "Alright, we're going to quantize those with Quantizers Q1 through QS, and these are distributed, so this is this is a different spatial locations, right?",
                    "label": 0
                },
                {
                    "sent": "And so this Q here doesn't get to see any other ex is this is a local calculation, so this quantizer then spits out a Z1, which is the quantized version of X1, and so on.",
                    "label": 0
                },
                {
                    "sent": "And this one it really is quantized in its cardinality, is much much smaller than M and then the central.",
                    "label": 0
                },
                {
                    "sent": "These then are transmitted to a central site over the radio and the central site hits a discriminant function to the Z values and tries to predict.",
                    "label": 0
                },
                {
                    "sent": "Yes or no.",
                    "label": 0
                },
                {
                    "sent": "You're in the green region and of course you were in the green.",
                    "label": 0
                },
                {
                    "sent": "Are you or not?",
                    "label": 0
                },
                {
                    "sent": "And depending on the value of that hypothesis, you get different distributions on your light sort sensors.",
                    "label": 0
                },
                {
                    "sent": "That problem is called decentralized detection.",
                    "label": 1
                },
                {
                    "sent": "It actually exists in the literature before we got involved with it in the 80s was a hot topic in electrical engineering.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Signal processing.",
                    "label": 0
                },
                {
                    "sent": "OK, so the general set up is going to be XY pairs.",
                    "label": 0
                },
                {
                    "sent": "Let's assume their ID for simplicity.",
                    "label": 0
                },
                {
                    "sent": "The wise are going to be 01.",
                    "label": 0
                },
                {
                    "sent": "We're going to have a quantizer now.",
                    "label": 0
                },
                {
                    "sent": "That takes the original covariate vector X and turns that into a quantized version Z.",
                    "label": 0
                },
                {
                    "sent": "And it would be quantization space in which Q lies in an Q is the space is some sort of random mapping, so it for our analysis that needs to be set of random mappings.",
                    "label": 1
                },
                {
                    "sent": "But in practice, we would often implemented as a deterministic mapping.",
                    "label": 1
                },
                {
                    "sent": "OK, so Q is a random mapping all.",
                    "label": 0
                },
                {
                    "sent": "Right now we're going to stick analysis of this kind of object and what kind of object is this?",
                    "label": 1
                },
                {
                    "sent": "Well, this is known as an experimental design, an offer mazina something like an analysis of variance table.",
                    "label": 0
                },
                {
                    "sent": "You know a person comes in and they went to sell 1, three or cell 45.",
                    "label": 0
                },
                {
                    "sent": "That's experimental design, but it's really a broader the broad mathematical problem is really just a kind of a mapping from some space Zita.",
                    "label": 0
                },
                {
                    "sent": "Some specs to Z.",
                    "label": 0
                },
                {
                    "sent": "And the mapping random.",
                    "label": 0
                },
                {
                    "sent": "In fact, in the analysis variance it is you come in here it's a randomized experiment you're assigned to some selling or some random assignment.",
                    "label": 0
                },
                {
                    "sent": "So the Z space here would be the cell of the analysis of variance table.",
                    "label": 0
                },
                {
                    "sent": "And that's a discrete variable.",
                    "label": 0
                },
                {
                    "sent": "Right index comes in its description of the person who gets put in one of these cells.",
                    "label": 0
                },
                {
                    "sent": "So that's just one example of a mapping Q.",
                    "label": 0
                },
                {
                    "sent": "But then there are many others.",
                    "label": 0
                },
                {
                    "sent": "So to set in generality you just allow you to be in some space and then you have the space be characterized in various different ways.",
                    "label": 0
                },
                {
                    "sent": "So I will use the language of experimental design.",
                    "label": 0
                },
                {
                    "sent": "Quantization is a special case of that analysis variance as a special case, and so on.",
                    "label": 0
                },
                {
                    "sent": "If you prefer one of those.",
                    "label": 0
                },
                {
                    "sent": "Special cases think in those terms.",
                    "label": 0
                },
                {
                    "sent": "Now that's half the problem is, is the thermal design, but the other half of the problem is the discriminant function I the classifier.",
                    "label": 0
                },
                {
                    "sent": "So we have this family of classifiers that lie in some family big gamma, and it's probably going to be a large family, nonparametric family and our problem is to define the decision like in our decision theoretic framework to know decision decision was two parts, it's to choose the quantizer Q and the choose the discriminant function.",
                    "label": 0
                },
                {
                    "sent": "That's the output is this tuple.",
                    "label": 0
                },
                {
                    "sent": "Alright, and what's our loss function?",
                    "label": 0
                },
                {
                    "sent": "Well, the risk is going to be the probability made an error, so that's why they want the quantized discriminant function value is not equal to the correct label that is A10 loss function.",
                    "label": 0
                },
                {
                    "sent": "If I take the expectation that the probability that they're not equal.",
                    "label": 0
                },
                {
                    "sent": "So this is the risk function as a function of Q and gamma, so it's different notation, but it is fits in my decision theoretic framework we talked about earlier.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many applications of this.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you look at the existing literature, there's sort of help on two sides of the equation, but not on both of them simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So the classical signal processing literature define this problem.",
                    "label": 1
                },
                {
                    "sent": "Decentralized detection, and it assumes that everything is known except for the quantizer.",
                    "label": 1
                },
                {
                    "sent": "OK, everything means that all the probability solutions are known.",
                    "label": 0
                },
                {
                    "sent": "The class conditional probability distributions, the class prior probabilities and so on.",
                    "label": 0
                },
                {
                    "sent": "So all that's not known is Q.",
                    "label": 0
                },
                {
                    "sent": "Might and so how do you find Q and something a little drawing?",
                    "label": 0
                },
                {
                    "sent": "I didn't do it on the slides here.",
                    "label": 0
                },
                {
                    "sent": "I guess I turn on that light over here.",
                    "label": 0
                },
                {
                    "sent": "I didn't come on.",
                    "label": 0
                },
                {
                    "sent": "did I do that wrong?",
                    "label": 0
                },
                {
                    "sent": "No, I didn't work.",
                    "label": 0
                },
                {
                    "sent": "So in my original space, there's the X space over here.",
                    "label": 0
                },
                {
                    "sent": "You know they had Class 1.",
                    "label": 0
                },
                {
                    "sent": "And class 0.",
                    "label": 0
                },
                {
                    "sent": "Maybe looking like that and it's sort of hard to discriminate boundary among these two things, so I might want to use a mapping queue which goes over to a space Z, which pushes them as far apart as possible.",
                    "label": 0
                },
                {
                    "sent": "Right, that would be a good choice of Q, and if I did a bad choice of Q it would smash them together even further.",
                    "label": 0
                },
                {
                    "sent": "Alright, so all I gotta do is measure in some ways the divergences among probability distributions and then optimize Q with respect to the versions.",
                    "label": 0
                },
                {
                    "sent": "What version should you use?",
                    "label": 0
                },
                {
                    "sent": "Alright, well you're trying to maximize divergences here.",
                    "label": 0
                },
                {
                    "sent": "It's often we talk about minimum divergences, a maximum divergent problem, and so these guys said, well, what are some divergences you can maximize?",
                    "label": 0
                },
                {
                    "sent": "And so they wrote down lots of kind of function functionals on probability solutions, and they found that some of them were easy to maximize.",
                    "label": 0
                },
                {
                    "sent": "Some of them were not, and so they picked those, and that's what they did.",
                    "label": 0
                },
                {
                    "sent": "So hell, injure badura a whole bunch of other kind of came out of that literature and became famous in other fields.",
                    "label": 0
                },
                {
                    "sent": "And they were set up because of this problem of diversions maximization.",
                    "label": 0
                },
                {
                    "sent": "So lots of radar has been done this way.",
                    "label": 0
                },
                {
                    "sent": "You know, it's a big literature where people have just picked a diversion, say, hell injure turn off or something.",
                    "label": 0
                },
                {
                    "sent": "And then it's a function of probability solution.",
                    "label": 0
                },
                {
                    "sent": "But you assume those positions are known.",
                    "label": 0
                },
                {
                    "sent": "And so just write down the expected vergence maximized spectacu and then pop out that Q back to the user and so then you put that into your radar on the radar quantizes.",
                    "label": 0
                },
                {
                    "sent": "In that way it's called signal set selection.",
                    "label": 1
                },
                {
                    "sent": "Alright, so that's the story and I would view this as a basically a heuristic literature.",
                    "label": 0
                },
                {
                    "sent": "It could turn the lights off now that I can have control control.",
                    "label": 0
                },
                {
                    "sent": "So it's heuristic.",
                    "label": 0
                },
                {
                    "sent": "What do I do?",
                    "label": 0
                },
                {
                    "sent": "Two sets of switches.",
                    "label": 0
                },
                {
                    "sent": "Alright, so it's basically using a plugin and then not really worrying about how well it performs.",
                    "label": 0
                },
                {
                    "sent": "You put it in and you don't then try to evaluate how well that does.",
                    "label": 1
                },
                {
                    "sent": "Alright, so the simple machine learning literature in their hands as focused on problems where the whole problem is to find the discriminant function and not worry at all about the experimental design.",
                    "label": 0
                },
                {
                    "sent": "So you assume that that's known and you try to find that OK, and the way it's done is by defining a surrogate loss function, boost indiscretions, Porter machines are all based on surrogate loss functions, and this is kind of it's more rigorous.",
                    "label": 1
                },
                {
                    "sent": "There's a decision theoretic flavor, there's consistent results and so on so forth, but isn't really facing the whole problem, which is to find the Q and the gamma simultaneously.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's build up a little machinery.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "F divergences these are the guys that have been discussed by the signal processing literature and of course they then appear in many others as well and kind of part.",
                    "label": 0
                },
                {
                    "sent": "Part of this talk is going to somehow unify these things.",
                    "label": 0
                },
                {
                    "sent": "It's just a list of things in your mind.",
                    "label": 0
                },
                {
                    "sent": "There's gonna be relationships here.",
                    "label": 0
                },
                {
                    "sent": "So in this talk about discrete random variables, just for simplicity is alright.",
                    "label": 0
                },
                {
                    "sent": "So instead of integrals, but you can do this with continuous as well.",
                    "label": 0
                },
                {
                    "sent": "So you define the F diversions between two measures mu and \u03c0, as F of the likelihood ratio, and the integrate, or you average with respect to the pie.",
                    "label": 0
                },
                {
                    "sent": "OK, so that looks kind of like Hilda vergence, and in fact, if F is chosen to be you log U, you get KL diversions.",
                    "label": 1
                },
                {
                    "sent": "Alright, but if F is chosen to be absolute value minus one, for example, then you get out the variational distance, which is here just the L1 distance on measures F can be any continuous convex function, so these are particular examples, But here's another one.",
                    "label": 1
                },
                {
                    "sent": "And if you plug that one in, you get out the Ellinger distance, and this goes on for several pages.",
                    "label": 0
                },
                {
                    "sent": "So you plug in any continuous convex F, and you look at you now define the new ally, Sylvie or F. Vergence.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so why did these guys use the app divergences while they were somehow intuitively appealing, but there was a little bit of kind of underlying theory behind that choice.",
                    "label": 0
                },
                {
                    "sent": "It's not entirely satisfactory.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's not really satisfying at all, but it.",
                    "label": 0
                },
                {
                    "sent": "It's a good starting place.",
                    "label": 0
                },
                {
                    "sent": "And the theorem was due to David Blackwell in 1951.",
                    "label": 0
                },
                {
                    "sent": "Classical paper.",
                    "label": 0
                },
                {
                    "sent": "Well worth reading had a big impact on economics.",
                    "label": 0
                },
                {
                    "sent": "And so his theorem stated the following.",
                    "label": 0
                },
                {
                    "sent": "If a procedure, a IE, some kind of an estimator has a smaller F divergent than a procedure be for some particular choice of F in your F diversions.",
                    "label": 1
                },
                {
                    "sent": "OK, then there exists some set of prior probabilities.",
                    "label": 0
                },
                {
                    "sent": "These are the class probabilities that you're in one class or the other, so some set of problems in the class is such that procedure A has a smaller probability of error than procedure be.",
                    "label": 1
                },
                {
                    "sent": "Well, that's what you care about.",
                    "label": 0
                },
                {
                    "sent": "That's the risk.",
                    "label": 0
                },
                {
                    "sent": "So we've now just learn the procedure as a smaller risk than procedure be.",
                    "label": 0
                },
                {
                    "sent": "That's good, we should now choose proper procedure A and we were told that by looking after Vergence is.",
                    "label": 0
                },
                {
                    "sent": "So now I have to watch it gives us some information risk, right?",
                    "label": 0
                },
                {
                    "sent": "Well, this is just an existence statement, though it says there exists some set of problems we don't know what the problems are.",
                    "label": 0
                },
                {
                    "sent": "For which that after versions gives us this ranking.",
                    "label": 0
                },
                {
                    "sent": "And we don't know in our particular problem which after versions to use.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not that help when it's not at all helpful in practice, but it does at least suggest that after versions are not unreasonable objects to be looking at, if you're trying to minimize the probability of error, and that's a good thing, because minimizing the probability error is course nonconvex problem, the risk the 01 loss is non convex.",
                    "label": 0
                },
                {
                    "sent": "And so you try to find some other kind of function you can optimize, and these things are convex and therefore you might try them by this theorem.",
                    "label": 0
                },
                {
                    "sent": "So that's what people did.",
                    "label": 0
                },
                {
                    "sent": "There's famous papers, Kayla, the 1967 and so on, choosing particular divergences and just kind of in some sense, hoping that the priors now we're right for that afterwards on the particular problem.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, now there are some supporting arguments for asymptotically these divergences also arise in other ways.",
                    "label": 0
                },
                {
                    "sent": "In fact, the original callback leader, Vergence, arose by analysis of hypothesis testing.",
                    "label": 0
                },
                {
                    "sent": "It characterizes the power function in hypothesis testing, where two classes are staying at a fixed distance apart.",
                    "label": 0
                },
                {
                    "sent": "As the number of data points gets large and similarly turn off, distance arise when you do the same analysis in the Bayesian setting, we have priors longer classes, so these divergences were kind of in some ways talking about probability of error directly in this case, but it's an asymptotically arguments hypothesis testing.",
                    "label": 0
                },
                {
                    "sent": "So anyway, it's still a heuristic literature.",
                    "label": 0
                },
                {
                    "sent": "Honey, let's turn to the other side of the coin.",
                    "label": 0
                },
                {
                    "sent": "How to choose the discriminant function, and so you kind of know know this stuff.",
                    "label": 0
                },
                {
                    "sent": "This is a machine learning kind of 101.",
                    "label": 0
                },
                {
                    "sent": "You choose a loss.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That measures the distance between your class labeling your discriminate you were going to start with.",
                    "label": 0
                },
                {
                    "sent": "01 loss is kind of the real loss we're trying to optimize, and in the binary case you can write that as the indicator function of when the labels disagree.",
                    "label": 1
                },
                {
                    "sent": "OK, the labels disagree.",
                    "label": 0
                },
                {
                    "sent": "In fact, now I'm using Y is 1 -- 1 and the discriminant function outputs also one or minus one or a real number in general.",
                    "label": 1
                },
                {
                    "sent": "So if they disagree in their sign, that's bad and you pay a loss of one in that case.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you pay loss of 0.",
                    "label": 1
                },
                {
                    "sent": "OK, so mainly focuses on the discriminant function.",
                    "label": 0
                },
                {
                    "sent": "Now we know also from this point of view, it's intractable.",
                    "label": 0
                },
                {
                    "sent": "Minimize your loss affected this argument as well.",
                    "label": 1
                },
                {
                    "sent": "So instead what people have done is they pick these surrogate loss functions which are convex up.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ground 01 loss, so hopefully you've all seen this picture.",
                    "label": 0
                },
                {
                    "sent": "Here is the 01 loss expressed in terms of this margin value, either product of the Y and the gamma of Z, and if you disagree, you're on this side.",
                    "label": 1
                },
                {
                    "sent": "You pay with loss of 1, otherwise you pay loss of zero, and so it's intractable.",
                    "label": 1
                },
                {
                    "sent": "To optimize this instead, people look at these upper bounds and the blue line is the is the support vector machine, the hinge loss, the red one.",
                    "label": 0
                },
                {
                    "sent": "Here is logistic, the green one I think is the boosting loss the exponential loss, which is what.",
                    "label": 0
                },
                {
                    "sent": "What gives you boosting?",
                    "label": 0
                },
                {
                    "sent": "And there's a whole bunch of others that are that you know this.",
                    "label": 0
                },
                {
                    "sent": "This page could be littered with examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so all those different procedures aren't that different from, you know, optimization point of view.",
                    "label": 0
                },
                {
                    "sent": "You just write down these just optimize over these particular choices curves and then you hopefully try to prove something about them.",
                    "label": 0
                },
                {
                    "sent": "What can you prove about them?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, let's try to set up a little bit of the theory so.",
                    "label": 1
                },
                {
                    "sent": "And we're doing optimization here.",
                    "label": 0
                },
                {
                    "sent": "This is sometimes in statistics is called M estimation.",
                    "label": 0
                },
                {
                    "sent": "M estimators.",
                    "label": 0
                },
                {
                    "sent": "You write that a function often is called a contrast instead of loss that distinguish between the thing you're trying to analyze.",
                    "label": 0
                },
                {
                    "sent": "This is a procedure, not analysis.",
                    "label": 0
                },
                {
                    "sent": "But you're optimizing, and that's called em estimation, and the machine learning that is often called empirical risk mitigation system is the same idea.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we have this idea.",
                    "label": 1
                },
                {
                    "sent": "Training data we write down and M estimation functional, which is this object here which just takes our contrast function or loss function.",
                    "label": 0
                },
                {
                    "sent": "So the exponential loss or the hinge loss, we sum it up over our datasets and we call that function.",
                    "label": 1
                },
                {
                    "sent": "We'd like to optimize either, so it's an empirical risk if you will.",
                    "label": 0
                },
                {
                    "sent": "OK. Alright, so here are some theory for this object.",
                    "label": 0
                },
                {
                    "sent": "This is a paper that my colleagues and I worked on that gives you necessary and sufficient conditions.",
                    "label": 0
                },
                {
                    "sent": "There have been work on sufficient conditions.",
                    "label": 0
                },
                {
                    "sent": "This gives a full treatment of these surrogate loss functions.",
                    "label": 1
                },
                {
                    "sent": "It gives both necessary and sufficient conditions for consistency, so we're trying to say if you use these loss functions, did you get the same answer?",
                    "label": 0
                },
                {
                    "sent": "The end as if you had optimized 01 loss?",
                    "label": 0
                },
                {
                    "sent": "That would be a satisfying story and all those guys do that.",
                    "label": 0
                },
                {
                    "sent": "And here.",
                    "label": 0
                },
                {
                    "sent": "Here's here's a theory that tells you that that is the case.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, not any arbitrary fee can be used.",
                    "label": 0
                },
                {
                    "sent": "It has to satisfy some properties.",
                    "label": 0
                },
                {
                    "sent": "In particular, we had a very weak condition called classification calibration, which is essentially a form of Fisher consistency.",
                    "label": 1
                },
                {
                    "sent": "And here's the equation that defines it.",
                    "label": 0
                },
                {
                    "sent": "Let me not spend a lot of time on this, But basically this says that if you disagree with the right answer, you pay a bigger loss that's bigger than if you don't disagree with the right answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so this sort of says that things kind of tilt up to the left that on the left side where you're making an error you have a bigger loss than on the right hand side where you're not making an error.",
                    "label": 1
                },
                {
                    "sent": "OK, so it turns out to be necessary and sufficient for what's called Bayes.",
                    "label": 0
                },
                {
                    "sent": "Consistent, it's not a Bayesian notion as I hope you remember.",
                    "label": 1
                },
                {
                    "sent": "I talk about Bayes risk earlier in the lecture.",
                    "label": 1
                },
                {
                    "sent": "This is consistency in the sense of 01 loss.",
                    "label": 1
                },
                {
                    "sent": "OK, so we will now define a surrogate loss function to be something that is classification calibrated.",
                    "label": 1
                },
                {
                    "sent": "That's the definition of this object here.",
                    "label": 1
                },
                {
                    "sent": "It satisfies this property.",
                    "label": 0
                },
                {
                    "sent": "All right now, the one that you can forget this definition because in the convex case where Phi is convex function, then it's clear if and only if its differentiable at zero and it has a negative derivative at 0.",
                    "label": 0
                },
                {
                    "sent": "So all those curves and don't turn the light 'cause it's not worth it, all those curves.",
                    "label": 0
                },
                {
                    "sent": "Tilted like this at the origin, they had a strictly negative derivative here and.",
                    "label": 0
                },
                {
                    "sent": "And that's all you need for classification calibration.",
                    "label": 0
                },
                {
                    "sent": "So it just matters what happens around the origin.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just kind of set up for the rest of this talk.",
                    "label": 0
                },
                {
                    "sent": "That's what we mean by a surrogate loss funk.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it turns out that those surrogate loss functions defined in this kind of machine learning literature turned out to have a very nice relationship.",
                    "label": 0
                },
                {
                    "sent": "They have divergences sort of surprising but true.",
                    "label": 0
                },
                {
                    "sent": "So there's going to be constructive and many to one correspondence between surrogate loss functions and afterwards they go back and forth in two directions and haven't done that.",
                    "label": 1
                },
                {
                    "sent": "There's about 2 space that we can work on.",
                    "label": 0
                },
                {
                    "sent": "You can be instant loss functions, and you can go instead and work in the space of divergences or vice versa.",
                    "label": 0
                },
                {
                    "sent": "And by doing that, we're able to define a notion of equivalence among loss functions.",
                    "label": 1
                },
                {
                    "sent": "So two loss functions will be equivalent if they map roughly over into the same after vergence.",
                    "label": 0
                },
                {
                    "sent": "Not quite, that's not exactly right.",
                    "label": 0
                },
                {
                    "sent": "There's kind of range of divergences, but we will define a notion of equivalence, and with that notion of equivalence, and it could be extremely easy to prove things like that.",
                    "label": 0
                },
                {
                    "sent": "This procedure is consistent, and this is not.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a nice characterization of a space of loss functions, space of loss functions has some structures, not just a list of loss function that has a lot of structure to it, and we have a theory that shows that explicates that structure.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I got about 10 minutes right?",
                    "label": 0
                },
                {
                    "sent": "Is that actually correct?",
                    "label": 0
                },
                {
                    "sent": "Is that give or take five?",
                    "label": 0
                },
                {
                    "sent": "I start a little late, but I'm trying to figure out where I'm going to be in the middle of this talk by the end of this, so I'm going to figure out where to how far to go.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just set up a little bit of notation.",
                    "label": 0
                },
                {
                    "sent": "This is kind of dull, but necessary notation, so remember, the risk function is a function of the expectations.",
                    "label": 0
                },
                {
                    "sent": "The frequentist expectation of the loss and then the data is the Y in the Z.",
                    "label": 0
                },
                {
                    "sent": "It's a tuple, and then the parameter which was Theta back in the original slides does not become a tuple as well, it's both.",
                    "label": 0
                },
                {
                    "sent": "It's both the discriminate in the quantizer.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, it would be nice to work with conditional distributions of Z, given why the unnormalized conditionals, i.e.",
                    "label": 0
                },
                {
                    "sent": "The joints so musian, piussi.",
                    "label": 0
                },
                {
                    "sent": "Are these class conditional densities unnormalized?",
                    "label": 0
                },
                {
                    "sent": "So P little P&Q are the priors he was 1 -- P?",
                    "label": 0
                },
                {
                    "sent": "And we integrate out the X, which is the unobserved covariates, and so this is not just a function of Z, so we take the class conditional distribution, integrate that out under the quantizer, we get a distribution on Z. OK, so using this notation you can now represent the fee risk.",
                    "label": 0
                },
                {
                    "sent": "The expectation here is of course over the two values that why can take on and so if we just do that expectation over why we get a new Anna Pie and we get a minus from Y equal to minus one and a one where Y is equal to 1, add those two up.",
                    "label": 0
                },
                {
                    "sent": "That's the expectation over Y and then we do the expectation over Z where the mu in the Z are the measures.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just a representation of the risks.",
                    "label": 0
                },
                {
                    "sent": "So this kind of looks a little bit of a convex flavor.",
                    "label": 0
                },
                {
                    "sent": "The fee tilts in One Direction and fee of minus tilt in the other direction.",
                    "label": 0
                },
                {
                    "sent": "This is a convex combination of that kind of, roughly speaking.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some convexity properties already emerging.",
                    "label": 0
                },
                {
                    "sent": "OK, so now as a frequentist you're free to do something what is called profiling.",
                    "label": 0
                },
                {
                    "sent": "I have a function of two arguments and.",
                    "label": 0
                },
                {
                    "sent": "How do I get rid of one of the arguments so I can optimize structure?",
                    "label": 0
                },
                {
                    "sent": "The other argument alright?",
                    "label": 0
                },
                {
                    "sent": "Well if you're a Bayesian, the kind of thing you want to do is to integrate out one of the arguments, but we don't.",
                    "label": 0
                },
                {
                    "sent": "How do you integrate out the discriminant function?",
                    "label": 0
                },
                {
                    "sent": "I don't know how to do that, but I can optimize it out.",
                    "label": 0
                },
                {
                    "sent": "That's called profiling statistics to optimize out one argument so you can focus on the other ones, called profiling.",
                    "label": 0
                },
                {
                    "sent": "You often with profile likelihood for example.",
                    "label": 1
                },
                {
                    "sent": "Alright, so let's profile this risk function by by optimizing out the discriminant function and we get a function.",
                    "label": 0
                },
                {
                    "sent": "Now just of Q and then we could use that as a function too for choosing Q. OK, so let's do that now for some examples.",
                    "label": 0
                },
                {
                    "sent": "So if we choose 01 loss.",
                    "label": 0
                },
                {
                    "sent": "You can easily figure out you can do this optimization and the answer is just that it's a difference of sign of the difference of the two measures, which makes sense if you're doing 0, unless you want to pick the guy that had the bigger measure.",
                    "label": 0
                },
                {
                    "sent": "So you plug that back in to this optimized you will get this function here and I've just done that in a couple of steps here.",
                    "label": 0
                },
                {
                    "sent": "If I plug it in, you just have to do a little calculation.",
                    "label": 0
                },
                {
                    "sent": "If your classification you will know how to do these calculations.",
                    "label": 0
                },
                {
                    "sent": "If not, get out of paper afterwards.",
                    "label": 0
                },
                {
                    "sent": "It's really easy, you plug back into 01 loss.",
                    "label": 0
                },
                {
                    "sent": "You get this this minimum.",
                    "label": 0
                },
                {
                    "sent": "Right makes intuitive sense.",
                    "label": 0
                },
                {
                    "sent": "You should pay the loss of the worst of the smaller class an if you just didn't know the absolute value is equal to this minimum.",
                    "label": 0
                },
                {
                    "sent": "It just really easy to see and then this thing here is 1 minus the variational distance, just by definition.",
                    "label": 0
                },
                {
                    "sent": "OK, alright, so it turns out this profiled risk.",
                    "label": 0
                },
                {
                    "sent": "Happened to be the negative of a of a vergence that's kind of interesting, so if you use this divergents effectively, what you were doing was working with the Profile 01 loss.",
                    "label": 0
                },
                {
                    "sent": "That's kind of interesting, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so we did that calculation and we thought well, does that hold more generally for other kind of losses other than 01 loss?",
                    "label": 1
                },
                {
                    "sent": "And turns out it did turn out.",
                    "label": 0
                },
                {
                    "sent": "It's a really fun exercise to do this for all kinds of losses.",
                    "label": 0
                },
                {
                    "sent": "If you start with the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "For example, an you profile out the discriminant function, you will get one minus the variational distance, so it turned out there.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are two different losses mapped into the same after vergence kind of interesting, so it's not that we thought it might be 1 to one.",
                    "label": 0
                },
                {
                    "sent": "This immediately proved it's not 1 to one relationship.",
                    "label": 0
                },
                {
                    "sent": "What about if you start the exponential loss?",
                    "label": 1
                },
                {
                    "sent": "The boosting loss?",
                    "label": 0
                },
                {
                    "sent": "Well, it turned out.",
                    "label": 0
                },
                {
                    "sent": "Then you got the hell injure distance, and that's a nice lecture size for you to do.",
                    "label": 0
                },
                {
                    "sent": "It's kind of surprising that the exponential function goes into square root.",
                    "label": 1
                },
                {
                    "sent": "If you start with the logistic loss, you got out something looking like the Cal.",
                    "label": 0
                },
                {
                    "sent": "This is kale divergent, symmetrized.",
                    "label": 0
                },
                {
                    "sent": "It's called the capacity discrimination and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So all the losses we could write down, they all turned into F divergences, and so we wondered, is there a general theory behind?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, alright, it turns out there is, yeah.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about that.",
                    "label": 0
                },
                {
                    "sent": "I'm bout to finish and I want to get that would take me a little bit of diversion.",
                    "label": 0
                },
                {
                    "sent": "So let me talk to you about that a little bit later.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that there is a general relationship here that this class of surrogate loss functions Maps over in the class of after vergence is for every surrogate loss function, there is a corresponding F divergent, and for every after vergence there is a class of loss functions and these partition the space an exhaust.",
                    "label": 1
                },
                {
                    "sent": "The space I've sort of glossed functions, so it's a complete characterization of the space of loss functions in terms of F divergences.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so I'm going to give you a little flavor of how that's proved.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's a theorem in annals paper.",
                    "label": 0
                },
                {
                    "sent": "I mentioned the keytool underline.",
                    "label": 0
                },
                {
                    "sent": "It is my favorite tool of convex analysis called conjugate Duality.",
                    "label": 0
                },
                {
                    "sent": "Unifies lots and lots of things.",
                    "label": 0
                },
                {
                    "sent": "So just to remind you what conjugate duality is is if you take a lower semi continuous convex function F. The convex dual is defined as the Supreme of a linear functional minus the original function.",
                    "label": 1
                },
                {
                    "sent": "That's necessarily a convex function, so the star means conjugate dual.",
                    "label": 0
                },
                {
                    "sent": "So we're going to work with F star of negative beta for technical reasons, so this PSI of beta is the thing to remember.",
                    "label": 0
                },
                {
                    "sent": "It's the conjugate dual function up to this flipping of the sign.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I think on the next slide I have the theorem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so let me just take a minute to explain the theorem.",
                    "label": 0
                },
                {
                    "sent": "So this is the theorem that shows that we have this.",
                    "label": 0
                },
                {
                    "sent": "This relationship between divergent and losses so One Direction is pretty straightforward for any certain margin based surrogate loss function.",
                    "label": 0
                },
                {
                    "sent": "There is an after version such that when you profile out.",
                    "label": 1
                },
                {
                    "sent": "The loss function.",
                    "label": 0
                },
                {
                    "sent": "The discriminant function, you get the negative and after versions for some convex function F. So that goes in One Direction.",
                    "label": 0
                },
                {
                    "sent": "Moreover, in going in that direction, it turns out that for fee that is continuous that you get some nice properties of that conjugate dual function.",
                    "label": 0
                },
                {
                    "sent": "These are sort of technical, it's decreasing and convex.",
                    "label": 0
                },
                {
                    "sent": "It kind of it has a fixed point property and there's kind of a cascade property kind of fixed point like property here.",
                    "label": 0
                },
                {
                    "sent": "So let's not worry about the technical details.",
                    "label": 0
                },
                {
                    "sent": "It's just that turns out even if you have this, we condition on fee.",
                    "label": 0
                },
                {
                    "sent": "That's continuous, which all the ones in practice are that you get out these kind of mathematical convexity properties of the conjugate dual.",
                    "label": 1
                },
                {
                    "sent": "Now the other direction is really interesting, one, which is that if F is a lower scimitars convex function that satisfies these conditions, when you take its conjugate dual.",
                    "label": 0
                },
                {
                    "sent": "OK, then there exists a loss function that induces that after version.",
                    "label": 1
                },
                {
                    "sent": "So this goes in the backward direction and shows that after Vergence is also characterized loss functions.",
                    "label": 0
                },
                {
                    "sent": "OK, the forward direction is actually trivial.",
                    "label": 0
                },
                {
                    "sent": "To prove this one page and the backwards direction requires a lot of convex analysis, isn't it?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Definitely non trivial, so here's the easy direction.",
                    "label": 1
                },
                {
                    "sent": "Even though the election I can take you through it, it's pretty easy.",
                    "label": 0
                },
                {
                    "sent": "Here's the risk function.",
                    "label": 0
                },
                {
                    "sent": "Remember, I wrote that down a little while ago.",
                    "label": 0
                },
                {
                    "sent": "Now let's profile that.",
                    "label": 0
                },
                {
                    "sent": "Let's optimize out the discriminant function, right?",
                    "label": 1
                },
                {
                    "sent": "Well, when you when you optimize this out over gamma, we're going to do this for each Z, and so we can move it inside and replace gamma Z with just a number.",
                    "label": 0
                },
                {
                    "sent": "So we get this expression inside.",
                    "label": 0
                },
                {
                    "sent": "OK, and now let's just pull out Pi of Z and now we're left with this expression here.",
                    "label": 0
                },
                {
                    "sent": "OK, just dividing and multiplying, dividing by \u03c0's and pulling it out.",
                    "label": 0
                },
                {
                    "sent": "Alright, and now if we look at this object right here, this is a function of the likelihood ratio, and Moreover this is a convex function.",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "'cause it's here it is.",
                    "label": 0
                },
                {
                    "sent": "I just wrote it down here.",
                    "label": 0
                },
                {
                    "sent": "It's just there's a linear, a bunch of linear family of linear function.",
                    "label": 0
                },
                {
                    "sent": "You take the infimum that gives you a concave function, you flip the sign, you get a convex function.",
                    "label": 0
                },
                {
                    "sent": "So we've identified the F. It's just this function and this thing is now \u03c0 of F of the likelihood ratio.",
                    "label": 0
                },
                {
                    "sent": "It is enough to vergence so all these little examples we were doing were all just examples of that.",
                    "label": 0
                },
                {
                    "sent": "So that's really very very easy.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other direction is hard, but there is a constructive consequences of it, which is that when you go through that proof at some point, you identify the loss function.",
                    "label": 1
                },
                {
                    "sent": "You can write it down, it has a certain form for Alpha which is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "It's just this fixed point.",
                    "label": 0
                },
                {
                    "sent": "For Alpha bigger than zero, it is the conjugate dual function of a free function G. This is like a degree of freedom.",
                    "label": 0
                },
                {
                    "sent": "You can choose any G you want that is increasing continuously convex.",
                    "label": 0
                },
                {
                    "sent": "I'll give us some examples here in the next slide, so sigh of that function and then when Alpha is negative you just get the G function itself, so it turns out that's I had this this form as part of the proof, so this kind of gives a little structure of the possible loss functions.",
                    "label": 0
                },
                {
                    "sent": "You can get an also gives you points out to where the freedom and since this Jeep.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Auction.",
                    "label": 0
                },
                {
                    "sent": "Now, so you can now do this for some examples.",
                    "label": 0
                },
                {
                    "sent": "So if you start with the hell injure distance, remember that is the F divergent where F is equal to the square root function.",
                    "label": 0
                },
                {
                    "sent": "Alright, if you now just take the conjugate dual of that, that's a little easy action.",
                    "label": 0
                },
                {
                    "sent": "Just a piece of calculus.",
                    "label": 0
                },
                {
                    "sent": "To do that.",
                    "label": 0
                },
                {
                    "sent": "You'll get this function.",
                    "label": 0
                },
                {
                    "sent": "That's the conjugate.",
                    "label": 0
                },
                {
                    "sent": "Do love the square root function and take the minus of that, and now that's the PSI function.",
                    "label": 0
                },
                {
                    "sent": "I've calculated it and now I can choose a bunch of GS and plug them into PSI and and I get out loss functions and so if I use G equal to E to EU minus one, that's a particular choice of G. Then I get out the red curve, which is the exponential loss.",
                    "label": 0
                },
                {
                    "sent": "So we've now we've got in the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "We started her distance and we recovered the boosting loss.",
                    "label": 0
                },
                {
                    "sent": "But also I could choose other.",
                    "label": 0
                },
                {
                    "sent": "Geez if I chose G equal to U squared or is equal to U, those those are continuous convex.",
                    "label": 0
                },
                {
                    "sent": "Then I get these other loss functions.",
                    "label": 0
                },
                {
                    "sent": "The reading great curve which are equivalent to the boosting laws in that they map into the same divergent function.",
                    "label": 0
                },
                {
                    "sent": "OK, yes so.",
                    "label": 0
                },
                {
                    "sent": "Version and this gives you constructive proof to go backwards and find correct entire class.",
                    "label": 0
                },
                {
                    "sent": "It defines the entire class in exactly the characterization.",
                    "label": 0
                },
                {
                    "sent": "It just turns out it's not as strong as we need for statistical theory.",
                    "label": 0
                },
                {
                    "sent": "This theory actually defines even a little broader class.",
                    "label": 0
                },
                {
                    "sent": "You take a step, divergences, you brought it out a little bit, and you go backwards to get all losses that map into that and those things turn out to be universally equivalent.",
                    "label": 0
                },
                {
                    "sent": "So you already headed in the right direction, but it turns out that for reasons I'll get into, it's a little broader than just one.",
                    "label": 0
                },
                {
                    "sent": "After versions defines the kind of the whole statistical story.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's the variational distance.",
                    "label": 0
                },
                {
                    "sent": "If you liked the hinge loss you have started with the red curve there, the underlined F divergences is based on the variational, and as we saw earlier slide, you can write that as the using F is equal to the men.",
                    "label": 0
                },
                {
                    "sent": "If you take the conjugate tool of that you get out this function here kind of got the hinge kind of look to it and if you plug in then different choices of G you get out these different curves including the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "And some other curves which are equivalent to the hinge loss in terms of giving you the same variational distance as the after.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And callback levler you can kind of play the same story and so on.",
                    "label": 0
                },
                {
                    "sent": "So I am.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About ready to run out of time.",
                    "label": 0
                },
                {
                    "sent": "Let me see.",
                    "label": 0
                },
                {
                    "sent": "Let me just page through a couple of things so I can see where I am and then 'cause I'm going to stop here.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I'm just let's see base consistency and the universal equivalent story in the theorem.",
                    "label": 0
                },
                {
                    "sent": "There's three theorems here.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get to 3, which is my favorite one, but I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "Just let's see where was I?",
                    "label": 0
                },
                {
                    "sent": "I think I'm just gonna set it up and I'm going to stop there so.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I've given you a theorem that relates after verses in losses and now I'm going to anticipate how what you are going to have this theorem so 01 loss is the goal of classification, so let's start with that and we now map into into diversion space and we get the variational distance with F chosen this way.",
                    "label": 0
                },
                {
                    "sent": "OK, now that's plug it in right here.",
                    "label": 0
                },
                {
                    "sent": "Let's now consider a broader class of F divergences defined by taking this kind of fine expansion of the original after versions of the men of you one.",
                    "label": 1
                },
                {
                    "sent": "This kind of a bigger class of after vergence is.",
                    "label": 0
                },
                {
                    "sent": "Alright, and now let's get all those F divergences.",
                    "label": 0
                },
                {
                    "sent": "So if I could draw a little picture here and this will return next time.",
                    "label": 0
                },
                {
                    "sent": "So we've started with with a particular loss.",
                    "label": 1
                },
                {
                    "sent": "We care about the 01 loss here.",
                    "label": 0
                },
                {
                    "sent": "We mapped over the space of F divergences there and we got the variational distance, and then we're going to broaden out a little bit and get a class of kind of all the affine combinations sort of thing of that one.",
                    "label": 0
                },
                {
                    "sent": "And then we're going backwards and get a broader class of loss functions.",
                    "label": 0
                },
                {
                    "sent": "All the ones that map into here.",
                    "label": 0
                },
                {
                    "sent": "It's kind of composed of several subsets.",
                    "label": 0
                },
                {
                    "sent": "There's a whole bunch of them over here.",
                    "label": 0
                },
                {
                    "sent": "Alright, it will turn out that all of these over here have the same statistical properties in 01 loss, right?",
                    "label": 0
                },
                {
                    "sent": "So you get consistency immediately for all of them.",
                    "label": 0
                },
                {
                    "sent": "Alright, anything outside of that does not.",
                    "label": 0
                },
                {
                    "sent": "It's there, not universal equivalence.",
                    "label": 0
                },
                {
                    "sent": "We can prove lack of consistency and consistency, so you don't see that yet.",
                    "label": 0
                },
                {
                    "sent": "'cause you don't know why I picked this particular fine thing, but you'll see that in theorem two or three.",
                    "label": 1
                },
                {
                    "sent": "But anyway, let's just assume that that's a reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "We do that and immediately that's going to tell us about Bayes consistency and will also give us the converse, which that only these flosses yield based consistency.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've set up the story, I guess the slides will be available.",
                    "label": 0
                },
                {
                    "sent": "You can look ahead if you want to read the annals paper.",
                    "label": 0
                },
                {
                    "sent": "You have nothing else to do with your time.",
                    "label": 0
                },
                {
                    "sent": "You can see these theorems and see how they proved right.",
                    "label": 0
                },
                {
                    "sent": "Let me stop and see if there are questions.",
                    "label": 0
                },
                {
                    "sent": "This is half of a story.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I OK so.",
                    "label": 0
                },
                {
                    "sent": "So this talk is about classification.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're centered around 01 loss and we're developing a kind of a theory for that.",
                    "label": 0
                },
                {
                    "sent": "There's a whole other sort of story to talk about, other loss, other so kind of the focus of the law.",
                    "label": 0
                },
                {
                    "sent": "So if I was doing regression, I would develop a different parallel story here when I get to the end of this sequence of theorems, I will be able to see a little bit more about that.",
                    "label": 0
                },
                {
                    "sent": "Alright, let's just focus on binary classification for now.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Would it also make sense about in this space so early in the summer school one of the lecture was saying that there's more?",
                    "label": 0
                },
                {
                    "sent": "More widely use of kernel methods in response to find distances between.",
                    "label": 0
                },
                {
                    "sent": "Yeah no.",
                    "label": 0
                },
                {
                    "sent": "So we worked on distance between distribution defined by kernels and in fact one of the applications of this will be to that at the end of this talk.",
                    "label": 0
                },
                {
                    "sent": "So will talk about kernel space characterizations and essentially comes about by having a function space over which you optimize and that functions based kernel space.",
                    "label": 0
                },
                {
                    "sent": "It's just one particular choice.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'll just one way to get a kind of rich class to optimize over so it's not a fundamental part of the story, but it's just kind of convenient part of the story, yeah?",
                    "label": 0
                },
                {
                    "sent": "University at all the estimators within that we couldn't even for the purpose of this particular efficiency or OK, let me tell you what in one sentence, what it is and and you'll see it next time.",
                    "label": 0
                },
                {
                    "sent": "Universal equivalence means that.",
                    "label": 0
                },
                {
                    "sent": "If I if two procedures are ranked the same by the F divergent, then there they have the same rank the same by the loss, so it's kind of the.",
                    "label": 0
                },
                {
                    "sent": "The Blackwell theorem that I mentioned earlier but extended to the whole class of divergences and loss functions.",
                    "label": 0
                },
                {
                    "sent": "But any given data for any given probability solutions universal in the sense of any probability distribution.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "So we'll be able to say if you're not universally equivalent to 01 loss, there must exist.",
                    "label": 0
                },
                {
                    "sent": "Some problem will get it.",
                    "label": 0
                },
                {
                    "sent": "You'll get a different answer than the 01 loss would give you.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's it's not a satisfactory loss function to be using 'cause you can get it wrong.",
                    "label": 0
                },
                {
                    "sent": "Sorry, no, no, this is not an apology theory.",
                    "label": 0
                },
                {
                    "sent": "Let me see if someone else has a question and I'll come back to you.",
                    "label": 0
                },
                {
                    "sent": "OK, go ahead.",
                    "label": 0
                },
                {
                    "sent": "I'm not familiar with F divergences, but in general, can you give us a sense what is so special about the mathematical properties of F divergent such that it works for F divergences but not other convexity?",
                    "label": 0
                },
                {
                    "sent": "That F function is a convex functional likely ratio.",
                    "label": 0
                },
                {
                    "sent": "That's the key.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Losses that.",
                    "label": 0
                },
                {
                    "sent": "Is there a best, most great question?",
                    "label": 0
                },
                {
                    "sent": "So no, this theory is silent on that.",
                    "label": 0
                },
                {
                    "sent": "It just identifies the class and then return it to you.",
                    "label": 0
                },
                {
                    "sent": "And I say now you choose among that class according to other principles, say complex in particular, or sparseness or some other principle that goes beyond decision theory.",
                    "label": 0
                },
                {
                    "sent": "But I think that's a good thing to do is I'm going to narrow down my class and say all these are good and now you can bring another principle to bear on choosing a moment.",
                    "label": 0
                },
                {
                    "sent": "So I don't want to tell you that you have to use that one.",
                    "label": 0
                },
                {
                    "sent": "That's got the frequently spirit.",
                    "label": 0
                },
                {
                    "sent": "It's rather to say that if you want to carve it down further, you better bring another principle.",
                    "label": 0
                },
                {
                    "sent": "Bear computational complexity.",
                    "label": 0
                },
                {
                    "sent": "Sparsity, some other principle.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Derivation, there was an unpractical assumption in the sense that you could optimize for each when you do your profiling to optimize over.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, that's not an assumption, but you're right, it's nonparametric in the sense that it's over all possible gamma.",
                    "label": 0
                },
                {
                    "sent": "That step wouldn't be true if it was restricted, its overall measurable functions gamma.",
                    "label": 0
                },
                {
                    "sent": "Which is which is.",
                    "label": 0
                },
                {
                    "sent": "That's right, so eventual theorem of consistency is going to have to be in a sieve of some kind where we're actually going up to a rich nonparametric class.",
                    "label": 0
                },
                {
                    "sent": "But that's the spirit of this kind of whole story is to do this for things like support groups supposed to converge for all possible generating distributions.",
                    "label": 0
                },
                {
                    "sent": "Alright, since the popcorn is stop popping now and we can, we're done.",
                    "label": 0
                }
            ]
        }
    }
}