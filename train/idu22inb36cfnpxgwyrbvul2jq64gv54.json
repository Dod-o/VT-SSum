{
    "id": "idu22inb36cfnpxgwyrbvul2jq64gv54",
    "title": "Degrees of Supervision",
    "info": {
        "author": [
            "Dario Garcia Garcia, Australian National University"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_garcia_garcia_supervision/",
    "segmentation": [
        [
            "Well, well, first of all I want to apologize because maybe some of you have been reading into their work so booklet, and we're expecting that by someone called that over karika.",
            "He hasn't been able to come here, so I will be doing the talking his behavior."
        ],
        [
            "Alright, so."
        ],
        [
            "Let's start one of the main motivation for this work and this talk in particular is that there are lots of different problems, which seems to have like an analogous output.",
            "The structure and seems to be fairly asking the kind of information that you provide to your learning algorithm.",
            "We can have supervised classification, then there is like methods which specialize in working with supervised classification, where their labels can be noisy.",
            "Then there is a huge little semi supervised learning.",
            "Then there are things like multiple instance learning or label proportions and things like quite exciting.",
            "No people come up with every year.",
            "So like my our main point is if we can view all this problem in a common way and if we can we can realize what is the common part and what is different.",
            "Carpeting all the things."
        ],
        [
            "And from quite an orthogonal direction, there is another motivation, which is that you know trying to define clustering is impossible.",
            "I know that, but I think this is possible in some particular applications to thinking of clustering as some kind of transactive unsupervised classification in the sense that we are given a sample.",
            "We are not given any labels of the samples, so we are free to post some statistical problem in that sample.",
            "So I guess one May think that we can ask for the.",
            "Easiest classification problem that can be solved in a given data set.",
            "So the main question is what is easy?",
            "For example, an example of this you can think of the.",
            "At maximum margin classroom, which seems like trying to find the easiest classification problem from the point of view of a support vector machine.",
            "And one of the key points of the talk is that in general, whenever we have some kind of ambiguity in the labels, we have some degree of freedom to choose our battle to choose what kind of problem do we really want to solve?"
        ],
        [
            "So for the standard supervised learning, we can think of the framework of statistical experiments where we have a set of hypothesis which manifest themselves as probability measure on some measurable sample space, or input the space, and then in the end we want to output at this eyesore, which takes us from that sample space to a class of hypothesis, just like a very broad description.",
            "In general, in machine learning we are in the example base kind of setting where we don't know the priority measures, but we are getting labeled samples.",
            "So usually what we call labels are like some kind of indices into our hypothesis space.",
            "So we are saying alright this sample and it has been generated by this hypothesis.",
            "So the examples that we see correspond directly with our kind of they kind of help that we want.",
            "Obviously this is kind of a strong assumption, so here we are going to see what happens if we consider more general kind of supervision."
        ],
        [
            "So that's what we call general supervision.",
            "The way we think about this right now is first we introduce this kind of compound experiment where we like kind of get a few hypothesis together and then we generate data according to that bunch of hypothesis.",
            "So from the point of statistical.",
            "Or a standard classification.",
            "We can think that we get together around a bunch of hypothesis.",
            "Then we generate one point from each one of them, and that's what we collect sample of the compound experiment.",
            "So obviously that induces some priority measure on this product space of the input space.",
            "What we do this, therefore why the main thing is that this will allow us to aggregate information up several samples into one observation.",
            "By aggregating I mean.",
            "Maybe we have quite some so quite a few samples and we just have one label for all of them, or one kind of supervision information for all of them.",
            "So we're kind of collapsing information down.",
            "And also we introduce two additional spaces apart from the input space and our hypothesis, which are the observation, the space and the supervision, the space.",
            "And we'll talk about them right now.",
            "And the main point is that our output is still going to be a function from the input space to their hypothesis, even if we are not going to directly be given samples of any of them.",
            "So we will be seeing things in different spaces."
        ],
        [
            "So the key point is that there are some stochastic mechanism which can be Markov kernels or conditional probabilities or whatever, which relates the hidden spaces which are like the input space and a hypothesis, and what we are actually going to see with things in this observation and supervision space, which are what we call the observation model, which is a Markov kernel from the input space observational space, so that any priority measure that you have on your input space will be transformed into a priority measure.",
            "On this observation space.",
            "And then similar thing for their hypothesis.",
            "So its hypothesis will map into a probability distribution over their supervision space.",
            "So regarding the supervision model there are like 2 extreme cases.",
            "We can think of the standard case where we actually get to see the hypothesis which generated given sample and that will be the case when our stochastic operator moving from hypothesis to supervision is actually some kind of Delta function.",
            "And on the other hand, we can think of.",
            "Legal point as some point for which the observation that we are given is a constant.",
            "So it's not informative it does.",
            "It is not telling us anything about the hypothesis which is actually generating the sample.",
            "And obviously there is a huge middle ground between those two cases.",
            "So for our purposes we will be given as an input to our learning system, a set of samples from the observation space and the supervision of space.",
            "So in the end it's just a generalization of the usual case where we are given points from the.",
            "Input the space on the hypothesis.",
            "The main thing here is that from a machine learning the vocabulary in test time we are going to be tested with points in the input.",
            "SpaceX butter training time.",
            "We are not seeing anything about that space in general.",
            "So like to make things clear, we can think of what we call the recovered experiment.",
            "So in this formula, I'm just assuming that we have conditional probabilities as our stochastic mechanism.",
            "So what we're actually going to be able to recover the samples is this conditional probability over here.",
            "But we are what we are interested on are the.",
            "The priority measures which defines the hypothesis in the input space.",
            "So how do I recover them?",
            "Well, I just can't use the chain rule for probabilities and marginalized out the things that I'm observing, which won't be necessary at this time.",
            "From the basic theory of comparison of experiments from which goes back to Blackwell, we know that every time that we do this kind of thing of using stochastic transformations, then we are losing information, or at least not creating new information out of the blue.",
            "So this means that our original experiment is going to dominate in the sense of Blackwell, our recovered experiment.",
            "This means that no matter what loss function we decide to use, we are always going to be performing at least as well on our original experiment.",
            "As on what we are able to recover, and in general who are going to be losing some statistical performance.",
            "Alright, but this is like a."
        ],
        [
            "General thing.",
            "And this introduces new challenges.",
            "As I said, like train and test points may live in different spaces.",
            "And most importantly, supervision can be ambiguous by this.",
            "Women that the task that we need to solve is not clear.",
            "I mean usually we are given a sample Anna hard set of labels from the samples.",
            "So the task projected on the sample is kind of unique, but here it's not the case because we have this supervision which is not totally informative about the hypothesis.",
            "So we have some uncertainty on the.",
            "That's that we want to solve.",
            "So then we need an inductive principle.",
            "And actually we need I should say we need another one, because obviously in every learning process we need some inductive principle.",
            "But the key thing here is that we need an inductive principle before solving our problem, just to know what our problem is.",
            "So we can go the way of lazy people like me and assume that the problem is as easy as possible.",
            "This is just an inductive principle as many others.",
            "So we will see that it has some nice properties.",
            "So then we need to make explicit the notion of.",
            "What is an easy problem or what is a hard problem into that end?",
            "We need a loss function because I mean in depending on the task that you want to solve, you want to consider different notions of easiness or hardness of a problem.",
            "And as a teacher, I will tell you that this will lead to what we call generalized minimum cross entropy funky notion?"
        ],
        [
            "So now we are going to particularly to particularize that very general description, and now we are in a probability estimation framework, so we will consider that our set of hypothesis is the K simplex, where K is the number of classes in our problem.",
            "Our observation the space is going to be the power set of the original space, which means that we can be given as individual observations, not just single points, but backs of points in general and our supervision.",
            "The space will be the power set of the K simplex.",
            "There is some aggregation going on because, for example, if you think that I am given a bag of five points.",
            "Then if I want to recover a full labeling of those points, in general I need some information about the five fold cross product of their privacy simplex, one for each point.",
            "But here we assume that there is some mechanism which collapse that information into the K simplex and that is why most of the problems that people try to solve right now and all this definition of Labor proportional all that stuff has some kind of this aggregation mechanism, which obviously is a lossy thing because.",
            "By collapsing information you are actually using statistical discrimination power.",
            "And the good thing about this is that we can have a clear interpretation about what our supervision is.",
            "We can think of it as a set of allowable states.",
            "So just to make this clear, in a fully supervised setting, we may think that our backs are have cardinality one.",
            "So in fact they are equivalent to individual points in the input space, and that our supervision are elements of the vertices of the simplex or stream points of the simplex.",
            "So in fact they are hard labels.",
            "Which is the standard case.",
            "In the label proposed in this scenario, we are given batch of points and ask their supervision for that bag of points.",
            "We are giving something like the average of the posterior probability of the points on that back and the good thing is that we can also think of unsupervised.",
            "So in some sense under this framework won't be unsupervised anymore.",
            "It will be like totally obviously supervised and that will means that for any point that I see I have a set of allowable states which will comprise the full.",
            "Wow.",
            "The whole simplex, so I don't have a clue about what is going on, so I will say alright, anything can go in this point."
        ],
        [
            "Then to motivate the the kind of point of view that we follow for to solve this problem, I want to consider this nice example about two different ways of learning a mixture model for classroom purposes.",
            "Well, yes, Mr Model in general.",
            "We have the standard maximum likelihood estimate, which obviously is maximizing the likelihood, or equivalently minimizing the KL divergent between the process which is generating the data and our estimate.",
            "And on the other hand, with her we have this thing called.",
            "Classification maximum likelihood, which is something which comes from the statistique literature from the 70s, and there are quite a few papers on it, but he will take a slight different point of view in broad sense classification.",
            "Maximum likelihood algorithm is optimizing jointly over the parameters of our probabilistic model and a set of fictitious labels.",
            "And then what we want to maximize is the conditional probability of our sample given that fictitious labels.",
            "So we can decompose that expression as follows.",
            "We have the same kind of maximum likelihood term that we had in the standard ML thing.",
            "That logarithm of 1 over Peter and the clear thing is that we have this.",
            "Part of the cost function will, which looks like something which we can find in a classification problem.",
            "Because it's a lot loss of the labels that we are making up an R probability under probability and the posterior probability estimate induced by are probably stick model.",
            "For example, one famous example of these classification matching likelihood is K means if you choose appropriately, the class of priority models that you are optimizing over this as topic Gaussians, then you can think of this.",
            "So its maximum likelihood is kind of a soft thing, while classification maximum likelihood is hard clustering mechanism.",
            "So the main difference is that for maximum likelihood there is no problem to have a solution with several components components which assigned.",
            "Like a high priority to the same region of the space, but from the point of view of classification maximum likelihood that will be bad.",
            "Why?",
            "Because if there are two components which give high priority to Orient in this space, the posterior probability will be around 1/2 in some sense or will be far from 011.",
            "So that will imply.",
            "A big risk from the point of view of the log loss, because the Bayes risk for that problem will be quite high.",
            "And we can think of this thing as instead of minimizing KL diversions, minimizing some notion of cross entropy.",
            "So the take home message is that in some sense, this classification maximum likelihood algorithms try to find the simplest classification problem according to the love loss.",
            "And with this regularization in some broad sense given by the likelihood of the data."
        ],
        [
            "So."
        ],
        [
            "So we get through this notion of generalized minimum cross entropy, but first it is good to know what cross entropies.",
            "So from an information theoretic perspective, we can think of cross entropy as how many bits in average we need to transmit samples generated by some distribution P using a code which is designed for another distribution.",
            "So there are two different things which are affecting the cross entropy.",
            "One is how hard is my original distribution, which is measured by the entropy and the other thing is how close are my real distribution and the distribution that I'm using to learn my code which is measured by the KL divergences between P&Q.",
            "So obviously this is really."
        ],
        [
            "Closely related to the lock losses Peter has explained before, which is I also associated to compression, but maybe we are not going to compress the data.",
            "Maybe we have another different statistical problem that we want to solve, so we are.",
            "We don't want to get stuck with a lot less.",
            "We want to have the flexibility of introducing our own notion of loss for a statistical problem, and this leads us to this.",
            "The following generalization.",
            "We have this expression for what we call the generalized concept, which is given by the sum of base risk, which is well known to be some kind of generalized.",
            "Entropy is what is usually called uncertainty measure, like something which is concave on the priority simplex.",
            "Ann usually goes to 0 on the extreme set.",
            "And we also have, like Bregman divergences between the in some sense, real distribution and our estimate.",
            "So we are yes, substituting the entropy, which is kind of the base rate for the log loss for a general based risk and the KL diversions for a break man diversions, but this may sound quite arbitrary, but."
        ],
        [
            "I think that it's not an.",
            "If we obtain the pointwise risk of such a loss, we get exactly what we call the generalized cross entropy.",
            "So in fact, maybe you know like recipes.",
            "Just a funny way of calling the standard loss, but it's good to think of it this way, like something which depends on the real problem and something which depends on our solution to the problem, because as I said earlier in this case of generalized supervision where we have some ambiguity on the labels and on the task that we want to solve.",
            "We actually need to consider the task.",
            "If that were not the case, if we had like really hard labels and we know exactly what problems we want to solve on the label points, then we just need to consider the Bregman divergent, so that will be a standard supervised classification setting."
        ],
        [
            "So we can think of some really general family of objective functions for solving this general supervision problems, as is usually the case, they are comprised of some regularization term, which would be nice to be data dependent so we can make nice connections with clustering and on the other hand an information functional which depends on our estimate and this labeled samples that we get during our learning process.",
            "And we define our information functional as you think, which is just.",
            "The minimum over the allowed said the state of states.",
            "Recall that this SY is the supervision that we get, for example, which, as I told you later, can be seen as a kind of set of allowed states for the temple.",
            "So from all those all over the states we choose the one which gives us a minimum cross entropy.",
            "Generalized cross entropy with respect to whatever property estimate we're doing on the sample.",
            "To make things clear, if we just go to the fully supervised setting."
        ],
        [
            "Then by applying the things that I told you earlier than the supervision will be an element of the simplex and buy some really simple properties of this generalized cross entropy that we saw in the paper.",
            "We have that this information term reduces to the regret of the learning process so.",
            "How much wars on the base risk are we performing by using this probability estimate, which makes sense and."
        ],
        [
            "If we go the other way and we go to a fully unsupervised learning clustering setup with what we go balance penalties like trying to?",
            "Do we have some idea about the prior probability over the sample and we want to enforce that during the classroom.",
            "Then we will have this term which for each point in our sample tries to find the minimum cross entropy.",
            "Plus that last term, which tries to enforce that balance penalty.",
            "In fact a balance penalty follows very nicely in this framework, because we can think of we are given a bag of points which actually comprises the whole sample, and then we are given a supervision value for the bag of points, which is exactly that balance that we are trying to enforce.",
            "And here we are optimizing over the extremal set of the simplex instead of just the simplex, because as we saw also in the paper, it's easy to see that minimizing this kind of cross entropy on their first argument over the whole simplex is going to give me a point which will be one hard label.",
            "So in fact it's like taking actual hard decisions based on our probability estimate."
        ],
        [
            "And finally, some things that we can do with this.",
            "We can think of taking some classification clustering algorithm like the maximum likelihood which realized yesterday, and those you know for smoothness and then try to add an information term to it.",
            "For example, spectral clustering is 1 example of algorithm which relies mostly on smoothness.",
            "Like once you do all the relaxation for former regional objective function and you get to this quadratic form involving the operation, where do you actually want to find in this product?",
            "Setup is like a really smooth function as measured by the operation, so it's like.",
            "A function which is smoothing their intrinsic geometry of the manifold.",
            "And which has to mean but.",
            "Here we also want that function to behave like a really nice classification problem, like one with a small base risk in some sense.",
            "So we introduced this penalty and it will force our solution to be close to an indicator function if we are in a binary clustering setting, we're pushing our function and we're asking it apart from being a smooth, it has to be really close to 0 or to one, so we don't want really any middle ground.",
            "It's kind of enforcing our hard clustering of data.",
            "Should go it is aligned with this idea of trying to find an easy classification problem because in an easy classification problem we don't want to pursue a varieties laying near the boundary.",
            "We are 1/2.",
            "We want priorities cluster around zero and one.",
            "So we have a really large margin and everything is really easy.",
            "And then if we do the same thing but instead of playing the particular shape of the information functional for supper by setting, we use the general shape of the function we get at this location regularize label proportions, which is just.",
            "The exact same thing, but with extra information.",
            "We are given information about label proportions within certain box over sample and we are using the operation as a regularization term.",
            "So I think that the good thing about this point of view is that apart from kind of getting a global view of what are all those kinds of supervision that people takes into account, how do we can write them in a compact way?",
            "It gives you an idea of how to create new algorithms, because you can always take a smoothness based clustering method and adds a new, added some information term to it, and go in play this game of finding easy classification problems on your.",
            "Example.",
            "Which may not be a good clustering procedure, but since nobody knows what the good crushing procedure is.",
            "So."
        ],
        [
            "That's more is what I wanted to say, so they take home messages are that we would like to provide a general view supervision encompassing many things which are being proposed in research, arranging for the continuum, from classification to clustering, including semi supervised learning but also this other more exotic kind of information that you can provide which can be summarized as label proportions.",
            "And then the main point is that when we have ability in our supervision, we are not given hard labels but set of labels or subsets of the simplex.",
            "In general we have that freedom to choose our own task so that forces us to have some inductive principle.",
            "To do that, an if your inductive principle is I want to solve the easiest problem that can be posed given this constraint then that will lead you to the minimal generalized cross entropy when you measured the hardness of your problem.",
            "By using the proper loss.",
            "So."
        ],
        [
            "That's all, and thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, well, first of all I want to apologize because maybe some of you have been reading into their work so booklet, and we're expecting that by someone called that over karika.",
                    "label": 0
                },
                {
                    "sent": "He hasn't been able to come here, so I will be doing the talking his behavior.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's start one of the main motivation for this work and this talk in particular is that there are lots of different problems, which seems to have like an analogous output.",
                    "label": 0
                },
                {
                    "sent": "The structure and seems to be fairly asking the kind of information that you provide to your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "We can have supervised classification, then there is like methods which specialize in working with supervised classification, where their labels can be noisy.",
                    "label": 1
                },
                {
                    "sent": "Then there is a huge little semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Then there are things like multiple instance learning or label proportions and things like quite exciting.",
                    "label": 1
                },
                {
                    "sent": "No people come up with every year.",
                    "label": 0
                },
                {
                    "sent": "So like my our main point is if we can view all this problem in a common way and if we can we can realize what is the common part and what is different.",
                    "label": 1
                },
                {
                    "sent": "Carpeting all the things.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And from quite an orthogonal direction, there is another motivation, which is that you know trying to define clustering is impossible.",
                    "label": 0
                },
                {
                    "sent": "I know that, but I think this is possible in some particular applications to thinking of clustering as some kind of transactive unsupervised classification in the sense that we are given a sample.",
                    "label": 0
                },
                {
                    "sent": "We are not given any labels of the samples, so we are free to post some statistical problem in that sample.",
                    "label": 0
                },
                {
                    "sent": "So I guess one May think that we can ask for the.",
                    "label": 0
                },
                {
                    "sent": "Easiest classification problem that can be solved in a given data set.",
                    "label": 1
                },
                {
                    "sent": "So the main question is what is easy?",
                    "label": 1
                },
                {
                    "sent": "For example, an example of this you can think of the.",
                    "label": 0
                },
                {
                    "sent": "At maximum margin classroom, which seems like trying to find the easiest classification problem from the point of view of a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "And one of the key points of the talk is that in general, whenever we have some kind of ambiguity in the labels, we have some degree of freedom to choose our battle to choose what kind of problem do we really want to solve?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the standard supervised learning, we can think of the framework of statistical experiments where we have a set of hypothesis which manifest themselves as probability measure on some measurable sample space, or input the space, and then in the end we want to output at this eyesore, which takes us from that sample space to a class of hypothesis, just like a very broad description.",
                    "label": 0
                },
                {
                    "sent": "In general, in machine learning we are in the example base kind of setting where we don't know the priority measures, but we are getting labeled samples.",
                    "label": 0
                },
                {
                    "sent": "So usually what we call labels are like some kind of indices into our hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "So we are saying alright this sample and it has been generated by this hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So the examples that we see correspond directly with our kind of they kind of help that we want.",
                    "label": 1
                },
                {
                    "sent": "Obviously this is kind of a strong assumption, so here we are going to see what happens if we consider more general kind of supervision.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's what we call general supervision.",
                    "label": 0
                },
                {
                    "sent": "The way we think about this right now is first we introduce this kind of compound experiment where we like kind of get a few hypothesis together and then we generate data according to that bunch of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So from the point of statistical.",
                    "label": 0
                },
                {
                    "sent": "Or a standard classification.",
                    "label": 0
                },
                {
                    "sent": "We can think that we get together around a bunch of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Then we generate one point from each one of them, and that's what we collect sample of the compound experiment.",
                    "label": 1
                },
                {
                    "sent": "So obviously that induces some priority measure on this product space of the input space.",
                    "label": 0
                },
                {
                    "sent": "What we do this, therefore why the main thing is that this will allow us to aggregate information up several samples into one observation.",
                    "label": 0
                },
                {
                    "sent": "By aggregating I mean.",
                    "label": 0
                },
                {
                    "sent": "Maybe we have quite some so quite a few samples and we just have one label for all of them, or one kind of supervision information for all of them.",
                    "label": 0
                },
                {
                    "sent": "So we're kind of collapsing information down.",
                    "label": 0
                },
                {
                    "sent": "And also we introduce two additional spaces apart from the input space and our hypothesis, which are the observation, the space and the supervision, the space.",
                    "label": 1
                },
                {
                    "sent": "And we'll talk about them right now.",
                    "label": 1
                },
                {
                    "sent": "And the main point is that our output is still going to be a function from the input space to their hypothesis, even if we are not going to directly be given samples of any of them.",
                    "label": 0
                },
                {
                    "sent": "So we will be seeing things in different spaces.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key point is that there are some stochastic mechanism which can be Markov kernels or conditional probabilities or whatever, which relates the hidden spaces which are like the input space and a hypothesis, and what we are actually going to see with things in this observation and supervision space, which are what we call the observation model, which is a Markov kernel from the input space observational space, so that any priority measure that you have on your input space will be transformed into a priority measure.",
                    "label": 1
                },
                {
                    "sent": "On this observation space.",
                    "label": 0
                },
                {
                    "sent": "And then similar thing for their hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So its hypothesis will map into a probability distribution over their supervision space.",
                    "label": 0
                },
                {
                    "sent": "So regarding the supervision model there are like 2 extreme cases.",
                    "label": 0
                },
                {
                    "sent": "We can think of the standard case where we actually get to see the hypothesis which generated given sample and that will be the case when our stochastic operator moving from hypothesis to supervision is actually some kind of Delta function.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, we can think of.",
                    "label": 0
                },
                {
                    "sent": "Legal point as some point for which the observation that we are given is a constant.",
                    "label": 0
                },
                {
                    "sent": "So it's not informative it does.",
                    "label": 0
                },
                {
                    "sent": "It is not telling us anything about the hypothesis which is actually generating the sample.",
                    "label": 0
                },
                {
                    "sent": "And obviously there is a huge middle ground between those two cases.",
                    "label": 0
                },
                {
                    "sent": "So for our purposes we will be given as an input to our learning system, a set of samples from the observation space and the supervision of space.",
                    "label": 0
                },
                {
                    "sent": "So in the end it's just a generalization of the usual case where we are given points from the.",
                    "label": 0
                },
                {
                    "sent": "Input the space on the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The main thing here is that from a machine learning the vocabulary in test time we are going to be tested with points in the input.",
                    "label": 0
                },
                {
                    "sent": "SpaceX butter training time.",
                    "label": 0
                },
                {
                    "sent": "We are not seeing anything about that space in general.",
                    "label": 0
                },
                {
                    "sent": "So like to make things clear, we can think of what we call the recovered experiment.",
                    "label": 0
                },
                {
                    "sent": "So in this formula, I'm just assuming that we have conditional probabilities as our stochastic mechanism.",
                    "label": 0
                },
                {
                    "sent": "So what we're actually going to be able to recover the samples is this conditional probability over here.",
                    "label": 0
                },
                {
                    "sent": "But we are what we are interested on are the.",
                    "label": 0
                },
                {
                    "sent": "The priority measures which defines the hypothesis in the input space.",
                    "label": 0
                },
                {
                    "sent": "So how do I recover them?",
                    "label": 0
                },
                {
                    "sent": "Well, I just can't use the chain rule for probabilities and marginalized out the things that I'm observing, which won't be necessary at this time.",
                    "label": 0
                },
                {
                    "sent": "From the basic theory of comparison of experiments from which goes back to Blackwell, we know that every time that we do this kind of thing of using stochastic transformations, then we are losing information, or at least not creating new information out of the blue.",
                    "label": 0
                },
                {
                    "sent": "So this means that our original experiment is going to dominate in the sense of Blackwell, our recovered experiment.",
                    "label": 0
                },
                {
                    "sent": "This means that no matter what loss function we decide to use, we are always going to be performing at least as well on our original experiment.",
                    "label": 0
                },
                {
                    "sent": "As on what we are able to recover, and in general who are going to be losing some statistical performance.",
                    "label": 0
                },
                {
                    "sent": "Alright, but this is like a.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "General thing.",
                    "label": 0
                },
                {
                    "sent": "And this introduces new challenges.",
                    "label": 0
                },
                {
                    "sent": "As I said, like train and test points may live in different spaces.",
                    "label": 1
                },
                {
                    "sent": "And most importantly, supervision can be ambiguous by this.",
                    "label": 0
                },
                {
                    "sent": "Women that the task that we need to solve is not clear.",
                    "label": 1
                },
                {
                    "sent": "I mean usually we are given a sample Anna hard set of labels from the samples.",
                    "label": 0
                },
                {
                    "sent": "So the task projected on the sample is kind of unique, but here it's not the case because we have this supervision which is not totally informative about the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So we have some uncertainty on the.",
                    "label": 1
                },
                {
                    "sent": "That's that we want to solve.",
                    "label": 0
                },
                {
                    "sent": "So then we need an inductive principle.",
                    "label": 0
                },
                {
                    "sent": "And actually we need I should say we need another one, because obviously in every learning process we need some inductive principle.",
                    "label": 1
                },
                {
                    "sent": "But the key thing here is that we need an inductive principle before solving our problem, just to know what our problem is.",
                    "label": 0
                },
                {
                    "sent": "So we can go the way of lazy people like me and assume that the problem is as easy as possible.",
                    "label": 0
                },
                {
                    "sent": "This is just an inductive principle as many others.",
                    "label": 0
                },
                {
                    "sent": "So we will see that it has some nice properties.",
                    "label": 0
                },
                {
                    "sent": "So then we need to make explicit the notion of.",
                    "label": 0
                },
                {
                    "sent": "What is an easy problem or what is a hard problem into that end?",
                    "label": 0
                },
                {
                    "sent": "We need a loss function because I mean in depending on the task that you want to solve, you want to consider different notions of easiness or hardness of a problem.",
                    "label": 0
                },
                {
                    "sent": "And as a teacher, I will tell you that this will lead to what we call generalized minimum cross entropy funky notion?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we are going to particularly to particularize that very general description, and now we are in a probability estimation framework, so we will consider that our set of hypothesis is the K simplex, where K is the number of classes in our problem.",
                    "label": 0
                },
                {
                    "sent": "Our observation the space is going to be the power set of the original space, which means that we can be given as individual observations, not just single points, but backs of points in general and our supervision.",
                    "label": 0
                },
                {
                    "sent": "The space will be the power set of the K simplex.",
                    "label": 0
                },
                {
                    "sent": "There is some aggregation going on because, for example, if you think that I am given a bag of five points.",
                    "label": 0
                },
                {
                    "sent": "Then if I want to recover a full labeling of those points, in general I need some information about the five fold cross product of their privacy simplex, one for each point.",
                    "label": 0
                },
                {
                    "sent": "But here we assume that there is some mechanism which collapse that information into the K simplex and that is why most of the problems that people try to solve right now and all this definition of Labor proportional all that stuff has some kind of this aggregation mechanism, which obviously is a lossy thing because.",
                    "label": 0
                },
                {
                    "sent": "By collapsing information you are actually using statistical discrimination power.",
                    "label": 0
                },
                {
                    "sent": "And the good thing about this is that we can have a clear interpretation about what our supervision is.",
                    "label": 0
                },
                {
                    "sent": "We can think of it as a set of allowable states.",
                    "label": 1
                },
                {
                    "sent": "So just to make this clear, in a fully supervised setting, we may think that our backs are have cardinality one.",
                    "label": 0
                },
                {
                    "sent": "So in fact they are equivalent to individual points in the input space, and that our supervision are elements of the vertices of the simplex or stream points of the simplex.",
                    "label": 0
                },
                {
                    "sent": "So in fact they are hard labels.",
                    "label": 0
                },
                {
                    "sent": "Which is the standard case.",
                    "label": 0
                },
                {
                    "sent": "In the label proposed in this scenario, we are given batch of points and ask their supervision for that bag of points.",
                    "label": 0
                },
                {
                    "sent": "We are giving something like the average of the posterior probability of the points on that back and the good thing is that we can also think of unsupervised.",
                    "label": 0
                },
                {
                    "sent": "So in some sense under this framework won't be unsupervised anymore.",
                    "label": 0
                },
                {
                    "sent": "It will be like totally obviously supervised and that will means that for any point that I see I have a set of allowable states which will comprise the full.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "The whole simplex, so I don't have a clue about what is going on, so I will say alright, anything can go in this point.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then to motivate the the kind of point of view that we follow for to solve this problem, I want to consider this nice example about two different ways of learning a mixture model for classroom purposes.",
                    "label": 1
                },
                {
                    "sent": "Well, yes, Mr Model in general.",
                    "label": 0
                },
                {
                    "sent": "We have the standard maximum likelihood estimate, which obviously is maximizing the likelihood, or equivalently minimizing the KL divergent between the process which is generating the data and our estimate.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, with her we have this thing called.",
                    "label": 0
                },
                {
                    "sent": "Classification maximum likelihood, which is something which comes from the statistique literature from the 70s, and there are quite a few papers on it, but he will take a slight different point of view in broad sense classification.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood algorithm is optimizing jointly over the parameters of our probabilistic model and a set of fictitious labels.",
                    "label": 0
                },
                {
                    "sent": "And then what we want to maximize is the conditional probability of our sample given that fictitious labels.",
                    "label": 0
                },
                {
                    "sent": "So we can decompose that expression as follows.",
                    "label": 0
                },
                {
                    "sent": "We have the same kind of maximum likelihood term that we had in the standard ML thing.",
                    "label": 0
                },
                {
                    "sent": "That logarithm of 1 over Peter and the clear thing is that we have this.",
                    "label": 0
                },
                {
                    "sent": "Part of the cost function will, which looks like something which we can find in a classification problem.",
                    "label": 0
                },
                {
                    "sent": "Because it's a lot loss of the labels that we are making up an R probability under probability and the posterior probability estimate induced by are probably stick model.",
                    "label": 0
                },
                {
                    "sent": "For example, one famous example of these classification matching likelihood is K means if you choose appropriately, the class of priority models that you are optimizing over this as topic Gaussians, then you can think of this.",
                    "label": 0
                },
                {
                    "sent": "So its maximum likelihood is kind of a soft thing, while classification maximum likelihood is hard clustering mechanism.",
                    "label": 0
                },
                {
                    "sent": "So the main difference is that for maximum likelihood there is no problem to have a solution with several components components which assigned.",
                    "label": 0
                },
                {
                    "sent": "Like a high priority to the same region of the space, but from the point of view of classification maximum likelihood that will be bad.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because if there are two components which give high priority to Orient in this space, the posterior probability will be around 1/2 in some sense or will be far from 011.",
                    "label": 0
                },
                {
                    "sent": "So that will imply.",
                    "label": 0
                },
                {
                    "sent": "A big risk from the point of view of the log loss, because the Bayes risk for that problem will be quite high.",
                    "label": 0
                },
                {
                    "sent": "And we can think of this thing as instead of minimizing KL diversions, minimizing some notion of cross entropy.",
                    "label": 1
                },
                {
                    "sent": "So the take home message is that in some sense, this classification maximum likelihood algorithms try to find the simplest classification problem according to the love loss.",
                    "label": 0
                },
                {
                    "sent": "And with this regularization in some broad sense given by the likelihood of the data.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we get through this notion of generalized minimum cross entropy, but first it is good to know what cross entropies.",
                    "label": 0
                },
                {
                    "sent": "So from an information theoretic perspective, we can think of cross entropy as how many bits in average we need to transmit samples generated by some distribution P using a code which is designed for another distribution.",
                    "label": 1
                },
                {
                    "sent": "So there are two different things which are affecting the cross entropy.",
                    "label": 0
                },
                {
                    "sent": "One is how hard is my original distribution, which is measured by the entropy and the other thing is how close are my real distribution and the distribution that I'm using to learn my code which is measured by the KL divergences between P&Q.",
                    "label": 0
                },
                {
                    "sent": "So obviously this is really.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Closely related to the lock losses Peter has explained before, which is I also associated to compression, but maybe we are not going to compress the data.",
                    "label": 1
                },
                {
                    "sent": "Maybe we have another different statistical problem that we want to solve, so we are.",
                    "label": 0
                },
                {
                    "sent": "We don't want to get stuck with a lot less.",
                    "label": 0
                },
                {
                    "sent": "We want to have the flexibility of introducing our own notion of loss for a statistical problem, and this leads us to this.",
                    "label": 0
                },
                {
                    "sent": "The following generalization.",
                    "label": 0
                },
                {
                    "sent": "We have this expression for what we call the generalized concept, which is given by the sum of base risk, which is well known to be some kind of generalized.",
                    "label": 0
                },
                {
                    "sent": "Entropy is what is usually called uncertainty measure, like something which is concave on the priority simplex.",
                    "label": 0
                },
                {
                    "sent": "Ann usually goes to 0 on the extreme set.",
                    "label": 0
                },
                {
                    "sent": "And we also have, like Bregman divergences between the in some sense, real distribution and our estimate.",
                    "label": 0
                },
                {
                    "sent": "So we are yes, substituting the entropy, which is kind of the base rate for the log loss for a general based risk and the KL diversions for a break man diversions, but this may sound quite arbitrary, but.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that it's not an.",
                    "label": 0
                },
                {
                    "sent": "If we obtain the pointwise risk of such a loss, we get exactly what we call the generalized cross entropy.",
                    "label": 0
                },
                {
                    "sent": "So in fact, maybe you know like recipes.",
                    "label": 0
                },
                {
                    "sent": "Just a funny way of calling the standard loss, but it's good to think of it this way, like something which depends on the real problem and something which depends on our solution to the problem, because as I said earlier in this case of generalized supervision where we have some ambiguity on the labels and on the task that we want to solve.",
                    "label": 0
                },
                {
                    "sent": "We actually need to consider the task.",
                    "label": 0
                },
                {
                    "sent": "If that were not the case, if we had like really hard labels and we know exactly what problems we want to solve on the label points, then we just need to consider the Bregman divergent, so that will be a standard supervised classification setting.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can think of some really general family of objective functions for solving this general supervision problems, as is usually the case, they are comprised of some regularization term, which would be nice to be data dependent so we can make nice connections with clustering and on the other hand an information functional which depends on our estimate and this labeled samples that we get during our learning process.",
                    "label": 0
                },
                {
                    "sent": "And we define our information functional as you think, which is just.",
                    "label": 0
                },
                {
                    "sent": "The minimum over the allowed said the state of states.",
                    "label": 0
                },
                {
                    "sent": "Recall that this SY is the supervision that we get, for example, which, as I told you later, can be seen as a kind of set of allowed states for the temple.",
                    "label": 0
                },
                {
                    "sent": "So from all those all over the states we choose the one which gives us a minimum cross entropy.",
                    "label": 0
                },
                {
                    "sent": "Generalized cross entropy with respect to whatever property estimate we're doing on the sample.",
                    "label": 0
                },
                {
                    "sent": "To make things clear, if we just go to the fully supervised setting.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then by applying the things that I told you earlier than the supervision will be an element of the simplex and buy some really simple properties of this generalized cross entropy that we saw in the paper.",
                    "label": 0
                },
                {
                    "sent": "We have that this information term reduces to the regret of the learning process so.",
                    "label": 0
                },
                {
                    "sent": "How much wars on the base risk are we performing by using this probability estimate, which makes sense and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we go the other way and we go to a fully unsupervised learning clustering setup with what we go balance penalties like trying to?",
                    "label": 0
                },
                {
                    "sent": "Do we have some idea about the prior probability over the sample and we want to enforce that during the classroom.",
                    "label": 0
                },
                {
                    "sent": "Then we will have this term which for each point in our sample tries to find the minimum cross entropy.",
                    "label": 0
                },
                {
                    "sent": "Plus that last term, which tries to enforce that balance penalty.",
                    "label": 0
                },
                {
                    "sent": "In fact a balance penalty follows very nicely in this framework, because we can think of we are given a bag of points which actually comprises the whole sample, and then we are given a supervision value for the bag of points, which is exactly that balance that we are trying to enforce.",
                    "label": 0
                },
                {
                    "sent": "And here we are optimizing over the extremal set of the simplex instead of just the simplex, because as we saw also in the paper, it's easy to see that minimizing this kind of cross entropy on their first argument over the whole simplex is going to give me a point which will be one hard label.",
                    "label": 0
                },
                {
                    "sent": "So in fact it's like taking actual hard decisions based on our probability estimate.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, some things that we can do with this.",
                    "label": 0
                },
                {
                    "sent": "We can think of taking some classification clustering algorithm like the maximum likelihood which realized yesterday, and those you know for smoothness and then try to add an information term to it.",
                    "label": 1
                },
                {
                    "sent": "For example, spectral clustering is 1 example of algorithm which relies mostly on smoothness.",
                    "label": 0
                },
                {
                    "sent": "Like once you do all the relaxation for former regional objective function and you get to this quadratic form involving the operation, where do you actually want to find in this product?",
                    "label": 0
                },
                {
                    "sent": "Setup is like a really smooth function as measured by the operation, so it's like.",
                    "label": 0
                },
                {
                    "sent": "A function which is smoothing their intrinsic geometry of the manifold.",
                    "label": 0
                },
                {
                    "sent": "And which has to mean but.",
                    "label": 0
                },
                {
                    "sent": "Here we also want that function to behave like a really nice classification problem, like one with a small base risk in some sense.",
                    "label": 0
                },
                {
                    "sent": "So we introduced this penalty and it will force our solution to be close to an indicator function if we are in a binary clustering setting, we're pushing our function and we're asking it apart from being a smooth, it has to be really close to 0 or to one, so we don't want really any middle ground.",
                    "label": 0
                },
                {
                    "sent": "It's kind of enforcing our hard clustering of data.",
                    "label": 0
                },
                {
                    "sent": "Should go it is aligned with this idea of trying to find an easy classification problem because in an easy classification problem we don't want to pursue a varieties laying near the boundary.",
                    "label": 0
                },
                {
                    "sent": "We are 1/2.",
                    "label": 0
                },
                {
                    "sent": "We want priorities cluster around zero and one.",
                    "label": 0
                },
                {
                    "sent": "So we have a really large margin and everything is really easy.",
                    "label": 1
                },
                {
                    "sent": "And then if we do the same thing but instead of playing the particular shape of the information functional for supper by setting, we use the general shape of the function we get at this location regularize label proportions, which is just.",
                    "label": 0
                },
                {
                    "sent": "The exact same thing, but with extra information.",
                    "label": 1
                },
                {
                    "sent": "We are given information about label proportions within certain box over sample and we are using the operation as a regularization term.",
                    "label": 0
                },
                {
                    "sent": "So I think that the good thing about this point of view is that apart from kind of getting a global view of what are all those kinds of supervision that people takes into account, how do we can write them in a compact way?",
                    "label": 0
                },
                {
                    "sent": "It gives you an idea of how to create new algorithms, because you can always take a smoothness based clustering method and adds a new, added some information term to it, and go in play this game of finding easy classification problems on your.",
                    "label": 0
                },
                {
                    "sent": "Example.",
                    "label": 0
                },
                {
                    "sent": "Which may not be a good clustering procedure, but since nobody knows what the good crushing procedure is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's more is what I wanted to say, so they take home messages are that we would like to provide a general view supervision encompassing many things which are being proposed in research, arranging for the continuum, from classification to clustering, including semi supervised learning but also this other more exotic kind of information that you can provide which can be summarized as label proportions.",
                    "label": 0
                },
                {
                    "sent": "And then the main point is that when we have ability in our supervision, we are not given hard labels but set of labels or subsets of the simplex.",
                    "label": 0
                },
                {
                    "sent": "In general we have that freedom to choose our own task so that forces us to have some inductive principle.",
                    "label": 1
                },
                {
                    "sent": "To do that, an if your inductive principle is I want to solve the easiest problem that can be posed given this constraint then that will lead you to the minimal generalized cross entropy when you measured the hardness of your problem.",
                    "label": 1
                },
                {
                    "sent": "By using the proper loss.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all, and thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}