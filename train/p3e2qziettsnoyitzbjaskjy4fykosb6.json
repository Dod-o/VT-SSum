{
    "id": "p3e2qziettsnoyitzbjaskjy4fykosb6",
    "title": "DUST: A Generalized Notion of Similarity between Uncertain Time Series",
    "info": {
        "author": [
            "Smruti Ranjan Sarangi, IBM India Research Lab"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd2010_sarangi_dust/",
    "segmentation": [
        [
            "I'm presenting the paper today dust generalized notion of similarity between."
        ],
        [
            "Uncertain time series.",
            "So what motivated us to do this work is as follows.",
            "IBM had the Smarter Planet Initiative in which IBM proposes to instrument things like the power grid or the water grid with possibly thousands or 10s of thousands of sensors.",
            "So one thing that we observed in the course of our work is that.",
            "These centers are physical devices since they are reading a real signal and that also with a lot of variance in temperature and humidity.",
            "Certain amount of error is introduced into the data.",
            "And if you want to do any data mining, analytics or business decisions using this data.",
            "I will have to have a means of dealing with uncertain data effectively.",
            "And we also observed that in a couple of privacy preserving techniques, some of the speakers just mentioned a certain amount of uncertainties added into the data to anonymize the data.",
            "So."
        ],
        [
            "A brief outline of the talk is as follows.",
            "I'll begin by motivating the problem and then give the properties of a generalized distance measure.",
            "And then I'll talk about the specific distance measure that we're proposing.",
            "It's called the dust."
        ],
        [
            "Distance.",
            "So the first slide here is atwaters.",
            "Uncertain data typically look like.",
            "So we have a observed value X, and that corresponds to the real value RX, and there is some amount of error with it.",
            "So the error we don't know what the error is.",
            "A priori, we just know that the error comes with a certain error distribution as shown on the right.",
            "The same thing we also see on the bottom that for any uncertain time series, just defining what time series is basically a set of time value pairs.",
            "So we see that we have the original signal and then we have some error over here.",
            "And the error is highly random in nature and what we observed is that a lot of error functions can be modeled as a normal distribution, but a lot of errors are not normal in character also.",
            "So we evaluate for both the cases."
        ],
        [
            "So typically when we want to do data mining on uncertain time series, I'm going to do clustering or classification or motive detection.",
            "Any kind of pattern recognition on uncertain data.",
            "Or we would need some way of comparing two different data values or two different time series.",
            "So we would at least require partial order between the elements, but to make it more generic, most often we need a total order.",
            "So let's say we're trying to answer the question are X&X dash closer than Y&Y Dash?",
            "Then we would need the notion of ordering the distance between X&X Dash.",
            "How does it compare to the distance between Y&Y Dash?",
            "However, a so so.",
            "But The thing is that in most cases, especially when dealing with complicated data like uncertain data.",
            "It is difficult to come with a good nice, well rounded distance function that can give us a good total order, but the basic idea here is that if we do have a total order, it makes a life significantly easier because suddenly all pairs of distances become incomparable.",
            "So given the.",
            "Starting with this initial motivation that we need a distance function between uncertain data and between uncertain time series.",
            "We stay."
        ],
        [
            "After looking at a set of some examples.",
            "So on there are three figures here with three different time series, T1T2 and T3.",
            "Other specific question that we're trying to answer here is that is T2 closer to T1 or ST3 closer to T1, right?",
            "So we see that in this case, on the top left it doesn't matter because both of them are very close.",
            "And I mean either answer right, it doesn't look qualitatively different than the other one.",
            "In the second case, clearly T3 is closer to T1.",
            "You can see that each and every value is closer.",
            "However, the tricky case is the third one.",
            "Uh, the reason for that is that the two has a lot of small perturbations.",
            "T3 has almost no perturbations with the last values significantly perturb.",
            "So how do you say which one is closer and what is the theoretical justification for which one is closer?",
            "So to answer this question, two slides later, after we go through."
        ],
        [
            "Some basic definitions.",
            "So let's consider two values here, X&X Dash.",
            "So we have a basic axiom that we use, and this has also been used in the prior work.",
            "That whatever distance you come up with between X&X Dash here in this figure, we should tell us something about the distance between the real values RX and RX Dash.",
            "The only problem is we don't know what the real values are, but nonetheless the distance that we get should be representative of the actual distance.",
            "So what people have done in prior approaches is that they have computed the probability distribution a priori probability distribution of the distances between the real values are X -- R X dash.",
            "And they have a preferred to work with.",
            "The statistical properties of this specific distribution, which is the mean and variance in this case.",
            "But some kind of a distribution is still not a distance measure in the sense what we desire is 1 chrispen concrete number given X&X dash.",
            "So at least now we are in a."
        ],
        [
            "Position that we can answer the question of which time series is closer.",
            "So I have some amount of text here, but let me quickly explain.",
            "So it is possible that in this case T1 and T2 are actually the same time series and T2 just has a higher error.",
            "However, we see that even in T3 can never be the same time series, because the last value is significantly perturb, so we all know that in.",
            "So let's say the error has a normal distribution in the last value is beyond three Sigma, right?",
            "It's very unlikely that they're the same time series, but however there is a finite probability that T1 and T2 are the same time series, and since we are always reasoning about the distance between real values, we would say that in this case T1 and T2 are closer and T1 and T3R.",
            "Not that close.",
            "So now we started to use conventional distance metrics like Euclidian distance or dynamic time warping to see if we can get the right answer.",
            "But unfortunately here we get the wrong answer.",
            "They say that T3 is closer, so that's the reason we worked on a new distance measure does."
        ],
        [
            "That says that the two is closer, so now I'll give a slightly more theoretical justification with some."
        ],
        [
            "Algebra.",
            "So let's first look at the properties of a distance measure and then we'll see that how it can be extended for uncertain data.",
            "So 124 are pretty obvious nonnegativity identity, symmetric and triangle inequality, yes, please."
        ],
        [
            "Yeah, so D3 is far away, right?",
            "And so that's the reason that we said 82 is closer.",
            "Yesterday Euclidian and dynamic time warping would say that the three is closer, the reason being that in this case the sum of the squares of distances is almost zero, and it would add up to this.",
            "So here I mean it's just a question of the math here, right?",
            "So."
        ],
        [
            "Uh.",
            "So so basically the 1st four conditions are relatively obvious, so they're there for other distance measures.",
            "The last condition is something that we add and we say that when the separation is much larger than the standard deviation of the error, any kind of uncertain distance measure should approach a deterministic distance measure.",
            "So in this case does should be approaching Euclidean or dynamic time warping.",
            "And if the magnitude of the error is small, so we'll see that this is the case with dust."
        ],
        [
            "So what we initially did is that we took a look at prior work and we tried to extend the result.",
            "Try to generalize the result.",
            "So what blood work was saying is that two time series are considered similar if the distance between time series Steven and T2.",
            "If this is less than epsilon, the entire probability of this happening is greater than Tau.",
            "And they also said that the distance between two time series here.",
            "So let me just quickly mention the Convention capital Dist.",
            "Upper Case is a distance between two time series and small and lower case disk is a distance between two individual values.",
            "So what we want prior work also assumes is that this follows a regular Euclidean distance like thing that distance between two time series is the square root of the sum of squares of individual distances.",
            "So I have some amount of algebra here in the next 2 slides with slightly cryptic said gloss over mostly gloss over that in the interest of time.",
            "And So what we do is primarily an assumption.",
            "So if we see here that if epsilon is small enough.",
            "We can write it in this fashion where we create this to zero and we multiply the.",
            "Probability density with epsilon.",
            "So if this if epsilon is small enough other limit.",
            "This is exactly correct.",
            "So what we do is we make a slightly broader and approximate assumption that irrespective of the size of epsilon, we assume this to be correct.",
            "So then now what we have is some simple algebra which I show in this slide."
        ],
        [
            "So let's consider 3 * 3 is your T1T2 and T3.",
            "Let's say that T1 and T2 are closer and T1 and T3 and all that close.",
            "So we see that the probability of them being closed is is higher than the other.",
            "So we just simplify it and then what we do is that we have also had the assumption of independence that different time series values are independent.",
            "This is also taken in prior work.",
            "Soon then all we do is simple manipulation.",
            "We there at this point we take the log on both sides.",
            "And what we claim here and the details are given in the paper.",
            "We see that we say that the negative of the logarithm of the probability right if we take the square root of this.",
            "It is the small dust distance between two elements, so I'll not go into the details and this looks pretty cryptic anyway, so I'll go to the next slide and we'll see.",
            "That's why we have such kind of slightly circuitous definition for the distance."
        ],
        [
            "So we start with the same result once again and we stop where we start, where we stopped in the previous slide.",
            "So by taking the logarithm on both on the both sides, we arrive at this specific expression.",
            "So, since we're defined the dust distance as the square root of this quantity over here, we see that this expression over here is almost similar to what you would expect in the traditional conventional distance measure, which basically the sum of squares is less than equal to the sum of the squares here.",
            "And the capital does distance is less than this.",
            "So let me just explain where two time series here T1T2 and T3 we are seeing here the T1 and T2 are closer than T1 and T3, and water algebra and our assumptions are given us.",
            "Is that the capital does distance between the two time series T1 and T2 is less than the dust distance between T1 and T3, which is sort of similar to this probabilistic argument on top.",
            "And it's just that it's a different way of representing.",
            "And what does that essentially given us is that it is given us a total ordering of the distances.",
            "And it also it's cognizant of the.",
            "Off the error function, the kind of error that is being introduced in the data is also behaving more or less like a standard distance measure.",
            "In the paper we also show we also prove that it obeys all the five properties of a distance, measure the dimension.",
            "The triangle inequality is violated."
        ],
        [
            "At some specific points, but not in general.",
            "Now a brief one minute introduction of the dust distance."
        ],
        [
            "So what are shown on the previous slide was some theoretical algebra, so it's possible to numerically compute it.",
            "We create an offline table and store it.",
            "So, uh, what we observed is that the dust distance similar to Euclidean distance if he, let's say, compute the sticks and why it's mostly dependent on the modulus of X -- Y and we show it is exactly true for most common distributions in the paper.",
            "So what I want to show in this slide is that using Bayes theorem and using numerical integration is possible to compute a look up table of those values.",
            "We can further compress it using a piecewise linear representation.",
            "And and we have appealing kind of representation the.",
            "Look up complexity is typically all organ because we would do a binary search to find the right segment.",
            "What we found is that we can slightly optimize this process because if most of the values are in this segment over here, right?",
            "So this is what is what should converge with Euclidean distance.",
            "Then we can source check if it if values lie in the last segment.",
            "If that's the case, we can directly calculate the value.",
            "Otherwise we do a binary search to find out in this area right which segment it belongs to.",
            "So the complexity is somewhere between oh one and oh log, and depending on the."
        ],
        [
            "Distribution of data.",
            "So, uh, here is just before going to the results I need to mention one thing.",
            "Initially, what we're trying to do is we're trying to take a standard data set, perturb every value with a normally distributed error an irrespective of the amount, irrespective of how bad we try it, right?",
            "We couldn't get any improvement whatsoever with the dust distance, so that's when we plotted this figure.",
            "So what we found is that for normally distributed error for all the elements in the time series.",
            "The dust distance is exactly the same as standard Euclidean distance.",
            "That's the reason we didn't see any benefit at all.",
            "But if we see other error distributions then at least in this region over here.",
            "We see that further log normal or exponential distribution things are different and the dust distance looks different from Euclidean.",
            "But in either case we see that when the magnitude of the separation between two points is much larger than the error does, distance does converge with the standard Euclidean.",
            "So we see it becomes a straight line over here, which is the 5th property that we had mentioned."
        ],
        [
            "I'll just quickly go over this slide in interest of time.",
            "Let's say that we have a time series where different sensor values have different error distributions.",
            "Part of it is normally distributed part uniform.",
            "So then what we do is that we propose to change the error functions beyond the point that we're interested in.",
            "So let's say there's a normal distribution.",
            "So between plus three Sigma and minus three Sigma, we're interested in the distribution.",
            "Beyond that most likely will not see any perturbations.",
            "So what we do is that we this red region over here we superimpose a hypothetical distribution.",
            "What this allows us to do is that it helps us combine different kinds of distributions and.",
            "So in this case."
        ],
        [
            "We are trying to combine three different normal distributions with three different standard deviations.",
            "So we see that the initial part they have a different day.",
            "They have a different behavior, but gradually all of them converge to the same Euclidean distance an it, and they become the same once the separation of the points is much larger than the error."
        ],
        [
            "Quickly go to the results over here."
        ],
        [
            "We took the standard UCR datasets.",
            "And we try to see that.",
            "Once the part of the data sets, what is the classification accuracy?",
            "So without any error, we typically got 77% and the moment we introduced error with Euclidian distance we got 62%.",
            "But with dust we could get 72.",
            "This basically means that we could recover more than half of the losses that were loss due to introducing uncertainty using the dust metric.",
            "We have similar results for dynamic time warping."
        ],
        [
            "There basically this is 78 with dust 74 and Euclidean distance with dynamic time warping is 67%.",
            "So we also did the two more experiments."
        ],
        [
            "One of this was finding motives.",
            "So under standard e.g data set which is also part of the UCR suite, we try to see that by introducing any kind of uncertain T, what percentage of motives are remaining right?",
            "So what we found is that when there's no uncertainty, of course, both the metrics are the same.",
            "And as we increase the standard division of the uncertainty.",
            "Of what happens is that the Euclidean distance traditional metric linearly decreases in quality, whereas the dust distance is much more resilient and things are much better till a much larger standard deviation.",
            "We still see some anomalous behavior over here and so reviews also pointed this out and we're in the process of finding why this is the case.",
            "And we haven't found that up till now and this is simple."
        ],
        [
            "This last slide of the results, which shows that four again and classification on the way for data set of UCR.",
            "We see this basically shows the number of nearest neighbors that are preserved with adding uncertainty.",
            "So we see that this in the case of Euclidean distance, things degrade very fast, whereas in the case of dust the degradation is much slower.",
            "It's able to hold its ground for a much longer time.",
            "Trying coming."
        ],
        [
            "The conclusions we observe that.",
            "Uncertainty in data is becoming increasingly more prevalent and especially with more and more sensor data and privacy preserving techniques, we are seeing uncertain data.",
            "More and more conventional approaches are not that good in the sense that we can.",
            "We found we can at least significantly improve upon conventional approaches by using a using our technique, which we call dust, which is distance for uncertain time series.",
            "And we see experimentally that it makes up for more than half of the accuracy that is lost.",
            "Due to uncertainty, thank you and I would love to take a."
        ],
        [
            "Questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm presenting the paper today dust generalized notion of similarity between.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uncertain time series.",
                    "label": 0
                },
                {
                    "sent": "So what motivated us to do this work is as follows.",
                    "label": 0
                },
                {
                    "sent": "IBM had the Smarter Planet Initiative in which IBM proposes to instrument things like the power grid or the water grid with possibly thousands or 10s of thousands of sensors.",
                    "label": 0
                },
                {
                    "sent": "So one thing that we observed in the course of our work is that.",
                    "label": 0
                },
                {
                    "sent": "These centers are physical devices since they are reading a real signal and that also with a lot of variance in temperature and humidity.",
                    "label": 0
                },
                {
                    "sent": "Certain amount of error is introduced into the data.",
                    "label": 0
                },
                {
                    "sent": "And if you want to do any data mining, analytics or business decisions using this data.",
                    "label": 1
                },
                {
                    "sent": "I will have to have a means of dealing with uncertain data effectively.",
                    "label": 0
                },
                {
                    "sent": "And we also observed that in a couple of privacy preserving techniques, some of the speakers just mentioned a certain amount of uncertainties added into the data to anonymize the data.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A brief outline of the talk is as follows.",
                    "label": 0
                },
                {
                    "sent": "I'll begin by motivating the problem and then give the properties of a generalized distance measure.",
                    "label": 1
                },
                {
                    "sent": "And then I'll talk about the specific distance measure that we're proposing.",
                    "label": 0
                },
                {
                    "sent": "It's called the dust.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distance.",
                    "label": 0
                },
                {
                    "sent": "So the first slide here is atwaters.",
                    "label": 0
                },
                {
                    "sent": "Uncertain data typically look like.",
                    "label": 1
                },
                {
                    "sent": "So we have a observed value X, and that corresponds to the real value RX, and there is some amount of error with it.",
                    "label": 0
                },
                {
                    "sent": "So the error we don't know what the error is.",
                    "label": 0
                },
                {
                    "sent": "A priori, we just know that the error comes with a certain error distribution as shown on the right.",
                    "label": 0
                },
                {
                    "sent": "The same thing we also see on the bottom that for any uncertain time series, just defining what time series is basically a set of time value pairs.",
                    "label": 0
                },
                {
                    "sent": "So we see that we have the original signal and then we have some error over here.",
                    "label": 0
                },
                {
                    "sent": "And the error is highly random in nature and what we observed is that a lot of error functions can be modeled as a normal distribution, but a lot of errors are not normal in character also.",
                    "label": 0
                },
                {
                    "sent": "So we evaluate for both the cases.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So typically when we want to do data mining on uncertain time series, I'm going to do clustering or classification or motive detection.",
                    "label": 1
                },
                {
                    "sent": "Any kind of pattern recognition on uncertain data.",
                    "label": 0
                },
                {
                    "sent": "Or we would need some way of comparing two different data values or two different time series.",
                    "label": 0
                },
                {
                    "sent": "So we would at least require partial order between the elements, but to make it more generic, most often we need a total order.",
                    "label": 1
                },
                {
                    "sent": "So let's say we're trying to answer the question are X&X dash closer than Y&Y Dash?",
                    "label": 1
                },
                {
                    "sent": "Then we would need the notion of ordering the distance between X&X Dash.",
                    "label": 0
                },
                {
                    "sent": "How does it compare to the distance between Y&Y Dash?",
                    "label": 0
                },
                {
                    "sent": "However, a so so.",
                    "label": 0
                },
                {
                    "sent": "But The thing is that in most cases, especially when dealing with complicated data like uncertain data.",
                    "label": 0
                },
                {
                    "sent": "It is difficult to come with a good nice, well rounded distance function that can give us a good total order, but the basic idea here is that if we do have a total order, it makes a life significantly easier because suddenly all pairs of distances become incomparable.",
                    "label": 0
                },
                {
                    "sent": "So given the.",
                    "label": 1
                },
                {
                    "sent": "Starting with this initial motivation that we need a distance function between uncertain data and between uncertain time series.",
                    "label": 0
                },
                {
                    "sent": "We stay.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After looking at a set of some examples.",
                    "label": 0
                },
                {
                    "sent": "So on there are three figures here with three different time series, T1T2 and T3.",
                    "label": 0
                },
                {
                    "sent": "Other specific question that we're trying to answer here is that is T2 closer to T1 or ST3 closer to T1, right?",
                    "label": 1
                },
                {
                    "sent": "So we see that in this case, on the top left it doesn't matter because both of them are very close.",
                    "label": 0
                },
                {
                    "sent": "And I mean either answer right, it doesn't look qualitatively different than the other one.",
                    "label": 0
                },
                {
                    "sent": "In the second case, clearly T3 is closer to T1.",
                    "label": 0
                },
                {
                    "sent": "You can see that each and every value is closer.",
                    "label": 0
                },
                {
                    "sent": "However, the tricky case is the third one.",
                    "label": 0
                },
                {
                    "sent": "Uh, the reason for that is that the two has a lot of small perturbations.",
                    "label": 0
                },
                {
                    "sent": "T3 has almost no perturbations with the last values significantly perturb.",
                    "label": 0
                },
                {
                    "sent": "So how do you say which one is closer and what is the theoretical justification for which one is closer?",
                    "label": 0
                },
                {
                    "sent": "So to answer this question, two slides later, after we go through.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some basic definitions.",
                    "label": 0
                },
                {
                    "sent": "So let's consider two values here, X&X Dash.",
                    "label": 1
                },
                {
                    "sent": "So we have a basic axiom that we use, and this has also been used in the prior work.",
                    "label": 0
                },
                {
                    "sent": "That whatever distance you come up with between X&X Dash here in this figure, we should tell us something about the distance between the real values RX and RX Dash.",
                    "label": 1
                },
                {
                    "sent": "The only problem is we don't know what the real values are, but nonetheless the distance that we get should be representative of the actual distance.",
                    "label": 1
                },
                {
                    "sent": "So what people have done in prior approaches is that they have computed the probability distribution a priori probability distribution of the distances between the real values are X -- R X dash.",
                    "label": 0
                },
                {
                    "sent": "And they have a preferred to work with.",
                    "label": 0
                },
                {
                    "sent": "The statistical properties of this specific distribution, which is the mean and variance in this case.",
                    "label": 0
                },
                {
                    "sent": "But some kind of a distribution is still not a distance measure in the sense what we desire is 1 chrispen concrete number given X&X dash.",
                    "label": 0
                },
                {
                    "sent": "So at least now we are in a.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Position that we can answer the question of which time series is closer.",
                    "label": 0
                },
                {
                    "sent": "So I have some amount of text here, but let me quickly explain.",
                    "label": 0
                },
                {
                    "sent": "So it is possible that in this case T1 and T2 are actually the same time series and T2 just has a higher error.",
                    "label": 1
                },
                {
                    "sent": "However, we see that even in T3 can never be the same time series, because the last value is significantly perturb, so we all know that in.",
                    "label": 1
                },
                {
                    "sent": "So let's say the error has a normal distribution in the last value is beyond three Sigma, right?",
                    "label": 0
                },
                {
                    "sent": "It's very unlikely that they're the same time series, but however there is a finite probability that T1 and T2 are the same time series, and since we are always reasoning about the distance between real values, we would say that in this case T1 and T2 are closer and T1 and T3R.",
                    "label": 0
                },
                {
                    "sent": "Not that close.",
                    "label": 0
                },
                {
                    "sent": "So now we started to use conventional distance metrics like Euclidian distance or dynamic time warping to see if we can get the right answer.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately here we get the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "They say that T3 is closer, so that's the reason we worked on a new distance measure does.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That says that the two is closer, so now I'll give a slightly more theoretical justification with some.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algebra.",
                    "label": 0
                },
                {
                    "sent": "So let's first look at the properties of a distance measure and then we'll see that how it can be extended for uncertain data.",
                    "label": 1
                },
                {
                    "sent": "So 124 are pretty obvious nonnegativity identity, symmetric and triangle inequality, yes, please.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so D3 is far away, right?",
                    "label": 0
                },
                {
                    "sent": "And so that's the reason that we said 82 is closer.",
                    "label": 0
                },
                {
                    "sent": "Yesterday Euclidian and dynamic time warping would say that the three is closer, the reason being that in this case the sum of the squares of distances is almost zero, and it would add up to this.",
                    "label": 1
                },
                {
                    "sent": "So here I mean it's just a question of the math here, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So so basically the 1st four conditions are relatively obvious, so they're there for other distance measures.",
                    "label": 0
                },
                {
                    "sent": "The last condition is something that we add and we say that when the separation is much larger than the standard deviation of the error, any kind of uncertain distance measure should approach a deterministic distance measure.",
                    "label": 0
                },
                {
                    "sent": "So in this case does should be approaching Euclidean or dynamic time warping.",
                    "label": 0
                },
                {
                    "sent": "And if the magnitude of the error is small, so we'll see that this is the case with dust.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we initially did is that we took a look at prior work and we tried to extend the result.",
                    "label": 0
                },
                {
                    "sent": "Try to generalize the result.",
                    "label": 0
                },
                {
                    "sent": "So what blood work was saying is that two time series are considered similar if the distance between time series Steven and T2.",
                    "label": 1
                },
                {
                    "sent": "If this is less than epsilon, the entire probability of this happening is greater than Tau.",
                    "label": 0
                },
                {
                    "sent": "And they also said that the distance between two time series here.",
                    "label": 0
                },
                {
                    "sent": "So let me just quickly mention the Convention capital Dist.",
                    "label": 0
                },
                {
                    "sent": "Upper Case is a distance between two time series and small and lower case disk is a distance between two individual values.",
                    "label": 0
                },
                {
                    "sent": "So what we want prior work also assumes is that this follows a regular Euclidean distance like thing that distance between two time series is the square root of the sum of squares of individual distances.",
                    "label": 0
                },
                {
                    "sent": "So I have some amount of algebra here in the next 2 slides with slightly cryptic said gloss over mostly gloss over that in the interest of time.",
                    "label": 0
                },
                {
                    "sent": "And So what we do is primarily an assumption.",
                    "label": 0
                },
                {
                    "sent": "So if we see here that if epsilon is small enough.",
                    "label": 0
                },
                {
                    "sent": "We can write it in this fashion where we create this to zero and we multiply the.",
                    "label": 0
                },
                {
                    "sent": "Probability density with epsilon.",
                    "label": 0
                },
                {
                    "sent": "So if this if epsilon is small enough other limit.",
                    "label": 0
                },
                {
                    "sent": "This is exactly correct.",
                    "label": 1
                },
                {
                    "sent": "So what we do is we make a slightly broader and approximate assumption that irrespective of the size of epsilon, we assume this to be correct.",
                    "label": 0
                },
                {
                    "sent": "So then now what we have is some simple algebra which I show in this slide.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's consider 3 * 3 is your T1T2 and T3.",
                    "label": 0
                },
                {
                    "sent": "Let's say that T1 and T2 are closer and T1 and T3 and all that close.",
                    "label": 0
                },
                {
                    "sent": "So we see that the probability of them being closed is is higher than the other.",
                    "label": 0
                },
                {
                    "sent": "So we just simplify it and then what we do is that we have also had the assumption of independence that different time series values are independent.",
                    "label": 0
                },
                {
                    "sent": "This is also taken in prior work.",
                    "label": 0
                },
                {
                    "sent": "Soon then all we do is simple manipulation.",
                    "label": 0
                },
                {
                    "sent": "We there at this point we take the log on both sides.",
                    "label": 0
                },
                {
                    "sent": "And what we claim here and the details are given in the paper.",
                    "label": 1
                },
                {
                    "sent": "We see that we say that the negative of the logarithm of the probability right if we take the square root of this.",
                    "label": 0
                },
                {
                    "sent": "It is the small dust distance between two elements, so I'll not go into the details and this looks pretty cryptic anyway, so I'll go to the next slide and we'll see.",
                    "label": 0
                },
                {
                    "sent": "That's why we have such kind of slightly circuitous definition for the distance.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we start with the same result once again and we stop where we start, where we stopped in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So by taking the logarithm on both on the both sides, we arrive at this specific expression.",
                    "label": 0
                },
                {
                    "sent": "So, since we're defined the dust distance as the square root of this quantity over here, we see that this expression over here is almost similar to what you would expect in the traditional conventional distance measure, which basically the sum of squares is less than equal to the sum of the squares here.",
                    "label": 0
                },
                {
                    "sent": "And the capital does distance is less than this.",
                    "label": 0
                },
                {
                    "sent": "So let me just explain where two time series here T1T2 and T3 we are seeing here the T1 and T2 are closer than T1 and T3, and water algebra and our assumptions are given us.",
                    "label": 0
                },
                {
                    "sent": "Is that the capital does distance between the two time series T1 and T2 is less than the dust distance between T1 and T3, which is sort of similar to this probabilistic argument on top.",
                    "label": 0
                },
                {
                    "sent": "And it's just that it's a different way of representing.",
                    "label": 0
                },
                {
                    "sent": "And what does that essentially given us is that it is given us a total ordering of the distances.",
                    "label": 0
                },
                {
                    "sent": "And it also it's cognizant of the.",
                    "label": 0
                },
                {
                    "sent": "Off the error function, the kind of error that is being introduced in the data is also behaving more or less like a standard distance measure.",
                    "label": 1
                },
                {
                    "sent": "In the paper we also show we also prove that it obeys all the five properties of a distance, measure the dimension.",
                    "label": 0
                },
                {
                    "sent": "The triangle inequality is violated.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At some specific points, but not in general.",
                    "label": 0
                },
                {
                    "sent": "Now a brief one minute introduction of the dust distance.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are shown on the previous slide was some theoretical algebra, so it's possible to numerically compute it.",
                    "label": 0
                },
                {
                    "sent": "We create an offline table and store it.",
                    "label": 0
                },
                {
                    "sent": "So, uh, what we observed is that the dust distance similar to Euclidean distance if he, let's say, compute the sticks and why it's mostly dependent on the modulus of X -- Y and we show it is exactly true for most common distributions in the paper.",
                    "label": 0
                },
                {
                    "sent": "So what I want to show in this slide is that using Bayes theorem and using numerical integration is possible to compute a look up table of those values.",
                    "label": 0
                },
                {
                    "sent": "We can further compress it using a piecewise linear representation.",
                    "label": 1
                },
                {
                    "sent": "And and we have appealing kind of representation the.",
                    "label": 0
                },
                {
                    "sent": "Look up complexity is typically all organ because we would do a binary search to find the right segment.",
                    "label": 1
                },
                {
                    "sent": "What we found is that we can slightly optimize this process because if most of the values are in this segment over here, right?",
                    "label": 1
                },
                {
                    "sent": "So this is what is what should converge with Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "Then we can source check if it if values lie in the last segment.",
                    "label": 0
                },
                {
                    "sent": "If that's the case, we can directly calculate the value.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we do a binary search to find out in this area right which segment it belongs to.",
                    "label": 0
                },
                {
                    "sent": "So the complexity is somewhere between oh one and oh log, and depending on the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution of data.",
                    "label": 0
                },
                {
                    "sent": "So, uh, here is just before going to the results I need to mention one thing.",
                    "label": 0
                },
                {
                    "sent": "Initially, what we're trying to do is we're trying to take a standard data set, perturb every value with a normally distributed error an irrespective of the amount, irrespective of how bad we try it, right?",
                    "label": 0
                },
                {
                    "sent": "We couldn't get any improvement whatsoever with the dust distance, so that's when we plotted this figure.",
                    "label": 0
                },
                {
                    "sent": "So what we found is that for normally distributed error for all the elements in the time series.",
                    "label": 0
                },
                {
                    "sent": "The dust distance is exactly the same as standard Euclidean distance.",
                    "label": 1
                },
                {
                    "sent": "That's the reason we didn't see any benefit at all.",
                    "label": 0
                },
                {
                    "sent": "But if we see other error distributions then at least in this region over here.",
                    "label": 0
                },
                {
                    "sent": "We see that further log normal or exponential distribution things are different and the dust distance looks different from Euclidean.",
                    "label": 0
                },
                {
                    "sent": "But in either case we see that when the magnitude of the separation between two points is much larger than the error does, distance does converge with the standard Euclidean.",
                    "label": 0
                },
                {
                    "sent": "So we see it becomes a straight line over here, which is the 5th property that we had mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll just quickly go over this slide in interest of time.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we have a time series where different sensor values have different error distributions.",
                    "label": 1
                },
                {
                    "sent": "Part of it is normally distributed part uniform.",
                    "label": 0
                },
                {
                    "sent": "So then what we do is that we propose to change the error functions beyond the point that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So let's say there's a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So between plus three Sigma and minus three Sigma, we're interested in the distribution.",
                    "label": 0
                },
                {
                    "sent": "Beyond that most likely will not see any perturbations.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that we this red region over here we superimpose a hypothetical distribution.",
                    "label": 0
                },
                {
                    "sent": "What this allows us to do is that it helps us combine different kinds of distributions and.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are trying to combine three different normal distributions with three different standard deviations.",
                    "label": 1
                },
                {
                    "sent": "So we see that the initial part they have a different day.",
                    "label": 0
                },
                {
                    "sent": "They have a different behavior, but gradually all of them converge to the same Euclidean distance an it, and they become the same once the separation of the points is much larger than the error.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quickly go to the results over here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We took the standard UCR datasets.",
                    "label": 0
                },
                {
                    "sent": "And we try to see that.",
                    "label": 0
                },
                {
                    "sent": "Once the part of the data sets, what is the classification accuracy?",
                    "label": 1
                },
                {
                    "sent": "So without any error, we typically got 77% and the moment we introduced error with Euclidian distance we got 62%.",
                    "label": 0
                },
                {
                    "sent": "But with dust we could get 72.",
                    "label": 0
                },
                {
                    "sent": "This basically means that we could recover more than half of the losses that were loss due to introducing uncertainty using the dust metric.",
                    "label": 0
                },
                {
                    "sent": "We have similar results for dynamic time warping.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There basically this is 78 with dust 74 and Euclidean distance with dynamic time warping is 67%.",
                    "label": 0
                },
                {
                    "sent": "So we also did the two more experiments.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of this was finding motives.",
                    "label": 0
                },
                {
                    "sent": "So under standard e.g data set which is also part of the UCR suite, we try to see that by introducing any kind of uncertain T, what percentage of motives are remaining right?",
                    "label": 0
                },
                {
                    "sent": "So what we found is that when there's no uncertainty, of course, both the metrics are the same.",
                    "label": 0
                },
                {
                    "sent": "And as we increase the standard division of the uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Of what happens is that the Euclidean distance traditional metric linearly decreases in quality, whereas the dust distance is much more resilient and things are much better till a much larger standard deviation.",
                    "label": 0
                },
                {
                    "sent": "We still see some anomalous behavior over here and so reviews also pointed this out and we're in the process of finding why this is the case.",
                    "label": 1
                },
                {
                    "sent": "And we haven't found that up till now and this is simple.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This last slide of the results, which shows that four again and classification on the way for data set of UCR.",
                    "label": 0
                },
                {
                    "sent": "We see this basically shows the number of nearest neighbors that are preserved with adding uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So we see that this in the case of Euclidean distance, things degrade very fast, whereas in the case of dust the degradation is much slower.",
                    "label": 0
                },
                {
                    "sent": "It's able to hold its ground for a much longer time.",
                    "label": 0
                },
                {
                    "sent": "Trying coming.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The conclusions we observe that.",
                    "label": 0
                },
                {
                    "sent": "Uncertainty in data is becoming increasingly more prevalent and especially with more and more sensor data and privacy preserving techniques, we are seeing uncertain data.",
                    "label": 1
                },
                {
                    "sent": "More and more conventional approaches are not that good in the sense that we can.",
                    "label": 0
                },
                {
                    "sent": "We found we can at least significantly improve upon conventional approaches by using a using our technique, which we call dust, which is distance for uncertain time series.",
                    "label": 1
                },
                {
                    "sent": "And we see experimentally that it makes up for more than half of the accuracy that is lost.",
                    "label": 0
                },
                {
                    "sent": "Due to uncertainty, thank you and I would love to take a.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                }
            ]
        }
    }
}