{
    "id": "svagwpwnscyvnnwkugorhyvyetkidgat",
    "title": "Compact Coding for Hyperplane Classifiers in Heterogeneous Environment",
    "info": {
        "author": [
            "Hao Shao, Department of Informatics, Kyushu University"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_shao_compact/",
    "segmentation": [
        [
            "Good morning everyone.",
            "My name is Hershel from Kyushu, Niversity, Japan, and today I will present my work titled Compact Coding for Hyperplane Classifiers in Heterogeneous Environment and the Co authors are being tone and my supervisor a machine so kissing Suzuki.",
            "And."
        ],
        [
            "At first I want to introduce the problem setting and the problem we are dealing with.",
            "We are dealing with the inductive transfer problems with multiple source tasks.",
            "So as input we have multiple source datasets from S1 to SK and we have one target that T. So each instance has identical nominal attributes.",
            "That and each instance has a class level either be zero or one.",
            "So our objective is to build a hyper hyperplane classifier on the target task.",
            "So take this for an example we want to deal with the binary classification problems for heart disease diagonals.",
            "So, given an you typed heart disease problems with only twenty level level examples, so it is not very easy for us to build a very good classifier on this.",
            "On this limited labeled information."
        ],
        [
            "So.",
            "We could borrow the stress from the existing datasets with very good results, and this is called transfer learning in which we want to transfer the useful information from the source domain to target domain to help to classify the.",
            "Target data set."
        ],
        [
            "However.",
            "There are several problems in the transfer learning setting, and there are two main problems.",
            "The first one is that our source data set may be dissimilar with the target data set due to the different distributions underlying the two domains and if we want to directly transfer the information from the source to the target domain, we may not get the satisfactory results, which is also known as a negative transfer problems.",
            "And also the second problem is that not all the components in in a similar source task is helpful, so.",
            "In this kind of setting we want to transfer the useful information from the source to the target domain."
        ],
        [
            "Number for each of the source data set.",
            "We have the positive parts and negative parts and we I mark here as the pupil.",
            "The negative parts and we want to avoid transferring the negative information from the source to the target domain.",
            "So it's.",
            "Therefore, it's very essential for us to measure the similarities between the two domains and also measure the similarities between the different parts with the target data set."
        ],
        [
            "There are some existing methods dealing with this inductive transfer learning problems.",
            "However, some of them has problems such as some of them considered only one source task.",
            "And most methods only consider one kind of similarities, which is either the similarity between tasks.",
            "All the similarity between the instances.",
            "Also, the sum methods are here."
        ],
        [
            "Stick so.",
            "In our algorithm, we want to design an algorithm based on a solid theoretical foundation which can deal with multiple source tasks.",
            "And we consider not only the similarity between the datasets and we also consider similarity of the different parts within one datasets to the target data set."
        ],
        [
            "For our problem setting we have a hyperplane classifiers from the induced from each source data set and we also could induce.",
            "The hyperplane classifiers from the labeled data set in the target data set it is denoted by the EVT.",
            "So instead of measuring the similarities directly between the two datasets, we use our compact coding method to measure the code length of the two hyperplanes based on the two datasets."
        ],
        [
            "So here is a very simple example.",
            "Suppose we have 8 labeled examples in the target data set, with the hyperplane induced from it as we T so W1 and W2's hyperplanes of the two source datasets S1 and S2.",
            "So as you can see from the figure, both the two hyperplanes has one wrong predictions, 1 prediction of eight examples of the target data set.",
            "So it is not easy for us to tell which one is more similar to the target data set."
        ],
        [
            "That's why we want to build a negative algorithm to measure the similarities.",
            "Our method is inspired by the minimum description, description, length, principle, and in this principle the best hypothesis could be induced from the data sets is 1 which could minimize the code length of the hypothesis and the code length of the data using the hypothesis.",
            "For example.",
            "We are given a data set D and the hypothesis family HHIHII is the integers and the best hypothesis at best could be induced from the data set is the one to minimize the first part is the code length of the data using the hypothesis and the second part is occurrence of the hypothesis itself.",
            "So we can say that the minimum description length principle could balance the simplicity of the hypothesis and the goodness feed of the fit to the data set.",
            "So in this way we could avoid overfitting.",
            "Sorry."
        ],
        [
            "So here is a framework of our method compact coding for hyperplane classifiers and we call it CHC.",
            "It contains 2 level evaluations.",
            "The first level is the macro level evaluation in which we source the source data sets in the descending order on the degrees of the similarity with the target data set and the similarity is denoted by our code, our code length.",
            "The second level is a micro level evaluation, which we divide each data set of the related source tasks into several components, and we select related parts to help training the classifier in the target domain, and this similarity measure is also based on our on our coding method."
        ],
        [
            "So here is a code length as a similarity measure.",
            "An posterior probability of a hyperplane WI given the source data set, as I could be presented by the probability of the weight given the data set."
        ],
        [
            "We want to measure the similarity between the hyperplane to the target data set.",
            "So we substitute the source data set as I with the target data set."
        ],
        [
            "T and decompose it by the Bear theorem.",
            "So."
        ],
        [
            "As you can see, it's not very easy for us to calculate the second distributions probability distributions.",
            "So as as I mentioned before, we already have the label information from the target data set and we can have a hyperplane Vt from the labeled information, so we could borrow, borrow the strength to help to code the hyperplane in the source data set.",
            "So we have this formula."
        ],
        [
            "And decomposing also by the Bear theorem.",
            "And for.",
            "At last we get two probability Dist."
        ],
        [
            "Revolutions and we could write our code lens as the two parts.",
            "The first part is to code the target data set with the hyperplane and the second part is to code the hyperplane with the help of another hyperplane."
        ],
        [
            "Before introducing our detailed coding method, I want to introduce the preliminaries of coding.",
            "The first one is to the code length of a binary string of length a, which consists of B binary ones.",
            "So this could be denoted by siedah function and in the first part we send the length A and the second part we specify the positions of the ones in the string.",
            "And also we want to coding we want to code a real number X on the assumption that X equals to mill is most likely where mu is also a real number.",
            "An F is a continuous probability with precision.",
            "So this is denoted by the Lambda function as this.",
            "1."
        ],
        [
            "And here is the coding method of our CHC, the first part of the code.",
            "Length to code hyperplane code are vector based on an another vector of the hyperplane is denoted by the Lambda function and the second part of the code length could be denoted by specifying the wrong predictions of the.",
            "The wrong predictions of the target data set.",
            "So the colons could be written as these two parts."
        ],
        [
            "Now we could solve the problems I mentioned before to calculate the code length of the two hyperplanes and to tell which one is more similar to the target data set.",
            "So after calculation we got the.",
            "The code length of the second data set is more similar to the target data set."
        ],
        [
            "And here is a pseudocode of our method.",
            "The first 3 lines are the macro level, which we thought the source data set in the ascending order of the colon switches equipped, which is equal to the descending order of the similarities and for the micro level we divide the source data set into several components and send the related parts from the source domain to the target domain.",
            "And we calculate the code length as proposed.",
            "Then terminate if we cannot find any shorter colons, output hyper plan that are."
        ],
        [
            "Inocente we evaluate our method on two kinds of datasets.",
            "The first one is the UCI datasets and we choose three of them from the UCI data set, and we also adopt A preprocessing method from this paper to speak split each data set into the source and target datasets.",
            "For text datasets, we choose the 20 newsgroups data sets.",
            "As with the preprocessing method get by this paper to form different tasks with sub categories.",
            "And for comparison, we have several state of the art methods such as SVM, tribest and so on."
        ],
        [
            "This is the results on the UCI datasets, which is the mushroom data set.",
            "As you could see, we evaluate two factors.",
            "One factor is the number of instances in target assets.",
            "We set each as 50 or 100 and the second factor we evaluate is percentage of noise, and we test our method under the noise level from zero percent to 15%.",
            "As you could see, the results of the mushroom datasets one the number of instances in the target data set is not so money.",
            "So many is better than the.",
            "The labeled information is abundant.",
            "And our method mostly outperforms other methods.",
            "And is able to achieve lower rate with only a few label information."
        ],
        [
            "Double.",
            "So for the two other datasets from UCI datasets do to limit page limit.",
            "I only present the results in one slide as you could see our method is most mostly outperforms other methods.",
            "The."
        ],
        [
            "For the text data sets, the first is to use the rec via stalk as a target data set.",
            "And therefore the two factors we evaluate.",
            "We evaluate different number of instances in the target data set and different noise conditions from zero to 15%.",
            "Our method is the best one among our methods and."
        ],
        [
            "For other two datasets in 20 newsgroup datasets, we also present the results in one slide."
        ],
        [
            "And we also evaluate transferred components in the text data set in the micro level.",
            "As you could see in this table, S1 and S2S3 denote the different source tasks from the source domain and the integers 0 means that there is no part transferred from source to target domain and one means there is one part transferred.",
            "And from this table we could say that not all the information from the similar source tasks are transferred from the source to the target domain and our micro level evaluation is effective, which can adapt adaptively.",
            "Select related parts for transferring."
        ],
        [
            "So for the summary of this work, we want designer coding method for hyperplane classifiers in the transfer learning setting and our method could adaptively select related parts in the source task in classifying the target task.",
            "We proposed a compact coding method inspired by MD RP to measure the similarity between the data by the code length.",
            "Experiments conducted on the both at UCI and text data set showed effectiveness of our method."
        ],
        [
            "Thank you very much for listening.",
            "OK, thank you very much.",
            "So we have time for one question.",
            "So in your talk you mentioned several times that your method tries to exploit similarity between different datasets.",
            "But if I interpret your model well, I would rather say your method is exploiting a similarity between the different models.",
            "Yes, for the different datasets, so at least to me this is.",
            "These are two slightly different things.",
            "If you have similar models, I would say this is a specific type of similarity in.",
            "The data set.",
            "It's just a small comment.",
            "Thank you very much, yeah.",
            "OK, so the number of questions that let's think this picture once more and."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Hershel from Kyushu, Niversity, Japan, and today I will present my work titled Compact Coding for Hyperplane Classifiers in Heterogeneous Environment and the Co authors are being tone and my supervisor a machine so kissing Suzuki.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At first I want to introduce the problem setting and the problem we are dealing with.",
                    "label": 0
                },
                {
                    "sent": "We are dealing with the inductive transfer problems with multiple source tasks.",
                    "label": 1
                },
                {
                    "sent": "So as input we have multiple source datasets from S1 to SK and we have one target that T. So each instance has identical nominal attributes.",
                    "label": 1
                },
                {
                    "sent": "That and each instance has a class level either be zero or one.",
                    "label": 1
                },
                {
                    "sent": "So our objective is to build a hyper hyperplane classifier on the target task.",
                    "label": 0
                },
                {
                    "sent": "So take this for an example we want to deal with the binary classification problems for heart disease diagonals.",
                    "label": 1
                },
                {
                    "sent": "So, given an you typed heart disease problems with only twenty level level examples, so it is not very easy for us to build a very good classifier on this.",
                    "label": 0
                },
                {
                    "sent": "On this limited labeled information.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We could borrow the stress from the existing datasets with very good results, and this is called transfer learning in which we want to transfer the useful information from the source domain to target domain to help to classify the.",
                    "label": 1
                },
                {
                    "sent": "Target data set.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "There are several problems in the transfer learning setting, and there are two main problems.",
                    "label": 0
                },
                {
                    "sent": "The first one is that our source data set may be dissimilar with the target data set due to the different distributions underlying the two domains and if we want to directly transfer the information from the source to the target domain, we may not get the satisfactory results, which is also known as a negative transfer problems.",
                    "label": 1
                },
                {
                    "sent": "And also the second problem is that not all the components in in a similar source task is helpful, so.",
                    "label": 0
                },
                {
                    "sent": "In this kind of setting we want to transfer the useful information from the source to the target domain.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Number for each of the source data set.",
                    "label": 1
                },
                {
                    "sent": "We have the positive parts and negative parts and we I mark here as the pupil.",
                    "label": 0
                },
                {
                    "sent": "The negative parts and we want to avoid transferring the negative information from the source to the target domain.",
                    "label": 1
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "Therefore, it's very essential for us to measure the similarities between the two domains and also measure the similarities between the different parts with the target data set.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are some existing methods dealing with this inductive transfer learning problems.",
                    "label": 0
                },
                {
                    "sent": "However, some of them has problems such as some of them considered only one source task.",
                    "label": 1
                },
                {
                    "sent": "And most methods only consider one kind of similarities, which is either the similarity between tasks.",
                    "label": 1
                },
                {
                    "sent": "All the similarity between the instances.",
                    "label": 0
                },
                {
                    "sent": "Also, the sum methods are here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stick so.",
                    "label": 0
                },
                {
                    "sent": "In our algorithm, we want to design an algorithm based on a solid theoretical foundation which can deal with multiple source tasks.",
                    "label": 1
                },
                {
                    "sent": "And we consider not only the similarity between the datasets and we also consider similarity of the different parts within one datasets to the target data set.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For our problem setting we have a hyperplane classifiers from the induced from each source data set and we also could induce.",
                    "label": 1
                },
                {
                    "sent": "The hyperplane classifiers from the labeled data set in the target data set it is denoted by the EVT.",
                    "label": 1
                },
                {
                    "sent": "So instead of measuring the similarities directly between the two datasets, we use our compact coding method to measure the code length of the two hyperplanes based on the two datasets.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a very simple example.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have 8 labeled examples in the target data set, with the hyperplane induced from it as we T so W1 and W2's hyperplanes of the two source datasets S1 and S2.",
                    "label": 1
                },
                {
                    "sent": "So as you can see from the figure, both the two hyperplanes has one wrong predictions, 1 prediction of eight examples of the target data set.",
                    "label": 0
                },
                {
                    "sent": "So it is not easy for us to tell which one is more similar to the target data set.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's why we want to build a negative algorithm to measure the similarities.",
                    "label": 0
                },
                {
                    "sent": "Our method is inspired by the minimum description, description, length, principle, and in this principle the best hypothesis could be induced from the data sets is 1 which could minimize the code length of the hypothesis and the code length of the data using the hypothesis.",
                    "label": 1
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "We are given a data set D and the hypothesis family HHIHII is the integers and the best hypothesis at best could be induced from the data set is the one to minimize the first part is the code length of the data using the hypothesis and the second part is occurrence of the hypothesis itself.",
                    "label": 1
                },
                {
                    "sent": "So we can say that the minimum description length principle could balance the simplicity of the hypothesis and the goodness feed of the fit to the data set.",
                    "label": 0
                },
                {
                    "sent": "So in this way we could avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a framework of our method compact coding for hyperplane classifiers and we call it CHC.",
                    "label": 0
                },
                {
                    "sent": "It contains 2 level evaluations.",
                    "label": 0
                },
                {
                    "sent": "The first level is the macro level evaluation in which we source the source data sets in the descending order on the degrees of the similarity with the target data set and the similarity is denoted by our code, our code length.",
                    "label": 1
                },
                {
                    "sent": "The second level is a micro level evaluation, which we divide each data set of the related source tasks into several components, and we select related parts to help training the classifier in the target domain, and this similarity measure is also based on our on our coding method.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a code length as a similarity measure.",
                    "label": 0
                },
                {
                    "sent": "An posterior probability of a hyperplane WI given the source data set, as I could be presented by the probability of the weight given the data set.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want to measure the similarity between the hyperplane to the target data set.",
                    "label": 0
                },
                {
                    "sent": "So we substitute the source data set as I with the target data set.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "T and decompose it by the Bear theorem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As you can see, it's not very easy for us to calculate the second distributions probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So as as I mentioned before, we already have the label information from the target data set and we can have a hyperplane Vt from the labeled information, so we could borrow, borrow the strength to help to code the hyperplane in the source data set.",
                    "label": 1
                },
                {
                    "sent": "So we have this formula.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And decomposing also by the Bear theorem.",
                    "label": 0
                },
                {
                    "sent": "And for.",
                    "label": 0
                },
                {
                    "sent": "At last we get two probability Dist.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Revolutions and we could write our code lens as the two parts.",
                    "label": 0
                },
                {
                    "sent": "The first part is to code the target data set with the hyperplane and the second part is to code the hyperplane with the help of another hyperplane.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before introducing our detailed coding method, I want to introduce the preliminaries of coding.",
                    "label": 0
                },
                {
                    "sent": "The first one is to the code length of a binary string of length a, which consists of B binary ones.",
                    "label": 1
                },
                {
                    "sent": "So this could be denoted by siedah function and in the first part we send the length A and the second part we specify the positions of the ones in the string.",
                    "label": 1
                },
                {
                    "sent": "And also we want to coding we want to code a real number X on the assumption that X equals to mill is most likely where mu is also a real number.",
                    "label": 0
                },
                {
                    "sent": "An F is a continuous probability with precision.",
                    "label": 0
                },
                {
                    "sent": "So this is denoted by the Lambda function as this.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is the coding method of our CHC, the first part of the code.",
                    "label": 1
                },
                {
                    "sent": "Length to code hyperplane code are vector based on an another vector of the hyperplane is denoted by the Lambda function and the second part of the code length could be denoted by specifying the wrong predictions of the.",
                    "label": 0
                },
                {
                    "sent": "The wrong predictions of the target data set.",
                    "label": 0
                },
                {
                    "sent": "So the colons could be written as these two parts.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we could solve the problems I mentioned before to calculate the code length of the two hyperplanes and to tell which one is more similar to the target data set.",
                    "label": 1
                },
                {
                    "sent": "So after calculation we got the.",
                    "label": 0
                },
                {
                    "sent": "The code length of the second data set is more similar to the target data set.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is a pseudocode of our method.",
                    "label": 0
                },
                {
                    "sent": "The first 3 lines are the macro level, which we thought the source data set in the ascending order of the colon switches equipped, which is equal to the descending order of the similarities and for the micro level we divide the source data set into several components and send the related parts from the source domain to the target domain.",
                    "label": 1
                },
                {
                    "sent": "And we calculate the code length as proposed.",
                    "label": 0
                },
                {
                    "sent": "Then terminate if we cannot find any shorter colons, output hyper plan that are.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inocente we evaluate our method on two kinds of datasets.",
                    "label": 0
                },
                {
                    "sent": "The first one is the UCI datasets and we choose three of them from the UCI data set, and we also adopt A preprocessing method from this paper to speak split each data set into the source and target datasets.",
                    "label": 1
                },
                {
                    "sent": "For text datasets, we choose the 20 newsgroups data sets.",
                    "label": 1
                },
                {
                    "sent": "As with the preprocessing method get by this paper to form different tasks with sub categories.",
                    "label": 0
                },
                {
                    "sent": "And for comparison, we have several state of the art methods such as SVM, tribest and so on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the results on the UCI datasets, which is the mushroom data set.",
                    "label": 0
                },
                {
                    "sent": "As you could see, we evaluate two factors.",
                    "label": 0
                },
                {
                    "sent": "One factor is the number of instances in target assets.",
                    "label": 0
                },
                {
                    "sent": "We set each as 50 or 100 and the second factor we evaluate is percentage of noise, and we test our method under the noise level from zero percent to 15%.",
                    "label": 0
                },
                {
                    "sent": "As you could see, the results of the mushroom datasets one the number of instances in the target data set is not so money.",
                    "label": 0
                },
                {
                    "sent": "So many is better than the.",
                    "label": 0
                },
                {
                    "sent": "The labeled information is abundant.",
                    "label": 1
                },
                {
                    "sent": "And our method mostly outperforms other methods.",
                    "label": 0
                },
                {
                    "sent": "And is able to achieve lower rate with only a few label information.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Double.",
                    "label": 0
                },
                {
                    "sent": "So for the two other datasets from UCI datasets do to limit page limit.",
                    "label": 0
                },
                {
                    "sent": "I only present the results in one slide as you could see our method is most mostly outperforms other methods.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the text data sets, the first is to use the rec via stalk as a target data set.",
                    "label": 0
                },
                {
                    "sent": "And therefore the two factors we evaluate.",
                    "label": 0
                },
                {
                    "sent": "We evaluate different number of instances in the target data set and different noise conditions from zero to 15%.",
                    "label": 0
                },
                {
                    "sent": "Our method is the best one among our methods and.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For other two datasets in 20 newsgroup datasets, we also present the results in one slide.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we also evaluate transferred components in the text data set in the micro level.",
                    "label": 1
                },
                {
                    "sent": "As you could see in this table, S1 and S2S3 denote the different source tasks from the source domain and the integers 0 means that there is no part transferred from source to target domain and one means there is one part transferred.",
                    "label": 1
                },
                {
                    "sent": "And from this table we could say that not all the information from the similar source tasks are transferred from the source to the target domain and our micro level evaluation is effective, which can adapt adaptively.",
                    "label": 1
                },
                {
                    "sent": "Select related parts for transferring.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the summary of this work, we want designer coding method for hyperplane classifiers in the transfer learning setting and our method could adaptively select related parts in the source task in classifying the target task.",
                    "label": 1
                },
                {
                    "sent": "We proposed a compact coding method inspired by MD RP to measure the similarity between the data by the code length.",
                    "label": 0
                },
                {
                    "sent": "Experiments conducted on the both at UCI and text data set showed effectiveness of our method.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you very much for listening.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 1
                },
                {
                    "sent": "So we have time for one question.",
                    "label": 0
                },
                {
                    "sent": "So in your talk you mentioned several times that your method tries to exploit similarity between different datasets.",
                    "label": 0
                },
                {
                    "sent": "But if I interpret your model well, I would rather say your method is exploiting a similarity between the different models.",
                    "label": 0
                },
                {
                    "sent": "Yes, for the different datasets, so at least to me this is.",
                    "label": 0
                },
                {
                    "sent": "These are two slightly different things.",
                    "label": 0
                },
                {
                    "sent": "If you have similar models, I would say this is a specific type of similarity in.",
                    "label": 0
                },
                {
                    "sent": "The data set.",
                    "label": 0
                },
                {
                    "sent": "It's just a small comment.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so the number of questions that let's think this picture once more and.",
                    "label": 0
                }
            ]
        }
    }
}