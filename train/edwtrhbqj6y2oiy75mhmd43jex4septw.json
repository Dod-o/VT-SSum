{
    "id": "edwtrhbqj6y2oiy75mhmd43jex4septw",
    "title": "Deep Learning for Efficient Discriminative Parsing",
    "info": {
        "author": [
            "Ronan Collobert, NEC Laboratories America, Inc."
        ],
        "published": "May 6, 2011",
        "recorded": "April 2011",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/aistats2011_collobert_deep/",
    "segmentation": [
        [
            "So as the title says, this talk is about passing.",
            "In fact syntactic parsing.",
            "Using like some evil deep learning approach.",
            "Which would be a kind of preview of what's coming next, right?",
            "This number, the workshop."
        ],
        [
            "But in fact, the main motivation was coming from the fact that we already have in the hands are kind of generic architecture for a bunch of natural language processing tasks, including like part of speech tagging, chunking, named entity recognition, and semantical labeling.",
            "So we made this architecture on generic by considering deep learning model.",
            "So instead of designing task specific features for each task of interest, we basically do machine learning and we let the machine train the features we are interested in for each task of interest.",
            "So.",
            "This architecture was actually leveraging one or presentations that we train on large scale unlabeled corpus.",
            "And the question we were wondering here is, I mean can we go beyond this kind of task which were basically just, you know, flat tagging task for each world you had only to put one single tag over the world.",
            "So here."
        ],
        [
            "When testing we are interested in like tackling a more complicated problem which is syntactic parsing, that is, finding the tree of constituent of syntactic constituent in the sentence.",
            "So things like verb, phrases, noun phrases.",
            "Prepositional phrases and so on.",
            "So it's not a flat taking task anymore, but I will show that you can still use the same kind of architecture we had before by making it just recurrent and by just adding like few constraints to be sure that you obtain your tree at the end.",
            "So a good part of this talk is going to be about the architecture, the architecture we had before.",
            "So keep in mind that this architecture is not like specific to parsing, but it can be also applied to a bunch of natural languages, natural language processing task."
        ],
        [
            "If you look at the literature of of syntactic parsing, you will find that state of the art parsers heavily relying on what we called probabilistic context free grammar features.",
            "Which are basically counting in your training set.",
            "How many times a node of the tree is split into two different nodes.",
            "So in fact this kind of PCF G features are lexicalized in the sense then when you consider our node of the tree, you consider not only the syntactic tag of the node, but also.",
            "Had one of the note that is the most important word in an order, if you like.",
            "So this kind of features are already task specific for passing.",
            "And they have been successfully successfully used first, like I would say with like the current sponsor in 99.",
            "And the Greens parser was basically a generative process, so it was able to generate a bunch in the trees given a sentence which would have the highest likelihood according to the PCG holds you train.",
            "I mean you trained on your training set.",
            "You update on your training set.",
            "So since then people have been trying to, you know, make this kind of generative passer more discriminative and a way to do that is like just re rank the top trees you obtain and to do this and re ranking you need again some task specific features, but it also work very well and other parts of research is like batteries based on the conditional random fields.",
            "But again state of the art parsers."
        ],
        [
            "Rely heavily on PCF features, so here we don't want this kind of task specific features and we're trying to find something.",
            "I mean the kind of architecture which can basically deal only with raw text.",
            "And to do so, we are going to consider on the passing task as just tagging task.",
            "So we take a tree.",
            "You get the leaves of the tree it gives you like, I mean, a bunch of nodes, possibly for each word in the sentence then so it gives you a level of the tree.",
            "If you like, you remove.",
            "This leaves, you cut the next leaves of the tree, gives you another level of the tree, and so on.",
            "OK, and for each level of the tree you can basically put a target for each word in the sentence.",
            "So if the world belongs to a node.",
            "Of the corresponding level, you assign A tag which corresponds to the tag of the node prefixed by something which tells you where you are in the node.",
            "So for example, if I take the top sentence here.",
            "The first one would like be prefixed by before like beginning and then inside for the next words.",
            "And you know for the end end world.",
            "So it's really like now attacking approach.",
            "We have to tag several levels and we are going to do that in a really crazy ways because we are going to try to predict all of the tree according to previous levels.",
            "So I say it's crazy because it's really greedy.",
            "The training crate time we are going to use doesn't consider the old tree at once, but really level by level, but still it works really well in practice.",
            "So if you consider this kind of approach trying to produce overall according to previous loan, you might encounter problems like you don't want your Tiger, you know to enter in the loop.",
            "For example, you don't want, it would always produce the same tags for each levels.",
            "Also, you want to be sure you produce your tree 'cause for example, if you got at some level.",
            "The node VP up there.",
            "You don't want at the next level that your Tiger split this node into two parts, otherwise you would not have a tree anymore.",
            "So you want to make sure that when a node in upper level overlaps with the node in a previous level, the node in the upper level includes completely the previous node.",
            "So to make sure that we are doing that, we transform the training trees or little bit.",
            "So when we see notes consecutive nodes in in 2 levels or three levels, or 4 levels which like spend the same words, we just like concatenate them into one single node with the corresponding concatenated tag.",
            "So now we are sure that the nodes always strictly grow in size and we will just enforce this tricky growing thing in our tagging procedure in a very simple way."
        ],
        [
            "So we are using deep learning to like try to capture the I mean train the most important, I mean the features relevant to passing.",
            "Hannah, I'm in.",
            "Deep learning is just a buzzword for non Nets and no, unless they are just a stack of matrix multiplications interleaved with some new ideas.",
            "So nothing magic there.",
            "And in fact you have a bit more complicated knowledge when you want to deal with like sequences like in text actually.",
            "But still, it's still like some matrix vector multiplication that you apply several times along the sequence.",
            "So."
        ],
        [
            "And the problem is that the only point is that we are considering sentences so that is, I mean you have words in sentences.",
            "So how do you transform words into a vector?",
            "So a simple way is just like to consider award as an index in your dictionary, which is completely equivalent to consider.",
            "Very sparse vector of the size of the dictionary where you put everywhere as yours, except at some point where you put one I mean corresponding to the index of the wall.",
            "So now if you do your little matrix vector multiplication using this kind of vector in your net, it's exactly as.",
            "That grabbing run in your matrix so it can be really efficiently learn and we call that a look up table operation.",
            "So for each word in the sentence, you just, you know, applies this look up table operation and you get water presentation.",
            "But this one representation is going to be trained in an manner in the system."
        ],
        [
            "So now we have to tag a sentence like for example this one.",
            "You are going to do it by taking each one one by one.",
            "So you consider for example one in very simple approach would be to consider a fixed size window around this world.",
            "So you just look at the what's around you, apply your look up table operation, you concatenate everything brutally.",
            "You pass that through matrix vector operations.",
            "Intel is within RIT and at the end you obtain 1 score.",
            "Per DAG in your passing task.",
            "So of course, sometimes you want more than just roll.",
            "Rotex features, in particular.",
            "In our case, we need like a history of what we had before in the previous level.",
            "Also, sometimes we want."
        ],
        [
            "Bottom speech so you can do the same thing.",
            "You know you just add them at the input of your network.",
            "Add like some extra lookup table operations and do the same thing.",
            "Then before concatenate everything and obtain a score."
        ],
        [
            "Yeah, it's cool.",
            "Tight end zone in the passing desk, so the problem with this kind of window approach is that it doesn't deal with long range dependencies because a wall which would depend.",
            "Another word far away in the sentence would not even seen in the window approach.",
            "So to overcome this problem, we propose, like very simple generalization of what we had before, which is a kind of condition or non net again.",
            "Where we input the complete sentence to inside no network.",
            "But this time we have to say to the network.",
            "Hey I want to tag this world so we had like a little feature which is just a relative distance for each one with respect to the wall of interest.",
            "So just the number.",
            "Again, you apply your lookup table operations.",
            "Now we locali combine both this one presentation and this distance representation to obtain features which can handle both at the same time.",
            "We perform a Max through the time of the sentence to try to capture the most important features along time for tagging the wall of interest, and then we feed that to layers in the same way.",
            "Then before we again obtain one score per tag of interest.",
            "So."
        ],
        [
            "Might think about training this kind of.",
            "Architecture like.",
            "I would say that by considering the world in a independent manner, but everybody knows in natural language processing that is really not a good idea because tags are always interdependence.",
            "So everybody I mean at the state of the art systems heavily relied on like some kind of structured output learning system.",
            "So here that's what we are going to do.",
            "Or training criterion is going to consider the full sentence for each word in the sentence we ask our network so.",
            "Either the window approach or the sentence approach to give us a score for each tag of the passing task.",
            "And we build up a very simple graph on the nodes of the graph you put in the score of the network and on the edges you put like some kind of transition score, which I mean for jumping from one level to the next one.",
            "So now if you have a passive tags in this simple graph, the score of the path of tags is going to be the sum of the transition call scores along this path plus the sum of the network scores along these paths.",
            "Alright, if you have a testing sentences, sorry, yeah, if you have a test sentence and you want to find the best path so the path which has a higher score, it's simply down by.",
            "Viterbi algorithm, it's pretty easy, and in the case of training where."
        ],
        [
            "We are going to interpret this score as a conditional probability by basically normalizing by all the possible paths.",
            "So you basically take your score, you put it at the exponential so it's positive you divide by the sum of the exponential.",
            "Overall, the path such that you know it's up to one to your party, so it's very classical, and when you get the log you get this quantity.",
            "Which when you try to maximize it, will try to push up the right pass.",
            "So the first time while pushing down all the other path, right?",
            "So the sum you can see contains, like I mean, is there some over like an exponential number of terms with respect to the length of the sentence, but still can be computed efficiently by the recursive forward algorithm.",
            "So we are going to train the network using this kind of criterion.",
            "We use just maximize maximization of the likelihood where the likelihood is defined like that and we use like stochastic gradient with no background tricks.",
            "So you can really do it yourself.",
            "It's not magic.",
            "The gradients are computed with like the chain rule.",
            "We really applies the trainer or not only through the forward or Christian algorithm, but also through the network up to the first layer.",
            "So everything is trained in end to end manner and performs great so you can see this kind of model as a kind of nonlinear conditional column field.",
            "In fact, it would be exactly a conditional random field for sequences if our network was linear, but it's not, which allows us to really train.",
            "The features we want for each task of interest."
        ],
        [
            "So in the case of passing, the only problem remaining now is how do we make sure that when we have a node at, I mean particle level of the tree which overlap with the previous node.",
            "How we make sure that this node is strictly larger than the previous node?",
            "So we can do that easily by just constraining a bit the graph that we have at the end of the tree.",
            "I wish that we have at the end of the network, so if for example you have a sequence sentence like this one, if you assume that you found like for example here or non phrase at some level of the tree at the next level you want to make sure that if you find a new chunk around these words of interest, well this junk has to include at least all this previous chunk, right?",
            "So we constrain your video graph.",
            "If the graph was unconstrained, it would be really like that so.",
            "You know from anytime you can jump to any other Tiger at the net at the next World.",
            "Here we we like constraint graph such that you have like 4 possible pass or actually three possible best protag plus one extra possible bus which is like you know you have nothing because you might find nothing around this.",
            "As you said, was possible.",
            "So if you find something you can maybe start at the same position, but then you will never end at anytime in this segment.",
            "If you can also enter the same position but never start at anytime in the previous segment, or of course you could always be in this segment never and never start.",
            "At anytime in this segment.",
            "So by just constraining like that, the graph you basically solve your problem."
        ],
        [
            "So we compared on network first on like the first discriminative parser, which has which I've been out there, which we're kind of through train, so they were trained only like on sentences with less than 15 words.",
            "And.",
            "We use their only like window approach with like roll once actually in low caps.",
            "So we had like cats feature on top of that with like a historic feature to keep track on what we predicted before hand.",
            "I mean for each level before and optionally we use also like a part of speech feature.",
            "We compared with like the generative parser from Collins and from like 2 State of the art on this data set at least.",
            "Discount spasso so all network straight out of the box without any trick is kind of far from the state of the art.",
            "If we initialize or what representation by the language model wall presentation that we obtain, you know along time ago then we really like have a huge boost which really shows that this water presentation give us a lot of power.",
            "If we use like butter speech features which are very common, you know for any kind of tasking language processing.",
            "We update you again.",
            "Are you to boost?",
            "And if you mix both, basically we are state of the art.",
            "So."
        ],
        [
            "We also compared with a bunch of parcels on the full data set.",
            "And.",
            "Yeah, so it's a bit of the same story or network straight out of the box is a bit far from the state of the art, but you know if you mix both the language model on the part of speech features, you are doing quite good.",
            "Well not that good actually with the window approach, but if you use a sentence approach, which is you know, catching long range dependencies, you are basically as the first.",
            "PCG based generative and.",
            "This competitive bustles, so I think it's quite encouraging because here we are really not using any task specific features.",
            "And also we are extremely fast compared to the implementations I could find on the web."
        ],
        [
            "So in fact this process is possible is going to be available as very soon together with, you know, other.",
            "Task we are we have like.",
            "We had previously and it's basically like 3000 lines of C code standalone running very fast, not requiring much memory.",
            "And yeah, I mean if you want I can even like try, you know.",
            "They have like something there, so it's it's about so in question.",
            "Quite small, I didn't.",
            "Life is like less than 3000 lines.",
            "You can compile it.",
            "Yourself.",
            "I was like you know, fast MoD.",
            "I like to use my so.",
            "Blah sporty.",
            "OK, so pretty small stand alone.",
            "I didn't lie again and I don't know I can try some sentence here.",
            "I didn't prepare.",
            "So maybe that.",
            "I have no idea if he's good anyways because I'm not a linguist, so.",
            "So you are part of speech tags.",
            "You are checking bags.",
            "You have named entity organization Tag, so amazingly it finds a status is an organization and then here you have like the semantical labeling and at the end of passing.",
            "So who knows if it's good.",
            "I'm sure there are some linguists around there, but.",
            "So that's all I have to say.",
            "If you want to try, let me know."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as the title says, this talk is about passing.",
                    "label": 0
                },
                {
                    "sent": "In fact syntactic parsing.",
                    "label": 0
                },
                {
                    "sent": "Using like some evil deep learning approach.",
                    "label": 1
                },
                {
                    "sent": "Which would be a kind of preview of what's coming next, right?",
                    "label": 0
                },
                {
                    "sent": "This number, the workshop.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in fact, the main motivation was coming from the fact that we already have in the hands are kind of generic architecture for a bunch of natural language processing tasks, including like part of speech tagging, chunking, named entity recognition, and semantical labeling.",
                    "label": 1
                },
                {
                    "sent": "So we made this architecture on generic by considering deep learning model.",
                    "label": 0
                },
                {
                    "sent": "So instead of designing task specific features for each task of interest, we basically do machine learning and we let the machine train the features we are interested in for each task of interest.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This architecture was actually leveraging one or presentations that we train on large scale unlabeled corpus.",
                    "label": 0
                },
                {
                    "sent": "And the question we were wondering here is, I mean can we go beyond this kind of task which were basically just, you know, flat tagging task for each world you had only to put one single tag over the world.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When testing we are interested in like tackling a more complicated problem which is syntactic parsing, that is, finding the tree of constituent of syntactic constituent in the sentence.",
                    "label": 0
                },
                {
                    "sent": "So things like verb, phrases, noun phrases.",
                    "label": 0
                },
                {
                    "sent": "Prepositional phrases and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's not a flat taking task anymore, but I will show that you can still use the same kind of architecture we had before by making it just recurrent and by just adding like few constraints to be sure that you obtain your tree at the end.",
                    "label": 0
                },
                {
                    "sent": "So a good part of this talk is going to be about the architecture, the architecture we had before.",
                    "label": 0
                },
                {
                    "sent": "So keep in mind that this architecture is not like specific to parsing, but it can be also applied to a bunch of natural languages, natural language processing task.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the literature of of syntactic parsing, you will find that state of the art parsers heavily relying on what we called probabilistic context free grammar features.",
                    "label": 0
                },
                {
                    "sent": "Which are basically counting in your training set.",
                    "label": 0
                },
                {
                    "sent": "How many times a node of the tree is split into two different nodes.",
                    "label": 0
                },
                {
                    "sent": "So in fact this kind of PCF G features are lexicalized in the sense then when you consider our node of the tree, you consider not only the syntactic tag of the node, but also.",
                    "label": 0
                },
                {
                    "sent": "Had one of the note that is the most important word in an order, if you like.",
                    "label": 0
                },
                {
                    "sent": "So this kind of features are already task specific for passing.",
                    "label": 0
                },
                {
                    "sent": "And they have been successfully successfully used first, like I would say with like the current sponsor in 99.",
                    "label": 0
                },
                {
                    "sent": "And the Greens parser was basically a generative process, so it was able to generate a bunch in the trees given a sentence which would have the highest likelihood according to the PCG holds you train.",
                    "label": 0
                },
                {
                    "sent": "I mean you trained on your training set.",
                    "label": 0
                },
                {
                    "sent": "You update on your training set.",
                    "label": 0
                },
                {
                    "sent": "So since then people have been trying to, you know, make this kind of generative passer more discriminative and a way to do that is like just re rank the top trees you obtain and to do this and re ranking you need again some task specific features, but it also work very well and other parts of research is like batteries based on the conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "But again state of the art parsers.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rely heavily on PCF features, so here we don't want this kind of task specific features and we're trying to find something.",
                    "label": 0
                },
                {
                    "sent": "I mean the kind of architecture which can basically deal only with raw text.",
                    "label": 0
                },
                {
                    "sent": "And to do so, we are going to consider on the passing task as just tagging task.",
                    "label": 1
                },
                {
                    "sent": "So we take a tree.",
                    "label": 0
                },
                {
                    "sent": "You get the leaves of the tree it gives you like, I mean, a bunch of nodes, possibly for each word in the sentence then so it gives you a level of the tree.",
                    "label": 0
                },
                {
                    "sent": "If you like, you remove.",
                    "label": 0
                },
                {
                    "sent": "This leaves, you cut the next leaves of the tree, gives you another level of the tree, and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, and for each level of the tree you can basically put a target for each word in the sentence.",
                    "label": 0
                },
                {
                    "sent": "So if the world belongs to a node.",
                    "label": 0
                },
                {
                    "sent": "Of the corresponding level, you assign A tag which corresponds to the tag of the node prefixed by something which tells you where you are in the node.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I take the top sentence here.",
                    "label": 0
                },
                {
                    "sent": "The first one would like be prefixed by before like beginning and then inside for the next words.",
                    "label": 0
                },
                {
                    "sent": "And you know for the end end world.",
                    "label": 0
                },
                {
                    "sent": "So it's really like now attacking approach.",
                    "label": 0
                },
                {
                    "sent": "We have to tag several levels and we are going to do that in a really crazy ways because we are going to try to predict all of the tree according to previous levels.",
                    "label": 1
                },
                {
                    "sent": "So I say it's crazy because it's really greedy.",
                    "label": 0
                },
                {
                    "sent": "The training crate time we are going to use doesn't consider the old tree at once, but really level by level, but still it works really well in practice.",
                    "label": 0
                },
                {
                    "sent": "So if you consider this kind of approach trying to produce overall according to previous loan, you might encounter problems like you don't want your Tiger, you know to enter in the loop.",
                    "label": 0
                },
                {
                    "sent": "For example, you don't want, it would always produce the same tags for each levels.",
                    "label": 0
                },
                {
                    "sent": "Also, you want to be sure you produce your tree 'cause for example, if you got at some level.",
                    "label": 1
                },
                {
                    "sent": "The node VP up there.",
                    "label": 0
                },
                {
                    "sent": "You don't want at the next level that your Tiger split this node into two parts, otherwise you would not have a tree anymore.",
                    "label": 1
                },
                {
                    "sent": "So you want to make sure that when a node in upper level overlaps with the node in a previous level, the node in the upper level includes completely the previous node.",
                    "label": 0
                },
                {
                    "sent": "So to make sure that we are doing that, we transform the training trees or little bit.",
                    "label": 1
                },
                {
                    "sent": "So when we see notes consecutive nodes in in 2 levels or three levels, or 4 levels which like spend the same words, we just like concatenate them into one single node with the corresponding concatenated tag.",
                    "label": 0
                },
                {
                    "sent": "So now we are sure that the nodes always strictly grow in size and we will just enforce this tricky growing thing in our tagging procedure in a very simple way.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we are using deep learning to like try to capture the I mean train the most important, I mean the features relevant to passing.",
                    "label": 0
                },
                {
                    "sent": "Hannah, I'm in.",
                    "label": 0
                },
                {
                    "sent": "Deep learning is just a buzzword for non Nets and no, unless they are just a stack of matrix multiplications interleaved with some new ideas.",
                    "label": 0
                },
                {
                    "sent": "So nothing magic there.",
                    "label": 0
                },
                {
                    "sent": "And in fact you have a bit more complicated knowledge when you want to deal with like sequences like in text actually.",
                    "label": 0
                },
                {
                    "sent": "But still, it's still like some matrix vector multiplication that you apply several times along the sequence.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the problem is that the only point is that we are considering sentences so that is, I mean you have words in sentences.",
                    "label": 0
                },
                {
                    "sent": "So how do you transform words into a vector?",
                    "label": 0
                },
                {
                    "sent": "So a simple way is just like to consider award as an index in your dictionary, which is completely equivalent to consider.",
                    "label": 0
                },
                {
                    "sent": "Very sparse vector of the size of the dictionary where you put everywhere as yours, except at some point where you put one I mean corresponding to the index of the wall.",
                    "label": 0
                },
                {
                    "sent": "So now if you do your little matrix vector multiplication using this kind of vector in your net, it's exactly as.",
                    "label": 0
                },
                {
                    "sent": "That grabbing run in your matrix so it can be really efficiently learn and we call that a look up table operation.",
                    "label": 0
                },
                {
                    "sent": "So for each word in the sentence, you just, you know, applies this look up table operation and you get water presentation.",
                    "label": 0
                },
                {
                    "sent": "But this one representation is going to be trained in an manner in the system.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have to tag a sentence like for example this one.",
                    "label": 1
                },
                {
                    "sent": "You are going to do it by taking each one one by one.",
                    "label": 0
                },
                {
                    "sent": "So you consider for example one in very simple approach would be to consider a fixed size window around this world.",
                    "label": 0
                },
                {
                    "sent": "So you just look at the what's around you, apply your look up table operation, you concatenate everything brutally.",
                    "label": 0
                },
                {
                    "sent": "You pass that through matrix vector operations.",
                    "label": 0
                },
                {
                    "sent": "Intel is within RIT and at the end you obtain 1 score.",
                    "label": 0
                },
                {
                    "sent": "Per DAG in your passing task.",
                    "label": 0
                },
                {
                    "sent": "So of course, sometimes you want more than just roll.",
                    "label": 0
                },
                {
                    "sent": "Rotex features, in particular.",
                    "label": 1
                },
                {
                    "sent": "In our case, we need like a history of what we had before in the previous level.",
                    "label": 0
                },
                {
                    "sent": "Also, sometimes we want.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bottom speech so you can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "You know you just add them at the input of your network.",
                    "label": 0
                },
                {
                    "sent": "Add like some extra lookup table operations and do the same thing.",
                    "label": 0
                },
                {
                    "sent": "Then before concatenate everything and obtain a score.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, it's cool.",
                    "label": 0
                },
                {
                    "sent": "Tight end zone in the passing desk, so the problem with this kind of window approach is that it doesn't deal with long range dependencies because a wall which would depend.",
                    "label": 0
                },
                {
                    "sent": "Another word far away in the sentence would not even seen in the window approach.",
                    "label": 0
                },
                {
                    "sent": "So to overcome this problem, we propose, like very simple generalization of what we had before, which is a kind of condition or non net again.",
                    "label": 0
                },
                {
                    "sent": "Where we input the complete sentence to inside no network.",
                    "label": 0
                },
                {
                    "sent": "But this time we have to say to the network.",
                    "label": 0
                },
                {
                    "sent": "Hey I want to tag this world so we had like a little feature which is just a relative distance for each one with respect to the wall of interest.",
                    "label": 0
                },
                {
                    "sent": "So just the number.",
                    "label": 0
                },
                {
                    "sent": "Again, you apply your lookup table operations.",
                    "label": 0
                },
                {
                    "sent": "Now we locali combine both this one presentation and this distance representation to obtain features which can handle both at the same time.",
                    "label": 0
                },
                {
                    "sent": "We perform a Max through the time of the sentence to try to capture the most important features along time for tagging the wall of interest, and then we feed that to layers in the same way.",
                    "label": 0
                },
                {
                    "sent": "Then before we again obtain one score per tag of interest.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Might think about training this kind of.",
                    "label": 0
                },
                {
                    "sent": "Architecture like.",
                    "label": 0
                },
                {
                    "sent": "I would say that by considering the world in a independent manner, but everybody knows in natural language processing that is really not a good idea because tags are always interdependence.",
                    "label": 0
                },
                {
                    "sent": "So everybody I mean at the state of the art systems heavily relied on like some kind of structured output learning system.",
                    "label": 1
                },
                {
                    "sent": "So here that's what we are going to do.",
                    "label": 0
                },
                {
                    "sent": "Or training criterion is going to consider the full sentence for each word in the sentence we ask our network so.",
                    "label": 0
                },
                {
                    "sent": "Either the window approach or the sentence approach to give us a score for each tag of the passing task.",
                    "label": 1
                },
                {
                    "sent": "And we build up a very simple graph on the nodes of the graph you put in the score of the network and on the edges you put like some kind of transition score, which I mean for jumping from one level to the next one.",
                    "label": 0
                },
                {
                    "sent": "So now if you have a passive tags in this simple graph, the score of the path of tags is going to be the sum of the transition call scores along this path plus the sum of the network scores along these paths.",
                    "label": 1
                },
                {
                    "sent": "Alright, if you have a testing sentences, sorry, yeah, if you have a test sentence and you want to find the best path so the path which has a higher score, it's simply down by.",
                    "label": 0
                },
                {
                    "sent": "Viterbi algorithm, it's pretty easy, and in the case of training where.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We are going to interpret this score as a conditional probability by basically normalizing by all the possible paths.",
                    "label": 1
                },
                {
                    "sent": "So you basically take your score, you put it at the exponential so it's positive you divide by the sum of the exponential.",
                    "label": 0
                },
                {
                    "sent": "Overall, the path such that you know it's up to one to your party, so it's very classical, and when you get the log you get this quantity.",
                    "label": 0
                },
                {
                    "sent": "Which when you try to maximize it, will try to push up the right pass.",
                    "label": 0
                },
                {
                    "sent": "So the first time while pushing down all the other path, right?",
                    "label": 0
                },
                {
                    "sent": "So the sum you can see contains, like I mean, is there some over like an exponential number of terms with respect to the length of the sentence, but still can be computed efficiently by the recursive forward algorithm.",
                    "label": 1
                },
                {
                    "sent": "So we are going to train the network using this kind of criterion.",
                    "label": 0
                },
                {
                    "sent": "We use just maximize maximization of the likelihood where the likelihood is defined like that and we use like stochastic gradient with no background tricks.",
                    "label": 0
                },
                {
                    "sent": "So you can really do it yourself.",
                    "label": 0
                },
                {
                    "sent": "It's not magic.",
                    "label": 0
                },
                {
                    "sent": "The gradients are computed with like the chain rule.",
                    "label": 1
                },
                {
                    "sent": "We really applies the trainer or not only through the forward or Christian algorithm, but also through the network up to the first layer.",
                    "label": 0
                },
                {
                    "sent": "So everything is trained in end to end manner and performs great so you can see this kind of model as a kind of nonlinear conditional column field.",
                    "label": 0
                },
                {
                    "sent": "In fact, it would be exactly a conditional random field for sequences if our network was linear, but it's not, which allows us to really train.",
                    "label": 0
                },
                {
                    "sent": "The features we want for each task of interest.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the case of passing, the only problem remaining now is how do we make sure that when we have a node at, I mean particle level of the tree which overlap with the previous node.",
                    "label": 0
                },
                {
                    "sent": "How we make sure that this node is strictly larger than the previous node?",
                    "label": 0
                },
                {
                    "sent": "So we can do that easily by just constraining a bit the graph that we have at the end of the tree.",
                    "label": 0
                },
                {
                    "sent": "I wish that we have at the end of the network, so if for example you have a sequence sentence like this one, if you assume that you found like for example here or non phrase at some level of the tree at the next level you want to make sure that if you find a new chunk around these words of interest, well this junk has to include at least all this previous chunk, right?",
                    "label": 0
                },
                {
                    "sent": "So we constrain your video graph.",
                    "label": 0
                },
                {
                    "sent": "If the graph was unconstrained, it would be really like that so.",
                    "label": 0
                },
                {
                    "sent": "You know from anytime you can jump to any other Tiger at the net at the next World.",
                    "label": 0
                },
                {
                    "sent": "Here we we like constraint graph such that you have like 4 possible pass or actually three possible best protag plus one extra possible bus which is like you know you have nothing because you might find nothing around this.",
                    "label": 0
                },
                {
                    "sent": "As you said, was possible.",
                    "label": 0
                },
                {
                    "sent": "So if you find something you can maybe start at the same position, but then you will never end at anytime in this segment.",
                    "label": 0
                },
                {
                    "sent": "If you can also enter the same position but never start at anytime in the previous segment, or of course you could always be in this segment never and never start.",
                    "label": 0
                },
                {
                    "sent": "At anytime in this segment.",
                    "label": 0
                },
                {
                    "sent": "So by just constraining like that, the graph you basically solve your problem.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we compared on network first on like the first discriminative parser, which has which I've been out there, which we're kind of through train, so they were trained only like on sentences with less than 15 words.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We use their only like window approach with like roll once actually in low caps.",
                    "label": 0
                },
                {
                    "sent": "So we had like cats feature on top of that with like a historic feature to keep track on what we predicted before hand.",
                    "label": 0
                },
                {
                    "sent": "I mean for each level before and optionally we use also like a part of speech feature.",
                    "label": 0
                },
                {
                    "sent": "We compared with like the generative parser from Collins and from like 2 State of the art on this data set at least.",
                    "label": 0
                },
                {
                    "sent": "Discount spasso so all network straight out of the box without any trick is kind of far from the state of the art.",
                    "label": 0
                },
                {
                    "sent": "If we initialize or what representation by the language model wall presentation that we obtain, you know along time ago then we really like have a huge boost which really shows that this water presentation give us a lot of power.",
                    "label": 0
                },
                {
                    "sent": "If we use like butter speech features which are very common, you know for any kind of tasking language processing.",
                    "label": 0
                },
                {
                    "sent": "We update you again.",
                    "label": 0
                },
                {
                    "sent": "Are you to boost?",
                    "label": 0
                },
                {
                    "sent": "And if you mix both, basically we are state of the art.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also compared with a bunch of parcels on the full data set.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's a bit of the same story or network straight out of the box is a bit far from the state of the art, but you know if you mix both the language model on the part of speech features, you are doing quite good.",
                    "label": 0
                },
                {
                    "sent": "Well not that good actually with the window approach, but if you use a sentence approach, which is you know, catching long range dependencies, you are basically as the first.",
                    "label": 0
                },
                {
                    "sent": "PCG based generative and.",
                    "label": 0
                },
                {
                    "sent": "This competitive bustles, so I think it's quite encouraging because here we are really not using any task specific features.",
                    "label": 0
                },
                {
                    "sent": "And also we are extremely fast compared to the implementations I could find on the web.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in fact this process is possible is going to be available as very soon together with, you know, other.",
                    "label": 0
                },
                {
                    "sent": "Task we are we have like.",
                    "label": 0
                },
                {
                    "sent": "We had previously and it's basically like 3000 lines of C code standalone running very fast, not requiring much memory.",
                    "label": 1
                },
                {
                    "sent": "And yeah, I mean if you want I can even like try, you know.",
                    "label": 0
                },
                {
                    "sent": "They have like something there, so it's it's about so in question.",
                    "label": 0
                },
                {
                    "sent": "Quite small, I didn't.",
                    "label": 0
                },
                {
                    "sent": "Life is like less than 3000 lines.",
                    "label": 1
                },
                {
                    "sent": "You can compile it.",
                    "label": 0
                },
                {
                    "sent": "Yourself.",
                    "label": 0
                },
                {
                    "sent": "I was like you know, fast MoD.",
                    "label": 0
                },
                {
                    "sent": "I like to use my so.",
                    "label": 0
                },
                {
                    "sent": "Blah sporty.",
                    "label": 0
                },
                {
                    "sent": "OK, so pretty small stand alone.",
                    "label": 0
                },
                {
                    "sent": "I didn't lie again and I don't know I can try some sentence here.",
                    "label": 0
                },
                {
                    "sent": "I didn't prepare.",
                    "label": 0
                },
                {
                    "sent": "So maybe that.",
                    "label": 0
                },
                {
                    "sent": "I have no idea if he's good anyways because I'm not a linguist, so.",
                    "label": 0
                },
                {
                    "sent": "So you are part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "You are checking bags.",
                    "label": 0
                },
                {
                    "sent": "You have named entity organization Tag, so amazingly it finds a status is an organization and then here you have like the semantical labeling and at the end of passing.",
                    "label": 0
                },
                {
                    "sent": "So who knows if it's good.",
                    "label": 0
                },
                {
                    "sent": "I'm sure there are some linguists around there, but.",
                    "label": 0
                },
                {
                    "sent": "So that's all I have to say.",
                    "label": 0
                },
                {
                    "sent": "If you want to try, let me know.",
                    "label": 0
                }
            ]
        }
    }
}