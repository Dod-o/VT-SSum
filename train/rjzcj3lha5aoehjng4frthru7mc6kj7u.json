{
    "id": "rjzcj3lha5aoehjng4frthru7mc6kj7u",
    "title": "Mixture of SVMs for Face Class Modeling",
    "info": {
        "author": [
            "Julien Meynet, Criteo"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "June 2004",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/mlmi04ch_meynet_msfcm/",
    "segmentation": [
        [
            "Yes, I'm going to present you the work that we're doing at the FFL about face detection.",
            "So we use a mixture of super big dimensions."
        ],
        [
            "So this is the outline of this presentation after so presentation of the context of his detection, I will explain approach with three main steps in the presentation.",
            "So the principal component analysis decomposition.",
            "Then I would give a few words by about the classical classification with SVM's.",
            "Then I will introduce a new structure of.",
            "Perfect machines in parallel, called mixtures of support vector machines.",
            "There is some experiments and results and I draw some conclusions.",
            "So."
        ],
        [
            "Context of face detection.",
            "What we want to do is to detect faces in images or video sequences.",
            "In this work, we will focus only on frontal faces.",
            "We can distinguish 2 main approaches that are commonly used in face detection.",
            "Basically we have the image based detections where we consider face as the following checked and we use machine learning algorithms to build the model.",
            "So we've got with highlighting faces the linear discriminant analysis, neural networks, and so on, and on the other side we have more geometrical based methods.",
            "We will try to find pretty precise parts of the face in the eyes, for example, and just to perform the decision so these second category of algorithms will use basically segmentation algorithms like skin color for example."
        ],
        [
            "So we have chosen method that you use is a preprocessing that was really robust.",
            "It's the face detector of violent Jones which uses boosted highlight features.",
            "So you think this preprocessing we can really have a fast detector with already important preprocessing, so we will focus on particular how to classify examples.",
            "Then then in our work we will use principal component analysis.",
            "Just to reduce the dimension dimensionality of the space.",
            "And finally, classification in the eigenfaces space using support vector machines in parallel way.",
            "Buy two different something new."
        ],
        [
            "So festival shot tradition of the principal component analysis.",
            "So rewrite the problem of such face detection algorithms is that we need a large set of examples in order to build the correct model, so.",
            "Before proceeding to classification, we reduce the dimensionality of the input space.",
            "So we find projection matrix matrix W here made of the cada mean and Teigen vectors of the governance metrics S and.",
            "This ensures that we minimize the reconstruction error between the reconstruction and and input image.",
            "So here you have some examples of eigenfaces.",
            "Typical images.",
            "Then you know."
        ],
        [
            "To improve.",
            "Project and the projection.",
            "With.",
            "With PCA we add another and another point that is the distance from future space.",
            "This is nothing more than the distance between the input image and its projection onto the eigenfaces space is contains some useful information and we will just add it at the end of the feature space feature vector for the classification.",
            "So we have a keypress 1 dimensional vector used for classification."
        ],
        [
            "Now show review.",
            "What is the Super vector machine algorithm?",
            "We want to find the hyperplane that correctly separates the data while maximizing the margin between positive and negative examples.",
            "So this is given by hedged level of X.",
            "Then the decision function function is just the sign of this function and we want to minimize.",
            "The square norm of the weights W and we add some slack variables.",
            "If in the case of nonlinear late lady and inseparable data.",
            "Sorry so this for something such a problem.",
            "We use LaGrange multipliers, so this is we have projected ticheli.",
            "Exit the problem so we just.",
            "If it's not enough to use such such an algorithm, we project the data into a higher dimensional space using journals.",
            "Basically radial basis functions, so polynomial kernels.",
            "As shown here at the bottom, and that Popovich and."
        ],
        [
            "Cheryl, has you have you?",
            "Just an algorithm like this for detecting faces and without using the next part of this presentation, and they already provided good results.",
            "But why do we try to improve this method?",
            "Because training such a super vector machine is quite complicated, because we need lots of examples.",
            "And and the complexity of this algorithm.",
            "So so very, very predict quadratically with the number of examples.",
            "So we will.",
            "We will try to improve it.",
            "So for this.",
            "We just take the initial data set that we have used before for VM's with positive and negative examples and we split it into N + 1 subsets by two different methods.",
            "Random sampling of by Kim K means clustering.",
            "Then with the end first subset we will turn.",
            "We will train SMS viance.",
            "Then we passed that the remaining examples through the 1st as VM's and we obtain.",
            "Second layer as them by training on the margins of.",
            "I put by the 1st SVM."
        ],
        [
            "So we have tested two different sampling techniques.",
            "The first one is only a random partitioning because we just take N + 1 subsets, independent subsets of the original data set, and we have also tested more complicated approach just just by.",
            "We take a first subset independent subset just to train the second layer SVM and then we cluster using came in K means clustering for example and the remaining examples in order to want to have M clusters for training the first layer as VM's.",
            "So we will see the results why we have done like this.",
            "So the First lady lesbian sucked when using cross validation with other business functions, kernels and interesting point is we can see that and we can view each first year as VM's as an expert and so the second layer is VM can be viewed as a function that turns.",
            "On the confidence output by the each individual experts of the first layer."
        ],
        [
            "So where is the advantage of this method we had?",
            "The complexity of older and square and then with a mixture of SVM will have the composite of in N + 1 problems of order N / N + 1 ^2.",
            "That complexity.",
            "That means that as we have much more examples than number of VM's in the first layer, we have large greater complexity again of complexity here."
        ],
        [
            "So in order to test the validity of this new approach, we have used the database made of face images from bank and extent of ETS.",
            "So something like thousands examples for training and.",
            "Something like the same for validation and on the other side we have chosen run random images and by bootstrapping we have build a set of where we have used.",
            "14,000 images for training this data in a lot of non faces images for training Alpha validation, sorry.",
            "The first experiment that you have done was to estimate the good dimensionality of the eigenfaces space.",
            "Estimates yes, and to estimate the needed number of examples to have a stabilized dimensionality.",
            "So it's shown in in in in the curve here and with many few thousands of examples we can build correct PC metrics."
        ],
        [
            "The first graph here shows how we have decomposed so data.",
            "In order to build the subsets there by random sampling of biclustering.",
            "So we have kept redoing more examples for the second layer margins people's DM because it is really quite important.",
            "The table here.",
            "Summary The behavior of each as VM so you have the events of the first layer and you can see that they are not performing very well.",
            "Something like 85% of correct classification of face examples.",
            "But the interesting point is that the second layer is here.",
            "Really improved this detections, and in the case of Camins pressuring so we have M clusters, in this case 5 clusters.",
            "Each cluster is in fact one expert on that particular domain of the deficit, and it's quite logical that the second layer is VM improve so much the detection because it just combined the decision of each expert.",
            "And then."
        ],
        [
            "In this slide, we compare that we see the generalization capabilities have this method compared to the classical SVM approach, so we can see that we have better general generalization capabilities.",
            "In the case of face detection.",
            "Moreover, we we know that in face detection particularly, we want to have quite high true positive rates.",
            "We want to detect almost all of the faces, even if we have a few false alarms.",
            "So we have improved this compared to the single support vector machine approach.",
            "Another important point is that we have effectively reduced the complexity of.",
            "The learning and the discipline.",
            "The single SVM.",
            "Because we had something like 2500 super vectors in the case of single SVM and we have largely reduced this this number, so we need less super vectors to perform better classification.",
            "So you can see here some OK, just some images taken on the bunker database processed by the virus detector.",
            "For."
        ],
        [
            "The conclusions.",
            "So we have built face detector, preprocessed by the boosted local features of aviola.",
            "So we can still have a real time processing because the cascade of IRA is really efficient for this point.",
            "For the speed of the detection, and then we can also improve the detection rates.",
            "Using this PCA plus distance from freezer space and mixture of SVM algorithm.",
            "So an improvement that we could do in the future would be to try maybe more appropriate appropriate metrics in the eigenfaces place.",
            "Because we have only used the occasion known, but it was not exactly the topic of this work.",
            "OK, so that's I think you will have lots of question.",
            "Hi I have a question for you.",
            "Show the size of the face.",
            "If he is called like used.",
            "In fact we process the images with the violence detector so this can detect also the size of the image and then we resize the image and we have used 15 by 20 pixel images so we always test images of the same size.",
            "It means 15 by 20.",
            "So we always have the same dimension dimensionality of the eigenfaces space and so.",
            "It's it's the virus detector who made the decision.",
            "So I understood at the beginning that the original motivation was to be quicker.",
            "Train because it's getting worse so slow.",
            "But in the end we didn't give us any timing formation.",
            "Was it pretty quicker?",
            "It's really quick at the train.",
            "Well, it's true I haven't.",
            "I kind of remember like this, but.",
            "Yeah, you have enough.",
            "We need less support vector, so it's truly a few more efficient for training for testing, but the improvement in terms of time is really important compared to the single lesbian.",
            "I was wondering if you were compared you approach to other architectures for multiple classifier systems.",
            "So for example, in begging you have subsets for the subsets are overlapping.",
            "It's a disjoint, and it's another example.",
            "You could replace the second support vector machine layer by a fixed combination rule that doesn't get to retrain program through someone so so, so do you think into this direction.",
            "Find that you mentioned here.",
            "Maybe we should try this because it should be interesting, but we haven't tried yet.",
            "But yes, you're right.",
            "Should be a good idea to do.",
            "Yeah, thank you for the.",
            "Just curious.",
            "I dislike 13.",
            "This one so it looks like there the pairing this this English speaking with them all folder.",
            "There are different operating points right?",
            "So they have different tradeoff between the scores on faces on faces.",
            "Did you look at?",
            "It certainly can set the operating point in other ways.",
            "So did you look at how there if they were the same on one of the other saying if you set it so that it was.",
            "Still not as good, but wanted to see if it is better than the other.",
            "Never been clear.",
            "Yes, I understand what you mean, but it is true that it's not really.",
            "We should have won all the Roc curve because it's not quite clear to put some numbers like this, because OK, if you increase the number of correct classified faces would decrease the number of correctly.",
            "Non face is classified but we haven't tested on particular data set without changing crystal.",
            "We should have done it.",
            "Yeah, we should have checked.",
            "Yeah, you're right."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, I'm going to present you the work that we're doing at the FFL about face detection.",
                    "label": 0
                },
                {
                    "sent": "So we use a mixture of super big dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the outline of this presentation after so presentation of the context of his detection, I will explain approach with three main steps in the presentation.",
                    "label": 0
                },
                {
                    "sent": "So the principal component analysis decomposition.",
                    "label": 1
                },
                {
                    "sent": "Then I would give a few words by about the classical classification with SVM's.",
                    "label": 0
                },
                {
                    "sent": "Then I will introduce a new structure of.",
                    "label": 1
                },
                {
                    "sent": "Perfect machines in parallel, called mixtures of support vector machines.",
                    "label": 1
                },
                {
                    "sent": "There is some experiments and results and I draw some conclusions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Context of face detection.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is to detect faces in images or video sequences.",
                    "label": 0
                },
                {
                    "sent": "In this work, we will focus only on frontal faces.",
                    "label": 0
                },
                {
                    "sent": "We can distinguish 2 main approaches that are commonly used in face detection.",
                    "label": 1
                },
                {
                    "sent": "Basically we have the image based detections where we consider face as the following checked and we use machine learning algorithms to build the model.",
                    "label": 1
                },
                {
                    "sent": "So we've got with highlighting faces the linear discriminant analysis, neural networks, and so on, and on the other side we have more geometrical based methods.",
                    "label": 0
                },
                {
                    "sent": "We will try to find pretty precise parts of the face in the eyes, for example, and just to perform the decision so these second category of algorithms will use basically segmentation algorithms like skin color for example.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have chosen method that you use is a preprocessing that was really robust.",
                    "label": 0
                },
                {
                    "sent": "It's the face detector of violent Jones which uses boosted highlight features.",
                    "label": 1
                },
                {
                    "sent": "So you think this preprocessing we can really have a fast detector with already important preprocessing, so we will focus on particular how to classify examples.",
                    "label": 0
                },
                {
                    "sent": "Then then in our work we will use principal component analysis.",
                    "label": 1
                },
                {
                    "sent": "Just to reduce the dimension dimensionality of the space.",
                    "label": 0
                },
                {
                    "sent": "And finally, classification in the eigenfaces space using support vector machines in parallel way.",
                    "label": 0
                },
                {
                    "sent": "Buy two different something new.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So festival shot tradition of the principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "So rewrite the problem of such face detection algorithms is that we need a large set of examples in order to build the correct model, so.",
                    "label": 0
                },
                {
                    "sent": "Before proceeding to classification, we reduce the dimensionality of the input space.",
                    "label": 0
                },
                {
                    "sent": "So we find projection matrix matrix W here made of the cada mean and Teigen vectors of the governance metrics S and.",
                    "label": 0
                },
                {
                    "sent": "This ensures that we minimize the reconstruction error between the reconstruction and and input image.",
                    "label": 0
                },
                {
                    "sent": "So here you have some examples of eigenfaces.",
                    "label": 0
                },
                {
                    "sent": "Typical images.",
                    "label": 0
                },
                {
                    "sent": "Then you know.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To improve.",
                    "label": 0
                },
                {
                    "sent": "Project and the projection.",
                    "label": 0
                },
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "With PCA we add another and another point that is the distance from future space.",
                    "label": 1
                },
                {
                    "sent": "This is nothing more than the distance between the input image and its projection onto the eigenfaces space is contains some useful information and we will just add it at the end of the feature space feature vector for the classification.",
                    "label": 1
                },
                {
                    "sent": "So we have a keypress 1 dimensional vector used for classification.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now show review.",
                    "label": 0
                },
                {
                    "sent": "What is the Super vector machine algorithm?",
                    "label": 0
                },
                {
                    "sent": "We want to find the hyperplane that correctly separates the data while maximizing the margin between positive and negative examples.",
                    "label": 1
                },
                {
                    "sent": "So this is given by hedged level of X.",
                    "label": 0
                },
                {
                    "sent": "Then the decision function function is just the sign of this function and we want to minimize.",
                    "label": 0
                },
                {
                    "sent": "The square norm of the weights W and we add some slack variables.",
                    "label": 0
                },
                {
                    "sent": "If in the case of nonlinear late lady and inseparable data.",
                    "label": 0
                },
                {
                    "sent": "Sorry so this for something such a problem.",
                    "label": 1
                },
                {
                    "sent": "We use LaGrange multipliers, so this is we have projected ticheli.",
                    "label": 0
                },
                {
                    "sent": "Exit the problem so we just.",
                    "label": 0
                },
                {
                    "sent": "If it's not enough to use such such an algorithm, we project the data into a higher dimensional space using journals.",
                    "label": 0
                },
                {
                    "sent": "Basically radial basis functions, so polynomial kernels.",
                    "label": 0
                },
                {
                    "sent": "As shown here at the bottom, and that Popovich and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cheryl, has you have you?",
                    "label": 0
                },
                {
                    "sent": "Just an algorithm like this for detecting faces and without using the next part of this presentation, and they already provided good results.",
                    "label": 0
                },
                {
                    "sent": "But why do we try to improve this method?",
                    "label": 0
                },
                {
                    "sent": "Because training such a super vector machine is quite complicated, because we need lots of examples.",
                    "label": 0
                },
                {
                    "sent": "And and the complexity of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So so very, very predict quadratically with the number of examples.",
                    "label": 0
                },
                {
                    "sent": "So we will.",
                    "label": 0
                },
                {
                    "sent": "We will try to improve it.",
                    "label": 0
                },
                {
                    "sent": "So for this.",
                    "label": 0
                },
                {
                    "sent": "We just take the initial data set that we have used before for VM's with positive and negative examples and we split it into N + 1 subsets by two different methods.",
                    "label": 0
                },
                {
                    "sent": "Random sampling of by Kim K means clustering.",
                    "label": 1
                },
                {
                    "sent": "Then with the end first subset we will turn.",
                    "label": 0
                },
                {
                    "sent": "We will train SMS viance.",
                    "label": 0
                },
                {
                    "sent": "Then we passed that the remaining examples through the 1st as VM's and we obtain.",
                    "label": 0
                },
                {
                    "sent": "Second layer as them by training on the margins of.",
                    "label": 1
                },
                {
                    "sent": "I put by the 1st SVM.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have tested two different sampling techniques.",
                    "label": 0
                },
                {
                    "sent": "The first one is only a random partitioning because we just take N + 1 subsets, independent subsets of the original data set, and we have also tested more complicated approach just just by.",
                    "label": 0
                },
                {
                    "sent": "We take a first subset independent subset just to train the second layer SVM and then we cluster using came in K means clustering for example and the remaining examples in order to want to have M clusters for training the first layer as VM's.",
                    "label": 0
                },
                {
                    "sent": "So we will see the results why we have done like this.",
                    "label": 0
                },
                {
                    "sent": "So the First lady lesbian sucked when using cross validation with other business functions, kernels and interesting point is we can see that and we can view each first year as VM's as an expert and so the second layer is VM can be viewed as a function that turns.",
                    "label": 0
                },
                {
                    "sent": "On the confidence output by the each individual experts of the first layer.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So where is the advantage of this method we had?",
                    "label": 0
                },
                {
                    "sent": "The complexity of older and square and then with a mixture of SVM will have the composite of in N + 1 problems of order N / N + 1 ^2.",
                    "label": 1
                },
                {
                    "sent": "That complexity.",
                    "label": 1
                },
                {
                    "sent": "That means that as we have much more examples than number of VM's in the first layer, we have large greater complexity again of complexity here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to test the validity of this new approach, we have used the database made of face images from bank and extent of ETS.",
                    "label": 1
                },
                {
                    "sent": "So something like thousands examples for training and.",
                    "label": 0
                },
                {
                    "sent": "Something like the same for validation and on the other side we have chosen run random images and by bootstrapping we have build a set of where we have used.",
                    "label": 1
                },
                {
                    "sent": "14,000 images for training this data in a lot of non faces images for training Alpha validation, sorry.",
                    "label": 1
                },
                {
                    "sent": "The first experiment that you have done was to estimate the good dimensionality of the eigenfaces space.",
                    "label": 1
                },
                {
                    "sent": "Estimates yes, and to estimate the needed number of examples to have a stabilized dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So it's shown in in in in the curve here and with many few thousands of examples we can build correct PC metrics.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first graph here shows how we have decomposed so data.",
                    "label": 0
                },
                {
                    "sent": "In order to build the subsets there by random sampling of biclustering.",
                    "label": 0
                },
                {
                    "sent": "So we have kept redoing more examples for the second layer margins people's DM because it is really quite important.",
                    "label": 0
                },
                {
                    "sent": "The table here.",
                    "label": 0
                },
                {
                    "sent": "Summary The behavior of each as VM so you have the events of the first layer and you can see that they are not performing very well.",
                    "label": 0
                },
                {
                    "sent": "Something like 85% of correct classification of face examples.",
                    "label": 0
                },
                {
                    "sent": "But the interesting point is that the second layer is here.",
                    "label": 0
                },
                {
                    "sent": "Really improved this detections, and in the case of Camins pressuring so we have M clusters, in this case 5 clusters.",
                    "label": 0
                },
                {
                    "sent": "Each cluster is in fact one expert on that particular domain of the deficit, and it's quite logical that the second layer is VM improve so much the detection because it just combined the decision of each expert.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this slide, we compare that we see the generalization capabilities have this method compared to the classical SVM approach, so we can see that we have better general generalization capabilities.",
                    "label": 0
                },
                {
                    "sent": "In the case of face detection.",
                    "label": 0
                },
                {
                    "sent": "Moreover, we we know that in face detection particularly, we want to have quite high true positive rates.",
                    "label": 1
                },
                {
                    "sent": "We want to detect almost all of the faces, even if we have a few false alarms.",
                    "label": 0
                },
                {
                    "sent": "So we have improved this compared to the single support vector machine approach.",
                    "label": 0
                },
                {
                    "sent": "Another important point is that we have effectively reduced the complexity of.",
                    "label": 0
                },
                {
                    "sent": "The learning and the discipline.",
                    "label": 1
                },
                {
                    "sent": "The single SVM.",
                    "label": 1
                },
                {
                    "sent": "Because we had something like 2500 super vectors in the case of single SVM and we have largely reduced this this number, so we need less super vectors to perform better classification.",
                    "label": 0
                },
                {
                    "sent": "So you can see here some OK, just some images taken on the bunker database processed by the virus detector.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The conclusions.",
                    "label": 0
                },
                {
                    "sent": "So we have built face detector, preprocessed by the boosted local features of aviola.",
                    "label": 1
                },
                {
                    "sent": "So we can still have a real time processing because the cascade of IRA is really efficient for this point.",
                    "label": 0
                },
                {
                    "sent": "For the speed of the detection, and then we can also improve the detection rates.",
                    "label": 0
                },
                {
                    "sent": "Using this PCA plus distance from freezer space and mixture of SVM algorithm.",
                    "label": 0
                },
                {
                    "sent": "So an improvement that we could do in the future would be to try maybe more appropriate appropriate metrics in the eigenfaces place.",
                    "label": 1
                },
                {
                    "sent": "Because we have only used the occasion known, but it was not exactly the topic of this work.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's I think you will have lots of question.",
                    "label": 0
                },
                {
                    "sent": "Hi I have a question for you.",
                    "label": 0
                },
                {
                    "sent": "Show the size of the face.",
                    "label": 0
                },
                {
                    "sent": "If he is called like used.",
                    "label": 0
                },
                {
                    "sent": "In fact we process the images with the violence detector so this can detect also the size of the image and then we resize the image and we have used 15 by 20 pixel images so we always test images of the same size.",
                    "label": 0
                },
                {
                    "sent": "It means 15 by 20.",
                    "label": 0
                },
                {
                    "sent": "So we always have the same dimension dimensionality of the eigenfaces space and so.",
                    "label": 1
                },
                {
                    "sent": "It's it's the virus detector who made the decision.",
                    "label": 0
                },
                {
                    "sent": "So I understood at the beginning that the original motivation was to be quicker.",
                    "label": 0
                },
                {
                    "sent": "Train because it's getting worse so slow.",
                    "label": 0
                },
                {
                    "sent": "But in the end we didn't give us any timing formation.",
                    "label": 0
                },
                {
                    "sent": "Was it pretty quicker?",
                    "label": 0
                },
                {
                    "sent": "It's really quick at the train.",
                    "label": 0
                },
                {
                    "sent": "Well, it's true I haven't.",
                    "label": 0
                },
                {
                    "sent": "I kind of remember like this, but.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you have enough.",
                    "label": 0
                },
                {
                    "sent": "We need less support vector, so it's truly a few more efficient for training for testing, but the improvement in terms of time is really important compared to the single lesbian.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if you were compared you approach to other architectures for multiple classifier systems.",
                    "label": 0
                },
                {
                    "sent": "So for example, in begging you have subsets for the subsets are overlapping.",
                    "label": 0
                },
                {
                    "sent": "It's a disjoint, and it's another example.",
                    "label": 0
                },
                {
                    "sent": "You could replace the second support vector machine layer by a fixed combination rule that doesn't get to retrain program through someone so so, so do you think into this direction.",
                    "label": 0
                },
                {
                    "sent": "Find that you mentioned here.",
                    "label": 0
                },
                {
                    "sent": "Maybe we should try this because it should be interesting, but we haven't tried yet.",
                    "label": 0
                },
                {
                    "sent": "But yes, you're right.",
                    "label": 0
                },
                {
                    "sent": "Should be a good idea to do.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you for the.",
                    "label": 0
                },
                {
                    "sent": "Just curious.",
                    "label": 0
                },
                {
                    "sent": "I dislike 13.",
                    "label": 0
                },
                {
                    "sent": "This one so it looks like there the pairing this this English speaking with them all folder.",
                    "label": 0
                },
                {
                    "sent": "There are different operating points right?",
                    "label": 0
                },
                {
                    "sent": "So they have different tradeoff between the scores on faces on faces.",
                    "label": 0
                },
                {
                    "sent": "Did you look at?",
                    "label": 0
                },
                {
                    "sent": "It certainly can set the operating point in other ways.",
                    "label": 0
                },
                {
                    "sent": "So did you look at how there if they were the same on one of the other saying if you set it so that it was.",
                    "label": 0
                },
                {
                    "sent": "Still not as good, but wanted to see if it is better than the other.",
                    "label": 0
                },
                {
                    "sent": "Never been clear.",
                    "label": 0
                },
                {
                    "sent": "Yes, I understand what you mean, but it is true that it's not really.",
                    "label": 0
                },
                {
                    "sent": "We should have won all the Roc curve because it's not quite clear to put some numbers like this, because OK, if you increase the number of correct classified faces would decrease the number of correctly.",
                    "label": 0
                },
                {
                    "sent": "Non face is classified but we haven't tested on particular data set without changing crystal.",
                    "label": 0
                },
                {
                    "sent": "We should have done it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we should have checked.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you're right.",
                    "label": 0
                }
            ]
        }
    }
}