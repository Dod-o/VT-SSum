{
    "id": "mfdnbeuqr4q7qafr52qpw4jgnspiaavn",
    "title": "Efficient Graph-based Document Similarity",
    "info": {
        "author": [
            "Christian Paul, IBM Deutschland GmbH"
        ],
        "published": "July 28, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_paul_document_similarity/",
    "segmentation": [
        [
            "Hi, my name is Christian Paul and I'll be presenting our work on efficient graph based document similarity.",
            "So this word came out of a collaboration of AFP at the Culture Institute of Technology and the ISI at the University of Southern California.",
            "Where most of the work was done when I was there for a research stay for my Masters thesis."
        ],
        [
            "Alright, so.",
            "Why did we get into the whole story?",
            "This doesn't work OK. Why did we get into the whole issue of efficient document similarity?",
            "The main motivation was that we wanted to be able to use knowledge based similarity in a setting of related document search.",
            "So in this scenario.",
            "We basically have one, let's call it query document here.",
            "An out of a potentially very large collection.",
            "Over here we want to be able to find those documents that are most related to it and rank them by this relevancy.",
            "And of course, in this case we don't want to go there and just compute pairwise similarities for each document in the corpus, because that's just not going to scale.",
            "So we're going to need an efficient retrieval mechanism along with a very good.",
            "Similarity measure alright.",
            "So to make this more tangible for, look at this example, you're going to see that we're probably going to be interested in matching it.",
            "In finding this one out of potentially very many documents."
        ],
        [
            "And just starting with a very simple measure, if we just matched on surface form of words.",
            "Then we're probably going to have Apple matching with Apple, although we can see easily that those two points are very different meanings of the word.",
            "So that can be very misleading."
        ],
        [
            "Another statistical approach to it maybe, and that's also very common to look at cooccurrence understanding and analyzing how words appear together in documents.",
            "And just saying that if they appear together frequently, they maybe they should be related in some way, so this can be very helpful like it's it's very clear that iPhone and Apple are related, but it may also be misleading like in the juice case up there, or there may.",
            "It's also be cases where words are very related to each other, but almost never appear together like you sometimes have that with synonyms like street and road or something like that.",
            "Alright, So what we can do if we use semantic technologies is basically two things.",
            "First, we can resolve the ambiguity in those words by using an appropriate entity extraction tool and linking, for example, that that mention of Apple to the actual company, and that one sorry to the Apple juice.",
            "And thus it will be hard to find a certain connection between those two.",
            "At the same time, what we can do then is by working on the entities instead of the plaintext.",
            "We can analyze the knowledge graph that they were extracted from and make a very elaborate decision on how these entities extracted from different documents.",
            "Actually, well, yeah, are connected or related.",
            "And also in this example it will probably point us towards the one that we were looking for.",
            "OK, so."
        ],
        [
            "This is exactly what we wanted.",
            "However, one thing that we have to take care of is we don't want to do this very expensive graph traversal a lot, especially not if we have a very large corpus and we have to do this for every document.",
            "So looking at."
        ],
        [
            "They did work.",
            "We basically found those two sides here, which is on the one hand, the statistical the distributional approaches, where you start off with TF IDF very classic approaches and you go down to more elaborate ones like the explicit semantic analysis that basically uses all of Wikipedia articles to build up statistics and then Maps documents into the space of Wikipedia articles.",
            "And thus uses a lot of extra information and can work very well.",
            "However, still it neglects all that is out there in the semantic web world, and this is what the approaches in the right side will use more.",
            "So for example, those two up there are examples for entity similarities using path based similarity measures.",
            "Then those three down there all considered groups of entities.",
            "Actually, the two ones, the ones by newness and Shoemaker, and consider those actually.",
            "Also, target document similarities and are quite similar to our approach.",
            "Where, for example, the one by Schumacher income said oh uses a graph edit distance between documents that are represented as a sub graph in a large knowledge graph.",
            "So.",
            "Especially from the one by Nunez and the one down there by Vidal.",
            "About annotation similarity.",
            "We took some of their measures that they used or we had them inspire us for what we worked on and that's what I'm going to show next."
        ],
        [
            "Because what we want to do is get in that gap.",
            "Basically, try to be efficient have this.",
            "This ability to work in the related document search setting at the same time have a very elaborate.",
            "Similarity measure"
        ],
        [
            "And this brings us to our core contributions.",
            "That we see and first of all, a scalable process for doing rate document search with a knowledge based similarity measure.",
            "That builds on doing all the heavy lifting.",
            "All the graph traversal and all that in a preprocessing stage so that search time all we have to do is very lightweight tasks to actually compute the similarity.",
            "And with this we achieve very similar performance to that of statistical approaches.",
            "The second one is the actual similarity measure that we use that we compute based on a bag of entity representation of documents.",
            "And that user."
        ],
        [
            "Different types of edges in the knowledge graph, which we call hierarchical and transversal, which means hierarchical.",
            "Just putting entities in a certain category or group and transversal, or we call just semantic links that are non hierarchical.",
            "And using this.",
            "In our experiments, we were able to achieve better, better scores, better nausha, better correlation with the human notion of similarity than we saw in any other related work."
        ],
        [
            "OK, so let's start with the approach.",
            "And here with the process for the related document search that puts A-frame around the next slides to come to.",
            "So it all starts with the semantic document expansion step.",
            "Given a document, just a new one that comes in and we want to determine or find related documents for it, we first take that draw document, then expanded with with relational knowledge from a graph.",
            "We then add it to the corpus.",
            "We indexed it as well for later retrieval.",
            "And then in the steps three and four we run a two step search mechanism that first does a rather rough approximation.",
            "It generates a candidate set that will then in the next step in the fourth step.",
            "Explore more in a more fine grade scale and going to go on the entity level and determine how similar those are.",
            "And we're now going to go through."
        ],
        [
            "Steps, so in the expansion step what we do as mentioned before is explored.",
            "Two types of edges in the graph.",
            "So assuming we have a document annotated with knowledge graph entities, we then go into the graph and get all its categories that those entities are in, as well as all the ancestors of those categories and store those along with the entity in the expanded document.",
            "On the transversal side, we just explore the neighborhood of entities, what, wherever they are, linking whatever entities we encounter, we determine how many paths go there and how long those paths are and assign a score based on that."
        ],
        [
            "After we do this, we have this expanded document and we indexed that in the corpus and the corpus as well.",
            "So when we actually go into searching.",
            "We have something that looks something like that.",
            "So assuming we have this query document again here in expanded state we want to do in the first search step is find documents that in some rough level just overall document entity domain that we kind of extracted.",
            "Want to find what overlaps.",
            "So we have an inverted index from the entities practically to the documents that they occur in.",
            "And in this way we're just going to extract those that have an overlap and rank them by the number of entities that overlap.",
            "And generate a canister candidate said like this.",
            "This is the assumption that if they overlap that they're just going to be similar on a kind of contextual level without actually going into the details of how single entities pair up.",
            "This is what we do."
        ],
        [
            "In the next step.",
            "We're we reconstruct from those expanded documents.",
            "The sub graph that spans between two documents, and we do this for every candidate document that we that we found earlier.",
            "And here again we use the information stored earlier and go into and compute hierarchical entrance versus similarity.",
            "We then want to use those single scores between the entities and combine them into one single document score.",
            "So the."
        ],
        [
            "So we do.",
            "This is first computer optical similarity for each entity.",
            "Which builds on that formula that basically that heavily relies on the concept of lowest common ancestor between the categories of those two entities.",
            "And it will rank something higher if the lowest common ancestor is rather low rather far from the root, because that.",
            "Kind of implies that it's very special that the that the similarities are very specific.",
            "And in this example, because the L say the green box here is very close to the root of the score, will be rather low with a .2 in this example."
        ],
        [
            "On the transversal side, we use the weights we stored earlier for each of the nodes in the neighborhood and just multiply those which is going to give us a score that follows this formula.",
            "Which again will penalize paths the longer they are.",
            "And also will count give a higher score in the end if more paths connect, so they connect those two.",
            "Again, in this example, we yield a score of .5 just.",
            "As an example."
        ],
        [
            "So after we've done those after we've computed hierarchical N transversal similarity, want to combine those two with a bit of normalization going on into one score between each pair of concepts?",
            "When we have that this theoretically gives us a bipartite graph between the document annotations.",
            "Now we're not going to use all the edges in that bipartite graph, because we argue that adds a lot of noise, so we select the sub graph of that and the way we do that is just extracting for each annotation the outgoing edge with the highest score.",
            "We call this the Max graph in this case.",
            "And you can see this in the bold lines and based on those bold edges, we can then compute the formula down there.",
            "That just sums up those maximum scores and divides them or normalizes them by the.",
            "Overall number of annotations in both documents."
        ],
        [
            "In this example, with DB pedia entities and probably 2 documents that are concerned with basketball.",
            "You can see the bold lines again, pretty much spanning where you would intuitively expect them to span between those two players between the two cities.",
            "In the state, Texas, and maybe to the state.",
            "So we only select those because that we we assume those are more meaningful for similarity.",
            "And then just put it into that formula and get that score of .63 in this case."
        ],
        [
            "Right, so let's now look into evaluation.",
            "And we use three different datasets in there of two types, basically once.",
            "We use the Lee 50 corpus that is very far spread.",
            "Very well known, has been cited hundreds of times and has been used in all the related work we could found.",
            "It find basically.",
            "And that is concerned with the document similarity, while the other two sets we used were about sentence similarity.",
            "So that was the motivation for us to see how far we could go in terms of reducing the number of entities that we can find in the text.",
            "And if we can still achieve good results.",
            "We use DB pedia data set of 2014 and the effort mentioned X Lisa tool of the FB for entity extraction."
        ],
        [
            "In the first part, document similarity.",
            "The Lee 50 corpus consists of 50 documents of rather short news articles all out of Australia, and there's also a gold standard set of similarities that were assigned by humans for all pairs of documents so that the task again is to measure the correlation of our approach with that of humans.",
            "And this correlation here expressed in the Pearson Spearman correlation numbers and the harmonic mean between those.",
            "Quite good, we're quite happy to see that we were able to outperform both the statistical approaches, LSA, ESA etc as well as the graph edit distance one by Schumer.",
            "Crampton said I mentioned earlier.",
            "So yeah, that was that was really good to see.",
            "And."
        ],
        [
            "We went into sentence similarity where on those two datasets, which are ones video descriptions and the other Flickr image descriptions, we also found that if we could at least extract one entity and focused on those sentences, because otherwise knowledge based approach is rather hard.",
            "Then comparing us to the unsupervised approaches that we could find listed there.",
            "We also achieved very good numbers and were able to outperform the competition.",
            "Just a quick note here, the GBS S is supposed to mean graph based, some semantic similarity and the little R subscript is supposed to represent the radius that we used in the document expansion.",
            "So it says, how far do we go out from the original annotations to explore the neighborhood and Add all this information to the document?"
        ],
        [
            "Alright, as one last piece of.",
            "Evaluation this.",
            "Graph basically has three takeaways, I'd say.",
            "Which is first of all, from the red line to the blue line.",
            "So from pre search to full search the ranking quality that we measure.",
            "Ranking quality is measured in normalized DCG and correlates with our our related document search use case.",
            "We see a clear increase, so after we extract we generate that candidate set on that rough document, wide entity overlap.",
            "We get a pretty decent quality of over .8.",
            "But if we then go into the next step and reorder the documents according to their entity level scores, we see a large increase in those numbers.",
            "Which is which is really cool.",
            "The second thing we can see is that the larger the candidate set size, obviously the better the results, because the smaller it is, the more documents may be missed before they actually get to that last step of entity level similarity.",
            "But we can also see that starting of a with a candidate set of 15 documents, we already achieve very good performance.",
            "Although granted, this was a very small data set that was the least 50 data set.",
            "And the third takeaway, I think, is that on that Gray line you can see the time consumption depending on the candidate set size.",
            "So we see a linear increase in the with the number of documents as candidates.",
            "And the only real dependence on the overall corpus size is in the inverted index in the extraction in the candidate set generation.",
            "And this is something that scales very well and is something that other statistical approaches do as well.",
            "So that in this final step here we can really configure that by just setting the candidate set size to something that is appropriate for us."
        ],
        [
            "Finally, just wrapping it up, we've presented a work about efficient graph based document similarity that combines hierarchical and transversal entity level similarities out of a knowledge graph.",
            "And with this is able to outperform both the distribution, ull and the knowledge based related approaches that we found on two different types of data set.",
            "At the same time, we're also able to run it very efficiently so that we can also use it in that related document search use case.",
            "During that work we understood a lot of new things, but some lessons learned here are definitely that there is value in the DB pedia relational knowledge for things like semantic similarity.",
            "So that worked quite well.",
            "And on the other side, what was interesting was that the more entities we could get our hands on for a document, the better it seemed to work generally.",
            "And what we argued was that if you just considered one word and you try to extract an entity from that, then you're probably not going to have that much success with disambiguation using an entity extractor that you're going to have.",
            "If you at least have a sentence.",
            "So if we have at least that context to use the disambiguation, that's going to help us already.",
            "And then when you have, we have many more entities.",
            "Then what I think really kicks in is the maximum entity edge selection that we do in that Max graph step at the end, which.",
            "Really puts an emphasis on the more meaningful relations.",
            "And is not too strict.",
            "Not doing a 11 matching or something, but really just emphasizes those that seem to be important.",
            "And that's it.",
            "I thank you very much for your time and I'm looking forward to your questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, my name is Christian Paul and I'll be presenting our work on efficient graph based document similarity.",
                    "label": 1
                },
                {
                    "sent": "So this word came out of a collaboration of AFP at the Culture Institute of Technology and the ISI at the University of Southern California.",
                    "label": 0
                },
                {
                    "sent": "Where most of the work was done when I was there for a research stay for my Masters thesis.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Why did we get into the whole story?",
                    "label": 0
                },
                {
                    "sent": "This doesn't work OK. Why did we get into the whole issue of efficient document similarity?",
                    "label": 0
                },
                {
                    "sent": "The main motivation was that we wanted to be able to use knowledge based similarity in a setting of related document search.",
                    "label": 0
                },
                {
                    "sent": "So in this scenario.",
                    "label": 0
                },
                {
                    "sent": "We basically have one, let's call it query document here.",
                    "label": 0
                },
                {
                    "sent": "An out of a potentially very large collection.",
                    "label": 0
                },
                {
                    "sent": "Over here we want to be able to find those documents that are most related to it and rank them by this relevancy.",
                    "label": 0
                },
                {
                    "sent": "And of course, in this case we don't want to go there and just compute pairwise similarities for each document in the corpus, because that's just not going to scale.",
                    "label": 0
                },
                {
                    "sent": "So we're going to need an efficient retrieval mechanism along with a very good.",
                    "label": 0
                },
                {
                    "sent": "Similarity measure alright.",
                    "label": 0
                },
                {
                    "sent": "So to make this more tangible for, look at this example, you're going to see that we're probably going to be interested in matching it.",
                    "label": 0
                },
                {
                    "sent": "In finding this one out of potentially very many documents.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just starting with a very simple measure, if we just matched on surface form of words.",
                    "label": 0
                },
                {
                    "sent": "Then we're probably going to have Apple matching with Apple, although we can see easily that those two points are very different meanings of the word.",
                    "label": 0
                },
                {
                    "sent": "So that can be very misleading.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another statistical approach to it maybe, and that's also very common to look at cooccurrence understanding and analyzing how words appear together in documents.",
                    "label": 0
                },
                {
                    "sent": "And just saying that if they appear together frequently, they maybe they should be related in some way, so this can be very helpful like it's it's very clear that iPhone and Apple are related, but it may also be misleading like in the juice case up there, or there may.",
                    "label": 0
                },
                {
                    "sent": "It's also be cases where words are very related to each other, but almost never appear together like you sometimes have that with synonyms like street and road or something like that.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what we can do if we use semantic technologies is basically two things.",
                    "label": 0
                },
                {
                    "sent": "First, we can resolve the ambiguity in those words by using an appropriate entity extraction tool and linking, for example, that that mention of Apple to the actual company, and that one sorry to the Apple juice.",
                    "label": 0
                },
                {
                    "sent": "And thus it will be hard to find a certain connection between those two.",
                    "label": 0
                },
                {
                    "sent": "At the same time, what we can do then is by working on the entities instead of the plaintext.",
                    "label": 0
                },
                {
                    "sent": "We can analyze the knowledge graph that they were extracted from and make a very elaborate decision on how these entities extracted from different documents.",
                    "label": 0
                },
                {
                    "sent": "Actually, well, yeah, are connected or related.",
                    "label": 0
                },
                {
                    "sent": "And also in this example it will probably point us towards the one that we were looking for.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is exactly what we wanted.",
                    "label": 0
                },
                {
                    "sent": "However, one thing that we have to take care of is we don't want to do this very expensive graph traversal a lot, especially not if we have a very large corpus and we have to do this for every document.",
                    "label": 0
                },
                {
                    "sent": "So looking at.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They did work.",
                    "label": 0
                },
                {
                    "sent": "We basically found those two sides here, which is on the one hand, the statistical the distributional approaches, where you start off with TF IDF very classic approaches and you go down to more elaborate ones like the explicit semantic analysis that basically uses all of Wikipedia articles to build up statistics and then Maps documents into the space of Wikipedia articles.",
                    "label": 1
                },
                {
                    "sent": "And thus uses a lot of extra information and can work very well.",
                    "label": 0
                },
                {
                    "sent": "However, still it neglects all that is out there in the semantic web world, and this is what the approaches in the right side will use more.",
                    "label": 0
                },
                {
                    "sent": "So for example, those two up there are examples for entity similarities using path based similarity measures.",
                    "label": 0
                },
                {
                    "sent": "Then those three down there all considered groups of entities.",
                    "label": 0
                },
                {
                    "sent": "Actually, the two ones, the ones by newness and Shoemaker, and consider those actually.",
                    "label": 0
                },
                {
                    "sent": "Also, target document similarities and are quite similar to our approach.",
                    "label": 0
                },
                {
                    "sent": "Where, for example, the one by Schumacher income said oh uses a graph edit distance between documents that are represented as a sub graph in a large knowledge graph.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Especially from the one by Nunez and the one down there by Vidal.",
                    "label": 0
                },
                {
                    "sent": "About annotation similarity.",
                    "label": 0
                },
                {
                    "sent": "We took some of their measures that they used or we had them inspire us for what we worked on and that's what I'm going to show next.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because what we want to do is get in that gap.",
                    "label": 0
                },
                {
                    "sent": "Basically, try to be efficient have this.",
                    "label": 0
                },
                {
                    "sent": "This ability to work in the related document search setting at the same time have a very elaborate.",
                    "label": 0
                },
                {
                    "sent": "Similarity measure",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this brings us to our core contributions.",
                    "label": 1
                },
                {
                    "sent": "That we see and first of all, a scalable process for doing rate document search with a knowledge based similarity measure.",
                    "label": 0
                },
                {
                    "sent": "That builds on doing all the heavy lifting.",
                    "label": 0
                },
                {
                    "sent": "All the graph traversal and all that in a preprocessing stage so that search time all we have to do is very lightweight tasks to actually compute the similarity.",
                    "label": 1
                },
                {
                    "sent": "And with this we achieve very similar performance to that of statistical approaches.",
                    "label": 0
                },
                {
                    "sent": "The second one is the actual similarity measure that we use that we compute based on a bag of entity representation of documents.",
                    "label": 0
                },
                {
                    "sent": "And that user.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different types of edges in the knowledge graph, which we call hierarchical and transversal, which means hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Just putting entities in a certain category or group and transversal, or we call just semantic links that are non hierarchical.",
                    "label": 0
                },
                {
                    "sent": "And using this.",
                    "label": 0
                },
                {
                    "sent": "In our experiments, we were able to achieve better, better scores, better nausha, better correlation with the human notion of similarity than we saw in any other related work.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's start with the approach.",
                    "label": 0
                },
                {
                    "sent": "And here with the process for the related document search that puts A-frame around the next slides to come to.",
                    "label": 0
                },
                {
                    "sent": "So it all starts with the semantic document expansion step.",
                    "label": 1
                },
                {
                    "sent": "Given a document, just a new one that comes in and we want to determine or find related documents for it, we first take that draw document, then expanded with with relational knowledge from a graph.",
                    "label": 0
                },
                {
                    "sent": "We then add it to the corpus.",
                    "label": 0
                },
                {
                    "sent": "We indexed it as well for later retrieval.",
                    "label": 1
                },
                {
                    "sent": "And then in the steps three and four we run a two step search mechanism that first does a rather rough approximation.",
                    "label": 0
                },
                {
                    "sent": "It generates a candidate set that will then in the next step in the fourth step.",
                    "label": 0
                },
                {
                    "sent": "Explore more in a more fine grade scale and going to go on the entity level and determine how similar those are.",
                    "label": 0
                },
                {
                    "sent": "And we're now going to go through.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Steps, so in the expansion step what we do as mentioned before is explored.",
                    "label": 0
                },
                {
                    "sent": "Two types of edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "So assuming we have a document annotated with knowledge graph entities, we then go into the graph and get all its categories that those entities are in, as well as all the ancestors of those categories and store those along with the entity in the expanded document.",
                    "label": 0
                },
                {
                    "sent": "On the transversal side, we just explore the neighborhood of entities, what, wherever they are, linking whatever entities we encounter, we determine how many paths go there and how long those paths are and assign a score based on that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After we do this, we have this expanded document and we indexed that in the corpus and the corpus as well.",
                    "label": 0
                },
                {
                    "sent": "So when we actually go into searching.",
                    "label": 0
                },
                {
                    "sent": "We have something that looks something like that.",
                    "label": 0
                },
                {
                    "sent": "So assuming we have this query document again here in expanded state we want to do in the first search step is find documents that in some rough level just overall document entity domain that we kind of extracted.",
                    "label": 0
                },
                {
                    "sent": "Want to find what overlaps.",
                    "label": 0
                },
                {
                    "sent": "So we have an inverted index from the entities practically to the documents that they occur in.",
                    "label": 0
                },
                {
                    "sent": "And in this way we're just going to extract those that have an overlap and rank them by the number of entities that overlap.",
                    "label": 0
                },
                {
                    "sent": "And generate a canister candidate said like this.",
                    "label": 0
                },
                {
                    "sent": "This is the assumption that if they overlap that they're just going to be similar on a kind of contextual level without actually going into the details of how single entities pair up.",
                    "label": 0
                },
                {
                    "sent": "This is what we do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the next step.",
                    "label": 0
                },
                {
                    "sent": "We're we reconstruct from those expanded documents.",
                    "label": 0
                },
                {
                    "sent": "The sub graph that spans between two documents, and we do this for every candidate document that we that we found earlier.",
                    "label": 0
                },
                {
                    "sent": "And here again we use the information stored earlier and go into and compute hierarchical entrance versus similarity.",
                    "label": 0
                },
                {
                    "sent": "We then want to use those single scores between the entities and combine them into one single document score.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we do.",
                    "label": 0
                },
                {
                    "sent": "This is first computer optical similarity for each entity.",
                    "label": 1
                },
                {
                    "sent": "Which builds on that formula that basically that heavily relies on the concept of lowest common ancestor between the categories of those two entities.",
                    "label": 0
                },
                {
                    "sent": "And it will rank something higher if the lowest common ancestor is rather low rather far from the root, because that.",
                    "label": 0
                },
                {
                    "sent": "Kind of implies that it's very special that the that the similarities are very specific.",
                    "label": 0
                },
                {
                    "sent": "And in this example, because the L say the green box here is very close to the root of the score, will be rather low with a .2 in this example.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the transversal side, we use the weights we stored earlier for each of the nodes in the neighborhood and just multiply those which is going to give us a score that follows this formula.",
                    "label": 0
                },
                {
                    "sent": "Which again will penalize paths the longer they are.",
                    "label": 0
                },
                {
                    "sent": "And also will count give a higher score in the end if more paths connect, so they connect those two.",
                    "label": 0
                },
                {
                    "sent": "Again, in this example, we yield a score of .5 just.",
                    "label": 0
                },
                {
                    "sent": "As an example.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So after we've done those after we've computed hierarchical N transversal similarity, want to combine those two with a bit of normalization going on into one score between each pair of concepts?",
                    "label": 0
                },
                {
                    "sent": "When we have that this theoretically gives us a bipartite graph between the document annotations.",
                    "label": 0
                },
                {
                    "sent": "Now we're not going to use all the edges in that bipartite graph, because we argue that adds a lot of noise, so we select the sub graph of that and the way we do that is just extracting for each annotation the outgoing edge with the highest score.",
                    "label": 0
                },
                {
                    "sent": "We call this the Max graph in this case.",
                    "label": 0
                },
                {
                    "sent": "And you can see this in the bold lines and based on those bold edges, we can then compute the formula down there.",
                    "label": 0
                },
                {
                    "sent": "That just sums up those maximum scores and divides them or normalizes them by the.",
                    "label": 0
                },
                {
                    "sent": "Overall number of annotations in both documents.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this example, with DB pedia entities and probably 2 documents that are concerned with basketball.",
                    "label": 0
                },
                {
                    "sent": "You can see the bold lines again, pretty much spanning where you would intuitively expect them to span between those two players between the two cities.",
                    "label": 0
                },
                {
                    "sent": "In the state, Texas, and maybe to the state.",
                    "label": 0
                },
                {
                    "sent": "So we only select those because that we we assume those are more meaningful for similarity.",
                    "label": 0
                },
                {
                    "sent": "And then just put it into that formula and get that score of .63 in this case.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so let's now look into evaluation.",
                    "label": 0
                },
                {
                    "sent": "And we use three different datasets in there of two types, basically once.",
                    "label": 0
                },
                {
                    "sent": "We use the Lee 50 corpus that is very far spread.",
                    "label": 0
                },
                {
                    "sent": "Very well known, has been cited hundreds of times and has been used in all the related work we could found.",
                    "label": 0
                },
                {
                    "sent": "It find basically.",
                    "label": 0
                },
                {
                    "sent": "And that is concerned with the document similarity, while the other two sets we used were about sentence similarity.",
                    "label": 0
                },
                {
                    "sent": "So that was the motivation for us to see how far we could go in terms of reducing the number of entities that we can find in the text.",
                    "label": 0
                },
                {
                    "sent": "And if we can still achieve good results.",
                    "label": 0
                },
                {
                    "sent": "We use DB pedia data set of 2014 and the effort mentioned X Lisa tool of the FB for entity extraction.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the first part, document similarity.",
                    "label": 0
                },
                {
                    "sent": "The Lee 50 corpus consists of 50 documents of rather short news articles all out of Australia, and there's also a gold standard set of similarities that were assigned by humans for all pairs of documents so that the task again is to measure the correlation of our approach with that of humans.",
                    "label": 0
                },
                {
                    "sent": "And this correlation here expressed in the Pearson Spearman correlation numbers and the harmonic mean between those.",
                    "label": 0
                },
                {
                    "sent": "Quite good, we're quite happy to see that we were able to outperform both the statistical approaches, LSA, ESA etc as well as the graph edit distance one by Schumer.",
                    "label": 0
                },
                {
                    "sent": "Crampton said I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "So yeah, that was that was really good to see.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We went into sentence similarity where on those two datasets, which are ones video descriptions and the other Flickr image descriptions, we also found that if we could at least extract one entity and focused on those sentences, because otherwise knowledge based approach is rather hard.",
                    "label": 0
                },
                {
                    "sent": "Then comparing us to the unsupervised approaches that we could find listed there.",
                    "label": 0
                },
                {
                    "sent": "We also achieved very good numbers and were able to outperform the competition.",
                    "label": 0
                },
                {
                    "sent": "Just a quick note here, the GBS S is supposed to mean graph based, some semantic similarity and the little R subscript is supposed to represent the radius that we used in the document expansion.",
                    "label": 0
                },
                {
                    "sent": "So it says, how far do we go out from the original annotations to explore the neighborhood and Add all this information to the document?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, as one last piece of.",
                    "label": 0
                },
                {
                    "sent": "Evaluation this.",
                    "label": 0
                },
                {
                    "sent": "Graph basically has three takeaways, I'd say.",
                    "label": 0
                },
                {
                    "sent": "Which is first of all, from the red line to the blue line.",
                    "label": 0
                },
                {
                    "sent": "So from pre search to full search the ranking quality that we measure.",
                    "label": 0
                },
                {
                    "sent": "Ranking quality is measured in normalized DCG and correlates with our our related document search use case.",
                    "label": 0
                },
                {
                    "sent": "We see a clear increase, so after we extract we generate that candidate set on that rough document, wide entity overlap.",
                    "label": 0
                },
                {
                    "sent": "We get a pretty decent quality of over .8.",
                    "label": 0
                },
                {
                    "sent": "But if we then go into the next step and reorder the documents according to their entity level scores, we see a large increase in those numbers.",
                    "label": 0
                },
                {
                    "sent": "Which is which is really cool.",
                    "label": 0
                },
                {
                    "sent": "The second thing we can see is that the larger the candidate set size, obviously the better the results, because the smaller it is, the more documents may be missed before they actually get to that last step of entity level similarity.",
                    "label": 0
                },
                {
                    "sent": "But we can also see that starting of a with a candidate set of 15 documents, we already achieve very good performance.",
                    "label": 0
                },
                {
                    "sent": "Although granted, this was a very small data set that was the least 50 data set.",
                    "label": 0
                },
                {
                    "sent": "And the third takeaway, I think, is that on that Gray line you can see the time consumption depending on the candidate set size.",
                    "label": 0
                },
                {
                    "sent": "So we see a linear increase in the with the number of documents as candidates.",
                    "label": 0
                },
                {
                    "sent": "And the only real dependence on the overall corpus size is in the inverted index in the extraction in the candidate set generation.",
                    "label": 0
                },
                {
                    "sent": "And this is something that scales very well and is something that other statistical approaches do as well.",
                    "label": 0
                },
                {
                    "sent": "So that in this final step here we can really configure that by just setting the candidate set size to something that is appropriate for us.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, just wrapping it up, we've presented a work about efficient graph based document similarity that combines hierarchical and transversal entity level similarities out of a knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "And with this is able to outperform both the distribution, ull and the knowledge based related approaches that we found on two different types of data set.",
                    "label": 0
                },
                {
                    "sent": "At the same time, we're also able to run it very efficiently so that we can also use it in that related document search use case.",
                    "label": 0
                },
                {
                    "sent": "During that work we understood a lot of new things, but some lessons learned here are definitely that there is value in the DB pedia relational knowledge for things like semantic similarity.",
                    "label": 0
                },
                {
                    "sent": "So that worked quite well.",
                    "label": 0
                },
                {
                    "sent": "And on the other side, what was interesting was that the more entities we could get our hands on for a document, the better it seemed to work generally.",
                    "label": 0
                },
                {
                    "sent": "And what we argued was that if you just considered one word and you try to extract an entity from that, then you're probably not going to have that much success with disambiguation using an entity extractor that you're going to have.",
                    "label": 0
                },
                {
                    "sent": "If you at least have a sentence.",
                    "label": 0
                },
                {
                    "sent": "So if we have at least that context to use the disambiguation, that's going to help us already.",
                    "label": 0
                },
                {
                    "sent": "And then when you have, we have many more entities.",
                    "label": 0
                },
                {
                    "sent": "Then what I think really kicks in is the maximum entity edge selection that we do in that Max graph step at the end, which.",
                    "label": 0
                },
                {
                    "sent": "Really puts an emphasis on the more meaningful relations.",
                    "label": 0
                },
                {
                    "sent": "And is not too strict.",
                    "label": 0
                },
                {
                    "sent": "Not doing a 11 matching or something, but really just emphasizes those that seem to be important.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "I thank you very much for your time and I'm looking forward to your questions.",
                    "label": 0
                }
            ]
        }
    }
}