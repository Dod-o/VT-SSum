{
    "id": "2iv2vdxathu6ezz443n2fhkpheqikdku",
    "title": "Domain adaptation for subtitles you can take seriously",
    "info": {
        "author": [
            "Muhammad Ali Tahir, RWTH Aachen University"
        ],
        "published": "June 23, 2014",
        "recorded": "April 2014",
        "category": [
            "Top->Social Sciences->Education",
            "Top->Social Sciences->Society->Public Policy"
        ]
    },
    "url": "http://videolectures.net/ocwc2014_ali_tahir_domain_adaptation/",
    "segmentation": [
        [
            "My name is Mohammed Ali to here and today I will be presenting the domain adaptation work that we and our partner, so audibility, Chalker and our partners have done for the translation project.",
            "So the title is domain adaptation for subtitles.",
            "You can take seriously, and so let's let me review."
        ],
        [
            "So.",
            "This is an outline of the presentation.",
            "First of all, the task definition.",
            "So some of these things have already been covered in the last presentation, so I will just quickly go over them and other things.",
            "I will explain in more detail and.",
            "So the task definition and then the main challenges and opportunities.",
            "Then different, there are three different avenues of the work that we are adaptation work that we did for trans lectures.",
            "The acoustic model language model translation model.",
            "So we will go over each of them and then some experimental results to show like how the.",
            "What is the effect of the adaptation that we did and then?",
            "Some tools that have been used or have been developed during the course of this work and conclusion."
        ],
        [
            "OK, so yeah.",
            "So as previously defined so we have this automatic speech recognition and then transfer to machine translation so audio to text is the automatic speech recognition.",
            "Then test text to another language machine translation an.",
            "Then of course that yeah the slides information is useful but cause the speaker will be talking about the things that have been mentioned in the slides, so that is helpful."
        ],
        [
            "OK, so the project had different challenges and as had the same time opportunities.",
            "So of course the conditions are quite rude to genius like especially for English lectures we have non native speakers and so if the system has been trained on.",
            "The larger because it the the systems that we have are statistical, so the statistical system would imply that if we have.",
            "The data that has been trained on a majority of native speakers, then the non native speakers would be difficult to recognize and then there is like the issue of echo in lecture room or some other acoustic conditions.",
            "And then of course there is a broad range of topics becausw the kind of data that we use is usually like, for example, before trans lectures, the data that we had for speech recognition, it was composed of Parliament speeches and.",
            "Television news and debates and these kind of data, but for the lectures we can have, like data that can contain a lot of technical terms and especially like when the speakers is defining some mathematical formula and using symbols.",
            "So it gets quite difficult because the training data that we have used that does not contain these kind of notations.",
            "And so therefore there is a need for adaptation methods for this particular domain.",
            "And then so we use two reported repositories as previously defined, so the videolectures.net an polymedia, so we so the task is about 707 thousand hours of speech and this is a challenge 'cause of course we can make a system that can run for an very long time.",
            "But of course since we have to recognize this amount of data, so we need we need to make the system too.",
            "Scale it down in such a way that the error rate is not too much compromised and at the same time we can achieve it using some, like whatever computational resources we have available.",
            "And then we have to do a translation into multiple languages, like from English, into French, German, souvenirs, Spanish and so.",
            "Of course efficiency is important, so yeah, so we talk about efficient adaptation methods.",
            "And so lecture slides, yeah?"
        ],
        [
            "So this is some prior knowledge that we can use.",
            "So this is a general diagram of a speech recognition system.",
            "So the input to the system is the audio.",
            "Then we do some feature extraction.",
            "So this is kind of some frequency we converted into a frequency representation and it is like a stream of vector of vectors we we try to make this feature representation such that we only have that information in this that is relevant to the phonetic information in the.",
            "Signal, and we discard all the other information that is not useful, so.",
            "And then we have two types of models.",
            "Acoustic model contains things like the phonetic representations like for every language we have different phonemes with different sounds which are specific to that language.",
            "So this is the acoustic model and then there is the pronunciation lexicon, which is essentially like a pronunciation dictionary where you have for every word you have defined representation of which phonetic units you can put together to make that word.",
            "And then the language model defines basically the grammar of the language.",
            "So how is?",
            "How do we put different words together to make it?",
            "The sentence makes sense an.",
            "We put all of this information into the search process and then.",
            "This search process that tries to optimize from the input audio it tries to pick out cause there can be an infinite infinite large number of.",
            "Sentences that can be recognized from there.",
            "To do so, we try to find out that sentence, which has the highest probability given the acoustic model and the language model that we have.",
            "And so this is.",
            "This is basically an outline."
        ],
        [
            "And similarly for machine translation we also have similar system.",
            "So we have some language text in the source language.",
            "Then we do some transformation, some preprocessing postprocessing to remove unnecessary information converted like digits to words in the number representation such things.",
            "And then we have this first one is the lexecon alignment.",
            "So this is like how the word, how the words and phrases of a language.",
            "I have two the words and phrases of another language, so this model contains this information and the language model.",
            "As previously it contains.",
            "How like if that particular order of words makes sense, so it is grammatically correct and again, so we put it into a search process to find out the like the combination of both these.",
            "So one is that the translation is correct and one is that the output sentence should be grammatically correct.",
            "So together we try to find out the best possible sentence which has the highest probability and then afterwards we do some post processing and then.",
            "We get the language in the output.",
            "We get the text in the output language."
        ],
        [
            "Yes, so this was the definition of the systems.",
            "Now of course we have to evaluate our system.",
            "So how do we do it so as in the previous presentation, so we see that we have the word error rate is a metric, so this is simply.",
            "This is simply a measure of like if you have a human transcribed text and you have some machine output to text from speech recognition.",
            "So we find out the number of deletions insertions so.",
            "Number of words that we can delete, insert, or substitute in one string to convert it into another string.",
            "So the minimum number of words.",
            "So this is an example where we do it with characters like.",
            "We have a word elephant, an relevant.",
            "So one of these is machine output.",
            "One of these human output.",
            "So there is a difference of three between these.",
            "And then for statistical machine translation we have an error measure which is basically quiet, it is.",
            "You can.",
            "So human generated translation and the machine learning generated translation.",
            "So it is a number between zero and one which says that how closely do they match.",
            "So this may.",
            "This measure is not useful for individual sentences because machine for the language to language translation is a more difficult problem.",
            "'cause if you have many different translators and you give them some text to translate, then it is possible that they have different styles, so they will translate it differently so.",
            "So this is just a rough measure of how closely do the sentences met how many phrases in those sentences meant something like this an so in practice it has been found to correlate to the actual quality of translation system."
        ],
        [
            "So the different partners involved in the project and he also quickly go over this slide because these are already explained."
        ],
        [
            "OK, so the adaptation of acoustic model so we have.",
            "So we have speech recognition system that has already been trained on previous data other than the lecture data that from the translation project and now we have to make this system adapt to the translator specific domain so.",
            "Um?",
            "So we have this maximum likelihood linear regression and constrained maximum likelihood linear request, and these are basically two transformations which convert the models to be useful for the task of some new."
        ],
        [
            "Like for example for this.",
            "So for example, this is the figure of the speech recognition system that we saw in some previous slide, and the yellow blocks show where to the.",
            "These transformations come so that the MLR is a transformation that there is applied on the acoustic model, that is, the phonetic representations of the words and the similar is a transformation that is applied to the the input features that we get from the audio an.",
            "Using these two transformation transformations, we can convert our general purpose system to a system that is more useful for.",
            "So in that particular context, we create these transformations for every speaker, like for first.",
            "Of all we we recognize the speech from a particular speaker by.",
            "A general purpose system.",
            "Then we get some output text and then we use this output text to train some of these transformations and then we do another recognition using this specific newly adopted system and then the second time our output of the speech recognition is better than the first one.",
            "And then.",
            "So the maximum posteriori adaptation is a technique in which we we take a system that has been.",
            "In the similar way as the previous ones.",
            "So we take a general purpose system and we have some new data for a new task and we adapt the system in such a way by interpolating with the with the new data such that the system becomes optimized for the new task.",
            "And so then the neural networks is an area which has, like for example, the deep neural networks is 1 area which has become important for speech recognition in the last five six years, and so we also do the deep neural networks for this and we have got some reasonable improvements.",
            "And then another interesting thing is the multilingual features."
        ],
        [
            "So with so.",
            "The idea behind this is that for individual languages we have, we can have small amount of data.",
            "So why don't we just put all of the audio data for training audio training data together to make such features that incorporate this phonetic information from all the languages and then use this information for all the languages so like?",
            "In this way, so we have French, English, German and Polish.",
            "We have around so 800 hours of data altogether, so we just put all this data together and train some neural net.",
            "Some deep neural networks.",
            "And then from this we we do are supervised, so we do specific like a training of for all the different.",
            "Languages and then we concatenate these features together.",
            "So it means that after doing this we so we get some acoustic features which are which have all the different phonetic information from different languages.",
            "And actually we we used such feature also for the Spanish language.",
            "So it means that.",
            "These features do not have any Spanish.",
            "The inside them, but still we use this feature features for Spanish language and we also found that it is better than if we only use the Spanish data for training the neural networks.",
            "Maybe it is because now we have much more data so we have better models and so.",
            "This is a very recent progress in yes, Sir.",
            "Probably the app.",
            "These are there very.",
            "1st result so ugly.",
            "Well at yes Sir.",
            "I mean I know the product they have been applied the.",
            "This this is a mouth we are reporting results in.",
            "All languages are covering using this system.",
            "In the past only.",
            "English menu or color?",
            "My I was wondering my question from our progress meeting last week.",
            "Have you tried the?",
            "That's a multi lingual.",
            "I mean.",
            "With different combinations of languages have been big closed.",
            "Maybe there are languages that are too far to to help each other, or maybe there are languages which are closed that can be more helpful.",
            "Yeah, a similar work has been so there was one colleague of ours who did something similar about some time ago, but he didn't do it with deep neural networks.",
            "He just did it with some shallow neural networks and he tried like features trained on some European languages for Chinese and he found out that it it was helpful.",
            "Yes, so only one of not deep neural networks because of, but I mean the technique was found to be helpful.",
            "For translators we have very similar languages because we have all the European languages.",
            "So I think in this sense we have a lot of overlap, but of course if we go some Asian languages maybe then the difference is more clear than can see, like for example for for Spanish.",
            "So we just use this these features and it brought a lot of improvement.",
            "So I mean if the languages are similar then can say it's helpful, yeah?",
            "In which, yeah, definitely.",
            "Yes, that is true.",
            "This could be useful here."
        ],
        [
            "OK, so this is an overview of the data that we have for training the system, so it includes the data that we have got from the translations, the project and also some other data that we have previously.",
            "And.",
            "Yeah.",
            "So.",
            "For for, for English and Spanish we have a large amount of data, so it's kind of hard.",
            "Things are at a level where even if we had more data, we will not get very significant improvement cause.",
            "But for Catalan, Slovenian, so we still the data is small amount of data is small, so we could benefit from word more data for this."
        ],
        [
            "OK.",
            "So these are some results where we do acoustic model adaptation and so for example for English language we have a 27.9% word error rate for the simple system that we have previously trained with before trans lectures and then when we do the trans lectures based data we use it for acoustic model adaptation.",
            "Then we go down to 22.8%.",
            "Similarly for Spanish and for sylvanian.",
            "An I mean the the error rates are different, but if you see in relative terms then they are similar improvements in all languages.",
            "Done.",
            "OK, and another thing that we use is the system combination.",
            "It means that we can have two different speech recognition systems.",
            "And we give the input audio to both of these systems and they both calculate different text output and then we combine the output of those systems together based on confidence is like 1 system gives the confidence for one particular word high and the other system gives that low and so we can do some kind of voting mechanism to create to merge the outputs of two or more different systems together an.",
            "In this way we also get some small improvement like the best system individual system was 15.6 and by combining the systems we get down to 15.3%."
        ],
        [
            "And then so this was so.",
            "This is in the context of speech recognition.",
            "So first of all, we in the previous slide we talked about acoustic model and then now we talked about the language models.",
            "So what we can do is that we can have a very large amount of data for outside the which we have from the domain outside the project.",
            "So what we can do is that we can simply add this data to our training data directly.",
            "But we can also do it more selectively like we do not add the all the data directly to our training data, but we say that OK, we only adds a subset of the data that gives some high measure on some particular like.",
            "Some criterion which which language?",
            "So which grammatically and so in.",
            "In terms of the usage of words, it which correlates better to the original data, so we can create sorted List, a list of sentences based on how closely it matches the the the translations, domain language data and then we can take a subset of that sorted list into the training data and this has shown to be helpful for this purpose.",
            "And.",
            "And then of course the slides cause for many of the slides that we have for the project they are in the form of images.",
            "So from from these images we have to expect.",
            "The text so so our partners.",
            "So they have from other projects, so some OCR tools which are having quite useful.",
            "So I mean we can give the slides as images and then we can do this optical character recognition, the OCR and then from the images of the slides we can get some text and this text can be used to improve the.",
            "Adopt our system for the recognition, purpose and another important thing is the language model.",
            "Cash.",
            "It means that if a speaker uses some particular word in his talk, then it is likely that after some time he will again use the same word or same phrase.",
            "Something like this.",
            "So in this way we can use some cache of words or phrases from like.",
            "In the like it can be some duration dependent, like we say that from the previous this much words we create a cache of the words that have been used and give them higher likelihood.",
            "Then the new phrases are recognized.",
            "So this is also helpful."
        ],
        [
            "So these are some results.",
            "And here we see that for example, for English we have 22.8% after the before the language model adaptation and after language model adaptation we have 21.2% and similar for Spanish and Slovenian and.",
            "For Spanish, we have relatively more improvement, 'cause probably the data that we had previously were not so matched to the translators domain."
        ],
        [
            "So another question that could arise is that what if we simply add more data like instead of using these adaptation techniques, we simply add more data?",
            "Would it also help or is there something special that we are doing with adaptation there that is useful?",
            "So in this experiment, so the initial word error rate and then we do some acoustic model adaptation and then we add some additional data and we see that the effect of.",
            "Adding additional data is quite small as compared to the adaptation techniques that we do.",
            "So it means that if the system already trained on a reasonably large amount of data then the adaptation is more useful than simply adding more data."
        ],
        [
            "And then some results.",
            "So these have been also also previously so."
        ],
        [
            "It.",
            "OK, and then so previously we talked about the automatic speech recognition adaptation.",
            "Now this is about machine translation, adaptation and.",
            "So of course we can have some domain specific language model that we get from the project and then from the large amount of data from outside the project, just As for the language model for speech recognition, we also for machine translation we can select some data to that closely matches the particular domain of the.",
            "At hand, and we can create the gas the same.",
            "The sorted list of phrases, and then we can select from that something like this.",
            "Another technique that was tried by so in the project from one partner is the lexical coverage feature.",
            "So it is like we append some features to the input of the translation features based on which of the which of the different training data.",
            "Contain that phrase in which of the training data do not contain that phrase an this has also been shown to be helpful, and so either we can do this the data selection, or we can do data waiting.",
            "So data waiting means that we include all the out of domain data for the training, but we waited differently, so if some data is found to be more relevant to the domain we added with more weight to the training data, and if it is less relevant to their domain, we added with less.",
            "Yeah, wait.",
            "And so, in this way we can train the language model and also the translation model.",
            "So recall that the translation model was the was the phrase to phrase or word to word mapping between one language and another language and the language model was for the grammatical correctness of the sentences.",
            "And this all can be done based on what what?",
            "Data that we get from the training domain and also we can do this specifically for the text that is being translated like we have some text that is being translated so that of course that would be very narrow because it may be a few 100 or maybe a few 1000 words, But so this is not that robust, but it could principle also be done."
        ],
        [
            "Yes.",
            "This is the amount of training data that we have available for different language pairs, like from Spanish to English.",
            "We have 17.9 million parallel sentences of both languages and from English to Spanish, again the same and also in the similar way for all the different languages for pairs."
        ],
        [
            "So these are some results.",
            "The baseline means that the whatever language, language, whatever system for translation we have before the project and then after.",
            "Doing the translation model adaptation, we get improvement, so this is the blue.",
            "This is the measure of translation quality, so the higher is better.",
            "So like from 26 we go to 27.3 and then from for English to Spanish we go from 33.5 two 35.4.",
            "So we get consistent improvements and for the language pairs which are already quite well optimized, like for example for English to Spanish we have a system that is already quite.",
            "Fine, so in that case the improvement coming from adaptation is smaller, but for that case where the the system is not so good, we have already then the improvement that we get is bigger.",
            "For example, for this English to Slovenian system we go from 12.0 two 15.9, which is more improvement in relative terms."
        ],
        [
            "So these are some results again, so we're getting consistently improvement in every.",
            "Like timing portion of the project."
        ],
        [
            "So the tools for adaptation so, so all the different partners.",
            "So we have, so they have different tools and for AML they have this transcription platform and then for our WTH we have this automatic speech recognition engine, our ASR which is open source, so it is available on our website so it of course yeah again so only the software is available But the data.",
            "Of course it's not available, but I mean the so.",
            "It can be started from like maybe a few 10s of hours of data.",
            "So to start a basic speech recognition recognition system an.",
            "Then for machine translation, there's Jenn Toolkit an from Valencia.",
            "The ASR told TLK and from Xerox they have the machine translation to Northern Tool Kit."
        ],
        [
            "OK, so the conclusions that we draw from this work is that we get 20 to 30% relative improvements for all languages by doing these by doing this adaptation techniques.",
            "Anne for English and Spanish we have accurate enough transcriptions, so which are quite useful for different purposes and for other languages.",
            "We are also moving into the direction of in the same word error rate range of that can be used.",
            "Um?",
            "And then the neural networks have been found to be quite useful.",
            "So this can be used to combine different features together and they can be used to adopt the.",
            "The recognition system to the translators domain.",
            "And also if we incorporate the slides information that we have, we have the slide test images and we can do optical character recognition to extract text from those slides and then it has also been found to be used to reduce the word error rate and then the initial release of the Financial Toolkit Toolkit and for machine translation we have if the system is very good.",
            "So we have high quality scores and if.",
            "The system is not already very good.",
            "Then we have large improvement from adaptation.",
            "And then so by using this data selection and data weighting techniques, we have got improvements in the translation.",
            "Quality and this blue score.",
            "So the future work that we envisage is that we improve the Slovenian speech recognition system by using possibly some techniques that can be used for falling for language scale where we have less amount of data.",
            "So we try some techniques.",
            "And then there is the Slovenian an German machine translation, like translation from Serbian to German and vice versa.",
            "So this is another.",
            "Direction."
        ],
        [
            "So this was it.",
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Mohammed Ali to here and today I will be presenting the domain adaptation work that we and our partner, so audibility, Chalker and our partners have done for the translation project.",
                    "label": 0
                },
                {
                    "sent": "So the title is domain adaptation for subtitles.",
                    "label": 1
                },
                {
                    "sent": "You can take seriously, and so let's let me review.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is an outline of the presentation.",
                    "label": 0
                },
                {
                    "sent": "First of all, the task definition.",
                    "label": 0
                },
                {
                    "sent": "So some of these things have already been covered in the last presentation, so I will just quickly go over them and other things.",
                    "label": 0
                },
                {
                    "sent": "I will explain in more detail and.",
                    "label": 0
                },
                {
                    "sent": "So the task definition and then the main challenges and opportunities.",
                    "label": 1
                },
                {
                    "sent": "Then different, there are three different avenues of the work that we are adaptation work that we did for trans lectures.",
                    "label": 0
                },
                {
                    "sent": "The acoustic model language model translation model.",
                    "label": 0
                },
                {
                    "sent": "So we will go over each of them and then some experimental results to show like how the.",
                    "label": 0
                },
                {
                    "sent": "What is the effect of the adaptation that we did and then?",
                    "label": 0
                },
                {
                    "sent": "Some tools that have been used or have been developed during the course of this work and conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So as previously defined so we have this automatic speech recognition and then transfer to machine translation so audio to text is the automatic speech recognition.",
                    "label": 1
                },
                {
                    "sent": "Then test text to another language machine translation an.",
                    "label": 0
                },
                {
                    "sent": "Then of course that yeah the slides information is useful but cause the speaker will be talking about the things that have been mentioned in the slides, so that is helpful.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the project had different challenges and as had the same time opportunities.",
                    "label": 0
                },
                {
                    "sent": "So of course the conditions are quite rude to genius like especially for English lectures we have non native speakers and so if the system has been trained on.",
                    "label": 0
                },
                {
                    "sent": "The larger because it the the systems that we have are statistical, so the statistical system would imply that if we have.",
                    "label": 0
                },
                {
                    "sent": "The data that has been trained on a majority of native speakers, then the non native speakers would be difficult to recognize and then there is like the issue of echo in lecture room or some other acoustic conditions.",
                    "label": 0
                },
                {
                    "sent": "And then of course there is a broad range of topics becausw the kind of data that we use is usually like, for example, before trans lectures, the data that we had for speech recognition, it was composed of Parliament speeches and.",
                    "label": 0
                },
                {
                    "sent": "Television news and debates and these kind of data, but for the lectures we can have, like data that can contain a lot of technical terms and especially like when the speakers is defining some mathematical formula and using symbols.",
                    "label": 0
                },
                {
                    "sent": "So it gets quite difficult because the training data that we have used that does not contain these kind of notations.",
                    "label": 0
                },
                {
                    "sent": "And so therefore there is a need for adaptation methods for this particular domain.",
                    "label": 0
                },
                {
                    "sent": "And then so we use two reported repositories as previously defined, so the videolectures.net an polymedia, so we so the task is about 707 thousand hours of speech and this is a challenge 'cause of course we can make a system that can run for an very long time.",
                    "label": 0
                },
                {
                    "sent": "But of course since we have to recognize this amount of data, so we need we need to make the system too.",
                    "label": 0
                },
                {
                    "sent": "Scale it down in such a way that the error rate is not too much compromised and at the same time we can achieve it using some, like whatever computational resources we have available.",
                    "label": 0
                },
                {
                    "sent": "And then we have to do a translation into multiple languages, like from English, into French, German, souvenirs, Spanish and so.",
                    "label": 1
                },
                {
                    "sent": "Of course efficiency is important, so yeah, so we talk about efficient adaptation methods.",
                    "label": 0
                },
                {
                    "sent": "And so lecture slides, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is some prior knowledge that we can use.",
                    "label": 0
                },
                {
                    "sent": "So this is a general diagram of a speech recognition system.",
                    "label": 0
                },
                {
                    "sent": "So the input to the system is the audio.",
                    "label": 0
                },
                {
                    "sent": "Then we do some feature extraction.",
                    "label": 1
                },
                {
                    "sent": "So this is kind of some frequency we converted into a frequency representation and it is like a stream of vector of vectors we we try to make this feature representation such that we only have that information in this that is relevant to the phonetic information in the.",
                    "label": 0
                },
                {
                    "sent": "Signal, and we discard all the other information that is not useful, so.",
                    "label": 0
                },
                {
                    "sent": "And then we have two types of models.",
                    "label": 1
                },
                {
                    "sent": "Acoustic model contains things like the phonetic representations like for every language we have different phonemes with different sounds which are specific to that language.",
                    "label": 0
                },
                {
                    "sent": "So this is the acoustic model and then there is the pronunciation lexicon, which is essentially like a pronunciation dictionary where you have for every word you have defined representation of which phonetic units you can put together to make that word.",
                    "label": 0
                },
                {
                    "sent": "And then the language model defines basically the grammar of the language.",
                    "label": 0
                },
                {
                    "sent": "So how is?",
                    "label": 0
                },
                {
                    "sent": "How do we put different words together to make it?",
                    "label": 0
                },
                {
                    "sent": "The sentence makes sense an.",
                    "label": 0
                },
                {
                    "sent": "We put all of this information into the search process and then.",
                    "label": 0
                },
                {
                    "sent": "This search process that tries to optimize from the input audio it tries to pick out cause there can be an infinite infinite large number of.",
                    "label": 0
                },
                {
                    "sent": "Sentences that can be recognized from there.",
                    "label": 0
                },
                {
                    "sent": "To do so, we try to find out that sentence, which has the highest probability given the acoustic model and the language model that we have.",
                    "label": 1
                },
                {
                    "sent": "And so this is.",
                    "label": 0
                },
                {
                    "sent": "This is basically an outline.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And similarly for machine translation we also have similar system.",
                    "label": 1
                },
                {
                    "sent": "So we have some language text in the source language.",
                    "label": 1
                },
                {
                    "sent": "Then we do some transformation, some preprocessing postprocessing to remove unnecessary information converted like digits to words in the number representation such things.",
                    "label": 0
                },
                {
                    "sent": "And then we have this first one is the lexecon alignment.",
                    "label": 0
                },
                {
                    "sent": "So this is like how the word, how the words and phrases of a language.",
                    "label": 1
                },
                {
                    "sent": "I have two the words and phrases of another language, so this model contains this information and the language model.",
                    "label": 0
                },
                {
                    "sent": "As previously it contains.",
                    "label": 0
                },
                {
                    "sent": "How like if that particular order of words makes sense, so it is grammatically correct and again, so we put it into a search process to find out the like the combination of both these.",
                    "label": 0
                },
                {
                    "sent": "So one is that the translation is correct and one is that the output sentence should be grammatically correct.",
                    "label": 0
                },
                {
                    "sent": "So together we try to find out the best possible sentence which has the highest probability and then afterwards we do some post processing and then.",
                    "label": 0
                },
                {
                    "sent": "We get the language in the output.",
                    "label": 0
                },
                {
                    "sent": "We get the text in the output language.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, so this was the definition of the systems.",
                    "label": 0
                },
                {
                    "sent": "Now of course we have to evaluate our system.",
                    "label": 0
                },
                {
                    "sent": "So how do we do it so as in the previous presentation, so we see that we have the word error rate is a metric, so this is simply.",
                    "label": 1
                },
                {
                    "sent": "This is simply a measure of like if you have a human transcribed text and you have some machine output to text from speech recognition.",
                    "label": 0
                },
                {
                    "sent": "So we find out the number of deletions insertions so.",
                    "label": 1
                },
                {
                    "sent": "Number of words that we can delete, insert, or substitute in one string to convert it into another string.",
                    "label": 0
                },
                {
                    "sent": "So the minimum number of words.",
                    "label": 0
                },
                {
                    "sent": "So this is an example where we do it with characters like.",
                    "label": 0
                },
                {
                    "sent": "We have a word elephant, an relevant.",
                    "label": 0
                },
                {
                    "sent": "So one of these is machine output.",
                    "label": 0
                },
                {
                    "sent": "One of these human output.",
                    "label": 0
                },
                {
                    "sent": "So there is a difference of three between these.",
                    "label": 0
                },
                {
                    "sent": "And then for statistical machine translation we have an error measure which is basically quiet, it is.",
                    "label": 1
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "So human generated translation and the machine learning generated translation.",
                    "label": 0
                },
                {
                    "sent": "So it is a number between zero and one which says that how closely do they match.",
                    "label": 0
                },
                {
                    "sent": "So this may.",
                    "label": 0
                },
                {
                    "sent": "This measure is not useful for individual sentences because machine for the language to language translation is a more difficult problem.",
                    "label": 0
                },
                {
                    "sent": "'cause if you have many different translators and you give them some text to translate, then it is possible that they have different styles, so they will translate it differently so.",
                    "label": 0
                },
                {
                    "sent": "So this is just a rough measure of how closely do the sentences met how many phrases in those sentences meant something like this an so in practice it has been found to correlate to the actual quality of translation system.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the different partners involved in the project and he also quickly go over this slide because these are already explained.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the adaptation of acoustic model so we have.",
                    "label": 1
                },
                {
                    "sent": "So we have speech recognition system that has already been trained on previous data other than the lecture data that from the translation project and now we have to make this system adapt to the translator specific domain so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we have this maximum likelihood linear regression and constrained maximum likelihood linear request, and these are basically two transformations which convert the models to be useful for the task of some new.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like for example for this.",
                    "label": 0
                },
                {
                    "sent": "So for example, this is the figure of the speech recognition system that we saw in some previous slide, and the yellow blocks show where to the.",
                    "label": 0
                },
                {
                    "sent": "These transformations come so that the MLR is a transformation that there is applied on the acoustic model, that is, the phonetic representations of the words and the similar is a transformation that is applied to the the input features that we get from the audio an.",
                    "label": 0
                },
                {
                    "sent": "Using these two transformation transformations, we can convert our general purpose system to a system that is more useful for.",
                    "label": 0
                },
                {
                    "sent": "So in that particular context, we create these transformations for every speaker, like for first.",
                    "label": 0
                },
                {
                    "sent": "Of all we we recognize the speech from a particular speaker by.",
                    "label": 0
                },
                {
                    "sent": "A general purpose system.",
                    "label": 0
                },
                {
                    "sent": "Then we get some output text and then we use this output text to train some of these transformations and then we do another recognition using this specific newly adopted system and then the second time our output of the speech recognition is better than the first one.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "So the maximum posteriori adaptation is a technique in which we we take a system that has been.",
                    "label": 0
                },
                {
                    "sent": "In the similar way as the previous ones.",
                    "label": 0
                },
                {
                    "sent": "So we take a general purpose system and we have some new data for a new task and we adapt the system in such a way by interpolating with the with the new data such that the system becomes optimized for the new task.",
                    "label": 0
                },
                {
                    "sent": "And so then the neural networks is an area which has, like for example, the deep neural networks is 1 area which has become important for speech recognition in the last five six years, and so we also do the deep neural networks for this and we have got some reasonable improvements.",
                    "label": 0
                },
                {
                    "sent": "And then another interesting thing is the multilingual features.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with so.",
                    "label": 0
                },
                {
                    "sent": "The idea behind this is that for individual languages we have, we can have small amount of data.",
                    "label": 0
                },
                {
                    "sent": "So why don't we just put all of the audio data for training audio training data together to make such features that incorporate this phonetic information from all the languages and then use this information for all the languages so like?",
                    "label": 0
                },
                {
                    "sent": "In this way, so we have French, English, German and Polish.",
                    "label": 1
                },
                {
                    "sent": "We have around so 800 hours of data altogether, so we just put all this data together and train some neural net.",
                    "label": 0
                },
                {
                    "sent": "Some deep neural networks.",
                    "label": 0
                },
                {
                    "sent": "And then from this we we do are supervised, so we do specific like a training of for all the different.",
                    "label": 0
                },
                {
                    "sent": "Languages and then we concatenate these features together.",
                    "label": 0
                },
                {
                    "sent": "So it means that after doing this we so we get some acoustic features which are which have all the different phonetic information from different languages.",
                    "label": 0
                },
                {
                    "sent": "And actually we we used such feature also for the Spanish language.",
                    "label": 0
                },
                {
                    "sent": "So it means that.",
                    "label": 0
                },
                {
                    "sent": "These features do not have any Spanish.",
                    "label": 0
                },
                {
                    "sent": "The inside them, but still we use this feature features for Spanish language and we also found that it is better than if we only use the Spanish data for training the neural networks.",
                    "label": 0
                },
                {
                    "sent": "Maybe it is because now we have much more data so we have better models and so.",
                    "label": 0
                },
                {
                    "sent": "This is a very recent progress in yes, Sir.",
                    "label": 0
                },
                {
                    "sent": "Probably the app.",
                    "label": 0
                },
                {
                    "sent": "These are there very.",
                    "label": 0
                },
                {
                    "sent": "1st result so ugly.",
                    "label": 0
                },
                {
                    "sent": "Well at yes Sir.",
                    "label": 0
                },
                {
                    "sent": "I mean I know the product they have been applied the.",
                    "label": 0
                },
                {
                    "sent": "This this is a mouth we are reporting results in.",
                    "label": 0
                },
                {
                    "sent": "All languages are covering using this system.",
                    "label": 0
                },
                {
                    "sent": "In the past only.",
                    "label": 0
                },
                {
                    "sent": "English menu or color?",
                    "label": 0
                },
                {
                    "sent": "My I was wondering my question from our progress meeting last week.",
                    "label": 0
                },
                {
                    "sent": "Have you tried the?",
                    "label": 0
                },
                {
                    "sent": "That's a multi lingual.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "With different combinations of languages have been big closed.",
                    "label": 0
                },
                {
                    "sent": "Maybe there are languages that are too far to to help each other, or maybe there are languages which are closed that can be more helpful.",
                    "label": 0
                },
                {
                    "sent": "Yeah, a similar work has been so there was one colleague of ours who did something similar about some time ago, but he didn't do it with deep neural networks.",
                    "label": 0
                },
                {
                    "sent": "He just did it with some shallow neural networks and he tried like features trained on some European languages for Chinese and he found out that it it was helpful.",
                    "label": 0
                },
                {
                    "sent": "Yes, so only one of not deep neural networks because of, but I mean the technique was found to be helpful.",
                    "label": 0
                },
                {
                    "sent": "For translators we have very similar languages because we have all the European languages.",
                    "label": 0
                },
                {
                    "sent": "So I think in this sense we have a lot of overlap, but of course if we go some Asian languages maybe then the difference is more clear than can see, like for example for for Spanish.",
                    "label": 0
                },
                {
                    "sent": "So we just use this these features and it brought a lot of improvement.",
                    "label": 0
                },
                {
                    "sent": "So I mean if the languages are similar then can say it's helpful, yeah?",
                    "label": 0
                },
                {
                    "sent": "In which, yeah, definitely.",
                    "label": 0
                },
                {
                    "sent": "Yes, that is true.",
                    "label": 0
                },
                {
                    "sent": "This could be useful here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is an overview of the data that we have for training the system, so it includes the data that we have got from the translations, the project and also some other data that we have previously.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For for, for English and Spanish we have a large amount of data, so it's kind of hard.",
                    "label": 0
                },
                {
                    "sent": "Things are at a level where even if we had more data, we will not get very significant improvement cause.",
                    "label": 0
                },
                {
                    "sent": "But for Catalan, Slovenian, so we still the data is small amount of data is small, so we could benefit from word more data for this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So these are some results where we do acoustic model adaptation and so for example for English language we have a 27.9% word error rate for the simple system that we have previously trained with before trans lectures and then when we do the trans lectures based data we use it for acoustic model adaptation.",
                    "label": 1
                },
                {
                    "sent": "Then we go down to 22.8%.",
                    "label": 0
                },
                {
                    "sent": "Similarly for Spanish and for sylvanian.",
                    "label": 0
                },
                {
                    "sent": "An I mean the the error rates are different, but if you see in relative terms then they are similar improvements in all languages.",
                    "label": 0
                },
                {
                    "sent": "Done.",
                    "label": 1
                },
                {
                    "sent": "OK, and another thing that we use is the system combination.",
                    "label": 0
                },
                {
                    "sent": "It means that we can have two different speech recognition systems.",
                    "label": 0
                },
                {
                    "sent": "And we give the input audio to both of these systems and they both calculate different text output and then we combine the output of those systems together based on confidence is like 1 system gives the confidence for one particular word high and the other system gives that low and so we can do some kind of voting mechanism to create to merge the outputs of two or more different systems together an.",
                    "label": 0
                },
                {
                    "sent": "In this way we also get some small improvement like the best system individual system was 15.6 and by combining the systems we get down to 15.3%.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then so this was so.",
                    "label": 0
                },
                {
                    "sent": "This is in the context of speech recognition.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we in the previous slide we talked about acoustic model and then now we talked about the language models.",
                    "label": 1
                },
                {
                    "sent": "So what we can do is that we can have a very large amount of data for outside the which we have from the domain outside the project.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is that we can simply add this data to our training data directly.",
                    "label": 0
                },
                {
                    "sent": "But we can also do it more selectively like we do not add the all the data directly to our training data, but we say that OK, we only adds a subset of the data that gives some high measure on some particular like.",
                    "label": 0
                },
                {
                    "sent": "Some criterion which which language?",
                    "label": 0
                },
                {
                    "sent": "So which grammatically and so in.",
                    "label": 0
                },
                {
                    "sent": "In terms of the usage of words, it which correlates better to the original data, so we can create sorted List, a list of sentences based on how closely it matches the the the translations, domain language data and then we can take a subset of that sorted list into the training data and this has shown to be helpful for this purpose.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "And then of course the slides cause for many of the slides that we have for the project they are in the form of images.",
                    "label": 0
                },
                {
                    "sent": "So from from these images we have to expect.",
                    "label": 0
                },
                {
                    "sent": "The text so so our partners.",
                    "label": 0
                },
                {
                    "sent": "So they have from other projects, so some OCR tools which are having quite useful.",
                    "label": 1
                },
                {
                    "sent": "So I mean we can give the slides as images and then we can do this optical character recognition, the OCR and then from the images of the slides we can get some text and this text can be used to improve the.",
                    "label": 1
                },
                {
                    "sent": "Adopt our system for the recognition, purpose and another important thing is the language model.",
                    "label": 0
                },
                {
                    "sent": "Cash.",
                    "label": 0
                },
                {
                    "sent": "It means that if a speaker uses some particular word in his talk, then it is likely that after some time he will again use the same word or same phrase.",
                    "label": 0
                },
                {
                    "sent": "Something like this.",
                    "label": 0
                },
                {
                    "sent": "So in this way we can use some cache of words or phrases from like.",
                    "label": 0
                },
                {
                    "sent": "In the like it can be some duration dependent, like we say that from the previous this much words we create a cache of the words that have been used and give them higher likelihood.",
                    "label": 0
                },
                {
                    "sent": "Then the new phrases are recognized.",
                    "label": 0
                },
                {
                    "sent": "So this is also helpful.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are some results.",
                    "label": 0
                },
                {
                    "sent": "And here we see that for example, for English we have 22.8% after the before the language model adaptation and after language model adaptation we have 21.2% and similar for Spanish and Slovenian and.",
                    "label": 1
                },
                {
                    "sent": "For Spanish, we have relatively more improvement, 'cause probably the data that we had previously were not so matched to the translators domain.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another question that could arise is that what if we simply add more data like instead of using these adaptation techniques, we simply add more data?",
                    "label": 0
                },
                {
                    "sent": "Would it also help or is there something special that we are doing with adaptation there that is useful?",
                    "label": 0
                },
                {
                    "sent": "So in this experiment, so the initial word error rate and then we do some acoustic model adaptation and then we add some additional data and we see that the effect of.",
                    "label": 1
                },
                {
                    "sent": "Adding additional data is quite small as compared to the adaptation techniques that we do.",
                    "label": 0
                },
                {
                    "sent": "So it means that if the system already trained on a reasonably large amount of data then the adaptation is more useful than simply adding more data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then some results.",
                    "label": 0
                },
                {
                    "sent": "So these have been also also previously so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "OK, and then so previously we talked about the automatic speech recognition adaptation.",
                    "label": 0
                },
                {
                    "sent": "Now this is about machine translation, adaptation and.",
                    "label": 0
                },
                {
                    "sent": "So of course we can have some domain specific language model that we get from the project and then from the large amount of data from outside the project, just As for the language model for speech recognition, we also for machine translation we can select some data to that closely matches the particular domain of the.",
                    "label": 0
                },
                {
                    "sent": "At hand, and we can create the gas the same.",
                    "label": 0
                },
                {
                    "sent": "The sorted list of phrases, and then we can select from that something like this.",
                    "label": 0
                },
                {
                    "sent": "Another technique that was tried by so in the project from one partner is the lexical coverage feature.",
                    "label": 0
                },
                {
                    "sent": "So it is like we append some features to the input of the translation features based on which of the which of the different training data.",
                    "label": 0
                },
                {
                    "sent": "Contain that phrase in which of the training data do not contain that phrase an this has also been shown to be helpful, and so either we can do this the data selection, or we can do data waiting.",
                    "label": 0
                },
                {
                    "sent": "So data waiting means that we include all the out of domain data for the training, but we waited differently, so if some data is found to be more relevant to the domain we added with more weight to the training data, and if it is less relevant to their domain, we added with less.",
                    "label": 0
                },
                {
                    "sent": "Yeah, wait.",
                    "label": 0
                },
                {
                    "sent": "And so, in this way we can train the language model and also the translation model.",
                    "label": 1
                },
                {
                    "sent": "So recall that the translation model was the was the phrase to phrase or word to word mapping between one language and another language and the language model was for the grammatical correctness of the sentences.",
                    "label": 0
                },
                {
                    "sent": "And this all can be done based on what what?",
                    "label": 1
                },
                {
                    "sent": "Data that we get from the training domain and also we can do this specifically for the text that is being translated like we have some text that is being translated so that of course that would be very narrow because it may be a few 100 or maybe a few 1000 words, But so this is not that robust, but it could principle also be done.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This is the amount of training data that we have available for different language pairs, like from Spanish to English.",
                    "label": 1
                },
                {
                    "sent": "We have 17.9 million parallel sentences of both languages and from English to Spanish, again the same and also in the similar way for all the different languages for pairs.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are some results.",
                    "label": 0
                },
                {
                    "sent": "The baseline means that the whatever language, language, whatever system for translation we have before the project and then after.",
                    "label": 1
                },
                {
                    "sent": "Doing the translation model adaptation, we get improvement, so this is the blue.",
                    "label": 1
                },
                {
                    "sent": "This is the measure of translation quality, so the higher is better.",
                    "label": 0
                },
                {
                    "sent": "So like from 26 we go to 27.3 and then from for English to Spanish we go from 33.5 two 35.4.",
                    "label": 0
                },
                {
                    "sent": "So we get consistent improvements and for the language pairs which are already quite well optimized, like for example for English to Spanish we have a system that is already quite.",
                    "label": 0
                },
                {
                    "sent": "Fine, so in that case the improvement coming from adaptation is smaller, but for that case where the the system is not so good, we have already then the improvement that we get is bigger.",
                    "label": 0
                },
                {
                    "sent": "For example, for this English to Slovenian system we go from 12.0 two 15.9, which is more improvement in relative terms.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are some results again, so we're getting consistently improvement in every.",
                    "label": 0
                },
                {
                    "sent": "Like timing portion of the project.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the tools for adaptation so, so all the different partners.",
                    "label": 1
                },
                {
                    "sent": "So we have, so they have different tools and for AML they have this transcription platform and then for our WTH we have this automatic speech recognition engine, our ASR which is open source, so it is available on our website so it of course yeah again so only the software is available But the data.",
                    "label": 0
                },
                {
                    "sent": "Of course it's not available, but I mean the so.",
                    "label": 0
                },
                {
                    "sent": "It can be started from like maybe a few 10s of hours of data.",
                    "label": 0
                },
                {
                    "sent": "So to start a basic speech recognition recognition system an.",
                    "label": 0
                },
                {
                    "sent": "Then for machine translation, there's Jenn Toolkit an from Valencia.",
                    "label": 0
                },
                {
                    "sent": "The ASR told TLK and from Xerox they have the machine translation to Northern Tool Kit.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the conclusions that we draw from this work is that we get 20 to 30% relative improvements for all languages by doing these by doing this adaptation techniques.",
                    "label": 0
                },
                {
                    "sent": "Anne for English and Spanish we have accurate enough transcriptions, so which are quite useful for different purposes and for other languages.",
                    "label": 1
                },
                {
                    "sent": "We are also moving into the direction of in the same word error rate range of that can be used.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then the neural networks have been found to be quite useful.",
                    "label": 0
                },
                {
                    "sent": "So this can be used to combine different features together and they can be used to adopt the.",
                    "label": 0
                },
                {
                    "sent": "The recognition system to the translators domain.",
                    "label": 0
                },
                {
                    "sent": "And also if we incorporate the slides information that we have, we have the slide test images and we can do optical character recognition to extract text from those slides and then it has also been found to be used to reduce the word error rate and then the initial release of the Financial Toolkit Toolkit and for machine translation we have if the system is very good.",
                    "label": 0
                },
                {
                    "sent": "So we have high quality scores and if.",
                    "label": 0
                },
                {
                    "sent": "The system is not already very good.",
                    "label": 0
                },
                {
                    "sent": "Then we have large improvement from adaptation.",
                    "label": 0
                },
                {
                    "sent": "And then so by using this data selection and data weighting techniques, we have got improvements in the translation.",
                    "label": 1
                },
                {
                    "sent": "Quality and this blue score.",
                    "label": 0
                },
                {
                    "sent": "So the future work that we envisage is that we improve the Slovenian speech recognition system by using possibly some techniques that can be used for falling for language scale where we have less amount of data.",
                    "label": 0
                },
                {
                    "sent": "So we try some techniques.",
                    "label": 0
                },
                {
                    "sent": "And then there is the Slovenian an German machine translation, like translation from Serbian to German and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So this is another.",
                    "label": 0
                },
                {
                    "sent": "Direction.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was it.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}