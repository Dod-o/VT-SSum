{
    "id": "gyhmlk3cq6lxhaebqsthoxlplmlsnfjf",
    "title": "Shrinkage Estimator for Bayesian Network Parametrs",
    "info": {
        "author": [
            "John Burge, Department of Computer Science, University of New Mexico"
        ],
        "published": "Jan. 30, 2008",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/ecml07_burge_seb/",
    "segmentation": [
        [
            "Oh yeah, so as you mentioned, I'm John Burge, and I'm going to talk today about shrinkage estimators for Bayesian network parameters, and this is work that I've done in conjunction with Taryn Lane."
        ],
        [
            "At the University of New Mexico.",
            "So the outline of my talk is pretty straightforward.",
            "1st I'm going to briefly introduce a neuroimaging demand that we work with.",
            "I won't go into too many details, as it's not the focus of our talk here and I'll talk a little bit about Bayesian networks and then about the shrink adjustments that we've generated for our Bayesian networks and then can."
        ],
        [
            "With some of our empirical results.",
            "So at a high level we have a set of patients in each of these patients undergo some type of experimental task and we're trying to understand whether or not, based on some neuroimaging data, whether or not correlations in these patients brains are affected by the onset of mental illness.",
            "So I won't go into the details of the type of neuroimaging data that we use, I'll just represent that data with this picture of the brain, so we can see that this brain is actually divided into several regions and associated with each one of these regions.",
            "The type of data that we have results in a time series for each one of these regions, and it's our task as I mentioned, to understand the correlations between these regions."
        ],
        [
            "So it will be important for this work that we realize that there's a relationship between the random variables on our domain.",
            "This relationship is going to be hierarchical in our work, such that one random variable in our domain is going to be the entire T of all of Gray matter.",
            "So we have some measurements associated with this Gray matter, and then the value for this random variable actually decomposes into some aggregate function of the values of smaller random variables in our domain.",
            "So our entire brain decomposes at one level of this hierarchy.",
            "Into smaller regions, which then each decompose themselves into smaller."
        ],
        [
            "Regions now this is actually the graph for the for the hierarchy that we that we really work with and you can see that it's significantly more complex than the little toy example that I had before.",
            "Again at the top level we have the entirety of Gray.",
            "Matter is 1 region, which decomposes into relatively small set of regions which then further decomposed into smaller and smaller regions in the brain.",
            "Now throughout this talk I'm going to refer to the relationships in this hierarchy as a hierarchy, but that's actually not quite correct as you might be able to make out some of the nodes in this set of relationships actually have two parents.",
            "Coming from them, so it's not a strict hierarchy.",
            "Instead, it's more general.",
            "It's a trellis.",
            "This will cause some issues that arrives and the paper will deal with these issues, but for the remainder of this talk I'm going to simply treat these relate."
        ],
        [
            "Shift is a hierarchical relationship.",
            "So we have these brain regions and we're interested in understanding what the correlations between these brain regions are.",
            "So we have a set of data, and then from this set of data we might find that certain regions or are correlated with certain other regions.",
            "Now we need some framework for representing the relationships between these random variables and we use Bayesian networks to do this.",
            "So what is the Bayesian network?",
            "Well, Bayesian network is a graphical model in which each one of the random variables in the underlying system is represented as a node in this directed acyclic graph.",
            "Correlations between the regions are represented as it is directed linked."
        ],
        [
            "Between the nodes now I'll refer to these nodes as either parents or children based on whether or not they originate or terminate these links and the fundamental structural unit of a Bayesian network is often referred to as a family of family consists of a single child, for instance, perhaps random variable like 6, and the set of all of that children's parents XX has only one parent, so this set of nodes right here represents one family in my Bayesian network, there's actually one family for each one of the nodes in the Bayesian network, I've just highlighted three of them here.",
            "Inside each one of these families is a set of information that gives a conditional probability distribution, but the child takes on certain values given that the parent is taken on certain values, we assume that are random variables are going to be discrete in nature, so with discrete random variables, the conditional probability distribution can be represented as a conditional probability table, and that table gives the probability that the that the child takes on certain values, or given all possible combinations of the child and the parent, it parameterizes the probability of the child has taken on certain values given the parent house.",
            "So for instance, here we have in the case that random variable X3 is in a low state, we have an entire multinomial associated with the distribution that the child takes on its certain values."
        ],
        [
            "So when we're given a certain when we're given a set of data and we want to learn a Bayesian network to represent that data, there's actually two tasks.",
            "The first task is learning the structure and the second task is to estimate the parameters of this Bayesian network.",
            "Now, this talk is not about learning structure, it's about estimating parameters.",
            "Once you have a structure, having an individual structure for your Bayesian network defines the set of parameters that you have in your Bayesian network.",
            "So the only thing that I'm going to say about structure search is that it might be that you.",
            "During structured search, want to estimate your parameters?",
            "This is not a necessary step for doing Bayesian at structure search, but in the case that you would like to have your Bayesian structure search include parameterisations, we wanted our shrinkage estimators to be fast enough such that they can compute the parameters quickly enough to be done.",
            "The very large number of times that that would have to be done during the structure surge, and the most common method for estimating the parameters for your Bayesian networks is to use the maximum likelihood estimate.",
            "Computing the maximum likelihood estimate is straightforward, and the details.",
            "Again, around the paper, unfortunately, using the matching likelihood estimate you tend to overfit your data, and perhaps you might not have certain events in your training data that occur in your testing data, and this can be problematic people tip."
        ],
        [
            "Then we'll take the maximum likelihood estimate and smooth.",
            "It stored some other distribution to deal with these problems.",
            "Oh, use an example to demonstrate this.",
            "So again, we have our brain and we have two small regions in our brain.",
            "And let's say that we have some structure in our Bayesian that we have some family in our Bayesian net structure.",
            "So we have this yellow.",
            "This small yellow region in the small purple region and we're interested in the correlation between these two regions.",
            "Now we have a conditional probability table associated with this family and will say that these values in this conditional probability table.",
            "Represent the maximum likelihood estimate for this relationship.",
            "We can already see 2 problems in this little toy example right here.",
            "I have these two zero percent values and these occur because this event here, the parent being in a very low state and the child being in a high state never occurred in my training data.",
            "If this event had a curd in my testing data.",
            "For instance, if I was trying to classify new point based on a Bayesian up structure that I was learning, I would invalidate my likelihoods, which are really just products or sums of these probabilities."
        ],
        [
            "So what we do then is we take this maximum likelihood estimate an in a very common smoothing technique referred to as a posse and smoothing.",
            "We smooth this distribution and I'm just going to refer to this one multinomial distribution.",
            "For now, we smooth that order uniform distribution.",
            "So after we smooth it towards the uniform distribution, we now have a distribution that's more like the uniform distribution before.",
            "But the smoothing processes added a certain amount of bias in RS2."
        ],
        [
            "Mating procedure.",
            "And that might bias may unfortunately be unrealistic.",
            "Why are we smoothing tored the uniform distribution?",
            "Well, it's been shown that using a lossy and smoother does in fact help Bayesian that parameter estimation, but it could be in certain domains.",
            "This uniform distribution is completely unrealistic.",
            "If we were to look at the relationship between regions in the brain, they're very rarely described by some type of uniform distribution."
        ],
        [
            "So is there some better way that we can come up with the distribution that we're trying to smooth towards in our estimation process and we of course say that there is and the technique that the general technique that we're going to be using is referred to as shrinkage.",
            "Shrinkage was introduced in 1955 by Stein and he recognized, or he demonstrated that the maximum likelihood estimator in certain cases was more risky than another class of estimators.",
            "The estimator that he referred to as shrinkage estimators.",
            "The maximum likelihood estimators tended to overestimate.",
            "The random variable.",
            "So his idea was in any system that has at least three random variables.",
            "If you can take your maximum likelihood estimator and smooth it towards some value computed based on all the other random variables in your system, he called the shrinkage as this tended to lower the value of your your your estimation.",
            "Now I'm not the first to propose that shrink, it should be used in learning the parameters for Bayesian networks or in machine learning.",
            "General free tagging, McCollum McCollum at all, and Grossman Domingos have previously demonstrated that when you have a class when you have a hierarchy of classes and you're learning parameters.",
            "In Bayesian network like models, for each one of these classes, the shrinking the estimate for one of your parameters in one class towards the estimate for your parameter in another class can often improve the quality of our models.",
            "So I demonstrate the case in which you don't have multiple classes of data, but instead you just have one class of data and instead of shrinking similar parameters across classes of data, I suggest."
        ],
        [
            "Shrink your values towards other distributions that you find in your data.",
            "So let's go back to the example that we talked about.",
            "We have these two small regions that are linked together and they have a maximum likelihood estimate that we've learned from the data well in the example that we work with, we have this hierarchy of random variables.",
            "So these two random variables don't exist in isolation in our data.",
            "In fact, this small purple region is a component of this larger purple region and small yellow region is in turn a component of this larger yellow region.",
            "So when we ask what distribution should be smooth towards, we make the suggestion that why don't we smooth towards the relationship between the two larger regions that the two smaller regions compose, so we can easily calculate the maximum likelihood estimate for this distribution as well.",
            "And I suggest smoothing towards that distribution.",
            "Now this estimate right here will have low lower variance in some sense because each of these regions are larger regions and they're based off of more data.",
            "But smoothing towards this distribution still adds a certain amount of bias to our estimator.",
            "However, it's a reasonable bias.",
            "In fact, it's a bias that exists within our data, so it's not as unrealistic as the uniform distribution might be.",
            "Now, as you may recall in the actual hierarchy that I showed, there were four different hierarchical levels here.",
            "In this toy example, I just have two, so in our real experiments I'm not smoothing Tord just one of their distribution, but I'm.",
            "Actually smoothing towards three or four distributions on our data.",
            "It could also be that smoothing towards the uniform distribution is helpful, so I'm still going to allow myself to smooth towards the uniform distribution."
        ],
        [
            "As well.",
            "So the question then is, how much are we going to smooth again?",
            "Now we have the relationship that we're actually interested in.",
            "The relationship of regions one level up on the hierarchy and the uniform distribution associated, each with each of these will be the maximum likelihood estimate or just the uniform distribution itself.",
            "I'll represent these conditional probability tables simply with these values, Theta, zero Hat, Theta, one hat and Theta two hat.",
            "So then my task then is to combine these data parameters somehow to come up with my final estimate, like those before me.",
            "I've chosen to use a linear combination of these parameters, so my task is then to calculate or to estimate the values for the."
        ],
        [
            "Mixing coefficients.",
            "So how do we figure out what the values for these lambdas?",
            "These mixing coefficients should be?",
            "Well, in order to do that, we need to understand what we're trying to do.",
            "We're trying to come up with a set of parameters that generalize to entire class of data as effectively as possible.",
            "So what we can do is we can take our training data and then we can split it up into the smaller component, which I'll still refer to as training data, and this left out component, so the smaller component of training data is the component that I'm using to train all of my maximum likelihood estimators and then the left out data.",
            "Unfortunately, there's a slight.",
            "Typo here in my slide this is going to be the maximum likelihood estimate for my left out data.",
            "I'm going to choose the mixing coefficients such that my final estimate is going to be as close as possible to the maximum likelihood estimate in my left out data.",
            "Given that I'm using the maximum likelihood estimates for my training data.",
            "So what this means is that I'm going to be selecting the parameters such that I'm maximizing the likelihood of my left out data.",
            "We can do this by simply algebraically inverting a solution in which we have some Theta hat matrix, but unfortunately some of these Theta values will be very similar to each other.",
            "This is what we found in our work, so this required the inversion of a nearly singular matrix.",
            "There's ways to get around this, but like previous work, we found that an expectation maximization algorithm works quite well for determining what these Lambda values should be, so we use and even algorithm to do this, and the details for that or."
        ],
        [
            "Also included in the paper.",
            "So as I've described it so far, we're inefficiently using our training data.",
            "Sometimes you won't have very much training data to train your Bayesian network, so we use a cross validation method to help us more effectively use our data.",
            "An instead of learning one set of maximum likelihood estimators, we now learn a small set will say 4, so during each one of these cross validation folds are learn an independent set of these maximum likelihood distributions.",
            "But I'm still going to learn.",
            "Just a single set of these Lambda mixing values.",
            "So what I'm doing now is I'm selecting the Lambda mixing value such that the difference between the estimators that I'm coming up with and the maximum likelihood estimate of the left out data is minimized simultaneously across all of my cross validation folds.",
            "So now all of my training data is used simultaneously or is used to estimate my maximum likelihood estimates as well as my."
        ],
        [
            "Which coefficients?",
            "So I won't go into the details of the algorithm, but I thought it would.",
            "I would share the evolution of mixture weights in the shrinkage algorithm, so recall that I have 4 levels in my hierarchy, so I'll have Lambda zero through Lambda, 3 being the coefficients for each of the correlation, or for each of the conditional probability tables, and then Lambda four will represent the weight associated with the with the uniform distribution.",
            "So what we see here in this EM algorithm?",
            "So the X axis is the iterations of the EM algorithm.",
            "The Y axis is how much weight they were associated with with each mixture coefficient, Lambda 0 gets a tremendous amount of weight and this makes sense.",
            "'cause Lambda zero corresponds to the maximum likelihood estimate for the relationship that we're interested in Lambda 2, then, which is going to be 2 levels up in the hierarchy.",
            "So a relationship between 2 random variables that are two levels up in the hierarchy gets a significant amount of weight, but Lambda one, Lambda three and Lambda 4 get a small but non zero amount of weight.",
            "So we're still smoothing towards the uniform distribution to a small degree.",
            "But the EM algorithm is finding that.",
            "We should instead be smoothing towards Lambda two far more than we are in Laplacian smoothing which would just be smoothing to Lambda 4.",
            "OK, so as I mentioned, we wanted to be able to do this during structured search.",
            "Unfortunately with the number of iterations that the same algorithm was requiring, it was simply too slow to do that with some relatively straightforward modifications to the EM algorithm, we found that we were able to in the details for that or in the paper we found that we were able to.",
            "Dramatically decrease the number of EM algorithm, M iterations that we that we required.",
            "So instead of needing hundreds of iterations, we now need generate between 5:00 and 10:00."
        ],
        [
            "OK so we have results on two sets of experiments.",
            "First we have some simulated data in which we can control the ground truth and then we have some neuroimaging data.",
            "So in the simulating data, what we basically do is we construct a random Bayesian network that has some random links and we can control the strength of the correlations between the random variables in this Bayesian network and then we generate a set of data and we learn a Bayesian net structure on that data an.",
            "I'm sorry we don't learn that the structure we learn the parameterisations and then we can compare the parameters that we learn.",
            "With the original permit."
        ],
        [
            "Is that we generated the data with?",
            "So what we have here is the result of 1 set of our experiments.",
            "In order to perform Laplacian smoothing you have to determine how much you want to smooth towards the uniform distribution.",
            "So we have that here on the X axis on the Y axis we have the amount of difference, just as the pairwise difference between all the elements in a conditional probability table with the learned estimate and the actual estimates that generated the data, we find that even when we know the exact value, or even when we can empirically determine the value to smooth towards for maximum likelihood estimate which occurs right here.",
            "It minimizes the difference between our learned estimate and the actual estimates.",
            "The shrinkage estimate, which doesn't have a Laplace smoothing constant.",
            "Of course, I'm just plotting it here as a reference line is capable of coming up with a set of conditional probability tables that are closer to the actual problem."
        ],
        [
            "The values that generated the data, so there's some other experiments inside of our data in which we determine whether or not the shrinkage estimate is more robust to noise on our simulated data.",
            "But the true experiments are really on the neuroimaging data, so in the neuroimaging data, the actual estimates are unknown.",
            "We don't know what the structure for the Bayesian network should be, nor do we know what the parameterisations for each individual structure would be.",
            "So instead of measuring the difference between conditional probability tables, we now leave some of our training data out, and we compute the likelihood.",
            "Of our left out data on Bayesian networks that we learned either with maximum likelihood estimate or shrink adjust."
        ],
        [
            "So this is what the results will look like.",
            "So on both axes we have negative log likelihood, so a smaller value indicates a larger likelihood of our left out data on the X axis.",
            "We have Bayesian networks trained with maximum likelihood estimates with Laplacian smoothing and on the Y axis we have the same Bayesian net structure trained with shrinkage parameters instead.",
            "So this one point right here has a lower negative log likelihood for shrinkage than it does for maximum likelihood estimate.",
            "So that means the likelihood of this point.",
            "ARM is actually larger on the Bayesian networks that have shrinkage parameters, so in essence the shrinkage parameters are doing a better job of generalizing to this data point.",
            "And again this is a left out piece of data.",
            "This was no way used to train the Bayesian that structure the parameters or the mixing."
        ],
        [
            "Efficient, so here's our set of results on the left hand side.",
            "We have Bayesian networks that were allowed to have two parents on average per family.",
            "The random variables that we use have a narrative four and this results in a total of 64 parameters per family.",
            "What we find in almost every case is the left out data has a higher likelihood, or in this case a lower negative log likelihood.",
            "If we use shrinkage to train our parameters, then when we use maximum likelihood estimate with Laplace smoothing, and this is going to be lost moving where we empirically determine what the best weight should be.",
            "Furthermore, as we go to more parents per family, so now we have four parents per family, which results in over 1000 parameters per family.",
            "So now we have about 150,000 parameters in our Bayesian network.",
            "We find the improvement from using shrinkage is far more dramatic, so when we add this many more parameters to our Bayesian network, the amount of data that we have associated to train each one of the parameters goes dramatically down, and we find that smoothing towards the uniform distribution is hurting our ability to generalize to our left out data.",
            "Much more so than when we use our shrinkage."
        ],
        [
            "Estimates so with that, I'll conclude what we've done in this work.",
            "As I mentioned, is we've introduced a shrinkage estimator which allows our Bayesian networks to generalize to elements of the same class more effectively.",
            "I've shown that on simulated data we can find conditional probability tables that were more like the conditional probability tables that generated the data on an entire another set of experiments that I didn't go through by adding random noise into both our simulated data in our neuroimaging data, we found that shriek adjustments changed less than the actual maximum likelihood estimate, so.",
            "Shrink adjustments were also more robust in the presence of noise than the maximum likelihood estimates.",
            "Our current work involves applying this shrinkage technique to learn parameters for Bayesian networks on domain on other domains that have natural hierarchy.",
            "We're currently working with some gene data in which genes naturally grouped together into gene group groupings, which then group into larger and larger gene groupings were also very interested in applying this technique to flat datasets that don't have a natural hierarchy.",
            "There's no notion of a hierarchy in the original work proposed by shrinkage.",
            "So we believe that shrinkage techniques will be able to do well in these flat domains."
        ],
        [
            "In addition, so I'd like to acknowledge the Mind Institute and the Dartmouth fMRI DC for providing access to their neuroimaging datasets this."
        ],
        [
            "Is a list of my sources of funding, and with that I'll conclude.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yeah, so as you mentioned, I'm John Burge, and I'm going to talk today about shrinkage estimators for Bayesian network parameters, and this is work that I've done in conjunction with Taryn Lane.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the University of New Mexico.",
                    "label": 0
                },
                {
                    "sent": "So the outline of my talk is pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "1st I'm going to briefly introduce a neuroimaging demand that we work with.",
                    "label": 0
                },
                {
                    "sent": "I won't go into too many details, as it's not the focus of our talk here and I'll talk a little bit about Bayesian networks and then about the shrink adjustments that we've generated for our Bayesian networks and then can.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With some of our empirical results.",
                    "label": 0
                },
                {
                    "sent": "So at a high level we have a set of patients in each of these patients undergo some type of experimental task and we're trying to understand whether or not, based on some neuroimaging data, whether or not correlations in these patients brains are affected by the onset of mental illness.",
                    "label": 0
                },
                {
                    "sent": "So I won't go into the details of the type of neuroimaging data that we use, I'll just represent that data with this picture of the brain, so we can see that this brain is actually divided into several regions and associated with each one of these regions.",
                    "label": 0
                },
                {
                    "sent": "The type of data that we have results in a time series for each one of these regions, and it's our task as I mentioned, to understand the correlations between these regions.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it will be important for this work that we realize that there's a relationship between the random variables on our domain.",
                    "label": 0
                },
                {
                    "sent": "This relationship is going to be hierarchical in our work, such that one random variable in our domain is going to be the entire T of all of Gray matter.",
                    "label": 0
                },
                {
                    "sent": "So we have some measurements associated with this Gray matter, and then the value for this random variable actually decomposes into some aggregate function of the values of smaller random variables in our domain.",
                    "label": 0
                },
                {
                    "sent": "So our entire brain decomposes at one level of this hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Into smaller regions, which then each decompose themselves into smaller.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regions now this is actually the graph for the for the hierarchy that we that we really work with and you can see that it's significantly more complex than the little toy example that I had before.",
                    "label": 0
                },
                {
                    "sent": "Again at the top level we have the entirety of Gray.",
                    "label": 0
                },
                {
                    "sent": "Matter is 1 region, which decomposes into relatively small set of regions which then further decomposed into smaller and smaller regions in the brain.",
                    "label": 0
                },
                {
                    "sent": "Now throughout this talk I'm going to refer to the relationships in this hierarchy as a hierarchy, but that's actually not quite correct as you might be able to make out some of the nodes in this set of relationships actually have two parents.",
                    "label": 0
                },
                {
                    "sent": "Coming from them, so it's not a strict hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Instead, it's more general.",
                    "label": 0
                },
                {
                    "sent": "It's a trellis.",
                    "label": 0
                },
                {
                    "sent": "This will cause some issues that arrives and the paper will deal with these issues, but for the remainder of this talk I'm going to simply treat these relate.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shift is a hierarchical relationship.",
                    "label": 0
                },
                {
                    "sent": "So we have these brain regions and we're interested in understanding what the correlations between these brain regions are.",
                    "label": 0
                },
                {
                    "sent": "So we have a set of data, and then from this set of data we might find that certain regions or are correlated with certain other regions.",
                    "label": 0
                },
                {
                    "sent": "Now we need some framework for representing the relationships between these random variables and we use Bayesian networks to do this.",
                    "label": 0
                },
                {
                    "sent": "So what is the Bayesian network?",
                    "label": 0
                },
                {
                    "sent": "Well, Bayesian network is a graphical model in which each one of the random variables in the underlying system is represented as a node in this directed acyclic graph.",
                    "label": 0
                },
                {
                    "sent": "Correlations between the regions are represented as it is directed linked.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between the nodes now I'll refer to these nodes as either parents or children based on whether or not they originate or terminate these links and the fundamental structural unit of a Bayesian network is often referred to as a family of family consists of a single child, for instance, perhaps random variable like 6, and the set of all of that children's parents XX has only one parent, so this set of nodes right here represents one family in my Bayesian network, there's actually one family for each one of the nodes in the Bayesian network, I've just highlighted three of them here.",
                    "label": 0
                },
                {
                    "sent": "Inside each one of these families is a set of information that gives a conditional probability distribution, but the child takes on certain values given that the parent is taken on certain values, we assume that are random variables are going to be discrete in nature, so with discrete random variables, the conditional probability distribution can be represented as a conditional probability table, and that table gives the probability that the that the child takes on certain values, or given all possible combinations of the child and the parent, it parameterizes the probability of the child has taken on certain values given the parent house.",
                    "label": 0
                },
                {
                    "sent": "So for instance, here we have in the case that random variable X3 is in a low state, we have an entire multinomial associated with the distribution that the child takes on its certain values.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we're given a certain when we're given a set of data and we want to learn a Bayesian network to represent that data, there's actually two tasks.",
                    "label": 0
                },
                {
                    "sent": "The first task is learning the structure and the second task is to estimate the parameters of this Bayesian network.",
                    "label": 1
                },
                {
                    "sent": "Now, this talk is not about learning structure, it's about estimating parameters.",
                    "label": 0
                },
                {
                    "sent": "Once you have a structure, having an individual structure for your Bayesian network defines the set of parameters that you have in your Bayesian network.",
                    "label": 1
                },
                {
                    "sent": "So the only thing that I'm going to say about structure search is that it might be that you.",
                    "label": 0
                },
                {
                    "sent": "During structured search, want to estimate your parameters?",
                    "label": 0
                },
                {
                    "sent": "This is not a necessary step for doing Bayesian at structure search, but in the case that you would like to have your Bayesian structure search include parameterisations, we wanted our shrinkage estimators to be fast enough such that they can compute the parameters quickly enough to be done.",
                    "label": 1
                },
                {
                    "sent": "The very large number of times that that would have to be done during the structure surge, and the most common method for estimating the parameters for your Bayesian networks is to use the maximum likelihood estimate.",
                    "label": 0
                },
                {
                    "sent": "Computing the maximum likelihood estimate is straightforward, and the details.",
                    "label": 0
                },
                {
                    "sent": "Again, around the paper, unfortunately, using the matching likelihood estimate you tend to overfit your data, and perhaps you might not have certain events in your training data that occur in your testing data, and this can be problematic people tip.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we'll take the maximum likelihood estimate and smooth.",
                    "label": 0
                },
                {
                    "sent": "It stored some other distribution to deal with these problems.",
                    "label": 0
                },
                {
                    "sent": "Oh, use an example to demonstrate this.",
                    "label": 0
                },
                {
                    "sent": "So again, we have our brain and we have two small regions in our brain.",
                    "label": 0
                },
                {
                    "sent": "And let's say that we have some structure in our Bayesian that we have some family in our Bayesian net structure.",
                    "label": 0
                },
                {
                    "sent": "So we have this yellow.",
                    "label": 0
                },
                {
                    "sent": "This small yellow region in the small purple region and we're interested in the correlation between these two regions.",
                    "label": 0
                },
                {
                    "sent": "Now we have a conditional probability table associated with this family and will say that these values in this conditional probability table.",
                    "label": 0
                },
                {
                    "sent": "Represent the maximum likelihood estimate for this relationship.",
                    "label": 0
                },
                {
                    "sent": "We can already see 2 problems in this little toy example right here.",
                    "label": 0
                },
                {
                    "sent": "I have these two zero percent values and these occur because this event here, the parent being in a very low state and the child being in a high state never occurred in my training data.",
                    "label": 0
                },
                {
                    "sent": "If this event had a curd in my testing data.",
                    "label": 0
                },
                {
                    "sent": "For instance, if I was trying to classify new point based on a Bayesian up structure that I was learning, I would invalidate my likelihoods, which are really just products or sums of these probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do then is we take this maximum likelihood estimate an in a very common smoothing technique referred to as a posse and smoothing.",
                    "label": 0
                },
                {
                    "sent": "We smooth this distribution and I'm just going to refer to this one multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "For now, we smooth that order uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "So after we smooth it towards the uniform distribution, we now have a distribution that's more like the uniform distribution before.",
                    "label": 0
                },
                {
                    "sent": "But the smoothing processes added a certain amount of bias in RS2.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mating procedure.",
                    "label": 0
                },
                {
                    "sent": "And that might bias may unfortunately be unrealistic.",
                    "label": 0
                },
                {
                    "sent": "Why are we smoothing tored the uniform distribution?",
                    "label": 1
                },
                {
                    "sent": "Well, it's been shown that using a lossy and smoother does in fact help Bayesian that parameter estimation, but it could be in certain domains.",
                    "label": 0
                },
                {
                    "sent": "This uniform distribution is completely unrealistic.",
                    "label": 1
                },
                {
                    "sent": "If we were to look at the relationship between regions in the brain, they're very rarely described by some type of uniform distribution.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is there some better way that we can come up with the distribution that we're trying to smooth towards in our estimation process and we of course say that there is and the technique that the general technique that we're going to be using is referred to as shrinkage.",
                    "label": 0
                },
                {
                    "sent": "Shrinkage was introduced in 1955 by Stein and he recognized, or he demonstrated that the maximum likelihood estimator in certain cases was more risky than another class of estimators.",
                    "label": 1
                },
                {
                    "sent": "The estimator that he referred to as shrinkage estimators.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood estimators tended to overestimate.",
                    "label": 0
                },
                {
                    "sent": "The random variable.",
                    "label": 0
                },
                {
                    "sent": "So his idea was in any system that has at least three random variables.",
                    "label": 0
                },
                {
                    "sent": "If you can take your maximum likelihood estimator and smooth it towards some value computed based on all the other random variables in your system, he called the shrinkage as this tended to lower the value of your your your estimation.",
                    "label": 0
                },
                {
                    "sent": "Now I'm not the first to propose that shrink, it should be used in learning the parameters for Bayesian networks or in machine learning.",
                    "label": 0
                },
                {
                    "sent": "General free tagging, McCollum McCollum at all, and Grossman Domingos have previously demonstrated that when you have a class when you have a hierarchy of classes and you're learning parameters.",
                    "label": 0
                },
                {
                    "sent": "In Bayesian network like models, for each one of these classes, the shrinking the estimate for one of your parameters in one class towards the estimate for your parameter in another class can often improve the quality of our models.",
                    "label": 0
                },
                {
                    "sent": "So I demonstrate the case in which you don't have multiple classes of data, but instead you just have one class of data and instead of shrinking similar parameters across classes of data, I suggest.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shrink your values towards other distributions that you find in your data.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to the example that we talked about.",
                    "label": 0
                },
                {
                    "sent": "We have these two small regions that are linked together and they have a maximum likelihood estimate that we've learned from the data well in the example that we work with, we have this hierarchy of random variables.",
                    "label": 0
                },
                {
                    "sent": "So these two random variables don't exist in isolation in our data.",
                    "label": 0
                },
                {
                    "sent": "In fact, this small purple region is a component of this larger purple region and small yellow region is in turn a component of this larger yellow region.",
                    "label": 0
                },
                {
                    "sent": "So when we ask what distribution should be smooth towards, we make the suggestion that why don't we smooth towards the relationship between the two larger regions that the two smaller regions compose, so we can easily calculate the maximum likelihood estimate for this distribution as well.",
                    "label": 0
                },
                {
                    "sent": "And I suggest smoothing towards that distribution.",
                    "label": 0
                },
                {
                    "sent": "Now this estimate right here will have low lower variance in some sense because each of these regions are larger regions and they're based off of more data.",
                    "label": 0
                },
                {
                    "sent": "But smoothing towards this distribution still adds a certain amount of bias to our estimator.",
                    "label": 0
                },
                {
                    "sent": "However, it's a reasonable bias.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's a bias that exists within our data, so it's not as unrealistic as the uniform distribution might be.",
                    "label": 0
                },
                {
                    "sent": "Now, as you may recall in the actual hierarchy that I showed, there were four different hierarchical levels here.",
                    "label": 0
                },
                {
                    "sent": "In this toy example, I just have two, so in our real experiments I'm not smoothing Tord just one of their distribution, but I'm.",
                    "label": 0
                },
                {
                    "sent": "Actually smoothing towards three or four distributions on our data.",
                    "label": 0
                },
                {
                    "sent": "It could also be that smoothing towards the uniform distribution is helpful, so I'm still going to allow myself to smooth towards the uniform distribution.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "So the question then is, how much are we going to smooth again?",
                    "label": 0
                },
                {
                    "sent": "Now we have the relationship that we're actually interested in.",
                    "label": 0
                },
                {
                    "sent": "The relationship of regions one level up on the hierarchy and the uniform distribution associated, each with each of these will be the maximum likelihood estimate or just the uniform distribution itself.",
                    "label": 0
                },
                {
                    "sent": "I'll represent these conditional probability tables simply with these values, Theta, zero Hat, Theta, one hat and Theta two hat.",
                    "label": 0
                },
                {
                    "sent": "So then my task then is to combine these data parameters somehow to come up with my final estimate, like those before me.",
                    "label": 0
                },
                {
                    "sent": "I've chosen to use a linear combination of these parameters, so my task is then to calculate or to estimate the values for the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mixing coefficients.",
                    "label": 0
                },
                {
                    "sent": "So how do we figure out what the values for these lambdas?",
                    "label": 1
                },
                {
                    "sent": "These mixing coefficients should be?",
                    "label": 0
                },
                {
                    "sent": "Well, in order to do that, we need to understand what we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "We're trying to come up with a set of parameters that generalize to entire class of data as effectively as possible.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can take our training data and then we can split it up into the smaller component, which I'll still refer to as training data, and this left out component, so the smaller component of training data is the component that I'm using to train all of my maximum likelihood estimators and then the left out data.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there's a slight.",
                    "label": 0
                },
                {
                    "sent": "Typo here in my slide this is going to be the maximum likelihood estimate for my left out data.",
                    "label": 0
                },
                {
                    "sent": "I'm going to choose the mixing coefficients such that my final estimate is going to be as close as possible to the maximum likelihood estimate in my left out data.",
                    "label": 0
                },
                {
                    "sent": "Given that I'm using the maximum likelihood estimates for my training data.",
                    "label": 1
                },
                {
                    "sent": "So what this means is that I'm going to be selecting the parameters such that I'm maximizing the likelihood of my left out data.",
                    "label": 1
                },
                {
                    "sent": "We can do this by simply algebraically inverting a solution in which we have some Theta hat matrix, but unfortunately some of these Theta values will be very similar to each other.",
                    "label": 1
                },
                {
                    "sent": "This is what we found in our work, so this required the inversion of a nearly singular matrix.",
                    "label": 0
                },
                {
                    "sent": "There's ways to get around this, but like previous work, we found that an expectation maximization algorithm works quite well for determining what these Lambda values should be, so we use and even algorithm to do this, and the details for that or.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also included in the paper.",
                    "label": 0
                },
                {
                    "sent": "So as I've described it so far, we're inefficiently using our training data.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you won't have very much training data to train your Bayesian network, so we use a cross validation method to help us more effectively use our data.",
                    "label": 0
                },
                {
                    "sent": "An instead of learning one set of maximum likelihood estimators, we now learn a small set will say 4, so during each one of these cross validation folds are learn an independent set of these maximum likelihood distributions.",
                    "label": 0
                },
                {
                    "sent": "But I'm still going to learn.",
                    "label": 0
                },
                {
                    "sent": "Just a single set of these Lambda mixing values.",
                    "label": 0
                },
                {
                    "sent": "So what I'm doing now is I'm selecting the Lambda mixing value such that the difference between the estimators that I'm coming up with and the maximum likelihood estimate of the left out data is minimized simultaneously across all of my cross validation folds.",
                    "label": 0
                },
                {
                    "sent": "So now all of my training data is used simultaneously or is used to estimate my maximum likelihood estimates as well as my.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which coefficients?",
                    "label": 0
                },
                {
                    "sent": "So I won't go into the details of the algorithm, but I thought it would.",
                    "label": 0
                },
                {
                    "sent": "I would share the evolution of mixture weights in the shrinkage algorithm, so recall that I have 4 levels in my hierarchy, so I'll have Lambda zero through Lambda, 3 being the coefficients for each of the correlation, or for each of the conditional probability tables, and then Lambda four will represent the weight associated with the with the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "So what we see here in this EM algorithm?",
                    "label": 0
                },
                {
                    "sent": "So the X axis is the iterations of the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "The Y axis is how much weight they were associated with with each mixture coefficient, Lambda 0 gets a tremendous amount of weight and this makes sense.",
                    "label": 0
                },
                {
                    "sent": "'cause Lambda zero corresponds to the maximum likelihood estimate for the relationship that we're interested in Lambda 2, then, which is going to be 2 levels up in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So a relationship between 2 random variables that are two levels up in the hierarchy gets a significant amount of weight, but Lambda one, Lambda three and Lambda 4 get a small but non zero amount of weight.",
                    "label": 0
                },
                {
                    "sent": "So we're still smoothing towards the uniform distribution to a small degree.",
                    "label": 1
                },
                {
                    "sent": "But the EM algorithm is finding that.",
                    "label": 0
                },
                {
                    "sent": "We should instead be smoothing towards Lambda two far more than we are in Laplacian smoothing which would just be smoothing to Lambda 4.",
                    "label": 0
                },
                {
                    "sent": "OK, so as I mentioned, we wanted to be able to do this during structured search.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately with the number of iterations that the same algorithm was requiring, it was simply too slow to do that with some relatively straightforward modifications to the EM algorithm, we found that we were able to in the details for that or in the paper we found that we were able to.",
                    "label": 1
                },
                {
                    "sent": "Dramatically decrease the number of EM algorithm, M iterations that we that we required.",
                    "label": 0
                },
                {
                    "sent": "So instead of needing hundreds of iterations, we now need generate between 5:00 and 10:00.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so we have results on two sets of experiments.",
                    "label": 0
                },
                {
                    "sent": "First we have some simulated data in which we can control the ground truth and then we have some neuroimaging data.",
                    "label": 1
                },
                {
                    "sent": "So in the simulating data, what we basically do is we construct a random Bayesian network that has some random links and we can control the strength of the correlations between the random variables in this Bayesian network and then we generate a set of data and we learn a Bayesian net structure on that data an.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry we don't learn that the structure we learn the parameterisations and then we can compare the parameters that we learn.",
                    "label": 0
                },
                {
                    "sent": "With the original permit.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that we generated the data with?",
                    "label": 0
                },
                {
                    "sent": "So what we have here is the result of 1 set of our experiments.",
                    "label": 0
                },
                {
                    "sent": "In order to perform Laplacian smoothing you have to determine how much you want to smooth towards the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "So we have that here on the X axis on the Y axis we have the amount of difference, just as the pairwise difference between all the elements in a conditional probability table with the learned estimate and the actual estimates that generated the data, we find that even when we know the exact value, or even when we can empirically determine the value to smooth towards for maximum likelihood estimate which occurs right here.",
                    "label": 0
                },
                {
                    "sent": "It minimizes the difference between our learned estimate and the actual estimates.",
                    "label": 0
                },
                {
                    "sent": "The shrinkage estimate, which doesn't have a Laplace smoothing constant.",
                    "label": 1
                },
                {
                    "sent": "Of course, I'm just plotting it here as a reference line is capable of coming up with a set of conditional probability tables that are closer to the actual problem.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The values that generated the data, so there's some other experiments inside of our data in which we determine whether or not the shrinkage estimate is more robust to noise on our simulated data.",
                    "label": 1
                },
                {
                    "sent": "But the true experiments are really on the neuroimaging data, so in the neuroimaging data, the actual estimates are unknown.",
                    "label": 1
                },
                {
                    "sent": "We don't know what the structure for the Bayesian network should be, nor do we know what the parameterisations for each individual structure would be.",
                    "label": 0
                },
                {
                    "sent": "So instead of measuring the difference between conditional probability tables, we now leave some of our training data out, and we compute the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Of our left out data on Bayesian networks that we learned either with maximum likelihood estimate or shrink adjust.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is what the results will look like.",
                    "label": 0
                },
                {
                    "sent": "So on both axes we have negative log likelihood, so a smaller value indicates a larger likelihood of our left out data on the X axis.",
                    "label": 1
                },
                {
                    "sent": "We have Bayesian networks trained with maximum likelihood estimates with Laplacian smoothing and on the Y axis we have the same Bayesian net structure trained with shrinkage parameters instead.",
                    "label": 0
                },
                {
                    "sent": "So this one point right here has a lower negative log likelihood for shrinkage than it does for maximum likelihood estimate.",
                    "label": 1
                },
                {
                    "sent": "So that means the likelihood of this point.",
                    "label": 1
                },
                {
                    "sent": "ARM is actually larger on the Bayesian networks that have shrinkage parameters, so in essence the shrinkage parameters are doing a better job of generalizing to this data point.",
                    "label": 0
                },
                {
                    "sent": "And again this is a left out piece of data.",
                    "label": 0
                },
                {
                    "sent": "This was no way used to train the Bayesian that structure the parameters or the mixing.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Efficient, so here's our set of results on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "We have Bayesian networks that were allowed to have two parents on average per family.",
                    "label": 0
                },
                {
                    "sent": "The random variables that we use have a narrative four and this results in a total of 64 parameters per family.",
                    "label": 0
                },
                {
                    "sent": "What we find in almost every case is the left out data has a higher likelihood, or in this case a lower negative log likelihood.",
                    "label": 1
                },
                {
                    "sent": "If we use shrinkage to train our parameters, then when we use maximum likelihood estimate with Laplace smoothing, and this is going to be lost moving where we empirically determine what the best weight should be.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, as we go to more parents per family, so now we have four parents per family, which results in over 1000 parameters per family.",
                    "label": 1
                },
                {
                    "sent": "So now we have about 150,000 parameters in our Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "We find the improvement from using shrinkage is far more dramatic, so when we add this many more parameters to our Bayesian network, the amount of data that we have associated to train each one of the parameters goes dramatically down, and we find that smoothing towards the uniform distribution is hurting our ability to generalize to our left out data.",
                    "label": 0
                },
                {
                    "sent": "Much more so than when we use our shrinkage.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Estimates so with that, I'll conclude what we've done in this work.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, is we've introduced a shrinkage estimator which allows our Bayesian networks to generalize to elements of the same class more effectively.",
                    "label": 1
                },
                {
                    "sent": "I've shown that on simulated data we can find conditional probability tables that were more like the conditional probability tables that generated the data on an entire another set of experiments that I didn't go through by adding random noise into both our simulated data in our neuroimaging data, we found that shriek adjustments changed less than the actual maximum likelihood estimate, so.",
                    "label": 0
                },
                {
                    "sent": "Shrink adjustments were also more robust in the presence of noise than the maximum likelihood estimates.",
                    "label": 0
                },
                {
                    "sent": "Our current work involves applying this shrinkage technique to learn parameters for Bayesian networks on domain on other domains that have natural hierarchy.",
                    "label": 0
                },
                {
                    "sent": "We're currently working with some gene data in which genes naturally grouped together into gene group groupings, which then group into larger and larger gene groupings were also very interested in applying this technique to flat datasets that don't have a natural hierarchy.",
                    "label": 1
                },
                {
                    "sent": "There's no notion of a hierarchy in the original work proposed by shrinkage.",
                    "label": 0
                },
                {
                    "sent": "So we believe that shrinkage techniques will be able to do well in these flat domains.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In addition, so I'd like to acknowledge the Mind Institute and the Dartmouth fMRI DC for providing access to their neuroimaging datasets this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a list of my sources of funding, and with that I'll conclude.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}