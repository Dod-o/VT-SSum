{
    "id": "alzl5u4pljgms6nykkhsvkdtmyiu7c6y",
    "title": "DBpedia SPARQL Benchmark \u2013 Performance Assessment with Real Queries on Real Data",
    "info": {
        "author": [
            "Mohamed Morsey, University of Leipzig"
        ],
        "published": "Nov. 25, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Semantic Search"
        ]
    },
    "url": "http://videolectures.net/iswc2011_morsey_real/",
    "segmentation": [
        [
            "The motivation for building benchmarks in general is to identify and determined the weak points and strong points of the systems under test.",
            "It also helps in building more and more advanced than more efficient benchmarks.",
            "We already know that there is a kind of.",
            "Uh, there's a benchmark for databases called TPC, which is mature enough to test the performance of databases, but we don't have.",
            "Such like mature mature benchmark for RDF data.",
            "So this is the motivation for building our benchmark."
        ],
        [
            "Here we will give a simple comparison between our pinch more and the other existing punch marks.",
            "We would compare our benchmark to Logan Benchmark, SP2 pinch and also burning Sparkle benchmark.",
            "Regarding the test data.",
            "The other benchmarks used synthetic or artificial data, whereas in our case we use real data."
        ],
        [
            "Also, regarding the queries we use the query log of the official DPS sparkle endpoint.",
            "So we used our our queries based on this query log, so we used real queries, whereas in the other benchmarks they used synthetic or artificial queries."
        ],
        [
            "At large datasets also is varying.",
            "For example, if we consider a loop, there is only 6.9 million triples, which is quite limited, whereas in our case we tested up to 300 mini."
        ],
        [
            "Triples regarding the number of queries and the other benchmarks, there are varying number of queries between 12 and 14 queries.",
            "In our case, we have tested our benchmark based on 25."
        ],
        [
            "Queries also the use cases are differing from 141 from one benchmark to the other.",
            "Some of them is based on E Commerce, the other is based on universities.",
            "In our case we use DPD as our use case."
        ],
        [
            "The most important point is the number of clauses and also number of properties."
        ],
        [
            "This is big cause it identifies how her true genius is.",
            "The RDF data on which we will base our benchmark, for example in SP2 bench.",
            "There there are only 8 clauses and 22 properties, which means that the generated data is somewhat homogeneous.",
            "But in our case in the case of the pedia we have more than 230 classes and about 1200 different properties, which means that the data we use is more heterogeneous.",
            "And this is true also for other real RDF data."
        ],
        [
            "Here is our approach.",
            "In general we we have a data set or a several data sets of different sizes and also the query log from which we will get our query list and also triple store or triple stores that will be tested for performance.",
            "We apply our methodology on them in order to get this benchmark results we will go into details of this of these steps in the next."
        ],
        [
            "In the next slide here we will describe the process of.",
            "Query log and from this query log how can we get the final query list we used in our benchmark?",
            "First we apply some process called preprocessing in order to get clean queries.",
            "After this we apply the clustering method in order to get query clusters out of these clusters.",
            "We apply manual analysis to get the final query list that will be used in our benchmark.",
            "We will also go into details of all of these steps in the preprocessing, clustering and feature selection."
        ],
        [
            "Here we will describe."
        ],
        [
            "In the pre processing.",
            "Here.",
            "We have a query log.",
            "We apply a variable unification in order to rename all the variables of the query becausw.",
            "The same query can be can be asked several times but with different variable names.",
            "We will also show this with an example in the next slide.",
            "After this we get the common queries.",
            "Then we discard the uncommon queries in order to in order to remove the queries that are not not that are not frequently asked out of this.",
            "We apply the common the common queered keyword removal in order to remove the keywords that are not relevant for our approach.",
            "Like the select statement like the where statement because it is almost in All in all queries, so we remove them in order to focus only on the query structure or the query body will."
        ],
        [
            "Here will give an example about the variable variable ramification if we have this query.",
            "We will rename all the variables O&S like this.",
            "All of the variables will be var, node, var one, var two and so on.",
            "So all the queries will be unified.",
            "So if the same query was asked several times, it will look like the same.",
            "After we apply this renaming, renaming procedure then we will get rid of the common keywords and also the white spaces like select and where like this.",
            "So we get this part.",
            "This will be used for for the query similarity.",
            "In which we will use later for the clustering as we will describe in the."
        ],
        [
            "Slide now will describe the cluster."
        ],
        [
            "Process.",
            "For the clustering process, we have clean queries after we have removed all the common keywords and focused only on the query body, we apply similarity computation in order to calculate how similar is the queries to each other.",
            "Then we apply the clustering method in order to get finally the query clusters."
        ],
        [
            "And for the similarity computations, after we have removed the uncommon queries, we get about 36,000 different different queries in order to apply the similarity about over them, which is based on the Levenshtein algorithm, which is order N squared, we need about 1.3 billion computations, which is quite large.",
            "So the solution is to use a tool called lines which is also based on the definition algorithm.",
            "But it is more efficient and it is also much faster, so we can downsize the number of computation instead of 1.3 billion to only 16% of the 1.3 billion, which is quite faster without any loss of the efficiency."
        ],
        [
            "Regarding the clustering process we need, we need we need 2 requirements to exist in the clustering algorithm we use first.",
            "It should be time efficient, which means that it is fast.",
            "It should be also applying soft clustering.",
            "This is because the same query may exist in several clusters based on its features.",
            "For example, a query can contain optional union and filter, so it may exist in several clusters.",
            "So the solution to this is to use the Porter flow application clustering application.",
            "Which is used for soft clustering.",
            "After after we after we apply this method we get about 15,000 different clusters.",
            "Most of them contain only one or two or two queries.",
            "So we discarded the clusters that contain one or two or three or four queries and consider only the clusters that contain 5 queries and more."
        ],
        [
            "Here we will describe the process of feature selection on which we will get the final query list that will be used."
        ],
        [
            "In our benchmark.",
            "This process is divided into 2 sub processes.",
            "First we apply the Sparkle feature selection in order to get the query list and we also then apply a strategy called query variability which is very important and we will describe it in the."
        ],
        [
            "Slide here in the queries we use, we have covered many features in sparkle like Union, distinct Lang and others.",
            "We"
        ],
        [
            "You have also applied this, this this strategy which."
        ],
        [
            "It's called query variability.",
            "This strategy is very important because if we use the same query to ask the same triple store over and over, this increases the probability of caching.",
            "So in this using this strategy we introduce a small variability to this query, so each time it is sent to the Apple Store it will be different like this exam."
        ],
        [
            "Like this example that will describe here if we have this query, we select a part of this query, which can be a UI or electoral, we mean a static constant part and then replace it with a placeholder here like this and use another query called the auxiliary query in order to fill this placeholder.",
            "So here the exhilarate query which is helpful for this query.",
            "Here we get 1000 different values for this placeholder and each time.",
            "We contact the triple store.",
            "We use a different value, a different value at random from from this 1000.",
            "So we decrease the probability of cashing in the Triple Store.",
            "He"
        ],
        [
            "We will get back to our approach because we will describe the process of this generation, which is apparent process through the process of query."
        ],
        [
            "Analysis.",
            "The main requirements for queried for this generation is that it should resemble the original data.",
            "It should allow the generation of datasets of different sizes.",
            "It should be applicable also to different two different datasets, not only DP.",
            "Pedia should be applicable to other datasets."
        ],
        [
            "Here is the main components of this generation process.",
            "We have an input data set which is the pedia in our case on which we apply a data set generator.",
            "In order to get data sets of different sizes.",
            "Then we load this data into the triple stores under test."
        ],
        [
            "Here we have two types of datasets.",
            "Smaller datasets which are the datasets that are smaller than the PPD itself.",
            "And larger datasets that are datasets which is larger than the PD itself.",
            "For generation of smaller datasets.",
            "For the generation of a smaller datasets, we start by selecting all classes of the Pedia in order to cover all the classes of the pedia.",
            "So we maintain the heterogeneity of the output data set.",
            "Then we we start by selecting something called seed innocences.",
            "See those seed innocences are used to follow the links.",
            "Yeah, in the in the PDF.",
            "For example, if we start with this instance and follow its length, it may lead to another instance in another class.",
            "We follow this link and this triple is considered an output triple that will be placed to the output data set.",
            "Afterwards we continue following the links, which leads us to another innocence which may be in the same class or in another class.",
            "This is also an output triple.",
            "Then we continue following the links so we can.",
            "We can be lead it to another instance in the same class."
        ],
        [
            "Continue following the links till we have reached all the triples that that we need.",
            "So in this step we stopped when we have reached the required number of triples."
        ],
        [
            "For larger datasets, which is a data set larger than BPD itself, we only duplicate all triples and just introduce a slight change to the namespace, which is instead of using the pedia.org, we use dpdatul.org as here in the.",
            "In the figure we have this class.",
            "With this innocence we just create another copy of it.",
            "With this change, instead of using the pedia, we use the PDF 2."
        ],
        [
            "Here regarding the system setup, we have applied our method on four different triple stores, namely virtues or Sesame, Big Island and Jenna TDB.",
            "We have used the same machine both as a client and at the server in order to in order to avoid any network traffic.",
            "We have also used the same setup for all for all triple stores in order to maintain the furnace between them.",
            "Our our benchmarking strategy consists of the following.",
            "First we have we apply a system restart in order to clear all caches.",
            "After this we apply a warm up phase of 20 minutes.",
            "After this we apply 60 minutes of hot run phase.",
            "Then after this this phase we got the actual benchmark result."
        ],
        [
            "Here we discuss.",
            "The results for 10% data set 5th."
        ],
        [
            "The person does it."
        ],
        [
            "For 100, is it actually we've based our benchmark on four different data set size is 10% which is 10% of DPD triples 50%, which is half of the pedia triples and 100 which is DPD itself as fully stand and 200 DB pedia which is double sized pedia.",
            "Here we can.",
            "We can describe some of the some of the running times over the queries.",
            "For example query 2.",
            "Here we can see that Virtusa LAN Sesame runs almost as fast as each other.",
            "But here when that data set size changes from 10% to 50%.",
            "Here we can see see that Sesame wins the competition and it becomes faster.",
            "Also for the transition from 100 to 200 week."
        ],
        [
            "And see here that this this query for example here.",
            "It was about 60, but here for 200 it's almost almost 40.",
            "We can see here also, this query query 16 was about 140, but here when the data set is double sized it runs faster.",
            "It's about 180.",
            "We will also describe the slow and fast queries in the coming slides."
        ],
        [
            "Here.",
            "Four virtues of the slowest queries for Virtusa is query number 10, which contains filter, distinct and optional.",
            "So we can conclude from this that.",
            "This combination of features is really problematic for Virtuoso.",
            "For system we have for three.",
            "Various query, four query 15 and query 18 for query four.",
            "It contains only union but with too many free variables.",
            "For query 18 and query 15, both of them contain union filter and blank as features, but query 18 contains STR string function as an extra feature, so we can conclude that this combination of features, union filter and length is heavy for four sesame and if we add another feature which is STR, this this place is this place is more burden over sesame and makes the whole query even slower.",
            "For Jenna TDB, there are two.",
            "There are two slow queries.",
            "Query 10 and query 20 queries and was already discussed with their chosen and pretend contains these features.",
            "Filter optional union and Lang.",
            "We can conclude also that this combination of features is heavy for Jenna TDB for general.",
            "For a big album, there are two slow queries, Query 10, which is already discussed with virtue and Query 15, which was already discussed with Jesse."
        ],
        [
            "Regarding the first queries, we can conclude that all of them are fast with query number one which contains only distinct as a feature, where Jenna TDB is fast with query two which contains filter only."
        ],
        [
            "Here.",
            "We will talk about the query mixes per hour, which gives a Broadview Broadview about the performance of the triple stores.",
            "Here we can conclude that virtue is the fastest triple store of all of them.",
            "Then be goulem, then come sesame, and finally comes.",
            "Jenna TDB."
        ],
        [
            "Regarding the scalability, which means how the performance of a triple store changes when the data set size changes from, for example, 10% of the pedia to 50% and from 50% to 100% and so on.",
            "We can say that we're choosing has the best scalability of them all and then big alaman system.",
            "We are competing for the second best scalability.",
            "For smaller datasets sesame is better or I mean for the transition from one 110% to 50%.",
            "But for larger deficits when this is it's size changes from 50% to 100% and from 100% to 200% big wins the competition and it becomes better generally is only better when the.",
            "Transition becomes from 100% DP pedia to 200% DP."
        ],
        [
            "Regarding the conclusion, we can say that we have built a new benchmark based on real data and real queries.",
            "The main advantage of this benchmark is that it is not only applicable on the pedia, but we can apply it on other datasets like link to data.",
            "For example, we have also introduced a new strategy of query variability, which means that we do not use the same query to contact the same triple store over and over any state we introduce a slight change to the same query in order to.",
            "Decrease the probability of cash."
        ],
        [
            "For the future work, we have several direction to go further with our benchmark.",
            "First, we can we should.",
            "We should enhance our data set generation because the reviewers of our paper has suggested that we can use a paper called apples and oranges for apples and oranges with apples and oranges.",
            "For RDR, for RDF benchmarking, which was published in segment 2011.",
            "Which of the scribes a bitter at this age generation than ours?",
            "We can adopt this this generation and the next version of our benchmark we can use also more features.",
            "For example, the features that exist in Sparkle 1.1.",
            "We can also evaluate the triplestores when the update process is applied.",
            "For example, when we add more triples or we when we delete more triples.",
            "We can also test our systems.",
            "The triple stores.",
            "I mean, when there is an is enabled be cause.",
            "For example, the developers of Big Column have spent too much time in making their system better and better in this sense.",
            "And we can also adopt more triple stores like 4 store or our graph.",
            "We can, we must stay here that there is a kind of or it might be a kind of bias becausw our query log.",
            "We are the query log we have based our benchmark on is based on the official end point of the pedia, which is hosted on virtual.",
            "So there might be some queries that were discarded because of they are uncommon queries.",
            "But this is because they are slow on virtual say so they were sent just once or twice because it takes too much time on their chosen.",
            "So we should also handle this process.",
            "We can also apply our benchmark using other resources like link due data."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The motivation for building benchmarks in general is to identify and determined the weak points and strong points of the systems under test.",
                    "label": 0
                },
                {
                    "sent": "It also helps in building more and more advanced than more efficient benchmarks.",
                    "label": 0
                },
                {
                    "sent": "We already know that there is a kind of.",
                    "label": 0
                },
                {
                    "sent": "Uh, there's a benchmark for databases called TPC, which is mature enough to test the performance of databases, but we don't have.",
                    "label": 0
                },
                {
                    "sent": "Such like mature mature benchmark for RDF data.",
                    "label": 0
                },
                {
                    "sent": "So this is the motivation for building our benchmark.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we will give a simple comparison between our pinch more and the other existing punch marks.",
                    "label": 0
                },
                {
                    "sent": "We would compare our benchmark to Logan Benchmark, SP2 pinch and also burning Sparkle benchmark.",
                    "label": 0
                },
                {
                    "sent": "Regarding the test data.",
                    "label": 0
                },
                {
                    "sent": "The other benchmarks used synthetic or artificial data, whereas in our case we use real data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, regarding the queries we use the query log of the official DPS sparkle endpoint.",
                    "label": 0
                },
                {
                    "sent": "So we used our our queries based on this query log, so we used real queries, whereas in the other benchmarks they used synthetic or artificial queries.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At large datasets also is varying.",
                    "label": 0
                },
                {
                    "sent": "For example, if we consider a loop, there is only 6.9 million triples, which is quite limited, whereas in our case we tested up to 300 mini.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Triples regarding the number of queries and the other benchmarks, there are varying number of queries between 12 and 14 queries.",
                    "label": 0
                },
                {
                    "sent": "In our case, we have tested our benchmark based on 25.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Queries also the use cases are differing from 141 from one benchmark to the other.",
                    "label": 0
                },
                {
                    "sent": "Some of them is based on E Commerce, the other is based on universities.",
                    "label": 0
                },
                {
                    "sent": "In our case we use DPD as our use case.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The most important point is the number of clauses and also number of properties.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is big cause it identifies how her true genius is.",
                    "label": 0
                },
                {
                    "sent": "The RDF data on which we will base our benchmark, for example in SP2 bench.",
                    "label": 0
                },
                {
                    "sent": "There there are only 8 clauses and 22 properties, which means that the generated data is somewhat homogeneous.",
                    "label": 0
                },
                {
                    "sent": "But in our case in the case of the pedia we have more than 230 classes and about 1200 different properties, which means that the data we use is more heterogeneous.",
                    "label": 0
                },
                {
                    "sent": "And this is true also for other real RDF data.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is our approach.",
                    "label": 0
                },
                {
                    "sent": "In general we we have a data set or a several data sets of different sizes and also the query log from which we will get our query list and also triple store or triple stores that will be tested for performance.",
                    "label": 0
                },
                {
                    "sent": "We apply our methodology on them in order to get this benchmark results we will go into details of this of these steps in the next.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the next slide here we will describe the process of.",
                    "label": 0
                },
                {
                    "sent": "Query log and from this query log how can we get the final query list we used in our benchmark?",
                    "label": 1
                },
                {
                    "sent": "First we apply some process called preprocessing in order to get clean queries.",
                    "label": 0
                },
                {
                    "sent": "After this we apply the clustering method in order to get query clusters out of these clusters.",
                    "label": 0
                },
                {
                    "sent": "We apply manual analysis to get the final query list that will be used in our benchmark.",
                    "label": 0
                },
                {
                    "sent": "We will also go into details of all of these steps in the preprocessing, clustering and feature selection.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we will describe.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the pre processing.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We have a query log.",
                    "label": 1
                },
                {
                    "sent": "We apply a variable unification in order to rename all the variables of the query becausw.",
                    "label": 0
                },
                {
                    "sent": "The same query can be can be asked several times but with different variable names.",
                    "label": 0
                },
                {
                    "sent": "We will also show this with an example in the next slide.",
                    "label": 0
                },
                {
                    "sent": "After this we get the common queries.",
                    "label": 0
                },
                {
                    "sent": "Then we discard the uncommon queries in order to in order to remove the queries that are not not that are not frequently asked out of this.",
                    "label": 0
                },
                {
                    "sent": "We apply the common the common queered keyword removal in order to remove the keywords that are not relevant for our approach.",
                    "label": 0
                },
                {
                    "sent": "Like the select statement like the where statement because it is almost in All in all queries, so we remove them in order to focus only on the query structure or the query body will.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here will give an example about the variable variable ramification if we have this query.",
                    "label": 0
                },
                {
                    "sent": "We will rename all the variables O&S like this.",
                    "label": 0
                },
                {
                    "sent": "All of the variables will be var, node, var one, var two and so on.",
                    "label": 0
                },
                {
                    "sent": "So all the queries will be unified.",
                    "label": 0
                },
                {
                    "sent": "So if the same query was asked several times, it will look like the same.",
                    "label": 0
                },
                {
                    "sent": "After we apply this renaming, renaming procedure then we will get rid of the common keywords and also the white spaces like select and where like this.",
                    "label": 0
                },
                {
                    "sent": "So we get this part.",
                    "label": 0
                },
                {
                    "sent": "This will be used for for the query similarity.",
                    "label": 0
                },
                {
                    "sent": "In which we will use later for the clustering as we will describe in the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slide now will describe the cluster.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Process.",
                    "label": 0
                },
                {
                    "sent": "For the clustering process, we have clean queries after we have removed all the common keywords and focused only on the query body, we apply similarity computation in order to calculate how similar is the queries to each other.",
                    "label": 0
                },
                {
                    "sent": "Then we apply the clustering method in order to get finally the query clusters.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the similarity computations, after we have removed the uncommon queries, we get about 36,000 different different queries in order to apply the similarity about over them, which is based on the Levenshtein algorithm, which is order N squared, we need about 1.3 billion computations, which is quite large.",
                    "label": 0
                },
                {
                    "sent": "So the solution is to use a tool called lines which is also based on the definition algorithm.",
                    "label": 0
                },
                {
                    "sent": "But it is more efficient and it is also much faster, so we can downsize the number of computation instead of 1.3 billion to only 16% of the 1.3 billion, which is quite faster without any loss of the efficiency.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regarding the clustering process we need, we need we need 2 requirements to exist in the clustering algorithm we use first.",
                    "label": 0
                },
                {
                    "sent": "It should be time efficient, which means that it is fast.",
                    "label": 0
                },
                {
                    "sent": "It should be also applying soft clustering.",
                    "label": 0
                },
                {
                    "sent": "This is because the same query may exist in several clusters based on its features.",
                    "label": 0
                },
                {
                    "sent": "For example, a query can contain optional union and filter, so it may exist in several clusters.",
                    "label": 0
                },
                {
                    "sent": "So the solution to this is to use the Porter flow application clustering application.",
                    "label": 0
                },
                {
                    "sent": "Which is used for soft clustering.",
                    "label": 0
                },
                {
                    "sent": "After after we after we apply this method we get about 15,000 different clusters.",
                    "label": 0
                },
                {
                    "sent": "Most of them contain only one or two or two queries.",
                    "label": 0
                },
                {
                    "sent": "So we discarded the clusters that contain one or two or three or four queries and consider only the clusters that contain 5 queries and more.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we will describe the process of feature selection on which we will get the final query list that will be used.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our benchmark.",
                    "label": 0
                },
                {
                    "sent": "This process is divided into 2 sub processes.",
                    "label": 0
                },
                {
                    "sent": "First we apply the Sparkle feature selection in order to get the query list and we also then apply a strategy called query variability which is very important and we will describe it in the.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slide here in the queries we use, we have covered many features in sparkle like Union, distinct Lang and others.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have also applied this, this this strategy which.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's called query variability.",
                    "label": 0
                },
                {
                    "sent": "This strategy is very important because if we use the same query to ask the same triple store over and over, this increases the probability of caching.",
                    "label": 0
                },
                {
                    "sent": "So in this using this strategy we introduce a small variability to this query, so each time it is sent to the Apple Store it will be different like this exam.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this example that will describe here if we have this query, we select a part of this query, which can be a UI or electoral, we mean a static constant part and then replace it with a placeholder here like this and use another query called the auxiliary query in order to fill this placeholder.",
                    "label": 0
                },
                {
                    "sent": "So here the exhilarate query which is helpful for this query.",
                    "label": 0
                },
                {
                    "sent": "Here we get 1000 different values for this placeholder and each time.",
                    "label": 0
                },
                {
                    "sent": "We contact the triple store.",
                    "label": 0
                },
                {
                    "sent": "We use a different value, a different value at random from from this 1000.",
                    "label": 0
                },
                {
                    "sent": "So we decrease the probability of cashing in the Triple Store.",
                    "label": 0
                },
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will get back to our approach because we will describe the process of this generation, which is apparent process through the process of query.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Analysis.",
                    "label": 0
                },
                {
                    "sent": "The main requirements for queried for this generation is that it should resemble the original data.",
                    "label": 0
                },
                {
                    "sent": "It should allow the generation of datasets of different sizes.",
                    "label": 0
                },
                {
                    "sent": "It should be applicable also to different two different datasets, not only DP.",
                    "label": 0
                },
                {
                    "sent": "Pedia should be applicable to other datasets.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the main components of this generation process.",
                    "label": 0
                },
                {
                    "sent": "We have an input data set which is the pedia in our case on which we apply a data set generator.",
                    "label": 0
                },
                {
                    "sent": "In order to get data sets of different sizes.",
                    "label": 1
                },
                {
                    "sent": "Then we load this data into the triple stores under test.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have two types of datasets.",
                    "label": 0
                },
                {
                    "sent": "Smaller datasets which are the datasets that are smaller than the PPD itself.",
                    "label": 0
                },
                {
                    "sent": "And larger datasets that are datasets which is larger than the PD itself.",
                    "label": 0
                },
                {
                    "sent": "For generation of smaller datasets.",
                    "label": 0
                },
                {
                    "sent": "For the generation of a smaller datasets, we start by selecting all classes of the Pedia in order to cover all the classes of the pedia.",
                    "label": 0
                },
                {
                    "sent": "So we maintain the heterogeneity of the output data set.",
                    "label": 0
                },
                {
                    "sent": "Then we we start by selecting something called seed innocences.",
                    "label": 0
                },
                {
                    "sent": "See those seed innocences are used to follow the links.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in the in the PDF.",
                    "label": 0
                },
                {
                    "sent": "For example, if we start with this instance and follow its length, it may lead to another instance in another class.",
                    "label": 0
                },
                {
                    "sent": "We follow this link and this triple is considered an output triple that will be placed to the output data set.",
                    "label": 0
                },
                {
                    "sent": "Afterwards we continue following the links, which leads us to another innocence which may be in the same class or in another class.",
                    "label": 0
                },
                {
                    "sent": "This is also an output triple.",
                    "label": 0
                },
                {
                    "sent": "Then we continue following the links so we can.",
                    "label": 0
                },
                {
                    "sent": "We can be lead it to another instance in the same class.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Continue following the links till we have reached all the triples that that we need.",
                    "label": 0
                },
                {
                    "sent": "So in this step we stopped when we have reached the required number of triples.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For larger datasets, which is a data set larger than BPD itself, we only duplicate all triples and just introduce a slight change to the namespace, which is instead of using the pedia.org, we use dpdatul.org as here in the.",
                    "label": 0
                },
                {
                    "sent": "In the figure we have this class.",
                    "label": 0
                },
                {
                    "sent": "With this innocence we just create another copy of it.",
                    "label": 0
                },
                {
                    "sent": "With this change, instead of using the pedia, we use the PDF 2.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here regarding the system setup, we have applied our method on four different triple stores, namely virtues or Sesame, Big Island and Jenna TDB.",
                    "label": 0
                },
                {
                    "sent": "We have used the same machine both as a client and at the server in order to in order to avoid any network traffic.",
                    "label": 0
                },
                {
                    "sent": "We have also used the same setup for all for all triple stores in order to maintain the furnace between them.",
                    "label": 0
                },
                {
                    "sent": "Our our benchmarking strategy consists of the following.",
                    "label": 0
                },
                {
                    "sent": "First we have we apply a system restart in order to clear all caches.",
                    "label": 0
                },
                {
                    "sent": "After this we apply a warm up phase of 20 minutes.",
                    "label": 0
                },
                {
                    "sent": "After this we apply 60 minutes of hot run phase.",
                    "label": 0
                },
                {
                    "sent": "Then after this this phase we got the actual benchmark result.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we discuss.",
                    "label": 0
                },
                {
                    "sent": "The results for 10% data set 5th.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The person does it.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For 100, is it actually we've based our benchmark on four different data set size is 10% which is 10% of DPD triples 50%, which is half of the pedia triples and 100 which is DPD itself as fully stand and 200 DB pedia which is double sized pedia.",
                    "label": 0
                },
                {
                    "sent": "Here we can.",
                    "label": 0
                },
                {
                    "sent": "We can describe some of the some of the running times over the queries.",
                    "label": 0
                },
                {
                    "sent": "For example query 2.",
                    "label": 0
                },
                {
                    "sent": "Here we can see that Virtusa LAN Sesame runs almost as fast as each other.",
                    "label": 0
                },
                {
                    "sent": "But here when that data set size changes from 10% to 50%.",
                    "label": 0
                },
                {
                    "sent": "Here we can see see that Sesame wins the competition and it becomes faster.",
                    "label": 0
                },
                {
                    "sent": "Also for the transition from 100 to 200 week.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And see here that this this query for example here.",
                    "label": 0
                },
                {
                    "sent": "It was about 60, but here for 200 it's almost almost 40.",
                    "label": 0
                },
                {
                    "sent": "We can see here also, this query query 16 was about 140, but here when the data set is double sized it runs faster.",
                    "label": 0
                },
                {
                    "sent": "It's about 180.",
                    "label": 0
                },
                {
                    "sent": "We will also describe the slow and fast queries in the coming slides.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Four virtues of the slowest queries for Virtusa is query number 10, which contains filter, distinct and optional.",
                    "label": 0
                },
                {
                    "sent": "So we can conclude from this that.",
                    "label": 0
                },
                {
                    "sent": "This combination of features is really problematic for Virtuoso.",
                    "label": 0
                },
                {
                    "sent": "For system we have for three.",
                    "label": 0
                },
                {
                    "sent": "Various query, four query 15 and query 18 for query four.",
                    "label": 0
                },
                {
                    "sent": "It contains only union but with too many free variables.",
                    "label": 0
                },
                {
                    "sent": "For query 18 and query 15, both of them contain union filter and blank as features, but query 18 contains STR string function as an extra feature, so we can conclude that this combination of features, union filter and length is heavy for four sesame and if we add another feature which is STR, this this place is this place is more burden over sesame and makes the whole query even slower.",
                    "label": 0
                },
                {
                    "sent": "For Jenna TDB, there are two.",
                    "label": 0
                },
                {
                    "sent": "There are two slow queries.",
                    "label": 0
                },
                {
                    "sent": "Query 10 and query 20 queries and was already discussed with their chosen and pretend contains these features.",
                    "label": 0
                },
                {
                    "sent": "Filter optional union and Lang.",
                    "label": 0
                },
                {
                    "sent": "We can conclude also that this combination of features is heavy for Jenna TDB for general.",
                    "label": 0
                },
                {
                    "sent": "For a big album, there are two slow queries, Query 10, which is already discussed with virtue and Query 15, which was already discussed with Jesse.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regarding the first queries, we can conclude that all of them are fast with query number one which contains only distinct as a feature, where Jenna TDB is fast with query two which contains filter only.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "We will talk about the query mixes per hour, which gives a Broadview Broadview about the performance of the triple stores.",
                    "label": 0
                },
                {
                    "sent": "Here we can conclude that virtue is the fastest triple store of all of them.",
                    "label": 0
                },
                {
                    "sent": "Then be goulem, then come sesame, and finally comes.",
                    "label": 0
                },
                {
                    "sent": "Jenna TDB.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regarding the scalability, which means how the performance of a triple store changes when the data set size changes from, for example, 10% of the pedia to 50% and from 50% to 100% and so on.",
                    "label": 0
                },
                {
                    "sent": "We can say that we're choosing has the best scalability of them all and then big alaman system.",
                    "label": 0
                },
                {
                    "sent": "We are competing for the second best scalability.",
                    "label": 0
                },
                {
                    "sent": "For smaller datasets sesame is better or I mean for the transition from one 110% to 50%.",
                    "label": 0
                },
                {
                    "sent": "But for larger deficits when this is it's size changes from 50% to 100% and from 100% to 200% big wins the competition and it becomes better generally is only better when the.",
                    "label": 0
                },
                {
                    "sent": "Transition becomes from 100% DP pedia to 200% DP.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regarding the conclusion, we can say that we have built a new benchmark based on real data and real queries.",
                    "label": 0
                },
                {
                    "sent": "The main advantage of this benchmark is that it is not only applicable on the pedia, but we can apply it on other datasets like link to data.",
                    "label": 0
                },
                {
                    "sent": "For example, we have also introduced a new strategy of query variability, which means that we do not use the same query to contact the same triple store over and over any state we introduce a slight change to the same query in order to.",
                    "label": 0
                },
                {
                    "sent": "Decrease the probability of cash.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the future work, we have several direction to go further with our benchmark.",
                    "label": 0
                },
                {
                    "sent": "First, we can we should.",
                    "label": 0
                },
                {
                    "sent": "We should enhance our data set generation because the reviewers of our paper has suggested that we can use a paper called apples and oranges for apples and oranges with apples and oranges.",
                    "label": 0
                },
                {
                    "sent": "For RDR, for RDF benchmarking, which was published in segment 2011.",
                    "label": 0
                },
                {
                    "sent": "Which of the scribes a bitter at this age generation than ours?",
                    "label": 0
                },
                {
                    "sent": "We can adopt this this generation and the next version of our benchmark we can use also more features.",
                    "label": 0
                },
                {
                    "sent": "For example, the features that exist in Sparkle 1.1.",
                    "label": 0
                },
                {
                    "sent": "We can also evaluate the triplestores when the update process is applied.",
                    "label": 0
                },
                {
                    "sent": "For example, when we add more triples or we when we delete more triples.",
                    "label": 0
                },
                {
                    "sent": "We can also test our systems.",
                    "label": 0
                },
                {
                    "sent": "The triple stores.",
                    "label": 0
                },
                {
                    "sent": "I mean, when there is an is enabled be cause.",
                    "label": 0
                },
                {
                    "sent": "For example, the developers of Big Column have spent too much time in making their system better and better in this sense.",
                    "label": 0
                },
                {
                    "sent": "And we can also adopt more triple stores like 4 store or our graph.",
                    "label": 0
                },
                {
                    "sent": "We can, we must stay here that there is a kind of or it might be a kind of bias becausw our query log.",
                    "label": 0
                },
                {
                    "sent": "We are the query log we have based our benchmark on is based on the official end point of the pedia, which is hosted on virtual.",
                    "label": 0
                },
                {
                    "sent": "So there might be some queries that were discarded because of they are uncommon queries.",
                    "label": 0
                },
                {
                    "sent": "But this is because they are slow on virtual say so they were sent just once or twice because it takes too much time on their chosen.",
                    "label": 0
                },
                {
                    "sent": "So we should also handle this process.",
                    "label": 0
                },
                {
                    "sent": "We can also apply our benchmark using other resources like link due data.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}