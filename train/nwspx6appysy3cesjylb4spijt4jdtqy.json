{
    "id": "nwspx6appysy3cesjylb4spijt4jdtqy",
    "title": "Estimating Local Optimums in EM Algorithm over Gaussian Mixture Model",
    "info": {
        "author": [
            "Zhenjie Zhang, Department of Computer Science, National University of Singapore"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_zhang_elo/",
    "segmentation": [
        [
            "Yeah.",
            "OK, thank you for coming to my talk today.",
            "My name is my name is Ginger John.",
            "I'm from National University of Singapore, so this is joint work with my colleague opinion Diane my Supervisor Anthony term.",
            "So the topic of this talk is about how we can estimate the local optimum over EM algorithm an over the Gaussian mixture model.",
            "So by estimating by having a good if we can have a good estimation of the local local optimal's we can accelerate the conventional.",
            "I am algorithms by pruning those unpromising solutions.",
            "So this is the basic idea."
        ],
        [
            "Of our method.",
            "So this is a brief outline of my talk today.",
            "At the beginning I will give some introduction to the breakfast of the Gaussian mixture model and the EM algorithms.",
            "An while we need some estimation method for year mechanism.",
            "And in the second part will present some new interesting properties of the EM algorithm over Gaussian mixture model which we call local tracking properties and by using this property we can define some maximum region on the local optimal and we can verify the maximum region in a very efficient way and then I will present some experiment results are accelerating the EM algorithm and finally I will conclude this talk and give some future work."
        ],
        [
            "So first let me give some quick review on the basics of Gaussian mixture model so we know that in Gaussian mixture model we try to use capon components to model the underlying distribution.",
            "An each component is is represented by some parameters, so mu J is the center of the Gaussian mixture model and Sigma J is a covariance matrix.",
            "An paije is the probability of this component so.",
            "The probability of observing some point X from component J can be represented by the Gaussian distribution.",
            "We can summarize the probability of observing some point X from all clusters or all components by using this formula."
        ],
        [
            "So the problem is, how can we find the configuration of all the Gaussian mixture models to maximize the probability of the whole data set?",
            "So which can be represented like this?",
            "And usually we we can.",
            "We can transfer the original problem to a new problem which tries to maximize log likelihood instead of the original likelihood.",
            "So it will be something like this and in this formula the Theta is a configuration of other K Gaussian mixtures.",
            "An fine is a posterior probability of the points from one cluster.",
            "So here Toy Jay is in this file.",
            "So this means the probability of point I coming from the component J."
        ],
        [
            "So usually we use got expectation maximization measure to find the the configuration which can maximize the probability.",
            "So here is a brief introduction to the M method.",
            "So initialization we randomly selected initial configuration, we call it Theta zero.",
            "Of course, when is it a zero to follow some conditions like the covariance matrix must be must be positive definite.",
            "And something like this.",
            "And after the initialization we run easy step M steps in a round Robin manner and in each step we try to calculate the cluster membership probability to ijt based on the current configuration and in M step we update the parameters of all components.",
            "So we can generate a new configuration city plus one based on Sititi, anti JT and finally the algorithm algorithm converges when there's no change on the membership probability."
        ],
        [
            "So we know that EM algorithm can only find some local optimum instead of the global optimum.",
            "So in practice we usually run multiple EM algorithms with different initialization initial configurations and returns the best one which can maximize the probabilities.",
            "So.",
            "It will be very interesting if we can do some early termination over some of the of the procedures.",
            "If we know that the current procedure cannot lead to any good solutions, then the existing solutions.",
            "So in this way first we can, we can improve the efficiency of this multi run EM algorithm and second because we can because we spend less time on the procedures, we can run more more procedures in a Standard Time, so it's more likely to find some better.",
            "Configurations, then the existing one.",
            "So this is a motivation for some estimation over the local optimum we can achieve based on the current configuration."
        ],
        [
            "So here is a graph figure showing how we can.",
            "We can do this so on the X axis this is number of the iterations of the current procedure of EM algorithm and the blue line is a log likelihood after after some iterations and the blue one is a best solution we have seen so far based on the previous runs of the year and if we can derive some upper bound on the local likelihood of the local optimum here.",
            "After every iteration, and if a summer, after some iteration, we found that the look at the upper bound of the local optimum is already smaller than the current best solution.",
            "We can stop this procedure at this moment, and so we can save the time for the rest of the iterations, and we know that in current practice many algorithms stopped based on the difference between two iterations.",
            "However, this is not a good solution.",
            "Why, for example, here you can see that the difference between two iterations is very small.",
            "However, we cannot stop here because we're not sure if we can leave too so much better solution.",
            "So our upper bound based termination method is much better than the current solution for them for multi Rainier."
        ],
        [
            "So.",
            "Now we have seen the motivation for the.",
            "Estimation of the local optimum.",
            "Let's see some interesting properties we proved in."
        ],
        [
            "In your paper.",
            "So first let me define the solution space in solution space.",
            "We every point in this space is a configuration of the Gaussian mixture model.",
            "Then it is easy to do this.",
            "We just need to concatenate all the parameters of the of the Gaussians so we can generate a very high sequence.",
            "And this is.",
            "This sequence can be mapped into a high dimensional space.",
            "So every every EM iteration will move the configuration from one position to another.",
            "So for example here the original iteration is City and after 1:00 PM iteration it moves to City plus one.",
            "And here we use the grade level to indicate the log likelihood the configuration can achieve.",
            "So the darker the better.",
            "So we know that City plus one must be a better solution than sititi.",
            "So the color here must be darker than sititi."
        ],
        [
            "And the local trapping is some very interesting property.",
            "We can find a pass for every for every pair of configurations across uh, EM iteration.",
            "We can find a path in this solution space connecting Cita tion setup P + 1 and we can prove that on this path every configuration can lead to some better solution than Theta T and we prove this by by first finding a new configuration.",
            "See to shop between city and set up plus one and present that every every configuration between.",
            "GNC to shop is better than city and also every configuration between Cedar shop and 70 + 1 is better than City Shop.",
            "So you can check the paper for the detail of this proof and this property is really interesting."
        ],
        [
            "So based on this local local trapping property.",
            "We can derive some some maximum region on the local optimums.",
            "How can we do this?",
            "We need to find some some region, for example for.",
            "For this one we can find this counter if every configuration on this control is no better than sititi, we know that the further iterations cannot jump out of this region, so in this way we can somehow bound the local local optimal in this region and we can try to derive some upper bound on the local optimum achieved by.",
            "Further me iterations.",
            "So this is a basic idea of our."
        ],
        [
            "Estimation.",
            "So now let me."
        ],
        [
            "Show some more definition of the maximum region, so a maximum region is some region in the solution space which can cover both the current configuration and the local optimum achieved based on if we follow the EM iterations after sititi.",
            "So based on the local tracking properties we can prove that a region is a maximum region if every configuration on the boundary of this region is no better than city.",
            "So the local optimum must be in this maximum region.",
            "By this property.",
            "So now we face 2 problems.",
            "First it is how can we?",
            "How can we define some class of maximal regions which can be easier to manipulate?",
            "And the second question is how we can derive some upper bound based on this class of maximum."
        ],
        [
            "Which is.",
            "To do this, we define a special class of maximal operations.",
            "Because this RCT and Delta, so Delta, is a is a real positive real value and city is a current configuration of the EM algorithm.",
            "So.",
            "This in this in this maximum region.",
            "It contains all configuration satisfying these three properties, so the first property is so for the component probability it is, so it is between one minus, Delta, \u03c0, JT and one plus Delta \u03c0 GT an for the covariance matrix.",
            "So it follows that the.",
            "If we multiply the current curve correction matrix over the.",
            "Oh sorry, so for this.",
            "For this to configuration in this in this region, the covariance matrix D inverse of biometrics multiplied by the by the current equivalent matrix minus identity matrix and trace over this matrix is between minus Delta and positive Delta an for the for the center of the covariance matrix, the distance in a rotated space is smaller than Delta.",
            "So."
        ],
        [
            "Anne.",
            "Of something interesting that we prove that we can we can verify if this origenism maximum region in bigger time and here and is a number of the point of the data set, so it can be done very efficiently.",
            "And another thing is that we can estimate we can derive some upper bound on the log likelihood of any configurations in this region in constant time.",
            "So in this way we can.",
            "We can do some very good estimation over the local optimal and we can prove the proof the current process."
        ],
        [
            "Are very efficiently."
        ],
        [
            "So here is some brief experimental results.",
            "So first we run some experimental results over synthetic data and so for this synthetic data we generated by randomly generate some some Gaussian Gaussian model Gaussian distributions in this space, and the number of the Gaussian mixture models is larger than the number of components is larger than K, and we use the small OK to to try to run the original EM algorithm and the new accelerated EM algorithms.",
            "We can see that the number of the other iterations is is much less in this new ear mechanisms, and so this leads to some much more efficient implementation of the EM algorithm."
        ],
        [
            "An forward for the for this real data set we present some we use some some data set here I just give someone one result over the spam data.",
            "You can download this from from UCI data set an also oh oh sorry in this data set the dimensionality is 58 and the number of point is about 5000.",
            "So.",
            "Also, the number of iterations is much much smaller in this new era algorithm than the previous old implementation, so we can.",
            "We can do it much more efficient here."
        ],
        [
            "So sorry, it's a little too quick, so something very interesting to do in the future is that we can use this to measure the stability.",
            "Of the year of the Gaussian mixture model.",
            "So this means we can move the move the point around and check if the if the new local optimum of the of the Gaussian mixture model is very far away from the from the previous one.",
            "So if the if the distortion is very small, we can say that this Gaussian mixture model is very stable, otherwise this result is not stable.",
            "So maybe this is not a good result.",
            "And another one is that we want to.",
            "We can do this on to detect the distribution change so we can keep insert objects into into this data set and keep remove it.",
            "Also keep removing data data points out of this data set an we can use.",
            "You can use this model to do some efficient detection on the change of the Gaussian mixture model.",
            "So if the Gaussian mixture mixture has a very very big change, we say that the distribution has has changed, otherwise the distribution is stable."
        ],
        [
            "So this is some conclusion of our paper, so we extend extend our understanding of the EM algorithm over the Gaussian mixture model.",
            "Because we know that the Gaussian mixture model is is very hard to analyze.",
            "And we showed that this this some some interesting properties in your mouth all over Gaussian model can be used to accelerate the multiple run EM algorithm and also the service many potentials for this.",
            "For this method inventory applications."
        ],
        [
            "OK, so thank you.",
            "The schedule is 01 overall end verification procedure works, so the basic idea is.",
            "Let me."
        ],
        [
            "So the basic idea is that for every first we derive the derived verification in two steps.",
            "In the first step, we assume that the posterior probability tyj is fixed, so all every cluster membership probability are fixed.",
            "So based on this we can derive some some we can derive some upper bound on the increase of the log.",
            "Likely sorry, the decrease of the log log likelihood function and then we try to change the the posterior probability to AJ and we try to find the optimal posterior probability of Taiji which can maximize the increase of the log likelihood if the if the increase in the second step is definitely smaller than the decrease in the first step, we know that.",
            "No, no configuration on the boundary can have a better.",
            "Can have a better result than the previous one.",
            "Yes.",
            "These results are analysis.",
            "So in our previous paper on a CDN we prove that this property also works in K means model and we are now trying to derive.",
            "So to extend this analysis to Bregman divergences and I believe Bregman divergences very likely to hold this property also.",
            "So this can also be some interesting work to do in the future.",
            "Exceptions to the phasing.",
            "Sorry.",
            "Fully based.",
            "Variation.",
            "Sorry I don't understand.",
            "Sorry.",
            "OK, OK I was wondering because some extent it's a general problem we have with greedy optimization, right?",
            "The very natural strategy is multiple restarts, yes?",
            "To compare this with my mean, I agree that this is much, much better in the sense that you know whether you can cut this branch safely.",
            "Yes, yes.",
            "It might be the case that document you had for showing we should wait, which is the plateau in the end.",
            "Have to be."
        ],
        [
            "Yes, this one, I mean, did you observe many of such practices in the experiments?",
            "Yes.",
            "So that's why we can we can.",
            "We can save a lot of of the time in the experiment because so so usually in the in the last few in the last few iterations they improve the offline likelihood is very small and it can continue for for many iterations.",
            "So in many cases we can.",
            "We can cut cut it in half.",
            "I was wondering, you see if you are if you are running several with several yes yes.",
            "And you may at the same time you mean at the same time, yeah?",
            "So you have all the sheets and then you increase them and some of them look well.",
            "Not yeah, yes.",
            "And you iterate and possibly you add new one.",
            "You see?",
            "Yeah, yes, this is the say, the economique way of doing same of improving the overall results of Parfait.",
            "Optimization.",
            "To compare this, no nothing.",
            "So this means that the current best solution is also varying, right?",
            "Because everyone is trying to iterate to a better solution so we can observe a better and better optimal solution for from every runs.",
            "Yes.",
            "Best one competing yes, yes this can be can be something interesting.",
            "And also we believe that it can be we can.",
            "Maybe we can derive some analysis to upper bound.",
            "The number of iterations based on this.",
            "This interesting property.",
            "In also, I was wondering, perhaps you could reduce.",
            "The partial results you.",
            "I mean, basically the fact that, for instance, some examples are together in the same cluster.",
            "OK, this might be something you might say transmit a bit of memory between between several say different.",
            "OK, so it's not that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you for coming to my talk today.",
                    "label": 0
                },
                {
                    "sent": "My name is my name is Ginger John.",
                    "label": 0
                },
                {
                    "sent": "I'm from National University of Singapore, so this is joint work with my colleague opinion Diane my Supervisor Anthony term.",
                    "label": 0
                },
                {
                    "sent": "So the topic of this talk is about how we can estimate the local optimum over EM algorithm an over the Gaussian mixture model.",
                    "label": 1
                },
                {
                    "sent": "So by estimating by having a good if we can have a good estimation of the local local optimal's we can accelerate the conventional.",
                    "label": 0
                },
                {
                    "sent": "I am algorithms by pruning those unpromising solutions.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic idea.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of our method.",
                    "label": 0
                },
                {
                    "sent": "So this is a brief outline of my talk today.",
                    "label": 0
                },
                {
                    "sent": "At the beginning I will give some introduction to the breakfast of the Gaussian mixture model and the EM algorithms.",
                    "label": 0
                },
                {
                    "sent": "An while we need some estimation method for year mechanism.",
                    "label": 0
                },
                {
                    "sent": "And in the second part will present some new interesting properties of the EM algorithm over Gaussian mixture model which we call local tracking properties and by using this property we can define some maximum region on the local optimal and we can verify the maximum region in a very efficient way and then I will present some experiment results are accelerating the EM algorithm and finally I will conclude this talk and give some future work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first let me give some quick review on the basics of Gaussian mixture model so we know that in Gaussian mixture model we try to use capon components to model the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "An each component is is represented by some parameters, so mu J is the center of the Gaussian mixture model and Sigma J is a covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "An paije is the probability of this component so.",
                    "label": 0
                },
                {
                    "sent": "The probability of observing some point X from component J can be represented by the Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "We can summarize the probability of observing some point X from all clusters or all components by using this formula.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem is, how can we find the configuration of all the Gaussian mixture models to maximize the probability of the whole data set?",
                    "label": 1
                },
                {
                    "sent": "So which can be represented like this?",
                    "label": 0
                },
                {
                    "sent": "And usually we we can.",
                    "label": 0
                },
                {
                    "sent": "We can transfer the original problem to a new problem which tries to maximize log likelihood instead of the original likelihood.",
                    "label": 0
                },
                {
                    "sent": "So it will be something like this and in this formula the Theta is a configuration of other K Gaussian mixtures.",
                    "label": 0
                },
                {
                    "sent": "An fine is a posterior probability of the points from one cluster.",
                    "label": 0
                },
                {
                    "sent": "So here Toy Jay is in this file.",
                    "label": 0
                },
                {
                    "sent": "So this means the probability of point I coming from the component J.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So usually we use got expectation maximization measure to find the the configuration which can maximize the probability.",
                    "label": 0
                },
                {
                    "sent": "So here is a brief introduction to the M method.",
                    "label": 0
                },
                {
                    "sent": "So initialization we randomly selected initial configuration, we call it Theta zero.",
                    "label": 0
                },
                {
                    "sent": "Of course, when is it a zero to follow some conditions like the covariance matrix must be must be positive definite.",
                    "label": 0
                },
                {
                    "sent": "And something like this.",
                    "label": 0
                },
                {
                    "sent": "And after the initialization we run easy step M steps in a round Robin manner and in each step we try to calculate the cluster membership probability to ijt based on the current configuration and in M step we update the parameters of all components.",
                    "label": 1
                },
                {
                    "sent": "So we can generate a new configuration city plus one based on Sititi, anti JT and finally the algorithm algorithm converges when there's no change on the membership probability.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we know that EM algorithm can only find some local optimum instead of the global optimum.",
                    "label": 0
                },
                {
                    "sent": "So in practice we usually run multiple EM algorithms with different initialization initial configurations and returns the best one which can maximize the probabilities.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It will be very interesting if we can do some early termination over some of the of the procedures.",
                    "label": 0
                },
                {
                    "sent": "If we know that the current procedure cannot lead to any good solutions, then the existing solutions.",
                    "label": 0
                },
                {
                    "sent": "So in this way first we can, we can improve the efficiency of this multi run EM algorithm and second because we can because we spend less time on the procedures, we can run more more procedures in a Standard Time, so it's more likely to find some better.",
                    "label": 0
                },
                {
                    "sent": "Configurations, then the existing one.",
                    "label": 0
                },
                {
                    "sent": "So this is a motivation for some estimation over the local optimum we can achieve based on the current configuration.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a graph figure showing how we can.",
                    "label": 0
                },
                {
                    "sent": "We can do this so on the X axis this is number of the iterations of the current procedure of EM algorithm and the blue line is a log likelihood after after some iterations and the blue one is a best solution we have seen so far based on the previous runs of the year and if we can derive some upper bound on the local likelihood of the local optimum here.",
                    "label": 0
                },
                {
                    "sent": "After every iteration, and if a summer, after some iteration, we found that the look at the upper bound of the local optimum is already smaller than the current best solution.",
                    "label": 0
                },
                {
                    "sent": "We can stop this procedure at this moment, and so we can save the time for the rest of the iterations, and we know that in current practice many algorithms stopped based on the difference between two iterations.",
                    "label": 0
                },
                {
                    "sent": "However, this is not a good solution.",
                    "label": 0
                },
                {
                    "sent": "Why, for example, here you can see that the difference between two iterations is very small.",
                    "label": 0
                },
                {
                    "sent": "However, we cannot stop here because we're not sure if we can leave too so much better solution.",
                    "label": 0
                },
                {
                    "sent": "So our upper bound based termination method is much better than the current solution for them for multi Rainier.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now we have seen the motivation for the.",
                    "label": 0
                },
                {
                    "sent": "Estimation of the local optimum.",
                    "label": 0
                },
                {
                    "sent": "Let's see some interesting properties we proved in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In your paper.",
                    "label": 0
                },
                {
                    "sent": "So first let me define the solution space in solution space.",
                    "label": 1
                },
                {
                    "sent": "We every point in this space is a configuration of the Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "Then it is easy to do this.",
                    "label": 0
                },
                {
                    "sent": "We just need to concatenate all the parameters of the of the Gaussians so we can generate a very high sequence.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "This sequence can be mapped into a high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "So every every EM iteration will move the configuration from one position to another.",
                    "label": 0
                },
                {
                    "sent": "So for example here the original iteration is City and after 1:00 PM iteration it moves to City plus one.",
                    "label": 0
                },
                {
                    "sent": "And here we use the grade level to indicate the log likelihood the configuration can achieve.",
                    "label": 0
                },
                {
                    "sent": "So the darker the better.",
                    "label": 0
                },
                {
                    "sent": "So we know that City plus one must be a better solution than sititi.",
                    "label": 0
                },
                {
                    "sent": "So the color here must be darker than sititi.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the local trapping is some very interesting property.",
                    "label": 1
                },
                {
                    "sent": "We can find a pass for every for every pair of configurations across uh, EM iteration.",
                    "label": 0
                },
                {
                    "sent": "We can find a path in this solution space connecting Cita tion setup P + 1 and we can prove that on this path every configuration can lead to some better solution than Theta T and we prove this by by first finding a new configuration.",
                    "label": 1
                },
                {
                    "sent": "See to shop between city and set up plus one and present that every every configuration between.",
                    "label": 0
                },
                {
                    "sent": "GNC to shop is better than city and also every configuration between Cedar shop and 70 + 1 is better than City Shop.",
                    "label": 0
                },
                {
                    "sent": "So you can check the paper for the detail of this proof and this property is really interesting.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So based on this local local trapping property.",
                    "label": 1
                },
                {
                    "sent": "We can derive some some maximum region on the local optimums.",
                    "label": 0
                },
                {
                    "sent": "How can we do this?",
                    "label": 0
                },
                {
                    "sent": "We need to find some some region, for example for.",
                    "label": 1
                },
                {
                    "sent": "For this one we can find this counter if every configuration on this control is no better than sititi, we know that the further iterations cannot jump out of this region, so in this way we can somehow bound the local local optimal in this region and we can try to derive some upper bound on the local optimum achieved by.",
                    "label": 0
                },
                {
                    "sent": "Further me iterations.",
                    "label": 0
                },
                {
                    "sent": "So this is a basic idea of our.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimation.",
                    "label": 0
                },
                {
                    "sent": "So now let me.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show some more definition of the maximum region, so a maximum region is some region in the solution space which can cover both the current configuration and the local optimum achieved based on if we follow the EM iterations after sititi.",
                    "label": 0
                },
                {
                    "sent": "So based on the local tracking properties we can prove that a region is a maximum region if every configuration on the boundary of this region is no better than city.",
                    "label": 1
                },
                {
                    "sent": "So the local optimum must be in this maximum region.",
                    "label": 0
                },
                {
                    "sent": "By this property.",
                    "label": 0
                },
                {
                    "sent": "So now we face 2 problems.",
                    "label": 0
                },
                {
                    "sent": "First it is how can we?",
                    "label": 0
                },
                {
                    "sent": "How can we define some class of maximal regions which can be easier to manipulate?",
                    "label": 0
                },
                {
                    "sent": "And the second question is how we can derive some upper bound based on this class of maximum.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "To do this, we define a special class of maximal operations.",
                    "label": 1
                },
                {
                    "sent": "Because this RCT and Delta, so Delta, is a is a real positive real value and city is a current configuration of the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This in this in this maximum region.",
                    "label": 0
                },
                {
                    "sent": "It contains all configuration satisfying these three properties, so the first property is so for the component probability it is, so it is between one minus, Delta, \u03c0, JT and one plus Delta \u03c0 GT an for the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So it follows that the.",
                    "label": 0
                },
                {
                    "sent": "If we multiply the current curve correction matrix over the.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, so for this.",
                    "label": 0
                },
                {
                    "sent": "For this to configuration in this in this region, the covariance matrix D inverse of biometrics multiplied by the by the current equivalent matrix minus identity matrix and trace over this matrix is between minus Delta and positive Delta an for the for the center of the covariance matrix, the distance in a rotated space is smaller than Delta.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Of something interesting that we prove that we can we can verify if this origenism maximum region in bigger time and here and is a number of the point of the data set, so it can be done very efficiently.",
                    "label": 0
                },
                {
                    "sent": "And another thing is that we can estimate we can derive some upper bound on the log likelihood of any configurations in this region in constant time.",
                    "label": 0
                },
                {
                    "sent": "So in this way we can.",
                    "label": 0
                },
                {
                    "sent": "We can do some very good estimation over the local optimal and we can prove the proof the current process.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are very efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is some brief experimental results.",
                    "label": 1
                },
                {
                    "sent": "So first we run some experimental results over synthetic data and so for this synthetic data we generated by randomly generate some some Gaussian Gaussian model Gaussian distributions in this space, and the number of the Gaussian mixture models is larger than the number of components is larger than K, and we use the small OK to to try to run the original EM algorithm and the new accelerated EM algorithms.",
                    "label": 0
                },
                {
                    "sent": "We can see that the number of the other iterations is is much less in this new ear mechanisms, and so this leads to some much more efficient implementation of the EM algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An forward for the for this real data set we present some we use some some data set here I just give someone one result over the spam data.",
                    "label": 0
                },
                {
                    "sent": "You can download this from from UCI data set an also oh oh sorry in this data set the dimensionality is 58 and the number of point is about 5000.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Also, the number of iterations is much much smaller in this new era algorithm than the previous old implementation, so we can.",
                    "label": 0
                },
                {
                    "sent": "We can do it much more efficient here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So sorry, it's a little too quick, so something very interesting to do in the future is that we can use this to measure the stability.",
                    "label": 0
                },
                {
                    "sent": "Of the year of the Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "So this means we can move the move the point around and check if the if the new local optimum of the of the Gaussian mixture model is very far away from the from the previous one.",
                    "label": 0
                },
                {
                    "sent": "So if the if the distortion is very small, we can say that this Gaussian mixture model is very stable, otherwise this result is not stable.",
                    "label": 0
                },
                {
                    "sent": "So maybe this is not a good result.",
                    "label": 0
                },
                {
                    "sent": "And another one is that we want to.",
                    "label": 0
                },
                {
                    "sent": "We can do this on to detect the distribution change so we can keep insert objects into into this data set and keep remove it.",
                    "label": 0
                },
                {
                    "sent": "Also keep removing data data points out of this data set an we can use.",
                    "label": 1
                },
                {
                    "sent": "You can use this model to do some efficient detection on the change of the Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "So if the Gaussian mixture mixture has a very very big change, we say that the distribution has has changed, otherwise the distribution is stable.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is some conclusion of our paper, so we extend extend our understanding of the EM algorithm over the Gaussian mixture model.",
                    "label": 0
                },
                {
                    "sent": "Because we know that the Gaussian mixture model is is very hard to analyze.",
                    "label": 1
                },
                {
                    "sent": "And we showed that this this some some interesting properties in your mouth all over Gaussian model can be used to accelerate the multiple run EM algorithm and also the service many potentials for this.",
                    "label": 0
                },
                {
                    "sent": "For this method inventory applications.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so thank you.",
                    "label": 0
                },
                {
                    "sent": "The schedule is 01 overall end verification procedure works, so the basic idea is.",
                    "label": 0
                },
                {
                    "sent": "Let me.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the basic idea is that for every first we derive the derived verification in two steps.",
                    "label": 0
                },
                {
                    "sent": "In the first step, we assume that the posterior probability tyj is fixed, so all every cluster membership probability are fixed.",
                    "label": 0
                },
                {
                    "sent": "So based on this we can derive some some we can derive some upper bound on the increase of the log.",
                    "label": 0
                },
                {
                    "sent": "Likely sorry, the decrease of the log log likelihood function and then we try to change the the posterior probability to AJ and we try to find the optimal posterior probability of Taiji which can maximize the increase of the log likelihood if the if the increase in the second step is definitely smaller than the decrease in the first step, we know that.",
                    "label": 0
                },
                {
                    "sent": "No, no configuration on the boundary can have a better.",
                    "label": 0
                },
                {
                    "sent": "Can have a better result than the previous one.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "These results are analysis.",
                    "label": 0
                },
                {
                    "sent": "So in our previous paper on a CDN we prove that this property also works in K means model and we are now trying to derive.",
                    "label": 0
                },
                {
                    "sent": "So to extend this analysis to Bregman divergences and I believe Bregman divergences very likely to hold this property also.",
                    "label": 0
                },
                {
                    "sent": "So this can also be some interesting work to do in the future.",
                    "label": 0
                },
                {
                    "sent": "Exceptions to the phasing.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Fully based.",
                    "label": 0
                },
                {
                    "sent": "Variation.",
                    "label": 0
                },
                {
                    "sent": "Sorry I don't understand.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, OK I was wondering because some extent it's a general problem we have with greedy optimization, right?",
                    "label": 0
                },
                {
                    "sent": "The very natural strategy is multiple restarts, yes?",
                    "label": 0
                },
                {
                    "sent": "To compare this with my mean, I agree that this is much, much better in the sense that you know whether you can cut this branch safely.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "It might be the case that document you had for showing we should wait, which is the plateau in the end.",
                    "label": 0
                },
                {
                    "sent": "Have to be.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, this one, I mean, did you observe many of such practices in the experiments?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So that's why we can we can.",
                    "label": 0
                },
                {
                    "sent": "We can save a lot of of the time in the experiment because so so usually in the in the last few in the last few iterations they improve the offline likelihood is very small and it can continue for for many iterations.",
                    "label": 0
                },
                {
                    "sent": "So in many cases we can.",
                    "label": 0
                },
                {
                    "sent": "We can cut cut it in half.",
                    "label": 0
                },
                {
                    "sent": "I was wondering, you see if you are if you are running several with several yes yes.",
                    "label": 0
                },
                {
                    "sent": "And you may at the same time you mean at the same time, yeah?",
                    "label": 0
                },
                {
                    "sent": "So you have all the sheets and then you increase them and some of them look well.",
                    "label": 0
                },
                {
                    "sent": "Not yeah, yes.",
                    "label": 0
                },
                {
                    "sent": "And you iterate and possibly you add new one.",
                    "label": 0
                },
                {
                    "sent": "You see?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yes, this is the say, the economique way of doing same of improving the overall results of Parfait.",
                    "label": 0
                },
                {
                    "sent": "Optimization.",
                    "label": 0
                },
                {
                    "sent": "To compare this, no nothing.",
                    "label": 0
                },
                {
                    "sent": "So this means that the current best solution is also varying, right?",
                    "label": 0
                },
                {
                    "sent": "Because everyone is trying to iterate to a better solution so we can observe a better and better optimal solution for from every runs.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Best one competing yes, yes this can be can be something interesting.",
                    "label": 0
                },
                {
                    "sent": "And also we believe that it can be we can.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can derive some analysis to upper bound.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations based on this.",
                    "label": 0
                },
                {
                    "sent": "This interesting property.",
                    "label": 0
                },
                {
                    "sent": "In also, I was wondering, perhaps you could reduce.",
                    "label": 0
                },
                {
                    "sent": "The partial results you.",
                    "label": 0
                },
                {
                    "sent": "I mean, basically the fact that, for instance, some examples are together in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "OK, this might be something you might say transmit a bit of memory between between several say different.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not that.",
                    "label": 0
                }
            ]
        }
    }
}