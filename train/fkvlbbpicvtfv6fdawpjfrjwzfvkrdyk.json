{
    "id": "fkvlbbpicvtfv6fdawpjfrjwzfvkrdyk",
    "title": "Bootstrapping Information Extraction from Semi-structured Web Pages",
    "info": {
        "author": [
            "Andrew Carlson, Machine Learning Department, School of Computer Science, Carnegie Mellon University",
            "Charles Schafer, Google, Inc."
        ],
        "published": "Oct. 10, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Information Extraction"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd08_carlson_bief/",
    "segmentation": [
        [
            "Email and this is joint work with Charles Shafer at Google in Pittsburgh and the topic of the talk is bootstrapping information extraction from sending structured web pages.",
            "Now many many."
        ],
        [
            "Sites on the Internet are template driven, meaning that behind them there's some structured information, typically from a database which has many attributes which are used to populate a template so that there's a spot in the template, say in this example, for the number of bedrooms and a description of this property and the address of it and an image and so on.",
            "So there are many different domains of such semi structured web pages as we'll call them in this work.",
            "Here we have an example of a vacation rentals page."
        ],
        [
            "We also have other examples like encyclopedia acknowledge like Nobel Prize winners where we have the Gender country affiliation when they won the prize, what they wanted in and so on listed."
        ],
        [
            "And we also have institutions such as the Smithsonian Smithsonian Institution, which makes the details of the items in its museum collections available online and so many others.",
            "Such museums might make their information online too."
        ],
        [
            "So each domain of semi structured websites.",
            "Then we'll have some schema of structured data which most of its sites will come close to conforming to.",
            "There may be some minor variation, but mostly they should conform to some particular schema.",
            "So as an example of structured data so that we're on the same page, here's a table of structured records from a property rentals domain.",
            "So here we have.",
            "Each row is a record of structured data and each column is an attribute such as the type of the property in the left column of the address of the property.",
            "The number of bedrooms, whether it's occupied or vacant, and so on."
        ],
        [
            "And so as the core motivation for this work, why do we want structured data?",
            "Well structured data is very, very useful.",
            "It unlocks many, many different applications of this data as one of many examples, it would enable faceted search, or it does enable actually.",
            "Right now faceted search such as this example, where a user can query for diamond bracelet Etsy items with images.",
            "That matched the query, but also drill down in the search by restricting the different attributes of the items they're looking for, such as price or what type of gold it is, and so on."
        ],
        [
            "So the classic paradigm for extracting these records from semi structured websites.",
            "Is supervised information extraction.",
            "So in this paradigm a user will annotate a number of pages from a target website.",
            "And then a model is learned which generalizes from these handful of training pages, which have been annotated to the rest of the pages in that website so that we can go on the left from a number of Pages 2 on the writes, a bunch of structured records that are extracted from the website."
        ],
        [
            "So while we were working on a project too.",
            "Obtain wrappers for a number of sites in a supervised way.",
            "We wondered if it was possible rather than, say, labeling a few pages on a website and then building a model to generalize to other pages on that website.",
            "Can we take a higher level approach and bootstrapping type approach where we are able to generalize?",
            "From the records that we extract from those few sites that we have labeled data for to the rest of the sites in that domain.",
            "So generalize at the website level as opposed to generalize across web sites as opposed to within websites.",
            "So the the problem that we consider here is bootstrapping information from structured web pages, where we assume that we have wrappers for a number of sites in a domain.",
            "And thus we can run those wrappers and extract many structured records from those websites.",
            "So can we use what we've learned from these records?",
            "To automatically wrap a new site in the same domain.",
            "So here if we have three vacation rental sites, can we automatically wrap with no supervision required?",
            "New vacation rental websites?"
        ],
        [
            "So drilling down a bit a core.",
            "Technique.",
            "The core techniques for both the supervised approach in her bootstrapping approach rely on the notion of Dom trees, so you can go from essentially Adama, Traore document object Model tree is just a simple tree based representation of the HTML contents of a web page, so this is a very simple mapping to perform."
        ],
        [
            "So if we obtain the Dom trees for.",
            "The collection of web pages that make up a website we can perform what's called tree alignment to produce a template tree which goes from here we have the two individual Dom trees for two different web pages on a website.",
            "We align those and we produce the template tree on the bottom and so the templates tree.",
            "You can think of each node in the template tree is sort of what you would intuitively if.",
            "If you have the two web pages side by side, sort of point at.",
            "These two spots are the same, the same spot within these two pages, so it's you align the content you try to match them as much as possible.",
            "If you're familiar with string alignments algorithms from bioinformatics where you're trying to align gene sequences, the process is actually quite similar to that."
        ],
        [
            "So then in the supervised setting what we do is we have a number of annotated pages from a website.",
            "We take those pages, turn them into Dom trees, and build a template for those pages, and then in order to produce an model which we can then use to label the rest of the pages on that one website.",
            "We look at the user annotations that were provided and we see which nodes in the template tree they are present on, and so we're able to then label this extraction template so that in order to label a new site, a new page.",
            "Within this site we take the page, we align it to the template and so we get a mapping of the correspondence between the nodes in this web page to the nodes in the template.",
            "And then we can just read off the labels."
        ],
        [
            "Now, in the bootstrapping setting, we don't have annotated data for this website.",
            "But what we do have are examples of each schema column from the training websites that we already had, and so rather than label the data fields.",
            "So sorry in the upper left corner.",
            "Take we want to bootstrap onto a new target site.",
            "So we take the pages for that target site and create a template in an unsupervised manner.",
            "We don't need labels in order to do that, and then the goal is to label the data fields in this template so the leaf nodes that contain text in this template.",
            "So we can extract the values across all of the pages on the target site.",
            "We can look at for each page we can look at the region that aligns to each node in the template and then for that node in the template we can get the distribution of values.",
            "So in this case the lower left node in the generalized template might contain this list which contains cities.",
            "This other text node might contain a very consistent label that indicates bedrooms and then the lower right node has this collection of integer values that we've observed, and so we can have a classifier for each schema column, such as city and number of bedrooms which classifieds each of these textual nodes in the template.",
            "As to how likely it is to contain that schema columns information.",
            "And so, after applying the classifiers, we can end up with the generalized template which we can use to extract records from this site where we've labeled the orange node indicating cities in the red reddish node indicating number of bedrooms."
        ],
        [
            "So in a bit more detail, the way these classifiers are trained.",
            "Is it through this classification problem where if we're training the classifier that decides if the data field contains city data or not?",
            "We have three training sites side A side B inside.",
            "See, we have the observed values in the data fields which were labeled with this city attribute from those sites.",
            "And then we also have so those are positive examples for a classifier for the city, schema column and then we have the rest of the data field values.",
            "From the other data fields on those training sites which are negative examples for the city class, and so the goal is to train for city, number of bedrooms and so on.",
            "The rest of the schema columns to train a classifier which can label this candidate data field on some new target site that's on the left of the screen.",
            "As for the.",
            "To classify whether or not it contains the data values for that particular schema column."
        ],
        [
            "So the types of features that we use of the values in a data field are based both on the content of the field as well as the context in which it appears in.",
            "So we use features that are just the counts of the different tokens that appear in the field.",
            "We also use what are called character 3 grams, which are just sub sequences.",
            "All sub sequences of three characters, so this helps us capture things like abbreviations which are very very common on websites.",
            "Um?",
            "And so the contextual features are also these character 3 gram sub sequences because you might have one site that says bedrooms and another site that says be DRMS.",
            "So you want some overlap there."
        ],
        [
            "So I'm naive way of solving this classification problem is just simply we have a bunch of features we have labels.",
            "Let's train a logistic regression model.",
            "So for each schema column we train a logistic regression classifier where each data field from the training sites that we have labeled data for.",
            "Is a labeled instance either positive or negative.",
            "For each schema column and we just we throw all the features that we just described in the examples.",
            "But the problem here is that each site will typically have on the order of, say, two dozen data fields detected in the template.",
            "And so we have a only on the order of 10s of training instances.",
            "But we have 10s of thousands of features, and so we we do observe in practice serious overfitting.",
            "As you would expect, 'cause even with regularizer aggression.",
            "This is a a poorly scaled problem for data versus features."
        ],
        [
            "So in order to obtain coarser features so that we can have a more appropriately scaled learning problem, we turn to distributional similarity.",
            "So rather than learn things rather than learn classifiers that have features like.",
            "The number of times RMS occurs in this data field is 7 and lots of very very fine grained features like that.",
            "We have distributional similarity features where each field is treated as a distribution of values and we extract features based on the distribution of similarity of.",
            "Of the.",
            "For each feature type, we have a distributional similarity feature, which says how similar the distribution of those features is to be labeled data, and so we use KL divergent and we just we apply a simple transformation to smoothen normalize it, which we call skew similarity."
        ],
        [
            "So in our smarter classification attempt, we called the sechskies model.",
            "So again, each field from the training sites is a labeled instance, but the features are the distribution similarity for each feature type.",
            "So for example the.",
            "The classifier that decides if something holds the value for the city schema column would have as its features how similar the candidate data fields, say contextual character 3 ground features are to the observed features in the city training data.",
            "So this approach was sorry, so from those four features, since we have 4 feature types.",
            "We train a linear regression model and this was inspired by working database schema matching by modifying it all in 2005.",
            "So now we have 10s of training instances.",
            "We have only one feature for feature type, so just a handful.",
            "We now have more training examples.",
            "Then we have features to learn, so this is an appropriately prize sized learning."
        ],
        [
            "Album so previous related work.",
            "There's been a lot of work on sort of learning the template so the Dom tree template.",
            "Method that I described is sort of one of several.",
            "Previous approaches which discovered the regularity's and can tell you this is data field one.",
            "This is data field to this.",
            "Is it a field 3?",
            "But most of that work doesn't attempt to label those data fields, so it can tell you this is actually data field that you're interested in an oh by the way, it contains the value for the city schema column, the work that has taken the extra step and labeled the data fields which with which schema columns that contain.",
            "Has mostly used brittle heuristic approaches which look for things like.",
            "This looks like it contains a price, so it's the price.",
            "But in practice websites or web pages will have lots of prices on them, and so if you had multiple matches of these heuristics, which actually happened quite a lot.",
            "The algorithm wouldn't be able to decide which was the correct data field."
        ],
        [
            "Attract.",
            "So in our evaluation we considered two domains.",
            "The first domain is vacation rentals, so it has a schema with seven different attributes in it.",
            "There were five vacation rental sites and we evaluated the system in a cross validated fashion by we'd hold out one vacation rental site so we wouldn't have any trading data for that site.",
            "We would then train the model on the four, help the four remaining vacation rental site and see how accurate we were on the held outside."
        ],
        [
            "We also used job listings as a second domain for evaluating the system.",
            "They said six different attributes that we were trying to distract in seven different websites."
        ],
        [
            "So our results are presented in accuracy by schema column.",
            "So here accuracy means.",
            "So for the vacation rental domain, for example, for each website which we had as a test example.",
            "We had the task of deciding which data field on this website contains the address of the property.",
            "Or we could say we don't think the address field appears on this website, so the accuracy here measures the accuracy of those decisions.",
            "So either picking the data field or saying it doesn't exist on this site.",
            "The left plot shows results for the vacation rental sites and the right plot shows the results for the job sites and the yellow bars showed the results for the logistic regression classifier and the blue results showed how the stacks queues model are hopefully smarter classification attempt did.",
            "So we see that we do significantly outperform logistic regression baseline, particularly on schema columns like the description of a vacation rental property or the unique ID of a job which.",
            "These are exactly the types of fields which have lots and lots of different unique text features, 'cause there's just a lot of words or the idea is a very unique code, so these are cases where we would really expect logistic regression overfit because there are going to be features that it can just pick out because there are so many different unique features, odds are it can pick out some unique features that perfectly labeled the training data, so it will put lots of weight on those when in reality they don't generalize very well.",
            "Um?",
            "The other observation to make is that these are quite usable accuracies, so in the vacation rental case, we're over 90%.",
            "In the job case, we are over 83% accurate, so with a very small fixed investment of human effort, specifically labeling the handful of pages on a handful of websites within each of these domains, we are able to create wrappers for hundreds of sites within that domain without any additional human effort."
        ],
        [
            "So thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Email and this is joint work with Charles Shafer at Google in Pittsburgh and the topic of the talk is bootstrapping information extraction from sending structured web pages.",
                    "label": 0
                },
                {
                    "sent": "Now many many.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sites on the Internet are template driven, meaning that behind them there's some structured information, typically from a database which has many attributes which are used to populate a template so that there's a spot in the template, say in this example, for the number of bedrooms and a description of this property and the address of it and an image and so on.",
                    "label": 0
                },
                {
                    "sent": "So there are many different domains of such semi structured web pages as we'll call them in this work.",
                    "label": 0
                },
                {
                    "sent": "Here we have an example of a vacation rentals page.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have other examples like encyclopedia acknowledge like Nobel Prize winners where we have the Gender country affiliation when they won the prize, what they wanted in and so on listed.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also have institutions such as the Smithsonian Smithsonian Institution, which makes the details of the items in its museum collections available online and so many others.",
                    "label": 0
                },
                {
                    "sent": "Such museums might make their information online too.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So each domain of semi structured websites.",
                    "label": 0
                },
                {
                    "sent": "Then we'll have some schema of structured data which most of its sites will come close to conforming to.",
                    "label": 0
                },
                {
                    "sent": "There may be some minor variation, but mostly they should conform to some particular schema.",
                    "label": 0
                },
                {
                    "sent": "So as an example of structured data so that we're on the same page, here's a table of structured records from a property rentals domain.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "Each row is a record of structured data and each column is an attribute such as the type of the property in the left column of the address of the property.",
                    "label": 0
                },
                {
                    "sent": "The number of bedrooms, whether it's occupied or vacant, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so as the core motivation for this work, why do we want structured data?",
                    "label": 0
                },
                {
                    "sent": "Well structured data is very, very useful.",
                    "label": 1
                },
                {
                    "sent": "It unlocks many, many different applications of this data as one of many examples, it would enable faceted search, or it does enable actually.",
                    "label": 0
                },
                {
                    "sent": "Right now faceted search such as this example, where a user can query for diamond bracelet Etsy items with images.",
                    "label": 0
                },
                {
                    "sent": "That matched the query, but also drill down in the search by restricting the different attributes of the items they're looking for, such as price or what type of gold it is, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the classic paradigm for extracting these records from semi structured websites.",
                    "label": 0
                },
                {
                    "sent": "Is supervised information extraction.",
                    "label": 0
                },
                {
                    "sent": "So in this paradigm a user will annotate a number of pages from a target website.",
                    "label": 1
                },
                {
                    "sent": "And then a model is learned which generalizes from these handful of training pages, which have been annotated to the rest of the pages in that website so that we can go on the left from a number of Pages 2 on the writes, a bunch of structured records that are extracted from the website.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So while we were working on a project too.",
                    "label": 0
                },
                {
                    "sent": "Obtain wrappers for a number of sites in a supervised way.",
                    "label": 1
                },
                {
                    "sent": "We wondered if it was possible rather than, say, labeling a few pages on a website and then building a model to generalize to other pages on that website.",
                    "label": 0
                },
                {
                    "sent": "Can we take a higher level approach and bootstrapping type approach where we are able to generalize?",
                    "label": 0
                },
                {
                    "sent": "From the records that we extract from those few sites that we have labeled data for to the rest of the sites in that domain.",
                    "label": 0
                },
                {
                    "sent": "So generalize at the website level as opposed to generalize across web sites as opposed to within websites.",
                    "label": 0
                },
                {
                    "sent": "So the the problem that we consider here is bootstrapping information from structured web pages, where we assume that we have wrappers for a number of sites in a domain.",
                    "label": 0
                },
                {
                    "sent": "And thus we can run those wrappers and extract many structured records from those websites.",
                    "label": 1
                },
                {
                    "sent": "So can we use what we've learned from these records?",
                    "label": 0
                },
                {
                    "sent": "To automatically wrap a new site in the same domain.",
                    "label": 1
                },
                {
                    "sent": "So here if we have three vacation rental sites, can we automatically wrap with no supervision required?",
                    "label": 0
                },
                {
                    "sent": "New vacation rental websites?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So drilling down a bit a core.",
                    "label": 0
                },
                {
                    "sent": "Technique.",
                    "label": 0
                },
                {
                    "sent": "The core techniques for both the supervised approach in her bootstrapping approach rely on the notion of Dom trees, so you can go from essentially Adama, Traore document object Model tree is just a simple tree based representation of the HTML contents of a web page, so this is a very simple mapping to perform.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we obtain the Dom trees for.",
                    "label": 0
                },
                {
                    "sent": "The collection of web pages that make up a website we can perform what's called tree alignment to produce a template tree which goes from here we have the two individual Dom trees for two different web pages on a website.",
                    "label": 1
                },
                {
                    "sent": "We align those and we produce the template tree on the bottom and so the templates tree.",
                    "label": 0
                },
                {
                    "sent": "You can think of each node in the template tree is sort of what you would intuitively if.",
                    "label": 0
                },
                {
                    "sent": "If you have the two web pages side by side, sort of point at.",
                    "label": 0
                },
                {
                    "sent": "These two spots are the same, the same spot within these two pages, so it's you align the content you try to match them as much as possible.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with string alignments algorithms from bioinformatics where you're trying to align gene sequences, the process is actually quite similar to that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then in the supervised setting what we do is we have a number of annotated pages from a website.",
                    "label": 1
                },
                {
                    "sent": "We take those pages, turn them into Dom trees, and build a template for those pages, and then in order to produce an model which we can then use to label the rest of the pages on that one website.",
                    "label": 0
                },
                {
                    "sent": "We look at the user annotations that were provided and we see which nodes in the template tree they are present on, and so we're able to then label this extraction template so that in order to label a new site, a new page.",
                    "label": 1
                },
                {
                    "sent": "Within this site we take the page, we align it to the template and so we get a mapping of the correspondence between the nodes in this web page to the nodes in the template.",
                    "label": 0
                },
                {
                    "sent": "And then we can just read off the labels.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, in the bootstrapping setting, we don't have annotated data for this website.",
                    "label": 1
                },
                {
                    "sent": "But what we do have are examples of each schema column from the training websites that we already had, and so rather than label the data fields.",
                    "label": 0
                },
                {
                    "sent": "So sorry in the upper left corner.",
                    "label": 0
                },
                {
                    "sent": "Take we want to bootstrap onto a new target site.",
                    "label": 0
                },
                {
                    "sent": "So we take the pages for that target site and create a template in an unsupervised manner.",
                    "label": 0
                },
                {
                    "sent": "We don't need labels in order to do that, and then the goal is to label the data fields in this template so the leaf nodes that contain text in this template.",
                    "label": 0
                },
                {
                    "sent": "So we can extract the values across all of the pages on the target site.",
                    "label": 0
                },
                {
                    "sent": "We can look at for each page we can look at the region that aligns to each node in the template and then for that node in the template we can get the distribution of values.",
                    "label": 1
                },
                {
                    "sent": "So in this case the lower left node in the generalized template might contain this list which contains cities.",
                    "label": 0
                },
                {
                    "sent": "This other text node might contain a very consistent label that indicates bedrooms and then the lower right node has this collection of integer values that we've observed, and so we can have a classifier for each schema column, such as city and number of bedrooms which classifieds each of these textual nodes in the template.",
                    "label": 0
                },
                {
                    "sent": "As to how likely it is to contain that schema columns information.",
                    "label": 0
                },
                {
                    "sent": "And so, after applying the classifiers, we can end up with the generalized template which we can use to extract records from this site where we've labeled the orange node indicating cities in the red reddish node indicating number of bedrooms.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in a bit more detail, the way these classifiers are trained.",
                    "label": 0
                },
                {
                    "sent": "Is it through this classification problem where if we're training the classifier that decides if the data field contains city data or not?",
                    "label": 0
                },
                {
                    "sent": "We have three training sites side A side B inside.",
                    "label": 0
                },
                {
                    "sent": "See, we have the observed values in the data fields which were labeled with this city attribute from those sites.",
                    "label": 0
                },
                {
                    "sent": "And then we also have so those are positive examples for a classifier for the city, schema column and then we have the rest of the data field values.",
                    "label": 0
                },
                {
                    "sent": "From the other data fields on those training sites which are negative examples for the city class, and so the goal is to train for city, number of bedrooms and so on.",
                    "label": 0
                },
                {
                    "sent": "The rest of the schema columns to train a classifier which can label this candidate data field on some new target site that's on the left of the screen.",
                    "label": 0
                },
                {
                    "sent": "As for the.",
                    "label": 0
                },
                {
                    "sent": "To classify whether or not it contains the data values for that particular schema column.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the types of features that we use of the values in a data field are based both on the content of the field as well as the context in which it appears in.",
                    "label": 0
                },
                {
                    "sent": "So we use features that are just the counts of the different tokens that appear in the field.",
                    "label": 0
                },
                {
                    "sent": "We also use what are called character 3 grams, which are just sub sequences.",
                    "label": 0
                },
                {
                    "sent": "All sub sequences of three characters, so this helps us capture things like abbreviations which are very very common on websites.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so the contextual features are also these character 3 gram sub sequences because you might have one site that says bedrooms and another site that says be DRMS.",
                    "label": 0
                },
                {
                    "sent": "So you want some overlap there.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm naive way of solving this classification problem is just simply we have a bunch of features we have labels.",
                    "label": 0
                },
                {
                    "sent": "Let's train a logistic regression model.",
                    "label": 0
                },
                {
                    "sent": "So for each schema column we train a logistic regression classifier where each data field from the training sites that we have labeled data for.",
                    "label": 0
                },
                {
                    "sent": "Is a labeled instance either positive or negative.",
                    "label": 0
                },
                {
                    "sent": "For each schema column and we just we throw all the features that we just described in the examples.",
                    "label": 0
                },
                {
                    "sent": "But the problem here is that each site will typically have on the order of, say, two dozen data fields detected in the template.",
                    "label": 0
                },
                {
                    "sent": "And so we have a only on the order of 10s of training instances.",
                    "label": 0
                },
                {
                    "sent": "But we have 10s of thousands of features, and so we we do observe in practice serious overfitting.",
                    "label": 0
                },
                {
                    "sent": "As you would expect, 'cause even with regularizer aggression.",
                    "label": 0
                },
                {
                    "sent": "This is a a poorly scaled problem for data versus features.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to obtain coarser features so that we can have a more appropriately scaled learning problem, we turn to distributional similarity.",
                    "label": 0
                },
                {
                    "sent": "So rather than learn things rather than learn classifiers that have features like.",
                    "label": 0
                },
                {
                    "sent": "The number of times RMS occurs in this data field is 7 and lots of very very fine grained features like that.",
                    "label": 0
                },
                {
                    "sent": "We have distributional similarity features where each field is treated as a distribution of values and we extract features based on the distribution of similarity of.",
                    "label": 1
                },
                {
                    "sent": "Of the.",
                    "label": 0
                },
                {
                    "sent": "For each feature type, we have a distributional similarity feature, which says how similar the distribution of those features is to be labeled data, and so we use KL divergent and we just we apply a simple transformation to smoothen normalize it, which we call skew similarity.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our smarter classification attempt, we called the sechskies model.",
                    "label": 0
                },
                {
                    "sent": "So again, each field from the training sites is a labeled instance, but the features are the distribution similarity for each feature type.",
                    "label": 1
                },
                {
                    "sent": "So for example the.",
                    "label": 0
                },
                {
                    "sent": "The classifier that decides if something holds the value for the city schema column would have as its features how similar the candidate data fields, say contextual character 3 ground features are to the observed features in the city training data.",
                    "label": 0
                },
                {
                    "sent": "So this approach was sorry, so from those four features, since we have 4 feature types.",
                    "label": 1
                },
                {
                    "sent": "We train a linear regression model and this was inspired by working database schema matching by modifying it all in 2005.",
                    "label": 1
                },
                {
                    "sent": "So now we have 10s of training instances.",
                    "label": 0
                },
                {
                    "sent": "We have only one feature for feature type, so just a handful.",
                    "label": 0
                },
                {
                    "sent": "We now have more training examples.",
                    "label": 0
                },
                {
                    "sent": "Then we have features to learn, so this is an appropriately prize sized learning.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Album so previous related work.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work on sort of learning the template so the Dom tree template.",
                    "label": 0
                },
                {
                    "sent": "Method that I described is sort of one of several.",
                    "label": 0
                },
                {
                    "sent": "Previous approaches which discovered the regularity's and can tell you this is data field one.",
                    "label": 0
                },
                {
                    "sent": "This is data field to this.",
                    "label": 0
                },
                {
                    "sent": "Is it a field 3?",
                    "label": 0
                },
                {
                    "sent": "But most of that work doesn't attempt to label those data fields, so it can tell you this is actually data field that you're interested in an oh by the way, it contains the value for the city schema column, the work that has taken the extra step and labeled the data fields which with which schema columns that contain.",
                    "label": 0
                },
                {
                    "sent": "Has mostly used brittle heuristic approaches which look for things like.",
                    "label": 0
                },
                {
                    "sent": "This looks like it contains a price, so it's the price.",
                    "label": 0
                },
                {
                    "sent": "But in practice websites or web pages will have lots of prices on them, and so if you had multiple matches of these heuristics, which actually happened quite a lot.",
                    "label": 0
                },
                {
                    "sent": "The algorithm wouldn't be able to decide which was the correct data field.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Attract.",
                    "label": 0
                },
                {
                    "sent": "So in our evaluation we considered two domains.",
                    "label": 0
                },
                {
                    "sent": "The first domain is vacation rentals, so it has a schema with seven different attributes in it.",
                    "label": 1
                },
                {
                    "sent": "There were five vacation rental sites and we evaluated the system in a cross validated fashion by we'd hold out one vacation rental site so we wouldn't have any trading data for that site.",
                    "label": 0
                },
                {
                    "sent": "We would then train the model on the four, help the four remaining vacation rental site and see how accurate we were on the held outside.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also used job listings as a second domain for evaluating the system.",
                    "label": 0
                },
                {
                    "sent": "They said six different attributes that we were trying to distract in seven different websites.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our results are presented in accuracy by schema column.",
                    "label": 1
                },
                {
                    "sent": "So here accuracy means.",
                    "label": 0
                },
                {
                    "sent": "So for the vacation rental domain, for example, for each website which we had as a test example.",
                    "label": 0
                },
                {
                    "sent": "We had the task of deciding which data field on this website contains the address of the property.",
                    "label": 0
                },
                {
                    "sent": "Or we could say we don't think the address field appears on this website, so the accuracy here measures the accuracy of those decisions.",
                    "label": 0
                },
                {
                    "sent": "So either picking the data field or saying it doesn't exist on this site.",
                    "label": 0
                },
                {
                    "sent": "The left plot shows results for the vacation rental sites and the right plot shows the results for the job sites and the yellow bars showed the results for the logistic regression classifier and the blue results showed how the stacks queues model are hopefully smarter classification attempt did.",
                    "label": 0
                },
                {
                    "sent": "So we see that we do significantly outperform logistic regression baseline, particularly on schema columns like the description of a vacation rental property or the unique ID of a job which.",
                    "label": 0
                },
                {
                    "sent": "These are exactly the types of fields which have lots and lots of different unique text features, 'cause there's just a lot of words or the idea is a very unique code, so these are cases where we would really expect logistic regression overfit because there are going to be features that it can just pick out because there are so many different unique features, odds are it can pick out some unique features that perfectly labeled the training data, so it will put lots of weight on those when in reality they don't generalize very well.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The other observation to make is that these are quite usable accuracies, so in the vacation rental case, we're over 90%.",
                    "label": 0
                },
                {
                    "sent": "In the job case, we are over 83% accurate, so with a very small fixed investment of human effort, specifically labeling the handful of pages on a handful of websites within each of these domains, we are able to create wrappers for hundreds of sites within that domain without any additional human effort.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you.",
                    "label": 0
                }
            ]
        }
    }
}