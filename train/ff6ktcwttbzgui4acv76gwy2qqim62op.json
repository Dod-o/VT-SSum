{
    "id": "ff6ktcwttbzgui4acv76gwy2qqim62op",
    "title": "Influence-based Policy Abstraction for Weakly-coupled DEC-POMDPs",
    "info": {
        "author": [
            "Stefan Witwicki, University of Michigan"
        ],
        "published": "Nov. 15, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/icaps2010_witwicki_ibpa/",
    "segmentation": [
        [
            "I name is Steven.",
            "This is joint work with my advisor at Durfee, on what we're calling influence based policy abstraction."
        ],
        [
            "So the problem that we addressed is one of team coordination under uncertainty.",
            "As an example, consider a team of satellites and Rovers that are exploring the surface of Mars.",
            "And each agent has its own internal activities with internal constraints as denoted by these blocks and connectors.",
            "For instance, here the Rover visits various sites of interest, digs into the ground, and analyzes whatever digs up.",
            "Satellite one on the other hand, orbits the planet photographing the surface, building Maps and.",
            "Forecasting weather now each of these activities has uncertain outcomes.",
            "For instance, if they are over to visit a particular site A.",
            "With probability .2, the pick three hours and achieve equality of two with probability .8, the trip would take six hours and she will slightly lower value quality.",
            "Then the reason for this uncertainties in part because of the Rovers limited perspective of its terrain.",
            "Instead of taking a direct route over Hill and May find that it needs to take a longer route around the Hill.",
            "You also notice that this activity is constrained to occur between the hours of three and 10, denoted by this window.",
            "Now the agents are by and large independent, but they can interact through their activities.",
            "So here a satellite could build a path for the Rover.",
            "Which would positively affect the outcome of the visit activity, ensuring that it takes less time with higher probability.",
            "No, in this particular example, the objective of the team is to maximize the accrued activity qualities.",
            "And you can see that the agents would benefit from coordinating their activities."
        ],
        [
            "So to model our teams activities, we turn to the decentralized Palm DP, which is the same model that Feng Feng used.",
            "I'll very briefly review this model.",
            "So the agents start out in a world state St.",
            "They don't observe the world state directly.",
            "Instead they each observe and observation.",
            "Which we will represent with a single variable for simplicity.",
            "OT.",
            "Then they select actions.",
            "And transition into a new world state.",
            "Also, receive a team reward.",
            "And then transition into into a new world state with a new observation.",
            "And this model is nice for a few reasons.",
            "First, it allows us to model agents that observe different aspects of their."
        ],
        [
            "Environment.",
            "This is the observation function.",
            "It also allows us to model outcome uncertainty, but via the nondeterministic state transitions.",
            "And.",
            "It provides a.",
            "Well defined notion of optimality.",
            "Here the planning task is to find the joint policy that maximizes some function of the teams reward.",
            "In our case, that's going to be a summation over finite horizon, or rather an expected summation over finite arising."
        ],
        [
            "So we have this wonderfully rich representation.",
            "The only problem is that it's incredibly difficult to solve.",
            "And as a consequence, a state of the art methods haven't been shown to scale beyond three agents and rarely even beyond two agents.",
            "Even on toy problems, without making some sacrifices.",
            "For instance, some methods assume a property called transition and observation independence, where agents cannot affect each others activity outcomes or their observations.",
            "There are also specialized subclasses of Dec pomdp's that, in addition to restricting interaction, also restrict agents individual behavior, and then there are also methods that scale more general classes of decom DPS, but without guaranteeing optimal or even near optimal solution quality.",
            "So the challenge that I addressed here is how to scale to more agents without making these sacrifices.",
            "Moreover, can we increase the quality bounded?",
            "Agent scalability, while still allowing some general form of transition dependence."
        ],
        [
            "And the way we approach this challenge is instead of restricting agents behavior, we're going to exploit structure and sparseness in their interactions.",
            "And we do this by characterizing the influences that they exert on one another.",
            "In such a way that they can coordinate over a smaller space but still achieve optimal solutions."
        ],
        [
            "So looking back at the Death Palm DP for a moment, the figure that I showed you before is actually dynamic vision network.",
            "That conveys the probabilistic dependence between individual variables.",
            "For instance, here the next state S T + 1 is dependent on the value of the previous state as well as the action.",
            "Within this framework, we can also talk about factories."
        ],
        [
            "Tations where instead of a single world state variable, we've broken the state up into individual features.",
            "Ann, this allows us to capture some conditional independence ease that may exist among individual features.",
            "So for instance, here we have a variable that represents dust storm D and you'll notice that there is no arrow leading from the joint action to the new value of D and this is because the agents can affect whether or not there's a dust storm on Mars through any actions that they take."
        ],
        [
            "So we could push this factor into one particular extreme, where we assume that the deck palm DP is really just a.",
            "A collection of completely independent, single agent Palm DP's.",
            "An so in that.",
            "The world state has been factored into local states.",
            "The transition function is also been factored so that.",
            "So it's dependent depend only on local state, each agents observation.",
            "Only depends on its local state variables, an local actions and also the reward function has been decomposed into independent local rewards that also depend only on local, state and local action.",
            "And this extreme isn't very interesting from a standpoint of coordination, because here our agents can't interact at all.",
            "But consider a slight."
        ],
        [
            "Variation.",
            "Where we now allow the local states to overlap.",
            "So now.",
            "Agent JS Local state could include a feature NJ which will call a non local feature that can be manipulated by another agent.",
            "High is actions.",
            "But that affects the subsequent transitions of Jay's own actions.",
            "Example of this would be a future path they built that gets set once the satellite completes its building path activity.",
            "And now suddenly agents are transition dependent.",
            "They are also observation dependent because both of their observations can depend on these shared features.",
            "And we call this the TD Palm DP or transition decoupler palm DP."
        ],
        [
            "So a TD palm DP may contain just a single shared feature.",
            "But there may be more.",
            "The key is that we are explicitly representing those features through which transition dependent agents interact.",
            "And this is demonstrated by the."
        ],
        [
            "Correction digraph shown on the side of this slide.",
            "We believe this to be a natural representation for the designer of the multi agent system to specify.",
            "Particularly when interactions are sparse."
        ],
        [
            "No to take advantage of the structure we've defined, we're going to build on work that others others have done on a decoupled solution methodology where each agent is responsible for computing its own local policy.",
            "And it does this by computing the best response to various candidate peer policies.",
            "And so you can imagine if the agents.",
            "All compute best responses to enough candidate peer policies.",
            "Then they can find the optimal joint policy and this method has been very successful in scaling transition and observation independent models, but much less so in the more general transition dependent case.",
            "There are a couple of reasons for this.",
            "First, the best response computation itself is very expensive in general, requiring that agents reason about the possible observations of their peers, and in fact the distribution over observation histories of their peers.",
            "Furthermore, because of the joint policy is quite large.",
            "This costly calculation has to be done over and over and over again in order to ensure that we find the optimal joint pool."
        ],
        [
            "So.",
            "Let's take a closer look at the best response calculation.",
            "And ask what's really going on inside this box?",
            "So I would argue that what's happening is that Agent J is accounting for the influence of eyes planned decissions encoded by eyes policy.",
            "And Jay is planning its own behavior accordingly.",
            "So with this in mind."
        ],
        [
            "We can turn back to our Satellite Rover example.",
            "And here you'll notice that given the constraints.",
            "The satellite can only build its path before time 3.",
            "And the rubber can only visit the site after time three.",
            "This means that there are only needs to worry about whether or not a path was built by time 3.",
            "In other words, the only influence that sat one has on ourselves is the likelihood that a path will be built by time 3.",
            "And in fact.",
            "There are many policies there.",
            "In fact, any two policies.",
            "That affecting this same.",
            "Likelihood of the site being prepared by time or the path being pulled by time 3.",
            "Would actually map to the same best response.",
            "And in fact, in this simple example, there are many such policies that are mapped to the same best response.",
            "So one can envision our partitioning of the policy space where all the policies in the same partition affecting the same influence, and we call this partitioning the influence space."
        ],
        [
            "So we propose to add a layer of abstraction to the best response calculation where pure influences are abstracted from policies and the best response computation becomes a function of influence, not have not appear policy.",
            "How can we achieve?"
        ],
        [
            "This.",
            "For the TD Palm DP, since we've explicitly represented those nonlocal features through agents or interacting influences, in fact correspond to the transition probabilities of those local features.",
            "In other words, influence can be represented with the distribution of new non local feature values conditioned on some dependent feature values."
        ],
        [
            "And in order to compute the best response.",
            "Simply involves augmenting the local state with any extra feature information that's required and then setting those transition probabilities according to the influence."
        ],
        [
            "And we've proven that if we represent influences in this way, then the probability distributions don't need to be conditioned on agents, other agents, observations.",
            "They don't need to be conditioned on most of the world state features.",
            "In fact, in order to compute optimal best responses only need to be conditioned.",
            "On features that are in this shared set.",
            "And these features themselves constitute a dynamic Bayesian network which will call the influence DBN."
        ],
        [
            "So the consequence is that as long as agents are weakly coupled, when we scale up to more agents, the best response for calculation remains remains smaller.",
            "The model remains compact.",
            "Furthermore, the number of parameters that we used to represent agents influences remain calm."
        ],
        [
            "Act and this suggests that the influence space itself should remain compact.",
            "The other implication is that.",
            "By finding the point in influence space that achieves the highest joint utility.",
            "This this is actually.",
            "The optimal joint policy."
        ],
        [
            "So we've reduced the problem of policy speech search to one of influence space search.",
            "In our paper, we give an algorithm for calculating the optimal influence in the optimal joint policy, which I won't go into detail here, but I would like to show you some results now."
        ],
        [
            "So one hypothesis that we had is that.",
            "It's when agents are weakly coupled that.",
            "That our influence abstraction has the most benefit.",
            "And this weak coupling relates to the number of interactions as well as the constraint edness of those interactions.",
            "So we developed a metric or a parameter called influence constraint edness.",
            "That could be systematically varied.",
            "When we generate problems which essentially relates to the window size of the non locally affecting activity.",
            "An what we expect is that.",
            "When influences are least constrained, we may have many partitions of the policy space, but as they become more and more constrained, there are fewer unique influences that this can happen.",
            "One another, to the point where all policies mapped to the same influence."
        ],
        [
            "So we compared our algorithm called OS optimal influence space search against two other state of the art optimal.",
            "Policy space coordination algorithms.",
            "The first one separable by linear programming and the second One Spider.",
            "What we see here is that.",
            "When we, when the influence constrained, this goes up, our OS sees a exponential speedup in runtime.",
            "While the Policy space coordination methods remain relatively flat, this is a runtime being plotted.",
            "And this suggests that there's potential for massive massive speedup with this influence based abstract."
        ],
        [
            "The other hypothesis.",
            "That we had.",
            "Is that because we're representing influences with probability distributions there?",
            "Many, many various methods of approximation that we could use."
        ],
        [
            "I think I'll skip over this result."
        ],
        [
            "Tell you some conclusions now though.",
            "So in conclusion, we find that by explicitly representing those features through its transition dependent agents interact an subject to some structural assumptions.",
            "We can decouple the joint planning problem into compact local best response models with optimality guarantees.",
            "This is possible through an isolation of the information that needs to be coordinated.",
            "Instead of exchanging full policy's agents, can exchange compact influences an?",
            "And initial empirical results suggest exponential speedup in efficiency.",
            "As well as scalability beyond the state of the art and for future work, we're running on, we're working on experiments with sets of problems on more complex problems, as well as.",
            "Varying parameters that relate to the interaction digraph structure.",
            "We'd also like to compare our methods against state of the art approximate technology solution methods.",
            "And finally, we'd like to work more on approximate methods of a full space search."
        ],
        [
            "Thanks for your attention.",
            "Are there any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I name is Steven.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with my advisor at Durfee, on what we're calling influence based policy abstraction.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem that we addressed is one of team coordination under uncertainty.",
                    "label": 1
                },
                {
                    "sent": "As an example, consider a team of satellites and Rovers that are exploring the surface of Mars.",
                    "label": 0
                },
                {
                    "sent": "And each agent has its own internal activities with internal constraints as denoted by these blocks and connectors.",
                    "label": 0
                },
                {
                    "sent": "For instance, here the Rover visits various sites of interest, digs into the ground, and analyzes whatever digs up.",
                    "label": 0
                },
                {
                    "sent": "Satellite one on the other hand, orbits the planet photographing the surface, building Maps and.",
                    "label": 0
                },
                {
                    "sent": "Forecasting weather now each of these activities has uncertain outcomes.",
                    "label": 1
                },
                {
                    "sent": "For instance, if they are over to visit a particular site A.",
                    "label": 0
                },
                {
                    "sent": "With probability .2, the pick three hours and achieve equality of two with probability .8, the trip would take six hours and she will slightly lower value quality.",
                    "label": 0
                },
                {
                    "sent": "Then the reason for this uncertainties in part because of the Rovers limited perspective of its terrain.",
                    "label": 0
                },
                {
                    "sent": "Instead of taking a direct route over Hill and May find that it needs to take a longer route around the Hill.",
                    "label": 0
                },
                {
                    "sent": "You also notice that this activity is constrained to occur between the hours of three and 10, denoted by this window.",
                    "label": 0
                },
                {
                    "sent": "Now the agents are by and large independent, but they can interact through their activities.",
                    "label": 0
                },
                {
                    "sent": "So here a satellite could build a path for the Rover.",
                    "label": 0
                },
                {
                    "sent": "Which would positively affect the outcome of the visit activity, ensuring that it takes less time with higher probability.",
                    "label": 0
                },
                {
                    "sent": "No, in this particular example, the objective of the team is to maximize the accrued activity qualities.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the agents would benefit from coordinating their activities.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to model our teams activities, we turn to the decentralized Palm DP, which is the same model that Feng Feng used.",
                    "label": 0
                },
                {
                    "sent": "I'll very briefly review this model.",
                    "label": 0
                },
                {
                    "sent": "So the agents start out in a world state St.",
                    "label": 0
                },
                {
                    "sent": "They don't observe the world state directly.",
                    "label": 0
                },
                {
                    "sent": "Instead they each observe and observation.",
                    "label": 0
                },
                {
                    "sent": "Which we will represent with a single variable for simplicity.",
                    "label": 0
                },
                {
                    "sent": "OT.",
                    "label": 0
                },
                {
                    "sent": "Then they select actions.",
                    "label": 0
                },
                {
                    "sent": "And transition into a new world state.",
                    "label": 0
                },
                {
                    "sent": "Also, receive a team reward.",
                    "label": 0
                },
                {
                    "sent": "And then transition into into a new world state with a new observation.",
                    "label": 0
                },
                {
                    "sent": "And this model is nice for a few reasons.",
                    "label": 0
                },
                {
                    "sent": "First, it allows us to model agents that observe different aspects of their.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Environment.",
                    "label": 0
                },
                {
                    "sent": "This is the observation function.",
                    "label": 0
                },
                {
                    "sent": "It also allows us to model outcome uncertainty, but via the nondeterministic state transitions.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It provides a.",
                    "label": 0
                },
                {
                    "sent": "Well defined notion of optimality.",
                    "label": 1
                },
                {
                    "sent": "Here the planning task is to find the joint policy that maximizes some function of the teams reward.",
                    "label": 0
                },
                {
                    "sent": "In our case, that's going to be a summation over finite horizon, or rather an expected summation over finite arising.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have this wonderfully rich representation.",
                    "label": 0
                },
                {
                    "sent": "The only problem is that it's incredibly difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "And as a consequence, a state of the art methods haven't been shown to scale beyond three agents and rarely even beyond two agents.",
                    "label": 0
                },
                {
                    "sent": "Even on toy problems, without making some sacrifices.",
                    "label": 0
                },
                {
                    "sent": "For instance, some methods assume a property called transition and observation independence, where agents cannot affect each others activity outcomes or their observations.",
                    "label": 0
                },
                {
                    "sent": "There are also specialized subclasses of Dec pomdp's that, in addition to restricting interaction, also restrict agents individual behavior, and then there are also methods that scale more general classes of decom DPS, but without guaranteeing optimal or even near optimal solution quality.",
                    "label": 0
                },
                {
                    "sent": "So the challenge that I addressed here is how to scale to more agents without making these sacrifices.",
                    "label": 0
                },
                {
                    "sent": "Moreover, can we increase the quality bounded?",
                    "label": 1
                },
                {
                    "sent": "Agent scalability, while still allowing some general form of transition dependence.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the way we approach this challenge is instead of restricting agents behavior, we're going to exploit structure and sparseness in their interactions.",
                    "label": 0
                },
                {
                    "sent": "And we do this by characterizing the influences that they exert on one another.",
                    "label": 0
                },
                {
                    "sent": "In such a way that they can coordinate over a smaller space but still achieve optimal solutions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So looking back at the Death Palm DP for a moment, the figure that I showed you before is actually dynamic vision network.",
                    "label": 0
                },
                {
                    "sent": "That conveys the probabilistic dependence between individual variables.",
                    "label": 0
                },
                {
                    "sent": "For instance, here the next state S T + 1 is dependent on the value of the previous state as well as the action.",
                    "label": 0
                },
                {
                    "sent": "Within this framework, we can also talk about factories.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tations where instead of a single world state variable, we've broken the state up into individual features.",
                    "label": 1
                },
                {
                    "sent": "Ann, this allows us to capture some conditional independence ease that may exist among individual features.",
                    "label": 0
                },
                {
                    "sent": "So for instance, here we have a variable that represents dust storm D and you'll notice that there is no arrow leading from the joint action to the new value of D and this is because the agents can affect whether or not there's a dust storm on Mars through any actions that they take.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we could push this factor into one particular extreme, where we assume that the deck palm DP is really just a.",
                    "label": 0
                },
                {
                    "sent": "A collection of completely independent, single agent Palm DP's.",
                    "label": 0
                },
                {
                    "sent": "An so in that.",
                    "label": 0
                },
                {
                    "sent": "The world state has been factored into local states.",
                    "label": 1
                },
                {
                    "sent": "The transition function is also been factored so that.",
                    "label": 1
                },
                {
                    "sent": "So it's dependent depend only on local state, each agents observation.",
                    "label": 1
                },
                {
                    "sent": "Only depends on its local state variables, an local actions and also the reward function has been decomposed into independent local rewards that also depend only on local, state and local action.",
                    "label": 0
                },
                {
                    "sent": "And this extreme isn't very interesting from a standpoint of coordination, because here our agents can't interact at all.",
                    "label": 0
                },
                {
                    "sent": "But consider a slight.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Variation.",
                    "label": 0
                },
                {
                    "sent": "Where we now allow the local states to overlap.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Agent JS Local state could include a feature NJ which will call a non local feature that can be manipulated by another agent.",
                    "label": 1
                },
                {
                    "sent": "High is actions.",
                    "label": 0
                },
                {
                    "sent": "But that affects the subsequent transitions of Jay's own actions.",
                    "label": 0
                },
                {
                    "sent": "Example of this would be a future path they built that gets set once the satellite completes its building path activity.",
                    "label": 0
                },
                {
                    "sent": "And now suddenly agents are transition dependent.",
                    "label": 0
                },
                {
                    "sent": "They are also observation dependent because both of their observations can depend on these shared features.",
                    "label": 0
                },
                {
                    "sent": "And we call this the TD Palm DP or transition decoupler palm DP.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a TD palm DP may contain just a single shared feature.",
                    "label": 0
                },
                {
                    "sent": "But there may be more.",
                    "label": 0
                },
                {
                    "sent": "The key is that we are explicitly representing those features through which transition dependent agents interact.",
                    "label": 0
                },
                {
                    "sent": "And this is demonstrated by the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Correction digraph shown on the side of this slide.",
                    "label": 0
                },
                {
                    "sent": "We believe this to be a natural representation for the designer of the multi agent system to specify.",
                    "label": 0
                },
                {
                    "sent": "Particularly when interactions are sparse.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No to take advantage of the structure we've defined, we're going to build on work that others others have done on a decoupled solution methodology where each agent is responsible for computing its own local policy.",
                    "label": 0
                },
                {
                    "sent": "And it does this by computing the best response to various candidate peer policies.",
                    "label": 1
                },
                {
                    "sent": "And so you can imagine if the agents.",
                    "label": 0
                },
                {
                    "sent": "All compute best responses to enough candidate peer policies.",
                    "label": 1
                },
                {
                    "sent": "Then they can find the optimal joint policy and this method has been very successful in scaling transition and observation independent models, but much less so in the more general transition dependent case.",
                    "label": 0
                },
                {
                    "sent": "There are a couple of reasons for this.",
                    "label": 0
                },
                {
                    "sent": "First, the best response computation itself is very expensive in general, requiring that agents reason about the possible observations of their peers, and in fact the distribution over observation histories of their peers.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, because of the joint policy is quite large.",
                    "label": 0
                },
                {
                    "sent": "This costly calculation has to be done over and over and over again in order to ensure that we find the optimal joint pool.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's take a closer look at the best response calculation.",
                    "label": 1
                },
                {
                    "sent": "And ask what's really going on inside this box?",
                    "label": 1
                },
                {
                    "sent": "So I would argue that what's happening is that Agent J is accounting for the influence of eyes planned decissions encoded by eyes policy.",
                    "label": 0
                },
                {
                    "sent": "And Jay is planning its own behavior accordingly.",
                    "label": 0
                },
                {
                    "sent": "So with this in mind.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can turn back to our Satellite Rover example.",
                    "label": 0
                },
                {
                    "sent": "And here you'll notice that given the constraints.",
                    "label": 0
                },
                {
                    "sent": "The satellite can only build its path before time 3.",
                    "label": 0
                },
                {
                    "sent": "And the rubber can only visit the site after time three.",
                    "label": 1
                },
                {
                    "sent": "This means that there are only needs to worry about whether or not a path was built by time 3.",
                    "label": 0
                },
                {
                    "sent": "In other words, the only influence that sat one has on ourselves is the likelihood that a path will be built by time 3.",
                    "label": 1
                },
                {
                    "sent": "And in fact.",
                    "label": 0
                },
                {
                    "sent": "There are many policies there.",
                    "label": 0
                },
                {
                    "sent": "In fact, any two policies.",
                    "label": 0
                },
                {
                    "sent": "That affecting this same.",
                    "label": 1
                },
                {
                    "sent": "Likelihood of the site being prepared by time or the path being pulled by time 3.",
                    "label": 1
                },
                {
                    "sent": "Would actually map to the same best response.",
                    "label": 0
                },
                {
                    "sent": "And in fact, in this simple example, there are many such policies that are mapped to the same best response.",
                    "label": 0
                },
                {
                    "sent": "So one can envision our partitioning of the policy space where all the policies in the same partition affecting the same influence, and we call this partitioning the influence space.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we propose to add a layer of abstraction to the best response calculation where pure influences are abstracted from policies and the best response computation becomes a function of influence, not have not appear policy.",
                    "label": 0
                },
                {
                    "sent": "How can we achieve?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "For the TD Palm DP, since we've explicitly represented those nonlocal features through agents or interacting influences, in fact correspond to the transition probabilities of those local features.",
                    "label": 0
                },
                {
                    "sent": "In other words, influence can be represented with the distribution of new non local feature values conditioned on some dependent feature values.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in order to compute the best response.",
                    "label": 0
                },
                {
                    "sent": "Simply involves augmenting the local state with any extra feature information that's required and then setting those transition probabilities according to the influence.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we've proven that if we represent influences in this way, then the probability distributions don't need to be conditioned on agents, other agents, observations.",
                    "label": 0
                },
                {
                    "sent": "They don't need to be conditioned on most of the world state features.",
                    "label": 1
                },
                {
                    "sent": "In fact, in order to compute optimal best responses only need to be conditioned.",
                    "label": 0
                },
                {
                    "sent": "On features that are in this shared set.",
                    "label": 1
                },
                {
                    "sent": "And these features themselves constitute a dynamic Bayesian network which will call the influence DBN.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the consequence is that as long as agents are weakly coupled, when we scale up to more agents, the best response for calculation remains remains smaller.",
                    "label": 0
                },
                {
                    "sent": "The model remains compact.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, the number of parameters that we used to represent agents influences remain calm.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Act and this suggests that the influence space itself should remain compact.",
                    "label": 0
                },
                {
                    "sent": "The other implication is that.",
                    "label": 0
                },
                {
                    "sent": "By finding the point in influence space that achieves the highest joint utility.",
                    "label": 0
                },
                {
                    "sent": "This this is actually.",
                    "label": 0
                },
                {
                    "sent": "The optimal joint policy.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've reduced the problem of policy speech search to one of influence space search.",
                    "label": 0
                },
                {
                    "sent": "In our paper, we give an algorithm for calculating the optimal influence in the optimal joint policy, which I won't go into detail here, but I would like to show you some results now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one hypothesis that we had is that.",
                    "label": 0
                },
                {
                    "sent": "It's when agents are weakly coupled that.",
                    "label": 0
                },
                {
                    "sent": "That our influence abstraction has the most benefit.",
                    "label": 0
                },
                {
                    "sent": "And this weak coupling relates to the number of interactions as well as the constraint edness of those interactions.",
                    "label": 0
                },
                {
                    "sent": "So we developed a metric or a parameter called influence constraint edness.",
                    "label": 0
                },
                {
                    "sent": "That could be systematically varied.",
                    "label": 0
                },
                {
                    "sent": "When we generate problems which essentially relates to the window size of the non locally affecting activity.",
                    "label": 0
                },
                {
                    "sent": "An what we expect is that.",
                    "label": 0
                },
                {
                    "sent": "When influences are least constrained, we may have many partitions of the policy space, but as they become more and more constrained, there are fewer unique influences that this can happen.",
                    "label": 0
                },
                {
                    "sent": "One another, to the point where all policies mapped to the same influence.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we compared our algorithm called OS optimal influence space search against two other state of the art optimal.",
                    "label": 0
                },
                {
                    "sent": "Policy space coordination algorithms.",
                    "label": 0
                },
                {
                    "sent": "The first one separable by linear programming and the second One Spider.",
                    "label": 0
                },
                {
                    "sent": "What we see here is that.",
                    "label": 0
                },
                {
                    "sent": "When we, when the influence constrained, this goes up, our OS sees a exponential speedup in runtime.",
                    "label": 0
                },
                {
                    "sent": "While the Policy space coordination methods remain relatively flat, this is a runtime being plotted.",
                    "label": 0
                },
                {
                    "sent": "And this suggests that there's potential for massive massive speedup with this influence based abstract.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That we had.",
                    "label": 0
                },
                {
                    "sent": "Is that because we're representing influences with probability distributions there?",
                    "label": 1
                },
                {
                    "sent": "Many, many various methods of approximation that we could use.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I'll skip over this result.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tell you some conclusions now though.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion, we find that by explicitly representing those features through its transition dependent agents interact an subject to some structural assumptions.",
                    "label": 0
                },
                {
                    "sent": "We can decouple the joint planning problem into compact local best response models with optimality guarantees.",
                    "label": 1
                },
                {
                    "sent": "This is possible through an isolation of the information that needs to be coordinated.",
                    "label": 0
                },
                {
                    "sent": "Instead of exchanging full policy's agents, can exchange compact influences an?",
                    "label": 0
                },
                {
                    "sent": "And initial empirical results suggest exponential speedup in efficiency.",
                    "label": 1
                },
                {
                    "sent": "As well as scalability beyond the state of the art and for future work, we're running on, we're working on experiments with sets of problems on more complex problems, as well as.",
                    "label": 0
                },
                {
                    "sent": "Varying parameters that relate to the interaction digraph structure.",
                    "label": 1
                },
                {
                    "sent": "We'd also like to compare our methods against state of the art approximate technology solution methods.",
                    "label": 1
                },
                {
                    "sent": "And finally, we'd like to work more on approximate methods of a full space search.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                }
            ]
        }
    }
}