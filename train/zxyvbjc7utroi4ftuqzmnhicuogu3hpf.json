{
    "id": "zxyvbjc7utroi4ftuqzmnhicuogu3hpf",
    "title": "Modelling Transcriptional Regulation with Gaussian Processes",
    "info": {
        "author": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "April 4, 2007",
        "recorded": "March 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/pesb07_lawrence_mtr/",
    "segmentation": [
        [
            "I mean, there's been a bit of a theme over the last three talks, which I guess was intentional.",
            "Being the last talk I'm going to try and keep the equations to the minimum and.",
            "Try and focus on the concepts, particularly because I think some of the concepts are introduced maybe nude some in the audience.",
            "I know there's definitely some that are familiar with them, so the talks entitled modeling transcription regulation with Gaussian processes and the really nice thing about.",
            "This is that the application I can talk about very quickly is basically our work."
        ],
        [
            "Completely inspired by me seeing a talk by Martina Barranco at Sheffield a couple of years ago, so it's take basically working with his data set, but with a different approach to doing it.",
            "So he had a linear response system, and that's what I'll focus on mainly to get the intuitions across, and then I'll briefly wave my hands a bit and say when you can do some of this with nonlinear response models and show you some results without going into too much details.",
            "So well, all the slides will be available online, but the source code is.",
            "This is very lonely."
        ],
        [
            "And all the demos that you see here you can download if you write in MATLAB, you can try them on your own."
        ],
        [
            "If they happen to be in a working state.",
            "So, as Martinez talked about, and we've just seen as well, many interaction networks have latent functions, and so they talk about this as a hidden variable dynamical model, and it would perhaps be clearer if we refer to this as an approach to solving the problem of a hidden variable dynamical model.",
            "What we're going to do is you doing many latent variable problems.",
            "You assume a prior distribution, so we're going to assume what's called a Gaussian process prior distribution for this latent function.",
            "Now this is a smooth continuous function overtime and a Gaussian process is a probabilistic model for functions.",
            "There's a couple of references there.",
            "So our approach is to take the differential equation model for the system that Martino talked about and derive a Gaussian process covariance jointly for the observed and latent function.",
            "So we get a Gaussian covariance which relates the observed measurements to the latent measurements.",
            "Now, by doing this, we sort of finesse some of the problems of derivative measurements, that sort of drops out.",
            "We never have to worry about estimating the derivatives, it's all part of the framework, because the derivative of Gaussian process is also a Gaussian process, and.",
            "This sort of joint covariance handles all that for us, so we only work with the direct observation of the functions.",
            "Then we maximize the likelihood with respect to the parameters of this covariance, and these parameters will turn out to be those decays and sensitivities, and basal transcription rates that Martino talked about earlier.",
            "And we also do hybrid Monte Carlo, two estimators.",
            "Martino said it's important to get uncertainty estimates, so we also do hybrid Monte Carlo on that.",
            "So I think I'll skip that, so this was there."
        ],
        [
            "Asian Martino had I don't have to talk too much about it 'cause he's already talked about it.",
            "We've got this.",
            "Some genes which we sort of known to be targets of a given transcription factor, in this case P53, or in the case Martino talked about P53 and the gradient of the production rate of the gene is given by basil transcription rate plus some sensitivity times the concentration of P 53 minus decay rate.",
            "There so.",
            "In this case, we've got a linear dependence on the transcription factor, and that's something we'll relax later, but I won't really talk about the details of relaxing to that.",
            "So you can solve this equation.",
            "Think."
        ],
        [
            "Tina probably had certainly has that in this paper that this paper is this.",
            "Each gene expression is given by something of this form, where this is a sort of.",
            "The plus style convolution between the function and this exponential here.",
            "Now what's interesting about Gaussian processes?",
            "Is there like Gaussian distributions?",
            "So if we model F of T as a Gaussian process, then this thing here only involves linear operations on.",
            "To get XLT it only involves linear operations on F of T, so because there's only linear operations on FT, what that implies is that X of T will also be a Gaussian process.",
            "Now the problems will come when we start to put nonlinearities on F of you, and then this is no longer true.",
            "But for the moment there's only linear operations on this, so.",
            "What's a Gaussian process?",
            "So Gaussian process is?"
        ],
        [
            "Allows for inference of continuous that should, say, profiles accounting naturally for the temporal structure in the data they allow joint estimation of the M RNA concentration and the production rate, so that's this point about derivative observations given a Gaussian process, the gradient of a Gaussian process is also a Gaussian process, and it's implied, and the uncertainty in your knowledge about the function in the Gaussian process propagates through to uncertainty in the gradients.",
            "So you get a consistent framework for maximum likelihood, inference of parameters or Bayesian inference.",
            "Raptors so they outstrip MCMC for computational efficiency.",
            "Now I should say, probably Martinos recent stuff, which I haven't seen till today, is probably about the same speed as what we're doing here.",
            "I would guess.",
            "So here's all the stuff with MCMC took longer.",
            "So some references on differential equations with GPS at the bottom which aren't quite relevant to this work, but they are there, so Gaussian process."
        ],
        [
            "This is governed by amine functional convergence function.",
            "It looks like a Gaussian distribution, but you have a mean and covariance which instead of being just vector and matrix two different functions.",
            "Now the main function is often taken to be 0.",
            "In fact, in this case it will turn out to be non 0 because some of the parameters not through.",
            "But the covariance is any positive definite function, so a common one to use for this covariance is a function, so it's a function that the elements of it when you compute it from data must lead to a positive definite matrix for to be a covariance matrix."
        ],
        [
            "So, once uses the RBF kernel, So what you have with this is you express, so there's a function of time.",
            "The correlation between two outputs from a function, and that's shown in this color map here.",
            "So I've aligned up neighboring time points along the sort of X&Y axis of this grayscale and then this green is zero and the red is high, so points close together.",
            "In the key space will also be close together in F space 'cause they are thought to be strongly correlated.",
            "So you can actually sample from these Gaussian processes.",
            "So what you do is you just generate this covariance and then you sample."
        ],
        [
            "Functions from these Gaussian processes and for that covariance function I've just shown you.",
            "These are what these samples look like.",
            "Now this is going to be a model for the transcription factor concentration, so this is what we call a prior model for the transcription factor concentration.",
            "So it's sort of nice and smooth, perhaps overly smooth, in fact, so this is where the particular length scale, so it has the sorry the length scale parameters change name, but it has this length scale which is value of 10 here and it's got some sort of variance which is 1 here.",
            "So that's actually the standard deviation of these functions."
        ],
        [
            "You can change the length scale and you get longer variation or you."
        ],
        [
            "Then sort of change the variance you get shorter variation over a wider interval, so you've got some control parameters, so these become the parameters over the function instead of optimizing with respect to the function itself.",
            "You're optimizing these these parameters.",
            "Here's another kernel, so this does sort of has different characteristics.",
            "It's called the MLP kernel function."
        ],
        [
            "I'm just mentioning now, 'cause we'll use it brief."
        ],
        [
            "Later and here you can see very different characteristics on the functions with sampling.",
            "These are with particular parameters.",
            "They sort of come in from a high level, can go past, and they tend to stay at the same point.",
            "The RBF parameter functions tended to drop back towards zero.",
            "These stay at the point at which they saturate."
        ],
        [
            "Here's some other parameters.",
            "OK, so these provide."
        ],
        [
            "Probabilistic prior over functions and by combining with data we get a posterior over functions.",
            "So this is standard Gaussian process regression, so this is a toy example of regression with GPS."
        ],
        [
            "So I've got two data points and I'm going to take my prior over what these functions should be.",
            "I combine that with."
        ],
        [
            "And I get a posterior with a mean function.",
            "An error bars.",
            "I can increase number of data points and the error bars improving quality.",
            "So this is standard regression, nothing to do with."
        ],
        [
            "Well, it's to do with what we're going to do, but it's just sort of standard weight treatment with Gaussian processes.",
            "Now, by assuming that the trans."
        ],
        [
            "Friction factor concentration is Gaussian process with an RBF covariance function, it turns out."
        ],
        [
            "But we can analytically solve for X of JXJ each gene.",
            "So the reason is because this is a linear operator here.",
            "Of this form, so this is sort of.",
            "I mean, I'm not really big into operator theory and things like that, so when I look at this, I see an infinite inner product, right?",
            "There's a vector there, which is the function.",
            "And here's another vector and they're being integrated over all values up to T, so that's like an inner product.",
            "So this is like an inner product on a Gaussian random variable, which leads to another Gaussian random variable.",
            "Because it's functions, it leads to another Gaussian process.",
            "The nice thing is that it turns out we can compute the covariance analytically.",
            "Using this relationship here, so this is the explicit form.",
            "I'm not going to show you the integrals, but they are tracked."
        ],
        [
            "Well, they just take 2 pages to do or something so you can.",
            "Once you've got this you can have not just the covariance between each gene and other genes, but the covariance between the."
        ],
        [
            "And function, and that's what I'm showing here.",
            "So before we saw this covariance structure for an RBF matrix, that's up to this quadrant here.",
            "This quadrant is the latent function, the protein concentration on its own.",
            "So you remove all that, and that's what's going on up there.",
            "But now we have this relationship with these decay and sensitivity parameters that Martino talked about before.",
            "Now with high decay and high sensitivity we get strong correlations between the input protein concentration and the derived gene expression.",
            "With low concentrations, the correlation is less, so focus on that a bit, because now I'll show joint samples from this."
        ],
        [
            "Variance, So what you're seeing now?",
            "Is joint samples from this covariance?",
            "I've just shown there the blue is the protein driving the system and we are jointly sampling 2 gene expressions.",
            "This is the high sensitivity, high decay.",
            "And this is low sensitivity, low decay.",
            "This is solving the differential equation numerically to four F to prove that they actually solve this differential equation, so you can ignore the thing on the right is a sanity check, and here's a couple of other exam."
        ],
        [
            "So this is the sort of."
        ],
        [
            "Model of what's going on.",
            "But a high sensitivity as Martino sort of showed an high decay.",
            "You tend to track what's going on in the protein in the latent space loader K. You tend to sort of integrate it, so you get this sort of slow drop off."
        ],
        [
            "OK, so Martina mentioned this and we prefer lucky in some sense because Martino's paper is great because at the end of it he's got in the mathematical section everything they had to do to get this working in terms of dealing with the uncertainties on the right expression data.",
            "Coincidentally, in other work with Magnus and students of his we developed approach for estimating noise level using noise levels using probe level processing of Affymetrix microarrays.",
            "So we could take Martino kindly provided his data and we process the raw data.",
            "To get a level of noise, which I think is probably vital in what follows.",
            "But we do the noise differently from the way Martino did it.",
            "We use our own approach which I would recommend anyway, and other papers was in June for doing differential expression from this, But that's sort of unrelated to this talk.",
            "So we have this noisy process that is equal to this this K we've described, plus this diagonal matrix.",
            "So this is heteroskedastic noise."
        ],
        [
            "Now on a toy problem, am I doing time wise?",
            "To bad.",
            "OK, so results from an artificial data set, so I guessed I didn't look at martinos talk again, but I've used a very similar toy problem, so it must have stuck in my head, so we've used a known TFC and arrived 6 RNA profiles.",
            "So we made up this transcription factor concentration by three Gaussian basis functions, and then we derived what the M RNA profiles were analytically.",
            "So we then took 14 subsamples from these and we corrupted them by noise.",
            "And different variance noise at each point.",
            "But we used, we pretended we knew that variance because our system gives us an estimate of it and this data was then used to learn the decays sensitivities and basal transcription rates and infer posterior distributions over the missing transcription factor concentration.",
            "So what you're going to see here is like that regression plot we did before.",
            "These are the original M RNA concentrations.",
            "I'm corrupted by noise, and when I click well, this is our estimate.",
            "Our current estimate of the transcription factor.",
            "Concentration and the red is the true transcription factor concentration, so very similar example to martinos.",
            "When I click well, you already have one point here, but it's meaningless for this covariance 'cause it only gives you the Basel transcription rate.",
            "So as I click you'll see more points."
        ],
        [
            "And you'll see the estimate of what the transcription factor concentration should be changing.",
            "Obviously it's lower variance here because we're seeing more points early on.",
            "Now, look how noisy this is.",
            "This is very noisy, so it's not surprising that you don't get excellent estimates from this one point here.",
            "So that's the estimate.",
            "Having seen 1 gene now, in theory, if you're going to be point estimate of parameters along with these parameters you don't have.",
            "If you're doing direct, estimate the function without a functional prior.",
            "You perhaps don't have enough identifiability to do this, so.",
            "This is showing 5, so let's quickly add the other genes in, so this is where the second gene and we already get much closer 1/3 gene.",
            "Fourth team and then 15 so we're getting reasonably good estimates of the truth.",
            "These here I haven't been estimating decay and basal transcription rates and sensitivities.",
            "Those I've taken from the learning we did before, so any mismatch is likely to do with miss learning of those those parameters, and it's not going to improve much on the inference here, so there's a good stylistically about the same, but it tends to wiggle where there's no wiggle, which is a characteristic of this covariance.",
            "So this is now martinos data."
        ],
        [
            "And all I'm going to do is compare with his results to show that we get, broadly speaking, the same thing."
        ],
        [
            "So these are point wise I should have included."
        ],
        [
            "These error bars, but these are pointwise estimates that he was showing from his Markov chain Monte Carlo, measured from his paper with a ruler.",
            "So there might not be 100% accurate.",
            "Interestingly, we get this wiggle here, which we're not really certain that is in the data were quite good on the point estimates we've got quite, you know that everything is within the error bars basically.",
            "So basically two methods seem to lead to something which is fairly consistent.",
            "Um?",
            "This is the estimate of the basal transcription rates.",
            "Now render, Martino said that he constrains the first point at zero.",
            "We sort of didn't do that, not 'cause you can't because we were put in the paper together and just didn't do it.",
            "So where the effect of that is, we tend to be much more higher discrepancy on the base."
        ],
        [
            "Transcription rates 'cause there's Martino said they're driven by fixing this first .0.",
            "We're close to 0, so that's why we're sort of similar.",
            "Here's the."
        ],
        [
            "So Tivities our results in black compared with his results in white."
        ],
        [
            "And here's the decays and as you can see they match very well, so we're using all his tricks to do this.",
            "The tricks he talked about earlier fixing these sensitivities and using learning about the decays.",
            "And if you don't the same thing happens for us as happened for him.",
            "You get non identifiability problems."
        ],
        [
            "So there's some oscillatory behavior which might be an artifact of this covariance which wiggles around a lot, but the results are in good accordance.",
            "And there's these two differences.",
            "Whether inconsistent, it's probably down to these two things, so this was 13 minutes to produce, but Martina was using 10,000,000 iterations of Monte Carlo.",
            "But of course now he's doing something which is much faster than that, so he's probably I would guess about the same time."
        ],
        [
            "So that's not very realistic response.",
            "The linear response, so I'm just going to flick through some different response models.",
            "You can no longer do this analytically if we put a nonlinear."
        ],
        [
            "Arity around here because this is no longer a linear operator on this Gaussian process, but this is clearly the right thing to do, because so far I've been showing you concentrations which go negative.",
            "That's not really sensible for concentration, so the first nonlinearity we did constrains it positive.",
            "So Oh well, sorry, I skipped something there.",
            "So the first thing we did sorry was a sanity check.",
            "We reran the result."
        ],
        [
            "Using our approximation techniques, so the approximation technique I haven't gone into details of, but it's basically a Hessian Laplace approximation in the functional space.",
            "So it should be exact in the linear case, but the nice thing is we can plug in any covariance kernel, so this is the so-called MLP kernel and you get a reduced oscillatory behavior with this so you know it's indicating that this might be better, but notice the log likelihood is lower than this log likelihood, so we can actually estimate the quality of these models, and that's a higher likelihood model, somewhat disappointingly.",
            "So for nonlinear responses."
        ],
        [
            "So we looked at an exponential response model.",
            "This log of 1 + y to the F, which is like 0 in the negative half plane, but then goes up in a linear way and then this sigmoidal, Michaelis, menten type a response model.",
            "And we can infer."
        ],
        [
            "Solution for the function plus error bars based on the Hessian, so these are similar sort of error bars to the ones Martino talked about, but they are in a continuous way across the function.",
            "So this is with an exponential constraint, so now."
        ],
        [
            "Thing goes negative, this is with this log plus 1 + y to the F, so this less distortion in this regime.",
            "So in fact this is a very accurate approximation in this regime, so it's probably slightly better model, and in fact it's the highest estimate of the log likelihood we have this model here."
        ],
        [
            "Um?",
            "This is the sigmoidal response, so this is like a Michaelis Menten type kinetics, so you get this artificial fudge factor of three which is taken from noticing that's three.",
            "If you don't have that there, you sort of saturate before you get that, so these are in some sense fully rigorously done, but there just to give an impression that you can do it.",
            "These are the sort of responses you get.",
            "OK, so."
        ],
        [
            "So we've described how GPS can use in the dynamics of a single little regulatory motif network motif, and our approach has some advantages over standard parametric approaches, so there's no need to restrict the inference to the observed time points.",
            "We get the temporal continuity in Ferd functions occurs in a natural way.",
            "GPS also allow us to handle the uncertainty in a really natural way that plugs into the framework and it's consistently uncertainty about the derivatives and the uncertainty about the actual observations is sort of handling the consistent way, so MCMC is computationally expensive, but martinos showed us he sold that already, so you can ignore that one.",
            "All the code for doing this is.",
            "Online, So what next?",
            "What are we interested in now?",
            "So this is very."
        ],
        [
            "Simple modeling situation.",
            "We're ignoring transcriptional delays.",
            "They can be included quite easily within this framework.",
            "We only have a single transcription factor.",
            "We're interested in regulatory pathways of more jeans.",
            "All these issues can be dealt with within the general framework we described that you start to have to approximate more and you need to overcome those difficulties as well.",
            "Thanks Martina for the data and BBS RC.",
            "That's it.",
            "So it seems like like your model and the previous one as well depends on the idea that you know there is a single transcription factor controlling those jeans that you have identified the genes, yeah?",
            "So in the linear case, we basically sort of written it down and we believe that's no problem, but the linear case is perhaps less interesting, so in the linear case it depends.",
            "Actually, if these transcription factors are themselves independent, so if their Co regulated by something else, then you have to be a bit careful about what's going on, because this sort of most trivial thing you can do is have independence, and then the sort of framework just drops out analytically.",
            "In the nonlinear case, you're back to doing these approximations that I didn't describe, but it's all possible.",
            "I think the interesting question is how do you resolve the parameters in these sort of things?",
            "You get more lack of identifiability, and this is sort of one of the reasons we're starting to hook up with Mark Jeremy and his group.",
            "We saw Ben called Airheads talk earlier about Markov chain Monte Carlo.",
            "In these sort of systems.",
            "Because this takes out one component that their sampling over at the moment, which is the function.",
            "But there's still gonna be the problems.",
            "Alpha parameter identification and I think that's very important to do these larger networks.",
            "I think mentally we think maybe something like 6 transcription factors might be plausable, but we haven't thought larger than that.",
            "The chair has a question.",
            "It looks like so many.",
            "So diagnostics to see this any covariance functions appropriate from the actual data, like in spatial statistics.",
            "Yeah, I think that's that we've not done that.",
            "Then that's a sensible thing to do.",
            "Really so far the first covariance function we did was the RBF 'cause it's analytic and we can show every element of that covariance matrix for overly smooth and things like there's a rational quadratic, which is a good.",
            "There's various other ones which all applicable to the framework.",
            "We can do them all, but it's also there also stationary RBF's, and I'm not sure stationarity is right for this model so we haven't sort of overly explored that, but it's definitely something we should be doing.",
            "OK, so.",
            "Yes.",
            "I was thinking the other speakers, not me."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean, there's been a bit of a theme over the last three talks, which I guess was intentional.",
                    "label": 0
                },
                {
                    "sent": "Being the last talk I'm going to try and keep the equations to the minimum and.",
                    "label": 0
                },
                {
                    "sent": "Try and focus on the concepts, particularly because I think some of the concepts are introduced maybe nude some in the audience.",
                    "label": 0
                },
                {
                    "sent": "I know there's definitely some that are familiar with them, so the talks entitled modeling transcription regulation with Gaussian processes and the really nice thing about.",
                    "label": 1
                },
                {
                    "sent": "This is that the application I can talk about very quickly is basically our work.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Completely inspired by me seeing a talk by Martina Barranco at Sheffield a couple of years ago, so it's take basically working with his data set, but with a different approach to doing it.",
                    "label": 0
                },
                {
                    "sent": "So he had a linear response system, and that's what I'll focus on mainly to get the intuitions across, and then I'll briefly wave my hands a bit and say when you can do some of this with nonlinear response models and show you some results without going into too much details.",
                    "label": 0
                },
                {
                    "sent": "So well, all the slides will be available online, but the source code is.",
                    "label": 0
                },
                {
                    "sent": "This is very lonely.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all the demos that you see here you can download if you write in MATLAB, you can try them on your own.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If they happen to be in a working state.",
                    "label": 0
                },
                {
                    "sent": "So, as Martinez talked about, and we've just seen as well, many interaction networks have latent functions, and so they talk about this as a hidden variable dynamical model, and it would perhaps be clearer if we refer to this as an approach to solving the problem of a hidden variable dynamical model.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is you doing many latent variable problems.",
                    "label": 0
                },
                {
                    "sent": "You assume a prior distribution, so we're going to assume what's called a Gaussian process prior distribution for this latent function.",
                    "label": 1
                },
                {
                    "sent": "Now this is a smooth continuous function overtime and a Gaussian process is a probabilistic model for functions.",
                    "label": 0
                },
                {
                    "sent": "There's a couple of references there.",
                    "label": 0
                },
                {
                    "sent": "So our approach is to take the differential equation model for the system that Martino talked about and derive a Gaussian process covariance jointly for the observed and latent function.",
                    "label": 1
                },
                {
                    "sent": "So we get a Gaussian covariance which relates the observed measurements to the latent measurements.",
                    "label": 0
                },
                {
                    "sent": "Now, by doing this, we sort of finesse some of the problems of derivative measurements, that sort of drops out.",
                    "label": 0
                },
                {
                    "sent": "We never have to worry about estimating the derivatives, it's all part of the framework, because the derivative of Gaussian process is also a Gaussian process, and.",
                    "label": 0
                },
                {
                    "sent": "This sort of joint covariance handles all that for us, so we only work with the direct observation of the functions.",
                    "label": 0
                },
                {
                    "sent": "Then we maximize the likelihood with respect to the parameters of this covariance, and these parameters will turn out to be those decays and sensitivities, and basal transcription rates that Martino talked about earlier.",
                    "label": 0
                },
                {
                    "sent": "And we also do hybrid Monte Carlo, two estimators.",
                    "label": 0
                },
                {
                    "sent": "Martino said it's important to get uncertainty estimates, so we also do hybrid Monte Carlo on that.",
                    "label": 0
                },
                {
                    "sent": "So I think I'll skip that, so this was there.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asian Martino had I don't have to talk too much about it 'cause he's already talked about it.",
                    "label": 0
                },
                {
                    "sent": "We've got this.",
                    "label": 0
                },
                {
                    "sent": "Some genes which we sort of known to be targets of a given transcription factor, in this case P53, or in the case Martino talked about P53 and the gradient of the production rate of the gene is given by basil transcription rate plus some sensitivity times the concentration of P 53 minus decay rate.",
                    "label": 1
                },
                {
                    "sent": "There so.",
                    "label": 0
                },
                {
                    "sent": "In this case, we've got a linear dependence on the transcription factor, and that's something we'll relax later, but I won't really talk about the details of relaxing to that.",
                    "label": 0
                },
                {
                    "sent": "So you can solve this equation.",
                    "label": 0
                },
                {
                    "sent": "Think.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tina probably had certainly has that in this paper that this paper is this.",
                    "label": 0
                },
                {
                    "sent": "Each gene expression is given by something of this form, where this is a sort of.",
                    "label": 0
                },
                {
                    "sent": "The plus style convolution between the function and this exponential here.",
                    "label": 0
                },
                {
                    "sent": "Now what's interesting about Gaussian processes?",
                    "label": 0
                },
                {
                    "sent": "Is there like Gaussian distributions?",
                    "label": 0
                },
                {
                    "sent": "So if we model F of T as a Gaussian process, then this thing here only involves linear operations on.",
                    "label": 1
                },
                {
                    "sent": "To get XLT it only involves linear operations on F of T, so because there's only linear operations on FT, what that implies is that X of T will also be a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Now the problems will come when we start to put nonlinearities on F of you, and then this is no longer true.",
                    "label": 0
                },
                {
                    "sent": "But for the moment there's only linear operations on this, so.",
                    "label": 0
                },
                {
                    "sent": "What's a Gaussian process?",
                    "label": 0
                },
                {
                    "sent": "So Gaussian process is?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Allows for inference of continuous that should, say, profiles accounting naturally for the temporal structure in the data they allow joint estimation of the M RNA concentration and the production rate, so that's this point about derivative observations given a Gaussian process, the gradient of a Gaussian process is also a Gaussian process, and it's implied, and the uncertainty in your knowledge about the function in the Gaussian process propagates through to uncertainty in the gradients.",
                    "label": 0
                },
                {
                    "sent": "So you get a consistent framework for maximum likelihood, inference of parameters or Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "Raptors so they outstrip MCMC for computational efficiency.",
                    "label": 0
                },
                {
                    "sent": "Now I should say, probably Martinos recent stuff, which I haven't seen till today, is probably about the same speed as what we're doing here.",
                    "label": 0
                },
                {
                    "sent": "I would guess.",
                    "label": 0
                },
                {
                    "sent": "So here's all the stuff with MCMC took longer.",
                    "label": 0
                },
                {
                    "sent": "So some references on differential equations with GPS at the bottom which aren't quite relevant to this work, but they are there, so Gaussian process.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is governed by amine functional convergence function.",
                    "label": 1
                },
                {
                    "sent": "It looks like a Gaussian distribution, but you have a mean and covariance which instead of being just vector and matrix two different functions.",
                    "label": 0
                },
                {
                    "sent": "Now the main function is often taken to be 0.",
                    "label": 1
                },
                {
                    "sent": "In fact, in this case it will turn out to be non 0 because some of the parameters not through.",
                    "label": 0
                },
                {
                    "sent": "But the covariance is any positive definite function, so a common one to use for this covariance is a function, so it's a function that the elements of it when you compute it from data must lead to a positive definite matrix for to be a covariance matrix.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, once uses the RBF kernel, So what you have with this is you express, so there's a function of time.",
                    "label": 0
                },
                {
                    "sent": "The correlation between two outputs from a function, and that's shown in this color map here.",
                    "label": 0
                },
                {
                    "sent": "So I've aligned up neighboring time points along the sort of X&Y axis of this grayscale and then this green is zero and the red is high, so points close together.",
                    "label": 0
                },
                {
                    "sent": "In the key space will also be close together in F space 'cause they are thought to be strongly correlated.",
                    "label": 0
                },
                {
                    "sent": "So you can actually sample from these Gaussian processes.",
                    "label": 1
                },
                {
                    "sent": "So what you do is you just generate this covariance and then you sample.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Functions from these Gaussian processes and for that covariance function I've just shown you.",
                    "label": 0
                },
                {
                    "sent": "These are what these samples look like.",
                    "label": 0
                },
                {
                    "sent": "Now this is going to be a model for the transcription factor concentration, so this is what we call a prior model for the transcription factor concentration.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of nice and smooth, perhaps overly smooth, in fact, so this is where the particular length scale, so it has the sorry the length scale parameters change name, but it has this length scale which is value of 10 here and it's got some sort of variance which is 1 here.",
                    "label": 0
                },
                {
                    "sent": "So that's actually the standard deviation of these functions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can change the length scale and you get longer variation or you.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then sort of change the variance you get shorter variation over a wider interval, so you've got some control parameters, so these become the parameters over the function instead of optimizing with respect to the function itself.",
                    "label": 0
                },
                {
                    "sent": "You're optimizing these these parameters.",
                    "label": 0
                },
                {
                    "sent": "Here's another kernel, so this does sort of has different characteristics.",
                    "label": 0
                },
                {
                    "sent": "It's called the MLP kernel function.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just mentioning now, 'cause we'll use it brief.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later and here you can see very different characteristics on the functions with sampling.",
                    "label": 0
                },
                {
                    "sent": "These are with particular parameters.",
                    "label": 0
                },
                {
                    "sent": "They sort of come in from a high level, can go past, and they tend to stay at the same point.",
                    "label": 0
                },
                {
                    "sent": "The RBF parameter functions tended to drop back towards zero.",
                    "label": 0
                },
                {
                    "sent": "These stay at the point at which they saturate.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's some other parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so these provide.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probabilistic prior over functions and by combining with data we get a posterior over functions.",
                    "label": 0
                },
                {
                    "sent": "So this is standard Gaussian process regression, so this is a toy example of regression with GPS.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I've got two data points and I'm going to take my prior over what these functions should be.",
                    "label": 0
                },
                {
                    "sent": "I combine that with.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I get a posterior with a mean function.",
                    "label": 0
                },
                {
                    "sent": "An error bars.",
                    "label": 0
                },
                {
                    "sent": "I can increase number of data points and the error bars improving quality.",
                    "label": 0
                },
                {
                    "sent": "So this is standard regression, nothing to do with.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it's to do with what we're going to do, but it's just sort of standard weight treatment with Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Now, by assuming that the trans.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Friction factor concentration is Gaussian process with an RBF covariance function, it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we can analytically solve for X of JXJ each gene.",
                    "label": 0
                },
                {
                    "sent": "So the reason is because this is a linear operator here.",
                    "label": 1
                },
                {
                    "sent": "Of this form, so this is sort of.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not really big into operator theory and things like that, so when I look at this, I see an infinite inner product, right?",
                    "label": 0
                },
                {
                    "sent": "There's a vector there, which is the function.",
                    "label": 0
                },
                {
                    "sent": "And here's another vector and they're being integrated over all values up to T, so that's like an inner product.",
                    "label": 0
                },
                {
                    "sent": "So this is like an inner product on a Gaussian random variable, which leads to another Gaussian random variable.",
                    "label": 0
                },
                {
                    "sent": "Because it's functions, it leads to another Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "The nice thing is that it turns out we can compute the covariance analytically.",
                    "label": 0
                },
                {
                    "sent": "Using this relationship here, so this is the explicit form.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to show you the integrals, but they are tracked.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, they just take 2 pages to do or something so you can.",
                    "label": 0
                },
                {
                    "sent": "Once you've got this you can have not just the covariance between each gene and other genes, but the covariance between the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And function, and that's what I'm showing here.",
                    "label": 0
                },
                {
                    "sent": "So before we saw this covariance structure for an RBF matrix, that's up to this quadrant here.",
                    "label": 0
                },
                {
                    "sent": "This quadrant is the latent function, the protein concentration on its own.",
                    "label": 0
                },
                {
                    "sent": "So you remove all that, and that's what's going on up there.",
                    "label": 0
                },
                {
                    "sent": "But now we have this relationship with these decay and sensitivity parameters that Martino talked about before.",
                    "label": 0
                },
                {
                    "sent": "Now with high decay and high sensitivity we get strong correlations between the input protein concentration and the derived gene expression.",
                    "label": 0
                },
                {
                    "sent": "With low concentrations, the correlation is less, so focus on that a bit, because now I'll show joint samples from this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Variance, So what you're seeing now?",
                    "label": 0
                },
                {
                    "sent": "Is joint samples from this covariance?",
                    "label": 1
                },
                {
                    "sent": "I've just shown there the blue is the protein driving the system and we are jointly sampling 2 gene expressions.",
                    "label": 0
                },
                {
                    "sent": "This is the high sensitivity, high decay.",
                    "label": 0
                },
                {
                    "sent": "And this is low sensitivity, low decay.",
                    "label": 1
                },
                {
                    "sent": "This is solving the differential equation numerically to four F to prove that they actually solve this differential equation, so you can ignore the thing on the right is a sanity check, and here's a couple of other exam.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the sort of.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model of what's going on.",
                    "label": 0
                },
                {
                    "sent": "But a high sensitivity as Martino sort of showed an high decay.",
                    "label": 0
                },
                {
                    "sent": "You tend to track what's going on in the protein in the latent space loader K. You tend to sort of integrate it, so you get this sort of slow drop off.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so Martina mentioned this and we prefer lucky in some sense because Martino's paper is great because at the end of it he's got in the mathematical section everything they had to do to get this working in terms of dealing with the uncertainties on the right expression data.",
                    "label": 0
                },
                {
                    "sent": "Coincidentally, in other work with Magnus and students of his we developed approach for estimating noise level using noise levels using probe level processing of Affymetrix microarrays.",
                    "label": 1
                },
                {
                    "sent": "So we could take Martino kindly provided his data and we process the raw data.",
                    "label": 0
                },
                {
                    "sent": "To get a level of noise, which I think is probably vital in what follows.",
                    "label": 0
                },
                {
                    "sent": "But we do the noise differently from the way Martino did it.",
                    "label": 0
                },
                {
                    "sent": "We use our own approach which I would recommend anyway, and other papers was in June for doing differential expression from this, But that's sort of unrelated to this talk.",
                    "label": 0
                },
                {
                    "sent": "So we have this noisy process that is equal to this this K we've described, plus this diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is heteroskedastic noise.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now on a toy problem, am I doing time wise?",
                    "label": 0
                },
                {
                    "sent": "To bad.",
                    "label": 0
                },
                {
                    "sent": "OK, so results from an artificial data set, so I guessed I didn't look at martinos talk again, but I've used a very similar toy problem, so it must have stuck in my head, so we've used a known TFC and arrived 6 RNA profiles.",
                    "label": 1
                },
                {
                    "sent": "So we made up this transcription factor concentration by three Gaussian basis functions, and then we derived what the M RNA profiles were analytically.",
                    "label": 0
                },
                {
                    "sent": "So we then took 14 subsamples from these and we corrupted them by noise.",
                    "label": 0
                },
                {
                    "sent": "And different variance noise at each point.",
                    "label": 0
                },
                {
                    "sent": "But we used, we pretended we knew that variance because our system gives us an estimate of it and this data was then used to learn the decays sensitivities and basal transcription rates and infer posterior distributions over the missing transcription factor concentration.",
                    "label": 1
                },
                {
                    "sent": "So what you're going to see here is like that regression plot we did before.",
                    "label": 0
                },
                {
                    "sent": "These are the original M RNA concentrations.",
                    "label": 0
                },
                {
                    "sent": "I'm corrupted by noise, and when I click well, this is our estimate.",
                    "label": 0
                },
                {
                    "sent": "Our current estimate of the transcription factor.",
                    "label": 0
                },
                {
                    "sent": "Concentration and the red is the true transcription factor concentration, so very similar example to martinos.",
                    "label": 0
                },
                {
                    "sent": "When I click well, you already have one point here, but it's meaningless for this covariance 'cause it only gives you the Basel transcription rate.",
                    "label": 0
                },
                {
                    "sent": "So as I click you'll see more points.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you'll see the estimate of what the transcription factor concentration should be changing.",
                    "label": 0
                },
                {
                    "sent": "Obviously it's lower variance here because we're seeing more points early on.",
                    "label": 0
                },
                {
                    "sent": "Now, look how noisy this is.",
                    "label": 0
                },
                {
                    "sent": "This is very noisy, so it's not surprising that you don't get excellent estimates from this one point here.",
                    "label": 0
                },
                {
                    "sent": "So that's the estimate.",
                    "label": 0
                },
                {
                    "sent": "Having seen 1 gene now, in theory, if you're going to be point estimate of parameters along with these parameters you don't have.",
                    "label": 0
                },
                {
                    "sent": "If you're doing direct, estimate the function without a functional prior.",
                    "label": 0
                },
                {
                    "sent": "You perhaps don't have enough identifiability to do this, so.",
                    "label": 0
                },
                {
                    "sent": "This is showing 5, so let's quickly add the other genes in, so this is where the second gene and we already get much closer 1/3 gene.",
                    "label": 0
                },
                {
                    "sent": "Fourth team and then 15 so we're getting reasonably good estimates of the truth.",
                    "label": 0
                },
                {
                    "sent": "These here I haven't been estimating decay and basal transcription rates and sensitivities.",
                    "label": 0
                },
                {
                    "sent": "Those I've taken from the learning we did before, so any mismatch is likely to do with miss learning of those those parameters, and it's not going to improve much on the inference here, so there's a good stylistically about the same, but it tends to wiggle where there's no wiggle, which is a characteristic of this covariance.",
                    "label": 0
                },
                {
                    "sent": "So this is now martinos data.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all I'm going to do is compare with his results to show that we get, broadly speaking, the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are point wise I should have included.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These error bars, but these are pointwise estimates that he was showing from his Markov chain Monte Carlo, measured from his paper with a ruler.",
                    "label": 0
                },
                {
                    "sent": "So there might not be 100% accurate.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, we get this wiggle here, which we're not really certain that is in the data were quite good on the point estimates we've got quite, you know that everything is within the error bars basically.",
                    "label": 0
                },
                {
                    "sent": "So basically two methods seem to lead to something which is fairly consistent.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is the estimate of the basal transcription rates.",
                    "label": 0
                },
                {
                    "sent": "Now render, Martino said that he constrains the first point at zero.",
                    "label": 0
                },
                {
                    "sent": "We sort of didn't do that, not 'cause you can't because we were put in the paper together and just didn't do it.",
                    "label": 0
                },
                {
                    "sent": "So where the effect of that is, we tend to be much more higher discrepancy on the base.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transcription rates 'cause there's Martino said they're driven by fixing this first .0.",
                    "label": 0
                },
                {
                    "sent": "We're close to 0, so that's why we're sort of similar.",
                    "label": 0
                },
                {
                    "sent": "Here's the.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Tivities our results in black compared with his results in white.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's the decays and as you can see they match very well, so we're using all his tricks to do this.",
                    "label": 0
                },
                {
                    "sent": "The tricks he talked about earlier fixing these sensitivities and using learning about the decays.",
                    "label": 0
                },
                {
                    "sent": "And if you don't the same thing happens for us as happened for him.",
                    "label": 0
                },
                {
                    "sent": "You get non identifiability problems.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's some oscillatory behavior which might be an artifact of this covariance which wiggles around a lot, but the results are in good accordance.",
                    "label": 1
                },
                {
                    "sent": "And there's these two differences.",
                    "label": 1
                },
                {
                    "sent": "Whether inconsistent, it's probably down to these two things, so this was 13 minutes to produce, but Martina was using 10,000,000 iterations of Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "But of course now he's doing something which is much faster than that, so he's probably I would guess about the same time.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's not very realistic response.",
                    "label": 0
                },
                {
                    "sent": "The linear response, so I'm just going to flick through some different response models.",
                    "label": 0
                },
                {
                    "sent": "You can no longer do this analytically if we put a nonlinear.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arity around here because this is no longer a linear operator on this Gaussian process, but this is clearly the right thing to do, because so far I've been showing you concentrations which go negative.",
                    "label": 0
                },
                {
                    "sent": "That's not really sensible for concentration, so the first nonlinearity we did constrains it positive.",
                    "label": 0
                },
                {
                    "sent": "So Oh well, sorry, I skipped something there.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we did sorry was a sanity check.",
                    "label": 0
                },
                {
                    "sent": "We reran the result.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using our approximation techniques, so the approximation technique I haven't gone into details of, but it's basically a Hessian Laplace approximation in the functional space.",
                    "label": 0
                },
                {
                    "sent": "So it should be exact in the linear case, but the nice thing is we can plug in any covariance kernel, so this is the so-called MLP kernel and you get a reduced oscillatory behavior with this so you know it's indicating that this might be better, but notice the log likelihood is lower than this log likelihood, so we can actually estimate the quality of these models, and that's a higher likelihood model, somewhat disappointingly.",
                    "label": 0
                },
                {
                    "sent": "So for nonlinear responses.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we looked at an exponential response model.",
                    "label": 1
                },
                {
                    "sent": "This log of 1 + y to the F, which is like 0 in the negative half plane, but then goes up in a linear way and then this sigmoidal, Michaelis, menten type a response model.",
                    "label": 0
                },
                {
                    "sent": "And we can infer.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution for the function plus error bars based on the Hessian, so these are similar sort of error bars to the ones Martino talked about, but they are in a continuous way across the function.",
                    "label": 0
                },
                {
                    "sent": "So this is with an exponential constraint, so now.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing goes negative, this is with this log plus 1 + y to the F, so this less distortion in this regime.",
                    "label": 0
                },
                {
                    "sent": "So in fact this is a very accurate approximation in this regime, so it's probably slightly better model, and in fact it's the highest estimate of the log likelihood we have this model here.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is the sigmoidal response, so this is like a Michaelis Menten type kinetics, so you get this artificial fudge factor of three which is taken from noticing that's three.",
                    "label": 0
                },
                {
                    "sent": "If you don't have that there, you sort of saturate before you get that, so these are in some sense fully rigorously done, but there just to give an impression that you can do it.",
                    "label": 0
                },
                {
                    "sent": "These are the sort of responses you get.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've described how GPS can use in the dynamics of a single little regulatory motif network motif, and our approach has some advantages over standard parametric approaches, so there's no need to restrict the inference to the observed time points.",
                    "label": 1
                },
                {
                    "sent": "We get the temporal continuity in Ferd functions occurs in a natural way.",
                    "label": 0
                },
                {
                    "sent": "GPS also allow us to handle the uncertainty in a really natural way that plugs into the framework and it's consistently uncertainty about the derivatives and the uncertainty about the actual observations is sort of handling the consistent way, so MCMC is computationally expensive, but martinos showed us he sold that already, so you can ignore that one.",
                    "label": 0
                },
                {
                    "sent": "All the code for doing this is.",
                    "label": 0
                },
                {
                    "sent": "Online, So what next?",
                    "label": 0
                },
                {
                    "sent": "What are we interested in now?",
                    "label": 0
                },
                {
                    "sent": "So this is very.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple modeling situation.",
                    "label": 0
                },
                {
                    "sent": "We're ignoring transcriptional delays.",
                    "label": 0
                },
                {
                    "sent": "They can be included quite easily within this framework.",
                    "label": 0
                },
                {
                    "sent": "We only have a single transcription factor.",
                    "label": 1
                },
                {
                    "sent": "We're interested in regulatory pathways of more jeans.",
                    "label": 0
                },
                {
                    "sent": "All these issues can be dealt with within the general framework we described that you start to have to approximate more and you need to overcome those difficulties as well.",
                    "label": 1
                },
                {
                    "sent": "Thanks Martina for the data and BBS RC.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "So it seems like like your model and the previous one as well depends on the idea that you know there is a single transcription factor controlling those jeans that you have identified the genes, yeah?",
                    "label": 0
                },
                {
                    "sent": "So in the linear case, we basically sort of written it down and we believe that's no problem, but the linear case is perhaps less interesting, so in the linear case it depends.",
                    "label": 0
                },
                {
                    "sent": "Actually, if these transcription factors are themselves independent, so if their Co regulated by something else, then you have to be a bit careful about what's going on, because this sort of most trivial thing you can do is have independence, and then the sort of framework just drops out analytically.",
                    "label": 0
                },
                {
                    "sent": "In the nonlinear case, you're back to doing these approximations that I didn't describe, but it's all possible.",
                    "label": 0
                },
                {
                    "sent": "I think the interesting question is how do you resolve the parameters in these sort of things?",
                    "label": 0
                },
                {
                    "sent": "You get more lack of identifiability, and this is sort of one of the reasons we're starting to hook up with Mark Jeremy and his group.",
                    "label": 0
                },
                {
                    "sent": "We saw Ben called Airheads talk earlier about Markov chain Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "In these sort of systems.",
                    "label": 0
                },
                {
                    "sent": "Because this takes out one component that their sampling over at the moment, which is the function.",
                    "label": 0
                },
                {
                    "sent": "But there's still gonna be the problems.",
                    "label": 0
                },
                {
                    "sent": "Alpha parameter identification and I think that's very important to do these larger networks.",
                    "label": 0
                },
                {
                    "sent": "I think mentally we think maybe something like 6 transcription factors might be plausable, but we haven't thought larger than that.",
                    "label": 0
                },
                {
                    "sent": "The chair has a question.",
                    "label": 0
                },
                {
                    "sent": "It looks like so many.",
                    "label": 0
                },
                {
                    "sent": "So diagnostics to see this any covariance functions appropriate from the actual data, like in spatial statistics.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's that we've not done that.",
                    "label": 0
                },
                {
                    "sent": "Then that's a sensible thing to do.",
                    "label": 0
                },
                {
                    "sent": "Really so far the first covariance function we did was the RBF 'cause it's analytic and we can show every element of that covariance matrix for overly smooth and things like there's a rational quadratic, which is a good.",
                    "label": 0
                },
                {
                    "sent": "There's various other ones which all applicable to the framework.",
                    "label": 0
                },
                {
                    "sent": "We can do them all, but it's also there also stationary RBF's, and I'm not sure stationarity is right for this model so we haven't sort of overly explored that, but it's definitely something we should be doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I was thinking the other speakers, not me.",
                    "label": 0
                }
            ]
        }
    }
}