{
    "id": "52yvfelmumcmu73cnh4q7tevf5qsngln",
    "title": "The Sensitivity of Latent Dirichlet Allocation for Information Retrieval",
    "info": {
        "author": [
            "Laurence A. F. Park, The University of Melbourne"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning->Dirichlet Processes"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_park_sldair/",
    "segmentation": [
        [
            "OK, hello everyone and hello everyone out there.",
            "So my presentation today is going to be on the sensitivity sensitivity of latent directly allocation for information retrieval.",
            "This work done by Amy Lawrence and rail Kotagiri.",
            "We call him real because it's too hard to pronounce the whole name there.",
            "From the University of Melbourne in Australia."
        ],
        [
            "So, just briefly go over what I'm going to present today.",
            "Anne.",
            "So the use of topic models adds to the precision of document retrieval.",
            "So when.",
            "Topics are used underneath.",
            "They used to find relationships between terms and documents, and so by knowing relationships between words or terms, then we're able to find more precise documents."
        ],
        [
            "So literally, allocation allows us to model documents and words with the model documents with the drizly distributed topics."
        ],
        [
            "So this if we use this juice like distribution then the models depend on this richly parameter.",
            "So initial versions of light and richlite allocation fitted this parameter to the document set that was used.",
            "And an hour later versions just simply pick a parameter.",
            "And so, in this presentation today, we're going to look and see if that actually makes a difference fitting the parameter."
        ],
        [
            "So the document said, or just picking a parameter.",
            "And.",
            "I'll spoil it for you and tell you that it doesn't."
        ],
        [
            "So we're going to look at Bruce briefly.",
            "Go over topic models for documents.",
            "We've seen this before in the first presentation, then briefly look at late and richly allocation.",
            "And then had to use the topic models to perform information retrieval.",
            "And finally we will look at experiments where we vary the display parameter to examine the retrieval results.",
            "So."
        ],
        [
            "1st just overview of topic based document models."
        ],
        [
            "So when we when we have a document, a piece of text, it's just a string of words that go from the first word in the document to the last word in the document.",
            "Um?",
            "So.",
            "To model the documents, we have to understand the purpose behind the document, what it was intended for.",
            "And we know that these documents are written to communicate knowledge.",
            "If we could simply get the contents of our brain and transfer it to someone else is brand, that would be ideal and we get complete understanding of what the person intended.",
            "But we can't do that.",
            "So we take what's in our head and convert it into text form, write it out and then give it to people so they can read.",
            "And hopefully that transformation shows what we were thinking.",
            "It doesn't all the time, but we do our best to do that.",
            "So by modeling documents, we're attempting to try and reproduce this process to get rid of to model that transformation from our our knowledge to this text data."
        ],
        [
            "So to do this, we gotta first think of.",
            "How are documents created so when I go to write a document?",
            "First I start off with this idea of what I want to write, 'cause if I don't have an idea then.",
            "I'll go off and do something else.",
            "Um?",
            "And so this in this naive document creation model, the author has had an idea, and then they've simply pick terms out.",
            "From the the idea and put them into a document, and so these terms are all independent and they have no relationship at all.",
            "And it's just that they have to use the exact terms.",
            "The exact words for the document.",
            "If there's any different words, then the document would have a different meaning.",
            "So for example, if I write a document containing baby and clothes, then it would have no relationship to any of the words.",
            "Documents have infant lawsuits in it, even though they are similar.",
            "So that's a very naive model."
        ],
        [
            "Topic based models.",
            "Take it to the next step, how?",
            "We have this idea.",
            "And then, rather than picking words straight away, we think of topics to fill in the document with.",
            "So we think of the first topic to put at the top and then the second topic.",
            "And then once we we know what topics we want to write about.",
            "We can take words from those topics.",
            "So there's all these here.",
            "Term 4, five and one or related to topic one, and so they have similar meaning.",
            "So we can pick any of those terms and put them in a document.",
            "And then for topic two, we can pick any of these three terms and put them in our document.",
            "So as long as this document has terms from the correct topics.",
            "Then it will be the same document the related document.",
            "So by constructing these, you're thinking this clusters were clustering the terms together in these topic forms.",
            "So."
        ],
        [
            "So look at light and richly allocation."
        ],
        [
            "Attempts to build these topics.",
            "Let's just play allocation is similar to probabilistic latent semantic analysis that we saw in the first presentation.",
            "How we try and compute these topics in the background just based on a document collection.",
            "So we have other words in the documents.",
            "Um?",
            "The difference is one of the major differences is that it uses Additionally distribution to model how the topics are sampled through the document.",
            "So."
        ],
        [
            "Look, we go here and I'll.",
            "I found this nice Wikipedia page.",
            "Everyone loves Wikipedia.",
            "Here's an example of a 3 dimensional display distribution.",
            "So it's got the parameters down here.",
            "And they cut off.",
            "So in this case.",
            "This is the three the space of the distribution, and we can sample from any one of these points and the places.",
            "That have a higher value out, more likely, more highly probable, being sampled.",
            "So you can see in this case that all the points in the X.",
            "More likely to be sampled then the the ones nearby Enzed.",
            "Anne.",
            "So here that we've got this.",
            "The parameter for this traditional distribution determines the shape of this curve.",
            "So here the parameter.",
            "There's three parameters and each one is different, giving it a slightly bent looking shape.",
            "But instead, if we if we set all the parameters to the same value.",
            "It becomes more symmetric, so here's this.",
            "This parameter all set to the same value and you can see the number changing at the top going from now it's above 1.",
            "So as it goes above one, it gets this curved.",
            "Um?",
            "An essence below 0 at dips in, but when it gets to one it's totally flat.",
            "So you can see that failure is 1.",
            "It means that we can pick any position in this space.",
            "It is equally likely, but as as the value is smaller than the points at the edges are more equal likely.",
            "And so a small parameter implies that we're more likely to pick only a few talk topics per document, because they're in the corners.",
            "But a large value implies that there's going to be many topics for document because it's got this high probability in the center.",
            "And a value of 1 means that any position is likely, which is similar to what happens in probabilistic latent semantic analysis."
        ],
        [
            "So the case where the parameters in the display distribution different is the general form.",
            "If all the parameters are equal.",
            "It's called an exchangeable directly distribution, meaning that you can exchange any of the parameters, and it's still the same.",
            "And so the case when all the parameters of 1 gives a uniformed richly distribution and it's been shown that in that case it's equivalent to probabilistic latent semantic analysis."
        ],
        [
            "So now how do we perform document retrieval with this topic models?"
        ],
        [
            "So in my case, I found that I've done some previous work on efficient retrieval with topic models and found that the most convenient form is to put the topics in a thesaurus.",
            "So we have this term by term matrix which contains all the relationships and this equation just shows that.",
            "A term given another term and the parameter can be simply.",
            "An shown in the form which literally allocation provides and the rest of that's in the paper.",
            "So."
        ],
        [
            "His traditional retrieval.",
            "How you have a query and you apply it to the Document Index an out of that comes at a set of rank documents.",
            "So we now case where we're now sliding in."
        ],
        [
            "The source, so we have a query and the query is expanded using our topic relationships.",
            "So now we have a.",
            "A larger set of query terms, so if there was two query terms, the beginning.",
            "We use our thoreson.",
            "We might expand it to 100 terms and then apply that to the Document index so the source is computed using the topic models 'cause we know the relationships between the words.",
            "Once we've completed the topics."
        ],
        [
            "Alright, so now we're going to look at how the Dirichlet parameter affects the precision of the results."
        ],
        [
            "So using an exchangeable display distribution, which is.",
            "It's used in LDA, the exchangeable form because if we use the generalized form, it's too complex to solve, so it means that the distribution only has one parameter.",
            "So we have the same parameter for every topic.",
            "So by varying this parameter we vary the number of topics that are associated to each term.",
            "So if a hobo Alpha leads to many topics being associated to each term.",
            "And illovo of Alpha means only a few topics being associated to each term.",
            "So just as a."
        ],
        [
            "This was shown in this nice Wikipedia demonstration, so if you didn't get this, I'll just explain this again.",
            "You can think of.",
            "Um?",
            "XY.",
            "And.",
            "As a result, there should be, so we have three corners.",
            "This means one term that's another term, and it's a third term.",
            "If we choose this side here, it's like choosing these two terms.",
            "If we pick of oh here's choosing these terms of value.",
            "In the middle is picking all terms.",
            "Sorry topics.",
            "So three different topics.",
            "If we sample something from the middle, that means we've sampled all three topics for that term.",
            "Something at this point would just be sampling, sampling one topic for that term.",
            "Or the other points different topics for each term, so you can see as the Alpha value drops.",
            "Down.",
            "A minute there we go.",
            "The The corners appointee so a small value of Alpha means that you're more likely to pick a value in the corner, which means you're more likely to have less topics.",
            "Related to HTM.",
            "But the value of one, you can see it becomes totally flat, which means.",
            "You can have any number of topics per term.",
            "It's not as restricted."
        ],
        [
            "So we do experiments.",
            "We we.",
            "Computed LDA.",
            "Topics.",
            "And fitted them and we ran other experiments but just sitting there perimeter one.",
            "We used the document sets from Trek disk two so The Associated Press financial Review, Wall Street Journal, and Ziff Publishing.",
            "So each of these contains between 50 and 100,000 documents, and we also used queries 5051 to 200, which were used in TREC 1, two, and three.",
            "And.",
            "We compare that to a term frequency index using BM 25, which is considered the state of the art in standard term.",
            "Document term."
        ],
        [
            "Frequency indexes.",
            "So the first experiment.",
            "We want to look at how the changing the number of topics in a number of terms included would affect the results.",
            "So here we we built a thesaurus using all terms and 100 topics.",
            "We built another one with 100 topics and only terms that appear in at least 50 documents.",
            "And then we built another one with 300 topics and terms that appear in 50 documents.",
            "And so by doing that we ran our queries on each of the document sets and we found that there was no statistical significance in the in the precision results.",
            "So from then on we used a thesaurus containing 300 topics and terms that appeared in at least 50 documents."
        ],
        [
            "So the next experiment was to look at the storage and the computation times of each of these for setting these parameters.",
            "So this is very interesting.",
            "So here we've set the.",
            "Alpha vote the parameter to one, so we've said it ourself.",
            "We haven't hit fitted to the document set.",
            "So the size of the thesaurus you can see here for the first document set 87 megabytes for the 2nd 129, the 3rd 190, and the 4th 142 megabytes, and the build times were.",
            "Between 20 minutes to an hour for computing the topics for this size document set.",
            "So the next case is when we fitted the Alpha parameter."
        ],
        [
            "See here at the office that was discovered by feeding it to the document set.",
            "Here we can see the storage sizes, which are pretty much equivalent, maybe slightly larger, but not too much larger.",
            "So 112 to 87 megabytes.",
            "30 to 2900 and 790 and 52 and 42, so there's not much change there.",
            "But when computing the topics, we can see that by fitting the parameter.",
            "The time is greatly increased because we've had to go through many inner iterations to fit it, so for this top document set is taken about two days, was a bit over 2 days.",
            "For the second one, Monday for the third document set.",
            "Around 2 1/2 days and the 4th 1 two days so you can see that's huge change here.",
            "So if we can show that this is OK, then that's a good case.",
            "We don't have to perform all this computation.",
            "So."
        ],
        [
            "The final experiment was we want to look at the retrieval precision for each of these these cases on each document set.",
            "So we built the sources by fitting the parameter and by using the parameter one.",
            "Um?",
            "And we also examined term expansion sizes of 1020 fifty 102 hundred.",
            "So we very thorough in our analysis and all the results are in the paper.",
            "But I'm not presenting them here because they.",
            "They'd just be covering the slide.",
            "So the results showed.",
            "When computing the precision.",
            "There was no increase in precision of the fitted LDA over the unfitted LDA for the financial review document set on on every in every case.",
            "There was a significant increase with the ZIP document set for some parameters.",
            "But at the same time, it wasn't significant over being 25.",
            "So precise.",
            "Mean average precision.",
            "After precision or precision at a certain cutoff, Woods precision.",
            "I'll show you on the next slide.",
            "We've gone through three different measures, yeah?",
            "Yes, it's in all cases it was.",
            "There was no significant change in three dimensions.",
            "I used Yep.",
            "Using this.",
            "Notice for some of the parameters there was no improvement over being 25.",
            "But yeah, but in that case the fitting didn't help.",
            "So in the case when there was a significant increase over not fitting, there wasn't over being 25.",
            "So it was like those parameters are very poorly set.",
            "If you can see what I mean, yeah.",
            "So that means that you wouldn't want to choose that range, so it's not a good, not a good indicator.",
            "And the third case.",
            "So there's a significant increase over both unfitted LDA and BM 25 for a certain mix parameter in Associated Press and the Wall Street Journal.",
            "So for this parameter .9.",
            "But as we see, unfortunately the best precision was obtained.",
            "When we set the mix to this value of point 7.8, so we're not sure if you'd want to set .9 in their case as well.",
            "So this last table."
        ],
        [
            "Shows a summary of these results.",
            "He's two different documents.",
            "It's The Associated Press in the financial review and we've got the fitted unfitted and being 25.",
            "So this being 25 is not using topics at all.",
            "It is just using words and documents.",
            "So you're retreiving other documents that contain the query words.",
            "And here we have three different metrics.",
            "Mean average precision, which is kind of like the area under the precision recall curve.",
            "Precision at 10, which is the precision.",
            "Just looking at the top ten retrieve documents.",
            "So it's pretty much how many documents were relevant in the top 10 / 10.",
            "An mean reciprocal rank which is.",
            "You find that the position of the first relevant document and you.",
            "You invert.",
            "So for the first relevant document, was at rank two, you get a value of .5.",
            "So these results we see that.",
            "Use the mouse here.",
            "Is my mouse here so?",
            "We can see that.",
            "There's.",
            "No real difference between fitted LDA and unfitted LDA for each of the three cases, but there is increases over being 25, which is good to know because otherwise there'd be no point using it at all.",
            "For the second document set, there's also pretty much the same.",
            "And an increase over being 25.",
            "So.",
            "Yeah.",
            "And his."
        ],
        [
            "Sorry, I'm this highlighting coming up here.",
            "Even better, so here there's a fitted and unfitted results.",
            "If you have fairly similar.",
            "And for the financial."
        ],
        [
            "New document set.",
            "They're both fairly similar as well."
        ],
        [
            "Of here we have."
        ],
        [
            "Other two documents that we use for the Wall Street Journal set.",
            "You can see that the all three metrics if similar results, and they're all increasing over being 25 and finally the."
        ],
        [
            "As if publishing articles you can see they're all very similar and give increases over being 25.",
            "So the."
        ],
        [
            "Conclusions after seeing this, this is a very interesting result.",
            "So we can see that we know that late and richly allocation allocates topics using address like distribution to try and skew where their positions.",
            "The Dursley distribution has a parameter that has to be defeated or estimated.",
            "We compared the effectiveness of information retrieval using latent originally allocation with both fitted and estimated rissler perimeter.",
            "And we found that the fitted primer didn't give any significant increase in precision.",
            "Sorry that the significant test was done using a Wilcoxon sign rank test as well.",
            "Um?",
            "So by not fitting the Latendresse Lee allocation document models we obtain around the 50 to 90% times gain in computational efficiency.",
            "So fitting LDA document models does not provide any better benefit for the information retrieval task.",
            "OK, thank you.",
            "Yes.",
            "Present value only fit symmetrically things, so you push people out before also.",
            "Yep so 1A for the whole document set is that we said.",
            "Yep.",
            "It was a symmetric.",
            "Overall, you're right.",
            "You had to save all yeah, yeah.",
            "Yeah, and that's what's been done for.",
            "So that was in Blazer model and also in the grissam Stivers.",
            "They also always choose just a an exchangeable distribution.",
            "Tell 'em topics such that they converge to their observed relative frequency so the Alpha converges to Bilderberg observable relative frequency in the document or face so very frequent topic.",
            "Scared that I'll try out kind of less frequent topics.",
            "Get smaller at night.",
            "Apply this.",
            "For the same problem is new that I generally get better results by this extra meeting, which is done by Sunday night.",
            "The difference is you say, fitting.",
            "Printing doesn't help at all, yeah?",
            "So this is.",
            "Weather pretty.",
            "Asymmetric image ladies to do different Alpha Sport in Bill.",
            "So, did you get retrieval results using these fitted?",
            "'cause even given, given what you said, I would be very surprised if it gave anything better.",
            "'cause I've I've, I've been looking into top models for awhile and it's really been a struggle.",
            "I think you would come in on that, so it will be interesting to conduct a simulated the test experiments so you can generate data with the evenly distributed copies and generator is very skewed distribution and then you see how this parameter tuning effect so so you may be so just generate artificial documents.",
            "Or is there something unexplainable?",
            "Did you apply early also to the query or did you only use LDA to construct the doors?",
            "So just for the across the documents and from that from the relationships the source was built?",
            "Because in the query?",
            "And they will develop different topics according to the words in the query.",
            "Yeah, so that was the the job of the hours to extract.",
            "It's kind of like computing the topics from the the query words and then from that you then compute the words that would be related to those topics that you computed.",
            "That's what the source is doing in this case.",
            "But it's like a very efficient form of doing it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, hello everyone and hello everyone out there.",
                    "label": 0
                },
                {
                    "sent": "So my presentation today is going to be on the sensitivity sensitivity of latent directly allocation for information retrieval.",
                    "label": 1
                },
                {
                    "sent": "This work done by Amy Lawrence and rail Kotagiri.",
                    "label": 0
                },
                {
                    "sent": "We call him real because it's too hard to pronounce the whole name there.",
                    "label": 1
                },
                {
                    "sent": "From the University of Melbourne in Australia.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, just briefly go over what I'm going to present today.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So the use of topic models adds to the precision of document retrieval.",
                    "label": 1
                },
                {
                    "sent": "So when.",
                    "label": 0
                },
                {
                    "sent": "Topics are used underneath.",
                    "label": 0
                },
                {
                    "sent": "They used to find relationships between terms and documents, and so by knowing relationships between words or terms, then we're able to find more precise documents.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So literally, allocation allows us to model documents and words with the model documents with the drizly distributed topics.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this if we use this juice like distribution then the models depend on this richly parameter.",
                    "label": 1
                },
                {
                    "sent": "So initial versions of light and richlite allocation fitted this parameter to the document set that was used.",
                    "label": 1
                },
                {
                    "sent": "And an hour later versions just simply pick a parameter.",
                    "label": 0
                },
                {
                    "sent": "And so, in this presentation today, we're going to look and see if that actually makes a difference fitting the parameter.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the document said, or just picking a parameter.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'll spoil it for you and tell you that it doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to look at Bruce briefly.",
                    "label": 0
                },
                {
                    "sent": "Go over topic models for documents.",
                    "label": 1
                },
                {
                    "sent": "We've seen this before in the first presentation, then briefly look at late and richly allocation.",
                    "label": 0
                },
                {
                    "sent": "And then had to use the topic models to perform information retrieval.",
                    "label": 0
                },
                {
                    "sent": "And finally we will look at experiments where we vary the display parameter to examine the retrieval results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st just overview of topic based document models.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we when we have a document, a piece of text, it's just a string of words that go from the first word in the document to the last word in the document.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To model the documents, we have to understand the purpose behind the document, what it was intended for.",
                    "label": 1
                },
                {
                    "sent": "And we know that these documents are written to communicate knowledge.",
                    "label": 0
                },
                {
                    "sent": "If we could simply get the contents of our brain and transfer it to someone else is brand, that would be ideal and we get complete understanding of what the person intended.",
                    "label": 0
                },
                {
                    "sent": "But we can't do that.",
                    "label": 0
                },
                {
                    "sent": "So we take what's in our head and convert it into text form, write it out and then give it to people so they can read.",
                    "label": 0
                },
                {
                    "sent": "And hopefully that transformation shows what we were thinking.",
                    "label": 0
                },
                {
                    "sent": "It doesn't all the time, but we do our best to do that.",
                    "label": 0
                },
                {
                    "sent": "So by modeling documents, we're attempting to try and reproduce this process to get rid of to model that transformation from our our knowledge to this text data.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do this, we gotta first think of.",
                    "label": 0
                },
                {
                    "sent": "How are documents created so when I go to write a document?",
                    "label": 0
                },
                {
                    "sent": "First I start off with this idea of what I want to write, 'cause if I don't have an idea then.",
                    "label": 0
                },
                {
                    "sent": "I'll go off and do something else.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so this in this naive document creation model, the author has had an idea, and then they've simply pick terms out.",
                    "label": 1
                },
                {
                    "sent": "From the the idea and put them into a document, and so these terms are all independent and they have no relationship at all.",
                    "label": 0
                },
                {
                    "sent": "And it's just that they have to use the exact terms.",
                    "label": 0
                },
                {
                    "sent": "The exact words for the document.",
                    "label": 0
                },
                {
                    "sent": "If there's any different words, then the document would have a different meaning.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I write a document containing baby and clothes, then it would have no relationship to any of the words.",
                    "label": 0
                },
                {
                    "sent": "Documents have infant lawsuits in it, even though they are similar.",
                    "label": 0
                },
                {
                    "sent": "So that's a very naive model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topic based models.",
                    "label": 0
                },
                {
                    "sent": "Take it to the next step, how?",
                    "label": 0
                },
                {
                    "sent": "We have this idea.",
                    "label": 0
                },
                {
                    "sent": "And then, rather than picking words straight away, we think of topics to fill in the document with.",
                    "label": 1
                },
                {
                    "sent": "So we think of the first topic to put at the top and then the second topic.",
                    "label": 0
                },
                {
                    "sent": "And then once we we know what topics we want to write about.",
                    "label": 0
                },
                {
                    "sent": "We can take words from those topics.",
                    "label": 0
                },
                {
                    "sent": "So there's all these here.",
                    "label": 0
                },
                {
                    "sent": "Term 4, five and one or related to topic one, and so they have similar meaning.",
                    "label": 0
                },
                {
                    "sent": "So we can pick any of those terms and put them in a document.",
                    "label": 0
                },
                {
                    "sent": "And then for topic two, we can pick any of these three terms and put them in our document.",
                    "label": 0
                },
                {
                    "sent": "So as long as this document has terms from the correct topics.",
                    "label": 0
                },
                {
                    "sent": "Then it will be the same document the related document.",
                    "label": 0
                },
                {
                    "sent": "So by constructing these, you're thinking this clusters were clustering the terms together in these topic forms.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So look at light and richly allocation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attempts to build these topics.",
                    "label": 0
                },
                {
                    "sent": "Let's just play allocation is similar to probabilistic latent semantic analysis that we saw in the first presentation.",
                    "label": 0
                },
                {
                    "sent": "How we try and compute these topics in the background just based on a document collection.",
                    "label": 0
                },
                {
                    "sent": "So we have other words in the documents.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The difference is one of the major differences is that it uses Additionally distribution to model how the topics are sampled through the document.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look, we go here and I'll.",
                    "label": 0
                },
                {
                    "sent": "I found this nice Wikipedia page.",
                    "label": 0
                },
                {
                    "sent": "Everyone loves Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of a 3 dimensional display distribution.",
                    "label": 0
                },
                {
                    "sent": "So it's got the parameters down here.",
                    "label": 0
                },
                {
                    "sent": "And they cut off.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "This is the three the space of the distribution, and we can sample from any one of these points and the places.",
                    "label": 0
                },
                {
                    "sent": "That have a higher value out, more likely, more highly probable, being sampled.",
                    "label": 0
                },
                {
                    "sent": "So you can see in this case that all the points in the X.",
                    "label": 0
                },
                {
                    "sent": "More likely to be sampled then the the ones nearby Enzed.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So here that we've got this.",
                    "label": 0
                },
                {
                    "sent": "The parameter for this traditional distribution determines the shape of this curve.",
                    "label": 0
                },
                {
                    "sent": "So here the parameter.",
                    "label": 0
                },
                {
                    "sent": "There's three parameters and each one is different, giving it a slightly bent looking shape.",
                    "label": 0
                },
                {
                    "sent": "But instead, if we if we set all the parameters to the same value.",
                    "label": 0
                },
                {
                    "sent": "It becomes more symmetric, so here's this.",
                    "label": 0
                },
                {
                    "sent": "This parameter all set to the same value and you can see the number changing at the top going from now it's above 1.",
                    "label": 0
                },
                {
                    "sent": "So as it goes above one, it gets this curved.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "An essence below 0 at dips in, but when it gets to one it's totally flat.",
                    "label": 0
                },
                {
                    "sent": "So you can see that failure is 1.",
                    "label": 0
                },
                {
                    "sent": "It means that we can pick any position in this space.",
                    "label": 0
                },
                {
                    "sent": "It is equally likely, but as as the value is smaller than the points at the edges are more equal likely.",
                    "label": 0
                },
                {
                    "sent": "And so a small parameter implies that we're more likely to pick only a few talk topics per document, because they're in the corners.",
                    "label": 0
                },
                {
                    "sent": "But a large value implies that there's going to be many topics for document because it's got this high probability in the center.",
                    "label": 0
                },
                {
                    "sent": "And a value of 1 means that any position is likely, which is similar to what happens in probabilistic latent semantic analysis.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the case where the parameters in the display distribution different is the general form.",
                    "label": 0
                },
                {
                    "sent": "If all the parameters are equal.",
                    "label": 0
                },
                {
                    "sent": "It's called an exchangeable directly distribution, meaning that you can exchange any of the parameters, and it's still the same.",
                    "label": 1
                },
                {
                    "sent": "And so the case when all the parameters of 1 gives a uniformed richly distribution and it's been shown that in that case it's equivalent to probabilistic latent semantic analysis.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now how do we perform document retrieval with this topic models?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in my case, I found that I've done some previous work on efficient retrieval with topic models and found that the most convenient form is to put the topics in a thesaurus.",
                    "label": 0
                },
                {
                    "sent": "So we have this term by term matrix which contains all the relationships and this equation just shows that.",
                    "label": 0
                },
                {
                    "sent": "A term given another term and the parameter can be simply.",
                    "label": 0
                },
                {
                    "sent": "An shown in the form which literally allocation provides and the rest of that's in the paper.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His traditional retrieval.",
                    "label": 0
                },
                {
                    "sent": "How you have a query and you apply it to the Document Index an out of that comes at a set of rank documents.",
                    "label": 0
                },
                {
                    "sent": "So we now case where we're now sliding in.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The source, so we have a query and the query is expanded using our topic relationships.",
                    "label": 0
                },
                {
                    "sent": "So now we have a.",
                    "label": 0
                },
                {
                    "sent": "A larger set of query terms, so if there was two query terms, the beginning.",
                    "label": 0
                },
                {
                    "sent": "We use our thoreson.",
                    "label": 0
                },
                {
                    "sent": "We might expand it to 100 terms and then apply that to the Document index so the source is computed using the topic models 'cause we know the relationships between the words.",
                    "label": 0
                },
                {
                    "sent": "Once we've completed the topics.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now we're going to look at how the Dirichlet parameter affects the precision of the results.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So using an exchangeable display distribution, which is.",
                    "label": 1
                },
                {
                    "sent": "It's used in LDA, the exchangeable form because if we use the generalized form, it's too complex to solve, so it means that the distribution only has one parameter.",
                    "label": 0
                },
                {
                    "sent": "So we have the same parameter for every topic.",
                    "label": 1
                },
                {
                    "sent": "So by varying this parameter we vary the number of topics that are associated to each term.",
                    "label": 1
                },
                {
                    "sent": "So if a hobo Alpha leads to many topics being associated to each term.",
                    "label": 1
                },
                {
                    "sent": "And illovo of Alpha means only a few topics being associated to each term.",
                    "label": 0
                },
                {
                    "sent": "So just as a.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was shown in this nice Wikipedia demonstration, so if you didn't get this, I'll just explain this again.",
                    "label": 0
                },
                {
                    "sent": "You can think of.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "XY.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As a result, there should be, so we have three corners.",
                    "label": 0
                },
                {
                    "sent": "This means one term that's another term, and it's a third term.",
                    "label": 0
                },
                {
                    "sent": "If we choose this side here, it's like choosing these two terms.",
                    "label": 0
                },
                {
                    "sent": "If we pick of oh here's choosing these terms of value.",
                    "label": 0
                },
                {
                    "sent": "In the middle is picking all terms.",
                    "label": 0
                },
                {
                    "sent": "Sorry topics.",
                    "label": 0
                },
                {
                    "sent": "So three different topics.",
                    "label": 0
                },
                {
                    "sent": "If we sample something from the middle, that means we've sampled all three topics for that term.",
                    "label": 0
                },
                {
                    "sent": "Something at this point would just be sampling, sampling one topic for that term.",
                    "label": 0
                },
                {
                    "sent": "Or the other points different topics for each term, so you can see as the Alpha value drops.",
                    "label": 0
                },
                {
                    "sent": "Down.",
                    "label": 0
                },
                {
                    "sent": "A minute there we go.",
                    "label": 0
                },
                {
                    "sent": "The The corners appointee so a small value of Alpha means that you're more likely to pick a value in the corner, which means you're more likely to have less topics.",
                    "label": 0
                },
                {
                    "sent": "Related to HTM.",
                    "label": 0
                },
                {
                    "sent": "But the value of one, you can see it becomes totally flat, which means.",
                    "label": 0
                },
                {
                    "sent": "You can have any number of topics per term.",
                    "label": 0
                },
                {
                    "sent": "It's not as restricted.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we do experiments.",
                    "label": 0
                },
                {
                    "sent": "We we.",
                    "label": 0
                },
                {
                    "sent": "Computed LDA.",
                    "label": 0
                },
                {
                    "sent": "Topics.",
                    "label": 0
                },
                {
                    "sent": "And fitted them and we ran other experiments but just sitting there perimeter one.",
                    "label": 0
                },
                {
                    "sent": "We used the document sets from Trek disk two so The Associated Press financial Review, Wall Street Journal, and Ziff Publishing.",
                    "label": 1
                },
                {
                    "sent": "So each of these contains between 50 and 100,000 documents, and we also used queries 5051 to 200, which were used in TREC 1, two, and three.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We compare that to a term frequency index using BM 25, which is considered the state of the art in standard term.",
                    "label": 0
                },
                {
                    "sent": "Document term.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Frequency indexes.",
                    "label": 0
                },
                {
                    "sent": "So the first experiment.",
                    "label": 0
                },
                {
                    "sent": "We want to look at how the changing the number of topics in a number of terms included would affect the results.",
                    "label": 0
                },
                {
                    "sent": "So here we we built a thesaurus using all terms and 100 topics.",
                    "label": 1
                },
                {
                    "sent": "We built another one with 100 topics and only terms that appear in at least 50 documents.",
                    "label": 1
                },
                {
                    "sent": "And then we built another one with 300 topics and terms that appear in 50 documents.",
                    "label": 1
                },
                {
                    "sent": "And so by doing that we ran our queries on each of the document sets and we found that there was no statistical significance in the in the precision results.",
                    "label": 0
                },
                {
                    "sent": "So from then on we used a thesaurus containing 300 topics and terms that appeared in at least 50 documents.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next experiment was to look at the storage and the computation times of each of these for setting these parameters.",
                    "label": 1
                },
                {
                    "sent": "So this is very interesting.",
                    "label": 0
                },
                {
                    "sent": "So here we've set the.",
                    "label": 0
                },
                {
                    "sent": "Alpha vote the parameter to one, so we've said it ourself.",
                    "label": 0
                },
                {
                    "sent": "We haven't hit fitted to the document set.",
                    "label": 0
                },
                {
                    "sent": "So the size of the thesaurus you can see here for the first document set 87 megabytes for the 2nd 129, the 3rd 190, and the 4th 142 megabytes, and the build times were.",
                    "label": 0
                },
                {
                    "sent": "Between 20 minutes to an hour for computing the topics for this size document set.",
                    "label": 0
                },
                {
                    "sent": "So the next case is when we fitted the Alpha parameter.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See here at the office that was discovered by feeding it to the document set.",
                    "label": 0
                },
                {
                    "sent": "Here we can see the storage sizes, which are pretty much equivalent, maybe slightly larger, but not too much larger.",
                    "label": 0
                },
                {
                    "sent": "So 112 to 87 megabytes.",
                    "label": 0
                },
                {
                    "sent": "30 to 2900 and 790 and 52 and 42, so there's not much change there.",
                    "label": 0
                },
                {
                    "sent": "But when computing the topics, we can see that by fitting the parameter.",
                    "label": 0
                },
                {
                    "sent": "The time is greatly increased because we've had to go through many inner iterations to fit it, so for this top document set is taken about two days, was a bit over 2 days.",
                    "label": 0
                },
                {
                    "sent": "For the second one, Monday for the third document set.",
                    "label": 0
                },
                {
                    "sent": "Around 2 1/2 days and the 4th 1 two days so you can see that's huge change here.",
                    "label": 0
                },
                {
                    "sent": "So if we can show that this is OK, then that's a good case.",
                    "label": 0
                },
                {
                    "sent": "We don't have to perform all this computation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The final experiment was we want to look at the retrieval precision for each of these these cases on each document set.",
                    "label": 0
                },
                {
                    "sent": "So we built the sources by fitting the parameter and by using the parameter one.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And we also examined term expansion sizes of 1020 fifty 102 hundred.",
                    "label": 1
                },
                {
                    "sent": "So we very thorough in our analysis and all the results are in the paper.",
                    "label": 0
                },
                {
                    "sent": "But I'm not presenting them here because they.",
                    "label": 0
                },
                {
                    "sent": "They'd just be covering the slide.",
                    "label": 0
                },
                {
                    "sent": "So the results showed.",
                    "label": 0
                },
                {
                    "sent": "When computing the precision.",
                    "label": 0
                },
                {
                    "sent": "There was no increase in precision of the fitted LDA over the unfitted LDA for the financial review document set on on every in every case.",
                    "label": 1
                },
                {
                    "sent": "There was a significant increase with the ZIP document set for some parameters.",
                    "label": 1
                },
                {
                    "sent": "But at the same time, it wasn't significant over being 25.",
                    "label": 0
                },
                {
                    "sent": "So precise.",
                    "label": 0
                },
                {
                    "sent": "Mean average precision.",
                    "label": 0
                },
                {
                    "sent": "After precision or precision at a certain cutoff, Woods precision.",
                    "label": 0
                },
                {
                    "sent": "I'll show you on the next slide.",
                    "label": 0
                },
                {
                    "sent": "We've gone through three different measures, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes, it's in all cases it was.",
                    "label": 0
                },
                {
                    "sent": "There was no significant change in three dimensions.",
                    "label": 0
                },
                {
                    "sent": "I used Yep.",
                    "label": 0
                },
                {
                    "sent": "Using this.",
                    "label": 0
                },
                {
                    "sent": "Notice for some of the parameters there was no improvement over being 25.",
                    "label": 0
                },
                {
                    "sent": "But yeah, but in that case the fitting didn't help.",
                    "label": 0
                },
                {
                    "sent": "So in the case when there was a significant increase over not fitting, there wasn't over being 25.",
                    "label": 0
                },
                {
                    "sent": "So it was like those parameters are very poorly set.",
                    "label": 0
                },
                {
                    "sent": "If you can see what I mean, yeah.",
                    "label": 0
                },
                {
                    "sent": "So that means that you wouldn't want to choose that range, so it's not a good, not a good indicator.",
                    "label": 0
                },
                {
                    "sent": "And the third case.",
                    "label": 0
                },
                {
                    "sent": "So there's a significant increase over both unfitted LDA and BM 25 for a certain mix parameter in Associated Press and the Wall Street Journal.",
                    "label": 1
                },
                {
                    "sent": "So for this parameter .9.",
                    "label": 0
                },
                {
                    "sent": "But as we see, unfortunately the best precision was obtained.",
                    "label": 0
                },
                {
                    "sent": "When we set the mix to this value of point 7.8, so we're not sure if you'd want to set .9 in their case as well.",
                    "label": 0
                },
                {
                    "sent": "So this last table.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shows a summary of these results.",
                    "label": 0
                },
                {
                    "sent": "He's two different documents.",
                    "label": 0
                },
                {
                    "sent": "It's The Associated Press in the financial review and we've got the fitted unfitted and being 25.",
                    "label": 0
                },
                {
                    "sent": "So this being 25 is not using topics at all.",
                    "label": 0
                },
                {
                    "sent": "It is just using words and documents.",
                    "label": 0
                },
                {
                    "sent": "So you're retreiving other documents that contain the query words.",
                    "label": 0
                },
                {
                    "sent": "And here we have three different metrics.",
                    "label": 0
                },
                {
                    "sent": "Mean average precision, which is kind of like the area under the precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "Precision at 10, which is the precision.",
                    "label": 0
                },
                {
                    "sent": "Just looking at the top ten retrieve documents.",
                    "label": 0
                },
                {
                    "sent": "So it's pretty much how many documents were relevant in the top 10 / 10.",
                    "label": 0
                },
                {
                    "sent": "An mean reciprocal rank which is.",
                    "label": 0
                },
                {
                    "sent": "You find that the position of the first relevant document and you.",
                    "label": 0
                },
                {
                    "sent": "You invert.",
                    "label": 0
                },
                {
                    "sent": "So for the first relevant document, was at rank two, you get a value of .5.",
                    "label": 0
                },
                {
                    "sent": "So these results we see that.",
                    "label": 0
                },
                {
                    "sent": "Use the mouse here.",
                    "label": 0
                },
                {
                    "sent": "Is my mouse here so?",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "There's.",
                    "label": 0
                },
                {
                    "sent": "No real difference between fitted LDA and unfitted LDA for each of the three cases, but there is increases over being 25, which is good to know because otherwise there'd be no point using it at all.",
                    "label": 0
                },
                {
                    "sent": "For the second document set, there's also pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "And an increase over being 25.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And his.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, I'm this highlighting coming up here.",
                    "label": 0
                },
                {
                    "sent": "Even better, so here there's a fitted and unfitted results.",
                    "label": 0
                },
                {
                    "sent": "If you have fairly similar.",
                    "label": 0
                },
                {
                    "sent": "And for the financial.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "New document set.",
                    "label": 0
                },
                {
                    "sent": "They're both fairly similar as well.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of here we have.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other two documents that we use for the Wall Street Journal set.",
                    "label": 0
                },
                {
                    "sent": "You can see that the all three metrics if similar results, and they're all increasing over being 25 and finally the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As if publishing articles you can see they're all very similar and give increases over being 25.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusions after seeing this, this is a very interesting result.",
                    "label": 0
                },
                {
                    "sent": "So we can see that we know that late and richly allocation allocates topics using address like distribution to try and skew where their positions.",
                    "label": 0
                },
                {
                    "sent": "The Dursley distribution has a parameter that has to be defeated or estimated.",
                    "label": 1
                },
                {
                    "sent": "We compared the effectiveness of information retrieval using latent originally allocation with both fitted and estimated rissler perimeter.",
                    "label": 1
                },
                {
                    "sent": "And we found that the fitted primer didn't give any significant increase in precision.",
                    "label": 0
                },
                {
                    "sent": "Sorry that the significant test was done using a Wilcoxon sign rank test as well.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So by not fitting the Latendresse Lee allocation document models we obtain around the 50 to 90% times gain in computational efficiency.",
                    "label": 1
                },
                {
                    "sent": "So fitting LDA document models does not provide any better benefit for the information retrieval task.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Present value only fit symmetrically things, so you push people out before also.",
                    "label": 0
                },
                {
                    "sent": "Yep so 1A for the whole document set is that we said.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "It was a symmetric.",
                    "label": 0
                },
                {
                    "sent": "Overall, you're right.",
                    "label": 0
                },
                {
                    "sent": "You had to save all yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and that's what's been done for.",
                    "label": 0
                },
                {
                    "sent": "So that was in Blazer model and also in the grissam Stivers.",
                    "label": 0
                },
                {
                    "sent": "They also always choose just a an exchangeable distribution.",
                    "label": 0
                },
                {
                    "sent": "Tell 'em topics such that they converge to their observed relative frequency so the Alpha converges to Bilderberg observable relative frequency in the document or face so very frequent topic.",
                    "label": 0
                },
                {
                    "sent": "Scared that I'll try out kind of less frequent topics.",
                    "label": 0
                },
                {
                    "sent": "Get smaller at night.",
                    "label": 0
                },
                {
                    "sent": "Apply this.",
                    "label": 0
                },
                {
                    "sent": "For the same problem is new that I generally get better results by this extra meeting, which is done by Sunday night.",
                    "label": 0
                },
                {
                    "sent": "The difference is you say, fitting.",
                    "label": 0
                },
                {
                    "sent": "Printing doesn't help at all, yeah?",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Weather pretty.",
                    "label": 0
                },
                {
                    "sent": "Asymmetric image ladies to do different Alpha Sport in Bill.",
                    "label": 0
                },
                {
                    "sent": "So, did you get retrieval results using these fitted?",
                    "label": 0
                },
                {
                    "sent": "'cause even given, given what you said, I would be very surprised if it gave anything better.",
                    "label": 0
                },
                {
                    "sent": "'cause I've I've, I've been looking into top models for awhile and it's really been a struggle.",
                    "label": 0
                },
                {
                    "sent": "I think you would come in on that, so it will be interesting to conduct a simulated the test experiments so you can generate data with the evenly distributed copies and generator is very skewed distribution and then you see how this parameter tuning effect so so you may be so just generate artificial documents.",
                    "label": 0
                },
                {
                    "sent": "Or is there something unexplainable?",
                    "label": 0
                },
                {
                    "sent": "Did you apply early also to the query or did you only use LDA to construct the doors?",
                    "label": 0
                },
                {
                    "sent": "So just for the across the documents and from that from the relationships the source was built?",
                    "label": 0
                },
                {
                    "sent": "Because in the query?",
                    "label": 0
                },
                {
                    "sent": "And they will develop different topics according to the words in the query.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that was the the job of the hours to extract.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like computing the topics from the the query words and then from that you then compute the words that would be related to those topics that you computed.",
                    "label": 0
                },
                {
                    "sent": "That's what the source is doing in this case.",
                    "label": 0
                },
                {
                    "sent": "But it's like a very efficient form of doing it.",
                    "label": 0
                }
            ]
        }
    }
}