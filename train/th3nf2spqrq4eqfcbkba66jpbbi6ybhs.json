{
    "id": "th3nf2spqrq4eqfcbkba66jpbbi6ybhs",
    "title": "Lower Bounds for Passive and Active Learning",
    "info": {
        "author": [
            "Maxim Raginsky, Department of Electrical and Computer Engineering, Duke University"
        ],
        "published": "Sept. 6, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_raginsky_passiveactive/",
    "segmentation": [
        [
            "Alright hello so.",
            "As Phil services, joint work with Sasha Rochman so weak."
        ],
        [
            "Consider two well known paradigms for statistical learning and then focusing on classification here in the passive paradigm you have ID examples from some distribution of instances and labels that the teacher throws at the learner and the learner comes up with a hypothesis.",
            "In the active learning setup, the learner generates instances and the teacher provides answers.",
            "Both have been studied extensively and we have a number of algorithms and bounds.",
            "And the bounds involve things like the VC dimension and that has been the case since the classic works up until now, and for active learning something called the disagreement coefficient.",
            "So we would like to.",
            "Constructive framework for deriving lower bounds on the sample complexity of passive and active learning.",
            "That hopefully will involve these quanta."
        ],
        [
            "He's as well.",
            "So the assumptions that we use are VC dimension.",
            "Of the class of hypothesis, so assumes a VC class and we also assume hard margin, which means that the regression function has a jump at the decision boundary an the magnitude that jump is controlled by this parameter H, But then it turns out that not all VC classes are created equal in the following sense.",
            "If you look for refined bounds, which will tend to be distribution dependent, you look at the optimal hypothesis in your class, which here shown as F star.",
            "And then you look at all hypothesis that are epsilon close to the performance of F star.",
            "So there's an object called Alexander's capacity which essentially measures.",
            "How large this set of instances is on which any two such epsilon good predictors will disagree, and the supremum of that if you if you crank the epsilon up and look at the supremum.",
            "That's called a disagreement coefficient, has appeared in works, works by Steve Hanneke, but the function itself that depends on epsilon is called the Alexanders capacity function.",
            "So we have our results.",
            "Upper these are lower bounds on the sample complexity of active and passive learning, and we see the dependence of these quantities on.",
            "On these structural parameters of the problem on VC dimension, the confidence.",
            "Delta the accuracy epsilon and the margin parameter, so these are the bounds.",
            "You can see the difference that so when the disagreement coefficient is a constant, when it's finite, you see the gain of.",
            "Active over passive, on the other hand, we capture the entire scale of complexity by looking at the Alexanders capacity as a function of epsilon."
        ],
        [
            "So the tools are actually information theoretic well.",
            "What we use is decomposition of essentially the information gain a learner gets with each sample, and we use something called the data processing inequality for feed divergences, KL divergences a special case, but here we actually show that choosing fee is crucial.",
            "Data processing essentially says that any post processing decreases.",
            "Divergents makes distributions more similar, so the classic final method.",
            "If you've seen that sort of.",
            "Machinery is a special case of this technique, but the final method is not suitable for active learning, so and we also use a nice packing lemma actually related to error correcting codes in our construction.",
            "So come see our poster W. 85 For more details.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright hello so.",
                    "label": 0
                },
                {
                    "sent": "As Phil services, joint work with Sasha Rochman so weak.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Consider two well known paradigms for statistical learning and then focusing on classification here in the passive paradigm you have ID examples from some distribution of instances and labels that the teacher throws at the learner and the learner comes up with a hypothesis.",
                    "label": 0
                },
                {
                    "sent": "In the active learning setup, the learner generates instances and the teacher provides answers.",
                    "label": 0
                },
                {
                    "sent": "Both have been studied extensively and we have a number of algorithms and bounds.",
                    "label": 0
                },
                {
                    "sent": "And the bounds involve things like the VC dimension and that has been the case since the classic works up until now, and for active learning something called the disagreement coefficient.",
                    "label": 1
                },
                {
                    "sent": "So we would like to.",
                    "label": 0
                },
                {
                    "sent": "Constructive framework for deriving lower bounds on the sample complexity of passive and active learning.",
                    "label": 0
                },
                {
                    "sent": "That hopefully will involve these quanta.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He's as well.",
                    "label": 0
                },
                {
                    "sent": "So the assumptions that we use are VC dimension.",
                    "label": 0
                },
                {
                    "sent": "Of the class of hypothesis, so assumes a VC class and we also assume hard margin, which means that the regression function has a jump at the decision boundary an the magnitude that jump is controlled by this parameter H, But then it turns out that not all VC classes are created equal in the following sense.",
                    "label": 1
                },
                {
                    "sent": "If you look for refined bounds, which will tend to be distribution dependent, you look at the optimal hypothesis in your class, which here shown as F star.",
                    "label": 0
                },
                {
                    "sent": "And then you look at all hypothesis that are epsilon close to the performance of F star.",
                    "label": 0
                },
                {
                    "sent": "So there's an object called Alexander's capacity which essentially measures.",
                    "label": 0
                },
                {
                    "sent": "How large this set of instances is on which any two such epsilon good predictors will disagree, and the supremum of that if you if you crank the epsilon up and look at the supremum.",
                    "label": 0
                },
                {
                    "sent": "That's called a disagreement coefficient, has appeared in works, works by Steve Hanneke, but the function itself that depends on epsilon is called the Alexanders capacity function.",
                    "label": 0
                },
                {
                    "sent": "So we have our results.",
                    "label": 0
                },
                {
                    "sent": "Upper these are lower bounds on the sample complexity of active and passive learning, and we see the dependence of these quantities on.",
                    "label": 0
                },
                {
                    "sent": "On these structural parameters of the problem on VC dimension, the confidence.",
                    "label": 0
                },
                {
                    "sent": "Delta the accuracy epsilon and the margin parameter, so these are the bounds.",
                    "label": 0
                },
                {
                    "sent": "You can see the difference that so when the disagreement coefficient is a constant, when it's finite, you see the gain of.",
                    "label": 0
                },
                {
                    "sent": "Active over passive, on the other hand, we capture the entire scale of complexity by looking at the Alexanders capacity as a function of epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the tools are actually information theoretic well.",
                    "label": 0
                },
                {
                    "sent": "What we use is decomposition of essentially the information gain a learner gets with each sample, and we use something called the data processing inequality for feed divergences, KL divergences a special case, but here we actually show that choosing fee is crucial.",
                    "label": 1
                },
                {
                    "sent": "Data processing essentially says that any post processing decreases.",
                    "label": 0
                },
                {
                    "sent": "Divergents makes distributions more similar, so the classic final method.",
                    "label": 0
                },
                {
                    "sent": "If you've seen that sort of.",
                    "label": 1
                },
                {
                    "sent": "Machinery is a special case of this technique, but the final method is not suitable for active learning, so and we also use a nice packing lemma actually related to error correcting codes in our construction.",
                    "label": 0
                },
                {
                    "sent": "So come see our poster W. 85 For more details.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}