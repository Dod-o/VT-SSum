{
    "id": "fqmeekxwb6ghfoxdqu7eoecd3o6qg3ku",
    "title": "Unsupervised Estimation for Noisy-Channel Models",
    "info": {
        "author": [
            "Markos Mylonakis, University of Amsterdam"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/icml07_mylonakis_uefn/",
    "segmentation": [
        [
            "So welcome to the presentation of the paper child unsupervised estimation for noise training models by myself, Marcus Miller Nikes and as well as collect semen from the University of Homes for them.",
            "And the back of car from the University of Pitt."
        ],
        [
            "So as the title notes, our work is occupied with the noisy tunnel approach.",
            "This is an old story by Claude Shannon, and the expiring is very influential not only in NLP where we actually use it in this paper, but also in other scientific domains, such as by informatics for example, and what it is all about is recovering a message from a corrupted version there on that we call observation.",
            "So to give an example, an observation could be.",
            "Any much of a scanned document and the message that we want to recover his the alphanumeric characters that actually make it or observation could be speech clip and the methods could be the transcription of that into words and sentences.",
            "So.",
            "To use an example that I will actually use throughout this presentation.",
            "Salvation could be a German sentence.",
            "In the case of machine translation and what we do through the noisy channel approach is we actually view it as a corrupted version of an English sentence that we now miss.",
            "And our task is to go and recover this missing original English sentence.",
            "So in stochastic terms, what we want to do is get the methods that maximizes the probability of itself given the observation that we have at hand."
        ],
        [
            "So on the figure above, this is about the machine translation example that I just used, so we want to translate.",
            "From a source language to a target language.",
            "So we view this process in the opposite way.",
            "So assuming that we already have a target language sentence that got corrupted in the source language sentence that we observe, and we're going to go back this way.",
            "So using an application of the base rule here, our sets objective gets.",
            "Cut into two different models.",
            "The first one is was the topic of the previous talk, so it's a language model over the target language.",
            "And which concerns itself with how do correct method is actually look like.",
            "So give low probabilities to messages which do not belong to the target language and high probabilities to run the actually do belong.",
            "And the second part, and the most important in this case, and I'll explain why is the channel model that describes how our messages corrupted two observations.",
            "The reason that this is in red is that in this paper were occupied with o'kaysions that we use it now external model and where we actually have available data from both target and source site.",
            "So given that we do have this one, it's relatively easy to estimate the language model for example and Ingram as we saw in the previous talk by doing relative frequency estimation or maximum likelihood on the available data that we already have.",
            "But what's really tricky is get that other model here, because even though we have data from here and from there, we don't really know how they interconnect with each other, because usually data misses this kind of deep level mapping between the two sides of the tunnel.",
            "So this is an unsupervised estimation setting, and what people usually do is they perform maximum likelihood on the available source data.",
            "And they do it.",
            "He's using the EM algorithm cause this cannot be done analytically."
        ],
        [
            "So.",
            "In this slide.",
            "I.",
            "Have plotted on the figure above the usual workflow of the EM algorithm is for this case for the noisy channel, so asking how it breaks apart into 2 steps expectation and maximization step and on the first step what we do is read.",
            "What we call incomplete data, which is in this case is the sentences from the source site.",
            "And using a mapping from source to target with generate all possible hypothesis from the target side that could have been translated to the source sentence that we have at hand.",
            "And after we do that and we have source and target.",
            "One percentage, with all its possible better with all its possible target translations.",
            "Then we go and assign probabilities to each and everyone of these pairs using a conditional probability estimate for the channel model.",
            "So from target to source and language model from target as we saw in the previous slide.",
            "In this way, we end up with a complete data corpus of source and target sentences together with their respective probabilities and what we do in the M step is we perform maximally accurate estimation on this complete data set to get a new channel model conditional probability estimate, which is then fed into the next iteration, and so on.",
            "So these guys work together well for men."
        ],
        [
            "Many years.",
            "But actually we and other people before us have noted that there is a problem there.",
            "So as I mentioned, we are occupied in instances when we do have data from both target and source site.",
            "So there is not nothing that.",
            "Makes us do the estimative process in One Direction from target to source, whereas we can also do it the other way around, reversing the roles of the two languages.",
            "In the case of machine translation or in the same way in every case that the noises channel is used.",
            "And if we do that so work one way and the other, and we go and focus a little bit on the joint probability level, then be cause of course.",
            "Data is that we have available for training is not perfect, so the language model here is usually are weak and the data we have available is of course not infinite.",
            "What we will see is that this joint probability estimates that we get by performing estimation in One Direction and the other they are not the same.",
            "So given that fact, the next question is, should we really care?",
            "I mean, as I mentioned, people have been using that for a long time and they didn't seem to notice that.",
            "Well, actually we do care because there is empirical evidence from the world of machine translation.",
            "That it is actually better to combine both directions So what people have already done is performance dimension in One Direction in the other direction and using ad hoc methods for their specific task they were occupied with.",
            "They saw that by combining the output of the two estimated directions, they actually could get better accuracy in whatever task they were performing.",
            "So accepting this fact, the next question is, can we actually do that in a principled manner, and not one that is specific to a particular problem and we would like to do that through the usually and process that people are already familiar with, and that it is well studied."
        ],
        [
            "So in our paper we are concerned with exactly that problem and we have formulated what we call the bidirectionally EM algorithm or by insert.",
            "And as you can see from the figure above.",
            "It follows again the workflow of the EM algorithm.",
            "But there are some important differences.",
            "So first, in order to reconcile the two estimative directions, we work with one single set of parameters, which is the joint probability parameters.",
            "End.",
            "The next important thing is that during the eastep we read data from both source and target side at the same time.",
            "So using a bidirectional mapping from source to target and the other way around, we generate for each source sentence all possible target hypothesis and again for its target sentence that we read all possible sentence hypothesis.",
            "So just to give you an indication, this mapping in a very very very simplistic translation model, Waterworld model could be, for example, a lexicon.",
            "Or much more sophisticated things in better today's models.",
            "After we do that, we use two language models.",
            "Once from source and one for the target site and the joint probability estimate to assign probabilities to each and everyone of these pairs and then have a single complete data corpus that spans over both source and target.",
            "And it's exactly on this single complete data corpus that we perform the maximum likelihood estimation of the M step to get a new joint probability estimate that is fed in the next iteration and so on.",
            "So what we do?",
            "Get out of this is that during the EM step we climb the joint likelihood of both source and target training corpora at the same time.",
            "So all the available data that we have for training.",
            "And you can find more information about how this work in our paper and what's interesting is that we developed it from first principles with no particular assumptions on how the data should look like for it, for it to work, and we saw that it's actually a true VM instance, which means that it inherits the good properties of them are going through like the guaranteed to converge, and the guarantee to non decrease the likelihood of your training data.",
            "And next we show empirical evidence at.",
            "It's actually better than using Union from 2 distinct NLP tasks."
        ],
        [
            "So the first set of experimental results is from the example that I've used all this way, so machine translation we translate from German to English noun sequences, and we do that using non parallel corpora.",
            "So just one piece of English 1, one side and one piece of German text on the other side with no interconnections between them at all.",
            "So not non parallel corpora and in between we use Alexa on ambiguous lesson from German to English.",
            "Of which we want to estimate their translation probabilities.",
            "In this way, we follow an experimental platform from Kona Night by 2000 who were occupied with this exact problem.",
            "So their model was assuming same translated sentence length and know what movement you can see it on, how the China model decomposes into here.",
            "And the reason we chose to do that is because these people use the unidirectionally M, so the simple am to perform these tasks they got really encouraging results and we wanted to see if the bidirectional emko actually do any better.",
            "So I'm there pickle questions that were occupied with our three.",
            "The first one is what happens when you feed your estimated processes with more data.",
            "Can you get better accuracy?",
            "And how do the unidirectional bidirectional EM algorithm fair on that?",
            "And there are two more questions that are directly linked with the bidirectional nature of the Byam algorithm, which is what happens when the data that you place on the source and the target side come from different domains, does it affect the estimated process?",
            "And the second question was what happens when you have a lot more data on one side than you have on the other.",
            "Again, does this cause a problem in the estimated output?"
        ],
        [
            "So as you can see from the tables here.",
            "On the 1st task we use closely related corpora and as we feed more sentences into the training algorithms in each and every case, the word level, translation accuracy and the blame metrics.",
            "You get a considerable performance advantage when using by in relation to the unidirectional M estimation.",
            "But also when we move to less related corpora.",
            "So corpora that come from more different.",
            "Collections of tasks text.",
            "Then again, we see that even though as the task becomes harder so the overall accuracy of both methods drops by me still able to give us in return a performance advantage over the unidirectional estimation."
        ],
        [
            "So.",
            "On the next slide, I could buy a Toyota by yourself with a third question, which is what happens when there is an imbalance on the amount of data that you have available for training on source and target side.",
            "So in this graph.",
            "On the Y axis I plot the accuracy game and on the X axis I plot the number of sentences that we used.",
            "On the second direction, while we give the number of sentences on the primary estimated direction fixed to 100,000.",
            "So zero here means that no data is used in the second direction, which is unidirectional am.",
            "So what's surprising for us when we noted this chart was that you can actually get most of your performance advantage right away when using a little data on the second side.",
            "And this gives an incentive to researchers to actually use bidirectional estimation, even though when data might be sparse on the second side of the noisy channel model."
        ],
        [
            "When I move on to our second set of experiments which was occupied with adaptation of POS tagger, so a post algorithm mechanism design, part of speech tags to the words of a sentence.",
            "So if every word is a variable, a noun or an adjective in etc an.",
            "With adaptation we mean that what we actually want to do is get a post Tiger that was trained.",
            "From 1880, so sure.",
            "It's language such as modern standard Arabic and port it to a related but resource for 11 languages and see if we can make it have also get a good accuracy performance there.",
            "The training corpora that we used was on one side.",
            "We have tagged modern standard Arabic text and on the other side you have a small eleventeen text only, which means unlabeled text corpus.",
            "And in between we use a tiny lexecon with around 300 Michael Pyle entries that Maps MSA and postdoc pairs 211 words and these tasks in Joyce.",
            "Big amount of data symmetry becausw on one side.",
            "You have a very large corpus.",
            "Another side you have corpus one order of magnitude smaller and also the nature of the two corpora is different.",
            "So one side you have attacked Corpus another side you have text only corpus, so from the first row here you can see the incentive to actually perform adaptation.",
            "So the POS tagger which.",
            "Achieves around 96% accuracy on the language that it has been trained on.",
            "Drops to around 70% when you port it to related language then.",
            "And the next two rows we can announce that by using the unidirectional estimation makes sense using the direction am to adapt the target to this problem and also from the last row we can see that bidirectional estimation gives a small but considerable performance advantage.",
            "On top of that.",
            "Again, for this second set of experiments."
        ],
        [
            "So.",
            "To wrap up and leave some time for questions in our paper, we present and avail bidirectional algorithm and we do that working from first principles.",
            "We show that it's an actual EM algorithm instances, which means that it inherits the well known properties of the amalgam.",
            "Anything to say a wide range of applications and namely whenever the noisy tunnel is applicable and whenever data from both sides is available.",
            "With this often the case and we have also shown it that it actually outperforms the standard unidirectional estimation that people were using in two distinct NLP tasks.",
            "So thank you for your attention.",
            "Question.",
            "So in this in the bidirectional estimation you have parameters for PFT and for PAS for target and source.",
            "Yeah, and also parameters going for each of the conditionals.",
            "No, no, no.",
            "Can you explain exactly how the show on another effort to perform something similar for machine translation?",
            "People you have actually used two sets of conditional parameters, so a distinct set from SD and this thing set from T2, S?",
            "And then they added in the likelihood that they were they wanted to raise an additional term that tried to somehow reconcile them along the way.",
            "So in our case we use one single set of parameters, which is a joint probabilities and with.",
            "For it's estimated directions, we derive the conditional probabilities from this single set of ramps.",
            "So.",
            "Have a space which is the space of meaning and the two different languages are two different sorts of sentence is actually being generated by this.",
            "Is it because the set of creating the space of meaning is difficult because with you?",
            "Better problem that conditions are not generating the same.",
            "What do you mean by space of meaning?",
            "Yeah.",
            "Meaning.",
            "Different, so I think you mean a single source, yeah?",
            "Anyway, even though in when we go in process its pair of sentence and hypothesis, we use the conditional probabilities derived from the joint probabilities.",
            "The fact that we use one set of joint probabilities for its pair of of source and target somehow hints intuitively that there is a single source that actually outputs both of them in the at the same time.",
            "So in a way we actually kind of do it.",
            "Question.",
            "OK thanks Michael."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So welcome to the presentation of the paper child unsupervised estimation for noise training models by myself, Marcus Miller Nikes and as well as collect semen from the University of Homes for them.",
                    "label": 0
                },
                {
                    "sent": "And the back of car from the University of Pitt.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as the title notes, our work is occupied with the noisy tunnel approach.",
                    "label": 0
                },
                {
                    "sent": "This is an old story by Claude Shannon, and the expiring is very influential not only in NLP where we actually use it in this paper, but also in other scientific domains, such as by informatics for example, and what it is all about is recovering a message from a corrupted version there on that we call observation.",
                    "label": 1
                },
                {
                    "sent": "So to give an example, an observation could be.",
                    "label": 0
                },
                {
                    "sent": "Any much of a scanned document and the message that we want to recover his the alphanumeric characters that actually make it or observation could be speech clip and the methods could be the transcription of that into words and sentences.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To use an example that I will actually use throughout this presentation.",
                    "label": 0
                },
                {
                    "sent": "Salvation could be a German sentence.",
                    "label": 0
                },
                {
                    "sent": "In the case of machine translation and what we do through the noisy channel approach is we actually view it as a corrupted version of an English sentence that we now miss.",
                    "label": 0
                },
                {
                    "sent": "And our task is to go and recover this missing original English sentence.",
                    "label": 0
                },
                {
                    "sent": "So in stochastic terms, what we want to do is get the methods that maximizes the probability of itself given the observation that we have at hand.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So on the figure above, this is about the machine translation example that I just used, so we want to translate.",
                    "label": 0
                },
                {
                    "sent": "From a source language to a target language.",
                    "label": 1
                },
                {
                    "sent": "So we view this process in the opposite way.",
                    "label": 0
                },
                {
                    "sent": "So assuming that we already have a target language sentence that got corrupted in the source language sentence that we observe, and we're going to go back this way.",
                    "label": 0
                },
                {
                    "sent": "So using an application of the base rule here, our sets objective gets.",
                    "label": 0
                },
                {
                    "sent": "Cut into two different models.",
                    "label": 0
                },
                {
                    "sent": "The first one is was the topic of the previous talk, so it's a language model over the target language.",
                    "label": 0
                },
                {
                    "sent": "And which concerns itself with how do correct method is actually look like.",
                    "label": 1
                },
                {
                    "sent": "So give low probabilities to messages which do not belong to the target language and high probabilities to run the actually do belong.",
                    "label": 0
                },
                {
                    "sent": "And the second part, and the most important in this case, and I'll explain why is the channel model that describes how our messages corrupted two observations.",
                    "label": 0
                },
                {
                    "sent": "The reason that this is in red is that in this paper were occupied with o'kaysions that we use it now external model and where we actually have available data from both target and source site.",
                    "label": 0
                },
                {
                    "sent": "So given that we do have this one, it's relatively easy to estimate the language model for example and Ingram as we saw in the previous talk by doing relative frequency estimation or maximum likelihood on the available data that we already have.",
                    "label": 0
                },
                {
                    "sent": "But what's really tricky is get that other model here, because even though we have data from here and from there, we don't really know how they interconnect with each other, because usually data misses this kind of deep level mapping between the two sides of the tunnel.",
                    "label": 1
                },
                {
                    "sent": "So this is an unsupervised estimation setting, and what people usually do is they perform maximum likelihood on the available source data.",
                    "label": 0
                },
                {
                    "sent": "And they do it.",
                    "label": 0
                },
                {
                    "sent": "He's using the EM algorithm cause this cannot be done analytically.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In this slide.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Have plotted on the figure above the usual workflow of the EM algorithm is for this case for the noisy channel, so asking how it breaks apart into 2 steps expectation and maximization step and on the first step what we do is read.",
                    "label": 0
                },
                {
                    "sent": "What we call incomplete data, which is in this case is the sentences from the source site.",
                    "label": 1
                },
                {
                    "sent": "And using a mapping from source to target with generate all possible hypothesis from the target side that could have been translated to the source sentence that we have at hand.",
                    "label": 0
                },
                {
                    "sent": "And after we do that and we have source and target.",
                    "label": 0
                },
                {
                    "sent": "One percentage, with all its possible better with all its possible target translations.",
                    "label": 0
                },
                {
                    "sent": "Then we go and assign probabilities to each and everyone of these pairs using a conditional probability estimate for the channel model.",
                    "label": 1
                },
                {
                    "sent": "So from target to source and language model from target as we saw in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "In this way, we end up with a complete data corpus of source and target sentences together with their respective probabilities and what we do in the M step is we perform maximally accurate estimation on this complete data set to get a new channel model conditional probability estimate, which is then fed into the next iteration, and so on.",
                    "label": 1
                },
                {
                    "sent": "So these guys work together well for men.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many years.",
                    "label": 0
                },
                {
                    "sent": "But actually we and other people before us have noted that there is a problem there.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, we are occupied in instances when we do have data from both target and source site.",
                    "label": 0
                },
                {
                    "sent": "So there is not nothing that.",
                    "label": 0
                },
                {
                    "sent": "Makes us do the estimative process in One Direction from target to source, whereas we can also do it the other way around, reversing the roles of the two languages.",
                    "label": 0
                },
                {
                    "sent": "In the case of machine translation or in the same way in every case that the noises channel is used.",
                    "label": 0
                },
                {
                    "sent": "And if we do that so work one way and the other, and we go and focus a little bit on the joint probability level, then be cause of course.",
                    "label": 0
                },
                {
                    "sent": "Data is that we have available for training is not perfect, so the language model here is usually are weak and the data we have available is of course not infinite.",
                    "label": 0
                },
                {
                    "sent": "What we will see is that this joint probability estimates that we get by performing estimation in One Direction and the other they are not the same.",
                    "label": 0
                },
                {
                    "sent": "So given that fact, the next question is, should we really care?",
                    "label": 0
                },
                {
                    "sent": "I mean, as I mentioned, people have been using that for a long time and they didn't seem to notice that.",
                    "label": 0
                },
                {
                    "sent": "Well, actually we do care because there is empirical evidence from the world of machine translation.",
                    "label": 1
                },
                {
                    "sent": "That it is actually better to combine both directions So what people have already done is performance dimension in One Direction in the other direction and using ad hoc methods for their specific task they were occupied with.",
                    "label": 1
                },
                {
                    "sent": "They saw that by combining the output of the two estimated directions, they actually could get better accuracy in whatever task they were performing.",
                    "label": 0
                },
                {
                    "sent": "So accepting this fact, the next question is, can we actually do that in a principled manner, and not one that is specific to a particular problem and we would like to do that through the usually and process that people are already familiar with, and that it is well studied.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our paper we are concerned with exactly that problem and we have formulated what we call the bidirectionally EM algorithm or by insert.",
                    "label": 0
                },
                {
                    "sent": "And as you can see from the figure above.",
                    "label": 0
                },
                {
                    "sent": "It follows again the workflow of the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "But there are some important differences.",
                    "label": 0
                },
                {
                    "sent": "So first, in order to reconcile the two estimative directions, we work with one single set of parameters, which is the joint probability parameters.",
                    "label": 0
                },
                {
                    "sent": "End.",
                    "label": 0
                },
                {
                    "sent": "The next important thing is that during the eastep we read data from both source and target side at the same time.",
                    "label": 0
                },
                {
                    "sent": "So using a bidirectional mapping from source to target and the other way around, we generate for each source sentence all possible target hypothesis and again for its target sentence that we read all possible sentence hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an indication, this mapping in a very very very simplistic translation model, Waterworld model could be, for example, a lexicon.",
                    "label": 0
                },
                {
                    "sent": "Or much more sophisticated things in better today's models.",
                    "label": 0
                },
                {
                    "sent": "After we do that, we use two language models.",
                    "label": 0
                },
                {
                    "sent": "Once from source and one for the target site and the joint probability estimate to assign probabilities to each and everyone of these pairs and then have a single complete data corpus that spans over both source and target.",
                    "label": 1
                },
                {
                    "sent": "And it's exactly on this single complete data corpus that we perform the maximum likelihood estimation of the M step to get a new joint probability estimate that is fed in the next iteration and so on.",
                    "label": 0
                },
                {
                    "sent": "So what we do?",
                    "label": 0
                },
                {
                    "sent": "Get out of this is that during the EM step we climb the joint likelihood of both source and target training corpora at the same time.",
                    "label": 0
                },
                {
                    "sent": "So all the available data that we have for training.",
                    "label": 0
                },
                {
                    "sent": "And you can find more information about how this work in our paper and what's interesting is that we developed it from first principles with no particular assumptions on how the data should look like for it, for it to work, and we saw that it's actually a true VM instance, which means that it inherits the good properties of them are going through like the guaranteed to converge, and the guarantee to non decrease the likelihood of your training data.",
                    "label": 1
                },
                {
                    "sent": "And next we show empirical evidence at.",
                    "label": 0
                },
                {
                    "sent": "It's actually better than using Union from 2 distinct NLP tasks.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first set of experimental results is from the example that I've used all this way, so machine translation we translate from German to English noun sequences, and we do that using non parallel corpora.",
                    "label": 1
                },
                {
                    "sent": "So just one piece of English 1, one side and one piece of German text on the other side with no interconnections between them at all.",
                    "label": 0
                },
                {
                    "sent": "So not non parallel corpora and in between we use Alexa on ambiguous lesson from German to English.",
                    "label": 0
                },
                {
                    "sent": "Of which we want to estimate their translation probabilities.",
                    "label": 0
                },
                {
                    "sent": "In this way, we follow an experimental platform from Kona Night by 2000 who were occupied with this exact problem.",
                    "label": 0
                },
                {
                    "sent": "So their model was assuming same translated sentence length and know what movement you can see it on, how the China model decomposes into here.",
                    "label": 0
                },
                {
                    "sent": "And the reason we chose to do that is because these people use the unidirectionally M, so the simple am to perform these tasks they got really encouraging results and we wanted to see if the bidirectional emko actually do any better.",
                    "label": 0
                },
                {
                    "sent": "So I'm there pickle questions that were occupied with our three.",
                    "label": 0
                },
                {
                    "sent": "The first one is what happens when you feed your estimated processes with more data.",
                    "label": 0
                },
                {
                    "sent": "Can you get better accuracy?",
                    "label": 0
                },
                {
                    "sent": "And how do the unidirectional bidirectional EM algorithm fair on that?",
                    "label": 0
                },
                {
                    "sent": "And there are two more questions that are directly linked with the bidirectional nature of the Byam algorithm, which is what happens when the data that you place on the source and the target side come from different domains, does it affect the estimated process?",
                    "label": 0
                },
                {
                    "sent": "And the second question was what happens when you have a lot more data on one side than you have on the other.",
                    "label": 0
                },
                {
                    "sent": "Again, does this cause a problem in the estimated output?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as you can see from the tables here.",
                    "label": 0
                },
                {
                    "sent": "On the 1st task we use closely related corpora and as we feed more sentences into the training algorithms in each and every case, the word level, translation accuracy and the blame metrics.",
                    "label": 0
                },
                {
                    "sent": "You get a considerable performance advantage when using by in relation to the unidirectional M estimation.",
                    "label": 0
                },
                {
                    "sent": "But also when we move to less related corpora.",
                    "label": 0
                },
                {
                    "sent": "So corpora that come from more different.",
                    "label": 0
                },
                {
                    "sent": "Collections of tasks text.",
                    "label": 0
                },
                {
                    "sent": "Then again, we see that even though as the task becomes harder so the overall accuracy of both methods drops by me still able to give us in return a performance advantage over the unidirectional estimation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "On the next slide, I could buy a Toyota by yourself with a third question, which is what happens when there is an imbalance on the amount of data that you have available for training on source and target side.",
                    "label": 0
                },
                {
                    "sent": "So in this graph.",
                    "label": 0
                },
                {
                    "sent": "On the Y axis I plot the accuracy game and on the X axis I plot the number of sentences that we used.",
                    "label": 0
                },
                {
                    "sent": "On the second direction, while we give the number of sentences on the primary estimated direction fixed to 100,000.",
                    "label": 0
                },
                {
                    "sent": "So zero here means that no data is used in the second direction, which is unidirectional am.",
                    "label": 0
                },
                {
                    "sent": "So what's surprising for us when we noted this chart was that you can actually get most of your performance advantage right away when using a little data on the second side.",
                    "label": 0
                },
                {
                    "sent": "And this gives an incentive to researchers to actually use bidirectional estimation, even though when data might be sparse on the second side of the noisy channel model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When I move on to our second set of experiments which was occupied with adaptation of POS tagger, so a post algorithm mechanism design, part of speech tags to the words of a sentence.",
                    "label": 0
                },
                {
                    "sent": "So if every word is a variable, a noun or an adjective in etc an.",
                    "label": 0
                },
                {
                    "sent": "With adaptation we mean that what we actually want to do is get a post Tiger that was trained.",
                    "label": 0
                },
                {
                    "sent": "From 1880, so sure.",
                    "label": 0
                },
                {
                    "sent": "It's language such as modern standard Arabic and port it to a related but resource for 11 languages and see if we can make it have also get a good accuracy performance there.",
                    "label": 0
                },
                {
                    "sent": "The training corpora that we used was on one side.",
                    "label": 0
                },
                {
                    "sent": "We have tagged modern standard Arabic text and on the other side you have a small eleventeen text only, which means unlabeled text corpus.",
                    "label": 0
                },
                {
                    "sent": "And in between we use a tiny lexecon with around 300 Michael Pyle entries that Maps MSA and postdoc pairs 211 words and these tasks in Joyce.",
                    "label": 0
                },
                {
                    "sent": "Big amount of data symmetry becausw on one side.",
                    "label": 0
                },
                {
                    "sent": "You have a very large corpus.",
                    "label": 0
                },
                {
                    "sent": "Another side you have corpus one order of magnitude smaller and also the nature of the two corpora is different.",
                    "label": 0
                },
                {
                    "sent": "So one side you have attacked Corpus another side you have text only corpus, so from the first row here you can see the incentive to actually perform adaptation.",
                    "label": 0
                },
                {
                    "sent": "So the POS tagger which.",
                    "label": 0
                },
                {
                    "sent": "Achieves around 96% accuracy on the language that it has been trained on.",
                    "label": 0
                },
                {
                    "sent": "Drops to around 70% when you port it to related language then.",
                    "label": 0
                },
                {
                    "sent": "And the next two rows we can announce that by using the unidirectional estimation makes sense using the direction am to adapt the target to this problem and also from the last row we can see that bidirectional estimation gives a small but considerable performance advantage.",
                    "label": 0
                },
                {
                    "sent": "On top of that.",
                    "label": 0
                },
                {
                    "sent": "Again, for this second set of experiments.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To wrap up and leave some time for questions in our paper, we present and avail bidirectional algorithm and we do that working from first principles.",
                    "label": 1
                },
                {
                    "sent": "We show that it's an actual EM algorithm instances, which means that it inherits the well known properties of the amalgam.",
                    "label": 0
                },
                {
                    "sent": "Anything to say a wide range of applications and namely whenever the noisy tunnel is applicable and whenever data from both sides is available.",
                    "label": 1
                },
                {
                    "sent": "With this often the case and we have also shown it that it actually outperforms the standard unidirectional estimation that people were using in two distinct NLP tasks.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So in this in the bidirectional estimation you have parameters for PFT and for PAS for target and source.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and also parameters going for each of the conditionals.",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "Can you explain exactly how the show on another effort to perform something similar for machine translation?",
                    "label": 0
                },
                {
                    "sent": "People you have actually used two sets of conditional parameters, so a distinct set from SD and this thing set from T2, S?",
                    "label": 0
                },
                {
                    "sent": "And then they added in the likelihood that they were they wanted to raise an additional term that tried to somehow reconcile them along the way.",
                    "label": 0
                },
                {
                    "sent": "So in our case we use one single set of parameters, which is a joint probabilities and with.",
                    "label": 0
                },
                {
                    "sent": "For it's estimated directions, we derive the conditional probabilities from this single set of ramps.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Have a space which is the space of meaning and the two different languages are two different sorts of sentence is actually being generated by this.",
                    "label": 0
                },
                {
                    "sent": "Is it because the set of creating the space of meaning is difficult because with you?",
                    "label": 0
                },
                {
                    "sent": "Better problem that conditions are not generating the same.",
                    "label": 0
                },
                {
                    "sent": "What do you mean by space of meaning?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Meaning.",
                    "label": 0
                },
                {
                    "sent": "Different, so I think you mean a single source, yeah?",
                    "label": 0
                },
                {
                    "sent": "Anyway, even though in when we go in process its pair of sentence and hypothesis, we use the conditional probabilities derived from the joint probabilities.",
                    "label": 0
                },
                {
                    "sent": "The fact that we use one set of joint probabilities for its pair of of source and target somehow hints intuitively that there is a single source that actually outputs both of them in the at the same time.",
                    "label": 0
                },
                {
                    "sent": "So in a way we actually kind of do it.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "OK thanks Michael.",
                    "label": 0
                }
            ]
        }
    }
}