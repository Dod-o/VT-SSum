{
    "id": "zzs7znu455wnx4esw2etf2zolitani4p",
    "title": "Hierarchical spike coding of sound",
    "info": {
        "author": [
            "Yan Karklin, Center for Neural Science, New York University (NYU)"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Speech Analysis"
        ]
    },
    "url": "http://videolectures.net/machine_karklin_sound/",
    "segmentation": [
        [
            "So if you heard Steven Millott talk about talk yesterday, he motivated our problem very well.",
            "How do you learn complex structure in signals like natural sounds that are inherently variable?",
            "So here's a simple of speech, and in a spectrogram you see repeated patterns.",
            "There's periods of silence and voicing this harmonic stacks, but also significant variability.",
            "For example, in the pitch in the onset in the precise shape of the acoustic features.",
            "So how do we learn the structure from data?",
            "And how do we form representations that can handle this?",
            "Variability, so in most previous work, the first step is to start with a signal to sound and convert it into spectrogram.",
            "However, this transform loses precise temporal information, and it suffers from a time frequency resolution tradeoff and also doesn't provide a probabilistic model in the original signal space, making difficult to do time domain tasks like denoising and source separation.",
            "Instead, we're going to start with a recently proposed generative models sound called despite Gram and extended hierarchically.",
            "So a spike gram is a very sparse collection of coefficients laid out in time and frequency, and these are obtained by doing inference in a sparse linear generative model.",
            "So each coefficient or spike corresponds to the placement in the original signal over shortwave form that at the time and the frequency indicated by the spike.",
            "So you can think of this loosely as a kind of very sparse version of a spectrogram, and you can see the features are quite similar, but they're much better defined in time into frequency.",
            "So to model structure in a spike gram, we could just divide it into temporal blocks and try to learn features, but patterns wherever they exist might not necessary line up with those blocks and you get artifacts, and learning is difficult instead."
        ],
        [
            "We'd like to do is identify features that could appear at any offset in time and frequency."
        ],
        [
            "So here's a basic diagram of the model we developed called hierarchal Spike coding and from the bottom up we assume that spikes in a spike gram are generated as an inhomogeneous person process.",
            "The log rate of this person process is given.",
            "By none observed rate map and we model the rate map as a superposition of high order kernels.",
            "The position in time and frequency of which are given by a second spiking representation receiving sparse in the first layer spike ground.",
            "So, given speech data encoded into spy grams, our goal then is to do is to infer the second layer representation, the spikes, and to learn model parameters including the rate kernels.",
            "So I'm not going to go into details of inference and learning.",
            "You can come to the poster for that, but trained on corpus of natural speech."
        ],
        [
            "The model learns a variety of kernels which include harmonics, tax, sharp onsets, frequency sweeps and high frequency bursts.",
            "So these kernels are not occurring in isolation.",
            "Multiple kernels get composed to describe complex phonetic structure and the relative time and frequency alignment of these kernels captures the variability in speaker an utterance.",
            "So because the model is a probabilistic, defines a probability probability distribution original space.",
            "We can synthesize directly from the model, and we can apply to tasks like denoising.",
            "So we show that the model outperforms standard methods on denoising and what's interesting is that the model does especially well when signal is corrupted with nonstationary noise, which is a challenge but also more realistic setting.",
            "So our posters tonight and I'm going to let our model conclude.",
            "Tiger.",
            "Alright."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you heard Steven Millott talk about talk yesterday, he motivated our problem very well.",
                    "label": 0
                },
                {
                    "sent": "How do you learn complex structure in signals like natural sounds that are inherently variable?",
                    "label": 0
                },
                {
                    "sent": "So here's a simple of speech, and in a spectrogram you see repeated patterns.",
                    "label": 0
                },
                {
                    "sent": "There's periods of silence and voicing this harmonic stacks, but also significant variability.",
                    "label": 0
                },
                {
                    "sent": "For example, in the pitch in the onset in the precise shape of the acoustic features.",
                    "label": 0
                },
                {
                    "sent": "So how do we learn the structure from data?",
                    "label": 0
                },
                {
                    "sent": "And how do we form representations that can handle this?",
                    "label": 0
                },
                {
                    "sent": "Variability, so in most previous work, the first step is to start with a signal to sound and convert it into spectrogram.",
                    "label": 0
                },
                {
                    "sent": "However, this transform loses precise temporal information, and it suffers from a time frequency resolution tradeoff and also doesn't provide a probabilistic model in the original signal space, making difficult to do time domain tasks like denoising and source separation.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're going to start with a recently proposed generative models sound called despite Gram and extended hierarchically.",
                    "label": 0
                },
                {
                    "sent": "So a spike gram is a very sparse collection of coefficients laid out in time and frequency, and these are obtained by doing inference in a sparse linear generative model.",
                    "label": 0
                },
                {
                    "sent": "So each coefficient or spike corresponds to the placement in the original signal over shortwave form that at the time and the frequency indicated by the spike.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this loosely as a kind of very sparse version of a spectrogram, and you can see the features are quite similar, but they're much better defined in time into frequency.",
                    "label": 0
                },
                {
                    "sent": "So to model structure in a spike gram, we could just divide it into temporal blocks and try to learn features, but patterns wherever they exist might not necessary line up with those blocks and you get artifacts, and learning is difficult instead.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We'd like to do is identify features that could appear at any offset in time and frequency.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a basic diagram of the model we developed called hierarchal Spike coding and from the bottom up we assume that spikes in a spike gram are generated as an inhomogeneous person process.",
                    "label": 0
                },
                {
                    "sent": "The log rate of this person process is given.",
                    "label": 0
                },
                {
                    "sent": "By none observed rate map and we model the rate map as a superposition of high order kernels.",
                    "label": 1
                },
                {
                    "sent": "The position in time and frequency of which are given by a second spiking representation receiving sparse in the first layer spike ground.",
                    "label": 0
                },
                {
                    "sent": "So, given speech data encoded into spy grams, our goal then is to do is to infer the second layer representation, the spikes, and to learn model parameters including the rate kernels.",
                    "label": 1
                },
                {
                    "sent": "So I'm not going to go into details of inference and learning.",
                    "label": 0
                },
                {
                    "sent": "You can come to the poster for that, but trained on corpus of natural speech.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The model learns a variety of kernels which include harmonics, tax, sharp onsets, frequency sweeps and high frequency bursts.",
                    "label": 1
                },
                {
                    "sent": "So these kernels are not occurring in isolation.",
                    "label": 0
                },
                {
                    "sent": "Multiple kernels get composed to describe complex phonetic structure and the relative time and frequency alignment of these kernels captures the variability in speaker an utterance.",
                    "label": 0
                },
                {
                    "sent": "So because the model is a probabilistic, defines a probability probability distribution original space.",
                    "label": 0
                },
                {
                    "sent": "We can synthesize directly from the model, and we can apply to tasks like denoising.",
                    "label": 0
                },
                {
                    "sent": "So we show that the model outperforms standard methods on denoising and what's interesting is that the model does especially well when signal is corrupted with nonstationary noise, which is a challenge but also more realistic setting.",
                    "label": 0
                },
                {
                    "sent": "So our posters tonight and I'm going to let our model conclude.",
                    "label": 0
                },
                {
                    "sent": "Tiger.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        }
    }
}