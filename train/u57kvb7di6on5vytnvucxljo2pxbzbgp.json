{
    "id": "u57kvb7di6on5vytnvucxljo2pxbzbgp",
    "title": "vbFRET: A Bayesian Approach to Single-Molecule Forster Resonance Energy Transfer Analysis",
    "info": {
        "author": [
            "Jonathan Eiseman Bronson, Department of Chemistry, Columbia University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning",
            "Top->Computer Science->Computational Biology"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_bronson_basm/",
    "segmentation": [
        [
            "So that lets you follow individual proteins or molecules on a glass slide as they move around in space."
        ],
        [
            "And this is a relatively new field.",
            "It will first started in 1990, but it's been growing exponentially.",
            "So five years ago people could publish maybe a dozen time series and that would be a full data set.",
            "But now if you want to do some real experiments, you need to have several 100 or several thousand time series, and so you need some kind of automated technique for going through and looking at the time series and learning from the time series.",
            "Copies of the same interaction over and over again?",
            "Yeah, so basically you can make a glass slide that has 300 individual molecules on it and then you just image all 300 of them in parallel and then you get 300 or however many of those time series.",
            "You're getting a lot of measurements in this evening, yeah?"
        ],
        [
            "Exactly and then the other problem is a lot of these measurements aren't very long because it turns out that dies oxidized and then explode after a few seconds, and so you can't take very, very long time series, so you have to take many in parallel and then combine that information.",
            "And so basically what the experimentalists want to learn are how many molecular confirmations are there in the data.",
            "For instance, in this DNA, does it just stay in the zip state and the unzipped state or does it do something in the middle along the way?",
            "And once you know how many states are in the data, then what's the transition transition rates between the states?",
            "What are the rate constants?",
            "How can you describe the kinetics of the system an related to that is can you take these noisy signals and then turn them into very clean signals?",
            "And that's just finding the Viterbi path through the noisy data.",
            "And the reason that they want to do that last cleaning step is because that's actually how they calculate the rate constants."
        ],
        [
            "Which may or may not be the best way of doing things, but for now that's how it's done in literature, and so that's our starting point.",
            "But basically, once you take the noisy signals, you turn into clean signals, then you can define a duration of being in state one or duration of being in state two.",
            "You can plot a dwell time histogram, and you can extract a kinetic rate constant from that, and so that's the basic goal of doing the inference on this threat data.",
            "And people have been looking at this problem for a few years now."
        ],
        [
            "In 2003, Talaga proposed that it would be a good idea to model these time series using hidden Markov models an, which makes sense because the basic idea would be that at each time step you have an observable noisy signal, which you can assume to be Gaussian noise and that noisy signal is really a function of the hidden variable, which is whether or not the protein is closed or opened or twisted or whatever it's confirmation is.",
            "So.",
            "And the probability that it stays in its current configuration depends on what it's currently doing.",
            "So if it's currently closed, it's probably still going to be closed.",
            "If it's currently open, it's probably still going to be open, so it makes sense to think of these things as hidden Markov models and then the other thing is that the data is naturally timed and just based on the way the cameras work, so it makes sense to talk about discrete time steps going from one step to the other to the other.",
            "And a few years ago."
        ],
        [
            "TJ Ha and his coworkers.",
            "Proposed that they use the hidden Markov model and then solve it using expectation maximization and so basically they would look at the data and say it looks like there's three states in this data.",
            "Let's fit it with five and then they would using maximum likelihood.",
            "Just find the parameters that make their data most likely and sometimes in order to figure out how many states that were in the data, they would plot these things called.",
            "Transition density plots where you just look at the transition, the fret before in the fret after and based on visualizing each transition they would say oh, it looks like there's six states in this data, or three states in this data.",
            "It's not very rigorous and it's very subjective, so we thought we would try and do something that had a little bit more mathematical rigor to it.",
            "And there's two other.",
            "You know there's two classic problems using just a Mac."
        ],
        [
            "Likelihood approach.",
            "The one problem is there's no way of figuring how many states are in your data because the more complex your model, the better you fit your data, and so if you say there's 10 states in the data, you're always going to get a better fit than even.",
            "Say there's five States and then the other problem is if you have a mixture model, which we do here and it has continuous outputs, then you can assume that one state is just one data point, and that state ends up having an infinite likelihood because the variance of the Gaussian top that state goes to 0.",
            "So the probability of that data point goes to Infinity and now you have a divergent solution and you don't have your fit of the data has no value, and so these are sort of the two problems with using just a standard maximum likelihood approach.",
            "That needs to be fixed.",
            "And so one possible fixes.",
            "You can just tack on a penalty term like the BI, See the Bayesian."
        ],
        [
            "Mission criteria and that will solve the overfitting problem, assuming that the data is well behaved enough and the PC actually works, which sometimes it does, and sometimes it doesn't, although it still doesn't fix the problem of having divergent solutions.",
            "And when you have your initializations of them, you have to be really careful about making sure that you don't end up with that version solutions, so it's still not ideal.",
            "You can also do cross validation, which also will fix overfitting, but is slow.",
            "It requires a lot of data.",
            "And you still have the divergance problem.",
            "Oh so if you have any mixture model, so in this case you know we're assuming that the the.",
            "Data can be in one of case States and each state is modeled by a Gaussian.",
            "If I assume that one data point gets its own state and then I say, what is the variance of the Gaussian over one data point, the maximum likelihood solution says that it should have zero variance.",
            "So then when you plug in the likelihood formula you end up.",
            "Yeah you guys think?",
            "Oh yeah, sorry yeah, so you're end up with infinite likelihood and you can't fit your data that way.",
            "So.",
            "Um?",
            "And so the final option is evidence maximization, which fixes the problem of the divergent solutions.",
            "It fixes the overfitting problem, and it just requires slightly more math.",
            "So the basic idea."
        ],
        [
            "Maximum evidence is that you extend maximum likelihood from finding the parameters that make the data most likely for a given model, complexity to just finding the model complexity that makes the data most likely and the way you do that is by assigning a prior to the all of the parameters that you have, and then you integrate out.",
            "Calculation unfortunately, is intractable like our last speaker said.",
            "All good calculations tend to be intractable, and this is one of those.",
            "So the way you get around that is you can bound it and you can bound the LGG evidence and you can show that the test distribution that you want to optimize is optimized.",
            "Once it looks like the posterior probability distribution for your data.",
            "So in other words, by trying to find the maximum evidence solution of your data, you also find the probability of the parameters that were in that data, and so you can simultaneously avoid overfitting.",
            "And you can find the distribution over parameters for your data, which you can then use to get those idealized perturbed paths.",
            "And so."
        ],
        [
            "So we wrote up a program that would do this that is downloadable for experimentalists.",
            "We call it a VB threat since it's variational Bayesian approach to fret analysis.",
            "And.",
            "And then our goal was to compare it to the current accepted practice in the field of using maximum likelihood and see how it does.",
            "So first, just to illustrate, this is something that is very."
        ],
        [
            "Talking to experimental biologists, but I'm sure in this community is not very surprising that when you try and take a time series like this is a synthetic data set that has three states an you fit it with with the maximum evidence approach and you look at the log evidence as you increase the number of states you fit to the data it maximizes at three States and then decreases again as you go to five states.",
            "Whereas if you use the maximum likelihood approach, you start to overfit a little bit and you also increase the likelihood of your data.",
            "By adding too many States and so if you want to know what the complexity of your data is, you just fit between one and however many states you want an using the maximum evidence approach.",
            "You just take the most most probable data set which provides a.",
            "A much more rigorous approach to fitting your data, then saying well, it looks like there's three states, so I should fit 5 to it.",
            "And.",
            "And so then we wanted to validate this using synthetic data and."
        ],
        [
            "Basically we take synthetic time series and just make them increasingly noisy and then we look at the performance of the variational Bayesian approach and the maximum likelihood approach as a function of increasing signal noise.",
            "And we also look at two types of traces, traces that are fast trans."
        ],
        [
            "Listening and traces that are slow transitioning, so are fast.",
            "Transitioning traces transition approximately every three to four times steps and are slow, transitioning traces transition every 15 time steps, and then we looked at how the Viterbi path, the idealized paths that we created, compared to the actual trajectories of the synthetic data.",
            "And so we looked at for metrics accuracy of number of states.",
            "In other words, if there's three states in the data, do you actually find 3 states accuracy of the inferred trajectory?",
            "So how often are you in the correct state in that trajectory?",
            "Sensitivity to transitions?",
            "You know what is the probability that you find a transition given the transition exists and specificity transitions, which is what's the probability that there is that you think there's no transition given that no transition existed, and it turns out specificity is not a very helpful metric because specificity specificity is always very high and it turns out that if you do a really bad job of fitting your data, your specificity actually gets even better.",
            "'cause you just assume that there's no transitions usually.",
            "But when we looked at.",
            "You know how well do you do a pretty number of states?",
            "Obviously the maximum evidence approach does much better because it's designed to do that.",
            "Maximum likelihood is not an when we compare the other metrics.",
            "Accuracy of the trajectory and sensitivity of transitions, we find that remarkably on slow transitioning data.",
            "Basically, either approach is OK, but for the fast transitioning data, the maximum likelihood approach does really badly, and it doesn't even matter what your initializations are.",
            "You can set your initializations to be the actual parameters of the.",
            "The system and it still does badly because because I think because of this divergance problem, because you have to start correcting for the fact that your likelihood is going to Infinity and then you have to do something to fix that.",
            "And by the time the algorithm is done fixing that, it does a bad job of fitting the data.",
            "OK, so."
        ],
        [
            "So then we tried to test this further on real experimental data taken from the ribosome and so basically the idea behind the ribosome is it has two halves and they sit like this and then you take a M RNA and you feed it through the middle and it kind of rotates back and forth while it reads the M RNA and approaching comes at the top.",
            "And there's also a tail in the back of the of the ribosome.",
            "Which during this rotation process or flutters open and closed, and so the experimental biologists we were working with are really interested in the kinetics of this tale movement during protein translation and the thing that's really nice about their data is they've been studying this for a few years and they've been studying this under a lot of different conditions, so they've attached fret probes to two parts of the ribosome.",
            "They've attached fret probes to the T RNA and to the ribosome, and they've also.",
            "Looked at the process under a lot of different experimental conditions so they have fast transitioning data.",
            "Then they have slow transitioning data and they have redundant data to check, which it makes it a good source for us to do our."
        ],
        [
            "Station on.",
            "And so the first thing we did was look at the number of states we found in the data relative to the number of states that the maximum likelihood approach found in the data, and so maximum likelihood basically finds A2 state system where you transition between one state and the other and we found it looks like a three state system where you basically travel from one state to an intermediate and then back to the other state an.",
            "So we were a little bit confused as to what the difference was, and then we thought about it for a little while and we realized that this data was time binned, and because because it's time bin data, every time the system."
        ],
        [
            "Undergoes a transition from a high state to a low state.",
            "There should be a single data point in the middle that represents the average of the high to low transition and so for some reason the maximum likelihood software wasn't picking this up, but our program was picking this up and then to confirm that it wasn't a real biological intermediate.",
            "We re recorded the data at a slower camera time an at a faster camera time and we basically find that this state exists for the exact same amount of time no matter what.",
            "Whereas if there's sort of real biological immediate if you make the camera speed twice as fast.",
            "Then a real biological event that was lasting one time step should last two time steps now and so.",
            "So in a way, it's very reassuring that we're finding this this fake intermediate.",
            "Which suggests that if there were real biological intermediates, we'd also be able to pick them up in the data, but this is probably just a two state process, so we found that reassuring.",
            "And then then we started looking at once we."
        ],
        [
            "Yeah, we start looking at the actual rate constants between the two real states in the system and basically so these are all ribosomes under various experimental conditions and these are the rate constants observed.",
            "So some of these are slow transitioning.",
            "They have slow rate constants in.",
            "Some of these are fast transitioning, and basically what we find is consistent with the synthetic validation that our results and the maximum likelihood results on the slow transitioning data are basically the same.",
            "But on the fast transition data, our results are significantly different from the experimental results and.",
            "You know, because it's real data, you can't say whether 11 inference method is more accurate than the other, but it's very consistent with our synthetic data, and so we think that also makes some compelling evidence that doing this maximum evidence approach is a better way to go.",
            "So that takes care of some of my experimental conclusions."
        ],
        [
            "The first is that maximum evidence is much better than maximum likelihood model selection.",
            "That maximum evidence is about as good as maximum likelihood for slow transitioning data.",
            "And for fast transitioning data, maximum evidence is much better than maximum likelihood, and we also found this intermediate that was not found in the maximum likelihood data.",
            "The, which again was a artifact in this case, but the program doesn't know that, and so we would think that if there's a real biological norm."
        ],
        [
            "But then we'll be able to find it.",
            "So then we wanted to start continuing this work too.",
            "To extend it to a hierarchical model and one of the problems is an.",
            "This is very recent.",
            "Last two to three weeks, so we're just starting to do this, but one of the problems whenever you do Bayesian inference is how do you pick the prior and we've gone through and check that.",
            "So the way we were doing it in the past is we start with a very non informative prior, so every state starts at five.",
            "Since everything starts, everything can vary between zero and one and they have very nondescript variances.",
            "The transition probabilities also very nondescript and then we go.",
            "And try and learn.",
            "Of course, whenever you do inference, if your prior is better than your inference will be better, because the more you know about your data, the easier it is to learn about your data, and So what we really want to do is take each individual time series and.",
            "Somehow combine that information to learn a prior about what we think our data is, and then use that to re analyze our data and it's a little bit circular, but I think it doesn't have an overfitting problem because instead of trying to learn a set of parameters, you're trying to learn the best parameter distribution from your data and so you can still avoid the overfitting problem and so So what we've done now is do one round of inference where you at where you fit each trace.",
            "And then you go back and you say OK, so we have two state system and the first date.",
            "Sometimes it has a mean of .8, sometimes it's a mean of .1, sometimes it's a mean of .6.",
            "So will say that's you know that state is .8 plus or minus .2 and then you can use that as your prior and do a second round of inference and.",
            "And then when you do that, it turns out that your new set of inference is."
        ],
        [
            "Much more accurate than your old set of inference, so the green is now doing this second round of inference.",
            "Once you've learned a prior from your data and the blue line is the original.",
            "Threat data that we are the maximum evidence approach.",
            "And the place you'd expect this to have the most value is on short traces, because if the data is long and you're looking at."
        ],
        [
            "Each trace individually it's OK, but if the data is spread over a lot of short individual time series then you really need to start combining information from each individual time series.",
            "And so we looked at synthetic traces of decreasing trace length and we find that using this hierarchical approach of learning the prior 1st and then re analyzing the data, we end up doing much better as the traces get shorter and then the final thing that we wanted to look at is changing how you learn the rate constants because it's kind of a.",
            "An arbitrary system.",
            "The way the experimentalists do it by plotting these dwell time."
        ],
        [
            "Histograms and it would make a lot more sense given that we're modeling everything is a hidden Markov model to just learn the transition matrix directly and then use that.",
            "So we so we started looking at how well this approach does at just learning the transition matrix directly.",
            "And we compared the Qu\u00e9bec levler divergance of the actual transition matrix and the one that we've learned, and so we find that using that approach to we also.",
            "As the traces get shorter and shorter, you can still learn the important parameters of the system, but.",
            "But using the approach of looking at individual traces one at a time, it starts to become problematic as things get short, so that's where we're moving now.",
            "I guess just as a final thank you.",
            "So Chris Wiggins is my."
        ],
        [
            "Research advisor Rub\u00e9n Gonz\u00e1lez runs the lab that did all of the ribosome data.",
            "Dave recommends just a professor at Columbia who's provided is helpful conversations.",
            "Ginny did all the experimental work, and Jake was also involved in the early planning, so thank you for your time and I'll take any questions.",
            "Yeah, could you it?",
            "Was there a slide in there with the general function evidence?",
            "The objective function from X methods?",
            "I've never played out of the crates.",
            "Yeah, sorry, OK sure.",
            "Yeah, sure sure, so basically.",
            "So for maximum likelihood, you're just saying what are the parameters.",
            "So pick a model complexity.",
            "I think I have a mixture of three Gaussians or something and then find the parameters for those three Gaussians that make the data set most likely.",
            "Maximum evidence says.",
            "So do that, but assign a distribution to all the parameters of those Gaussians.",
            "So say that each Gaussians mean has some probability.",
            "So I think one Gaussians at 5 plus or minus two, and it has a variance of three plus or minus something and then, but then integrate out over over those parameters.",
            "So then you're left with instead of the probability of the data given your parameters, you're left with the probability of the data given the model sort of, averaged over all possible parameter sets.",
            "Anne.",
            "It's a I think it's a trick that's about 10 years old and machine learning now.",
            "And if you have any questions about any more of the details, I'm be happy to talk to you about it afterwards.",
            "Other questions, yeah.",
            "Can I ask how appropriate?",
            "Yeah, so the agent I guess your model seems there.",
            "Molecules are discrete states, it does.",
            "Yeah, I guess if you have.",
            "Yeah so.",
            "Yeah, so this would not be a good technique for for looking at a.",
            "A protein that's just in an unfolded state and moving around with no order.",
            "This is only a useful technique if you have something like an enzyme that's going through three catalytic steps, or a ribosome that's going from open to closed, or DNA that zipping or unzipping.",
            "So you have to be careful of the problem that you use for this technique, but there's a lot of problems where proteins exist in multiple district discreet confirmations, so that's a good point."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that lets you follow individual proteins or molecules on a glass slide as they move around in space.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a relatively new field.",
                    "label": 0
                },
                {
                    "sent": "It will first started in 1990, but it's been growing exponentially.",
                    "label": 0
                },
                {
                    "sent": "So five years ago people could publish maybe a dozen time series and that would be a full data set.",
                    "label": 0
                },
                {
                    "sent": "But now if you want to do some real experiments, you need to have several 100 or several thousand time series, and so you need some kind of automated technique for going through and looking at the time series and learning from the time series.",
                    "label": 0
                },
                {
                    "sent": "Copies of the same interaction over and over again?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so basically you can make a glass slide that has 300 individual molecules on it and then you just image all 300 of them in parallel and then you get 300 or however many of those time series.",
                    "label": 0
                },
                {
                    "sent": "You're getting a lot of measurements in this evening, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exactly and then the other problem is a lot of these measurements aren't very long because it turns out that dies oxidized and then explode after a few seconds, and so you can't take very, very long time series, so you have to take many in parallel and then combine that information.",
                    "label": 0
                },
                {
                    "sent": "And so basically what the experimentalists want to learn are how many molecular confirmations are there in the data.",
                    "label": 1
                },
                {
                    "sent": "For instance, in this DNA, does it just stay in the zip state and the unzipped state or does it do something in the middle along the way?",
                    "label": 1
                },
                {
                    "sent": "And once you know how many states are in the data, then what's the transition transition rates between the states?",
                    "label": 1
                },
                {
                    "sent": "What are the rate constants?",
                    "label": 0
                },
                {
                    "sent": "How can you describe the kinetics of the system an related to that is can you take these noisy signals and then turn them into very clean signals?",
                    "label": 0
                },
                {
                    "sent": "And that's just finding the Viterbi path through the noisy data.",
                    "label": 0
                },
                {
                    "sent": "And the reason that they want to do that last cleaning step is because that's actually how they calculate the rate constants.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which may or may not be the best way of doing things, but for now that's how it's done in literature, and so that's our starting point.",
                    "label": 0
                },
                {
                    "sent": "But basically, once you take the noisy signals, you turn into clean signals, then you can define a duration of being in state one or duration of being in state two.",
                    "label": 0
                },
                {
                    "sent": "You can plot a dwell time histogram, and you can extract a kinetic rate constant from that, and so that's the basic goal of doing the inference on this threat data.",
                    "label": 0
                },
                {
                    "sent": "And people have been looking at this problem for a few years now.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In 2003, Talaga proposed that it would be a good idea to model these time series using hidden Markov models an, which makes sense because the basic idea would be that at each time step you have an observable noisy signal, which you can assume to be Gaussian noise and that noisy signal is really a function of the hidden variable, which is whether or not the protein is closed or opened or twisted or whatever it's confirmation is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And the probability that it stays in its current configuration depends on what it's currently doing.",
                    "label": 0
                },
                {
                    "sent": "So if it's currently closed, it's probably still going to be closed.",
                    "label": 0
                },
                {
                    "sent": "If it's currently open, it's probably still going to be open, so it makes sense to think of these things as hidden Markov models and then the other thing is that the data is naturally timed and just based on the way the cameras work, so it makes sense to talk about discrete time steps going from one step to the other to the other.",
                    "label": 0
                },
                {
                    "sent": "And a few years ago.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "TJ Ha and his coworkers.",
                    "label": 0
                },
                {
                    "sent": "Proposed that they use the hidden Markov model and then solve it using expectation maximization and so basically they would look at the data and say it looks like there's three states in this data.",
                    "label": 0
                },
                {
                    "sent": "Let's fit it with five and then they would using maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Just find the parameters that make their data most likely and sometimes in order to figure out how many states that were in the data, they would plot these things called.",
                    "label": 0
                },
                {
                    "sent": "Transition density plots where you just look at the transition, the fret before in the fret after and based on visualizing each transition they would say oh, it looks like there's six states in this data, or three states in this data.",
                    "label": 0
                },
                {
                    "sent": "It's not very rigorous and it's very subjective, so we thought we would try and do something that had a little bit more mathematical rigor to it.",
                    "label": 0
                },
                {
                    "sent": "And there's two other.",
                    "label": 0
                },
                {
                    "sent": "You know there's two classic problems using just a Mac.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Likelihood approach.",
                    "label": 0
                },
                {
                    "sent": "The one problem is there's no way of figuring how many states are in your data because the more complex your model, the better you fit your data, and so if you say there's 10 states in the data, you're always going to get a better fit than even.",
                    "label": 0
                },
                {
                    "sent": "Say there's five States and then the other problem is if you have a mixture model, which we do here and it has continuous outputs, then you can assume that one state is just one data point, and that state ends up having an infinite likelihood because the variance of the Gaussian top that state goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So the probability of that data point goes to Infinity and now you have a divergent solution and you don't have your fit of the data has no value, and so these are sort of the two problems with using just a standard maximum likelihood approach.",
                    "label": 0
                },
                {
                    "sent": "That needs to be fixed.",
                    "label": 0
                },
                {
                    "sent": "And so one possible fixes.",
                    "label": 0
                },
                {
                    "sent": "You can just tack on a penalty term like the BI, See the Bayesian.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission criteria and that will solve the overfitting problem, assuming that the data is well behaved enough and the PC actually works, which sometimes it does, and sometimes it doesn't, although it still doesn't fix the problem of having divergent solutions.",
                    "label": 0
                },
                {
                    "sent": "And when you have your initializations of them, you have to be really careful about making sure that you don't end up with that version solutions, so it's still not ideal.",
                    "label": 0
                },
                {
                    "sent": "You can also do cross validation, which also will fix overfitting, but is slow.",
                    "label": 0
                },
                {
                    "sent": "It requires a lot of data.",
                    "label": 0
                },
                {
                    "sent": "And you still have the divergance problem.",
                    "label": 1
                },
                {
                    "sent": "Oh so if you have any mixture model, so in this case you know we're assuming that the the.",
                    "label": 0
                },
                {
                    "sent": "Data can be in one of case States and each state is modeled by a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If I assume that one data point gets its own state and then I say, what is the variance of the Gaussian over one data point, the maximum likelihood solution says that it should have zero variance.",
                    "label": 0
                },
                {
                    "sent": "So then when you plug in the likelihood formula you end up.",
                    "label": 0
                },
                {
                    "sent": "Yeah you guys think?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, sorry yeah, so you're end up with infinite likelihood and you can't fit your data that way.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so the final option is evidence maximization, which fixes the problem of the divergent solutions.",
                    "label": 1
                },
                {
                    "sent": "It fixes the overfitting problem, and it just requires slightly more math.",
                    "label": 1
                },
                {
                    "sent": "So the basic idea.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maximum evidence is that you extend maximum likelihood from finding the parameters that make the data most likely for a given model, complexity to just finding the model complexity that makes the data most likely and the way you do that is by assigning a prior to the all of the parameters that you have, and then you integrate out.",
                    "label": 0
                },
                {
                    "sent": "Calculation unfortunately, is intractable like our last speaker said.",
                    "label": 0
                },
                {
                    "sent": "All good calculations tend to be intractable, and this is one of those.",
                    "label": 0
                },
                {
                    "sent": "So the way you get around that is you can bound it and you can bound the LGG evidence and you can show that the test distribution that you want to optimize is optimized.",
                    "label": 0
                },
                {
                    "sent": "Once it looks like the posterior probability distribution for your data.",
                    "label": 0
                },
                {
                    "sent": "So in other words, by trying to find the maximum evidence solution of your data, you also find the probability of the parameters that were in that data, and so you can simultaneously avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "And you can find the distribution over parameters for your data, which you can then use to get those idealized perturbed paths.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we wrote up a program that would do this that is downloadable for experimentalists.",
                    "label": 0
                },
                {
                    "sent": "We call it a VB threat since it's variational Bayesian approach to fret analysis.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And then our goal was to compare it to the current accepted practice in the field of using maximum likelihood and see how it does.",
                    "label": 0
                },
                {
                    "sent": "So first, just to illustrate, this is something that is very.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talking to experimental biologists, but I'm sure in this community is not very surprising that when you try and take a time series like this is a synthetic data set that has three states an you fit it with with the maximum evidence approach and you look at the log evidence as you increase the number of states you fit to the data it maximizes at three States and then decreases again as you go to five states.",
                    "label": 1
                },
                {
                    "sent": "Whereas if you use the maximum likelihood approach, you start to overfit a little bit and you also increase the likelihood of your data.",
                    "label": 0
                },
                {
                    "sent": "By adding too many States and so if you want to know what the complexity of your data is, you just fit between one and however many states you want an using the maximum evidence approach.",
                    "label": 0
                },
                {
                    "sent": "You just take the most most probable data set which provides a.",
                    "label": 0
                },
                {
                    "sent": "A much more rigorous approach to fitting your data, then saying well, it looks like there's three states, so I should fit 5 to it.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And so then we wanted to validate this using synthetic data and.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically we take synthetic time series and just make them increasingly noisy and then we look at the performance of the variational Bayesian approach and the maximum likelihood approach as a function of increasing signal noise.",
                    "label": 0
                },
                {
                    "sent": "And we also look at two types of traces, traces that are fast trans.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Listening and traces that are slow transitioning, so are fast.",
                    "label": 0
                },
                {
                    "sent": "Transitioning traces transition approximately every three to four times steps and are slow, transitioning traces transition every 15 time steps, and then we looked at how the Viterbi path, the idealized paths that we created, compared to the actual trajectories of the synthetic data.",
                    "label": 0
                },
                {
                    "sent": "And so we looked at for metrics accuracy of number of states.",
                    "label": 1
                },
                {
                    "sent": "In other words, if there's three states in the data, do you actually find 3 states accuracy of the inferred trajectory?",
                    "label": 0
                },
                {
                    "sent": "So how often are you in the correct state in that trajectory?",
                    "label": 0
                },
                {
                    "sent": "Sensitivity to transitions?",
                    "label": 0
                },
                {
                    "sent": "You know what is the probability that you find a transition given the transition exists and specificity transitions, which is what's the probability that there is that you think there's no transition given that no transition existed, and it turns out specificity is not a very helpful metric because specificity specificity is always very high and it turns out that if you do a really bad job of fitting your data, your specificity actually gets even better.",
                    "label": 0
                },
                {
                    "sent": "'cause you just assume that there's no transitions usually.",
                    "label": 0
                },
                {
                    "sent": "But when we looked at.",
                    "label": 0
                },
                {
                    "sent": "You know how well do you do a pretty number of states?",
                    "label": 0
                },
                {
                    "sent": "Obviously the maximum evidence approach does much better because it's designed to do that.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood is not an when we compare the other metrics.",
                    "label": 1
                },
                {
                    "sent": "Accuracy of the trajectory and sensitivity of transitions, we find that remarkably on slow transitioning data.",
                    "label": 0
                },
                {
                    "sent": "Basically, either approach is OK, but for the fast transitioning data, the maximum likelihood approach does really badly, and it doesn't even matter what your initializations are.",
                    "label": 0
                },
                {
                    "sent": "You can set your initializations to be the actual parameters of the.",
                    "label": 0
                },
                {
                    "sent": "The system and it still does badly because because I think because of this divergance problem, because you have to start correcting for the fact that your likelihood is going to Infinity and then you have to do something to fix that.",
                    "label": 0
                },
                {
                    "sent": "And by the time the algorithm is done fixing that, it does a bad job of fitting the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we tried to test this further on real experimental data taken from the ribosome and so basically the idea behind the ribosome is it has two halves and they sit like this and then you take a M RNA and you feed it through the middle and it kind of rotates back and forth while it reads the M RNA and approaching comes at the top.",
                    "label": 0
                },
                {
                    "sent": "And there's also a tail in the back of the of the ribosome.",
                    "label": 0
                },
                {
                    "sent": "Which during this rotation process or flutters open and closed, and so the experimental biologists we were working with are really interested in the kinetics of this tale movement during protein translation and the thing that's really nice about their data is they've been studying this for a few years and they've been studying this under a lot of different conditions, so they've attached fret probes to two parts of the ribosome.",
                    "label": 0
                },
                {
                    "sent": "They've attached fret probes to the T RNA and to the ribosome, and they've also.",
                    "label": 0
                },
                {
                    "sent": "Looked at the process under a lot of different experimental conditions so they have fast transitioning data.",
                    "label": 0
                },
                {
                    "sent": "Then they have slow transitioning data and they have redundant data to check, which it makes it a good source for us to do our.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Station on.",
                    "label": 0
                },
                {
                    "sent": "And so the first thing we did was look at the number of states we found in the data relative to the number of states that the maximum likelihood approach found in the data, and so maximum likelihood basically finds A2 state system where you transition between one state and the other and we found it looks like a three state system where you basically travel from one state to an intermediate and then back to the other state an.",
                    "label": 0
                },
                {
                    "sent": "So we were a little bit confused as to what the difference was, and then we thought about it for a little while and we realized that this data was time binned, and because because it's time bin data, every time the system.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Undergoes a transition from a high state to a low state.",
                    "label": 0
                },
                {
                    "sent": "There should be a single data point in the middle that represents the average of the high to low transition and so for some reason the maximum likelihood software wasn't picking this up, but our program was picking this up and then to confirm that it wasn't a real biological intermediate.",
                    "label": 0
                },
                {
                    "sent": "We re recorded the data at a slower camera time an at a faster camera time and we basically find that this state exists for the exact same amount of time no matter what.",
                    "label": 0
                },
                {
                    "sent": "Whereas if there's sort of real biological immediate if you make the camera speed twice as fast.",
                    "label": 0
                },
                {
                    "sent": "Then a real biological event that was lasting one time step should last two time steps now and so.",
                    "label": 0
                },
                {
                    "sent": "So in a way, it's very reassuring that we're finding this this fake intermediate.",
                    "label": 0
                },
                {
                    "sent": "Which suggests that if there were real biological intermediates, we'd also be able to pick them up in the data, but this is probably just a two state process, so we found that reassuring.",
                    "label": 0
                },
                {
                    "sent": "And then then we started looking at once we.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, we start looking at the actual rate constants between the two real states in the system and basically so these are all ribosomes under various experimental conditions and these are the rate constants observed.",
                    "label": 0
                },
                {
                    "sent": "So some of these are slow transitioning.",
                    "label": 0
                },
                {
                    "sent": "They have slow rate constants in.",
                    "label": 0
                },
                {
                    "sent": "Some of these are fast transitioning, and basically what we find is consistent with the synthetic validation that our results and the maximum likelihood results on the slow transitioning data are basically the same.",
                    "label": 0
                },
                {
                    "sent": "But on the fast transition data, our results are significantly different from the experimental results and.",
                    "label": 1
                },
                {
                    "sent": "You know, because it's real data, you can't say whether 11 inference method is more accurate than the other, but it's very consistent with our synthetic data, and so we think that also makes some compelling evidence that doing this maximum evidence approach is a better way to go.",
                    "label": 0
                },
                {
                    "sent": "So that takes care of some of my experimental conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first is that maximum evidence is much better than maximum likelihood model selection.",
                    "label": 1
                },
                {
                    "sent": "That maximum evidence is about as good as maximum likelihood for slow transitioning data.",
                    "label": 1
                },
                {
                    "sent": "And for fast transitioning data, maximum evidence is much better than maximum likelihood, and we also found this intermediate that was not found in the maximum likelihood data.",
                    "label": 1
                },
                {
                    "sent": "The, which again was a artifact in this case, but the program doesn't know that, and so we would think that if there's a real biological norm.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then we'll be able to find it.",
                    "label": 0
                },
                {
                    "sent": "So then we wanted to start continuing this work too.",
                    "label": 0
                },
                {
                    "sent": "To extend it to a hierarchical model and one of the problems is an.",
                    "label": 0
                },
                {
                    "sent": "This is very recent.",
                    "label": 0
                },
                {
                    "sent": "Last two to three weeks, so we're just starting to do this, but one of the problems whenever you do Bayesian inference is how do you pick the prior and we've gone through and check that.",
                    "label": 0
                },
                {
                    "sent": "So the way we were doing it in the past is we start with a very non informative prior, so every state starts at five.",
                    "label": 0
                },
                {
                    "sent": "Since everything starts, everything can vary between zero and one and they have very nondescript variances.",
                    "label": 0
                },
                {
                    "sent": "The transition probabilities also very nondescript and then we go.",
                    "label": 0
                },
                {
                    "sent": "And try and learn.",
                    "label": 0
                },
                {
                    "sent": "Of course, whenever you do inference, if your prior is better than your inference will be better, because the more you know about your data, the easier it is to learn about your data, and So what we really want to do is take each individual time series and.",
                    "label": 0
                },
                {
                    "sent": "Somehow combine that information to learn a prior about what we think our data is, and then use that to re analyze our data and it's a little bit circular, but I think it doesn't have an overfitting problem because instead of trying to learn a set of parameters, you're trying to learn the best parameter distribution from your data and so you can still avoid the overfitting problem and so So what we've done now is do one round of inference where you at where you fit each trace.",
                    "label": 0
                },
                {
                    "sent": "And then you go back and you say OK, so we have two state system and the first date.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it has a mean of .8, sometimes it's a mean of .1, sometimes it's a mean of .6.",
                    "label": 0
                },
                {
                    "sent": "So will say that's you know that state is .8 plus or minus .2 and then you can use that as your prior and do a second round of inference and.",
                    "label": 0
                },
                {
                    "sent": "And then when you do that, it turns out that your new set of inference is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much more accurate than your old set of inference, so the green is now doing this second round of inference.",
                    "label": 0
                },
                {
                    "sent": "Once you've learned a prior from your data and the blue line is the original.",
                    "label": 0
                },
                {
                    "sent": "Threat data that we are the maximum evidence approach.",
                    "label": 0
                },
                {
                    "sent": "And the place you'd expect this to have the most value is on short traces, because if the data is long and you're looking at.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each trace individually it's OK, but if the data is spread over a lot of short individual time series then you really need to start combining information from each individual time series.",
                    "label": 0
                },
                {
                    "sent": "And so we looked at synthetic traces of decreasing trace length and we find that using this hierarchical approach of learning the prior 1st and then re analyzing the data, we end up doing much better as the traces get shorter and then the final thing that we wanted to look at is changing how you learn the rate constants because it's kind of a.",
                    "label": 0
                },
                {
                    "sent": "An arbitrary system.",
                    "label": 0
                },
                {
                    "sent": "The way the experimentalists do it by plotting these dwell time.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Histograms and it would make a lot more sense given that we're modeling everything is a hidden Markov model to just learn the transition matrix directly and then use that.",
                    "label": 0
                },
                {
                    "sent": "So we so we started looking at how well this approach does at just learning the transition matrix directly.",
                    "label": 0
                },
                {
                    "sent": "And we compared the Qu\u00e9bec levler divergance of the actual transition matrix and the one that we've learned, and so we find that using that approach to we also.",
                    "label": 0
                },
                {
                    "sent": "As the traces get shorter and shorter, you can still learn the important parameters of the system, but.",
                    "label": 0
                },
                {
                    "sent": "But using the approach of looking at individual traces one at a time, it starts to become problematic as things get short, so that's where we're moving now.",
                    "label": 0
                },
                {
                    "sent": "I guess just as a final thank you.",
                    "label": 0
                },
                {
                    "sent": "So Chris Wiggins is my.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Research advisor Rub\u00e9n Gonz\u00e1lez runs the lab that did all of the ribosome data.",
                    "label": 0
                },
                {
                    "sent": "Dave recommends just a professor at Columbia who's provided is helpful conversations.",
                    "label": 0
                },
                {
                    "sent": "Ginny did all the experimental work, and Jake was also involved in the early planning, so thank you for your time and I'll take any questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, could you it?",
                    "label": 0
                },
                {
                    "sent": "Was there a slide in there with the general function evidence?",
                    "label": 0
                },
                {
                    "sent": "The objective function from X methods?",
                    "label": 0
                },
                {
                    "sent": "I've never played out of the crates.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry, OK sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure sure, so basically.",
                    "label": 0
                },
                {
                    "sent": "So for maximum likelihood, you're just saying what are the parameters.",
                    "label": 0
                },
                {
                    "sent": "So pick a model complexity.",
                    "label": 0
                },
                {
                    "sent": "I think I have a mixture of three Gaussians or something and then find the parameters for those three Gaussians that make the data set most likely.",
                    "label": 0
                },
                {
                    "sent": "Maximum evidence says.",
                    "label": 0
                },
                {
                    "sent": "So do that, but assign a distribution to all the parameters of those Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So say that each Gaussians mean has some probability.",
                    "label": 0
                },
                {
                    "sent": "So I think one Gaussians at 5 plus or minus two, and it has a variance of three plus or minus something and then, but then integrate out over over those parameters.",
                    "label": 0
                },
                {
                    "sent": "So then you're left with instead of the probability of the data given your parameters, you're left with the probability of the data given the model sort of, averaged over all possible parameter sets.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "It's a I think it's a trick that's about 10 years old and machine learning now.",
                    "label": 0
                },
                {
                    "sent": "And if you have any questions about any more of the details, I'm be happy to talk to you about it afterwards.",
                    "label": 0
                },
                {
                    "sent": "Other questions, yeah.",
                    "label": 0
                },
                {
                    "sent": "Can I ask how appropriate?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the agent I guess your model seems there.",
                    "label": 0
                },
                {
                    "sent": "Molecules are discrete states, it does.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess if you have.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this would not be a good technique for for looking at a.",
                    "label": 0
                },
                {
                    "sent": "A protein that's just in an unfolded state and moving around with no order.",
                    "label": 0
                },
                {
                    "sent": "This is only a useful technique if you have something like an enzyme that's going through three catalytic steps, or a ribosome that's going from open to closed, or DNA that zipping or unzipping.",
                    "label": 0
                },
                {
                    "sent": "So you have to be careful of the problem that you use for this technique, but there's a lot of problems where proteins exist in multiple district discreet confirmations, so that's a good point.",
                    "label": 0
                }
            ]
        }
    }
}