{
    "id": "ahz4zhby2om3hbsumqqmkhbxbkmexy7y",
    "title": "Topic Significance Ranking of LDA Generative Models",
    "info": {
        "author": [
            "Loulwah AlSumait, Kuwait University"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_alsumait_tsrlda/",
    "segmentation": [
        [
            "Good afternoon, my name is Louis Smith and I'll present to you the work part of the work I did for my thesis.",
            "Which is topic significance?",
            "Ranking of LDA generative modeling sorry for the Miss typing my own title.",
            "I did this work with number of my committee members and.",
            "I have graduate."
        ],
        [
            "In August.",
            "1st to start with an introduction and then I'll go and define the junk in significance topic definitions that I have introduced in the paper and the distance measures are used.",
            "In the approach I used to rank the topics, which is a four levels of weighted combination.",
            "Approach then giving some excellent results and conclusions with future works."
        ],
        [
            "K. So again, for people who are not familiar with Leighton Deuschle topic model, it was presented by playing his colleagues in 2003.",
            "It's a 3 level hierarchical Bayesian network that represent the generative probabilistic model of a corpus of documents.",
            "It represents documents.",
            "Up, sorry.",
            "The documents.",
            "As a mixture of topics.",
            "And the topics.",
            "As a mixture of words.",
            "And the relationship between or multinomial distribution of rewards.",
            "The relationship between the document and the word is not directly.",
            "It is governed by another latent variable, Z, which corresponds to the topic word assignments.",
            "So it corresponds to the responsibility of a particular topic in generating that particular word in that particular document.",
            "So the power of LDA is by introducing late richley priors.",
            "Over the model, which enables the model to.",
            "We enabled this the model to model A unseen documents as well.",
            "Exacting first of the hidden variables is not intractable, so since 2003 many approximation approaches were introduced, including variational inference, Gibbs sampling and others.",
            "In general, what I'm concerned with is that the model takes the K the number of components, the number of topics as an input, and in addition to the what I know about the the distribution of the topic and it gives as an output the topic distribution and the document distribution of these over these."
        ],
        [
            "Topics so there is a critical effect of the setting of the number of components.",
            "It directly affect the distribution of the discovered underlying structure of the topics, and I provide this example from the 20 newsgroups.",
            "This is the topic discovered by LDA from the 20 newsgroups and you can see this topic corresponds to over the 1st three topics.",
            "Corresponds to classes and known classes from the 20 newsgroup.",
            "The first one is about the coffee.",
            "The second is about the ships, the third is about the oil and crude and so on.",
            "However, it also has have another set of topics that are meaningless, like this one.",
            "It has all the verbs and this has all the numbers and so on and all the people who work on LDA.",
            "We do a manual check be after the run of LDA to select some legitimate topics to report for our experiments.",
            "So in my paper I present the 1st at least as as far as I know, the first automated unsupervised approach.",
            "Post analysis for the LDA topic model in order to distinguish.",
            "Good or legitimate topics from this junk or insignificant topics?",
            "The basic idea is to quantify the semantic significance of a topic by.",
            "Measuring the difference of this discovered topic distribution from a well known.",
            "Junk insignificant topic distributions.",
            "That's the basic idea and what gives power to this idea is the exactly the analysis I I found from analyzing the 20 newsgroups.",
            "For example, if X denote the density of the topic, so you can, you can see 100% of the topic density falls in."
        ],
        [
            "Half of the dictionary.",
            "And.",
            "75% of the dictionary that if I consider all that, all the topics.",
            "If I remove the topics that I can see they are junk like the examples I showed you earlier.",
            "This number drops to 3%.",
            "Or let's say nearly 4%.",
            "Of the total dictionary and 46% of the number of total number of documents.",
            "So this gives the.",
            "The motive to use."
        ],
        [
            "Some junk, insignificant topic definitions and see how far is this.",
            "The generated topic is from these, the definitions, and by this I can measure its significance.",
            "So in my paper I introduced 3.",
            "Ji definitions, the first is the uniform distribution over words.",
            "When writing a paper and author usually use a specific set or pool of dictionary that corresponds to the topics that he's writing about.",
            "So in this sense, the topic is usually skewed toward a small set of the dictionary, rather than having all the words involved in that topic.",
            "So basically this is the first definition and the extreme case the uniform distribution over the whole dictionary is my first definition.",
            "Of a junk topic, the second topic distribution is the vacuous semantic distribution.",
            "In this sense.",
            "I'm sorry.",
            "So in this sense, the.",
            "If I considered the whole empirical distribution of the of the whole data set, I will end up having a non nonsense distribution.",
            "But if I take the whole distribution as a whole as a whole.",
            "However, if I if I generate the topics that could construct this.",
            "This the underneath this data set that generated this data set, each specific topic has an identity, so summing over all these topics will is another definition of a junk topic, and that's the second definition I use here.",
            "It's basically the sum.",
            "Of all the distributions of the topic distributions, and that's what they call the empirical distribution of the the data set itself Ji have 3.",
            "Measures of the GI top, the GI topic.",
            "It's the uniformity of a topic, and this is the donation of that.",
            "Measure and the vacuousness of the topic, which is denoted by V sub K and the background of the topics denoted by the B sub K. So, given these three topic definitions.",
            "If I use these to compute these and compute the distance from these topics to the discovered topics, I can have some sense of how good this topic or how specific this topic."
        ],
        [
            "Is.",
            "So I use three distance measures.",
            "This semantical divergance, the cosine and the coefficient correlation and I will end up having nine measurements.",
            "That give an information about how the goodness of the topic so under each distance measure, I will have 3.",
            "Measurements.",
            "A total of nine measurements at the end, so I need to combine these midterms into one score about the topic significance.",
            "And the basic idea uses the from the multi criterion weighted decision making."
        ],
        [
            "This isn't strategy, which is basically taking each score waited, waited the score by some specific weight, and then combining all the scores.",
            "The weighted scores in order to get some rank or score of final score.",
            "So in my paper I use for phase.",
            "A weighted combination.",
            "At the 1st, at the very beginning, I need to standardize these measurements in order to eliminate the differences between the age distance range.",
            "You can combine them.",
            "Without, regardless of the range of each distance.",
            "So in order to unify or to preserve the relative distances instead of the actual values, I need to do a standardization, apply standardization procedure.",
            "And in my paper I use two centralization procedures.",
            "Want to construct this course, and the other is to construct a weights from the same measurements.",
            "So and these are the two centralization procedures, the 1st.",
            "Takes this court and takes.",
            "How the weight of that score, relative with relative to the total scores?",
            "And by this I can.",
            "As I said, I could capture the relative importance between our relative relationship between the distance instead of the actual values while the others is just taking the minimax standardization procedure, and this will result in a 00 IT value between zero and one."
        ],
        [
            "Then I need to combine within each definition of a GI GI to the definition.",
            "I need to combine all the measurements and that's the step of the intra criterion weighted combination in which I combined the standardization measures within each definition and.",
            "Sorry.",
            "And by this I end up with six measurements.",
            "For every definition, one is the weight and one is the actual or the what.",
            "I consider this core and then I combine across the definitions.",
            "I combine these values into a weight and score.",
            "At the end I take the multiplication of this weighted score and I consider this TCR the topic significance rank."
        ],
        [
            "So I first I experimented this approach on a synthetic data and then I fixed the settings of the weights.",
            "Into the my real data here.",
            "This is the data consists of three topics about a River, topic bank and a financial bank topic and the factory topic.",
            "As you can see, there is some words that are common across all the all the topics and there within two topics.",
            "There are also some common words.",
            "In the first experiment I, I ran out the A using different number of cases and in this I report the one with the K = 5.",
            "And you can see how they are ordered according to the to the TCR.",
            "The River Stream is the number one topic and then the factory labor production number 2 and then the money loan depth that the number 3 topic in insignificance.",
            "According to the TCR.",
            "And then comes the two insignificant topics full behind which are the Reporter bank and the bank news topics.",
            "And if you can see that this topic has the most common words, and that's why I mean, it gives sense that it is less important or less significant than the.",
            "Other two I experiment the same with injecting and purpose, injecting some junk topics one is based on the vacuous topic distribution and the 2nd is based on the uniform topic distribution and these fold.",
            "In the last two significa."
        ],
        [
            "Topics in this list and the most more legend topics are on the top.",
            "So based on this, I experimented on the 20 newsgroups and the nipc datasets.",
            "This is the 10 most significant topics according to the TCR TSR.",
            "Since I have the ground truth for the 20 newsgroups, I computed that probabilistic F1 measure and I have assigned.",
            "Etopic a class to that topic if it has.",
            "If it is, if this topic has the highest number of are mapped the highest number of documents of that particular class.",
            "So by this sense I computed probabilistic F1 measure in order just to compare how well I'm doing in my TC TSR.",
            "So if you see the top ten significant topics, six of these 10, both the F1 measure and the TSR.",
            "Have agreed in their judgment about the topic, so most of the topics.",
            "For example, most of the documents where mapped to this from the Christian class were mapped to the topic ID 12 and you can see the distribution does correspond to that class.",
            "And my TSR have assigned it high score.",
            "If we take a look above for the one that does not match, for example.",
            "The one from the graph graphics this this this class was not assigned to the topic.",
            "37 The topics that this class was assigned to another topic according to the F1 measure.",
            "However, if you see the distribution, where is this OK?",
            "This we see this distribution.",
            "It does correspond to a graphic topic, and by this I can see the TSR was has a better judgment about the significance of the topic.",
            "Semantic of the topic.",
            "And the same thing goes for the other.",
            "Examples.",
            "If you look at that lowest 10 significant topics according to my my figure or to my rank, you can see.",
            "The worst five.",
            "They even didn't have an F1.",
            "A significant F1 measure.",
            "So by this I can see they do agree with each other and you can see the distribution does not reflect any legitimate semantic topic.",
            "Looking for example for a topic like.",
            "18 you can see it does correspond to a general terminology from the religion.",
            "Christian class but but but it's not.",
            "As."
        ],
        [
            "Well and as specific as the one that was discovered before.",
            "And it's also by F1 measure it didn't.",
            "It wasn't significant to have been assigned to that class, so in this sense they do agree with each other.",
            "If you look this to this topic 25.",
            "Was assigned by F1 measure to the graphic.",
            "A class, however, you can see the terminology is completely does not correspond to this to that particular class, and my judgment of the TSR TSR judgment is better in this sense."
        ],
        [
            "So for nips.",
            "The same thing can be said about the discovered topics.",
            "You can see the first.",
            "This is the first, the highest end topics according to TSR, and most of them correspond to.",
            "Unknown topics from that area from the neural processing areas like reinforcement learning, neuroscience, learning, neuroscience, and this, speech recognition and so on.",
            "And this is the lowest 10 significant topics and most of them are general topics, terminologies that are used across all the documents in NIPS.",
            "Like algorithm or even the word neural.",
            "It does appear, or training or."
        ],
        [
            "So.",
            "If I can, if I wanna say how well I mean, why should I combine all these measures and does not do not depend on one score or individual score from individual measurement I I introduce here I represent here the the score of the individual individual.",
            "Um?",
            "Measures or individual distances compared to the TSR here the list.",
            "This is on the simulated data and the list is ordered by the Cal diverges measure and you can see how."
        ],
        [
            "The order is does not reflect the true semantic significance of the topic, so combining all these values gives a better indication about the significance.",
            "Instead of using individual judgments.",
            "And this is the same.",
            "Analysis over the 20 newsgroups and you can see just focus on this topic.",
            "You can see this has a high rank under the cosine.",
            "Measure while it does not correspond to a legitimate topic, it is a very.",
            "And I mean background as a background words topic.",
            "The same thing goes for the for this 20 topic 20 which has which is assigned to Mideast politics.",
            "This class and the distribution does correspond to.",
            "I mean to that top to that class, but it was assigned a very high.",
            "Or a higher rank under that individual judgment.",
            "And so on."
        ],
        [
            "So by this I conclude my talk.",
            "I introduced the unsupervised numerical approach to quantify the semantic significance of a topic.",
            "As far as I know, this is the first post analysis on LDA in modeling to do this.",
            "To this, to do this task, it did this by introducing three Ji topic distributions and combine the judgments in a four level of weighted combinations.",
            "Approach.",
            "As a future direction, I intend to analyze the sensitivity of the TSR.",
            "So the approach itself, the combination approach itself and to the setting of K. And to the setting of the weights.",
            "In addition, more definitions of Ji topics is considered.",
            "And to see if it kind of provide a tool to visualize the evolution of a topic in an online setting, that's my token.",
            "Thank you for."
        ],
        [
            "You're for listening.",
            "Or more frequently than more.",
            "So this you're asking if I investigated the relative.",
            "The relative frequency of the topic may be rated according to the document template.",
            "Versus the rock.",
            "The more evolved morning wealthy is our measure.",
            "If the relative frequency of a topic corresponds to the probability of the topic, it does.",
            "It's not biased toward I didn't report this, but it doesn't.",
            "It's not biased toward free more.",
            "More probable topics.",
            "And that's the power I did this analysis in, and I mean I did some work in the last point in which I have an online setting.",
            "So topics that are insignificant in one point, even though sorry topic that are significant in one point, even though they are not important like the neuroscience topic.",
            "I showed you, it has the lowest topic probability.",
            "However it had a high.",
            "As you can see, it was the 2nd.",
            "Nope.",
            "Hypo puppies less.",
            "If it has this background, words like the words that the OR the adverbs or linking verbs or whatever these are actually on are not significant topics.",
            "So even if they are higher probability, I have no problem.",
            "Yeah, but for legit topics.",
            "According to what I have seen so far, the probability of a topic does not affect.",
            "But it needs more scientific analysis, but thank you for that point.",
            "Thanks for help.",
            "Want to know?",
            "Fire.",
            "Visualized actually, as I said, I did this and it is my thesis.",
            "You can see how the topic when it is evolved, how it could increase or decrease in in semantic significance.",
            "In its semantic significance, for example.",
            "For a topic like SVM emerged in NIPS at 1995, when it was first started, it was a weak topic in three documents only so.",
            "It did it.",
            "It was a mixture.",
            "It occupied the topic of Patrick's character recognition, so you can see it did increase the little of the significance of the topic, but the significance was much more increased when the topic was a pure SVM topic at three or four years later.",
            "So in that sense I can see how.",
            "TSR can provide a tool to visualize the evolution of the topic.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon, my name is Louis Smith and I'll present to you the work part of the work I did for my thesis.",
                    "label": 0
                },
                {
                    "sent": "Which is topic significance?",
                    "label": 0
                },
                {
                    "sent": "Ranking of LDA generative modeling sorry for the Miss typing my own title.",
                    "label": 1
                },
                {
                    "sent": "I did this work with number of my committee members and.",
                    "label": 0
                },
                {
                    "sent": "I have graduate.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In August.",
                    "label": 0
                },
                {
                    "sent": "1st to start with an introduction and then I'll go and define the junk in significance topic definitions that I have introduced in the paper and the distance measures are used.",
                    "label": 1
                },
                {
                    "sent": "In the approach I used to rank the topics, which is a four levels of weighted combination.",
                    "label": 0
                },
                {
                    "sent": "Approach then giving some excellent results and conclusions with future works.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K. So again, for people who are not familiar with Leighton Deuschle topic model, it was presented by playing his colleagues in 2003.",
                    "label": 0
                },
                {
                    "sent": "It's a 3 level hierarchical Bayesian network that represent the generative probabilistic model of a corpus of documents.",
                    "label": 0
                },
                {
                    "sent": "It represents documents.",
                    "label": 0
                },
                {
                    "sent": "Up, sorry.",
                    "label": 0
                },
                {
                    "sent": "The documents.",
                    "label": 0
                },
                {
                    "sent": "As a mixture of topics.",
                    "label": 0
                },
                {
                    "sent": "And the topics.",
                    "label": 0
                },
                {
                    "sent": "As a mixture of words.",
                    "label": 0
                },
                {
                    "sent": "And the relationship between or multinomial distribution of rewards.",
                    "label": 0
                },
                {
                    "sent": "The relationship between the document and the word is not directly.",
                    "label": 1
                },
                {
                    "sent": "It is governed by another latent variable, Z, which corresponds to the topic word assignments.",
                    "label": 0
                },
                {
                    "sent": "So it corresponds to the responsibility of a particular topic in generating that particular word in that particular document.",
                    "label": 0
                },
                {
                    "sent": "So the power of LDA is by introducing late richley priors.",
                    "label": 0
                },
                {
                    "sent": "Over the model, which enables the model to.",
                    "label": 0
                },
                {
                    "sent": "We enabled this the model to model A unseen documents as well.",
                    "label": 0
                },
                {
                    "sent": "Exacting first of the hidden variables is not intractable, so since 2003 many approximation approaches were introduced, including variational inference, Gibbs sampling and others.",
                    "label": 1
                },
                {
                    "sent": "In general, what I'm concerned with is that the model takes the K the number of components, the number of topics as an input, and in addition to the what I know about the the distribution of the topic and it gives as an output the topic distribution and the document distribution of these over these.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topics so there is a critical effect of the setting of the number of components.",
                    "label": 1
                },
                {
                    "sent": "It directly affect the distribution of the discovered underlying structure of the topics, and I provide this example from the 20 newsgroups.",
                    "label": 0
                },
                {
                    "sent": "This is the topic discovered by LDA from the 20 newsgroups and you can see this topic corresponds to over the 1st three topics.",
                    "label": 0
                },
                {
                    "sent": "Corresponds to classes and known classes from the 20 newsgroup.",
                    "label": 0
                },
                {
                    "sent": "The first one is about the coffee.",
                    "label": 0
                },
                {
                    "sent": "The second is about the ships, the third is about the oil and crude and so on.",
                    "label": 0
                },
                {
                    "sent": "However, it also has have another set of topics that are meaningless, like this one.",
                    "label": 0
                },
                {
                    "sent": "It has all the verbs and this has all the numbers and so on and all the people who work on LDA.",
                    "label": 0
                },
                {
                    "sent": "We do a manual check be after the run of LDA to select some legitimate topics to report for our experiments.",
                    "label": 0
                },
                {
                    "sent": "So in my paper I present the 1st at least as as far as I know, the first automated unsupervised approach.",
                    "label": 0
                },
                {
                    "sent": "Post analysis for the LDA topic model in order to distinguish.",
                    "label": 0
                },
                {
                    "sent": "Good or legitimate topics from this junk or insignificant topics?",
                    "label": 1
                },
                {
                    "sent": "The basic idea is to quantify the semantic significance of a topic by.",
                    "label": 0
                },
                {
                    "sent": "Measuring the difference of this discovered topic distribution from a well known.",
                    "label": 0
                },
                {
                    "sent": "Junk insignificant topic distributions.",
                    "label": 0
                },
                {
                    "sent": "That's the basic idea and what gives power to this idea is the exactly the analysis I I found from analyzing the 20 newsgroups.",
                    "label": 0
                },
                {
                    "sent": "For example, if X denote the density of the topic, so you can, you can see 100% of the topic density falls in.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Half of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "75% of the dictionary that if I consider all that, all the topics.",
                    "label": 0
                },
                {
                    "sent": "If I remove the topics that I can see they are junk like the examples I showed you earlier.",
                    "label": 0
                },
                {
                    "sent": "This number drops to 3%.",
                    "label": 0
                },
                {
                    "sent": "Or let's say nearly 4%.",
                    "label": 0
                },
                {
                    "sent": "Of the total dictionary and 46% of the number of total number of documents.",
                    "label": 0
                },
                {
                    "sent": "So this gives the.",
                    "label": 0
                },
                {
                    "sent": "The motive to use.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some junk, insignificant topic definitions and see how far is this.",
                    "label": 0
                },
                {
                    "sent": "The generated topic is from these, the definitions, and by this I can measure its significance.",
                    "label": 0
                },
                {
                    "sent": "So in my paper I introduced 3.",
                    "label": 0
                },
                {
                    "sent": "Ji definitions, the first is the uniform distribution over words.",
                    "label": 1
                },
                {
                    "sent": "When writing a paper and author usually use a specific set or pool of dictionary that corresponds to the topics that he's writing about.",
                    "label": 0
                },
                {
                    "sent": "So in this sense, the topic is usually skewed toward a small set of the dictionary, rather than having all the words involved in that topic.",
                    "label": 0
                },
                {
                    "sent": "So basically this is the first definition and the extreme case the uniform distribution over the whole dictionary is my first definition.",
                    "label": 1
                },
                {
                    "sent": "Of a junk topic, the second topic distribution is the vacuous semantic distribution.",
                    "label": 0
                },
                {
                    "sent": "In this sense.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "So in this sense, the.",
                    "label": 0
                },
                {
                    "sent": "If I considered the whole empirical distribution of the of the whole data set, I will end up having a non nonsense distribution.",
                    "label": 0
                },
                {
                    "sent": "But if I take the whole distribution as a whole as a whole.",
                    "label": 0
                },
                {
                    "sent": "However, if I if I generate the topics that could construct this.",
                    "label": 0
                },
                {
                    "sent": "This the underneath this data set that generated this data set, each specific topic has an identity, so summing over all these topics will is another definition of a junk topic, and that's the second definition I use here.",
                    "label": 0
                },
                {
                    "sent": "It's basically the sum.",
                    "label": 0
                },
                {
                    "sent": "Of all the distributions of the topic distributions, and that's what they call the empirical distribution of the the data set itself Ji have 3.",
                    "label": 1
                },
                {
                    "sent": "Measures of the GI top, the GI topic.",
                    "label": 0
                },
                {
                    "sent": "It's the uniformity of a topic, and this is the donation of that.",
                    "label": 0
                },
                {
                    "sent": "Measure and the vacuousness of the topic, which is denoted by V sub K and the background of the topics denoted by the B sub K. So, given these three topic definitions.",
                    "label": 0
                },
                {
                    "sent": "If I use these to compute these and compute the distance from these topics to the discovered topics, I can have some sense of how good this topic or how specific this topic.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "So I use three distance measures.",
                    "label": 1
                },
                {
                    "sent": "This semantical divergance, the cosine and the coefficient correlation and I will end up having nine measurements.",
                    "label": 0
                },
                {
                    "sent": "That give an information about how the goodness of the topic so under each distance measure, I will have 3.",
                    "label": 0
                },
                {
                    "sent": "Measurements.",
                    "label": 0
                },
                {
                    "sent": "A total of nine measurements at the end, so I need to combine these midterms into one score about the topic significance.",
                    "label": 0
                },
                {
                    "sent": "And the basic idea uses the from the multi criterion weighted decision making.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This isn't strategy, which is basically taking each score waited, waited the score by some specific weight, and then combining all the scores.",
                    "label": 0
                },
                {
                    "sent": "The weighted scores in order to get some rank or score of final score.",
                    "label": 0
                },
                {
                    "sent": "So in my paper I use for phase.",
                    "label": 0
                },
                {
                    "sent": "A weighted combination.",
                    "label": 0
                },
                {
                    "sent": "At the 1st, at the very beginning, I need to standardize these measurements in order to eliminate the differences between the age distance range.",
                    "label": 0
                },
                {
                    "sent": "You can combine them.",
                    "label": 0
                },
                {
                    "sent": "Without, regardless of the range of each distance.",
                    "label": 0
                },
                {
                    "sent": "So in order to unify or to preserve the relative distances instead of the actual values, I need to do a standardization, apply standardization procedure.",
                    "label": 0
                },
                {
                    "sent": "And in my paper I use two centralization procedures.",
                    "label": 0
                },
                {
                    "sent": "Want to construct this course, and the other is to construct a weights from the same measurements.",
                    "label": 0
                },
                {
                    "sent": "So and these are the two centralization procedures, the 1st.",
                    "label": 0
                },
                {
                    "sent": "Takes this court and takes.",
                    "label": 0
                },
                {
                    "sent": "How the weight of that score, relative with relative to the total scores?",
                    "label": 0
                },
                {
                    "sent": "And by this I can.",
                    "label": 0
                },
                {
                    "sent": "As I said, I could capture the relative importance between our relative relationship between the distance instead of the actual values while the others is just taking the minimax standardization procedure, and this will result in a 00 IT value between zero and one.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I need to combine within each definition of a GI GI to the definition.",
                    "label": 0
                },
                {
                    "sent": "I need to combine all the measurements and that's the step of the intra criterion weighted combination in which I combined the standardization measures within each definition and.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "And by this I end up with six measurements.",
                    "label": 0
                },
                {
                    "sent": "For every definition, one is the weight and one is the actual or the what.",
                    "label": 0
                },
                {
                    "sent": "I consider this core and then I combine across the definitions.",
                    "label": 0
                },
                {
                    "sent": "I combine these values into a weight and score.",
                    "label": 0
                },
                {
                    "sent": "At the end I take the multiplication of this weighted score and I consider this TCR the topic significance rank.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I first I experimented this approach on a synthetic data and then I fixed the settings of the weights.",
                    "label": 0
                },
                {
                    "sent": "Into the my real data here.",
                    "label": 0
                },
                {
                    "sent": "This is the data consists of three topics about a River, topic bank and a financial bank topic and the factory topic.",
                    "label": 0
                },
                {
                    "sent": "As you can see, there is some words that are common across all the all the topics and there within two topics.",
                    "label": 0
                },
                {
                    "sent": "There are also some common words.",
                    "label": 0
                },
                {
                    "sent": "In the first experiment I, I ran out the A using different number of cases and in this I report the one with the K = 5.",
                    "label": 0
                },
                {
                    "sent": "And you can see how they are ordered according to the to the TCR.",
                    "label": 0
                },
                {
                    "sent": "The River Stream is the number one topic and then the factory labor production number 2 and then the money loan depth that the number 3 topic in insignificance.",
                    "label": 0
                },
                {
                    "sent": "According to the TCR.",
                    "label": 0
                },
                {
                    "sent": "And then comes the two insignificant topics full behind which are the Reporter bank and the bank news topics.",
                    "label": 0
                },
                {
                    "sent": "And if you can see that this topic has the most common words, and that's why I mean, it gives sense that it is less important or less significant than the.",
                    "label": 0
                },
                {
                    "sent": "Other two I experiment the same with injecting and purpose, injecting some junk topics one is based on the vacuous topic distribution and the 2nd is based on the uniform topic distribution and these fold.",
                    "label": 0
                },
                {
                    "sent": "In the last two significa.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topics in this list and the most more legend topics are on the top.",
                    "label": 0
                },
                {
                    "sent": "So based on this, I experimented on the 20 newsgroups and the nipc datasets.",
                    "label": 0
                },
                {
                    "sent": "This is the 10 most significant topics according to the TCR TSR.",
                    "label": 0
                },
                {
                    "sent": "Since I have the ground truth for the 20 newsgroups, I computed that probabilistic F1 measure and I have assigned.",
                    "label": 0
                },
                {
                    "sent": "Etopic a class to that topic if it has.",
                    "label": 0
                },
                {
                    "sent": "If it is, if this topic has the highest number of are mapped the highest number of documents of that particular class.",
                    "label": 0
                },
                {
                    "sent": "So by this sense I computed probabilistic F1 measure in order just to compare how well I'm doing in my TC TSR.",
                    "label": 0
                },
                {
                    "sent": "So if you see the top ten significant topics, six of these 10, both the F1 measure and the TSR.",
                    "label": 0
                },
                {
                    "sent": "Have agreed in their judgment about the topic, so most of the topics.",
                    "label": 0
                },
                {
                    "sent": "For example, most of the documents where mapped to this from the Christian class were mapped to the topic ID 12 and you can see the distribution does correspond to that class.",
                    "label": 0
                },
                {
                    "sent": "And my TSR have assigned it high score.",
                    "label": 0
                },
                {
                    "sent": "If we take a look above for the one that does not match, for example.",
                    "label": 0
                },
                {
                    "sent": "The one from the graph graphics this this this class was not assigned to the topic.",
                    "label": 0
                },
                {
                    "sent": "37 The topics that this class was assigned to another topic according to the F1 measure.",
                    "label": 0
                },
                {
                    "sent": "However, if you see the distribution, where is this OK?",
                    "label": 0
                },
                {
                    "sent": "This we see this distribution.",
                    "label": 0
                },
                {
                    "sent": "It does correspond to a graphic topic, and by this I can see the TSR was has a better judgment about the significance of the topic.",
                    "label": 0
                },
                {
                    "sent": "Semantic of the topic.",
                    "label": 0
                },
                {
                    "sent": "And the same thing goes for the other.",
                    "label": 0
                },
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "If you look at that lowest 10 significant topics according to my my figure or to my rank, you can see.",
                    "label": 1
                },
                {
                    "sent": "The worst five.",
                    "label": 0
                },
                {
                    "sent": "They even didn't have an F1.",
                    "label": 0
                },
                {
                    "sent": "A significant F1 measure.",
                    "label": 0
                },
                {
                    "sent": "So by this I can see they do agree with each other and you can see the distribution does not reflect any legitimate semantic topic.",
                    "label": 0
                },
                {
                    "sent": "Looking for example for a topic like.",
                    "label": 0
                },
                {
                    "sent": "18 you can see it does correspond to a general terminology from the religion.",
                    "label": 0
                },
                {
                    "sent": "Christian class but but but it's not.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well and as specific as the one that was discovered before.",
                    "label": 0
                },
                {
                    "sent": "And it's also by F1 measure it didn't.",
                    "label": 0
                },
                {
                    "sent": "It wasn't significant to have been assigned to that class, so in this sense they do agree with each other.",
                    "label": 0
                },
                {
                    "sent": "If you look this to this topic 25.",
                    "label": 0
                },
                {
                    "sent": "Was assigned by F1 measure to the graphic.",
                    "label": 0
                },
                {
                    "sent": "A class, however, you can see the terminology is completely does not correspond to this to that particular class, and my judgment of the TSR TSR judgment is better in this sense.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for nips.",
                    "label": 0
                },
                {
                    "sent": "The same thing can be said about the discovered topics.",
                    "label": 0
                },
                {
                    "sent": "You can see the first.",
                    "label": 0
                },
                {
                    "sent": "This is the first, the highest end topics according to TSR, and most of them correspond to.",
                    "label": 0
                },
                {
                    "sent": "Unknown topics from that area from the neural processing areas like reinforcement learning, neuroscience, learning, neuroscience, and this, speech recognition and so on.",
                    "label": 0
                },
                {
                    "sent": "And this is the lowest 10 significant topics and most of them are general topics, terminologies that are used across all the documents in NIPS.",
                    "label": 1
                },
                {
                    "sent": "Like algorithm or even the word neural.",
                    "label": 0
                },
                {
                    "sent": "It does appear, or training or.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If I can, if I wanna say how well I mean, why should I combine all these measures and does not do not depend on one score or individual score from individual measurement I I introduce here I represent here the the score of the individual individual.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Measures or individual distances compared to the TSR here the list.",
                    "label": 0
                },
                {
                    "sent": "This is on the simulated data and the list is ordered by the Cal diverges measure and you can see how.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The order is does not reflect the true semantic significance of the topic, so combining all these values gives a better indication about the significance.",
                    "label": 0
                },
                {
                    "sent": "Instead of using individual judgments.",
                    "label": 0
                },
                {
                    "sent": "And this is the same.",
                    "label": 0
                },
                {
                    "sent": "Analysis over the 20 newsgroups and you can see just focus on this topic.",
                    "label": 1
                },
                {
                    "sent": "You can see this has a high rank under the cosine.",
                    "label": 0
                },
                {
                    "sent": "Measure while it does not correspond to a legitimate topic, it is a very.",
                    "label": 0
                },
                {
                    "sent": "And I mean background as a background words topic.",
                    "label": 0
                },
                {
                    "sent": "The same thing goes for the for this 20 topic 20 which has which is assigned to Mideast politics.",
                    "label": 0
                },
                {
                    "sent": "This class and the distribution does correspond to.",
                    "label": 0
                },
                {
                    "sent": "I mean to that top to that class, but it was assigned a very high.",
                    "label": 0
                },
                {
                    "sent": "Or a higher rank under that individual judgment.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So by this I conclude my talk.",
                    "label": 0
                },
                {
                    "sent": "I introduced the unsupervised numerical approach to quantify the semantic significance of a topic.",
                    "label": 0
                },
                {
                    "sent": "As far as I know, this is the first post analysis on LDA in modeling to do this.",
                    "label": 0
                },
                {
                    "sent": "To this, to do this task, it did this by introducing three Ji topic distributions and combine the judgments in a four level of weighted combinations.",
                    "label": 1
                },
                {
                    "sent": "Approach.",
                    "label": 0
                },
                {
                    "sent": "As a future direction, I intend to analyze the sensitivity of the TSR.",
                    "label": 0
                },
                {
                    "sent": "So the approach itself, the combination approach itself and to the setting of K. And to the setting of the weights.",
                    "label": 1
                },
                {
                    "sent": "In addition, more definitions of Ji topics is considered.",
                    "label": 0
                },
                {
                    "sent": "And to see if it kind of provide a tool to visualize the evolution of a topic in an online setting, that's my token.",
                    "label": 0
                },
                {
                    "sent": "Thank you for.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're for listening.",
                    "label": 0
                },
                {
                    "sent": "Or more frequently than more.",
                    "label": 0
                },
                {
                    "sent": "So this you're asking if I investigated the relative.",
                    "label": 0
                },
                {
                    "sent": "The relative frequency of the topic may be rated according to the document template.",
                    "label": 0
                },
                {
                    "sent": "Versus the rock.",
                    "label": 0
                },
                {
                    "sent": "The more evolved morning wealthy is our measure.",
                    "label": 0
                },
                {
                    "sent": "If the relative frequency of a topic corresponds to the probability of the topic, it does.",
                    "label": 0
                },
                {
                    "sent": "It's not biased toward I didn't report this, but it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's not biased toward free more.",
                    "label": 0
                },
                {
                    "sent": "More probable topics.",
                    "label": 0
                },
                {
                    "sent": "And that's the power I did this analysis in, and I mean I did some work in the last point in which I have an online setting.",
                    "label": 0
                },
                {
                    "sent": "So topics that are insignificant in one point, even though sorry topic that are significant in one point, even though they are not important like the neuroscience topic.",
                    "label": 0
                },
                {
                    "sent": "I showed you, it has the lowest topic probability.",
                    "label": 0
                },
                {
                    "sent": "However it had a high.",
                    "label": 0
                },
                {
                    "sent": "As you can see, it was the 2nd.",
                    "label": 0
                },
                {
                    "sent": "Nope.",
                    "label": 0
                },
                {
                    "sent": "Hypo puppies less.",
                    "label": 0
                },
                {
                    "sent": "If it has this background, words like the words that the OR the adverbs or linking verbs or whatever these are actually on are not significant topics.",
                    "label": 0
                },
                {
                    "sent": "So even if they are higher probability, I have no problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but for legit topics.",
                    "label": 0
                },
                {
                    "sent": "According to what I have seen so far, the probability of a topic does not affect.",
                    "label": 0
                },
                {
                    "sent": "But it needs more scientific analysis, but thank you for that point.",
                    "label": 0
                },
                {
                    "sent": "Thanks for help.",
                    "label": 0
                },
                {
                    "sent": "Want to know?",
                    "label": 0
                },
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "Visualized actually, as I said, I did this and it is my thesis.",
                    "label": 0
                },
                {
                    "sent": "You can see how the topic when it is evolved, how it could increase or decrease in in semantic significance.",
                    "label": 0
                },
                {
                    "sent": "In its semantic significance, for example.",
                    "label": 0
                },
                {
                    "sent": "For a topic like SVM emerged in NIPS at 1995, when it was first started, it was a weak topic in three documents only so.",
                    "label": 0
                },
                {
                    "sent": "It did it.",
                    "label": 0
                },
                {
                    "sent": "It was a mixture.",
                    "label": 0
                },
                {
                    "sent": "It occupied the topic of Patrick's character recognition, so you can see it did increase the little of the significance of the topic, but the significance was much more increased when the topic was a pure SVM topic at three or four years later.",
                    "label": 0
                },
                {
                    "sent": "So in that sense I can see how.",
                    "label": 0
                },
                {
                    "sent": "TSR can provide a tool to visualize the evolution of the topic.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}