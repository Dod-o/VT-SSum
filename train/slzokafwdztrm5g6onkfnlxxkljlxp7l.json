{
    "id": "slzokafwdztrm5g6onkfnlxxkljlxp7l",
    "title": "Covariance functions and Bayes errors for GP regression on random graphs",
    "info": {
        "author": [
            "Peter Sollich, King's College London"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Mathematics->Graph Theory"
        ]
    },
    "url": "http://videolectures.net/bark08_sollich_cfabefgr/",
    "segmentation": [
        [
            "OK, thank you Neal.",
            "This talk very is sort of similar in spirit to Magnus talk in the sense that it's not about black box learning more about case studies.",
            "It's really make up your own scenario and then do a case study on it.",
            "So it's like a third flavor and I was trying to explain sort of to use the image of the kitchen to sort of explain that the approach.",
            "So imagine you've got your favorite knife in your base in kitchen, which is GP regression and you know it works well on carrots and tomatoes and lettuce.",
            "And then your friend visited said here.",
            "I have a yam.",
            "Is it going to work on that and you think?",
            "Oh, that's interesting.",
            "Let me have a look and anything.",
            "Actually, I don't really want to get my hands dirty with a real yam, so I'm going to do a calculation on spherical yams where I can workout exactly how the density of the size is going to fix the answer.",
            "OK, sorry.",
            "Well, they've only done the calculation.",
            "I can't taste him anyway, so that's the spirit."
        ],
        [
            "So I do a very short bit about motivation while we're doing this and then talks about two parts.",
            "One is about what properties of covariance functions on graphs which are defined from the graph Laplacian.",
            "I'll deliberative analysis on regular graphs where you can use the tree approximation and talk about the effect of loops.",
            "And the second part I want to look at these errors on these graphs.",
            "I'll use a very simple approximation that's quite old by now.",
            "That month it's worked out and I did some work on as well.",
            "Again will look at the effect of loops and the effect of the kernel parameters."
        ],
        [
            "OK.",
            "So here's the yam slide, as it were.",
            "OK, so there's the tomatoes and carrots over continuous spaces.",
            "GP regression is relatively well understood to month.",
            "It's done quite a lot of work with dirty.",
            "I did some as well, so we know roughly how the learning curves go, for example.",
            "How they depend on the smoothness of the covariance function in the region where the two inputs are similar and things like that?",
            "OK, but there's lots of discrete spaces that come up in many applications, like sequences, strings, all sorts of things.",
            "There's many sort of biological systems when it happens, and so on.",
            "Also, of course, text lots of other things you can see.",
            "I'm a theorist, 'cause I've not listed a lot of examples here, but you can make up your own so the idea of the talk is really just to find out what we can say about GP learning on these discrete spaces.",
            "I'm going to look at random graphs with finite connectivity.",
            "Because I think that's a sort of fairly parity Matic case.",
            "Of course, whether whether real discrete spaces really have the same connectivity as a random graph, that's a different question, but.",
            "That fits this verical yam, right?",
            "So I don't quite know what the real yams are going."
        ],
        [
            "Look like.",
            "OK so just look at covariance functions on."
        ],
        [
            "Graphs that this is just a little bit of introduction that absolutely nothing new here.",
            "OK, so the.",
            "One way of defining probably the most popular one of defining covariance way of defining covariance functions on graphs is based on the graph Laplacian.",
            "So this slide just runs you through how that definition works.",
            "OK, so you characterize your graph by adjacency matrix A and it just is just zero or one, depending on whether there's a link between two nodes.",
            "OK, so if you got a graph with Capital V nodes or vertices then this A is a V bar V matrix.",
            "And we're going to run directed links.",
            "So that means this matrix is symmetric and we're going to assume there's no self loop, so the diagonal elements of that matrix are all zero, OK, and then the degree of a node is just a number of the number of links it has.",
            "So that's the sum over the specific a row or column of the matrix A and we'll call that DI OK. And if you define a diagonal matrix that gathers all the degrees and we call that capital D, then the graph Laplacian is defined as the identity matrix minus.",
            "The the inverse square root of B times the adjacency matrix times the inverse square root.",
            "The inverse square roots there there basically there to make the problem symmetric an I'll say on the next slide what the interpretation of this Alice in terms of random walks where it comes from.",
            "Yeah, so the L Now is also symmetric.",
            "OK, on the based on a random walk, which I'll say next, you normally come up with something that's not symmetric, so these factors of D are really just there to make things symmetric OK, and then there's a nice result from spectral graph theory.",
            "That DL that you've defined in this way it's called Eigen values in the range zero to two.",
            "So like the Laplacian continuous space is positive definite.",
            "What's different here is that this actually about abounded spectrum, so you can't make the eigenvalues arbitrary."
        ],
        [
            "The large and I think intuitively it's something to do with the fact that you're working on discrete space rather than a continuous space.",
            "OK.",
            "Right before I go to the random walk, let me just talk about the standard covariance functions that one can define, and then I'm going to look at here.",
            "OK, so covariance function on a graph, really, it's just it's a V bar V matrix.",
            "But of course we could be very large, so you might still want to think of it as a covariance function.",
            "OK, then the random walk kernel and I explained the name shortly has this form.",
            "You take some constant a -- L raise that to power P. And if you write out the definition of L, then you get the the second form over there where you got a -- 1 times the identity plus the adjacency matrix.",
            "With these factors of degree either side to the power P. And then there's a diffusion kernel, which is like the continuous type continuous of time analog of that one.",
            "As I'll say shortly, we just exponentiate minus Sigma squared over two times the Laplacian.",
            "And actually, if you're not probably not used to seeing the RBF kernel in this way, but you can also write the arbiter standard squared exponential kernel in that way in Fourier space is just E to the minus Sigma squared over 2 * K ^2, which is basically the Laplacian.",
            "If K is a wave later, OK, so just to say that never mind if you just didn't follow what I just said very quickly.",
            "This is like the squared exponential kernel for a graph.",
            "And we'll always assume that these guys are normalized so that they've got.",
            "On average, unit variance for each node.",
            "OK, so the function we're going to look at, I'm just choosing the scales that the scale of the function each node is 1.",
            "Right, so where do these definitions come from and have they got any?"
        ],
        [
            "Meaning.",
            "So if you think of a random walk on a graph.",
            "Then the transition probability matrix for that is this one.",
            "OK?",
            "So if I just spell this out a bit, if I'm making a transition from the OJ to node I OK then what I'm saying here is that OK?",
            "I'm going to choose one of my DJ neighbors 'cause I'm sitting on the J and I've got DJ neighbors OK. And then I'm going to choose one of those DJ nodes to which I'm connected, and you can see that this is normalized device.",
            "Um, over I, I just get the degree DJ and so that just cancel that give me one.",
            "So that's a nice transition probability matrix.",
            "And so if you take S steps over a random walk, then you get obviously the F. The power of this matrix and you can write that in this form of these powers of D if you just rearrange the inverse powers of the degree matrix there, right?",
            "So I've just pulled out, just separated the well.",
            "I've put in a D to the half day to the minus half on the left, and I've separated out the D to the minus one on the right, OK?",
            "Now, if you write out the random walk and I had on the previous slide, let me just go back."
        ],
        [
            "OK, so it looked like that.",
            "So I'm going to normalize this anyway, so let me just divide this through by a little a OK. Then I and then I can just have to sort out this powerpyx there so I get a binomial expansion."
        ],
        [
            "So.",
            "I have a factor of this is the a -- 1 / A I get to the power P -- S that's the number of times I'm not taking a step as it were and then the other factor pre fact the other term at a pre factor of 1 / a I get that to the power S and then I get just this same matrix here to the power is so that's the same matrix that comes up there.",
            "So if you look at this and compare with what you've got up there, then apart from these annoying factors, left and right, which are basically trivial.",
            "What you have here is an average over S&R delivery chosen S, because that's effectively the number of steps in the random walk that you're taking and you're averaging over a binomial distribution of the number of steps that what you would get in the random walk.",
            "OK, so this random walk kernel.",
            "What is doing is saying perform a random walk on the graph OK?",
            "And draw your number of steps from binomial distribution with number of samples P and probability of taking a Step 1 / A OK and the smallest probability you can have their sorry.",
            "The largest one is 1/2 and that's just that comes from the bound on the spectrum of the policy.",
            "And if you make a smaller than that you get into problems.",
            "OK and you can do the same calculation for the diffusion kernel.",
            "OK and you get the same result except that the number of steps in that case comes from across all distribution with Mean Sigma squared over 2.",
            "OK, and then you see that as you make P large.",
            "But keeping P / a constant constant, and in particular equal to Sigma squared over 2, then the diffusion kernel is just the limit with P and a go to Infinity, but the ratio stays constant.",
            "OK, that's just one way of constructing basically continuous time random walk.",
            "You say I'm going to allow lots of steps, but each time I'm only going to take a step with a small probability.",
            "OK, so that means large P and large a sub one over a small.",
            "Is there also a way of sort of going to continuous time but discrete steps?",
            "That's what the diffusion kernel is.",
            "I mean, there's always the discrete structure of the graph is always preserved, right?",
            "Because you're working on a graph, so you've always got a random walk.",
            "But the diffusion kernel is exactly what you just said, so it's basically doing diffusion for a certain amount of time, which is basically the Sigma squared OK. And then seeing how spread out you're currently.",
            "OK, and anything you then doing is you putting in these factors of degree in.",
            "That's just there to make this this overall result to make the C symmetric as it should be for covariance function.",
            "So the summary so far is that you've got these curve."
        ],
        [
            "Nose.",
            "I will just go back to the definitions and the random walk kernel says walk around on the graph randomly.",
            "Take a step with probability at each time step, move with probability 1 / A and do this P times OK and the diffusion step.",
            "The diffusion kernel is the same version of continuous time where you say do this for overtime Sigma squared over 2.",
            "So there's a reasonable ways of generalizing the standard squared exponential kernels to graph.",
            "So of course the square exponential kernel you could just do that, getting that get that by doing diffusion in real space for certain."
        ],
        [
            "Of time."
        ],
        [
            "I'm not quite sure how familiar people are with this stuff.",
            "Obviously interrupt me if I'm not being clear.",
            "I would say this to my students at least in this kind of scenario.",
            "People would actually ask questions if I was being confusing.",
            "It never works in calculus one.",
            "I can tell you.",
            "Right, OK, so we're going to try and understand what these covariance functions do."
        ],
        [
            "On regular graphs, regular graphs are the simplest ones where every node has got the same degree D. OK, so we're going to look at a random graph.",
            "Ensembles I don't want to specify a particular graph, so I'm just going to assume.",
            "A randomly drawn graph with degree D and with the given number of vertices.",
            "Right now I'm assuming I'm uniformly sampling from this graph, so I just put pull one of those graphs out of a bag and then I want to see what does the covariance function look like on that graph.",
            "One thing that's useful about these graphs is that loops are typically long, so if I make the graph big for make very large, then the loop de loop links typically have order log V. Log V isn't terribly big, but if I'm being a mathematician and saying OK, I can make very very large and I can make.",
            "The loops in principle, as long as I want OK.",
            "So locally and graph that I've sampled from this ensemble is tree like OK, and of course that's at the base of all sorts of things like BP.",
            "OK, and the question we want to understand is how do the graph covariance functions behave on graphs like that and what you would think is that if you take many random walk steps so you take Peter Infinity OK, then you should get a flat kernel.",
            "That's what happens in in Euclidean space, right?",
            "In the continuous space you make the Sigma squared larger squared exponential.",
            "It gets the kernel becomes flat.",
            "So we would expect to find that this covariance function CIJ just becomes 1 means the function values are fully correlated across all night.",
            "I wouldn't be going on about this if that wasn't."
        ],
        [
            "See what happened tonight.",
            "So before I show you the answer Livermore how we actually do this.",
            "OK, so on the regulatory.",
            "Basically all nodes are equivalent.",
            "OK, so if you are going now going to ignore the loops.",
            "Just try and focus on a large randomly drawn regular graph which is locally tree like, so we should be able to just throw away the loops and do the analysis like that, OK?",
            "And then the covariance function of the kernel is a function depends only on the distance measured along the graph.",
            "OK, so that's just number of hops between two nodes.",
            "I'm going to call that distance L and now I can explicitly calculate this for the kernels that I've described.",
            "So of course if I'm taking no steps then I have a Delta correlation kernel.",
            "So every function value is only correlated with itself and then to get from PETA people as one would need to take one extra step in random walk.",
            "So just to run you through these two there's a.",
            "Probably the most difficult equations in the talk.",
            "So to get from at position zero, that's my original node.",
            "OK, if I take one extra step, two things can happen.",
            "I'm going to stay where I was.",
            "That happens with probability 1 -- 1 / A or I'm going to get.",
            "Someone I'm going to get the guy who was one one step away, moving back towards the origin.",
            "That happens with probability 1 / A.",
            "D but I've got the neighbors, so there's an extra factor of D up there.",
            "OK.",
            "The one of a D comes from the two powers of the to the minus 1/2 in the way the kernel is constructed.",
            "the D comes just from counting how many neighbors I've got, which are one hop away and remember every node is not exactly the neighbors.",
            "Then I could do the same thing when I'm further away with probability 1 -- 1 / A. I'm looking to move with probability 1 / A. D. I'm going to move from a node that's further in towards my origin.",
            "And with probability 1 / a D, I'm going to move from a node further out, but I've got D -- 1 nodes which are further away.",
            "I've got one note that further in if I'm not at the origin myself and I've got D -- 1 nodes which are further out.",
            "OK, so I've got these simple recursion so I can start from P = 0 and I can just evaluate this.",
            "So that gives me the shape of the covariance function on a regular tree.",
            "So large in principle that should be the large random regular graph.",
            "OK, so we're going to take D = 3, so.",
            "You have three links meeting each node.",
            "I'm going to smallest value of A and just increase P and see what happens."
        ],
        [
            "OK, here's the graph.",
            "So you start with P = 1, in which case you only have correlations out to one step away.",
            "Makes sense OK, then you get P = 2, that's the red line.",
            "You get some problem correlation.",
            "Also two steps, but it's quite small OK and you keep pushing P up OK. Somewhere here is P = 5, then there's P = 10 OK and you keep going further out, but at some point you stop making progress, right?",
            "This is P. Where have I gone this is P = 100 = 500.",
            "That line is P equals Infinity and tell you later how we calculate that OK.",
            "So there's a weird result here that on a regular tree you've taken the obvious limit of making the random walk do as many steps as you can do.",
            "The covariance function doesn't become uniform, it stops there at some finite limiting shape.",
            "You think how come surely you've made a mistake?"
        ],
        [
            "That Peter.",
            "That can't be right.",
            "OK, so let's just try and understand what's going on there.",
            "And one useful way to think about this is rather than think about the covariance function.",
            "You exploit this probability picture and say I'm going to just look at, not probability of being on a specific node, but at the probability of being a certain distance away from origin.",
            "So I'm going to lump all the probabilities at that distance.",
            "That's basically a shell around my node into one probability, and I'm going to call that probability SLP OK, and so at the origin is only one node to lump together, and in the shell distance Alloway.",
            "I've got this many nodes.",
            "So that's just a simple rescaling of the problem.",
            "OK, now I can take my recursion from the previous previous slide, and I can write down what it looks like in terms of these variables.",
            "And what you find.",
            "This is not terribly surprising that the recursion you get just represents a biased random walk with a barrier reflecting barrier at the origin.",
            "So rather than showing you the equation, I've just tried to draw here what the random walk looks like at each time step.",
            "You've got probability of 1 -- 1 / A of not moving, probably moving was 1 / a OK. Well, if you're at the origin, you always have to move out, so you've got probably 1 every day going there.",
            "Every other node you've got probability D -- 1 / 80 of moving out and a smaller probability of moving.",
            "Of course, that makes sense.",
            "You've got a lot more neighbors as you go out.",
            "Then you've got neighbors going in, so the walk is biased, right?",
            "You tend to go away.",
            "You don't tend to come back to the origin if you if you have the origin, you can only ever go out.",
            "OK, so it's a biased random walk, OK, big deal.",
            "Why does that give us these funny covariance funk?"
        ],
        [
            "Who is the random walk?",
            "And I've got different pieces here and different else.",
            "OK, so this is distance measured along the graph and the different curves relate to different numbers of steps of the random walk.",
            "And the obvious thing happens, these probabilities are peak.",
            "These are actually log probabilities, so these are very small.",
            "But that's just show what happen.",
            "I need that to get through the argument with covariance function OK, and basically the peak is sort of in the obvious place, 'cause if you just.",
            "Use the fact that you go outwards with this probability and you go inwards with probability 1 / a D. Then on average you go.",
            "1 / 8 * D -- 2 D In each step Udupi steps.",
            "So the peak is this number, so it goes up linearly with POK and so 4 = 3 = 2.",
            "That number is P / 6.",
            "So if I take 5000 steps then typically I'm about I've got about 800 steps OK or 833 so that all works fine.",
            "It does nothing surprising."
        ],
        [
            "How do you think happens?",
            "Is that what?",
            "Is the effect you get when you convert this back to C, so that was the covariance function that we wanted, right?",
            "So the conversion factor was proportional to D -- 1 power minus one.",
            "OK, just counting how many nodes are in each shell.",
            "OK, so now I'm going to look at this graph, not as things move out, but how these probabilities down here how they move down as the random wall keeps going.",
            "As I make more more steps, I've got a smaller probability of still being at the origin.",
            "OK, now if I convert this back to the covariance function, OK, it turns out that actually what I pick up is this behavior around here?",
            "OK, so the increase in this exponential tail once I've converted it with this simple exponential variation with L is what gives me the decrease of the covariance function asymptotically.",
            "OK, so here over a much larger scale than before, I've drawn you how the.",
            "Covariance function storage should see how that depends on L. Again for increasing, please OK. And over smaller scale, this is what we saw earlier that as you increase P you get to this limiting form OK, where the kernel just stops going to one OK and we can use the behavior near the tail which is known of the biased random walk to workout.",
            "This limiting form and actually has this.",
            "Really rather simple form.",
            "It has a little has a linear dependence on L in here, which is relatively unimportant and otherwise you just decay exponentially with sqrt D -- 1 OK.",
            "So that's how we can calculate what this kernel actually does.",
            "So I'm not sure whether this provides enough intuition, But basically what happens is that because the tree gets increasingly big as you move out OK, the random walk analogy doesn't work right.",
            "This walk is biased, right?",
            "So the idea that the random walk probabilities basically flatten out OK doesn't happen as soon as you've got more than two neighbors, right?",
            "Once you've got two neighbors, then you just keep going out with you never come back, and so this idea that things will eventually diffuse and become smooth doesn't work.",
            "Not on the tree anyway.",
            "What you also see is that if D goes to two.",
            "Which is a chain, in which case you don't have a tree anymore and everything should work fine, gives you the expected result because you get one plus.",
            "Actually, I think there's a typo that distribute the minus two.",
            "This term goes away in this term, becomes one, everything becomes one.",
            "So sorry bout that, there's a."
        ],
        [
            "Misprint there.",
            "Right, so that car."
        ],
        [
            "Happened on real graphs, obviously because there are loops, so that should give us an idea of so this tree behavior that I've just described should apply for awhile and then eventually you should see the fact that the graph isn't actually just a tree and eventually you can go back on yourself and there's a very simple way of estimating when that's going to happen.",
            "You can count the number of nodes in a tree object out, which looks like that OK, and basically you have to have a loop when you've run out of nodes.",
            "OK, so if you got a regular graph of some size and you just put the size in here, then the L can't really be bigger than the L on the right here.",
            "You just don't have enough nodes to make a tree without loops OK?",
            "So the estimate for when the loops kick in is log V over log D -- 1.",
            "Remember, D was the degree of each node, so that's consistent with what I said earlier that the loops are typically of length.",
            "Log me OK, but here you get this extra factor.",
            "This log of D -- 1.",
            "OK, and we know that the random walk thought of a random walk on the graph typically takes P / a steps OK because one over raise the probability of moving.",
            "So this is an estimate for where the where the loop effects kick in.",
            "So we check this.",
            "We measured the correlation across one link in this manner.",
            "So we took real graphs, workout the kernel and then did this normalized correlation function between neighboring nodes."
        ],
        [
            "That should hopefully agree with the theory.",
            "So what you have there is as a function of the number of steps divided by a.",
            "That should be the right variable to look at, at least if both of those things are large and you've got.",
            "Lines there which are the tree theory that I showed you earlier?",
            "OK, so let's look at those first.",
            "So for example, for a = 2, that's more than what we had before.",
            "As I take more and more steps.",
            "Eventually I level off my nearest neighbor.",
            "Correlation does not go to one.",
            "OK, that was the tree with all that showed you earlier.",
            "OK, if I take a = 4, which means I move a little bit more slowly for a given P. Eventually the same thing happens, right?",
            "In fact, as you as I explained earlier, given that for large P and a you ought to get the same answers.",
            "These two curves eventually merge, which is what happened.",
            "'cause that over there on the right is basically the limit where I could equally well just take a diffusion kernel.",
            "Right now the symbols the little circles here are what we measured in the real graphs.",
            "OK, as a function of P array and you see that they follow these tree results very nicely.",
            "OK, so initially the graph readers behave like a tree and the covariance function has to expect to shape.",
            "But then when I put here with an arrow where we expect things to go wrong, some point round here, you do eventually branch off and the correlation goes to one.",
            "OK, so on the real graph because you have loops, eventually the covariance function does become flat, but.",
            "The thing that surprised us at least, is that you need the loops for that, right?",
            "So, unlike lots of things you can do with regular graphs where you can just think about the tree structure right.",
            "With this.",
            "Actually the loops are really important because they're the only thing that eventually makes the covariance function become fully correlated between nodes."
        ],
        [
            "OK, so that's the first.",
            "Half the talk taking off quicker.",
            "So now we want to look at Bayes errors on on random graphs.",
            "Or learning curve so the learning curves are basically in the same spirit that Marcus was talking about.",
            "We're going to average over datasets and look at these errors."
        ],
        [
            "OK.",
            "So of course, as you all know that if you take the generalization error over GP regressor then you can express that in terms of the covariance function for a given data set.",
            "And we're going to call that quantity epsilon.",
            "I'm going to assume.",
            "That we have the correct priors, so I'm going to assume.",
            "This was like the case of Magnus distribution, one that the prior and the thing I'm using for the inference are exactly matched and then epsilon is just the base error and I'm going to use the standard loss for regression which is squared deviation.",
            "OK, I'm going to average over datasets of some given size N and that gives me the learning curve.",
            "So epsilon is a function of North.",
            "And I'm going to make an assumption about how inputs are distributed across the graph, and I'm going to use the simplest case that they're uniformly distributed.",
            "So the learning curve has several parameters that influence it.",
            "Obviously number of data samples I've got size of the graph, the connectivity of the graph, and I'm always going to keep that three to keep things.",
            "Simpler and then the two parameters A&P that specify the kernel and finally the noise on the problem right?",
            "So I'm assuming that's usually called additive noise."
        ],
        [
            "On my data.",
            "So here are some simulation results.",
            "Just to give you some idea what might be going on.",
            "So this is a graph with 500 nodes are random graph, 500 nodes connectivity three and I've taken the one of those random walk kernels with equals 2 and P = 10 OK, and I've plotted the learning curve, the epsilon and I've plotted it with a little bit of four sites not against the wrong number of examples, but the number of examples divided by the size of the graph.",
            "You expect that that's the sort of right quantity that should tell you how well you've learned, because obviously you have to.",
            "More or less cover a good fraction of the graph before the error will go down OK, and then finally there are four curves here for different amounts of noise.",
            "OK, and you see that, for example, if you look at the blue curve very clearly, this has got two different regimes.",
            "First, you gotta relatively fast decay and it crosses over to something much slow on this log log plot.",
            "It's roughly straight here, so that base is just here.",
            "It's a power law here with something faster, OK, and if you then look where these crossovers take place, so the noise level here is 10 to the minus 4.",
            "There so that crossovers round roughly around the point where the error is gone down to off the order of your noise variance, and then if you check that on the other curves again, that's roughly true.",
            "There's 10 to the minus 310 to the minus two, and so on.",
            "So then it gives the cost of two bits to them and initial section before the error gets down to roughly the noise level and then a final section which looks more like a power law.",
            "So we'd like to a little bit of theory."
        ],
        [
            "Explain maybe what's going on there, OK?",
            "And a simple approximation for.",
            "Describing the learning curve is one that's based on the eigenvalues of the Colonel, in this case of a discrete graph, the eigenvalue eigen values are just defined in this way.",
            "So the eigenfunction Phi has a value at each node.",
            "Then averaging over nodes J at this product CI J5 J, you must get back the original function five times, the multiplier Lambda, which is the eigenvalue OK. And then in terms of these lambdas, there's a relatively simple approximation, but turns out to be quite accurate.",
            "Often is that you calculate this function.",
            "Which is basically a trace of the kernel evaluated in this eigenvalue basis.",
            "This is the inverse of the inverse eigenvalue plus some number H and then into this you plug the number of examples divided by the error plus Sigma squared.",
            "OK. That equation isn't yet explicit because I've got epsilon the right and the left only to solve that self consistently.",
            "But once I've got this function, GI could do that.",
            "OK, it's not too difficult, OK?",
            "And the G is always because I'm because of this normalization of the variance of the function of each node that's always normalized at that race.",
            "So the sum of all the eigenvalues equal to 1.",
            "So G of zero is 1.",
            "And you can see already from this that actually this approximation will capture these two regimes that I talked about, 'cause if the error is much smaller than Sigma squared, then basically I can stop solving this thing self consistently.",
            "I could just put N over Sigma squared in.",
            "Here was if there is much bigger than Sigma squared, I could basically drop the Sigma squared and I should get something that doesn't depend on the noise."
        ],
        [
            "OK, now I need to do these eigenvalues.",
            "The good thing is that.",
            "For large graph, the tree approximation, at least for the spectrum, should be good.",
            "OK, and for the tree, for the Laplacian on the tree the eigenvalue density is known and it's got this funny form which I'm not going to read out to you.",
            "It's symmetric around 1:00 and it's bound between zero and two, as you'd hope so.",
            "Or eat.",
            "It's it's known from at least the 1950s.",
            "You can get it very simply with a cavity type trick, so you just say OK, local node and I'm decomposing to all the tree branches.",
            "And use that to calculate the resultant.",
            "Yeah, but there's there's.",
            "There's at least four different derivations of this in the literature.",
            "There's loads OK.",
            "But it has some singularity Zig 0 into.",
            "So if this is I didn't write down the bounds.",
            "If you look at this, this is actually less than one, and it's this is non zero only where the square root is well defined."
        ],
        [
            "So I'll show you the picture there is.",
            "But I might have been better to show that first, actually so.",
            "This is the case of D = 3 and I'm just showing you the prediction for the eigenvalue spectrum with the red curve versus something that we've actually sampled for.",
            "A graph that has 2000 nodes, so that's how it looks like, and for D above 2 is actually bounded away from zero and from two OK."
        ],
        [
            "Talk about these these captions spectrum.",
            "They're not terribly important.",
            "Anyway, so once we've got this guy, so we've got the eigenvalues of the of the tree.",
            "Then of course, we've also got the eigenvalue of the Laplacian and we also got the eigenvalues of the covariance function.",
            "Well, because they're just.",
            "Determined by the way I constructed the kernel, so it's just a minus days plus in eigenvalues Lambda to the power P, and then they're normalized by one of the V which comes from the definition.",
            "OK, and so I'm just going to interfere.",
            "We're just going to use this eigenvalue density and evaluate the approximate learning curves using the equation I had on the previous slide.",
            "OK, and then because of this scaling in the eigenvalues, they direct they depend directly on the density of examples.",
            "So the number of examples per node."
        ],
        [
            "She said, which is a reasonable."
        ],
        [
            "Behave it again, OK?",
            "So here's that, here are the simulations from before, and this time I've added the theory.",
            "That I value it evaluated like in the way I've just described.",
            "And you see that the theory isn't bad in the sense that it picks up the fact that there's two regimes.",
            "It's pretty good in this final regime where you basically learning against the noise, OK?",
            "It's not.",
            "To be accurate in this region where you sort of go between the two.",
            "OK, so the initial decay where you sort of as it were, covering the graph with examples that's not as accurate.",
            "Can be approximated with was much better.",
            "Do you know?",
            "I mean to look at that or people going to haven't managed?",
            "Also there it was.",
            "It was the case that the biggest deviations were around here.",
            "I don't remember quantitatively how big they were."
        ],
        [
            "Yeah.",
            "Right, so this is deliberate.",
            "This graph looks exactly like the next one.",
            "OK, because all I'm trying to say there is that the scaling of the learning curves with number of examples per node is actually very good.",
            "OK, So what you've got on this graph compared to the last one?",
            "You've also got these empty circles, which are the results for a graph with 1000 nodes.",
            "And so that scaling works pretty well.",
            "It wasn't entirely clear to us how how it would how this would work, but it's.",
            "The theory is actually by definition almost has that scaling.",
            "So there's only a single lot of theory curves.",
            "I'm showing this mainly to compare the results for a smaller graph and a larger graph, and you see that the big circles are essentially on top of the small circles.",
            "Have I answered the question?",
            "If I've left Manfred both baffled."
        ],
        [
            "I've done something wrong."
        ],
        [
            "Right, let's quickly look at the effect of loops, so so far I've just used this tree approximation OK, which gives me the scaling of the learning curve with a number of examples per per node, But eventually this must fall over when I make the steps in the number of steps in the random walk large, and eventually if I get to the limit with covariance functions basic uniform, then I've only got 1 value to learn, which gives me the function everywhere.",
            "So the learning curve should just go to this, 'cause that's the primary inverse variance, and that's how much I reduced the inverse variance with each example, OK?",
            "I'm just going to show you one case like that where in the 500 node graph I've taken 200 steps.",
            "OK, so I should be around all the loops by then."
        ],
        [
            "OK, what's quite interesting about that is.",
            "Here's a knife.",
            "This naive estimate that just.",
            "Wrote down that's the blue line you see.",
            "It's not actually terribly good, OK?",
            "And this is in spite of the fact that the largest eigenvalue of this kernel is .994.",
            "Remember, the email is to one.",
            "OK, so it's pretty uniform, right?",
            "But Even so, there's actually a fairly strong deviation from the estimate that you get.",
            "This function really was uniform across the graph, and the theory works pretty well at capturing the contribution from these much smaller eigenvalues.",
            "OK, so the theory is relatively accurate in.",
            "So in this case I've given the theory the true eigenvalues for that particular kernel.",
            "Yeah, that's right, because I can't calculate those using the tree information 'cause this is the.",
            "This is the regime where I've got lots of loops.",
            "OK, so that's why I measured the eigenvalues here and then evaluated the theory.",
            "The what the theory?",
            "I think it does eventually.",
            "I mean it does go to one, it's just it's the same effect as before that basically the crossover onto the noise dominated regime isn't.",
            "That's where the theory is inaccurate.",
            "The noise in this case is .1, so the crossover happens actually already around here.",
            "So this is basically the same effect as you saw before.",
            "OK, that's just."
        ],
        [
            "The theory isn't terribly good.",
            "OK, so just a last thing."
        ],
        [
            "On kernel parameters.",
            "So we can now ask so everything I showed you so far was a = 2 and then a small P 10 and a large P 200.",
            "So if we look at more systematically and what happens as P becomes larger, it seems that the theory does become more accurate, so the gaps between these, the simulations and the.",
            "This is the theory becomes smaller, unfortunately, have no good, no good reason to explain why this would be the case.",
            "That's just an op."
        ],
        [
            "Salvation at this point.",
            "OK. Let's assume that the theory works well for large P OK, and you can work out what it says then and the asymptotic form.",
            "So the tail of the learning curve looks like this.",
            "You've got Sigma squared over density of examples, which is sort of what you would expect.",
            "There are two interesting things.",
            "One is that you've got these smaller, strange looking corrections to that which go like log of the power 3 / 2 of the number of examples.",
            "More importantly, maybe you've gotta factor.",
            "See here, OK which?",
            "Goes like P. The number of steps in the random walk to the power minus 3 / 2.",
            "So if I look at the number, the density of examples I need on the graph to reach a certain error.",
            "As I make P large it still it goes down.",
            "It goes down like P to the minus 3 / 2 just by equating by setting this to some number.",
            "Some epsilon I want to reach.",
            "So then, if you've been awake throughout, which is would be quite an achievement, then you should think hang on, that's strange.",
            "'cause you told us that in the first part that if I make pee large things stop changing, the kernel goes to some shape and it doesn't do anything anymore.",
            "So how come the learning curve if you keep making pelargir?",
            "Get better so you keep going down faster as you make pee larger.",
            "Even though the kernel doesn't seem to change.",
            "So this name and again I don't have an exact explanation for that, but I think what must be happening there is that the kernel of course it only gets turns to constant if you fix the distance you're looking at, and then you make P large.",
            "But if you're looking at distances of the order of P, it will keep changing.",
            "So I think it must be those changes at the larger distances that actually keep decreasing your error in the problem.",
            "Anything?",
            "The eigenvalue come the while the eigenvalue distribution.",
            "So the spectrum of the covariance function.",
            "It's quite it's a subtle question because basically what happens is that the kernel eigenvalues, they all get bunched together.",
            "But there's a sort of we could talk about this separately, 'cause that's quite technical.",
            "What happens is that you get a power lower part of the spectrum with exponent that's very close to minus one.",
            "So actually most of the eigenvalues tend to be extremely small.",
            "OK, and it's sort of an almost non integrable spectrum.",
            "OK, so.",
            "So I think there is a solution to this problem that just means that it's kind of a warning that you have to be slightly careful that even though it looks like the kernel has this limiting shape, there is still effect on the learning."
        ],
        [
            "Case with the pain.",
            "And if you increase a, the theory gets worse.",
            "OK, so here I've gone from equals two 2 = 4, which means I'm making the kernel more short range because I'm taking fewer steps in the random walk.",
            "So the theory here becomes obviously less accurate.",
            "In fact, for 10 to the minus four, but out to like 5000 examples where I can reasonably simulate, I've only just sort of falling back onto the theory.",
            "This theory becomes worse."
        ],
        [
            "And you can actually understand relatively easily how it becomes worse, OK?",
            "In the limit where you take A to Infinity, you got a very simple covariance function because it's just a Delta function, right?",
            "Because you take no steps and in that limit you can evaluate the exact result, which is this averaged over binomial distribution of examples at each node.",
            "And the approximation looks like that.",
            "So the approximation says after I've given every node one example, an average.",
            "I've learned everything perfectly, whereas of course that's not true, 'cause there will be some nodes which haven't had any examples yet.",
            "And if you."
        ],
        [
            "But there's two functions for similar parameters to the ones I had before then, you see that you get exactly this shape of deviation with the approximation underestimates the error.",
            "OK, so it says you're going to learn more quickly because the theory neglect some fluctuations OK."
        ],
        [
            "OK, so let me try and summarize.",
            "In the first part of the talk I've tried to explain that kernels on graphs.",
            "I've got some fairly counterintuitive properties.",
            "In particular, you reached the limit where everything becomes fully correlated.",
            "Because of these loop effects, so you can't, within the tree approximation you get this nontrivial limiting kernel shape.",
            "From we can calculate that from the biased random walk.",
            "If you're in the regime with the loops aren't important, so where P is not too large, then the learning curve scale with.",
            "The density of examples.",
            "So the number of examples per node, and that's relatively reasonable.",
            "'cause imagine this big graph and you're trying to just cover the graph.",
            "We sort of blobs of correlated function values, so you're going to need a number of blobs that proportional to the size of the graph.",
            "For large P do approach this fully correlated limit.",
            "But as I showed you for the one case of P = 200, there's quite a lot of Corrections to that.",
            "The approximation that I showed you works quite well, except when P array is small.",
            "So when the kernel is very short ranged, OK. And in the course of a region, it never works terribly well.",
            "There's some future work that I quite like to do, although this has taken me kind of two years to finish just because I was distracted by other things.",
            "So for the purposes of this meeting, of course we should look at prior mismatch.",
            "We did that in the continuous space, and this might be interesting scenario to look at that in a different scenario.",
            "And there are always other things you could look at other graph structures, maybe taking graph structures from data with graph structures are known the obvious.",
            "Other ones that one could look at from a theoretical point of view, prosen graph where you just throw links down with random with certain probability or small world graphs which might be relevant for things like biology where you've got some heavily connected nodes and some nodes that are weakly connected and I'll leave you with that, thank you.",
            "Question.",
            "So I mean, it seems like small world is can be really hard.",
            "It is difficult to deal with a case where you have.",
            "Weakling.",
            "Have some highly interconnected.",
            "Possibly I'm not.",
            "I haven't thought about the small world case in any detail.",
            "So for example, I don't know what the loop lengths are in the small world graphs, whether there's a regime where there's any sort of reasonable treelike approximation one can use.",
            "I mean, even if that doesn't work, I mean certainly the theoretical approach described.",
            "Would need as input only the eigenvalue density of the Laplacian.",
            "I think there's been at least some work on that on small world graphs, although I don't have the, I don't know what the results actually are, but so the.",
            "For Calkins learning curves, you just need to know the the graph spectrum.",
            "Basically, and that's known for a lot of these graph ensembles, at least to some reasonable approximation.",
            "Several times of random graphs of degree three regular graph.",
            "How did you come from uniform?",
            "Yeah.",
            "Is an algorithm due to people?",
            "Which is known to sample asymptotically uniformly as the number of nodes is large.",
            "So as we're doing relatively large graphs here, that's a fairly good algorithm for sampling from those.",
            "Technically, if you wanted to go away from the simple approximation that you were sketching where you started the eigenvalue distribution, do you think it's possible?",
            "I mean, it seems like it you seem to have a thermodynamic limit type of thing where you introduce a scaling variable.",
            "What was it in over V an?",
            "So have you tried really to go into the replicas and all the no, that's too hard for me, Manfred.",
            "I'll have to get you to do that.",
            "No, the short answer is no.",
            "I haven't tried.",
            "I haven't tried, but it would be interesting to try that.",
            "Thank you Peter again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you Neal.",
                    "label": 0
                },
                {
                    "sent": "This talk very is sort of similar in spirit to Magnus talk in the sense that it's not about black box learning more about case studies.",
                    "label": 0
                },
                {
                    "sent": "It's really make up your own scenario and then do a case study on it.",
                    "label": 0
                },
                {
                    "sent": "So it's like a third flavor and I was trying to explain sort of to use the image of the kitchen to sort of explain that the approach.",
                    "label": 0
                },
                {
                    "sent": "So imagine you've got your favorite knife in your base in kitchen, which is GP regression and you know it works well on carrots and tomatoes and lettuce.",
                    "label": 0
                },
                {
                    "sent": "And then your friend visited said here.",
                    "label": 0
                },
                {
                    "sent": "I have a yam.",
                    "label": 0
                },
                {
                    "sent": "Is it going to work on that and you think?",
                    "label": 0
                },
                {
                    "sent": "Oh, that's interesting.",
                    "label": 0
                },
                {
                    "sent": "Let me have a look and anything.",
                    "label": 0
                },
                {
                    "sent": "Actually, I don't really want to get my hands dirty with a real yam, so I'm going to do a calculation on spherical yams where I can workout exactly how the density of the size is going to fix the answer.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry.",
                    "label": 0
                },
                {
                    "sent": "Well, they've only done the calculation.",
                    "label": 0
                },
                {
                    "sent": "I can't taste him anyway, so that's the spirit.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I do a very short bit about motivation while we're doing this and then talks about two parts.",
                    "label": 0
                },
                {
                    "sent": "One is about what properties of covariance functions on graphs which are defined from the graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "I'll deliberative analysis on regular graphs where you can use the tree approximation and talk about the effect of loops.",
                    "label": 1
                },
                {
                    "sent": "And the second part I want to look at these errors on these graphs.",
                    "label": 0
                },
                {
                    "sent": "I'll use a very simple approximation that's quite old by now.",
                    "label": 0
                },
                {
                    "sent": "That month it's worked out and I did some work on as well.",
                    "label": 1
                },
                {
                    "sent": "Again will look at the effect of loops and the effect of the kernel parameters.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here's the yam slide, as it were.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's the tomatoes and carrots over continuous spaces.",
                    "label": 1
                },
                {
                    "sent": "GP regression is relatively well understood to month.",
                    "label": 1
                },
                {
                    "sent": "It's done quite a lot of work with dirty.",
                    "label": 0
                },
                {
                    "sent": "I did some as well, so we know roughly how the learning curves go, for example.",
                    "label": 0
                },
                {
                    "sent": "How they depend on the smoothness of the covariance function in the region where the two inputs are similar and things like that?",
                    "label": 1
                },
                {
                    "sent": "OK, but there's lots of discrete spaces that come up in many applications, like sequences, strings, all sorts of things.",
                    "label": 1
                },
                {
                    "sent": "There's many sort of biological systems when it happens, and so on.",
                    "label": 0
                },
                {
                    "sent": "Also, of course, text lots of other things you can see.",
                    "label": 0
                },
                {
                    "sent": "I'm a theorist, 'cause I've not listed a lot of examples here, but you can make up your own so the idea of the talk is really just to find out what we can say about GP learning on these discrete spaces.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at random graphs with finite connectivity.",
                    "label": 1
                },
                {
                    "sent": "Because I think that's a sort of fairly parity Matic case.",
                    "label": 0
                },
                {
                    "sent": "Of course, whether whether real discrete spaces really have the same connectivity as a random graph, that's a different question, but.",
                    "label": 0
                },
                {
                    "sent": "That fits this verical yam, right?",
                    "label": 0
                },
                {
                    "sent": "So I don't quite know what the real yams are going.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look like.",
                    "label": 0
                },
                {
                    "sent": "OK so just look at covariance functions on.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graphs that this is just a little bit of introduction that absolutely nothing new here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                },
                {
                    "sent": "One way of defining probably the most popular one of defining covariance way of defining covariance functions on graphs is based on the graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So this slide just runs you through how that definition works.",
                    "label": 0
                },
                {
                    "sent": "OK, so you characterize your graph by adjacency matrix A and it just is just zero or one, depending on whether there's a link between two nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you got a graph with Capital V nodes or vertices then this A is a V bar V matrix.",
                    "label": 1
                },
                {
                    "sent": "And we're going to run directed links.",
                    "label": 0
                },
                {
                    "sent": "So that means this matrix is symmetric and we're going to assume there's no self loop, so the diagonal elements of that matrix are all zero, OK, and then the degree of a node is just a number of the number of links it has.",
                    "label": 0
                },
                {
                    "sent": "So that's the sum over the specific a row or column of the matrix A and we'll call that DI OK. And if you define a diagonal matrix that gathers all the degrees and we call that capital D, then the graph Laplacian is defined as the identity matrix minus.",
                    "label": 0
                },
                {
                    "sent": "The the inverse square root of B times the adjacency matrix times the inverse square root.",
                    "label": 0
                },
                {
                    "sent": "The inverse square roots there there basically there to make the problem symmetric an I'll say on the next slide what the interpretation of this Alice in terms of random walks where it comes from.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the L Now is also symmetric.",
                    "label": 0
                },
                {
                    "sent": "OK, on the based on a random walk, which I'll say next, you normally come up with something that's not symmetric, so these factors of D are really just there to make things symmetric OK, and then there's a nice result from spectral graph theory.",
                    "label": 0
                },
                {
                    "sent": "That DL that you've defined in this way it's called Eigen values in the range zero to two.",
                    "label": 0
                },
                {
                    "sent": "So like the Laplacian continuous space is positive definite.",
                    "label": 0
                },
                {
                    "sent": "What's different here is that this actually about abounded spectrum, so you can't make the eigenvalues arbitrary.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The large and I think intuitively it's something to do with the fact that you're working on discrete space rather than a continuous space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right before I go to the random walk, let me just talk about the standard covariance functions that one can define, and then I'm going to look at here.",
                    "label": 1
                },
                {
                    "sent": "OK, so covariance function on a graph, really, it's just it's a V bar V matrix.",
                    "label": 0
                },
                {
                    "sent": "But of course we could be very large, so you might still want to think of it as a covariance function.",
                    "label": 1
                },
                {
                    "sent": "OK, then the random walk kernel and I explained the name shortly has this form.",
                    "label": 0
                },
                {
                    "sent": "You take some constant a -- L raise that to power P. And if you write out the definition of L, then you get the the second form over there where you got a -- 1 times the identity plus the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "With these factors of degree either side to the power P. And then there's a diffusion kernel, which is like the continuous type continuous of time analog of that one.",
                    "label": 0
                },
                {
                    "sent": "As I'll say shortly, we just exponentiate minus Sigma squared over two times the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And actually, if you're not probably not used to seeing the RBF kernel in this way, but you can also write the arbiter standard squared exponential kernel in that way in Fourier space is just E to the minus Sigma squared over 2 * K ^2, which is basically the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "If K is a wave later, OK, so just to say that never mind if you just didn't follow what I just said very quickly.",
                    "label": 0
                },
                {
                    "sent": "This is like the squared exponential kernel for a graph.",
                    "label": 1
                },
                {
                    "sent": "And we'll always assume that these guys are normalized so that they've got.",
                    "label": 0
                },
                {
                    "sent": "On average, unit variance for each node.",
                    "label": 0
                },
                {
                    "sent": "OK, so the function we're going to look at, I'm just choosing the scales that the scale of the function each node is 1.",
                    "label": 0
                },
                {
                    "sent": "Right, so where do these definitions come from and have they got any?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Meaning.",
                    "label": 0
                },
                {
                    "sent": "So if you think of a random walk on a graph.",
                    "label": 1
                },
                {
                    "sent": "Then the transition probability matrix for that is this one.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "So if I just spell this out a bit, if I'm making a transition from the OJ to node I OK then what I'm saying here is that OK?",
                    "label": 0
                },
                {
                    "sent": "I'm going to choose one of my DJ neighbors 'cause I'm sitting on the J and I've got DJ neighbors OK. And then I'm going to choose one of those DJ nodes to which I'm connected, and you can see that this is normalized device.",
                    "label": 0
                },
                {
                    "sent": "Um, over I, I just get the degree DJ and so that just cancel that give me one.",
                    "label": 1
                },
                {
                    "sent": "So that's a nice transition probability matrix.",
                    "label": 0
                },
                {
                    "sent": "And so if you take S steps over a random walk, then you get obviously the F. The power of this matrix and you can write that in this form of these powers of D if you just rearrange the inverse powers of the degree matrix there, right?",
                    "label": 0
                },
                {
                    "sent": "So I've just pulled out, just separated the well.",
                    "label": 0
                },
                {
                    "sent": "I've put in a D to the half day to the minus half on the left, and I've separated out the D to the minus one on the right, OK?",
                    "label": 0
                },
                {
                    "sent": "Now, if you write out the random walk and I had on the previous slide, let me just go back.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it looked like that.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to normalize this anyway, so let me just divide this through by a little a OK. Then I and then I can just have to sort out this powerpyx there so I get a binomial expansion.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I have a factor of this is the a -- 1 / A I get to the power P -- S that's the number of times I'm not taking a step as it were and then the other factor pre fact the other term at a pre factor of 1 / a I get that to the power S and then I get just this same matrix here to the power is so that's the same matrix that comes up there.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this and compare with what you've got up there, then apart from these annoying factors, left and right, which are basically trivial.",
                    "label": 0
                },
                {
                    "sent": "What you have here is an average over S&R delivery chosen S, because that's effectively the number of steps in the random walk that you're taking and you're averaging over a binomial distribution of the number of steps that what you would get in the random walk.",
                    "label": 1
                },
                {
                    "sent": "OK, so this random walk kernel.",
                    "label": 1
                },
                {
                    "sent": "What is doing is saying perform a random walk on the graph OK?",
                    "label": 0
                },
                {
                    "sent": "And draw your number of steps from binomial distribution with number of samples P and probability of taking a Step 1 / A OK and the smallest probability you can have their sorry.",
                    "label": 0
                },
                {
                    "sent": "The largest one is 1/2 and that's just that comes from the bound on the spectrum of the policy.",
                    "label": 0
                },
                {
                    "sent": "And if you make a smaller than that you get into problems.",
                    "label": 0
                },
                {
                    "sent": "OK and you can do the same calculation for the diffusion kernel.",
                    "label": 0
                },
                {
                    "sent": "OK and you get the same result except that the number of steps in that case comes from across all distribution with Mean Sigma squared over 2.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you see that as you make P large.",
                    "label": 0
                },
                {
                    "sent": "But keeping P / a constant constant, and in particular equal to Sigma squared over 2, then the diffusion kernel is just the limit with P and a go to Infinity, but the ratio stays constant.",
                    "label": 0
                },
                {
                    "sent": "OK, that's just one way of constructing basically continuous time random walk.",
                    "label": 0
                },
                {
                    "sent": "You say I'm going to allow lots of steps, but each time I'm only going to take a step with a small probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so that means large P and large a sub one over a small.",
                    "label": 0
                },
                {
                    "sent": "Is there also a way of sort of going to continuous time but discrete steps?",
                    "label": 1
                },
                {
                    "sent": "That's what the diffusion kernel is.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's always the discrete structure of the graph is always preserved, right?",
                    "label": 0
                },
                {
                    "sent": "Because you're working on a graph, so you've always got a random walk.",
                    "label": 0
                },
                {
                    "sent": "But the diffusion kernel is exactly what you just said, so it's basically doing diffusion for a certain amount of time, which is basically the Sigma squared OK. And then seeing how spread out you're currently.",
                    "label": 0
                },
                {
                    "sent": "OK, and anything you then doing is you putting in these factors of degree in.",
                    "label": 0
                },
                {
                    "sent": "That's just there to make this this overall result to make the C symmetric as it should be for covariance function.",
                    "label": 0
                },
                {
                    "sent": "So the summary so far is that you've got these curve.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nose.",
                    "label": 0
                },
                {
                    "sent": "I will just go back to the definitions and the random walk kernel says walk around on the graph randomly.",
                    "label": 1
                },
                {
                    "sent": "Take a step with probability at each time step, move with probability 1 / A and do this P times OK and the diffusion step.",
                    "label": 1
                },
                {
                    "sent": "The diffusion kernel is the same version of continuous time where you say do this for overtime Sigma squared over 2.",
                    "label": 0
                },
                {
                    "sent": "So there's a reasonable ways of generalizing the standard squared exponential kernels to graph.",
                    "label": 0
                },
                {
                    "sent": "So of course the square exponential kernel you could just do that, getting that get that by doing diffusion in real space for certain.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of time.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not quite sure how familiar people are with this stuff.",
                    "label": 0
                },
                {
                    "sent": "Obviously interrupt me if I'm not being clear.",
                    "label": 0
                },
                {
                    "sent": "I would say this to my students at least in this kind of scenario.",
                    "label": 0
                },
                {
                    "sent": "People would actually ask questions if I was being confusing.",
                    "label": 0
                },
                {
                    "sent": "It never works in calculus one.",
                    "label": 0
                },
                {
                    "sent": "I can tell you.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so we're going to try and understand what these covariance functions do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On regular graphs, regular graphs are the simplest ones where every node has got the same degree D. OK, so we're going to look at a random graph.",
                    "label": 1
                },
                {
                    "sent": "Ensembles I don't want to specify a particular graph, so I'm just going to assume.",
                    "label": 0
                },
                {
                    "sent": "A randomly drawn graph with degree D and with the given number of vertices.",
                    "label": 0
                },
                {
                    "sent": "Right now I'm assuming I'm uniformly sampling from this graph, so I just put pull one of those graphs out of a bag and then I want to see what does the covariance function look like on that graph.",
                    "label": 0
                },
                {
                    "sent": "One thing that's useful about these graphs is that loops are typically long, so if I make the graph big for make very large, then the loop de loop links typically have order log V. Log V isn't terribly big, but if I'm being a mathematician and saying OK, I can make very very large and I can make.",
                    "label": 0
                },
                {
                    "sent": "The loops in principle, as long as I want OK.",
                    "label": 0
                },
                {
                    "sent": "So locally and graph that I've sampled from this ensemble is tree like OK, and of course that's at the base of all sorts of things like BP.",
                    "label": 1
                },
                {
                    "sent": "OK, and the question we want to understand is how do the graph covariance functions behave on graphs like that and what you would think is that if you take many random walk steps so you take Peter Infinity OK, then you should get a flat kernel.",
                    "label": 0
                },
                {
                    "sent": "That's what happens in in Euclidean space, right?",
                    "label": 1
                },
                {
                    "sent": "In the continuous space you make the Sigma squared larger squared exponential.",
                    "label": 0
                },
                {
                    "sent": "It gets the kernel becomes flat.",
                    "label": 0
                },
                {
                    "sent": "So we would expect to find that this covariance function CIJ just becomes 1 means the function values are fully correlated across all night.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't be going on about this if that wasn't.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See what happened tonight.",
                    "label": 0
                },
                {
                    "sent": "So before I show you the answer Livermore how we actually do this.",
                    "label": 0
                },
                {
                    "sent": "OK, so on the regulatory.",
                    "label": 0
                },
                {
                    "sent": "Basically all nodes are equivalent.",
                    "label": 1
                },
                {
                    "sent": "OK, so if you are going now going to ignore the loops.",
                    "label": 0
                },
                {
                    "sent": "Just try and focus on a large randomly drawn regular graph which is locally tree like, so we should be able to just throw away the loops and do the analysis like that, OK?",
                    "label": 0
                },
                {
                    "sent": "And then the covariance function of the kernel is a function depends only on the distance measured along the graph.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's just number of hops between two nodes.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call that distance L and now I can explicitly calculate this for the kernels that I've described.",
                    "label": 0
                },
                {
                    "sent": "So of course if I'm taking no steps then I have a Delta correlation kernel.",
                    "label": 0
                },
                {
                    "sent": "So every function value is only correlated with itself and then to get from PETA people as one would need to take one extra step in random walk.",
                    "label": 0
                },
                {
                    "sent": "So just to run you through these two there's a.",
                    "label": 0
                },
                {
                    "sent": "Probably the most difficult equations in the talk.",
                    "label": 0
                },
                {
                    "sent": "So to get from at position zero, that's my original node.",
                    "label": 0
                },
                {
                    "sent": "OK, if I take one extra step, two things can happen.",
                    "label": 1
                },
                {
                    "sent": "I'm going to stay where I was.",
                    "label": 0
                },
                {
                    "sent": "That happens with probability 1 -- 1 / A or I'm going to get.",
                    "label": 0
                },
                {
                    "sent": "Someone I'm going to get the guy who was one one step away, moving back towards the origin.",
                    "label": 0
                },
                {
                    "sent": "That happens with probability 1 / A.",
                    "label": 0
                },
                {
                    "sent": "D but I've got the neighbors, so there's an extra factor of D up there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The one of a D comes from the two powers of the to the minus 1/2 in the way the kernel is constructed.",
                    "label": 0
                },
                {
                    "sent": "the D comes just from counting how many neighbors I've got, which are one hop away and remember every node is not exactly the neighbors.",
                    "label": 0
                },
                {
                    "sent": "Then I could do the same thing when I'm further away with probability 1 -- 1 / A. I'm looking to move with probability 1 / A. D. I'm going to move from a node that's further in towards my origin.",
                    "label": 0
                },
                {
                    "sent": "And with probability 1 / a D, I'm going to move from a node further out, but I've got D -- 1 nodes which are further away.",
                    "label": 0
                },
                {
                    "sent": "I've got one note that further in if I'm not at the origin myself and I've got D -- 1 nodes which are further out.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've got these simple recursion so I can start from P = 0 and I can just evaluate this.",
                    "label": 0
                },
                {
                    "sent": "So that gives me the shape of the covariance function on a regular tree.",
                    "label": 0
                },
                {
                    "sent": "So large in principle that should be the large random regular graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to take D = 3, so.",
                    "label": 0
                },
                {
                    "sent": "You have three links meeting each node.",
                    "label": 0
                },
                {
                    "sent": "I'm going to smallest value of A and just increase P and see what happens.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's the graph.",
                    "label": 0
                },
                {
                    "sent": "So you start with P = 1, in which case you only have correlations out to one step away.",
                    "label": 0
                },
                {
                    "sent": "Makes sense OK, then you get P = 2, that's the red line.",
                    "label": 0
                },
                {
                    "sent": "You get some problem correlation.",
                    "label": 0
                },
                {
                    "sent": "Also two steps, but it's quite small OK and you keep pushing P up OK. Somewhere here is P = 5, then there's P = 10 OK and you keep going further out, but at some point you stop making progress, right?",
                    "label": 0
                },
                {
                    "sent": "This is P. Where have I gone this is P = 100 = 500.",
                    "label": 0
                },
                {
                    "sent": "That line is P equals Infinity and tell you later how we calculate that OK.",
                    "label": 0
                },
                {
                    "sent": "So there's a weird result here that on a regular tree you've taken the obvious limit of making the random walk do as many steps as you can do.",
                    "label": 0
                },
                {
                    "sent": "The covariance function doesn't become uniform, it stops there at some finite limiting shape.",
                    "label": 0
                },
                {
                    "sent": "You think how come surely you've made a mistake?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That Peter.",
                    "label": 0
                },
                {
                    "sent": "That can't be right.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just try and understand what's going on there.",
                    "label": 0
                },
                {
                    "sent": "And one useful way to think about this is rather than think about the covariance function.",
                    "label": 0
                },
                {
                    "sent": "You exploit this probability picture and say I'm going to just look at, not probability of being on a specific node, but at the probability of being a certain distance away from origin.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to lump all the probabilities at that distance.",
                    "label": 0
                },
                {
                    "sent": "That's basically a shell around my node into one probability, and I'm going to call that probability SLP OK, and so at the origin is only one node to lump together, and in the shell distance Alloway.",
                    "label": 0
                },
                {
                    "sent": "I've got this many nodes.",
                    "label": 0
                },
                {
                    "sent": "So that's just a simple rescaling of the problem.",
                    "label": 0
                },
                {
                    "sent": "OK, now I can take my recursion from the previous previous slide, and I can write down what it looks like in terms of these variables.",
                    "label": 0
                },
                {
                    "sent": "And what you find.",
                    "label": 0
                },
                {
                    "sent": "This is not terribly surprising that the recursion you get just represents a biased random walk with a barrier reflecting barrier at the origin.",
                    "label": 1
                },
                {
                    "sent": "So rather than showing you the equation, I've just tried to draw here what the random walk looks like at each time step.",
                    "label": 1
                },
                {
                    "sent": "You've got probability of 1 -- 1 / A of not moving, probably moving was 1 / a OK. Well, if you're at the origin, you always have to move out, so you've got probably 1 every day going there.",
                    "label": 0
                },
                {
                    "sent": "Every other node you've got probability D -- 1 / 80 of moving out and a smaller probability of moving.",
                    "label": 0
                },
                {
                    "sent": "Of course, that makes sense.",
                    "label": 0
                },
                {
                    "sent": "You've got a lot more neighbors as you go out.",
                    "label": 0
                },
                {
                    "sent": "Then you've got neighbors going in, so the walk is biased, right?",
                    "label": 0
                },
                {
                    "sent": "You tend to go away.",
                    "label": 0
                },
                {
                    "sent": "You don't tend to come back to the origin if you if you have the origin, you can only ever go out.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's a biased random walk, OK, big deal.",
                    "label": 0
                },
                {
                    "sent": "Why does that give us these funny covariance funk?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who is the random walk?",
                    "label": 0
                },
                {
                    "sent": "And I've got different pieces here and different else.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is distance measured along the graph and the different curves relate to different numbers of steps of the random walk.",
                    "label": 0
                },
                {
                    "sent": "And the obvious thing happens, these probabilities are peak.",
                    "label": 0
                },
                {
                    "sent": "These are actually log probabilities, so these are very small.",
                    "label": 0
                },
                {
                    "sent": "But that's just show what happen.",
                    "label": 0
                },
                {
                    "sent": "I need that to get through the argument with covariance function OK, and basically the peak is sort of in the obvious place, 'cause if you just.",
                    "label": 0
                },
                {
                    "sent": "Use the fact that you go outwards with this probability and you go inwards with probability 1 / a D. Then on average you go.",
                    "label": 0
                },
                {
                    "sent": "1 / 8 * D -- 2 D In each step Udupi steps.",
                    "label": 0
                },
                {
                    "sent": "So the peak is this number, so it goes up linearly with POK and so 4 = 3 = 2.",
                    "label": 0
                },
                {
                    "sent": "That number is P / 6.",
                    "label": 0
                },
                {
                    "sent": "So if I take 5000 steps then typically I'm about I've got about 800 steps OK or 833 so that all works fine.",
                    "label": 0
                },
                {
                    "sent": "It does nothing surprising.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do you think happens?",
                    "label": 0
                },
                {
                    "sent": "Is that what?",
                    "label": 0
                },
                {
                    "sent": "Is the effect you get when you convert this back to C, so that was the covariance function that we wanted, right?",
                    "label": 0
                },
                {
                    "sent": "So the conversion factor was proportional to D -- 1 power minus one.",
                    "label": 0
                },
                {
                    "sent": "OK, just counting how many nodes are in each shell.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to look at this graph, not as things move out, but how these probabilities down here how they move down as the random wall keeps going.",
                    "label": 0
                },
                {
                    "sent": "As I make more more steps, I've got a smaller probability of still being at the origin.",
                    "label": 0
                },
                {
                    "sent": "OK, now if I convert this back to the covariance function, OK, it turns out that actually what I pick up is this behavior around here?",
                    "label": 0
                },
                {
                    "sent": "OK, so the increase in this exponential tail once I've converted it with this simple exponential variation with L is what gives me the decrease of the covariance function asymptotically.",
                    "label": 0
                },
                {
                    "sent": "OK, so here over a much larger scale than before, I've drawn you how the.",
                    "label": 0
                },
                {
                    "sent": "Covariance function storage should see how that depends on L. Again for increasing, please OK. And over smaller scale, this is what we saw earlier that as you increase P you get to this limiting form OK, where the kernel just stops going to one OK and we can use the behavior near the tail which is known of the biased random walk to workout.",
                    "label": 0
                },
                {
                    "sent": "This limiting form and actually has this.",
                    "label": 0
                },
                {
                    "sent": "Really rather simple form.",
                    "label": 0
                },
                {
                    "sent": "It has a little has a linear dependence on L in here, which is relatively unimportant and otherwise you just decay exponentially with sqrt D -- 1 OK.",
                    "label": 0
                },
                {
                    "sent": "So that's how we can calculate what this kernel actually does.",
                    "label": 0
                },
                {
                    "sent": "So I'm not sure whether this provides enough intuition, But basically what happens is that because the tree gets increasingly big as you move out OK, the random walk analogy doesn't work right.",
                    "label": 0
                },
                {
                    "sent": "This walk is biased, right?",
                    "label": 0
                },
                {
                    "sent": "So the idea that the random walk probabilities basically flatten out OK doesn't happen as soon as you've got more than two neighbors, right?",
                    "label": 0
                },
                {
                    "sent": "Once you've got two neighbors, then you just keep going out with you never come back, and so this idea that things will eventually diffuse and become smooth doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Not on the tree anyway.",
                    "label": 0
                },
                {
                    "sent": "What you also see is that if D goes to two.",
                    "label": 0
                },
                {
                    "sent": "Which is a chain, in which case you don't have a tree anymore and everything should work fine, gives you the expected result because you get one plus.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think there's a typo that distribute the minus two.",
                    "label": 0
                },
                {
                    "sent": "This term goes away in this term, becomes one, everything becomes one.",
                    "label": 0
                },
                {
                    "sent": "So sorry bout that, there's a.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Misprint there.",
                    "label": 0
                },
                {
                    "sent": "Right, so that car.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Happened on real graphs, obviously because there are loops, so that should give us an idea of so this tree behavior that I've just described should apply for awhile and then eventually you should see the fact that the graph isn't actually just a tree and eventually you can go back on yourself and there's a very simple way of estimating when that's going to happen.",
                    "label": 0
                },
                {
                    "sent": "You can count the number of nodes in a tree object out, which looks like that OK, and basically you have to have a loop when you've run out of nodes.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you got a regular graph of some size and you just put the size in here, then the L can't really be bigger than the L on the right here.",
                    "label": 0
                },
                {
                    "sent": "You just don't have enough nodes to make a tree without loops OK?",
                    "label": 0
                },
                {
                    "sent": "So the estimate for when the loops kick in is log V over log D -- 1.",
                    "label": 0
                },
                {
                    "sent": "Remember, D was the degree of each node, so that's consistent with what I said earlier that the loops are typically of length.",
                    "label": 0
                },
                {
                    "sent": "Log me OK, but here you get this extra factor.",
                    "label": 0
                },
                {
                    "sent": "This log of D -- 1.",
                    "label": 0
                },
                {
                    "sent": "OK, and we know that the random walk thought of a random walk on the graph typically takes P / a steps OK because one over raise the probability of moving.",
                    "label": 1
                },
                {
                    "sent": "So this is an estimate for where the where the loop effects kick in.",
                    "label": 0
                },
                {
                    "sent": "So we check this.",
                    "label": 0
                },
                {
                    "sent": "We measured the correlation across one link in this manner.",
                    "label": 0
                },
                {
                    "sent": "So we took real graphs, workout the kernel and then did this normalized correlation function between neighboring nodes.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That should hopefully agree with the theory.",
                    "label": 0
                },
                {
                    "sent": "So what you have there is as a function of the number of steps divided by a.",
                    "label": 0
                },
                {
                    "sent": "That should be the right variable to look at, at least if both of those things are large and you've got.",
                    "label": 0
                },
                {
                    "sent": "Lines there which are the tree theory that I showed you earlier?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at those first.",
                    "label": 0
                },
                {
                    "sent": "So for example, for a = 2, that's more than what we had before.",
                    "label": 0
                },
                {
                    "sent": "As I take more and more steps.",
                    "label": 0
                },
                {
                    "sent": "Eventually I level off my nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "Correlation does not go to one.",
                    "label": 0
                },
                {
                    "sent": "OK, that was the tree with all that showed you earlier.",
                    "label": 0
                },
                {
                    "sent": "OK, if I take a = 4, which means I move a little bit more slowly for a given P. Eventually the same thing happens, right?",
                    "label": 0
                },
                {
                    "sent": "In fact, as you as I explained earlier, given that for large P and a you ought to get the same answers.",
                    "label": 1
                },
                {
                    "sent": "These two curves eventually merge, which is what happened.",
                    "label": 0
                },
                {
                    "sent": "'cause that over there on the right is basically the limit where I could equally well just take a diffusion kernel.",
                    "label": 0
                },
                {
                    "sent": "Right now the symbols the little circles here are what we measured in the real graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, as a function of P array and you see that they follow these tree results very nicely.",
                    "label": 0
                },
                {
                    "sent": "OK, so initially the graph readers behave like a tree and the covariance function has to expect to shape.",
                    "label": 0
                },
                {
                    "sent": "But then when I put here with an arrow where we expect things to go wrong, some point round here, you do eventually branch off and the correlation goes to one.",
                    "label": 0
                },
                {
                    "sent": "OK, so on the real graph because you have loops, eventually the covariance function does become flat, but.",
                    "label": 0
                },
                {
                    "sent": "The thing that surprised us at least, is that you need the loops for that, right?",
                    "label": 0
                },
                {
                    "sent": "So, unlike lots of things you can do with regular graphs where you can just think about the tree structure right.",
                    "label": 0
                },
                {
                    "sent": "With this.",
                    "label": 0
                },
                {
                    "sent": "Actually the loops are really important because they're the only thing that eventually makes the covariance function become fully correlated between nodes.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's the first.",
                    "label": 0
                },
                {
                    "sent": "Half the talk taking off quicker.",
                    "label": 0
                },
                {
                    "sent": "So now we want to look at Bayes errors on on random graphs.",
                    "label": 1
                },
                {
                    "sent": "Or learning curve so the learning curves are basically in the same spirit that Marcus was talking about.",
                    "label": 0
                },
                {
                    "sent": "We're going to average over datasets and look at these errors.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So of course, as you all know that if you take the generalization error over GP regressor then you can express that in terms of the covariance function for a given data set.",
                    "label": 1
                },
                {
                    "sent": "And we're going to call that quantity epsilon.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume.",
                    "label": 1
                },
                {
                    "sent": "That we have the correct priors, so I'm going to assume.",
                    "label": 0
                },
                {
                    "sent": "This was like the case of Magnus distribution, one that the prior and the thing I'm using for the inference are exactly matched and then epsilon is just the base error and I'm going to use the standard loss for regression which is squared deviation.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to average over datasets of some given size N and that gives me the learning curve.",
                    "label": 1
                },
                {
                    "sent": "So epsilon is a function of North.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to make an assumption about how inputs are distributed across the graph, and I'm going to use the simplest case that they're uniformly distributed.",
                    "label": 0
                },
                {
                    "sent": "So the learning curve has several parameters that influence it.",
                    "label": 0
                },
                {
                    "sent": "Obviously number of data samples I've got size of the graph, the connectivity of the graph, and I'm always going to keep that three to keep things.",
                    "label": 0
                },
                {
                    "sent": "Simpler and then the two parameters A&P that specify the kernel and finally the noise on the problem right?",
                    "label": 0
                },
                {
                    "sent": "So I'm assuming that's usually called additive noise.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On my data.",
                    "label": 0
                },
                {
                    "sent": "So here are some simulation results.",
                    "label": 1
                },
                {
                    "sent": "Just to give you some idea what might be going on.",
                    "label": 0
                },
                {
                    "sent": "So this is a graph with 500 nodes are random graph, 500 nodes connectivity three and I've taken the one of those random walk kernels with equals 2 and P = 10 OK, and I've plotted the learning curve, the epsilon and I've plotted it with a little bit of four sites not against the wrong number of examples, but the number of examples divided by the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "You expect that that's the sort of right quantity that should tell you how well you've learned, because obviously you have to.",
                    "label": 0
                },
                {
                    "sent": "More or less cover a good fraction of the graph before the error will go down OK, and then finally there are four curves here for different amounts of noise.",
                    "label": 1
                },
                {
                    "sent": "OK, and you see that, for example, if you look at the blue curve very clearly, this has got two different regimes.",
                    "label": 0
                },
                {
                    "sent": "First, you gotta relatively fast decay and it crosses over to something much slow on this log log plot.",
                    "label": 0
                },
                {
                    "sent": "It's roughly straight here, so that base is just here.",
                    "label": 0
                },
                {
                    "sent": "It's a power law here with something faster, OK, and if you then look where these crossovers take place, so the noise level here is 10 to the minus 4.",
                    "label": 0
                },
                {
                    "sent": "There so that crossovers round roughly around the point where the error is gone down to off the order of your noise variance, and then if you check that on the other curves again, that's roughly true.",
                    "label": 0
                },
                {
                    "sent": "There's 10 to the minus 310 to the minus two, and so on.",
                    "label": 0
                },
                {
                    "sent": "So then it gives the cost of two bits to them and initial section before the error gets down to roughly the noise level and then a final section which looks more like a power law.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to a little bit of theory.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Explain maybe what's going on there, OK?",
                    "label": 0
                },
                {
                    "sent": "And a simple approximation for.",
                    "label": 0
                },
                {
                    "sent": "Describing the learning curve is one that's based on the eigenvalues of the Colonel, in this case of a discrete graph, the eigenvalue eigen values are just defined in this way.",
                    "label": 1
                },
                {
                    "sent": "So the eigenfunction Phi has a value at each node.",
                    "label": 0
                },
                {
                    "sent": "Then averaging over nodes J at this product CI J5 J, you must get back the original function five times, the multiplier Lambda, which is the eigenvalue OK. And then in terms of these lambdas, there's a relatively simple approximation, but turns out to be quite accurate.",
                    "label": 0
                },
                {
                    "sent": "Often is that you calculate this function.",
                    "label": 0
                },
                {
                    "sent": "Which is basically a trace of the kernel evaluated in this eigenvalue basis.",
                    "label": 0
                },
                {
                    "sent": "This is the inverse of the inverse eigenvalue plus some number H and then into this you plug the number of examples divided by the error plus Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "OK. That equation isn't yet explicit because I've got epsilon the right and the left only to solve that self consistently.",
                    "label": 0
                },
                {
                    "sent": "But once I've got this function, GI could do that.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not too difficult, OK?",
                    "label": 0
                },
                {
                    "sent": "And the G is always because I'm because of this normalization of the variance of the function of each node that's always normalized at that race.",
                    "label": 0
                },
                {
                    "sent": "So the sum of all the eigenvalues equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So G of zero is 1.",
                    "label": 0
                },
                {
                    "sent": "And you can see already from this that actually this approximation will capture these two regimes that I talked about, 'cause if the error is much smaller than Sigma squared, then basically I can stop solving this thing self consistently.",
                    "label": 0
                },
                {
                    "sent": "I could just put N over Sigma squared in.",
                    "label": 0
                },
                {
                    "sent": "Here was if there is much bigger than Sigma squared, I could basically drop the Sigma squared and I should get something that doesn't depend on the noise.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now I need to do these eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "The good thing is that.",
                    "label": 0
                },
                {
                    "sent": "For large graph, the tree approximation, at least for the spectrum, should be good.",
                    "label": 1
                },
                {
                    "sent": "OK, and for the tree, for the Laplacian on the tree the eigenvalue density is known and it's got this funny form which I'm not going to read out to you.",
                    "label": 0
                },
                {
                    "sent": "It's symmetric around 1:00 and it's bound between zero and two, as you'd hope so.",
                    "label": 0
                },
                {
                    "sent": "Or eat.",
                    "label": 0
                },
                {
                    "sent": "It's it's known from at least the 1950s.",
                    "label": 0
                },
                {
                    "sent": "You can get it very simply with a cavity type trick, so you just say OK, local node and I'm decomposing to all the tree branches.",
                    "label": 0
                },
                {
                    "sent": "And use that to calculate the resultant.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but there's there's.",
                    "label": 0
                },
                {
                    "sent": "There's at least four different derivations of this in the literature.",
                    "label": 0
                },
                {
                    "sent": "There's loads OK.",
                    "label": 0
                },
                {
                    "sent": "But it has some singularity Zig 0 into.",
                    "label": 0
                },
                {
                    "sent": "So if this is I didn't write down the bounds.",
                    "label": 0
                },
                {
                    "sent": "If you look at this, this is actually less than one, and it's this is non zero only where the square root is well defined.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll show you the picture there is.",
                    "label": 0
                },
                {
                    "sent": "But I might have been better to show that first, actually so.",
                    "label": 0
                },
                {
                    "sent": "This is the case of D = 3 and I'm just showing you the prediction for the eigenvalue spectrum with the red curve versus something that we've actually sampled for.",
                    "label": 0
                },
                {
                    "sent": "A graph that has 2000 nodes, so that's how it looks like, and for D above 2 is actually bounded away from zero and from two OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about these these captions spectrum.",
                    "label": 0
                },
                {
                    "sent": "They're not terribly important.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so once we've got this guy, so we've got the eigenvalues of the of the tree.",
                    "label": 0
                },
                {
                    "sent": "Then of course, we've also got the eigenvalue of the Laplacian and we also got the eigenvalues of the covariance function.",
                    "label": 1
                },
                {
                    "sent": "Well, because they're just.",
                    "label": 0
                },
                {
                    "sent": "Determined by the way I constructed the kernel, so it's just a minus days plus in eigenvalues Lambda to the power P, and then they're normalized by one of the V which comes from the definition.",
                    "label": 0
                },
                {
                    "sent": "OK, and so I'm just going to interfere.",
                    "label": 0
                },
                {
                    "sent": "We're just going to use this eigenvalue density and evaluate the approximate learning curves using the equation I had on the previous slide.",
                    "label": 1
                },
                {
                    "sent": "OK, and then because of this scaling in the eigenvalues, they direct they depend directly on the density of examples.",
                    "label": 0
                },
                {
                    "sent": "So the number of examples per node.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "She said, which is a reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Behave it again, OK?",
                    "label": 0
                },
                {
                    "sent": "So here's that, here are the simulations from before, and this time I've added the theory.",
                    "label": 0
                },
                {
                    "sent": "That I value it evaluated like in the way I've just described.",
                    "label": 0
                },
                {
                    "sent": "And you see that the theory isn't bad in the sense that it picks up the fact that there's two regimes.",
                    "label": 0
                },
                {
                    "sent": "It's pretty good in this final regime where you basically learning against the noise, OK?",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "To be accurate in this region where you sort of go between the two.",
                    "label": 0
                },
                {
                    "sent": "OK, so the initial decay where you sort of as it were, covering the graph with examples that's not as accurate.",
                    "label": 0
                },
                {
                    "sent": "Can be approximated with was much better.",
                    "label": 0
                },
                {
                    "sent": "Do you know?",
                    "label": 0
                },
                {
                    "sent": "I mean to look at that or people going to haven't managed?",
                    "label": 0
                },
                {
                    "sent": "Also there it was.",
                    "label": 0
                },
                {
                    "sent": "It was the case that the biggest deviations were around here.",
                    "label": 0
                },
                {
                    "sent": "I don't remember quantitatively how big they were.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is deliberate.",
                    "label": 0
                },
                {
                    "sent": "This graph looks exactly like the next one.",
                    "label": 0
                },
                {
                    "sent": "OK, because all I'm trying to say there is that the scaling of the learning curves with number of examples per node is actually very good.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you've got on this graph compared to the last one?",
                    "label": 0
                },
                {
                    "sent": "You've also got these empty circles, which are the results for a graph with 1000 nodes.",
                    "label": 0
                },
                {
                    "sent": "And so that scaling works pretty well.",
                    "label": 0
                },
                {
                    "sent": "It wasn't entirely clear to us how how it would how this would work, but it's.",
                    "label": 0
                },
                {
                    "sent": "The theory is actually by definition almost has that scaling.",
                    "label": 0
                },
                {
                    "sent": "So there's only a single lot of theory curves.",
                    "label": 0
                },
                {
                    "sent": "I'm showing this mainly to compare the results for a smaller graph and a larger graph, and you see that the big circles are essentially on top of the small circles.",
                    "label": 0
                },
                {
                    "sent": "Have I answered the question?",
                    "label": 0
                },
                {
                    "sent": "If I've left Manfred both baffled.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've done something wrong.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, let's quickly look at the effect of loops, so so far I've just used this tree approximation OK, which gives me the scaling of the learning curve with a number of examples per per node, But eventually this must fall over when I make the steps in the number of steps in the random walk large, and eventually if I get to the limit with covariance functions basic uniform, then I've only got 1 value to learn, which gives me the function everywhere.",
                    "label": 1
                },
                {
                    "sent": "So the learning curve should just go to this, 'cause that's the primary inverse variance, and that's how much I reduced the inverse variance with each example, OK?",
                    "label": 0
                },
                {
                    "sent": "I'm just going to show you one case like that where in the 500 node graph I've taken 200 steps.",
                    "label": 0
                },
                {
                    "sent": "OK, so I should be around all the loops by then.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what's quite interesting about that is.",
                    "label": 0
                },
                {
                    "sent": "Here's a knife.",
                    "label": 0
                },
                {
                    "sent": "This naive estimate that just.",
                    "label": 1
                },
                {
                    "sent": "Wrote down that's the blue line you see.",
                    "label": 0
                },
                {
                    "sent": "It's not actually terribly good, OK?",
                    "label": 0
                },
                {
                    "sent": "And this is in spite of the fact that the largest eigenvalue of this kernel is .994.",
                    "label": 0
                },
                {
                    "sent": "Remember, the email is to one.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's pretty uniform, right?",
                    "label": 0
                },
                {
                    "sent": "But Even so, there's actually a fairly strong deviation from the estimate that you get.",
                    "label": 1
                },
                {
                    "sent": "This function really was uniform across the graph, and the theory works pretty well at capturing the contribution from these much smaller eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "OK, so the theory is relatively accurate in.",
                    "label": 0
                },
                {
                    "sent": "So in this case I've given the theory the true eigenvalues for that particular kernel.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right, because I can't calculate those using the tree information 'cause this is the.",
                    "label": 1
                },
                {
                    "sent": "This is the regime where I've got lots of loops.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why I measured the eigenvalues here and then evaluated the theory.",
                    "label": 0
                },
                {
                    "sent": "The what the theory?",
                    "label": 0
                },
                {
                    "sent": "I think it does eventually.",
                    "label": 0
                },
                {
                    "sent": "I mean it does go to one, it's just it's the same effect as before that basically the crossover onto the noise dominated regime isn't.",
                    "label": 0
                },
                {
                    "sent": "That's where the theory is inaccurate.",
                    "label": 0
                },
                {
                    "sent": "The noise in this case is .1, so the crossover happens actually already around here.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the same effect as you saw before.",
                    "label": 0
                },
                {
                    "sent": "OK, that's just.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The theory isn't terribly good.",
                    "label": 0
                },
                {
                    "sent": "OK, so just a last thing.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On kernel parameters.",
                    "label": 0
                },
                {
                    "sent": "So we can now ask so everything I showed you so far was a = 2 and then a small P 10 and a large P 200.",
                    "label": 0
                },
                {
                    "sent": "So if we look at more systematically and what happens as P becomes larger, it seems that the theory does become more accurate, so the gaps between these, the simulations and the.",
                    "label": 0
                },
                {
                    "sent": "This is the theory becomes smaller, unfortunately, have no good, no good reason to explain why this would be the case.",
                    "label": 0
                },
                {
                    "sent": "That's just an op.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Salvation at this point.",
                    "label": 0
                },
                {
                    "sent": "OK. Let's assume that the theory works well for large P OK, and you can work out what it says then and the asymptotic form.",
                    "label": 1
                },
                {
                    "sent": "So the tail of the learning curve looks like this.",
                    "label": 0
                },
                {
                    "sent": "You've got Sigma squared over density of examples, which is sort of what you would expect.",
                    "label": 0
                },
                {
                    "sent": "There are two interesting things.",
                    "label": 0
                },
                {
                    "sent": "One is that you've got these smaller, strange looking corrections to that which go like log of the power 3 / 2 of the number of examples.",
                    "label": 0
                },
                {
                    "sent": "More importantly, maybe you've gotta factor.",
                    "label": 0
                },
                {
                    "sent": "See here, OK which?",
                    "label": 0
                },
                {
                    "sent": "Goes like P. The number of steps in the random walk to the power minus 3 / 2.",
                    "label": 0
                },
                {
                    "sent": "So if I look at the number, the density of examples I need on the graph to reach a certain error.",
                    "label": 1
                },
                {
                    "sent": "As I make P large it still it goes down.",
                    "label": 0
                },
                {
                    "sent": "It goes down like P to the minus 3 / 2 just by equating by setting this to some number.",
                    "label": 0
                },
                {
                    "sent": "Some epsilon I want to reach.",
                    "label": 0
                },
                {
                    "sent": "So then, if you've been awake throughout, which is would be quite an achievement, then you should think hang on, that's strange.",
                    "label": 0
                },
                {
                    "sent": "'cause you told us that in the first part that if I make pee large things stop changing, the kernel goes to some shape and it doesn't do anything anymore.",
                    "label": 0
                },
                {
                    "sent": "So how come the learning curve if you keep making pelargir?",
                    "label": 0
                },
                {
                    "sent": "Get better so you keep going down faster as you make pee larger.",
                    "label": 0
                },
                {
                    "sent": "Even though the kernel doesn't seem to change.",
                    "label": 0
                },
                {
                    "sent": "So this name and again I don't have an exact explanation for that, but I think what must be happening there is that the kernel of course it only gets turns to constant if you fix the distance you're looking at, and then you make P large.",
                    "label": 0
                },
                {
                    "sent": "But if you're looking at distances of the order of P, it will keep changing.",
                    "label": 0
                },
                {
                    "sent": "So I think it must be those changes at the larger distances that actually keep decreasing your error in the problem.",
                    "label": 0
                },
                {
                    "sent": "Anything?",
                    "label": 0
                },
                {
                    "sent": "The eigenvalue come the while the eigenvalue distribution.",
                    "label": 0
                },
                {
                    "sent": "So the spectrum of the covariance function.",
                    "label": 0
                },
                {
                    "sent": "It's quite it's a subtle question because basically what happens is that the kernel eigenvalues, they all get bunched together.",
                    "label": 0
                },
                {
                    "sent": "But there's a sort of we could talk about this separately, 'cause that's quite technical.",
                    "label": 0
                },
                {
                    "sent": "What happens is that you get a power lower part of the spectrum with exponent that's very close to minus one.",
                    "label": 0
                },
                {
                    "sent": "So actually most of the eigenvalues tend to be extremely small.",
                    "label": 0
                },
                {
                    "sent": "OK, and it's sort of an almost non integrable spectrum.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So I think there is a solution to this problem that just means that it's kind of a warning that you have to be slightly careful that even though it looks like the kernel has this limiting shape, there is still effect on the learning.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case with the pain.",
                    "label": 0
                },
                {
                    "sent": "And if you increase a, the theory gets worse.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I've gone from equals two 2 = 4, which means I'm making the kernel more short range because I'm taking fewer steps in the random walk.",
                    "label": 0
                },
                {
                    "sent": "So the theory here becomes obviously less accurate.",
                    "label": 0
                },
                {
                    "sent": "In fact, for 10 to the minus four, but out to like 5000 examples where I can reasonably simulate, I've only just sort of falling back onto the theory.",
                    "label": 0
                },
                {
                    "sent": "This theory becomes worse.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can actually understand relatively easily how it becomes worse, OK?",
                    "label": 0
                },
                {
                    "sent": "In the limit where you take A to Infinity, you got a very simple covariance function because it's just a Delta function, right?",
                    "label": 0
                },
                {
                    "sent": "Because you take no steps and in that limit you can evaluate the exact result, which is this averaged over binomial distribution of examples at each node.",
                    "label": 0
                },
                {
                    "sent": "And the approximation looks like that.",
                    "label": 0
                },
                {
                    "sent": "So the approximation says after I've given every node one example, an average.",
                    "label": 0
                },
                {
                    "sent": "I've learned everything perfectly, whereas of course that's not true, 'cause there will be some nodes which haven't had any examples yet.",
                    "label": 0
                },
                {
                    "sent": "And if you.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's two functions for similar parameters to the ones I had before then, you see that you get exactly this shape of deviation with the approximation underestimates the error.",
                    "label": 0
                },
                {
                    "sent": "OK, so it says you're going to learn more quickly because the theory neglect some fluctuations OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me try and summarize.",
                    "label": 0
                },
                {
                    "sent": "In the first part of the talk I've tried to explain that kernels on graphs.",
                    "label": 0
                },
                {
                    "sent": "I've got some fairly counterintuitive properties.",
                    "label": 0
                },
                {
                    "sent": "In particular, you reached the limit where everything becomes fully correlated.",
                    "label": 0
                },
                {
                    "sent": "Because of these loop effects, so you can't, within the tree approximation you get this nontrivial limiting kernel shape.",
                    "label": 1
                },
                {
                    "sent": "From we can calculate that from the biased random walk.",
                    "label": 0
                },
                {
                    "sent": "If you're in the regime with the loops aren't important, so where P is not too large, then the learning curve scale with.",
                    "label": 0
                },
                {
                    "sent": "The density of examples.",
                    "label": 0
                },
                {
                    "sent": "So the number of examples per node, and that's relatively reasonable.",
                    "label": 0
                },
                {
                    "sent": "'cause imagine this big graph and you're trying to just cover the graph.",
                    "label": 0
                },
                {
                    "sent": "We sort of blobs of correlated function values, so you're going to need a number of blobs that proportional to the size of the graph.",
                    "label": 1
                },
                {
                    "sent": "For large P do approach this fully correlated limit.",
                    "label": 0
                },
                {
                    "sent": "But as I showed you for the one case of P = 200, there's quite a lot of Corrections to that.",
                    "label": 0
                },
                {
                    "sent": "The approximation that I showed you works quite well, except when P array is small.",
                    "label": 0
                },
                {
                    "sent": "So when the kernel is very short ranged, OK. And in the course of a region, it never works terribly well.",
                    "label": 0
                },
                {
                    "sent": "There's some future work that I quite like to do, although this has taken me kind of two years to finish just because I was distracted by other things.",
                    "label": 0
                },
                {
                    "sent": "So for the purposes of this meeting, of course we should look at prior mismatch.",
                    "label": 0
                },
                {
                    "sent": "We did that in the continuous space, and this might be interesting scenario to look at that in a different scenario.",
                    "label": 0
                },
                {
                    "sent": "And there are always other things you could look at other graph structures, maybe taking graph structures from data with graph structures are known the obvious.",
                    "label": 0
                },
                {
                    "sent": "Other ones that one could look at from a theoretical point of view, prosen graph where you just throw links down with random with certain probability or small world graphs which might be relevant for things like biology where you've got some heavily connected nodes and some nodes that are weakly connected and I'll leave you with that, thank you.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it seems like small world is can be really hard.",
                    "label": 0
                },
                {
                    "sent": "It is difficult to deal with a case where you have.",
                    "label": 0
                },
                {
                    "sent": "Weakling.",
                    "label": 0
                },
                {
                    "sent": "Have some highly interconnected.",
                    "label": 0
                },
                {
                    "sent": "Possibly I'm not.",
                    "label": 0
                },
                {
                    "sent": "I haven't thought about the small world case in any detail.",
                    "label": 0
                },
                {
                    "sent": "So for example, I don't know what the loop lengths are in the small world graphs, whether there's a regime where there's any sort of reasonable treelike approximation one can use.",
                    "label": 0
                },
                {
                    "sent": "I mean, even if that doesn't work, I mean certainly the theoretical approach described.",
                    "label": 0
                },
                {
                    "sent": "Would need as input only the eigenvalue density of the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "I think there's been at least some work on that on small world graphs, although I don't have the, I don't know what the results actually are, but so the.",
                    "label": 0
                },
                {
                    "sent": "For Calkins learning curves, you just need to know the the graph spectrum.",
                    "label": 0
                },
                {
                    "sent": "Basically, and that's known for a lot of these graph ensembles, at least to some reasonable approximation.",
                    "label": 0
                },
                {
                    "sent": "Several times of random graphs of degree three regular graph.",
                    "label": 0
                },
                {
                    "sent": "How did you come from uniform?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Is an algorithm due to people?",
                    "label": 0
                },
                {
                    "sent": "Which is known to sample asymptotically uniformly as the number of nodes is large.",
                    "label": 0
                },
                {
                    "sent": "So as we're doing relatively large graphs here, that's a fairly good algorithm for sampling from those.",
                    "label": 0
                },
                {
                    "sent": "Technically, if you wanted to go away from the simple approximation that you were sketching where you started the eigenvalue distribution, do you think it's possible?",
                    "label": 0
                },
                {
                    "sent": "I mean, it seems like it you seem to have a thermodynamic limit type of thing where you introduce a scaling variable.",
                    "label": 0
                },
                {
                    "sent": "What was it in over V an?",
                    "label": 0
                },
                {
                    "sent": "So have you tried really to go into the replicas and all the no, that's too hard for me, Manfred.",
                    "label": 0
                },
                {
                    "sent": "I'll have to get you to do that.",
                    "label": 0
                },
                {
                    "sent": "No, the short answer is no.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried, but it would be interesting to try that.",
                    "label": 0
                },
                {
                    "sent": "Thank you Peter again.",
                    "label": 0
                }
            ]
        }
    }
}