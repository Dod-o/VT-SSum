{
    "id": "dyjbmrdsnbkbijqe2v247vdaljget7tj",
    "title": "A Tensor Spectral Approach to Learning Mixed Membership Community Models",
    "info": {
        "author": [
            "Rong Ge, Department of Computer Science, Princeton University"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_ge_models/",
    "segmentation": [
        [
            "This is strong work with Anima and Kumar from UCI and then use you and John Card from Microsoft New England.",
            "So before talking about learning communities less first."
        ],
        [
            "Try to answer the question what is the community while intuitively community is just a group of people that are better connected to each other than with the rest of people.",
            "For example, in this picture there are two communities, red and blue.",
            "Understanding community has become integral part of social network analysis.",
            "Given the social network, if we can learn how many communities are there and what are these communities, there will be many useful applications.",
            "However, despite the huge interest in communities and a long line of research, there is still no consensus on what is the right, definition or model.",
            "For our community."
        ],
        [
            "One very popular model for learning community is called Stochastic Block model.",
            "In this model, every person is assumed to be in random community.",
            "And people from the same community are more likely to know each other.",
            "From different communities.",
            "So if we connect two people when they are in the same community, we get a graph whose adjacency matrix should look like this.",
            "Sir, very well starting model.",
            "There are many algorithms that are guaranteed to recover the correct community memberships.",
            "Given a graph like this.",
            "There are even many generalizations, but this model, together with all its generalizations, still make one key assumption.",
            "That is, every person can only be in one community, but not in more than one communities."
        ],
        [
            "But in practice, can communities overlap?",
            "Well, let's look at the simple example.",
            "Just consider a computer science Department.",
            "Let's say that we connect to people when they spend a lot of time talking about research to cancer, then probably main community in this graph will be by advisor.",
            "But of course, students from different advisors can also be involved in the same project, and this project itself can be one of their communities.",
            "So in the."
        ],
        [
            "And the graph with all the communities may look like this, and there can indeed be overlapping in communities."
        ],
        [
            "The possibility of overlapping community has become one of the main challenges in learning communities.",
            "Becausw both stochastic block model I just mentioned and many graph partitioning based algorithms do assume communities do not overlap.",
            "What way to overcome this child?"
        ],
        [
            "Which of course is by having a model that allow every people to be in more than one community.",
            "One such model is called mixed membership starcast block model.",
            "Just a son unsupervised model were communities with mixed memberships, meaning every person can be in more than one community.",
            "To Spyro, Diplay, Finberg and thing 2008.",
            "So how does this model work?",
            "Well, unlike the previous model, which fixed?"
        ],
        [
            "One community for every person so small though for every person that make pics mixed.",
            "Membership vector pifi soap.",
            "I will just be a probability distribution on communities.",
            "And if a particular entry is large it means this person spend a lot of time with this community.",
            "This vector is chosen according to directly prior.",
            "Let's recall that directly prior its shape of this prior is determined by some concentration parameter in the parameter is small, so.",
            "Prior is very sparse and when this parameter is large, the vector will not be so sparse.",
            "In the community setting, we are more likely to be in the sparse case, 'cause every person he's not in a single community.",
            "He can be in, say, 10 or 20 communities, but that's very unlikely that anyone is in 1000 communities."
        ],
        [
            "After choosing these mixed membership vectors for each pair of people inj, we first pick random communities S&T for them according to their mix membership vectors, and then we should connect these two people with higher probability.",
            "If, as is equal to T&R lower probability Q if X is not equal to the T."
        ],
        [
            "For this model, there are no known algorithm.",
            "Said are guaranteed to recover the correct mixed membership vectors."
        ],
        [
            "So to better understand how this model works, let's look at this illustration.",
            "Here we have three people.",
            "The first step of the model is to select the mixed memberships, so."
        ],
        [
            "So let's assume these are it's a mix memberships for these people.",
            "So now the second step for this pair of people.",
            "We randomly sample communities according to their mixed membership.",
            "It happens to be the case that these two communities we sampled are the same, so they are likely."
        ],
        [
            "To know each other.",
            "Similarly for this pair of people."
        ],
        [
            "We again sample communities for them.",
            "Notice that the community for this guy is resampled, and this is done for actually for every pair.",
            "So at this time the these two communities are different, so they."
        ],
        [
            "Are unlikely to know each other, and similarly for the third pair."
        ],
        [
            "Sometimes we are interested in the expectation of the Community model conditioned on all these community memberships.",
            "In this case."
        ],
        [
            "Expectation will look like this.",
            "This pair of people may know each other because they share one community.",
            "This pair of people may also know each answer because they also share one community, but it's very unlikely that these two know each other, cause their communities are disjoint."
        ],
        [
            "So in our paper we give the first efficient algorithm that, given a graph generated according to this model, we can learn Community membership vectors.",
            "Our algorithm has very tight sample code complexity guarantees.",
            "It is guaranteed to detect large community memberships when this equation is satisfied.",
            "So P&Q are just some probability we mentioned before in a typical case, let's say left hand side can be a constant.",
            "In this case, what the right hand side is saying is when the concentration parameter is a constant in the natural sparse case.",
            "So number of communities K can be almost as large as the square root of the number of people, so this is almost tight 'cause of the subgraph problem mentioned this morning, and it also matches.",
            "So matches a work by chance Hungary and shoot 2012 worth stochastic block model.",
            "So also our result applies to the more complicated mixed membership model when we apply it to the simple model it matches state of our result for stochastic block model."
        ],
        [
            "Notice that this work produces very different technique and it's it has a bad sample complexity after a long line of work."
        ],
        [
            "Our algorithm is a method of moment algorithm.",
            "So the first step of the algorithm guess what they mean by large screen."
        ],
        [
            "Yeah, large community membership 'cause we have a Community membership vector, so some entries will be large.",
            "Some interest will be very smart.",
            "We will ignore the interest that are very small and identify the interests that are very large.",
            "Largely depends on the value of P&Q.",
            "Let's say when P&Q are all constant, large is also some constant."
        ],
        [
            "So the first step of our algorithm is to collect some moments from the graph, so the algorithm will arbitrarily partition the graph into four parts X and ABC, and the moments we are interested in, our edge counts and three star cons.",
            "So edge counts are actually just adjacency matrix between X and a 1X and B&X and see we will compute the expectation of these adjacency matrix is conditioned on the membership vectors pie."
        ],
        [
            "I claim that the expectation of edge count is equal to this equation.",
            "This is because the probability that you knows X."
        ],
        [
            "Can be computed this way, so this is a membership vector for you.",
            "This P is matrix whose diagonals are larger and off diagnose are smaller and this is vector for X.",
            "So if we call Pi A transpose PF of a we say the expectation of edge count will just be FA times \u03c0 X."
        ],
        [
            "And we are also interested in three stroke on for three Star, can't we pick you from a from B&W from C and we ask, we ask the question how many people in X knows all three of you BMW?",
            "And this is one of the three star count and we have many three star counts because we can choose different use and BS and WS all these three star counts are represented using a 3 dimensional tensor.",
            "And we are all."
        ],
        [
            "Oconto computer expectation of that tensor you have to believe me after some calculation and slightly cheating.",
            "This tensor turns out to be a rank tensor, and it is very closely related to this F matrix, which just appeared in the expectation of edge counts.",
            "Using just these two kinds of information, our algorithm will be able to learn the matrix is F and \u03c0.",
            "So now you."
        ],
        [
            "I wonder why do you use three star?",
            "Why don't you use two star or four star?",
            "Well, it's a short answer is 2 star does not suffice because there are different sets of parameters that can give rise to the same tool store count.",
            "Three star works, so we don't want to use a more complicated 4 star.",
            "And there's also an interesting analog between learning mixed membership community models and topic modeling, and in fact, the reason we use three star is the same reason as latent directly allocation algorithm in Afh KL 2012.",
            "Why is this algorithm uses three word for document?",
            "To make this analogue more pretty."
        ],
        [
            "Nice.",
            "Let's first recall a topic.",
            "Modeling is just we are given a bunch of documents and we want to understand what are the underlying topic structure.",
            "So to reduce community learning to topic modeling.",
            "So we should first call."
        ],
        [
            "Communities, topics and since we have partitions of graph into four parts, let's think that people in ABC.",
            "They are all holding a blackboard and on the board is English word."
        ],
        [
            "So we build as."
        ],
        [
            "We will people in ABC as words in vocabulary and then we can view."
        ],
        [
            "And then X as documents.",
            "And what are the words of this document?",
            "While for every person I in X?",
            "If this person."
        ],
        [
            "I know some people J in AB or C, then we say that word change should appear in document I.",
            "So in this way we give for reductions that reduces Community learning to topic modeling.",
            "And to make this more precise, let's look at actually the moment expected."
        ],
        [
            "I just mentioned the expectation of the adjacency matrix or edge count for our model is for this MSP model is FA times \u03c0 of X.",
            "Similarly for the latent directly allocation or model for topic modeling, the expectation of some matrix called document term matrix is in fact all times \u03c0.",
            "Here these two are really similar because both Pi of X and Pi are matrices.",
            "Whose columns are chosen according to directly prior.",
            "So mathematically, these two models are very similar and now you may wonder, well you can reduce Community model to topic modeling, and there's a very nice algorithm for topic modeling social.",
            "We just apply this algorithm and we are done."
        ],
        [
            "Well, not really big cause people are anyways still different from documents and words.",
            "So subtle correlation issue when we work with Community model that can only be solved when these sets AB&C are large.",
            "On the other hand, topic modeling algorithm I just mentioned requires a number of documents.",
            "Should be much larger than the effective vocabulary science.",
            "And translate this back to the Community model.",
            "We have to have the size of X should be much larger than the size of a B&C.",
            "But these two are obviously contradicting with each other, because when X is larger, ABC must be smaller.",
            "In order to overcome this conflict, we need really tight matrix and tensor concentration results.",
            "Basically we need to be very careful in every step of our algorithm in order to get a sample complexity that matches the best known bound for Star castec block model."
        ],
        [
            "So here's the outline of our algorithm.",
            "So first step is of course collecting moments, but after that we do something called simultaneous, frightening.",
            "Explain this later, but in this step what's important is we need to use matrix Princeton bonds to get really tight matrix concentration bounds.",
            "We then use a technique called tensor power method that appeared in this paper Age HKT 2012.",
            "It's a step we need to prove tensor concentration bounds, and we also need to carefully initialize this tensor power method.",
            "And finally, the final step is called thresholding, which is very important if we would like to get exact membership for stochastic block model.",
            "And this is also very useful in improving the guarantee for a mixed membership."
        ],
        [
            "Low.",
            "So first what is simultaneous widening?",
            "Let's first recall what is whitening whiten inserts for any matrix F of a.",
            "We can find a linear transformation W of a that transform columns of this matrix into and also normal basis.",
            "But we have more than one matrix.",
            "We have also F of B. Shall we do the same for FB?",
            "Well, the only problem is if we do these independently.",
            "So also normal basis will be different for A&B, so we need to use some other matrix.",
            "Work at about the equation."
        ],
        [
            "Idea is we can find 3 linear transformations that simultaneously transform FASB NFC to the same also normal basis, and these three matrices are called symbol tanias Lee widening matrixes."
        ],
        [
            "After we get these matrices, we apply these to the three star contents are and we get tensor of slightly nicer form.",
            "It's nicer because these are eyes are also orthogonal to each other.",
            "What we want now is to find out what are the slammed I NRI which are actually the eigenvalues and eigenvectors of this tensor.",
            "So to do that we use tensor power method.",
            "Unfortunately I won't have time to go into the details, but the idea is very similar to matrix power method.",
            "And the key new idea here is in order to get better sample complexity bond, we need to use better initial vectors that are obtained from white, an adjacency matrix."
        ],
        [
            "And the final step will be thresholding after the first three steps, our algorithm already has estimation for the Pi matrix.",
            "That's close to the true prime matrix in L2 norm, but for community is really the natural scaling will be in L1 norm cause mixed membership vector is a probability distribution.",
            "So in order to get stronger L1 guarantee we use the fact that Community memberships are sparse because all these columns are chosen according to directly prior.",
            "So we show that we can simply remove interests that are below a certain threshold, and because of the natural or the nature of the directly prior, this will give us an L1 guarantee, and in particular the thresholding step is very important if we want to get exact recovery result for stochastic block model which allow us to compare our results to previous works."
        ],
        [
            "So that's roughly how our algorithm work, so there are still many open problems in Community learning.",
            "So first we would like to implement our algorithm and see how it works in practice.",
            "And also.",
            "It would be nice if we can get parameter estimation algorithms for other different models that also have the feature of overlapping communities.",
            "And finally, maybe more importantly, for Community detection, what is the right definition or model or what is the assumptions we can make when we are trying to look for communities in social networks?",
            "Sing"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is strong work with Anima and Kumar from UCI and then use you and John Card from Microsoft New England.",
                    "label": 0
                },
                {
                    "sent": "So before talking about learning communities less first.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Try to answer the question what is the community while intuitively community is just a group of people that are better connected to each other than with the rest of people.",
                    "label": 1
                },
                {
                    "sent": "For example, in this picture there are two communities, red and blue.",
                    "label": 1
                },
                {
                    "sent": "Understanding community has become integral part of social network analysis.",
                    "label": 0
                },
                {
                    "sent": "Given the social network, if we can learn how many communities are there and what are these communities, there will be many useful applications.",
                    "label": 0
                },
                {
                    "sent": "However, despite the huge interest in communities and a long line of research, there is still no consensus on what is the right, definition or model.",
                    "label": 0
                },
                {
                    "sent": "For our community.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One very popular model for learning community is called Stochastic Block model.",
                    "label": 1
                },
                {
                    "sent": "In this model, every person is assumed to be in random community.",
                    "label": 0
                },
                {
                    "sent": "And people from the same community are more likely to know each other.",
                    "label": 1
                },
                {
                    "sent": "From different communities.",
                    "label": 0
                },
                {
                    "sent": "So if we connect two people when they are in the same community, we get a graph whose adjacency matrix should look like this.",
                    "label": 0
                },
                {
                    "sent": "Sir, very well starting model.",
                    "label": 0
                },
                {
                    "sent": "There are many algorithms that are guaranteed to recover the correct community memberships.",
                    "label": 0
                },
                {
                    "sent": "Given a graph like this.",
                    "label": 0
                },
                {
                    "sent": "There are even many generalizations, but this model, together with all its generalizations, still make one key assumption.",
                    "label": 0
                },
                {
                    "sent": "That is, every person can only be in one community, but not in more than one communities.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in practice, can communities overlap?",
                    "label": 0
                },
                {
                    "sent": "Well, let's look at the simple example.",
                    "label": 0
                },
                {
                    "sent": "Just consider a computer science Department.",
                    "label": 1
                },
                {
                    "sent": "Let's say that we connect to people when they spend a lot of time talking about research to cancer, then probably main community in this graph will be by advisor.",
                    "label": 1
                },
                {
                    "sent": "But of course, students from different advisors can also be involved in the same project, and this project itself can be one of their communities.",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the graph with all the communities may look like this, and there can indeed be overlapping in communities.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The possibility of overlapping community has become one of the main challenges in learning communities.",
                    "label": 0
                },
                {
                    "sent": "Becausw both stochastic block model I just mentioned and many graph partitioning based algorithms do assume communities do not overlap.",
                    "label": 0
                },
                {
                    "sent": "What way to overcome this child?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which of course is by having a model that allow every people to be in more than one community.",
                    "label": 0
                },
                {
                    "sent": "One such model is called mixed membership starcast block model.",
                    "label": 1
                },
                {
                    "sent": "Just a son unsupervised model were communities with mixed memberships, meaning every person can be in more than one community.",
                    "label": 1
                },
                {
                    "sent": "To Spyro, Diplay, Finberg and thing 2008.",
                    "label": 0
                },
                {
                    "sent": "So how does this model work?",
                    "label": 0
                },
                {
                    "sent": "Well, unlike the previous model, which fixed?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One community for every person so small though for every person that make pics mixed.",
                    "label": 0
                },
                {
                    "sent": "Membership vector pifi soap.",
                    "label": 0
                },
                {
                    "sent": "I will just be a probability distribution on communities.",
                    "label": 0
                },
                {
                    "sent": "And if a particular entry is large it means this person spend a lot of time with this community.",
                    "label": 0
                },
                {
                    "sent": "This vector is chosen according to directly prior.",
                    "label": 0
                },
                {
                    "sent": "Let's recall that directly prior its shape of this prior is determined by some concentration parameter in the parameter is small, so.",
                    "label": 0
                },
                {
                    "sent": "Prior is very sparse and when this parameter is large, the vector will not be so sparse.",
                    "label": 0
                },
                {
                    "sent": "In the community setting, we are more likely to be in the sparse case, 'cause every person he's not in a single community.",
                    "label": 0
                },
                {
                    "sent": "He can be in, say, 10 or 20 communities, but that's very unlikely that anyone is in 1000 communities.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After choosing these mixed membership vectors for each pair of people inj, we first pick random communities S&T for them according to their mix membership vectors, and then we should connect these two people with higher probability.",
                    "label": 0
                },
                {
                    "sent": "If, as is equal to T&R lower probability Q if X is not equal to the T.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this model, there are no known algorithm.",
                    "label": 0
                },
                {
                    "sent": "Said are guaranteed to recover the correct mixed membership vectors.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to better understand how this model works, let's look at this illustration.",
                    "label": 0
                },
                {
                    "sent": "Here we have three people.",
                    "label": 0
                },
                {
                    "sent": "The first step of the model is to select the mixed memberships, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's assume these are it's a mix memberships for these people.",
                    "label": 0
                },
                {
                    "sent": "So now the second step for this pair of people.",
                    "label": 0
                },
                {
                    "sent": "We randomly sample communities according to their mixed membership.",
                    "label": 0
                },
                {
                    "sent": "It happens to be the case that these two communities we sampled are the same, so they are likely.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To know each other.",
                    "label": 0
                },
                {
                    "sent": "Similarly for this pair of people.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We again sample communities for them.",
                    "label": 0
                },
                {
                    "sent": "Notice that the community for this guy is resampled, and this is done for actually for every pair.",
                    "label": 0
                },
                {
                    "sent": "So at this time the these two communities are different, so they.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are unlikely to know each other, and similarly for the third pair.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sometimes we are interested in the expectation of the Community model conditioned on all these community memberships.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expectation will look like this.",
                    "label": 0
                },
                {
                    "sent": "This pair of people may know each other because they share one community.",
                    "label": 0
                },
                {
                    "sent": "This pair of people may also know each answer because they also share one community, but it's very unlikely that these two know each other, cause their communities are disjoint.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our paper we give the first efficient algorithm that, given a graph generated according to this model, we can learn Community membership vectors.",
                    "label": 1
                },
                {
                    "sent": "Our algorithm has very tight sample code complexity guarantees.",
                    "label": 0
                },
                {
                    "sent": "It is guaranteed to detect large community memberships when this equation is satisfied.",
                    "label": 1
                },
                {
                    "sent": "So P&Q are just some probability we mentioned before in a typical case, let's say left hand side can be a constant.",
                    "label": 0
                },
                {
                    "sent": "In this case, what the right hand side is saying is when the concentration parameter is a constant in the natural sparse case.",
                    "label": 0
                },
                {
                    "sent": "So number of communities K can be almost as large as the square root of the number of people, so this is almost tight 'cause of the subgraph problem mentioned this morning, and it also matches.",
                    "label": 0
                },
                {
                    "sent": "So matches a work by chance Hungary and shoot 2012 worth stochastic block model.",
                    "label": 0
                },
                {
                    "sent": "So also our result applies to the more complicated mixed membership model when we apply it to the simple model it matches state of our result for stochastic block model.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notice that this work produces very different technique and it's it has a bad sample complexity after a long line of work.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our algorithm is a method of moment algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the first step of the algorithm guess what they mean by large screen.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, large community membership 'cause we have a Community membership vector, so some entries will be large.",
                    "label": 0
                },
                {
                    "sent": "Some interest will be very smart.",
                    "label": 0
                },
                {
                    "sent": "We will ignore the interest that are very small and identify the interests that are very large.",
                    "label": 0
                },
                {
                    "sent": "Largely depends on the value of P&Q.",
                    "label": 0
                },
                {
                    "sent": "Let's say when P&Q are all constant, large is also some constant.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first step of our algorithm is to collect some moments from the graph, so the algorithm will arbitrarily partition the graph into four parts X and ABC, and the moments we are interested in, our edge counts and three star cons.",
                    "label": 0
                },
                {
                    "sent": "So edge counts are actually just adjacency matrix between X and a 1X and B&X and see we will compute the expectation of these adjacency matrix is conditioned on the membership vectors pie.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I claim that the expectation of edge count is equal to this equation.",
                    "label": 0
                },
                {
                    "sent": "This is because the probability that you knows X.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be computed this way, so this is a membership vector for you.",
                    "label": 0
                },
                {
                    "sent": "This P is matrix whose diagonals are larger and off diagnose are smaller and this is vector for X.",
                    "label": 0
                },
                {
                    "sent": "So if we call Pi A transpose PF of a we say the expectation of edge count will just be FA times \u03c0 X.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we are also interested in three stroke on for three Star, can't we pick you from a from B&W from C and we ask, we ask the question how many people in X knows all three of you BMW?",
                    "label": 0
                },
                {
                    "sent": "And this is one of the three star count and we have many three star counts because we can choose different use and BS and WS all these three star counts are represented using a 3 dimensional tensor.",
                    "label": 0
                },
                {
                    "sent": "And we are all.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oconto computer expectation of that tensor you have to believe me after some calculation and slightly cheating.",
                    "label": 0
                },
                {
                    "sent": "This tensor turns out to be a rank tensor, and it is very closely related to this F matrix, which just appeared in the expectation of edge counts.",
                    "label": 0
                },
                {
                    "sent": "Using just these two kinds of information, our algorithm will be able to learn the matrix is F and \u03c0.",
                    "label": 0
                },
                {
                    "sent": "So now you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I wonder why do you use three star?",
                    "label": 0
                },
                {
                    "sent": "Why don't you use two star or four star?",
                    "label": 0
                },
                {
                    "sent": "Well, it's a short answer is 2 star does not suffice because there are different sets of parameters that can give rise to the same tool store count.",
                    "label": 1
                },
                {
                    "sent": "Three star works, so we don't want to use a more complicated 4 star.",
                    "label": 1
                },
                {
                    "sent": "And there's also an interesting analog between learning mixed membership community models and topic modeling, and in fact, the reason we use three star is the same reason as latent directly allocation algorithm in Afh KL 2012.",
                    "label": 0
                },
                {
                    "sent": "Why is this algorithm uses three word for document?",
                    "label": 0
                },
                {
                    "sent": "To make this analogue more pretty.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice.",
                    "label": 0
                },
                {
                    "sent": "Let's first recall a topic.",
                    "label": 0
                },
                {
                    "sent": "Modeling is just we are given a bunch of documents and we want to understand what are the underlying topic structure.",
                    "label": 0
                },
                {
                    "sent": "So to reduce community learning to topic modeling.",
                    "label": 1
                },
                {
                    "sent": "So we should first call.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Communities, topics and since we have partitions of graph into four parts, let's think that people in ABC.",
                    "label": 0
                },
                {
                    "sent": "They are all holding a blackboard and on the board is English word.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we build as.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will people in ABC as words in vocabulary and then we can view.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then X as documents.",
                    "label": 0
                },
                {
                    "sent": "And what are the words of this document?",
                    "label": 0
                },
                {
                    "sent": "While for every person I in X?",
                    "label": 1
                },
                {
                    "sent": "If this person.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I know some people J in AB or C, then we say that word change should appear in document I.",
                    "label": 0
                },
                {
                    "sent": "So in this way we give for reductions that reduces Community learning to topic modeling.",
                    "label": 1
                },
                {
                    "sent": "And to make this more precise, let's look at actually the moment expected.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I just mentioned the expectation of the adjacency matrix or edge count for our model is for this MSP model is FA times \u03c0 of X.",
                    "label": 0
                },
                {
                    "sent": "Similarly for the latent directly allocation or model for topic modeling, the expectation of some matrix called document term matrix is in fact all times \u03c0.",
                    "label": 0
                },
                {
                    "sent": "Here these two are really similar because both Pi of X and Pi are matrices.",
                    "label": 0
                },
                {
                    "sent": "Whose columns are chosen according to directly prior.",
                    "label": 0
                },
                {
                    "sent": "So mathematically, these two models are very similar and now you may wonder, well you can reduce Community model to topic modeling, and there's a very nice algorithm for topic modeling social.",
                    "label": 1
                },
                {
                    "sent": "We just apply this algorithm and we are done.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, not really big cause people are anyways still different from documents and words.",
                    "label": 1
                },
                {
                    "sent": "So subtle correlation issue when we work with Community model that can only be solved when these sets AB&C are large.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, topic modeling algorithm I just mentioned requires a number of documents.",
                    "label": 0
                },
                {
                    "sent": "Should be much larger than the effective vocabulary science.",
                    "label": 1
                },
                {
                    "sent": "And translate this back to the Community model.",
                    "label": 0
                },
                {
                    "sent": "We have to have the size of X should be much larger than the size of a B&C.",
                    "label": 0
                },
                {
                    "sent": "But these two are obviously contradicting with each other, because when X is larger, ABC must be smaller.",
                    "label": 0
                },
                {
                    "sent": "In order to overcome this conflict, we need really tight matrix and tensor concentration results.",
                    "label": 0
                },
                {
                    "sent": "Basically we need to be very careful in every step of our algorithm in order to get a sample complexity that matches the best known bound for Star castec block model.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the outline of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So first step is of course collecting moments, but after that we do something called simultaneous, frightening.",
                    "label": 0
                },
                {
                    "sent": "Explain this later, but in this step what's important is we need to use matrix Princeton bonds to get really tight matrix concentration bounds.",
                    "label": 0
                },
                {
                    "sent": "We then use a technique called tensor power method that appeared in this paper Age HKT 2012.",
                    "label": 0
                },
                {
                    "sent": "It's a step we need to prove tensor concentration bounds, and we also need to carefully initialize this tensor power method.",
                    "label": 1
                },
                {
                    "sent": "And finally, the final step is called thresholding, which is very important if we would like to get exact membership for stochastic block model.",
                    "label": 1
                },
                {
                    "sent": "And this is also very useful in improving the guarantee for a mixed membership.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Low.",
                    "label": 0
                },
                {
                    "sent": "So first what is simultaneous widening?",
                    "label": 0
                },
                {
                    "sent": "Let's first recall what is whitening whiten inserts for any matrix F of a.",
                    "label": 0
                },
                {
                    "sent": "We can find a linear transformation W of a that transform columns of this matrix into and also normal basis.",
                    "label": 0
                },
                {
                    "sent": "But we have more than one matrix.",
                    "label": 0
                },
                {
                    "sent": "We have also F of B. Shall we do the same for FB?",
                    "label": 1
                },
                {
                    "sent": "Well, the only problem is if we do these independently.",
                    "label": 1
                },
                {
                    "sent": "So also normal basis will be different for A&B, so we need to use some other matrix.",
                    "label": 0
                },
                {
                    "sent": "Work at about the equation.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea is we can find 3 linear transformations that simultaneously transform FASB NFC to the same also normal basis, and these three matrices are called symbol tanias Lee widening matrixes.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After we get these matrices, we apply these to the three star contents are and we get tensor of slightly nicer form.",
                    "label": 0
                },
                {
                    "sent": "It's nicer because these are eyes are also orthogonal to each other.",
                    "label": 0
                },
                {
                    "sent": "What we want now is to find out what are the slammed I NRI which are actually the eigenvalues and eigenvectors of this tensor.",
                    "label": 0
                },
                {
                    "sent": "So to do that we use tensor power method.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately I won't have time to go into the details, but the idea is very similar to matrix power method.",
                    "label": 0
                },
                {
                    "sent": "And the key new idea here is in order to get better sample complexity bond, we need to use better initial vectors that are obtained from white, an adjacency matrix.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the final step will be thresholding after the first three steps, our algorithm already has estimation for the Pi matrix.",
                    "label": 0
                },
                {
                    "sent": "That's close to the true prime matrix in L2 norm, but for community is really the natural scaling will be in L1 norm cause mixed membership vector is a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So in order to get stronger L1 guarantee we use the fact that Community memberships are sparse because all these columns are chosen according to directly prior.",
                    "label": 1
                },
                {
                    "sent": "So we show that we can simply remove interests that are below a certain threshold, and because of the natural or the nature of the directly prior, this will give us an L1 guarantee, and in particular the thresholding step is very important if we want to get exact recovery result for stochastic block model which allow us to compare our results to previous works.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's roughly how our algorithm work, so there are still many open problems in Community learning.",
                    "label": 0
                },
                {
                    "sent": "So first we would like to implement our algorithm and see how it works in practice.",
                    "label": 0
                },
                {
                    "sent": "And also.",
                    "label": 0
                },
                {
                    "sent": "It would be nice if we can get parameter estimation algorithms for other different models that also have the feature of overlapping communities.",
                    "label": 1
                },
                {
                    "sent": "And finally, maybe more importantly, for Community detection, what is the right definition or model or what is the assumptions we can make when we are trying to look for communities in social networks?",
                    "label": 0
                },
                {
                    "sent": "Sing",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}