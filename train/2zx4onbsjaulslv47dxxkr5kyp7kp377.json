{
    "id": "2zx4onbsjaulslv47dxxkr5kyp7kp377",
    "title": "Accelerated Gibbs Sampling for the Indian Buffet Process",
    "info": {
        "author": [
            "Finale Doshi, Department of Engineering, University of Cambridge"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_doshi_velez_ags/",
    "segmentation": [
        [
            "Thank you very much.",
            "So even though the title of this paper is accelerated, Gibbs sampling for the Indian buffet process, what?"
        ],
        [
            "I'm going to be talking about today is an inference technique that's really applicable for a much more general class of models that come up a lot in machine learning, and these are models of the form where your data is equal to some matrix product plus some error, and these things come up all that."
        ],
        [
            "Time factor analysis, which we just heard about earlier.",
            "Lots of applications in sociology and other place."
        ],
        [
            "This probabilistic PCA again has the same form."
        ],
        [
            "Um, probabilistic matrix factorization, which is quite general, recently been used a lot in recommender systems."
        ],
        [
            "And finally the the model that I'm most interested in personally, which is the Indian buffet process with a linear likelihood and kind of as we heard earlier, this is a situation where the Indian buffet process is telling us what features are present in a data and we have another matrix representing what the features look like, and we're just combining them in a linear way."
        ],
        [
            "So it's common in all of these models.",
            "Again, is that we have the data being produced by some some product of a matrix plus some air an.",
            "Let's suppose that we're interested in doing large scale Bayesian inference in these models.",
            "So being Bayesian, we're going to be putting priors on both the zed and the A an.",
            "We're going to be assuming that we have a large number of samples.",
            "The inference technique that we've developed is useful under a certain set of conditions.",
            "So suppose that.",
            "We can integrate out the A so, so in my case this is going to be the factor.",
            "The fact the set of factors that's the Indian buffet process and this is going to be the features.",
            "So let's say we can integrate out the features.",
            "But this is really expensive so it's an integral that we can do, but we'd rather not do it if we can avoid it.",
            "We can compute the probability of a given X and said that's the posterior on the features and you know the given these two.",
            "This is just a linear model.",
            "Hopefully that's easy to do in a lot of situations.",
            "And finally the form of this model is such that we cannot directly represent the joint posterior.",
            "The probability of zedan a given X, and that's the thing that we're really interested in.",
            "And So what we're going to do is we're going to use sampling.",
            "We're going to use Gibbs sampling.",
            "To draw samples from this posterior, and if that's our goal, then I'm going to be describing a fast technique to do inference in these sorts of."
        ],
        [
            "Situations.",
            "Now before I go into the details of the inference, just here's a quick overview of the Indian buffet process.",
            "So we saw the Beta Bernoulli description of it just in the last talk.",
            "Another way to look at it is that it."
        ],
        [
            "Say we have an infinite buffet that represents our features and then customers come into the buffet.",
            "The first one picks out a few dishes.",
            "Other cuss"
        ],
        [
            "Others come in and sample new dishes."
        ],
        [
            "Based on their popularity and then sometimes they also pick new dishes as well."
        ],
        [
            "Now, what does this have to do with actual practical things?",
            "Well, if we assume that these features, the dishes are now features, the customers are data points and we have a prior that tells us what features are present in which data points.",
            "And the nice thing about this prior is that we don't have to limit the number of features a priority, but at the same time we're guaranteed that in a finite data set there will only be a finite number of features expressed.",
            "The other nice property about the Indian buffet process is the observations are exchangeable, so the way that I drew this through this out, it looks like the first customer kind of affects what the second customer does, etc.",
            "But we can actually reorder these these rows and the distribution is unchanged."
        ],
        [
            "OK, so now that we've got the prior.",
            "That's the prior on this said matrix right here.",
            "Let's put it together with the remaining pieces.",
            "So now we have the features A and the data X and I'm going to refer to N as the number of observations that we have, and D is the dimensions.",
            "So we're dealing in situations where this is large, where we have a large end."
        ],
        [
            "In those sorts of situations, maybe we don't want to deal with all of the data at the same time.",
            "There's just too much of it.",
            "So what our technique is going to do is we're going to say, well, what if we can only afford to look at part of the data at a time?",
            "So we're going to refer to XW, where W is for for window as a small window of the data, an it has a corresponding zed W, and that is the corresponding feature assignments that go with, and I'm going to just note right now.",
            "This is not going to be blocked Gibbs sampling 'cause we're still going to be doing normal Gibbs sampling.",
            "We're only going to be looking at one piece of the data at a."
        ],
        [
            "I'm great, so now we have that model in place.",
            "What is it correspond to in terms of a graphical model?",
            "Well, here's the data and it's being produced by its feature assignments and the values of the features, and similarly for all the data that we're not dealing with right now."
        ],
        [
            "Well, um.",
            "In this situation, if we're doing just normal Gibbs sampling, what do we do?",
            "Well, we can start out by sampling all the elements in here, one at a time, again not as a block.",
            "All the elements in here, all the feature assignments given everything else."
        ],
        [
            "Then we can turn around and sample everything in here.",
            "Given everything else, notice that I swapped the arrow here.",
            "That's again, that's a specific property of the Indian buffet process, because we can imagine the set that we're working with to be kind of the last set of customers that came into the buffet.",
            "Now, if you have a model where you don't have this dependency, then obviously this error would not would."
        ],
        [
            "Be there.",
            "We can re sample what the feature values actually are."
        ],
        [
            "And then repeat so this is a standard, uncollapsed Gibbs sampler."
        ],
        [
            "The advantage is that each iteration is fast to compute, or at least we hope it is.",
            "If that's not fast to compute, we've got other problems, but the disadvantage is that it's often slow to mix because we're explicitly representing each of these."
        ],
        [
            "Things.",
            "So what's the other alternative?",
            "Well, usually when we're doing Gibbs sampling, we're trying to decide whether we should collapse something or whether we should leave it uncollapsed.",
            "So recall that I said that.",
            "Suppose we can we can integrate out the A so we can compute this."
        ],
        [
            "If X given said well, let's go ahead and do that.",
            "Well, what that does is that's going to kind of mash everything together, and so even though we can do this."
        ],
        [
            "Same sort of sampling as before."
        ],
        [
            "We have this.",
            "We have this issue that."
        ],
        [
            "Inference no longer scale, so it will be faster to mix at least in like the Indian buffet process with with say Gaussian likelihood model.",
            "But since whenever I'm sampling stuff in here now I'm dependent on stuff over here, I've got problems in terms of scaling to very large datasets."
        ],
        [
            "So what our solution is in terms of our accelerated sampler, is to instead of either neither representing the features explicitly or to integrate them out completely.",
            "We keep a posterior on the features, so this node you can think about it intuitively.",
            "This note is still here, so the the feature assignments here are still independent of the data that's sitting up here.",
            "However, because we're keeping a posterior now.",
            "There is still that fuzziness that we get from the collapse sampler that allows things to move in a more."
        ],
        [
            "Efficient way.",
            "So now just a bit more formally, just to show you that this is exact, let's say now just one element here, so just one row we're considering that.",
            "Let's say that I want to re sample one of these elements.",
            "So the probability that some zed NK is equal to 1, well, that just using Bayes rule is going to be proportional to."
        ],
        [
            "You, uh, the prior the prior term, which is the probability of that N = 1 given the rest of this ad matrix times the likelihood term over here.",
            "Now I'm assuming that in whatever model you're working with, this one is easy to compute in the ICP model, this requires just summing up a set of accounts, so this is a very scalable, and then we have this trip over here where I."
        ],
        [
            "And expand it in terms of the the X by representing a explicitly."
        ],
        [
            "And then if I represent a explicitly, I can split up this likelihood term here into two pieces because given a.",
            "And the Zed's these two pieces are independent of each other.",
            "And then I'm going to just apply Bayes rule one more time to combine these two terms together and."
        ],
        [
            "Thus, what I end up with is an expression for sampling.",
            "Sampling a single element of the assignment matrix which has the the X and the the rest of the zed matrix only inside the posterior.",
            "And this is nice because we no longer have to deal with all of the data when sampling one element.",
            "We only have to deal with this posterior over here.",
            "And again, this is exact.",
            "We haven't done anything that approximate in this in this approach.",
            "So gay."
        ],
        [
            "In this, given this observation, here is the algorithm that goes with.",
            "So we initialize some sort of feature assignments and some feature posterior, and now for each window of observations we start out, we have some posterior over our features.",
            "We remove the effect of the window, so we get the posterior without the window there.",
            "So just in this cartoon we can imagine it is kind of flattening out that posterior.",
            "Now, given this right here, that corresponds to this term right here.",
            "We can perform inference on are just small window of data and once where.",
            "Once we're happy with that, we can reconstruct the full posterior using that using that new set of assignments.",
            "And then we move on to a new window.",
            "Now some of the considerations when doing this is how many observations should we consider at once?",
            "How large should the window be an?",
            "That really depends on the cost, the relative costs of computing the feature posterior.",
            "Updating the feature posterior and computing the likelihood.",
            "So if this right here is expensive, then maybe we want the window to be larger 'cause we need to this bit right here is going to get expensive right here, but if computing the likelihood is expensive then maybe we want the window to be smaller.",
            "So this bit does not take a lot of computation.",
            "Another side issue is that depending on how you're modeling this, you can run into some numerical issues and that may also affect what sort of window sizes."
        ],
        [
            "You want to work with.",
            "So so far I've kept it pretty general.",
            "This can apply to any sort of bilinear model, or really even more general models that where you have this kind of property now, in particular for the Indian buffet process, let's suppose that the prior on the features and the noise is also Gaussian.",
            "In this situation, the posterior on the features will also be Gaussian.",
            "It can be updated, uh, efficiently with the.",
            "Rank 1 updates.",
            "And because this update is actually extremely efficient, the optimal window size turns out to be one.",
            "Just as a side note, now we mention this a little bit more in our paper, but intelligently choosing how to represent the Gaussians can be pretty important for maintaining numerical accuracy.",
            "That is, whether you use the inverse or information form or the full covariance form.",
            "But again, details about that in the paper or also talk to me later."
        ],
        [
            "OK, so next we did a series of experiments on synthetic data, so this is just generated from the prior an we have a series of sizes of datasets, all pretty small, but just to illustrate the point now, it was hard to see, But basically the takeaway point here is that here we're measuring mixing ineffective samples per sample.",
            "And the accelerated Gibbs sampler is right here.",
            "The collapsed Gibbs sampler is right here and you can see that they have similar mixing properties.",
            "Again, this is just a run from one experiment we can show mathematically that they should have identical mixing properties and now we look at the running time.",
            "So now we have runtime here and the number of observations and this is on a log scale.",
            "If you can't see the axes very well, so the slope is what's important and we see that the evolution in running time as we increase the number of samplers.",
            "Samples is the same for the accelerated Gibbs sampler and for the uncollapsed one, whereas the collapse one has a higher slope and it turns out in our particular case that these are both linear in the number of observations and this one is cubic."
        ],
        [
            "So then we ran this on different datasets which were not drawn from the prior.",
            "So over here we have a really popular block images data set that's featured in a lot of Indian buffet process type papers where N = 1000 and it's just a small 36 dimensional set of pixels.",
            "And here here's the accelerated sampler over here, Uncollapsed and.",
            "Fast and you can see there kind of all going to kind of the same place in terms of finding a mode to explore that.",
            "They're kind of happy to explore.",
            "This is a log scale though on the plot, so we're reaching places where these samplers are going.",
            "At least two orders of magnitude faster in terms of runtime.",
            "Here's a slightly more complicated data set.",
            "These are dimodica emoticons downloaded off the Web 722 of them.",
            "And again, here is our sampler, and it's kind of getting to an equilibrium again.",
            "The log scale kind of squashes together all of the points.",
            "Towards the end, about 3 orders of magnitude faster then."
        ],
        [
            "The other samplers.",
            "So moving on to even larger datasets, and this is just to show that we can scale to things that you could never scale the ICP too before it had been limited largely to toy problems.",
            "This is a faces frontal face image data set with 2600 observations and close to 1600 dimensions.",
            "This is a data set of piano music with 10,000 samples, and in these cases, so here you did see the sampler starting to.",
            "Starting to pick up here, but the our accelerated sampler just does just fine.",
            "On these datasets.",
            "We ran the other samplers.",
            "They weren't able to complete an iteration in about a day and a half, and at that point we kind of gave up.",
            "We let them run for about a week and didn't really get anywhere."
        ],
        [
            "So we began so the question.",
            "Often when we're doing Gibbs sampling, is you marginalized out a variable?",
            "Do you keep it?",
            "And I guess one of the main takeaways from this talk is that maybe there's a third."
        ],
        [
            "So in conclusion."
        ],
        [
            "Maintaining a posterior within a sampler can allow us to perform fast interference in an important class of models, and this could be a very useful technique.",
            "For things much more general than the Indian buffet process and then in particular we were able to show that by using this technique for the Indian buffet process with linear Gaussian models, we can scale inference to data set sizes that were previously not possible.",
            "And if you're curious, if you want to play around with this code for all the different samplers is available at my website and I'd be happy to answer any questions on that.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So even though the title of this paper is accelerated, Gibbs sampling for the Indian buffet process, what?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to be talking about today is an inference technique that's really applicable for a much more general class of models that come up a lot in machine learning, and these are models of the form where your data is equal to some matrix product plus some error, and these things come up all that.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time factor analysis, which we just heard about earlier.",
                    "label": 0
                },
                {
                    "sent": "Lots of applications in sociology and other place.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This probabilistic PCA again has the same form.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, probabilistic matrix factorization, which is quite general, recently been used a lot in recommender systems.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally the the model that I'm most interested in personally, which is the Indian buffet process with a linear likelihood and kind of as we heard earlier, this is a situation where the Indian buffet process is telling us what features are present in a data and we have another matrix representing what the features look like, and we're just combining them in a linear way.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's common in all of these models.",
                    "label": 0
                },
                {
                    "sent": "Again, is that we have the data being produced by some some product of a matrix plus some air an.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose that we're interested in doing large scale Bayesian inference in these models.",
                    "label": 1
                },
                {
                    "sent": "So being Bayesian, we're going to be putting priors on both the zed and the A an.",
                    "label": 0
                },
                {
                    "sent": "We're going to be assuming that we have a large number of samples.",
                    "label": 0
                },
                {
                    "sent": "The inference technique that we've developed is useful under a certain set of conditions.",
                    "label": 0
                },
                {
                    "sent": "So suppose that.",
                    "label": 0
                },
                {
                    "sent": "We can integrate out the A so, so in my case this is going to be the factor.",
                    "label": 0
                },
                {
                    "sent": "The fact the set of factors that's the Indian buffet process and this is going to be the features.",
                    "label": 0
                },
                {
                    "sent": "So let's say we can integrate out the features.",
                    "label": 0
                },
                {
                    "sent": "But this is really expensive so it's an integral that we can do, but we'd rather not do it if we can avoid it.",
                    "label": 0
                },
                {
                    "sent": "We can compute the probability of a given X and said that's the posterior on the features and you know the given these two.",
                    "label": 0
                },
                {
                    "sent": "This is just a linear model.",
                    "label": 0
                },
                {
                    "sent": "Hopefully that's easy to do in a lot of situations.",
                    "label": 0
                },
                {
                    "sent": "And finally the form of this model is such that we cannot directly represent the joint posterior.",
                    "label": 0
                },
                {
                    "sent": "The probability of zedan a given X, and that's the thing that we're really interested in.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do is we're going to use sampling.",
                    "label": 0
                },
                {
                    "sent": "We're going to use Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "To draw samples from this posterior, and if that's our goal, then I'm going to be describing a fast technique to do inference in these sorts of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Situations.",
                    "label": 0
                },
                {
                    "sent": "Now before I go into the details of the inference, just here's a quick overview of the Indian buffet process.",
                    "label": 1
                },
                {
                    "sent": "So we saw the Beta Bernoulli description of it just in the last talk.",
                    "label": 0
                },
                {
                    "sent": "Another way to look at it is that it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say we have an infinite buffet that represents our features and then customers come into the buffet.",
                    "label": 1
                },
                {
                    "sent": "The first one picks out a few dishes.",
                    "label": 0
                },
                {
                    "sent": "Other cuss",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Others come in and sample new dishes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on their popularity and then sometimes they also pick new dishes as well.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, what does this have to do with actual practical things?",
                    "label": 0
                },
                {
                    "sent": "Well, if we assume that these features, the dishes are now features, the customers are data points and we have a prior that tells us what features are present in which data points.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about this prior is that we don't have to limit the number of features a priority, but at the same time we're guaranteed that in a finite data set there will only be a finite number of features expressed.",
                    "label": 1
                },
                {
                    "sent": "The other nice property about the Indian buffet process is the observations are exchangeable, so the way that I drew this through this out, it looks like the first customer kind of affects what the second customer does, etc.",
                    "label": 1
                },
                {
                    "sent": "But we can actually reorder these these rows and the distribution is unchanged.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now that we've got the prior.",
                    "label": 0
                },
                {
                    "sent": "That's the prior on this said matrix right here.",
                    "label": 0
                },
                {
                    "sent": "Let's put it together with the remaining pieces.",
                    "label": 0
                },
                {
                    "sent": "So now we have the features A and the data X and I'm going to refer to N as the number of observations that we have, and D is the dimensions.",
                    "label": 0
                },
                {
                    "sent": "So we're dealing in situations where this is large, where we have a large end.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In those sorts of situations, maybe we don't want to deal with all of the data at the same time.",
                    "label": 0
                },
                {
                    "sent": "There's just too much of it.",
                    "label": 0
                },
                {
                    "sent": "So what our technique is going to do is we're going to say, well, what if we can only afford to look at part of the data at a time?",
                    "label": 0
                },
                {
                    "sent": "So we're going to refer to XW, where W is for for window as a small window of the data, an it has a corresponding zed W, and that is the corresponding feature assignments that go with, and I'm going to just note right now.",
                    "label": 0
                },
                {
                    "sent": "This is not going to be blocked Gibbs sampling 'cause we're still going to be doing normal Gibbs sampling.",
                    "label": 1
                },
                {
                    "sent": "We're only going to be looking at one piece of the data at a.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm great, so now we have that model in place.",
                    "label": 0
                },
                {
                    "sent": "What is it correspond to in terms of a graphical model?",
                    "label": 1
                },
                {
                    "sent": "Well, here's the data and it's being produced by its feature assignments and the values of the features, and similarly for all the data that we're not dealing with right now.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, um.",
                    "label": 0
                },
                {
                    "sent": "In this situation, if we're doing just normal Gibbs sampling, what do we do?",
                    "label": 0
                },
                {
                    "sent": "Well, we can start out by sampling all the elements in here, one at a time, again not as a block.",
                    "label": 0
                },
                {
                    "sent": "All the elements in here, all the feature assignments given everything else.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can turn around and sample everything in here.",
                    "label": 0
                },
                {
                    "sent": "Given everything else, notice that I swapped the arrow here.",
                    "label": 0
                },
                {
                    "sent": "That's again, that's a specific property of the Indian buffet process, because we can imagine the set that we're working with to be kind of the last set of customers that came into the buffet.",
                    "label": 0
                },
                {
                    "sent": "Now, if you have a model where you don't have this dependency, then obviously this error would not would.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be there.",
                    "label": 0
                },
                {
                    "sent": "We can re sample what the feature values actually are.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then repeat so this is a standard, uncollapsed Gibbs sampler.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The advantage is that each iteration is fast to compute, or at least we hope it is.",
                    "label": 0
                },
                {
                    "sent": "If that's not fast to compute, we've got other problems, but the disadvantage is that it's often slow to mix because we're explicitly representing each of these.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "So what's the other alternative?",
                    "label": 0
                },
                {
                    "sent": "Well, usually when we're doing Gibbs sampling, we're trying to decide whether we should collapse something or whether we should leave it uncollapsed.",
                    "label": 0
                },
                {
                    "sent": "So recall that I said that.",
                    "label": 0
                },
                {
                    "sent": "Suppose we can we can integrate out the A so we can compute this.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If X given said well, let's go ahead and do that.",
                    "label": 0
                },
                {
                    "sent": "Well, what that does is that's going to kind of mash everything together, and so even though we can do this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same sort of sampling as before.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have this.",
                    "label": 0
                },
                {
                    "sent": "We have this issue that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inference no longer scale, so it will be faster to mix at least in like the Indian buffet process with with say Gaussian likelihood model.",
                    "label": 0
                },
                {
                    "sent": "But since whenever I'm sampling stuff in here now I'm dependent on stuff over here, I've got problems in terms of scaling to very large datasets.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what our solution is in terms of our accelerated sampler, is to instead of either neither representing the features explicitly or to integrate them out completely.",
                    "label": 0
                },
                {
                    "sent": "We keep a posterior on the features, so this node you can think about it intuitively.",
                    "label": 1
                },
                {
                    "sent": "This note is still here, so the the feature assignments here are still independent of the data that's sitting up here.",
                    "label": 0
                },
                {
                    "sent": "However, because we're keeping a posterior now.",
                    "label": 0
                },
                {
                    "sent": "There is still that fuzziness that we get from the collapse sampler that allows things to move in a more.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Efficient way.",
                    "label": 0
                },
                {
                    "sent": "So now just a bit more formally, just to show you that this is exact, let's say now just one element here, so just one row we're considering that.",
                    "label": 1
                },
                {
                    "sent": "Let's say that I want to re sample one of these elements.",
                    "label": 0
                },
                {
                    "sent": "So the probability that some zed NK is equal to 1, well, that just using Bayes rule is going to be proportional to.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You, uh, the prior the prior term, which is the probability of that N = 1 given the rest of this ad matrix times the likelihood term over here.",
                    "label": 0
                },
                {
                    "sent": "Now I'm assuming that in whatever model you're working with, this one is easy to compute in the ICP model, this requires just summing up a set of accounts, so this is a very scalable, and then we have this trip over here where I.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And expand it in terms of the the X by representing a explicitly.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if I represent a explicitly, I can split up this likelihood term here into two pieces because given a.",
                    "label": 0
                },
                {
                    "sent": "And the Zed's these two pieces are independent of each other.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to just apply Bayes rule one more time to combine these two terms together and.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thus, what I end up with is an expression for sampling.",
                    "label": 0
                },
                {
                    "sent": "Sampling a single element of the assignment matrix which has the the X and the the rest of the zed matrix only inside the posterior.",
                    "label": 0
                },
                {
                    "sent": "And this is nice because we no longer have to deal with all of the data when sampling one element.",
                    "label": 0
                },
                {
                    "sent": "We only have to deal with this posterior over here.",
                    "label": 0
                },
                {
                    "sent": "And again, this is exact.",
                    "label": 0
                },
                {
                    "sent": "We haven't done anything that approximate in this in this approach.",
                    "label": 0
                },
                {
                    "sent": "So gay.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this, given this observation, here is the algorithm that goes with.",
                    "label": 0
                },
                {
                    "sent": "So we initialize some sort of feature assignments and some feature posterior, and now for each window of observations we start out, we have some posterior over our features.",
                    "label": 0
                },
                {
                    "sent": "We remove the effect of the window, so we get the posterior without the window there.",
                    "label": 0
                },
                {
                    "sent": "So just in this cartoon we can imagine it is kind of flattening out that posterior.",
                    "label": 0
                },
                {
                    "sent": "Now, given this right here, that corresponds to this term right here.",
                    "label": 0
                },
                {
                    "sent": "We can perform inference on are just small window of data and once where.",
                    "label": 1
                },
                {
                    "sent": "Once we're happy with that, we can reconstruct the full posterior using that using that new set of assignments.",
                    "label": 0
                },
                {
                    "sent": "And then we move on to a new window.",
                    "label": 0
                },
                {
                    "sent": "Now some of the considerations when doing this is how many observations should we consider at once?",
                    "label": 1
                },
                {
                    "sent": "How large should the window be an?",
                    "label": 1
                },
                {
                    "sent": "That really depends on the cost, the relative costs of computing the feature posterior.",
                    "label": 0
                },
                {
                    "sent": "Updating the feature posterior and computing the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So if this right here is expensive, then maybe we want the window to be larger 'cause we need to this bit right here is going to get expensive right here, but if computing the likelihood is expensive then maybe we want the window to be smaller.",
                    "label": 0
                },
                {
                    "sent": "So this bit does not take a lot of computation.",
                    "label": 0
                },
                {
                    "sent": "Another side issue is that depending on how you're modeling this, you can run into some numerical issues and that may also affect what sort of window sizes.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want to work with.",
                    "label": 0
                },
                {
                    "sent": "So so far I've kept it pretty general.",
                    "label": 0
                },
                {
                    "sent": "This can apply to any sort of bilinear model, or really even more general models that where you have this kind of property now, in particular for the Indian buffet process, let's suppose that the prior on the features and the noise is also Gaussian.",
                    "label": 1
                },
                {
                    "sent": "In this situation, the posterior on the features will also be Gaussian.",
                    "label": 1
                },
                {
                    "sent": "It can be updated, uh, efficiently with the.",
                    "label": 0
                },
                {
                    "sent": "Rank 1 updates.",
                    "label": 0
                },
                {
                    "sent": "And because this update is actually extremely efficient, the optimal window size turns out to be one.",
                    "label": 1
                },
                {
                    "sent": "Just as a side note, now we mention this a little bit more in our paper, but intelligently choosing how to represent the Gaussians can be pretty important for maintaining numerical accuracy.",
                    "label": 1
                },
                {
                    "sent": "That is, whether you use the inverse or information form or the full covariance form.",
                    "label": 0
                },
                {
                    "sent": "But again, details about that in the paper or also talk to me later.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so next we did a series of experiments on synthetic data, so this is just generated from the prior an we have a series of sizes of datasets, all pretty small, but just to illustrate the point now, it was hard to see, But basically the takeaway point here is that here we're measuring mixing ineffective samples per sample.",
                    "label": 1
                },
                {
                    "sent": "And the accelerated Gibbs sampler is right here.",
                    "label": 0
                },
                {
                    "sent": "The collapsed Gibbs sampler is right here and you can see that they have similar mixing properties.",
                    "label": 0
                },
                {
                    "sent": "Again, this is just a run from one experiment we can show mathematically that they should have identical mixing properties and now we look at the running time.",
                    "label": 0
                },
                {
                    "sent": "So now we have runtime here and the number of observations and this is on a log scale.",
                    "label": 0
                },
                {
                    "sent": "If you can't see the axes very well, so the slope is what's important and we see that the evolution in running time as we increase the number of samplers.",
                    "label": 0
                },
                {
                    "sent": "Samples is the same for the accelerated Gibbs sampler and for the uncollapsed one, whereas the collapse one has a higher slope and it turns out in our particular case that these are both linear in the number of observations and this one is cubic.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we ran this on different datasets which were not drawn from the prior.",
                    "label": 0
                },
                {
                    "sent": "So over here we have a really popular block images data set that's featured in a lot of Indian buffet process type papers where N = 1000 and it's just a small 36 dimensional set of pixels.",
                    "label": 0
                },
                {
                    "sent": "And here here's the accelerated sampler over here, Uncollapsed and.",
                    "label": 0
                },
                {
                    "sent": "Fast and you can see there kind of all going to kind of the same place in terms of finding a mode to explore that.",
                    "label": 0
                },
                {
                    "sent": "They're kind of happy to explore.",
                    "label": 0
                },
                {
                    "sent": "This is a log scale though on the plot, so we're reaching places where these samplers are going.",
                    "label": 0
                },
                {
                    "sent": "At least two orders of magnitude faster in terms of runtime.",
                    "label": 0
                },
                {
                    "sent": "Here's a slightly more complicated data set.",
                    "label": 0
                },
                {
                    "sent": "These are dimodica emoticons downloaded off the Web 722 of them.",
                    "label": 0
                },
                {
                    "sent": "And again, here is our sampler, and it's kind of getting to an equilibrium again.",
                    "label": 0
                },
                {
                    "sent": "The log scale kind of squashes together all of the points.",
                    "label": 0
                },
                {
                    "sent": "Towards the end, about 3 orders of magnitude faster then.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other samplers.",
                    "label": 0
                },
                {
                    "sent": "So moving on to even larger datasets, and this is just to show that we can scale to things that you could never scale the ICP too before it had been limited largely to toy problems.",
                    "label": 0
                },
                {
                    "sent": "This is a faces frontal face image data set with 2600 observations and close to 1600 dimensions.",
                    "label": 0
                },
                {
                    "sent": "This is a data set of piano music with 10,000 samples, and in these cases, so here you did see the sampler starting to.",
                    "label": 0
                },
                {
                    "sent": "Starting to pick up here, but the our accelerated sampler just does just fine.",
                    "label": 0
                },
                {
                    "sent": "On these datasets.",
                    "label": 0
                },
                {
                    "sent": "We ran the other samplers.",
                    "label": 0
                },
                {
                    "sent": "They weren't able to complete an iteration in about a day and a half, and at that point we kind of gave up.",
                    "label": 0
                },
                {
                    "sent": "We let them run for about a week and didn't really get anywhere.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we began so the question.",
                    "label": 0
                },
                {
                    "sent": "Often when we're doing Gibbs sampling, is you marginalized out a variable?",
                    "label": 0
                },
                {
                    "sent": "Do you keep it?",
                    "label": 0
                },
                {
                    "sent": "And I guess one of the main takeaways from this talk is that maybe there's a third.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maintaining a posterior within a sampler can allow us to perform fast interference in an important class of models, and this could be a very useful technique.",
                    "label": 1
                },
                {
                    "sent": "For things much more general than the Indian buffet process and then in particular we were able to show that by using this technique for the Indian buffet process with linear Gaussian models, we can scale inference to data set sizes that were previously not possible.",
                    "label": 0
                },
                {
                    "sent": "And if you're curious, if you want to play around with this code for all the different samplers is available at my website and I'd be happy to answer any questions on that.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}