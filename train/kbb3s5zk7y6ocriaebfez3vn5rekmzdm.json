{
    "id": "kbb3s5zk7y6ocriaebfez3vn5rekmzdm",
    "title": "Multimodal Learning with Deep Boltzmann Machines",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Machine Learning Department, Carnegie Mellon University"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/nips2012_salakhutdinov_multimodal_learning/",
    "segmentation": [
        [
            "So first of all, I want to acknowledge a student I'm Co.",
            "Advising at University of Toronto.",
            "Nitish threw it away.",
            "It's mainly his work and unfortunately up until last week he was practicing to give this dog.",
            "But last week he was selected for a random background check so he couldn't get his visa.",
            "So in any case so his keys real talent here so."
        ],
        [
            "When I talk about multimodal learning with deep Boltzmann machines and you know data data nowadays is really coming from a collection of modalities, right?",
            "We have images, text, audio.",
            "If you're looking at various.",
            "Data that's coming to us from social networking data.",
            "Product recommendation systems.",
            "Again, we we see data is coming at multiple sources of modalities."
        ],
        [
            "And robotics applications.",
            "It's we might have multiple sensors.",
            "And by the way, I should point out this was the most exciting slide in terms of visual presentation, so I hope you enjoyed it.",
            "Now."
        ],
        [
            "In terms of what we're trying to do to have a mental picture, we have what we call modality full representation.",
            "So for example, here we're looking at images of images and text and we really want to get this.",
            "We're going to get some notion of a concept or with commonality free representation, right?",
            "So how are we going to how we're going to do that?"
        ],
        [
            "Well, we're going to learn a generative model, a joint density model.",
            "So we're going to be learning a generative model of images in text H here would represent the notion of latent or hidden concept.",
            "And really, we can think of it as a fused representation.",
            "And we can use it for classification or retrieval or whatever task you want to use it for now."
        ],
        [
            "Because if it's a generative model, we can do fun things like generated data from one modality condition, another modality right.",
            "So for example, for things."
        ],
        [
            "Image annotation condition on an image.",
            "We can generate a distribution of a possible tags or we can go the other way around.",
            "Condition on the text we can generate the possible distribution of images and those things could be useful for things like image annotation or image retrieval.",
            "Right, but."
        ],
        [
            "There are several challenges that we face when we started looking at this problem.",
            "The very first challenges that often different data modalities have very different statistical properties, right?",
            "So the the they have very different input representation.",
            "So for example, if you look at images, images are typically represented in terms of real value dense representation.",
            "If you're looking at text, typically if we're using bag of words representation then.",
            "It's discrete if sparse, so we can see that these are very, very different data modalities and really it's very difficult to learn meaningful cross model features from from these low level representations, right?",
            "So how can we go about that?"
        ],
        [
            "The second challenge is that you know we're dealing with the data that's often noisy or missing, right?",
            "So here's some examples from the Flickr data set.",
            "We have images, but sometimes some of the text that we observe is is not very meaningful, right?",
            "Sometimes it's not meaningful.",
            "Sometimes it's missing.",
            "Sometimes if you look at this last figure here last pictures, it's somewhat meaningful, right?",
            "It tells tells us something that there is a picture of a nature, but this is what we actually do see in the date."
        ],
        [
            "In terms of text generated by the model, this is the kind of tax that our model can generate, just a little bit of a little bit of a preview, so it does capture certain notion of what you see in those images.",
            "Right now, before I get into specific details about the model."
        ],
        [
            "And what the model is, let me just show you a demo.",
            "So here you can think of this model is a generative model condition.",
            "Only image we're going to generate the distribution of a possible words and just let the model decide what what distributions it can generate.",
            "Right now.",
            "What I'm doing here because it's a generative model I'm running what's called the Markov chain Monte Carlo Gibbs Sampler.",
            "Here, an here.",
            "You just seeing what it does after every every 50 gives updates, so notice that.",
            "After about 100 steps, it's already generating something meaningful, right?",
            "So you're looking at C Beach Island, and you know sometimes it generates Canadian things.",
            "Canada BC Lake, Italy, Waterboat and such.",
            "So it does go through these different modes and thus tend to generate reasonable things, right?",
            "Notice how fast it it sort of converges to what I would I think it converges right?",
            "In terms of generating meaningful things.",
            "Now let me get into specific definition of what the model is so."
        ],
        [
            "But I describe what that is.",
            "The very first building block of our model is a specific type of model called restricted Boltzmann machines.",
            "Many of you know bout restricted both machines, just to just to formalize things.",
            "It's a Markov random field, you have stochastic binary visible units that stochastic binary hidden units, and you have bipartite connections.",
            "You can write the joint probability distribution this way, and it's it's a very standard way of defining undirected graphical models.",
            "You have pairwise potentials.",
            "You have unary potentials.",
            "The most important thing here is that you can think of stochastic hidden variables is detecting certain patterns that you see in the data and the conditional distribution.",
            "Here is again taking a very simple form.",
            "It's the product of the conditional swage conditional takes logistic funk."
        ],
        [
            "Now when dealing with real value data like images, we can make slight modification to the model and work with a model called Gaussian Bernoulli market random field right?",
            "Again, the structure of the model remains pretty much the same.",
            "You have pairwise potentials, unary potentials, and the conditional distributions of the observed data is given by the product of conditions, where each conditional takes a Gaussian distribution and these kinds of models have been used for modeling images.",
            "Image patches and such.",
            "Now at the final stage, if we want to model discrete sparse data account data like in the back of."
        ],
        [
            "Was representation we again make a slight modification to the model and this is what's called replicated softmax model.",
            "It's not directly topic model.",
            "Again, it has a similar structure.",
            "The most important thing is that the conditional distribution he is given by the softmax distribution.",
            "So if you think of having bag of words representation you have vocabulary of size K and you have the words, then we can use that model again.",
            "The point I want to make here is that we have this building block and we can use it for modeling different kinds of.",
            "Data now one useful thing about this."
        ],
        [
            "The most machines is that it's very easy to infer the states of the latent variables, so the states of the hidden variables right and takes a closed form has a closed form solution, so it's very important for things like information retrieval classification, where perception is easy, conditional input that can instantly tell you what latent prints."
        ],
        [
            "Agents are, and if you look at all of these different models, they essentially all have binary hidden variables and they're using them to model different kinds of data."
        ],
        [
            "Now you can think of a simple model, simple multi model model where you just using a joint binary hidden layer.",
            "Now there are several problems with that particular model.",
            "In particular, as I mentioned before, inputs have very different statistical properties, and it's very difficult to learn meaningful cross model features right.",
            "And frankly, we tried this model and it wasn't working very well for us.",
            "And sometimes people call these models you doing harmoniums."
        ],
        [
            "So we need to go beyond that.",
            "So what we're doing here is we're using a class of models called deep Boltzmann machines.",
            "The bullshit you can think of them as just Markov random fields.",
            "You have latent variables, but you introducing dependencies within hidden variables, right?",
            "So what changed from the previous model is that you're having these two additional terms, and these terms are modeling dependencies within H1 and H2H2 in HD, so explicitly putting dependencies within latent variable."
        ],
        [
            "And all connections are undirected and in this case it's a much more expressive model than a simple RBM, but also it's a little bit more complicated because hidden variables become dependent even when you conditionally input, so it will play a little bit of when we Start learning these models, it becomes a little bit more challenging to fit these models, But again."
        ],
        [
            "They are much more expressive, so now what we can do in terms of building our model is we can use Gaussian RBM essentially to Model D dance real valued image features.",
            "We can use replicated softmax model toward word count data and we can build ourselves a hierarchical model here.",
            "OK, one nice thing about these D Boltzmann machines.",
            "In this case multimodal development."
        ],
        [
            "Is that there is very natural notion of bottom up and top down right?",
            "So information can flow from the low level features of words all the way down to the low level feature representation of images, right?",
            "So it's an it's unlike some of the other models like deep belief networks and neural Nets type of type of models.",
            "Now in terms of."
        ],
        [
            "Learning let me just give you a preview of how learning is being done with these models.",
            "We can do approximate maximum likelihood learning if you look at the gritty of the log probability with respect to parameters in this model, it comes down to being difference between two expected terms, so it's expected sufficient statistics driven by the data minus expected sufficient statistics driven by the model."
        ],
        [
            "In this particular case, unfortunately, even the expected statistics sufficient statistics driven by the data, you cannot compute it in closed form because you have dependencies between hidden variables.",
            "But there are ways of fitting these models.",
            "Actively, so we're using mean field to approximate the first set of expectations, and we're using Markov chain Monte Carlo or what's called Stochastic approximation algorithm to approximate the second set of expectations.",
            "Now, I'm not going to get into much into theory behind it, but you can show almost sure convergence guarantees under certain conditions of of this learning rule to answer practically asymptotically stable point of variational lower bound, so there's some nice theory behind these algorithms, and you can prove certain properties about them.",
            "But now the question is how well does it?",
            "The."
        ],
        [
            "Is it do right?",
            "So here is again some a few images.",
            "This is conditioned on the images we generating tax from those images and in most of the cases it does capture meaningful structure right?",
            "So it does capture what what you see and there is always a bias towards Canada right?",
            "'cause this model was created in Canada.",
            "But in any case there are also some failures right?",
            "So the."
        ],
        [
            "Model is not perfect and here's some daily examples, so if you're looking at this line, it thinks it's just basically women soldiers right now impression of that is because this is a Flickr data set.",
            "Most of the time people post pictures of themselves and photos as opposed to pictures of animals, right?",
            "I also like the second one.",
            "You see it confuses it with Barack Obama, election and politics.",
            "And my student convinced me that it's because you know, Obama signs typically carry blue and white signs, so it's making a mistake here.",
            "But I have a different theory.",
            "I think if you just keep looking at this picture for long enough, you'll just start seeing hope and change and and obviously the model does the right.",
            "The right choice here.",
            "You can also do."
        ],
        [
            "The other way around, so this is the case where we can because it's a generative model.",
            "Again, we can sample from one modality condition, another modality, and again the model he is initialized at.",
            "The completely random state.",
            "So here we're going to be generating cars and automobiles and such, and you know one interesting thing about these kinds of models.",
            "Notice how fast the Markov chain mixes between different modes, right?",
            "And jumps between pictures that are very different from each other.",
            "And sometimes it gets stuck for a few iterations, but then it does move around quite quickly, so there is a conceptual wisdom that Markov chain Monte Carlo methods are very slow.",
            "In this case, it doesn't seem to be the case, right?",
            "So it does.",
            "It does do a reasonable job.",
            "Right, and that's"
        ],
        [
            "That's what it does now in terms of some other examples.",
            "Given some text, these are images that the model is generating or retrieving in some cases.",
            "Again, there are failure modes here.",
            "If you look at the very last row and the query here was chocolate cake, so it retrieves images even though again, I would argue these kind of look like chocolate cakes, right?",
            "Always you can make stuff up.",
            "But"
        ],
        [
            "Now, one other thing about this model is that.",
            "Another thing about this model is that we can leverage large volumes of unlabeled data to train these models, and that will be crucial in our application so we can pre train these different pathways separately."
        ],
        [
            "Now, in terms of just let me give you a couple of examples and give you some quantitative evaluation of this model.",
            "This was done using Mirp Flickr data set.",
            "You have about 1,000,000 images with user sign tags, so we have images and tags.",
            "Sometimes they're meaningful, sometimes they."
        ],
        [
            "Not in terms of the kind of architecture we're using.",
            "We have multimodal Diebold machine with about 12 million parameters and about 6 million latent variables.",
            "So hidden variables in terms of features were using just sift impact, about 4000 Dimensional image features were using 2200 most frequently used tags, and there is for this particular data set there is a subset of 25,000 labeled examples and we have 38 classes.",
            "Classes such as Sky, Tree, Baby and so forth.",
            "We are using 50K for training and 10K for testing OK."
        ],
        [
            "The way we evaluating these models is that conditionally input, we infer the distribution of a latent variables at the very top level, and we just using a simple logistic regression there, right?",
            "So no fancy backpropagation here now.",
            "If we're looking at the performance we want to compare the results to some previously published methods, and here we're using mean average precision, which is a standard metric for evaluating these models.",
            "Now, if we use the same set of features and we only use 25,000 labeled examples, just what the other people did, then we're doing slightly better compared to support vector machines and linear discriminant analysis, right?",
            "The interesting thing is that if we actually."
        ],
        [
            "Take 1 million unlabeled examples.",
            "Then there's a huge jump here from .5 two .58.",
            "So it does tell us that training these models and using unlabeled data is helpful."
        ],
        [
            "And if we add SIFT features which wasn't part of the original set of features that previous people were using, then we can improve slightly and sort of improvement compared to deep belief Nets and not encoders.",
            "We're doing slightly better than those models.",
            "Now."
        ],
        [
            "Let me just talk about the final experiment that we've done is is we tried to figure out whether these extra modalities helping us so at the training phase we had images in associated text, but the test time we were only given images, right?",
            "So the text modality was completely absent and you know what we're seeing is again having a separate modality helps us.",
            "It helps us maybe in terms of regularising our model, but you can see there is a big jump from just ignoring the text modality.",
            "Then then just using it even though you know it, the test time that modality is not going to be useful, OK?"
        ],
        [
            "Thank you one thing I should point out is that there are more results in code available at nutritious website.",
            "So if you just run the software you can reproduce these results that you're seeing in the paper and at that talk so if you're really interested in working in real data.",
            "You should you should look there.",
            "Of course, if you're interested in working on any data set you should talk to me.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, I want to acknowledge a student I'm Co.",
                    "label": 0
                },
                {
                    "sent": "Advising at University of Toronto.",
                    "label": 0
                },
                {
                    "sent": "Nitish threw it away.",
                    "label": 0
                },
                {
                    "sent": "It's mainly his work and unfortunately up until last week he was practicing to give this dog.",
                    "label": 0
                },
                {
                    "sent": "But last week he was selected for a random background check so he couldn't get his visa.",
                    "label": 0
                },
                {
                    "sent": "So in any case so his keys real talent here so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When I talk about multimodal learning with deep Boltzmann machines and you know data data nowadays is really coming from a collection of modalities, right?",
                    "label": 0
                },
                {
                    "sent": "We have images, text, audio.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at various.",
                    "label": 0
                },
                {
                    "sent": "Data that's coming to us from social networking data.",
                    "label": 0
                },
                {
                    "sent": "Product recommendation systems.",
                    "label": 0
                },
                {
                    "sent": "Again, we we see data is coming at multiple sources of modalities.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And robotics applications.",
                    "label": 0
                },
                {
                    "sent": "It's we might have multiple sensors.",
                    "label": 0
                },
                {
                    "sent": "And by the way, I should point out this was the most exciting slide in terms of visual presentation, so I hope you enjoyed it.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of what we're trying to do to have a mental picture, we have what we call modality full representation.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we're looking at images of images and text and we really want to get this.",
                    "label": 0
                },
                {
                    "sent": "We're going to get some notion of a concept or with commonality free representation, right?",
                    "label": 0
                },
                {
                    "sent": "So how are we going to how we're going to do that?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we're going to learn a generative model, a joint density model.",
                    "label": 1
                },
                {
                    "sent": "So we're going to be learning a generative model of images in text H here would represent the notion of latent or hidden concept.",
                    "label": 0
                },
                {
                    "sent": "And really, we can think of it as a fused representation.",
                    "label": 0
                },
                {
                    "sent": "And we can use it for classification or retrieval or whatever task you want to use it for now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because if it's a generative model, we can do fun things like generated data from one modality condition, another modality right.",
                    "label": 0
                },
                {
                    "sent": "So for example, for things.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image annotation condition on an image.",
                    "label": 0
                },
                {
                    "sent": "We can generate a distribution of a possible tags or we can go the other way around.",
                    "label": 0
                },
                {
                    "sent": "Condition on the text we can generate the possible distribution of images and those things could be useful for things like image annotation or image retrieval.",
                    "label": 0
                },
                {
                    "sent": "Right, but.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are several challenges that we face when we started looking at this problem.",
                    "label": 0
                },
                {
                    "sent": "The very first challenges that often different data modalities have very different statistical properties, right?",
                    "label": 0
                },
                {
                    "sent": "So the the they have very different input representation.",
                    "label": 1
                },
                {
                    "sent": "So for example, if you look at images, images are typically represented in terms of real value dense representation.",
                    "label": 0
                },
                {
                    "sent": "If you're looking at text, typically if we're using bag of words representation then.",
                    "label": 0
                },
                {
                    "sent": "It's discrete if sparse, so we can see that these are very, very different data modalities and really it's very difficult to learn meaningful cross model features from from these low level representations, right?",
                    "label": 1
                },
                {
                    "sent": "So how can we go about that?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second challenge is that you know we're dealing with the data that's often noisy or missing, right?",
                    "label": 0
                },
                {
                    "sent": "So here's some examples from the Flickr data set.",
                    "label": 0
                },
                {
                    "sent": "We have images, but sometimes some of the text that we observe is is not very meaningful, right?",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's not meaningful.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's missing.",
                    "label": 0
                },
                {
                    "sent": "Sometimes if you look at this last figure here last pictures, it's somewhat meaningful, right?",
                    "label": 0
                },
                {
                    "sent": "It tells tells us something that there is a picture of a nature, but this is what we actually do see in the date.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of text generated by the model, this is the kind of tax that our model can generate, just a little bit of a little bit of a preview, so it does capture certain notion of what you see in those images.",
                    "label": 0
                },
                {
                    "sent": "Right now, before I get into specific details about the model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what the model is, let me just show you a demo.",
                    "label": 0
                },
                {
                    "sent": "So here you can think of this model is a generative model condition.",
                    "label": 0
                },
                {
                    "sent": "Only image we're going to generate the distribution of a possible words and just let the model decide what what distributions it can generate.",
                    "label": 0
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "What I'm doing here because it's a generative model I'm running what's called the Markov chain Monte Carlo Gibbs Sampler.",
                    "label": 0
                },
                {
                    "sent": "Here, an here.",
                    "label": 0
                },
                {
                    "sent": "You just seeing what it does after every every 50 gives updates, so notice that.",
                    "label": 1
                },
                {
                    "sent": "After about 100 steps, it's already generating something meaningful, right?",
                    "label": 0
                },
                {
                    "sent": "So you're looking at C Beach Island, and you know sometimes it generates Canadian things.",
                    "label": 0
                },
                {
                    "sent": "Canada BC Lake, Italy, Waterboat and such.",
                    "label": 0
                },
                {
                    "sent": "So it does go through these different modes and thus tend to generate reasonable things, right?",
                    "label": 0
                },
                {
                    "sent": "Notice how fast it it sort of converges to what I would I think it converges right?",
                    "label": 0
                },
                {
                    "sent": "In terms of generating meaningful things.",
                    "label": 0
                },
                {
                    "sent": "Now let me get into specific definition of what the model is so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I describe what that is.",
                    "label": 0
                },
                {
                    "sent": "The very first building block of our model is a specific type of model called restricted Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "Many of you know bout restricted both machines, just to just to formalize things.",
                    "label": 0
                },
                {
                    "sent": "It's a Markov random field, you have stochastic binary visible units that stochastic binary hidden units, and you have bipartite connections.",
                    "label": 1
                },
                {
                    "sent": "You can write the joint probability distribution this way, and it's it's a very standard way of defining undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "You have pairwise potentials.",
                    "label": 0
                },
                {
                    "sent": "You have unary potentials.",
                    "label": 0
                },
                {
                    "sent": "The most important thing here is that you can think of stochastic hidden variables is detecting certain patterns that you see in the data and the conditional distribution.",
                    "label": 0
                },
                {
                    "sent": "Here is again taking a very simple form.",
                    "label": 0
                },
                {
                    "sent": "It's the product of the conditional swage conditional takes logistic funk.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now when dealing with real value data like images, we can make slight modification to the model and work with a model called Gaussian Bernoulli market random field right?",
                    "label": 0
                },
                {
                    "sent": "Again, the structure of the model remains pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "You have pairwise potentials, unary potentials, and the conditional distributions of the observed data is given by the product of conditions, where each conditional takes a Gaussian distribution and these kinds of models have been used for modeling images.",
                    "label": 0
                },
                {
                    "sent": "Image patches and such.",
                    "label": 0
                },
                {
                    "sent": "Now at the final stage, if we want to model discrete sparse data account data like in the back of.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was representation we again make a slight modification to the model and this is what's called replicated softmax model.",
                    "label": 0
                },
                {
                    "sent": "It's not directly topic model.",
                    "label": 0
                },
                {
                    "sent": "Again, it has a similar structure.",
                    "label": 0
                },
                {
                    "sent": "The most important thing is that the conditional distribution he is given by the softmax distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you think of having bag of words representation you have vocabulary of size K and you have the words, then we can use that model again.",
                    "label": 0
                },
                {
                    "sent": "The point I want to make here is that we have this building block and we can use it for modeling different kinds of.",
                    "label": 0
                },
                {
                    "sent": "Data now one useful thing about this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The most machines is that it's very easy to infer the states of the latent variables, so the states of the hidden variables right and takes a closed form has a closed form solution, so it's very important for things like information retrieval classification, where perception is easy, conditional input that can instantly tell you what latent prints.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Agents are, and if you look at all of these different models, they essentially all have binary hidden variables and they're using them to model different kinds of data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you can think of a simple model, simple multi model model where you just using a joint binary hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Now there are several problems with that particular model.",
                    "label": 0
                },
                {
                    "sent": "In particular, as I mentioned before, inputs have very different statistical properties, and it's very difficult to learn meaningful cross model features right.",
                    "label": 0
                },
                {
                    "sent": "And frankly, we tried this model and it wasn't working very well for us.",
                    "label": 0
                },
                {
                    "sent": "And sometimes people call these models you doing harmoniums.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need to go beyond that.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing here is we're using a class of models called deep Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "The bullshit you can think of them as just Markov random fields.",
                    "label": 0
                },
                {
                    "sent": "You have latent variables, but you introducing dependencies within hidden variables, right?",
                    "label": 0
                },
                {
                    "sent": "So what changed from the previous model is that you're having these two additional terms, and these terms are modeling dependencies within H1 and H2H2 in HD, so explicitly putting dependencies within latent variable.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all connections are undirected and in this case it's a much more expressive model than a simple RBM, but also it's a little bit more complicated because hidden variables become dependent even when you conditionally input, so it will play a little bit of when we Start learning these models, it becomes a little bit more challenging to fit these models, But again.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They are much more expressive, so now what we can do in terms of building our model is we can use Gaussian RBM essentially to Model D dance real valued image features.",
                    "label": 1
                },
                {
                    "sent": "We can use replicated softmax model toward word count data and we can build ourselves a hierarchical model here.",
                    "label": 0
                },
                {
                    "sent": "OK, one nice thing about these D Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "In this case multimodal development.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that there is very natural notion of bottom up and top down right?",
                    "label": 0
                },
                {
                    "sent": "So information can flow from the low level features of words all the way down to the low level feature representation of images, right?",
                    "label": 0
                },
                {
                    "sent": "So it's an it's unlike some of the other models like deep belief networks and neural Nets type of type of models.",
                    "label": 0
                },
                {
                    "sent": "Now in terms of.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning let me just give you a preview of how learning is being done with these models.",
                    "label": 0
                },
                {
                    "sent": "We can do approximate maximum likelihood learning if you look at the gritty of the log probability with respect to parameters in this model, it comes down to being difference between two expected terms, so it's expected sufficient statistics driven by the data minus expected sufficient statistics driven by the model.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this particular case, unfortunately, even the expected statistics sufficient statistics driven by the data, you cannot compute it in closed form because you have dependencies between hidden variables.",
                    "label": 0
                },
                {
                    "sent": "But there are ways of fitting these models.",
                    "label": 0
                },
                {
                    "sent": "Actively, so we're using mean field to approximate the first set of expectations, and we're using Markov chain Monte Carlo or what's called Stochastic approximation algorithm to approximate the second set of expectations.",
                    "label": 0
                },
                {
                    "sent": "Now, I'm not going to get into much into theory behind it, but you can show almost sure convergence guarantees under certain conditions of of this learning rule to answer practically asymptotically stable point of variational lower bound, so there's some nice theory behind these algorithms, and you can prove certain properties about them.",
                    "label": 0
                },
                {
                    "sent": "But now the question is how well does it?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it do right?",
                    "label": 0
                },
                {
                    "sent": "So here is again some a few images.",
                    "label": 0
                },
                {
                    "sent": "This is conditioned on the images we generating tax from those images and in most of the cases it does capture meaningful structure right?",
                    "label": 0
                },
                {
                    "sent": "So it does capture what what you see and there is always a bias towards Canada right?",
                    "label": 0
                },
                {
                    "sent": "'cause this model was created in Canada.",
                    "label": 0
                },
                {
                    "sent": "But in any case there are also some failures right?",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model is not perfect and here's some daily examples, so if you're looking at this line, it thinks it's just basically women soldiers right now impression of that is because this is a Flickr data set.",
                    "label": 0
                },
                {
                    "sent": "Most of the time people post pictures of themselves and photos as opposed to pictures of animals, right?",
                    "label": 0
                },
                {
                    "sent": "I also like the second one.",
                    "label": 0
                },
                {
                    "sent": "You see it confuses it with Barack Obama, election and politics.",
                    "label": 0
                },
                {
                    "sent": "And my student convinced me that it's because you know, Obama signs typically carry blue and white signs, so it's making a mistake here.",
                    "label": 0
                },
                {
                    "sent": "But I have a different theory.",
                    "label": 0
                },
                {
                    "sent": "I think if you just keep looking at this picture for long enough, you'll just start seeing hope and change and and obviously the model does the right.",
                    "label": 0
                },
                {
                    "sent": "The right choice here.",
                    "label": 0
                },
                {
                    "sent": "You can also do.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other way around, so this is the case where we can because it's a generative model.",
                    "label": 0
                },
                {
                    "sent": "Again, we can sample from one modality condition, another modality, and again the model he is initialized at.",
                    "label": 0
                },
                {
                    "sent": "The completely random state.",
                    "label": 0
                },
                {
                    "sent": "So here we're going to be generating cars and automobiles and such, and you know one interesting thing about these kinds of models.",
                    "label": 0
                },
                {
                    "sent": "Notice how fast the Markov chain mixes between different modes, right?",
                    "label": 0
                },
                {
                    "sent": "And jumps between pictures that are very different from each other.",
                    "label": 0
                },
                {
                    "sent": "And sometimes it gets stuck for a few iterations, but then it does move around quite quickly, so there is a conceptual wisdom that Markov chain Monte Carlo methods are very slow.",
                    "label": 0
                },
                {
                    "sent": "In this case, it doesn't seem to be the case, right?",
                    "label": 0
                },
                {
                    "sent": "So it does.",
                    "label": 0
                },
                {
                    "sent": "It does do a reasonable job.",
                    "label": 0
                },
                {
                    "sent": "Right, and that's",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what it does now in terms of some other examples.",
                    "label": 0
                },
                {
                    "sent": "Given some text, these are images that the model is generating or retrieving in some cases.",
                    "label": 0
                },
                {
                    "sent": "Again, there are failure modes here.",
                    "label": 0
                },
                {
                    "sent": "If you look at the very last row and the query here was chocolate cake, so it retrieves images even though again, I would argue these kind of look like chocolate cakes, right?",
                    "label": 0
                },
                {
                    "sent": "Always you can make stuff up.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, one other thing about this model is that.",
                    "label": 0
                },
                {
                    "sent": "Another thing about this model is that we can leverage large volumes of unlabeled data to train these models, and that will be crucial in our application so we can pre train these different pathways separately.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in terms of just let me give you a couple of examples and give you some quantitative evaluation of this model.",
                    "label": 0
                },
                {
                    "sent": "This was done using Mirp Flickr data set.",
                    "label": 0
                },
                {
                    "sent": "You have about 1,000,000 images with user sign tags, so we have images and tags.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they're meaningful, sometimes they.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not in terms of the kind of architecture we're using.",
                    "label": 0
                },
                {
                    "sent": "We have multimodal Diebold machine with about 12 million parameters and about 6 million latent variables.",
                    "label": 0
                },
                {
                    "sent": "So hidden variables in terms of features were using just sift impact, about 4000 Dimensional image features were using 2200 most frequently used tags, and there is for this particular data set there is a subset of 25,000 labeled examples and we have 38 classes.",
                    "label": 0
                },
                {
                    "sent": "Classes such as Sky, Tree, Baby and so forth.",
                    "label": 1
                },
                {
                    "sent": "We are using 50K for training and 10K for testing OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The way we evaluating these models is that conditionally input, we infer the distribution of a latent variables at the very top level, and we just using a simple logistic regression there, right?",
                    "label": 0
                },
                {
                    "sent": "So no fancy backpropagation here now.",
                    "label": 0
                },
                {
                    "sent": "If we're looking at the performance we want to compare the results to some previously published methods, and here we're using mean average precision, which is a standard metric for evaluating these models.",
                    "label": 1
                },
                {
                    "sent": "Now, if we use the same set of features and we only use 25,000 labeled examples, just what the other people did, then we're doing slightly better compared to support vector machines and linear discriminant analysis, right?",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that if we actually.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take 1 million unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "Then there's a huge jump here from .5 two .58.",
                    "label": 0
                },
                {
                    "sent": "So it does tell us that training these models and using unlabeled data is helpful.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we add SIFT features which wasn't part of the original set of features that previous people were using, then we can improve slightly and sort of improvement compared to deep belief Nets and not encoders.",
                    "label": 0
                },
                {
                    "sent": "We're doing slightly better than those models.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just talk about the final experiment that we've done is is we tried to figure out whether these extra modalities helping us so at the training phase we had images in associated text, but the test time we were only given images, right?",
                    "label": 0
                },
                {
                    "sent": "So the text modality was completely absent and you know what we're seeing is again having a separate modality helps us.",
                    "label": 0
                },
                {
                    "sent": "It helps us maybe in terms of regularising our model, but you can see there is a big jump from just ignoring the text modality.",
                    "label": 0
                },
                {
                    "sent": "Then then just using it even though you know it, the test time that modality is not going to be useful, OK?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you one thing I should point out is that there are more results in code available at nutritious website.",
                    "label": 1
                },
                {
                    "sent": "So if you just run the software you can reproduce these results that you're seeing in the paper and at that talk so if you're really interested in working in real data.",
                    "label": 0
                },
                {
                    "sent": "You should you should look there.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you're interested in working on any data set you should talk to me.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}