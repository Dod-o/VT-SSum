{
    "id": "qrpsrbit4yeym3afqudihkvojtzjm2b3",
    "title": "The use of machine translation tools for cross-lingual text-mining",
    "info": {
        "author": [
            "Bla\u017e Fortuna, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "April 12, 2007",
        "recorded": "August 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Machine Learning->Human Language Technology"
        ]
    },
    "url": "http://videolectures.net/icml05_fortuna_tuo/",
    "segmentation": [
        [
            "Sean Taylor and it's not the use of machine translation tools for crossing through text mining.",
            "OK, so.",
            "I'm back for tonight from your Steven Institute, Pennsylvania.",
            "Now what they presented the joint work with Joshua Taylor.",
            "It's on how machine translation can help at cross lingual text mining.",
            "So here is the outline of the talk.",
            "First people short introduction."
        ],
        [
            "What are the?",
            "But issues appear at cross lingual text mining.",
            "What's the difference between normal text mining?",
            "Then I give a short introduction to kernel CCA.",
            "And.",
            "I relate machine translation to kernel kisses, KCI and present the experiments we made about involving machine translation.",
            "And I finish with some conclusions about the work.",
            "So first crossing."
        ],
        [
            "Text mining, so sometimes we have text corpora in more languages and we want to run some standard text mining algorithms on it.",
            "But because of different languages, some new issues appear, for example, that information retrieval.",
            "We have like we want to just issue one query in one language, but we want to get to do the search over the whole corpus of documents no matter what the language of the document is.",
            "So that means that what we want.",
            "What's important is the meaning of a query, not exact words of the query.",
            "Another example is classification.",
            "When we would like just to learn one classifier that can classify all the documents and not separate classifier for each language.",
            "And for example in clustering.",
            "Probably if you just run it without doing anything, just group the documents by language.",
            "So, but we want to group them by meaning not by language.",
            "OK, so one of the kernel kissick canonic."
        ],
        [
            "Correlation analysis is a one way of solving this issues, so it's gets on the input we get.",
            "Aligned corpus data set.",
            "So these are for example here for English and German documents, and we must have a so pairs of documents.",
            "So this is like English document and it's German translation.",
            "And from this we would like to learn this so called semantic dimensions.",
            "So the words.",
            "Which word in English share common meaning with them?",
            "Which words in German?",
            "Because this and in this way we would by using this as new representation these dimensions as new representation of our documents.",
            "So no more just not anymore bag of words model.",
            "But this more semantic dimensions would get so called.",
            "We would get so called language independent space.",
            "And then we get the new document.",
            "For example, in English language we can just project it into this.",
            "Semantic dimensions and we get a representation of the document which is invariant to the language.",
            "So, but there's one."
        ],
        [
            "Issue with this approach, we need a third training set and that can be very expensive to get.",
            "So if you know so this part here is prepared training set.",
            "So when we are Wendy's, usually this we don't have this information available and we have.",
            "Generally have two options.",
            "One is that we can just use datasets, train party datasets with some other domain.",
            "But discuss one big issues that.",
            "Maybe their domain is talking about different things, uses different words, so and then the gap between words maybe 2.",
            "Alright, so the dimensions, semantic dimension is learned from their training set, would wouldn't be of much use for our task and the other.",
            "Possible ways to use machine translations.",
            "For example Google language tools.",
            "And we would just take a sample of the documents from other domain and it would translate them to.",
            "For example, would take a set of figures, documents that state into French, take a set of transactions, translate into English, and this would be a very fast and cheap way to get paired train set.",
            "So but"
        ],
        [
            "Now we would like to check if this is true.",
            "If you can really help with this.",
            "So we.",
            "We would like to investigate the quality of this training set for Casey.",
            "We generate on this way, so on 1st we will check when we take a domain for which we have already available Bert Dispatch training set.",
            "And we will.",
            "We will compare it to an artificially generated one without using Google translation.",
            "And we see how much do we.",
            "How much do we lose comparing to the human translators?",
            "Or if you lose anything at all?",
            "And then we'll move to.",
            "This is this, and then we move to another domain, for which there will be another task which we won't have a.",
            "Any training set available and will.",
            "See how much can we if this machine translation can help?",
            "So for the first part."
        ],
        [
            "We use information retrieval task.",
            "We compared this to corpus is corpora forest hanserd corpus it's a.",
            "Generated from Parliament proceedings, Canadian Parliament proceedings where they have everything written down in English and French language and it's aligned by paragraphs and we use this as training input for CCA.",
            "And we generated artificial corpus 2, so we took.",
            "We split this answer corpus in half.",
            "We took the.",
            "From the first theorist, all the French translations an from the.",
            "The second half we erased all the English translations and we use Google to.",
            "To obtain these translations again, so basically it was totally machine translation, nothing, no human translating work was done except this.",
            "And then.",
            "So these are the corpora be used for different corpora we used for training CCA.",
            "And then for experience experiments, we.",
            "Extracted from each document from test set.",
            "We extracted 5 keywords according to the TF.",
            "IDF weights word weights.",
            "And use these words as a query for the.",
            "And the goal was to find the.",
            "To retrieve the document from which we extracted these keywords or experta documents each translation.",
            "So this is a.",
            "Procedure how it went.",
            "So first we learn PCA 1500 pair documents.",
            "So this pair documents are from these two corpora.",
            "Then all the case documents.",
            "So this test documents come together with answer to corpora.",
            "We projected them into this semantic space on these semantic dimensions.",
            "And then when we got on a query we projected we represented this query as a document.",
            "We imagine it as a document.",
            "We mapped it again into this semantic space and nearest neighbor there.",
            "Through their closest documents.",
            "So these are the."
        ],
        [
            "Once we got.",
            "So here we have two.",
            "Both corpora concert and artificial corpora.",
            "Columns mean this means this.",
            "For example, this means that the variables in English and researching over English documents.",
            "This means that the query was English in researching French documents and so on.",
            "And these these two numbers are so there.",
            "For example, in this case, 65 means that.",
            "Out of all the queries in 65%.",
            "Times in the French query was issued the right English documents appeared in the 1st place along retrieve documents and this means that 95% of the times when French query was issued the.",
            "English documents appear among the first 10.",
            "Results.",
            "We can notice, for example, first that when both the query in the search space, the space of documents in which we were searching there in the same language.",
            "Let's see.",
            "Basically, let's get the same results almost so.",
            "That shows us that the semantic dimensions found from the machine translation from this artificial corpora are as good as the ones found from corpora mean by human translators.",
            "However, the correlations between this semantic dimensions was not so good.",
            "As you can see here.",
            "So when the query was in English for query and was in different languages and the documents you're searching over, then the.",
            "Performance drop for allowance.",
            "5 to 10% the same for this case, and so you can see here these differences.",
            "The second."
        ],
        [
            "Experiment with classification.",
            "So here we picked a domain where we there was no parent corpora available.",
            "For training.",
            "And so I mean, again, we use two different corpora.",
            "Train sets for training CCA.",
            "The first one was the same as in previous example.",
            "We took that from Canadian Parliament proceedings, so this is a.",
            "This is done by human translators.",
            "And for the second corpora, we picked some documents from.",
            "From English news part of the Reuters news, and we picked some news from the French part of writers and we translated them to get their para documents.",
            "And use that as a thing for the input for learning CCA.",
            "So again, the procedure here must be trained.",
            "CCA first turned it on this on hand certainly train it on the artificial corpora.",
            "Then the whole writers were projected into the CCA semantic space into this language independent representation.",
            "An in.",
            "Inside that space we trained linear SVM classifier.",
            "On a subset of 303 thousand documents infested over 50,000 documents, and so all the presented results are averaged over 5 random splits.",
            "So these are the result."
        ],
        [
            "So first.",
            "This left line shows four categories we picked from the writers for which were testing the classification, then this.",
            "FYFY means that for example, FF means that the training set was in French and testing centers in French.",
            "It means that the training set was were composed from French documents, but the testing testing was done on English documents and so on, so here.",
            "Here it's written, it's.",
            "Break even point.",
            "So it's a.",
            "Measure for classification accuracy.",
            "And you can see, for example, that in so the light bar is is artificial.",
            "Was is for the case where semantic space restraint on RT artificially generated.",
            "Training set and black bar is when the training was done on.",
            "Human translated documents but taken from some other domain.",
            "And as you can see that this line line bars are.",
            "In most cases, significantly better than the black one.",
            "Concert once, except in this front cases here, but even there the.",
            "Mostly the.",
            "Artificial semantic space means.",
            "You can also see that.",
            "The accuracy doesn't really change, for example, when you go from.",
            "When we learn classifier or English and go to test on French, it's not much different than if you do both things in English documents.",
            "And another thing we tried, but it's not showing here.",
            "We also tried classification in the original space so Justin original bag of words, space and the drop in performance is around 5% so.",
            "By going into semantic space, we lose lose around 5%, but within this option of.",
            "Classifying documents in different languages.",
            "Average precision break even points over the.",
            "I can hear the conclusion."
        ],
        [
            "So first we show that we can do this.",
            "We can use machine translation as a very for generating.",
            "Birthday to set training sets when we don't have a.",
            "Already when we don't have available one generated by human persons by human translators.",
            "And it can significantly decrease the cost of obtaining such training set.",
            "And it it's it's also better than taking training set from some other domains.",
            "So the IT matters what kind of what kind of.",
            "What kind of topics are in the documents you are training CCR on?",
            "And I would like MacArthur for providing the interface for Google language tools for automatic translation.",
            "That's it."
        ],
        [
            "So we have time for questions.",
            "Silly question, but if we manage to cope.",
            "Well then she gave me.",
            "Would be nice.",
            "I think we're using language too.",
            "So here all the only time we need this machine translation is a training time after that.",
            "So once you get the semantic representations you can do everything part efficiently.",
            "Don't need to when you get a new query, you don't need to go and translate it, you just.",
            "You just project it into this semantic space and that's it.",
            "So if otherwise we would always have to go and do the translations and so on.",
            "Is this?",
            "OK is she getting some bounces Aversa right but you know they don't get sentences, grammar or pencil for both the classification.",
            "Really looking for combinations of Mount Olympus type questions.",
            "So if you think this is just announced that predicting.",
            "Just trying to write down.",
            "But it didn't really try to use maybe 1.",
            "One way would be to just use English Dictionary, English French dictionary and translate some words, but didn't try that.",
            "But again you would.",
            "Maybe it would be a cheaper vapor.",
            "Generating this training set but still at the end, if you want to use it real time is the same issue, so we always have to go to dictionary and map everything.",
            "Question.",
            "OK, so thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sean Taylor and it's not the use of machine translation tools for crossing through text mining.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm back for tonight from your Steven Institute, Pennsylvania.",
                    "label": 0
                },
                {
                    "sent": "Now what they presented the joint work with Joshua Taylor.",
                    "label": 0
                },
                {
                    "sent": "It's on how machine translation can help at cross lingual text mining.",
                    "label": 0
                },
                {
                    "sent": "So here is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "First people short introduction.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are the?",
                    "label": 0
                },
                {
                    "sent": "But issues appear at cross lingual text mining.",
                    "label": 0
                },
                {
                    "sent": "What's the difference between normal text mining?",
                    "label": 1
                },
                {
                    "sent": "Then I give a short introduction to kernel CCA.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I relate machine translation to kernel kisses, KCI and present the experiments we made about involving machine translation.",
                    "label": 0
                },
                {
                    "sent": "And I finish with some conclusions about the work.",
                    "label": 0
                },
                {
                    "sent": "So first crossing.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Text mining, so sometimes we have text corpora in more languages and we want to run some standard text mining algorithms on it.",
                    "label": 1
                },
                {
                    "sent": "But because of different languages, some new issues appear, for example, that information retrieval.",
                    "label": 0
                },
                {
                    "sent": "We have like we want to just issue one query in one language, but we want to get to do the search over the whole corpus of documents no matter what the language of the document is.",
                    "label": 0
                },
                {
                    "sent": "So that means that what we want.",
                    "label": 0
                },
                {
                    "sent": "What's important is the meaning of a query, not exact words of the query.",
                    "label": 1
                },
                {
                    "sent": "Another example is classification.",
                    "label": 0
                },
                {
                    "sent": "When we would like just to learn one classifier that can classify all the documents and not separate classifier for each language.",
                    "label": 1
                },
                {
                    "sent": "And for example in clustering.",
                    "label": 0
                },
                {
                    "sent": "Probably if you just run it without doing anything, just group the documents by language.",
                    "label": 0
                },
                {
                    "sent": "So, but we want to group them by meaning not by language.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of the kernel kissick canonic.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correlation analysis is a one way of solving this issues, so it's gets on the input we get.",
                    "label": 1
                },
                {
                    "sent": "Aligned corpus data set.",
                    "label": 0
                },
                {
                    "sent": "So these are for example here for English and German documents, and we must have a so pairs of documents.",
                    "label": 0
                },
                {
                    "sent": "So this is like English document and it's German translation.",
                    "label": 0
                },
                {
                    "sent": "And from this we would like to learn this so called semantic dimensions.",
                    "label": 0
                },
                {
                    "sent": "So the words.",
                    "label": 0
                },
                {
                    "sent": "Which word in English share common meaning with them?",
                    "label": 0
                },
                {
                    "sent": "Which words in German?",
                    "label": 0
                },
                {
                    "sent": "Because this and in this way we would by using this as new representation these dimensions as new representation of our documents.",
                    "label": 0
                },
                {
                    "sent": "So no more just not anymore bag of words model.",
                    "label": 0
                },
                {
                    "sent": "But this more semantic dimensions would get so called.",
                    "label": 0
                },
                {
                    "sent": "We would get so called language independent space.",
                    "label": 1
                },
                {
                    "sent": "And then we get the new document.",
                    "label": 0
                },
                {
                    "sent": "For example, in English language we can just project it into this.",
                    "label": 0
                },
                {
                    "sent": "Semantic dimensions and we get a representation of the document which is invariant to the language.",
                    "label": 1
                },
                {
                    "sent": "So, but there's one.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Issue with this approach, we need a third training set and that can be very expensive to get.",
                    "label": 1
                },
                {
                    "sent": "So if you know so this part here is prepared training set.",
                    "label": 1
                },
                {
                    "sent": "So when we are Wendy's, usually this we don't have this information available and we have.",
                    "label": 0
                },
                {
                    "sent": "Generally have two options.",
                    "label": 0
                },
                {
                    "sent": "One is that we can just use datasets, train party datasets with some other domain.",
                    "label": 1
                },
                {
                    "sent": "But discuss one big issues that.",
                    "label": 0
                },
                {
                    "sent": "Maybe their domain is talking about different things, uses different words, so and then the gap between words maybe 2.",
                    "label": 1
                },
                {
                    "sent": "Alright, so the dimensions, semantic dimension is learned from their training set, would wouldn't be of much use for our task and the other.",
                    "label": 0
                },
                {
                    "sent": "Possible ways to use machine translations.",
                    "label": 0
                },
                {
                    "sent": "For example Google language tools.",
                    "label": 1
                },
                {
                    "sent": "And we would just take a sample of the documents from other domain and it would translate them to.",
                    "label": 0
                },
                {
                    "sent": "For example, would take a set of figures, documents that state into French, take a set of transactions, translate into English, and this would be a very fast and cheap way to get paired train set.",
                    "label": 0
                },
                {
                    "sent": "So but",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we would like to check if this is true.",
                    "label": 0
                },
                {
                    "sent": "If you can really help with this.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "We would like to investigate the quality of this training set for Casey.",
                    "label": 1
                },
                {
                    "sent": "We generate on this way, so on 1st we will check when we take a domain for which we have already available Bert Dispatch training set.",
                    "label": 0
                },
                {
                    "sent": "And we will.",
                    "label": 0
                },
                {
                    "sent": "We will compare it to an artificially generated one without using Google translation.",
                    "label": 0
                },
                {
                    "sent": "And we see how much do we.",
                    "label": 1
                },
                {
                    "sent": "How much do we lose comparing to the human translators?",
                    "label": 0
                },
                {
                    "sent": "Or if you lose anything at all?",
                    "label": 0
                },
                {
                    "sent": "And then we'll move to.",
                    "label": 0
                },
                {
                    "sent": "This is this, and then we move to another domain, for which there will be another task which we won't have a.",
                    "label": 1
                },
                {
                    "sent": "Any training set available and will.",
                    "label": 1
                },
                {
                    "sent": "See how much can we if this machine translation can help?",
                    "label": 0
                },
                {
                    "sent": "So for the first part.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use information retrieval task.",
                    "label": 0
                },
                {
                    "sent": "We compared this to corpus is corpora forest hanserd corpus it's a.",
                    "label": 0
                },
                {
                    "sent": "Generated from Parliament proceedings, Canadian Parliament proceedings where they have everything written down in English and French language and it's aligned by paragraphs and we use this as training input for CCA.",
                    "label": 1
                },
                {
                    "sent": "And we generated artificial corpus 2, so we took.",
                    "label": 0
                },
                {
                    "sent": "We split this answer corpus in half.",
                    "label": 0
                },
                {
                    "sent": "We took the.",
                    "label": 0
                },
                {
                    "sent": "From the first theorist, all the French translations an from the.",
                    "label": 0
                },
                {
                    "sent": "The second half we erased all the English translations and we use Google to.",
                    "label": 0
                },
                {
                    "sent": "To obtain these translations again, so basically it was totally machine translation, nothing, no human translating work was done except this.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "So these are the corpora be used for different corpora we used for training CCA.",
                    "label": 0
                },
                {
                    "sent": "And then for experience experiments, we.",
                    "label": 0
                },
                {
                    "sent": "Extracted from each document from test set.",
                    "label": 0
                },
                {
                    "sent": "We extracted 5 keywords according to the TF.",
                    "label": 0
                },
                {
                    "sent": "IDF weights word weights.",
                    "label": 0
                },
                {
                    "sent": "And use these words as a query for the.",
                    "label": 0
                },
                {
                    "sent": "And the goal was to find the.",
                    "label": 0
                },
                {
                    "sent": "To retrieve the document from which we extracted these keywords or experta documents each translation.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "Procedure how it went.",
                    "label": 0
                },
                {
                    "sent": "So first we learn PCA 1500 pair documents.",
                    "label": 0
                },
                {
                    "sent": "So this pair documents are from these two corpora.",
                    "label": 0
                },
                {
                    "sent": "Then all the case documents.",
                    "label": 0
                },
                {
                    "sent": "So this test documents come together with answer to corpora.",
                    "label": 0
                },
                {
                    "sent": "We projected them into this semantic space on these semantic dimensions.",
                    "label": 0
                },
                {
                    "sent": "And then when we got on a query we projected we represented this query as a document.",
                    "label": 0
                },
                {
                    "sent": "We imagine it as a document.",
                    "label": 0
                },
                {
                    "sent": "We mapped it again into this semantic space and nearest neighbor there.",
                    "label": 0
                },
                {
                    "sent": "Through their closest documents.",
                    "label": 0
                },
                {
                    "sent": "So these are the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we got.",
                    "label": 0
                },
                {
                    "sent": "So here we have two.",
                    "label": 0
                },
                {
                    "sent": "Both corpora concert and artificial corpora.",
                    "label": 0
                },
                {
                    "sent": "Columns mean this means this.",
                    "label": 0
                },
                {
                    "sent": "For example, this means that the variables in English and researching over English documents.",
                    "label": 0
                },
                {
                    "sent": "This means that the query was English in researching French documents and so on.",
                    "label": 0
                },
                {
                    "sent": "And these these two numbers are so there.",
                    "label": 0
                },
                {
                    "sent": "For example, in this case, 65 means that.",
                    "label": 0
                },
                {
                    "sent": "Out of all the queries in 65%.",
                    "label": 0
                },
                {
                    "sent": "Times in the French query was issued the right English documents appeared in the 1st place along retrieve documents and this means that 95% of the times when French query was issued the.",
                    "label": 0
                },
                {
                    "sent": "English documents appear among the first 10.",
                    "label": 0
                },
                {
                    "sent": "Results.",
                    "label": 0
                },
                {
                    "sent": "We can notice, for example, first that when both the query in the search space, the space of documents in which we were searching there in the same language.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "Basically, let's get the same results almost so.",
                    "label": 0
                },
                {
                    "sent": "That shows us that the semantic dimensions found from the machine translation from this artificial corpora are as good as the ones found from corpora mean by human translators.",
                    "label": 0
                },
                {
                    "sent": "However, the correlations between this semantic dimensions was not so good.",
                    "label": 0
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                },
                {
                    "sent": "So when the query was in English for query and was in different languages and the documents you're searching over, then the.",
                    "label": 0
                },
                {
                    "sent": "Performance drop for allowance.",
                    "label": 0
                },
                {
                    "sent": "5 to 10% the same for this case, and so you can see here these differences.",
                    "label": 0
                },
                {
                    "sent": "The second.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment with classification.",
                    "label": 0
                },
                {
                    "sent": "So here we picked a domain where we there was no parent corpora available.",
                    "label": 0
                },
                {
                    "sent": "For training.",
                    "label": 0
                },
                {
                    "sent": "And so I mean, again, we use two different corpora.",
                    "label": 0
                },
                {
                    "sent": "Train sets for training CCA.",
                    "label": 0
                },
                {
                    "sent": "The first one was the same as in previous example.",
                    "label": 0
                },
                {
                    "sent": "We took that from Canadian Parliament proceedings, so this is a.",
                    "label": 0
                },
                {
                    "sent": "This is done by human translators.",
                    "label": 0
                },
                {
                    "sent": "And for the second corpora, we picked some documents from.",
                    "label": 0
                },
                {
                    "sent": "From English news part of the Reuters news, and we picked some news from the French part of writers and we translated them to get their para documents.",
                    "label": 0
                },
                {
                    "sent": "And use that as a thing for the input for learning CCA.",
                    "label": 0
                },
                {
                    "sent": "So again, the procedure here must be trained.",
                    "label": 0
                },
                {
                    "sent": "CCA first turned it on this on hand certainly train it on the artificial corpora.",
                    "label": 0
                },
                {
                    "sent": "Then the whole writers were projected into the CCA semantic space into this language independent representation.",
                    "label": 1
                },
                {
                    "sent": "An in.",
                    "label": 1
                },
                {
                    "sent": "Inside that space we trained linear SVM classifier.",
                    "label": 0
                },
                {
                    "sent": "On a subset of 303 thousand documents infested over 50,000 documents, and so all the presented results are averaged over 5 random splits.",
                    "label": 1
                },
                {
                    "sent": "So these are the result.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first.",
                    "label": 0
                },
                {
                    "sent": "This left line shows four categories we picked from the writers for which were testing the classification, then this.",
                    "label": 0
                },
                {
                    "sent": "FYFY means that for example, FF means that the training set was in French and testing centers in French.",
                    "label": 0
                },
                {
                    "sent": "It means that the training set was were composed from French documents, but the testing testing was done on English documents and so on, so here.",
                    "label": 0
                },
                {
                    "sent": "Here it's written, it's.",
                    "label": 0
                },
                {
                    "sent": "Break even point.",
                    "label": 0
                },
                {
                    "sent": "So it's a.",
                    "label": 0
                },
                {
                    "sent": "Measure for classification accuracy.",
                    "label": 0
                },
                {
                    "sent": "And you can see, for example, that in so the light bar is is artificial.",
                    "label": 0
                },
                {
                    "sent": "Was is for the case where semantic space restraint on RT artificially generated.",
                    "label": 0
                },
                {
                    "sent": "Training set and black bar is when the training was done on.",
                    "label": 1
                },
                {
                    "sent": "Human translated documents but taken from some other domain.",
                    "label": 1
                },
                {
                    "sent": "And as you can see that this line line bars are.",
                    "label": 1
                },
                {
                    "sent": "In most cases, significantly better than the black one.",
                    "label": 0
                },
                {
                    "sent": "Concert once, except in this front cases here, but even there the.",
                    "label": 0
                },
                {
                    "sent": "Mostly the.",
                    "label": 0
                },
                {
                    "sent": "Artificial semantic space means.",
                    "label": 0
                },
                {
                    "sent": "You can also see that.",
                    "label": 0
                },
                {
                    "sent": "The accuracy doesn't really change, for example, when you go from.",
                    "label": 0
                },
                {
                    "sent": "When we learn classifier or English and go to test on French, it's not much different than if you do both things in English documents.",
                    "label": 0
                },
                {
                    "sent": "And another thing we tried, but it's not showing here.",
                    "label": 0
                },
                {
                    "sent": "We also tried classification in the original space so Justin original bag of words, space and the drop in performance is around 5% so.",
                    "label": 0
                },
                {
                    "sent": "By going into semantic space, we lose lose around 5%, but within this option of.",
                    "label": 0
                },
                {
                    "sent": "Classifying documents in different languages.",
                    "label": 0
                },
                {
                    "sent": "Average precision break even points over the.",
                    "label": 0
                },
                {
                    "sent": "I can hear the conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first we show that we can do this.",
                    "label": 0
                },
                {
                    "sent": "We can use machine translation as a very for generating.",
                    "label": 1
                },
                {
                    "sent": "Birthday to set training sets when we don't have a.",
                    "label": 1
                },
                {
                    "sent": "Already when we don't have available one generated by human persons by human translators.",
                    "label": 0
                },
                {
                    "sent": "And it can significantly decrease the cost of obtaining such training set.",
                    "label": 1
                },
                {
                    "sent": "And it it's it's also better than taking training set from some other domains.",
                    "label": 0
                },
                {
                    "sent": "So the IT matters what kind of what kind of.",
                    "label": 0
                },
                {
                    "sent": "What kind of topics are in the documents you are training CCR on?",
                    "label": 1
                },
                {
                    "sent": "And I would like MacArthur for providing the interface for Google language tools for automatic translation.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have time for questions.",
                    "label": 0
                },
                {
                    "sent": "Silly question, but if we manage to cope.",
                    "label": 0
                },
                {
                    "sent": "Well then she gave me.",
                    "label": 0
                },
                {
                    "sent": "Would be nice.",
                    "label": 0
                },
                {
                    "sent": "I think we're using language too.",
                    "label": 0
                },
                {
                    "sent": "So here all the only time we need this machine translation is a training time after that.",
                    "label": 0
                },
                {
                    "sent": "So once you get the semantic representations you can do everything part efficiently.",
                    "label": 0
                },
                {
                    "sent": "Don't need to when you get a new query, you don't need to go and translate it, you just.",
                    "label": 0
                },
                {
                    "sent": "You just project it into this semantic space and that's it.",
                    "label": 0
                },
                {
                    "sent": "So if otherwise we would always have to go and do the translations and so on.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "OK is she getting some bounces Aversa right but you know they don't get sentences, grammar or pencil for both the classification.",
                    "label": 0
                },
                {
                    "sent": "Really looking for combinations of Mount Olympus type questions.",
                    "label": 0
                },
                {
                    "sent": "So if you think this is just announced that predicting.",
                    "label": 0
                },
                {
                    "sent": "Just trying to write down.",
                    "label": 0
                },
                {
                    "sent": "But it didn't really try to use maybe 1.",
                    "label": 0
                },
                {
                    "sent": "One way would be to just use English Dictionary, English French dictionary and translate some words, but didn't try that.",
                    "label": 0
                },
                {
                    "sent": "But again you would.",
                    "label": 0
                },
                {
                    "sent": "Maybe it would be a cheaper vapor.",
                    "label": 0
                },
                {
                    "sent": "Generating this training set but still at the end, if you want to use it real time is the same issue, so we always have to go to dictionary and map everything.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "OK, so thank you again.",
                    "label": 0
                }
            ]
        }
    }
}