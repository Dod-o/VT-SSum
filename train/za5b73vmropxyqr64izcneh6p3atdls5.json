{
    "id": "za5b73vmropxyqr64izcneh6p3atdls5",
    "title": "Decentralized Indexing over a Network of RDF Peers",
    "info": {
        "author": [
            "Christian Aebeloe, Aalborg University"
        ],
        "published": "Nov. 27, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_aebeloe_decentralized_indexing/",
    "segmentation": [
        [
            "Welcome to my talk.",
            "My name is cash Enabler and I will present our paper decentralized indexing over a network of RDF peers.",
            "So today sparkling points often unavailable."
        ],
        [
            "In fact, the recent study found that over half of all public employees bargaining points are available less than 95% of the time.",
            "So this is an issue we earlier tried to address with our system."
        ],
        [
            "Called Picnic which is based on unstructured peer to peer architecture and relies on replications in order to replication of data sets in order to ensure that even when node fails, even when the upload fails, we still have access to the data.",
            "So unstructured peer to peer network just means that it consists of a bunch of nodes that functions both as client and as the server.",
            "And then they all have a random connections or random neighbors to other nodes within the network.",
            "No current processing in such a net."
        ],
        [
            "Work consists of generally three steps.",
            "First, we want to reorder triple patterns.",
            "That is, we want to execute the triple patterns with the least amount of results first.",
            "Then"
        ],
        [
            "Based on this order, we evaluate each tribal pattern individually by first selecting the sources within the network to Curry and then retrieve the results from these sources."
        ],
        [
            "And then finally we obtain the full results by joining these.",
            "These bindings of change for the for the individual patterns."
        ],
        [
            "Or this is the issue though with the with the state of with the current state of the art, is that the source selection and retrieval?",
            "It relies on flooding and what this means is that the issuing node will forward the request for each tribal pattern to all of its neighbors, which then forward the request to all of their neighbors and so on."
        ],
        [
            "This means that we have a huge overhead in terms of we have to perform a lot of network requests when issuing accurate.",
            "This is what we try to add."
        ],
        [
            "This in this paper, so the idea is we want to be able to pinpoint which notes to ask for, which for answers to which tribal patterns so that we can avoid the flooding.",
            "So the first thing we define is the locational indexes, which establishes a baseline.",
            "It contains 2 functions to 1st can give in a predicate from the triple pattern, determine which graphs which fragments within the network contains troubles with this predicate.",
            "So if for example we have a triple pattern with the predicate RDF type, in this case we would.",
            "We the function would return that the Brown and the yellow graphs and contains answers to the trouble pattern.",
            "Second function can then given these graphs identify which nodes contain the graph."
        ],
        [
            "So, given that we just found that the Brown and yellow graphs contain answers to the triple pattern, we get a bunch of nodes that contain each of the graphs.",
            "And in this specific example, we see that both nodes are contained within node one or both graphs are contained within node one, and we can easily just.",
            "Avoid flooding an process that ripple pattern over, just not one."
        ],
        [
            "This this index is precomputed by the notes by and this is done by flooding the network with a simple request asking for meta data about the about the fragments that they contain.",
            "This obviously is for a certain step.",
            "So."
        ],
        [
            "This approach only considers the graphs or the fragments individually and not actually within the context of the query and take a look at this example where the first triple pattern can be answered by the blue graph and the second triple pattern can be answered by the green and the orange graphs.",
            "The thing is, the orange and the blue graph actually produced no giant results since they have in this specific case, since they have no subjects in common.",
            "This means that during the orange graph, it doesn't really matter.",
            "We don't have to do it, and we can prove it.",
            "So how do we achieve this?",
            "Well, if we index the subjects and objects of all the photographs, we can check the overlap.",
            "If if two prayers for two joining triple patterns do not overlap, we can prune them."
        ],
        [
            "So of course storing entire lists of topics not is not really feasible for large datasets, so we use a compressed representation of these sets called Bloom filters.",
            "Bloom filters contains a set of hash functions and opid vector.",
            "So when we insert an element, we hash the element using the hash functions and set the corresponding bits to one.",
            "When we then look up an element, we hash the element again using the hash functions and check whether or not all the bits are set to 1.",
            "If just one bit corresponding bits to the hash values zero, then we say that the element is definitely not within the set.",
            "If all of the bits are one, then we say that the element might be in this set.",
            "We can still not be completely sure."
        ],
        [
            "So we're not completely out of the Woods yet, because as I mentioned, there can be a lot of subjects and objects for large datasets an for bit vectors in order to keep relatively low false positive rate, we need a lot of bits.",
            "This is not that big of an issue for if we just had to store 1 bit vector, but when we have that thousands of fragments and they all have to be the same size to check the overlap, this quickly becomes unfeasible."
        ],
        [
            "So the way we solve this is that we partition the bit vectors based on common prefixes in the UI's.",
            "The idea."
        ],
        [
            "Is that most of these particular partitions will actually have quite a quite few elements and thus require fewer amounts of bitzan even for the party."
        ],
        [
            "Actions that then are large and false positives are more tolerable, since we can still distinguish between prefixes and your eyes with common prefixes are more likely to be contained within the same graph."
        ],
        [
            "So we present the prefix petition Bloomfield Sorptive's, which are similar to regular Bloom filters but contains 1 bit vector for each UI prefix that has been inserted into into the set.",
            "When we insert an element, we check for the petition if if a petition for the specific prefixes there.",
            "If not, we create a new petition.",
            "If it is, we insert the element into the.",
            "Into the correct position when we look up an element.",
            "If there's no petition for the prefix, obviously the element will not be in the set.",
            "If there is, we check to check whether or not it is within that partition.",
            "So how do you use this to check the overlap of two of two PDF's?"
        ],
        [
            "So we when we check the overlap between two sets, we take the intersection right so?",
            "The intersection of two bloom filters is a bitwise and operation that approximates the intersection.",
            "And we essentially do the same just for the positions that the two PBS have in common.",
            "And then we ignore the risk because they won't be in the intersection anyway.",
            "So using this doing current processing or SA precomputation step each node."
        ],
        [
            "Will will download the PPS for all the fragments that are reachable within the neighborhood.",
            "And then of course, create the PPS for their own fragments.",
            "Doing career processing A they will then for two triple patterns to join they will check all if all the graphs overlap.",
            "If a graph or triple pattern overlaps with no graph for another triple pattern that joins with the children pattern."
        ],
        [
            "We prune the graph.",
            "And then we just ask the notes directly to the that have the graphs that we matched.",
            "So let me give you an example of how we can use PDF's to match."
        ],
        [
            "A triple patterns to a.",
            "Set of graphs using PPS.",
            "The first step is to go through the server patterns and see which ones which graphs correspond to its triple patterns of 41 here.",
            "The Brown and yellow graphs have troubles with the predicate IDF type.",
            "However, the yellow graph does not contain DBO president."
        ],
        [
            "Yes, it's a it's PPF and so we can already prove the yellow graph.",
            "For the second server pattern, the blue graph has to predicate the bird, nationality and DPI United State."
        ],
        [
            "Which is within the PPF.",
            "For the last verbal pattern.",
            "Green and orange."
        ],
        [
            "Refs have troubles with the predicate party."
        ],
        [
            "So the next step is to then go through the trouble patterns that join and check whether or not the fragments overlap in this case.",
            "We see that tip it Runescape three join NPR.",
            "However, the pughs at the intersection after peace between the orange and blue graph is empty and therefore they produce no John result.",
            "As a result, the Orange graph produced by John results with any graph 42 and we can prune the on scraf."
        ],
        [
            "So we eliminate it.",
            "The yellow and orange graph, and thus we don't have two orders.",
            "We have to carry fewer nodes in total."
        ],
        [
            "So using this this matching that we just made incur processing in this example, we already processed TP one and two 2.",
            "And the resulting findings for those is that we have two presidents in our data set.",
            "Donald Trump and Barack Obama.",
            "A.",
            "So what we do is when we evaluate the third table pattern, we look in the matching.",
            "We find that the.",
            "ATP3 is matched to the green graph.",
            "The green graph is located on two and four and five.",
            "Which one of those we select a similar matter?",
            "In this case we select and four and then we send to in 4, three with the bindings that we obtained for to be 21, and then as a result we get that done tremors from the Republican Party and burger farmers from the Democratic Party."
        ],
        [
            "Which also is the result of the query."
        ],
        [
            "So in order to test whether or not our indexes work, we implemented them on top of picnic.",
            "As I mentioned before.",
            "We used to set up with 200 picnic nodes running concurrently."
        ],
        [
            "A setup with four 16 core processors and about 500 gigabytes of RAM."
        ],
        [
            "We used lots of bench for curious and data.",
            "Which."
        ],
        [
            "Price is 13 datasets with a total of over a billion trips."
        ],
        [
            "And 40 different queries in four different categories."
        ],
        [
            "So far setup we use the time to live value that is.",
            "How many hops our neighborhoods, stretches of five replication factor 5%.",
            "That means all the fragments are replicated on 5% of the nodes and it's not had five neighbors.",
            "In our experiments, we compared picnic without using indexes to picnic using location indexes and picnic using peeps."
        ],
        [
            "So in this first graph we see on the Y axis the number of exchange messages in log scale an and the trend here, and this is the case for all.",
            "For all of the curious is that both indexes result in larger messages and thus.",
            "Yes, we have reduced the amount of network traffic.",
            "And this."
        ],
        [
            "This this is more significant for PF.",
            "Stand for locational indexes.",
            "This translates to the execution time.",
            "On in seconds and log scale on the Y axis.",
            "Here for the same queries where.",
            "Where both indexes reduce.",
            "Reduce execution time MPPS more significantly.",
            "You will note that there are some cases where some of the queries are actually a little bit slower for PPS then for locational indexes.",
            "This is due to the two to these queries.",
            "The PPS were not able to prune any.",
            "Any graphs, and thus the slide overhead and actually matching to the sort of triple patterns, mean that we have a slightly slower execution time.",
            "So in summary, we presented two decentralized indexes to be used in an unstructured peer network."
        ],
        [
            "We implemented them on top of a state of the art system picnic.",
            "And we were able to.",
            "We were able to process curious up to up to 233 times faster than.",
            "With our indexes compared to two picnic without using the indexes.",
            "And this is probably this is partially due to the reduction in traffic that we were able to obtain.",
            "We were able to reduce the network traffic by up to 876 times compared to not using our indexes.",
            "And I think one of the most important points is that using our indexes we were able to actually answer curious queries that otherwise timed out for.",
            "Or alternative approaches, and specifically also a picnic without indexes.",
            "Thank you For more information visit our website.",
            "Thanks, it was a really nice talk.",
            "I have two questions related to the experiments.",
            "The first one is.",
            "How did you partition the data?",
            "You say it in this benchmark.",
            "It's 13 datasets.",
            "You have 200 something notes.",
            "How did you partition these?",
            "So each each datasets were fragmented based on the predicates.",
            "So we create a fragment for each predicate within the datasets.",
            "This is similar to the way we did it in picnic.",
            "And then a.",
            "These fragments were assigned were signed to a specific note that distributed defragment over 5% of the notes within the network.",
            "So this is the same as how it's done in picnic.",
            "OK, but doesn't this bias the experimental results becausw?",
            "Then essentially all of these.",
            "Each of these graphs has only one predicate.",
            "Yeah, I mean they have only one predicate, each of the fragments, but this is say this is as I mentioned, similar to.",
            "So the way it's done in picnic end and.",
            "A.",
            "It's it's not really.",
            "I guess what we.",
            "OK, but just so these indexes can be used in picnic, but they could also be used in other decentralized or even distributed approaches, right?",
            "Without this kind of fragmentation and the other question is related to.",
            "This extended notion of the bloom filters, which kind of has several bloom filters or for each UI prefix I'm you did not tell us how big individual bloom filters have been.",
            "The ones that you used in experiment's first and second did you experiment with modifying this size of the bloom filters, and what effect would this have on the on?",
            "The quality of the estimates?",
            "Yeah so.",
            "So in the.",
            "Defence data set start total of 500 million.",
            "Around 500 million distinct subjects and objects and.",
            "This means that in order to keep a relatively low false positive rate, we would need 4 billion, around 4 billion bits, which of course it's not a problem if you would just saw one Bloom filter Part 4.",
            "But when you have thousands of fragments, then it can usually become an issue, right?",
            "We then try to lower the amount of bits to the same amount as the largest.",
            "Has the largest PPF in total over all the partitions within it and we got a very high high sorry false positive rate.",
            "My question is how many bits do the individual bloom filters have in the experiments?",
            "They had oh so the most amount of space that these took up on the disk.",
            "For each node was around 1 megabyte for each of the nodes, which is.",
            "I. I can't remember exactly how many bits each do all do.",
            "All of the individual bloom filters have the same size.",
            "Yeah, they they have to, otherwise we can't take the intersection of them so they all have to have the same sense right?",
            "OK so did but you did not experiment by kind of changing the size of this of all of these individual bloom filters and see what the effect would be.",
            "So we did a little bit.",
            "We tried to make experiments with non partition bloom filters as well but again we would have to have a lot smaller.",
            "Filters in order to afford to be feasible to store and as a result we got essentially all of all the times we tried to check the overlap we we got with the result that there was an overlap, even though that wasn't so.",
            "So those are resolved.",
            "Thank you any other question.",
            "I do have also.",
            "These these benchmarks you use uses DDR 3.4, right?",
            "Yeah, we we we.",
            "Because there are several datasets in that benchmark.",
            "Yeah, so maybe I didn't understand it correctly, but every note is storing the indexes.",
            "They bloom filters yes.",
            "So every note downloads to Bloom filters off the fragments that they can and which notice doing the query processing because maybe I'm yeah so.",
            "So we when we did the valuations, we essentially selected random nodes to do the evaluation and then.",
            "We did that three times and took the average of those on.",
            "OK, thank you, and if we scale that.",
            "The GPS 3.4 to the latest, the PEDIA.",
            "How could impact that the intermediate processing of intermediate results?",
            "Because this going this is going to scale and do you have a look at this possible problem so we didn't have a look at that, but I mean just a case.",
            "Sorry I want to I guess.",
            "I mean, I think it is going to achieve scale quite well up to a certain point.",
            "Where I mean obviously when you have larger sets and larger sets of subjects and objects, room filters have to be larger and so do the individual partitions.",
            "However, that scales much slower than competition bloom filters, and so the.",
            "Yeah, will scale it better than under.",
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Welcome to my talk.",
                    "label": 0
                },
                {
                    "sent": "My name is cash Enabler and I will present our paper decentralized indexing over a network of RDF peers.",
                    "label": 1
                },
                {
                    "sent": "So today sparkling points often unavailable.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, the recent study found that over half of all public employees bargaining points are available less than 95% of the time.",
                    "label": 0
                },
                {
                    "sent": "So this is an issue we earlier tried to address with our system.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Called Picnic which is based on unstructured peer to peer architecture and relies on replications in order to replication of data sets in order to ensure that even when node fails, even when the upload fails, we still have access to the data.",
                    "label": 0
                },
                {
                    "sent": "So unstructured peer to peer network just means that it consists of a bunch of nodes that functions both as client and as the server.",
                    "label": 0
                },
                {
                    "sent": "And then they all have a random connections or random neighbors to other nodes within the network.",
                    "label": 0
                },
                {
                    "sent": "No current processing in such a net.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work consists of generally three steps.",
                    "label": 0
                },
                {
                    "sent": "First, we want to reorder triple patterns.",
                    "label": 1
                },
                {
                    "sent": "That is, we want to execute the triple patterns with the least amount of results first.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on this order, we evaluate each tribal pattern individually by first selecting the sources within the network to Curry and then retrieve the results from these sources.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then finally we obtain the full results by joining these.",
                    "label": 0
                },
                {
                    "sent": "These bindings of change for the for the individual patterns.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or this is the issue though with the with the state of with the current state of the art, is that the source selection and retrieval?",
                    "label": 0
                },
                {
                    "sent": "It relies on flooding and what this means is that the issuing node will forward the request for each tribal pattern to all of its neighbors, which then forward the request to all of their neighbors and so on.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This means that we have a huge overhead in terms of we have to perform a lot of network requests when issuing accurate.",
                    "label": 0
                },
                {
                    "sent": "This is what we try to add.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This in this paper, so the idea is we want to be able to pinpoint which notes to ask for, which for answers to which tribal patterns so that we can avoid the flooding.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we define is the locational indexes, which establishes a baseline.",
                    "label": 0
                },
                {
                    "sent": "It contains 2 functions to 1st can give in a predicate from the triple pattern, determine which graphs which fragments within the network contains troubles with this predicate.",
                    "label": 0
                },
                {
                    "sent": "So if for example we have a triple pattern with the predicate RDF type, in this case we would.",
                    "label": 0
                },
                {
                    "sent": "We the function would return that the Brown and the yellow graphs and contains answers to the trouble pattern.",
                    "label": 0
                },
                {
                    "sent": "Second function can then given these graphs identify which nodes contain the graph.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, given that we just found that the Brown and yellow graphs contain answers to the triple pattern, we get a bunch of nodes that contain each of the graphs.",
                    "label": 0
                },
                {
                    "sent": "And in this specific example, we see that both nodes are contained within node one or both graphs are contained within node one, and we can easily just.",
                    "label": 0
                },
                {
                    "sent": "Avoid flooding an process that ripple pattern over, just not one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this index is precomputed by the notes by and this is done by flooding the network with a simple request asking for meta data about the about the fragments that they contain.",
                    "label": 0
                },
                {
                    "sent": "This obviously is for a certain step.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This approach only considers the graphs or the fragments individually and not actually within the context of the query and take a look at this example where the first triple pattern can be answered by the blue graph and the second triple pattern can be answered by the green and the orange graphs.",
                    "label": 0
                },
                {
                    "sent": "The thing is, the orange and the blue graph actually produced no giant results since they have in this specific case, since they have no subjects in common.",
                    "label": 0
                },
                {
                    "sent": "This means that during the orange graph, it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "We don't have to do it, and we can prove it.",
                    "label": 0
                },
                {
                    "sent": "So how do we achieve this?",
                    "label": 0
                },
                {
                    "sent": "Well, if we index the subjects and objects of all the photographs, we can check the overlap.",
                    "label": 0
                },
                {
                    "sent": "If if two prayers for two joining triple patterns do not overlap, we can prune them.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course storing entire lists of topics not is not really feasible for large datasets, so we use a compressed representation of these sets called Bloom filters.",
                    "label": 0
                },
                {
                    "sent": "Bloom filters contains a set of hash functions and opid vector.",
                    "label": 0
                },
                {
                    "sent": "So when we insert an element, we hash the element using the hash functions and set the corresponding bits to one.",
                    "label": 0
                },
                {
                    "sent": "When we then look up an element, we hash the element again using the hash functions and check whether or not all the bits are set to 1.",
                    "label": 0
                },
                {
                    "sent": "If just one bit corresponding bits to the hash values zero, then we say that the element is definitely not within the set.",
                    "label": 0
                },
                {
                    "sent": "If all of the bits are one, then we say that the element might be in this set.",
                    "label": 0
                },
                {
                    "sent": "We can still not be completely sure.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're not completely out of the Woods yet, because as I mentioned, there can be a lot of subjects and objects for large datasets an for bit vectors in order to keep relatively low false positive rate, we need a lot of bits.",
                    "label": 0
                },
                {
                    "sent": "This is not that big of an issue for if we just had to store 1 bit vector, but when we have that thousands of fragments and they all have to be the same size to check the overlap, this quickly becomes unfeasible.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way we solve this is that we partition the bit vectors based on common prefixes in the UI's.",
                    "label": 0
                },
                {
                    "sent": "The idea.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that most of these particular partitions will actually have quite a quite few elements and thus require fewer amounts of bitzan even for the party.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions that then are large and false positives are more tolerable, since we can still distinguish between prefixes and your eyes with common prefixes are more likely to be contained within the same graph.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we present the prefix petition Bloomfield Sorptive's, which are similar to regular Bloom filters but contains 1 bit vector for each UI prefix that has been inserted into into the set.",
                    "label": 0
                },
                {
                    "sent": "When we insert an element, we check for the petition if if a petition for the specific prefixes there.",
                    "label": 0
                },
                {
                    "sent": "If not, we create a new petition.",
                    "label": 0
                },
                {
                    "sent": "If it is, we insert the element into the.",
                    "label": 0
                },
                {
                    "sent": "Into the correct position when we look up an element.",
                    "label": 0
                },
                {
                    "sent": "If there's no petition for the prefix, obviously the element will not be in the set.",
                    "label": 0
                },
                {
                    "sent": "If there is, we check to check whether or not it is within that partition.",
                    "label": 0
                },
                {
                    "sent": "So how do you use this to check the overlap of two of two PDF's?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we when we check the overlap between two sets, we take the intersection right so?",
                    "label": 0
                },
                {
                    "sent": "The intersection of two bloom filters is a bitwise and operation that approximates the intersection.",
                    "label": 0
                },
                {
                    "sent": "And we essentially do the same just for the positions that the two PBS have in common.",
                    "label": 0
                },
                {
                    "sent": "And then we ignore the risk because they won't be in the intersection anyway.",
                    "label": 0
                },
                {
                    "sent": "So using this doing current processing or SA precomputation step each node.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will will download the PPS for all the fragments that are reachable within the neighborhood.",
                    "label": 0
                },
                {
                    "sent": "And then of course, create the PPS for their own fragments.",
                    "label": 0
                },
                {
                    "sent": "Doing career processing A they will then for two triple patterns to join they will check all if all the graphs overlap.",
                    "label": 0
                },
                {
                    "sent": "If a graph or triple pattern overlaps with no graph for another triple pattern that joins with the children pattern.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We prune the graph.",
                    "label": 0
                },
                {
                    "sent": "And then we just ask the notes directly to the that have the graphs that we matched.",
                    "label": 0
                },
                {
                    "sent": "So let me give you an example of how we can use PDF's to match.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A triple patterns to a.",
                    "label": 0
                },
                {
                    "sent": "Set of graphs using PPS.",
                    "label": 0
                },
                {
                    "sent": "The first step is to go through the server patterns and see which ones which graphs correspond to its triple patterns of 41 here.",
                    "label": 0
                },
                {
                    "sent": "The Brown and yellow graphs have troubles with the predicate IDF type.",
                    "label": 0
                },
                {
                    "sent": "However, the yellow graph does not contain DBO president.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, it's a it's PPF and so we can already prove the yellow graph.",
                    "label": 0
                },
                {
                    "sent": "For the second server pattern, the blue graph has to predicate the bird, nationality and DPI United State.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is within the PPF.",
                    "label": 0
                },
                {
                    "sent": "For the last verbal pattern.",
                    "label": 0
                },
                {
                    "sent": "Green and orange.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Refs have troubles with the predicate party.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next step is to then go through the trouble patterns that join and check whether or not the fragments overlap in this case.",
                    "label": 0
                },
                {
                    "sent": "We see that tip it Runescape three join NPR.",
                    "label": 0
                },
                {
                    "sent": "However, the pughs at the intersection after peace between the orange and blue graph is empty and therefore they produce no John result.",
                    "label": 0
                },
                {
                    "sent": "As a result, the Orange graph produced by John results with any graph 42 and we can prune the on scraf.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we eliminate it.",
                    "label": 0
                },
                {
                    "sent": "The yellow and orange graph, and thus we don't have two orders.",
                    "label": 0
                },
                {
                    "sent": "We have to carry fewer nodes in total.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So using this this matching that we just made incur processing in this example, we already processed TP one and two 2.",
                    "label": 0
                },
                {
                    "sent": "And the resulting findings for those is that we have two presidents in our data set.",
                    "label": 0
                },
                {
                    "sent": "Donald Trump and Barack Obama.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "So what we do is when we evaluate the third table pattern, we look in the matching.",
                    "label": 0
                },
                {
                    "sent": "We find that the.",
                    "label": 0
                },
                {
                    "sent": "ATP3 is matched to the green graph.",
                    "label": 0
                },
                {
                    "sent": "The green graph is located on two and four and five.",
                    "label": 0
                },
                {
                    "sent": "Which one of those we select a similar matter?",
                    "label": 0
                },
                {
                    "sent": "In this case we select and four and then we send to in 4, three with the bindings that we obtained for to be 21, and then as a result we get that done tremors from the Republican Party and burger farmers from the Democratic Party.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which also is the result of the query.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in order to test whether or not our indexes work, we implemented them on top of picnic.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "We used to set up with 200 picnic nodes running concurrently.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A setup with four 16 core processors and about 500 gigabytes of RAM.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We used lots of bench for curious and data.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Price is 13 datasets with a total of over a billion trips.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And 40 different queries in four different categories.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far setup we use the time to live value that is.",
                    "label": 0
                },
                {
                    "sent": "How many hops our neighborhoods, stretches of five replication factor 5%.",
                    "label": 0
                },
                {
                    "sent": "That means all the fragments are replicated on 5% of the nodes and it's not had five neighbors.",
                    "label": 0
                },
                {
                    "sent": "In our experiments, we compared picnic without using indexes to picnic using location indexes and picnic using peeps.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this first graph we see on the Y axis the number of exchange messages in log scale an and the trend here, and this is the case for all.",
                    "label": 0
                },
                {
                    "sent": "For all of the curious is that both indexes result in larger messages and thus.",
                    "label": 0
                },
                {
                    "sent": "Yes, we have reduced the amount of network traffic.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this is more significant for PF.",
                    "label": 0
                },
                {
                    "sent": "Stand for locational indexes.",
                    "label": 0
                },
                {
                    "sent": "This translates to the execution time.",
                    "label": 0
                },
                {
                    "sent": "On in seconds and log scale on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "Here for the same queries where.",
                    "label": 0
                },
                {
                    "sent": "Where both indexes reduce.",
                    "label": 0
                },
                {
                    "sent": "Reduce execution time MPPS more significantly.",
                    "label": 0
                },
                {
                    "sent": "You will note that there are some cases where some of the queries are actually a little bit slower for PPS then for locational indexes.",
                    "label": 0
                },
                {
                    "sent": "This is due to the two to these queries.",
                    "label": 0
                },
                {
                    "sent": "The PPS were not able to prune any.",
                    "label": 0
                },
                {
                    "sent": "Any graphs, and thus the slide overhead and actually matching to the sort of triple patterns, mean that we have a slightly slower execution time.",
                    "label": 0
                },
                {
                    "sent": "So in summary, we presented two decentralized indexes to be used in an unstructured peer network.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We implemented them on top of a state of the art system picnic.",
                    "label": 0
                },
                {
                    "sent": "And we were able to.",
                    "label": 0
                },
                {
                    "sent": "We were able to process curious up to up to 233 times faster than.",
                    "label": 0
                },
                {
                    "sent": "With our indexes compared to two picnic without using the indexes.",
                    "label": 0
                },
                {
                    "sent": "And this is probably this is partially due to the reduction in traffic that we were able to obtain.",
                    "label": 0
                },
                {
                    "sent": "We were able to reduce the network traffic by up to 876 times compared to not using our indexes.",
                    "label": 0
                },
                {
                    "sent": "And I think one of the most important points is that using our indexes we were able to actually answer curious queries that otherwise timed out for.",
                    "label": 0
                },
                {
                    "sent": "Or alternative approaches, and specifically also a picnic without indexes.",
                    "label": 0
                },
                {
                    "sent": "Thank you For more information visit our website.",
                    "label": 0
                },
                {
                    "sent": "Thanks, it was a really nice talk.",
                    "label": 0
                },
                {
                    "sent": "I have two questions related to the experiments.",
                    "label": 0
                },
                {
                    "sent": "The first one is.",
                    "label": 0
                },
                {
                    "sent": "How did you partition the data?",
                    "label": 0
                },
                {
                    "sent": "You say it in this benchmark.",
                    "label": 0
                },
                {
                    "sent": "It's 13 datasets.",
                    "label": 0
                },
                {
                    "sent": "You have 200 something notes.",
                    "label": 0
                },
                {
                    "sent": "How did you partition these?",
                    "label": 0
                },
                {
                    "sent": "So each each datasets were fragmented based on the predicates.",
                    "label": 0
                },
                {
                    "sent": "So we create a fragment for each predicate within the datasets.",
                    "label": 0
                },
                {
                    "sent": "This is similar to the way we did it in picnic.",
                    "label": 0
                },
                {
                    "sent": "And then a.",
                    "label": 0
                },
                {
                    "sent": "These fragments were assigned were signed to a specific note that distributed defragment over 5% of the notes within the network.",
                    "label": 0
                },
                {
                    "sent": "So this is the same as how it's done in picnic.",
                    "label": 0
                },
                {
                    "sent": "OK, but doesn't this bias the experimental results becausw?",
                    "label": 0
                },
                {
                    "sent": "Then essentially all of these.",
                    "label": 0
                },
                {
                    "sent": "Each of these graphs has only one predicate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean they have only one predicate, each of the fragments, but this is say this is as I mentioned, similar to.",
                    "label": 0
                },
                {
                    "sent": "So the way it's done in picnic end and.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "It's it's not really.",
                    "label": 0
                },
                {
                    "sent": "I guess what we.",
                    "label": 0
                },
                {
                    "sent": "OK, but just so these indexes can be used in picnic, but they could also be used in other decentralized or even distributed approaches, right?",
                    "label": 0
                },
                {
                    "sent": "Without this kind of fragmentation and the other question is related to.",
                    "label": 0
                },
                {
                    "sent": "This extended notion of the bloom filters, which kind of has several bloom filters or for each UI prefix I'm you did not tell us how big individual bloom filters have been.",
                    "label": 0
                },
                {
                    "sent": "The ones that you used in experiment's first and second did you experiment with modifying this size of the bloom filters, and what effect would this have on the on?",
                    "label": 0
                },
                {
                    "sent": "The quality of the estimates?",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "So in the.",
                    "label": 0
                },
                {
                    "sent": "Defence data set start total of 500 million.",
                    "label": 0
                },
                {
                    "sent": "Around 500 million distinct subjects and objects and.",
                    "label": 0
                },
                {
                    "sent": "This means that in order to keep a relatively low false positive rate, we would need 4 billion, around 4 billion bits, which of course it's not a problem if you would just saw one Bloom filter Part 4.",
                    "label": 0
                },
                {
                    "sent": "But when you have thousands of fragments, then it can usually become an issue, right?",
                    "label": 0
                },
                {
                    "sent": "We then try to lower the amount of bits to the same amount as the largest.",
                    "label": 0
                },
                {
                    "sent": "Has the largest PPF in total over all the partitions within it and we got a very high high sorry false positive rate.",
                    "label": 0
                },
                {
                    "sent": "My question is how many bits do the individual bloom filters have in the experiments?",
                    "label": 0
                },
                {
                    "sent": "They had oh so the most amount of space that these took up on the disk.",
                    "label": 0
                },
                {
                    "sent": "For each node was around 1 megabyte for each of the nodes, which is.",
                    "label": 0
                },
                {
                    "sent": "I. I can't remember exactly how many bits each do all do.",
                    "label": 0
                },
                {
                    "sent": "All of the individual bloom filters have the same size.",
                    "label": 0
                },
                {
                    "sent": "Yeah, they they have to, otherwise we can't take the intersection of them so they all have to have the same sense right?",
                    "label": 0
                },
                {
                    "sent": "OK so did but you did not experiment by kind of changing the size of this of all of these individual bloom filters and see what the effect would be.",
                    "label": 0
                },
                {
                    "sent": "So we did a little bit.",
                    "label": 0
                },
                {
                    "sent": "We tried to make experiments with non partition bloom filters as well but again we would have to have a lot smaller.",
                    "label": 0
                },
                {
                    "sent": "Filters in order to afford to be feasible to store and as a result we got essentially all of all the times we tried to check the overlap we we got with the result that there was an overlap, even though that wasn't so.",
                    "label": 0
                },
                {
                    "sent": "So those are resolved.",
                    "label": 0
                },
                {
                    "sent": "Thank you any other question.",
                    "label": 0
                },
                {
                    "sent": "I do have also.",
                    "label": 0
                },
                {
                    "sent": "These these benchmarks you use uses DDR 3.4, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we we we.",
                    "label": 0
                },
                {
                    "sent": "Because there are several datasets in that benchmark.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so maybe I didn't understand it correctly, but every note is storing the indexes.",
                    "label": 0
                },
                {
                    "sent": "They bloom filters yes.",
                    "label": 0
                },
                {
                    "sent": "So every note downloads to Bloom filters off the fragments that they can and which notice doing the query processing because maybe I'm yeah so.",
                    "label": 0
                },
                {
                    "sent": "So we when we did the valuations, we essentially selected random nodes to do the evaluation and then.",
                    "label": 0
                },
                {
                    "sent": "We did that three times and took the average of those on.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you, and if we scale that.",
                    "label": 0
                },
                {
                    "sent": "The GPS 3.4 to the latest, the PEDIA.",
                    "label": 0
                },
                {
                    "sent": "How could impact that the intermediate processing of intermediate results?",
                    "label": 0
                },
                {
                    "sent": "Because this going this is going to scale and do you have a look at this possible problem so we didn't have a look at that, but I mean just a case.",
                    "label": 0
                },
                {
                    "sent": "Sorry I want to I guess.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think it is going to achieve scale quite well up to a certain point.",
                    "label": 0
                },
                {
                    "sent": "Where I mean obviously when you have larger sets and larger sets of subjects and objects, room filters have to be larger and so do the individual partitions.",
                    "label": 0
                },
                {
                    "sent": "However, that scales much slower than competition bloom filters, and so the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, will scale it better than under.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}