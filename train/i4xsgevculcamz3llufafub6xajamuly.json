{
    "id": "i4xsgevculcamz3llufafub6xajamuly",
    "title": "Persistence-based Clustering",
    "info": {
        "author": [
            "Primo\u017e \u0160kraba, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "April 28, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/solomon_skraba_pbc/",
    "segmentation": [
        [
            "Yeah.",
            "People.",
            "Pretty much my.",
            "So that's what you got now.",
            "In Eclipse is that?",
            "Vanished.",
            "In today.",
            "Yeah.",
            "And in a post in Stanford.",
            "Stanford.",
            "Medical computer science in.",
            "Interview parisa I don't want to be sitting here.",
            "Natural.",
            "Well done.",
            "The paper towel timoci that created a moment about him with double dealer.",
            "In OK.",
            "I've got chairs or somebody will be broad based analogy to damage or English unit on but her sister Connie Garica some Thursday, Dow subliminal Manglish.",
            "So the this is on persistence based clustering, which is current work with Frederick Chazal and Steve would also add geometrica at INRIA and my PhD advisor Leonidas."
        ],
        [
            "Yes so.",
            "Clustering, I'm sure everyone in this room knows a lot more about clustering than I do, but the main idea here is that we have some input samples, a point cloud, some unstructured data."
        ],
        [
            "And what we want to do is we want to find some quote, unquote important segments or clusters without any user intervention or with as little user intervention as possible."
        ],
        [
            "So this is obviously a nil post problem because.",
            "Theoretically, we have absolutely no idea what important actually means."
        ],
        [
            "So, OK, there's this is kind of one of the classic problems in machine learning and artificial intelligence, and there is really entirely too much work to kind of go over what's been done.",
            "But three methods that I'll mention before, hands that are very popular and applications are K means clustering, spectral clustering and mode shift, or mean shift clustering, and I'll really leave it just at this.",
            "But as I go through the talk, I'll try to re late.",
            "What we do to these?"
        ],
        [
            "Other methods?",
            "OK, so.",
            "We come from a much very theoretical background, so we'd like to formalize things in our viewpoints on the clustering problem.",
            "Is really these data points?",
            "We're going to assume are some random samples that come from some unknown density distribution, so we don't know the density distribution.",
            "In practice, we won't even know in what space these points live in.",
            "We're just going to have some approximation of the space and some approximation of the density function.",
            "And we're going to see what we."
        ],
        [
            "Actually find out from this.",
            "So what's the intuitive definition of a cluster?",
            "Well, suppose we have a 1 dimensional density function like this kind of.",
            "A natural definition is to say that."
        ],
        [
            "We are looking for the basins of the peaks of F, so we have our two peaks and we want to say if we follow the gradients at which peak do we end up so kind of a very nice simple."
        ],
        [
            "Definition, but you know, in the best world we would actually have access to the function to the underlying function and the underlying space, but usually we're only going to have finite samples, so."
        ],
        [
            "So the question will be can we approximate these basins of attraction from the samples?",
            "To what extent can we approximate them and what other information can we?"
        ],
        [
            "Correct, OK, so switching now to topology, because really what this talk is about is applying topological tools to clustering.",
            "So topology is the study of how spaces are connected and it deals with the classification of spaces.",
            "So one of my other coworkers, Vin de Silva, likes to say that apology is the branch of mathematics that can tell a donut from a coffee mug and the reason is is because each of them has one hole, and so in the eyes of topology they're exactly the same."
        ],
        [
            "And.",
            "Uh, so.",
            "In math, there's been a topology goes back 100 hundred and 50 years, and there's a lot of work done, but we deal primarily more computational topology, which is we're trying to find qualitative.",
            "But rigorous descriptions for large datasets so.",
            "My background is in geometry where we have some descriptions for shapes and things like this, but these things aren't very robust in the sense that you can change geometry a little bit and.",
            "All the descriptions that you want to say about them will change a lot.",
            "And also things need to be very precise with geometry, whereas with computational topology our only assumption is that we know local neighborhood information, so we don't really know what the entire thing looks like.",
            "But given a point we can say roughly what our neighbor, what's in our neighborhood.",
            "So we really only have kinda."
        ],
        [
            "Proximity information and today I'll be talking about homology, which is 1 very specific invariant that's used in topology."
        ],
        [
            "And.",
            "What it says is basically the idea is that if two things have the same homology, then they may or may not be the same shape, but if they have different Hama."
        ],
        [
            "Geez, we know for certain that they're not the same shape, so in this case if we have a donut.",
            "Empty or a Taurus rather than the sphere.",
            "We know that they're not the same because their homology."
        ],
        [
            "These are different and at least in low dimension, homology has kind of a nice geometric interpretation in the zero dimension, the homology, the of the dimension of homology is really just the connected components of your space.",
            "In one dimension it's the number of holes that your space has, and in two dimension it's the number of enclosed spaces or voids so.",
            "The.",
            "The formalisms and the machinery behind.",
            "Really quite complicated, but today we're only going to be dealing with 0 dimensional homology, which is the connected components, which is, I think, much."
        ],
        [
            "Simpler to understand.",
            "OK, so we're not just going to be dealing with homology, but with persistent technology, which is an extension that goes back from about the year 2000.",
            "So what we're going to look at is, given some function, we're going to look at the evolution of the topology of this super level sets so."
        ],
        [
            "What does this mean?",
            "This means that we're going to take we're going to have a level, and we're going to start sweeping down and we're going to look at the intersection of this space and our function, and here when we hit this maximum point, we're going to get one connected component here and then."
        ],
        [
            "We move this.",
            "We move this down.",
            "This space we look at the intersection.",
            "It's grows.",
            "This one is born again.",
            "So now we have two connected component."
        ],
        [
            "And then as we move down this merge."
        ],
        [
            "Chiz"
        ],
        [
            "And this continues until we get the entire space.",
            "So.",
            "Just as a.",
            "Bit of a tangent is for the vocabulary so that we're on the same page.",
            "Is this sequence of super level sets is called a filtration and all this filtration means is that for.",
            "It's parameterized by some."
        ],
        [
            "By some parameter here Alpha, but it doesn't.",
            "It could be really any parameter it could be in.",
            "The natural number is anything, but it's all it is.",
            "Is this increasing sequence of spaces?",
            "So for any Alpha bigger than beta in this case, since we're going down since our parameters decreasing each space here is bigger than any space here.",
            "So that's the only requirement, and this is called a filtration."
        ],
        [
            "OK, so since we're going to be dealing with density functions, we were going to assume our density function doesn't actually go to Infinity, so we can cross that off.",
            "And we know that our density function is non negative, so we can kind of ignore everything on the right here and so we have a finite filtration which is which goes from some maximum thing to 0."
        ],
        [
            "OK, and so if you remember on the previous slides we had these components when new components were born.",
            "We call that the birth time and when they merge we call that the death time and so in the filtration, there's a point where component appears in a time when it gets merged.",
            "And I'll explain this a little more."
        ],
        [
            "In a second, but just to say that you know the study of filtrations is.",
            "There's a lot of work in math on this and two papers if you're interested into how this relates to persistence is actually the original persistence paper and the follow up which are."
        ],
        [
            "Listed here.",
            "So going back to this so OK, now we're tracking the.",
            "Evolution of this apology so."
        ],
        [
            "As we go down soon before we add this this space, everything here is included here."
        ],
        [
            "And we."
        ],
        [
            "This down here, but what we get by tracking these birth and death times is a bar code or rather a bar.",
            "So this the beginning of the bar is when the component.",
            "Big came into the space and this is when it became merged and OK, what what's why?",
            "Why is this good at all?"
        ],
        [
            "Um well.",
            "There's a proof proved by coercion, or else we're not in her that if our if our function is close, so say that we have some smooth underlying function, or actually any underlying function that's close to the thing that we're measuring.",
            "The bar codes will be close as well."
        ],
        [
            "So this seems promising.",
            "So now what we're going to do is we're just going to say the topological definition of a cluster while we have our basins of attraction again, and are so."
        ],
        [
            "Peoples butt what we're going to do is we're going to say OK, Now we know these are clusters, but these barcodes are really just the signature of these of these of these peaks, so prominent peaks are going to correspond to prominent, persistent, connected components.",
            "So if a peak is big enough, we're going to see a long bar because."
        ],
        [
            "We know that these things are stable.",
            "OK, and so I've been saying that they're close in their stable and I wanted to just say a few words about what this actually means.",
            "So say we've computed some barcodes by an algorithm that all."
        ],
        [
            "Talk about later, but you know we do.",
            "We do it on one space.",
            "We do it on another so we have the blue and the red and now we."
        ],
        [
            "To compare these.",
            "Well, the first thing we're going to do is we're going to map this to R2, so just the birth time is going to be mapped into the into the Y axis and the the sorry.",
            "The birth time will be mapped to the X axis and the death time will be mapped to the Y axis.",
            "So now we have a point now."
        ],
        [
            "Why do we do this?",
            "Well, OK, we if we do this for all the bars we get a bunch of blue and red points."
        ],
        [
            "That's great, but now what we're going to do is we."
        ],
        [
            "Superimpose the two.",
            "And we find a matching a perfect matching.",
            "Where were allowed to match to the diagonal as well."
        ],
        [
            "And the distance we define is called the bottleneck distance and all it says is that over all possible bijections between points blue and red points, we take the the minimum maximum distance.",
            "So it's the Infinity norm and."
        ],
        [
            "Once we have this distance, well.",
            "What we can say is that.",
            "Under function perturbations under kind of distances in the space, and I'll give a few more precise examples later.",
            "We can say that this bottleneck distance remains small if our perturbations are small.",
            "So the point really is that if we have something, we change it a little bit.",
            "These points really theoretically cannot."
        ],
        [
            "Move very far.",
            "OK, so these.",
            "Barcodes and persistence diagrams seem kind of strange the first time you see them, but.",
            "This kind of concept is well known in machine learning.",
            "Dendrograms are actually a form of barcodes and pro."
        ],
        [
            "Distance diagrams, and.",
            "Specifically, if you think about single linkage clustering, all it really is is a distance filtration and I'll explain why."
        ],
        [
            "I mean by that.",
            "So say we have these points and where the distance is their distance in the X axis.",
            "And we get this dendrogram but just buy some."
        ],
        [
            "Linkage clustering, what we're doing is really we're drawing circles around each point, and when there's an intersection, we connect it.",
            "This is exactly just a filtration where we connect things based on how far they are, which we usually call a distance."
        ],
        [
            "Operation, and so we continue this and then this point gets connected to this component.",
            "So we connect."
        ],
        [
            "Edit and we do this."
        ],
        [
            "Then until we've connected everything.",
            "And so our barcode really is just this collection of vertical pieces.",
            "This is exactly a barcode and barcode has exactly the same information as a person."
        ],
        [
            "Since diagram OK, so a natural question is is well, you know why are we throwing away this information here?",
            "Because obviously this has more information than just the persistence diagram, because here we know how we're actually merging rather than just how long."
        ],
        [
            "Survive.",
            "And the question in the answer is well.",
            "We know that the lengths are stable and this is actually known in single linkage clustering as well, but it's also known in single linkage clustering that the connections themselves are.",
            "In general, not stable.",
            "And actually, as it turns out, even over the filtration that we define, which is based on the density function, it's it's not necessarily stable, and in general you really can't hope for any kind of real stability here."
        ],
        [
            "In terms of the connections, OK, so now.",
            "We're getting actually to the algorithm so.",
            "I've talked about persistence diagrams and topology and all these things, but where does the where do the actual clusters come in?",
            "Well, it's."
        ],
        [
            "Quite simple.",
            "So we start off with some space here so it's you have 4 four peaks here, one peak here in a circle like a little crater here and.",
            "And two."
        ],
        [
            "Pics down there.",
            "And So what we're going to do is.",
            "Well, we're going to build some graph on them, and we're going to find some density estimates, and we're just going to do mode seeking, which means from each point we're just going to follow the gradient along the graph until we get."
        ],
        [
            "Stock and what we're going to get is a bunch of clusters that don't really look like they make any sense whatsoever.",
            "But then."
        ],
        [
            "Because we know that the long bars correspond to that.",
            "Things that persist for a long time are going to last.",
            "We're going to use this information to merge these small clusters together so that we end up with something that makes sense.",
            "So here we get the five peaks and then this ring here."
        ],
        [
            "OK, so what's the algorithm?",
            "Our input is a point set with the function value, some graph and a parameter Alpha.",
            "Then I'll talk about."
        ],
        [
            "OK, so it's very simple.",
            "We sort according to F in decreasing order of F because we want the peaks of our density function and for each point we look at our neighbors at the neighborhood in the graph.",
            "And if it's if there's no higher neighbour, then it's a new cluster.",
            "Otherwise we assign it to some pseudo gradient.",
            "It doesn't really matter what.",
            "And then if this point is adjacent to more clusters we look at.",
            "If these clusters are more than Alpha persistent, and if they're not, we merge it into the oldest adjacent cluster.",
            "It's very simple, it's very efficient.",
            "You can implement this using the Union find algorithm, so it runs in.",
            "Roughly, the size of your graph."
        ],
        [
            "Linear in the size of the graph.",
            "OK, so.",
            "Putting this all together, now we have all of the tools that we really need, so we estimate our density and the first thing we do is we run, run the algorithm with Alpha equals to Infinity.",
            "And why do why do we do this?",
            "This is really the standard persistence algorithm, which is how we end up, which is how we compute our persistence diagram.",
            "So in this case, what you see is.",
            "You know we have.",
            "These are the birth times.",
            "These are the death times and we see we have 123456 prominent clusters and a bunch of noise.",
            "So you know we can kind of see a gap here, so we're going to pick our Alpha anywhere in between here and then run the algorithm again an we end up."
        ],
        [
            "Star clusters so if we if we didn't do anything.",
            "So if we picked Alpha equal to 0, this is exactly what we would get.",
            "Picking Alpha a little bit bigger, we end up with.",
            "This nice this nice answer."
        ],
        [
            "OK, so.",
            "I've talked a lot about graphs and parameters, but there's a few kind of graphs that are standard choices that we can actually say something theoretically about them, so the need for a graph is really because we need this notion of a neighborhood, because as I said, topology really just depends on.",
            "This localnet idea of."
        ],
        [
            "Local neighborhood OK so."
        ],
        [
            "Are many choices for graphs, but.",
            "I'm going to talk about 3.",
            "The first is the Vietoris graph, or also known as just the Delta neighborhood Graph, where the our neighborhood graph it's all."
        ],
        [
            "It really says is that.",
            "If for some parameter Delta if too."
        ],
        [
            "Points are within that Delta.",
            "We connect them with an edge and if they are further away, we don't.",
            "This is very.",
            "This is kind of the simplest graph."
        ],
        [
            "If we can build, it's has it's very simple to compute.",
            "We just need pairwise distances.",
            "We can compute in any metric space.",
            "And theoretically, it's also very nice because we can say a lot about what we end up with.",
            "In the end, we can actually have provable reconstruction on quite general spaces, and you know, theoretically it's very nice to work with."
        ],
        [
            "Another very common thing to use is the K nearest neighbor graph, which is you know you have some points and you basically just."
        ],
        [
            "Connect the K nearest neighbors and you do this everywhere so."
        ],
        [
            "This is nice because it's also simple to compute.",
            "Also requires pairwise distances can be computed in any metric space, but it's also sparse.",
            "The Vietoris rips graph can be very very large for large values of Delta, which can slow things down a lot, but this is sparse theoretically it's not quite as nice to work with.",
            "We can say a few things about it, but.",
            "Because it's not the geom.",
            "It's not quite as obviously a geometric graph.",
            "It's a little more you have to work a little bit more."
        ],
        [
            "To say something concrete about it.",
            "And finally, there's the Delaunay graph which is given some points we simply construct."
        ],
        [
            "Any edge that's Adele on edge so I didn't draw the circles, but if.",
            "The Delani graph is basically all the edges that are in a Delaunay triangulation, which is defined in any dimension, and basically it says that for any circum circle, any for any three points where the triangle is in there, the circum circle is empty.",
            "So this is geometric processing."
        ],
        [
            "101 but.",
            "Then really nice properties of these things is that it is sparse.",
            "There is very fast computation if we're in low dimensional space, but we need in some sense to be in a Euclidean space.",
            "And it's Canonical so.",
            "The other two, the Vietoris rips and the K nearest neighbor graph, both require parameter either the size of your neighborhood, the number of neighbors.",
            "This is Canonical.",
            "There is for any inputs set of points.",
            "There's only one possible graph, so when we can build it, it is very nice because it's one less thing we have to experiment across 2."
        ],
        [
            "To get some answers OK, now density estimation, I'm just going to go over very quickly.",
            "It's a huge field of study.",
            "There's books written on it.",
            "There's thousands of papers."
        ],
        [
            "But where we only really use two kind of density estimators because?",
            "I the two most common ones because this is really not our area of expertise.",
            "First is the Gaussian kernel which takes some bandwidth parameter H, which again is kind of a smoothing parameter."
        ],
        [
            "Whereas the other one we used was this distance to measure an.",
            "I'm afraid I forgot the reference to this, but it was it's work by Frederick shizzle, David Koechner and can't on Merry go at all from INRIA.",
            "And so all it says is that it's the root mean squared distance.",
            "To the K nearest neighbors, and they proved there's some very nice stability results about this and it's smooth and it in practice it works very, very nicely.",
            "So these are the two density estimators we use.",
            "There's numerous other ones you can use, and so you can really just pick your favorite estimator."
        ],
        [
            "One requirement that we do have is that your that your estimated function value and your true function value.",
            "You have to be close in the Infinity Norm, so you can't allow single points to be very far off.",
            "Now this is not ideal because often we have outliers and want some of our current work is now how to deal with that but.",
            "Actually, in a lot of cases, like for the Gaussian kernel or in these two cases, you can actually show that you know with sufficient sampling you can actually get this condition, because these things converge fast enough that you know with high probability you're not going to get any outliers."
        ],
        [
            "OK.",
            "So.",
            "Now I wanted to go over some specific guarantee that's, you know.",
            "I've talked about the algorithm and showed you one toy example that works.",
            "But you know what's an?",
            "I've said we can prove a lot of results, theoretical results, but I wanted to give you one example, one concrete example.",
            "So say we are dealing with a manifold and our function is C Lipschitz.",
            "If we have an epsilon cover, which means that any points on the manifold is within epsilon of a sample point.",
            "Well, OK, well on a manifold we can measure distances so we can build a rips graph."
        ],
        [
            "So we build the RIPS graph and the guarantee says that.",
            "So in the soda paper my soda paper from 2009 we showed that this RIPS filtration and the Super level set filtration are close.",
            "Now in the literature we call this interleaved and I didn't really want to get into this because there's quite a lot of machinery to go over, but it's interleaved with a parameter that depends on the Lipschitz constant of the function and.",
            "Delta, which is the parameter of our of our rips graph and the only requirement is that Delta has to be bigger than our sampling, so you know if any point on the spaces within epsilon the smallest rips graph we can build so that this works is that epsilon and by this paper at sausage.",
            "Because these two are close in the sense of interleaved their persistence."
        ],
        [
            "Grams will be close to and what this means is that if we have a sufficient sampling.",
            "And good estimates of our function values.",
            "We will get a good approximation of the persistence diagram.",
            "Now this works for uniform sampling and it's works for any function and pretty much in any space.",
            "I believe any meta."
        ],
        [
            "Space.",
            "OK, but I've mentioned you know we uniform sampling epsilon covers, but we're sampling from some distribution, so our sampling isn't going to be uniform.",
            "And our approximation dip."
        ],
        [
            "Ends on this C Delta.",
            "So what does this actually mean?",
            "So so we have some Gaussian type of thing and we sample, so we get more points near the center than the outside."
        ],
        [
            "So you know what we need to do is we need to find well sampled regions and.",
            "We know that our approximation results holds there."
        ],
        [
            "So.",
            "It's it's a tradeoff because when we're building the rips parameter, we have this scale that we can choose Delta.",
            "So if we choose small Delta, our approximation will be good for a large Delta.",
            "It's the approximation will be less good, but we're going to get more of."
        ],
        [
            "Space, so in this case say we choose a very small Delta, so we draw small balls around everything.",
            "So we're really well sampled only in this super level set.",
            "So we have a good approximation here and here below it.",
            "We can only say a few things."
        ],
        [
            "Whereas if we increase Delta well now we cover more of this space, but our persistence diagram guarantee goes down.",
            "So you know it's a tradeoff."
        ],
        [
            "OK.",
            "So now to actually say some things as well.",
            "OK, we know our persistence diagram is roughly right, but now we can define a signal to noise ratio.",
            "So what we say is we're going to require our persistence diagram to have this structure, so things will either need to be quite far from the diagonal or close the diagonal and some gap in between."
        ],
        [
            "Formally.",
            "Oh, I guess I deleted that but OK.",
            "So if we have this structure what we can say is that while the number of clusters is stable, because we know that the points can't move around too much, we know that a point from here can't go into here and vice versa.",
            "And the proof of it is it's a little tedious, but it's quite geometric.",
            "Basically you just have to decompose your space into some disjoint regions.",
            "An look at each region.",
            "What happens to the points and you can say.",
            "And in the end you can conclude that the number of points in this area.",
            "Is stable over all these over any kind of perturbations that satisfy."
        ],
        [
            "Certain criteria.",
            "OK, so.",
            "The upshot of this is if the peaks are prominent enough, we will get the right number of clusters.",
            "So the number of clusters is stable and Additionally the the more samples you have, the smaller the noise so."
        ],
        [
            "If we go back so.",
            "As we.",
            "As we increase the number of samples we have this this whole area here stays the same or moves around a little bit.",
            "But this D1 depends on the number of samples we have, so this part goes towards the diagonal, where in the if we had an infinite number of points.",
            "Basically we could go arbitrarily close to this diagonal."
        ],
        [
            "Yeah, so yeah, so this part shrinks as the number of points increases, whereas this part stays the same.",
            "So in principle if we had enough points.",
            "Any diagram should be.",
            "Any diagram should be.",
            "Well separated."
        ],
        [
            "OK, but you know this is kind of nice theoretically, but we don't have usually don't have an infinite number of points.",
            "So what does this say practically?",
            "Well, it does give us a sense of how stable the clusters that we actually get our if we know the number of clusters we're looking for, it's very easy to choose a threshold, you can just you just order your points and you cut somewhere when you reach the number of clusters you want.",
            "And actually I found during my experiments that it can help with the choice of other parameters to, for example, the scale of our RIPS parameter, the scale of our graph.",
            "If we choose it too large, what we find happens is that we end up with one connected component that's prominent, and everything else is noise, so that's not very interesting.",
            "Whereas if we choose it too small, we end up with everything that looks like noise, and so often, you know by running it a few times and looking at the persistence diagram, we can kind of say whether we've hits the sweet spots in terms of these."
        ],
        [
            "Other parameters depending on the choices that we've made.",
            "OK, so the last the last theoretical guarantee before I kind of get to the experiments are spatial stability, so I've talked about the number of clusters, the persistence dog, the persistence diagrams, and you know that we're going to get the right number, but.",
            "I've said nothing about that these things look like the actual basins of attraction."
        ],
        [
            "And.",
            "You know, under the SC Lipschitz assumption, which basically means that our function isn't too crazy.",
            "Women in clusters having minimum size."
        ],
        [
            "And so under small perturbations, these peaks are always going to be part of the same class."
        ],
        [
            "So you know we have some clusters and these are the peaks in them."
        ],
        [
            "And we perturb things a little bit, and so our actual maximum can move a lot.",
            "Like think about a flat plane, and if you perturb it just a little bit, your actual Maxima can move arbitrarily far, but we have a guarantee that it will be.",
            "They will still be clustered together."
        ],
        [
            "And so.",
            "That means that if we have different perturbations or different samplings or anything like that, we have a correspondence between the clusters.",
            "Assuming this well set."
        ],
        [
            "Rated condition.",
            "OK, now the bad news is that the unstable part, the parts that don't fall into that, can be arbitrarily large.",
            "So we have this counter example of this.",
            "These two peaks and there's a little Moat around this peak, and so we have.",
            "These are the two peaks.",
            "And then there's another little peak here."
        ],
        [
            "So what does this look like?",
            "Well, at one ripps parameter, everything outside here gets merged into here.",
            "Same thing here, but by shifting the values just a little bit we can make everything go to this into this maximum.",
            "So basically everything here.",
            "Everything on the outside here, so this part is stable.",
            "This part is stable.",
            "Everything outside here is kind of arbitrary into where it goes."
        ],
        [
            "OK, so now for some experimental results, so I'll show you three datasets.",
            "Two are synthetic, inspire two spirals for rings, and then I'll give a biological exam."
        ],
        [
            "As well on simulator.",
            "So R2D example is these two interlock spirals with.",
            "So this is about 100,000 points with a very noisy."
        ],
        [
            "Density estimate, but actually to compare with spectral clustering we had to downsample quite a bit, so this is the actual data we use.",
            "So this is the density.",
            "The point set with the density and the persistence diagram for the rips parameter and you see you get 1, two prominent ones and then some noise, and so this is before merging and after merging and after merging you see you actually do recover the IT actually does go all the way in for a."
        ],
        [
            "For the clusters and for the other.",
            "The nice thing is is that under the different graphs it's actually quite nice as well, so this is again with the RIPS parameter.",
            "This is with a.",
            "With a smaller rips parameter.",
            "So here we decrease the size, the scale size and what we see is we end up with a lot of prominence components here, but we know that since they appear very late we can actually filter them out.",
            "And when we do this well we end up with basically everything on the outside.",
            "That's noise that's been added, gets filtered out because all of this noise corresponds to these small connected components."
        ],
        [
            "OK, and finally yeah, so these are the other two graphs, so this is with the nearest neighbor graph.",
            "So again I mean the persistence diagrams don't look identical, but the general structure is there and this is with Adele on a graph.",
            "And here I want to point out, you know, you may notice that the delani graph actually has a lot more points near the diagonal, and it seems like a lot more noise.",
            "But the reason for this is because the Delaunay graph doesn't really have any sense of scale.",
            "It fills whatever space you have completely.",
            "So what we had to do is we had to subdivide each edge and re estimate the density at that edge.",
            "So in Euclidean space this is quite simple to do because you can just pick some point and estimate the density.",
            "The same you would at any sample point.",
            "And if it's small, it's small.",
            "If it's large it's large, but basically you just do this.",
            "And you end up with the same the same structure to the."
        ],
        [
            "To the persistence diagram.",
            "OK, so you know we like nice examples that look you know that look interesting.",
            "So we did, these four interlocked rings.",
            "So we have.",
            "So there's one ring here, wondering here.",
            "And then both of these rings.",
            "Are are interlocked with it, so it's about 12,000 points.",
            "I think I went up to again.",
            "Two or three 100,000.",
            "But because it's in our."
        ],
        [
            "Then you can't see anything.",
            "So yeah, using the rips graph or I think this is maybe with the nearest neighbor graph.",
            "Again, you see the four clusters which.",
            "Nicely enough correspond to the four rings.",
            "So that's nice."
        ],
        [
            "And you know we compared it across the different graphs, and so you know for a small rips parameter most most of the noise gets filtered out, which is shown in black here.",
            "For the nearest neighbor, obviously everything gets connected so you don't get this filtering property, but we still end up with the same.",
            "With the same four rings in the same before clusters in the persistence diagram and then the davonte again you see this.",
            "The large number of points by by the diagonal, and you know the same four rings.",
            "So the reason we actually chose these two examples is because I spent quite a lot of time trying to get these to work with spectral clustering.",
            "To my frustration, because what ends up happening is all the uniform noise that's been added.",
            "Completely kills any spectral clustering method because you end up you end up with no sparse cuts and you really can't do much about it."
        ],
        [
            "OK. Yeah, so here's a finally one example where this was an earlier four rings example, which you can kind of see here with a lot more points, but you notice here that the gap isn't quite as prominent, so this was.",
            "We're trying to figure out, you know, should we use this example because it's it's not a very good indication of a well separated diagram, but it turns out that you know it wasn't our method, it was the data set that actually has this property.",
            "What I had done was geometrically uniformly sample along the rings, or rather so, each ring had the same number of samples, which means that the larger rings had many, had a lot fewer samples than the inner ring, and so the inner rings were much more prominent.",
            "And you know the inner part was much more prominent than the outer part, and that's why this this last outer cluster, which was which corresponds to this outer ring, is so not prominent because it's actually not very dense.",
            "And in this case, here is just a histogram of these points and you see this is the gap that we chose."
        ],
        [
            "If we decrease the.",
            "Decrease the rips parameter.",
            "The gap actually grows, which is shown here, and you know, here.",
            "This really just illustrates that there is this correspondence of that.",
            "You want to find the smallest.",
            "Rips parameter or kind of the smallest edge lengths.",
            "To get the best approximation while getting as much of your space as possible, because if you decrease it a little bit too much then then all of these, you know all of these components are going to start coming this way, and so you're going to end up with just annoys.",
            "A lot of small component."
        ],
        [
            "That don't mean anything OK, so now for the real data example.",
            "So is comes from biology.",
            "So what biologists do is they look at protein confirmations."
        ],
        [
            "So they don't want to do that many experiments.",
            "So what they do is they have simulations and these are physical simulations where they actually simulates the forces and everything and.",
            "You know they they can only do this for very short periods of time picoseconds.",
            "But the interest the biological phenomena they want to see in protein confirmations is much longer.",
            "It's on the order of milliseconds.",
            "So the problem is now they have all these confirmations and you know they have these short trajectories, but they don't.",
            "They want to find some longer term behave."
        ],
        [
            "So.",
            "You know what they want, what they've noticed, though, is that what happens?",
            "You know, the protein confirmation doesn't just kind of randomly go everywhere.",
            "It has several kind of fuzzy stable States and then it jumps from one stable state to another and then stays in the stable state for a long time.",
            "So these are actually called meta."
        ],
        [
            "Stable states.",
            "And the example I'm going to show you was produced by Vijay Pandas Group at Stanford and is the L9 dipeptide molecule which don't ask me what it does.",
            "But we did have 200,000 confirmations in R22 so it's in 22 dimensional space.",
            "It has a non Euclidean metric.",
            "You have to do for any two confirmations.",
            "You have to find the best rigid transform and then compute the root mean squared distance.",
            "So it's very non Euclidean.",
            "It's very high dimensional."
        ],
        [
            "But biologists have spent a long time studying this, so they know what the correct projection of this is.",
            "So this they know that this actual molecule only has 2 degrees of freedom, and this is called a Ramachandran plot, and it's these are two angles of the confirmation, and these are the 200,000.",
            "Individual confirmations mapped into this, so you know from the biologists we know this is the right answer, and so we ran this on our with our algorithm and this is the persistence diagram we got.",
            "22 dimensional points projected to two.",
            "So buy some buy something that they've figured out through the biological mechanisms that this has to be.",
            "It's map down into this, yeah, so they're not the same.",
            "The point the distances here are not the same as the distances in the actual confirmation.",
            "Yeah."
        ],
        [
            "The actual distances are.",
            "Yeah.",
            "No, no, no.",
            "So so for each two so so this.",
            "So when we computed this first we spent, you know I think half a day just computing some distances because you need to look at every two points.",
            "Find the best transform and then compute the distance and so you end as far as I know, there's no faster way to do it so."
        ],
        [
            "Yeah.",
            "So this was the persistence diagram we got, and it doesn't look too promising because it's, I mean there's no really well separated structure, but."
        ],
        [
            "And we when we actually cluster, we see there's I'll show you two potential clusterings.",
            "One is with six clusters and one is with seven, and biologists have spent a lot of time trying to get these six clusters.",
            "And you know, there's at least two or three papers written on just getting this kind of clustering.",
            "And we actually got it without too much trouble.",
            "Usually what ends up happening is because these states are much less frequent than these states.",
            "This all ends up being merged into one cluster, so they usually get five or four clusters rather than the six that they're actually looking for.",
            "But you know, we said, well, you know why not seven clusters?",
            "I mean, the persistence diagram tells us that it's I mean it's just as reasonable as six from the."
        ],
        [
            "Austin's point of view, and so you know, because if we map this here, we see there's kind of four clusters that are, you know, roughly on the order of 10 to 3, then we have kind of another two that are still pretty high, and then you know three that are noise and.",
            "The actual metric that the biologists use to measure how good their clustering is is something called metastability.",
            "And so I won't go into what the actual definition of it is, but basically it measures how within these trajectories, how often do you stay within within one cluster.",
            "So the idea is that if you want to what you want to do is you want to maximize the number of clusters, while minimizing how often they jump between clusters, and so you know we took our clustering and we compute it and we saw that you know OK, actually there is no good reason for having six clusters versus 7.",
            "Even based on their own metric of what it is, and from our point of view, this just makes this makes sense, because from the persistence diagram we don't really know what the right number of clusters is, but there are experts in the field where the data comes from.",
            "The can say OK, well, we can tell them, you know we can have 456 or seven clusters and it all makes sense.",
            "Now you have to tell me what.",
            "You have to give me some additional information so that I can decide what the true number of clusters is.",
            "In this case it's the metastability and then you can decide that you actually want seven clusters because.",
            "Let's say for this initial 6 clusters, while they like 6.",
            "This was because really actually."
        ],
        [
            "So the gold standard was a biologist, went and looked at this.",
            "Andrew lines this way and said these should be the six clusters that we're going to that we want.",
            "No no no."
        ],
        [
            "Stop.",
            "Yep, so OK.",
            "The take home message of this talk really is that you know what I've shown you is kind of a very generic practical algorithm, and for us it's kind of important to make it as general as possible, so it's nice that we can.",
            "Most of what I've said today works in any metric space, so we don't really need a lot of geometry.",
            "We don't need the manifold, we don't need Euclidean space.",
            "We can really just work with metric spaces, or even in principle.",
            "We could even just work with similarities.",
            "It's very generic, you know we're not experts in density estimation or.",
            "You know like choosing graph, so you can choose your favorite and just run the algorithm with it.",
            "It's completely agnostic to."
        ],
        [
            "Their choice.",
            "But the point really is that I think you know, for whatever choices you make, the persistence diagram really is the structure of the function in the space.",
            "In some sense, it really is, you know, given what your estimate, what you end up estimating and these things.",
            "This really is what you're looking at.",
            "There is no kind of additional artifacts or anything like this if you know if you don't like what your what, your persistence diagram looks like, you need to change.",
            "If some assumptions, maybe a different graph, a different estimator.",
            "Perhaps you know?",
            "Look over, perhaps, maybe it's not really a smooth density function that this is sampled from something like."
        ],
        [
            "This.",
            "And you know the nice part for us is that it does come with theoretical guarantees.",
            "We have the number of clusters we have, the spatial stability, and it opens a lot of other in."
        ],
        [
            "Listing questions, so I'm just going to go over some.",
            "Very quickly over some current work that we're doing so you know, I said if there's a gap, what we can do is we can perturb our function so we can introduce noise and what we can do is well, then we can kind of start to detect boundaries and we can start to do kind of soft clustering.",
            "We can say, OK, well, a point may be unstable, but let's give it a probability that it goes to this cluster of that cluster.",
            "So you know this is preliminary work, but it."
        ],
        [
            "Seems to work very well.",
            "Other interesting question is how do we use higher dimensional features?",
            "So I've mentioned that we only use the connected components, but all the topological persistence works in arbitrary dimension, so you know we not only get the number of components, but we can figure out that this is a crater we can figure out that this is a Taurus, you know, but the question is for clustering, how can we actually use this or what?",
            "What can this actually tell us for real machine learning problems, is I mean?",
            "Is this information useful at all?"
        ],
        [
            "And the other thing is, as I mentioned, we are not restricted to density functions.",
            "We can use arbitrary functions, and right now I'm working with.",
            "With someone also at Stanford on doing segmentation using a particular function and the results are actually quite promising as well."
        ],
        [
            "OK, and the code if you're interested in trying it, is available here and are there any questions?",
            "Yes, so so for the clustering.",
            "You can use the Union find algorithm which is in the size of the graph.",
            "But for higher dimensions you need to use the full homology computation which is cubed.",
            "Roughly.",
            "To, well, that's to reduce the.",
            "It is.",
            "So the difference between this and discrete Morse theory is that here we're actually we're not requiring our samples to be on the space, it just has to be close to the space.",
            "So in some sense, this is a little more general.",
            "But again, I think the result is the same even in discrete Morse theory that you're not going to be able to get any.",
            "You're in some sense, your choice of gradients in the discrete case is arbitrary, and I might have forgotten to mention it, but the persistence diagram.",
            "It doesn't matter what gradient you choose, the persistence diagram is actually the same, so.",
            "So you know the problem is, once you actually have to choose the right gradient, estimating the gradient is a much more difficult problem than just estimating the function.",
            "So where we can we try to avoid it?",
            "Persistent diagram.",
            "The answer is true.",
            "This is a very fine way to visualize the structure function, yes.",
            "Well, the information is important.",
            "It seems that if you very different parameters.",
            "Diagram can change quite a bit, yes.",
            "You can.",
            "Well, you know.",
            "So the point is that you can.",
            "There's a stable part in an unstable part, and actually you can track this if your changes are small enough you can.",
            "I mean, you can in some sense stack them and they're going to change continuously like each point will actually move continuously, so.",
            "You know one other thing we're trying to define now is say that you have all these different things.",
            "Can we actually define a distribution on these persistence diagrams and in practice, yes we can.",
            "But it's not clear from.",
            "I mean, there's kind of several theoretical questions that are very tricky to answer in terms of you know what are we actually converging to.",
            "What is is there one distribution function that?",
            "We're looking at or because you know, for example, if we look at the space so we have some estimated function and we look at the space of all close functions we can't.",
            "We can't define a uniform sampling on this space because it's infinite dimensional, so you know, you know we can certainly perturb things and see what we get, but theoretically we're not really sure what this means so.",
            "Yeah, that's that's kinda work.",
            "Any idea how?",
            "Complicated would be to make this argument incremental, so local changes in data.",
            "Local local changes in the clustering.",
            "Yeah, you can do this so you can actually make this dynamic.",
            "It's so you know if you are to change things.",
            "So with the graph, it's a little more complicated, so if the function is just change, it's very easy.",
            "If the actual graph changes, you can also do it, but it's a little more tricky like it can be generalized, though I think.",
            "This would be.",
            "Step function banning the road.",
            "Yeah, it's but the theoretical tools you need are a little more involved.",
            "So yeah.",
            "So I mean for the graph it doesn't matter.",
            "Yeah no, no.",
            "Yeah, so so as long as so the nice thing is for the for the Euclidean cases, we just use the KD tree to compute distance.",
            "So you have.",
            "I mean it's each distance query is log time and each.",
            "And the space you need is constant.",
            "Essentially so, but yeah, I mean, you still are constrained by kind of being able to find these neighborhoods.",
            "So if you have to look over your entire data set.",
            "Yeah yeah, yeah.",
            "Function.",
            "Yeah, so it.",
            "Well it's it's not implemented yet, but we do have.",
            "I do have the theoretical results.",
            "Let's say that it works and.",
            "But yeah, it's it's in the process of I don't have any nice pictures to show or a movie.",
            "So I decided to skip it.",
            "Yeah.",
            "Topological mode analysis tool.",
            "So far.",
            "And image segmentation.",
            "Yeah, so I thought yeah those are the things we've tried.",
            "Send yeah, there's there's been a few other things but nothing.",
            "Nothing really extends no extensive testing, so.",
            "OK. How do I say thanks?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "People.",
                    "label": 0
                },
                {
                    "sent": "Pretty much my.",
                    "label": 0
                },
                {
                    "sent": "So that's what you got now.",
                    "label": 0
                },
                {
                    "sent": "In Eclipse is that?",
                    "label": 0
                },
                {
                    "sent": "Vanished.",
                    "label": 0
                },
                {
                    "sent": "In today.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And in a post in Stanford.",
                    "label": 0
                },
                {
                    "sent": "Stanford.",
                    "label": 0
                },
                {
                    "sent": "Medical computer science in.",
                    "label": 0
                },
                {
                    "sent": "Interview parisa I don't want to be sitting here.",
                    "label": 0
                },
                {
                    "sent": "Natural.",
                    "label": 0
                },
                {
                    "sent": "Well done.",
                    "label": 0
                },
                {
                    "sent": "The paper towel timoci that created a moment about him with double dealer.",
                    "label": 0
                },
                {
                    "sent": "In OK.",
                    "label": 0
                },
                {
                    "sent": "I've got chairs or somebody will be broad based analogy to damage or English unit on but her sister Connie Garica some Thursday, Dow subliminal Manglish.",
                    "label": 0
                },
                {
                    "sent": "So the this is on persistence based clustering, which is current work with Frederick Chazal and Steve would also add geometrica at INRIA and my PhD advisor Leonidas.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes so.",
                    "label": 0
                },
                {
                    "sent": "Clustering, I'm sure everyone in this room knows a lot more about clustering than I do, but the main idea here is that we have some input samples, a point cloud, some unstructured data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we want to do is we want to find some quote, unquote important segments or clusters without any user intervention or with as little user intervention as possible.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is obviously a nil post problem because.",
                    "label": 0
                },
                {
                    "sent": "Theoretically, we have absolutely no idea what important actually means.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, OK, there's this is kind of one of the classic problems in machine learning and artificial intelligence, and there is really entirely too much work to kind of go over what's been done.",
                    "label": 0
                },
                {
                    "sent": "But three methods that I'll mention before, hands that are very popular and applications are K means clustering, spectral clustering and mode shift, or mean shift clustering, and I'll really leave it just at this.",
                    "label": 0
                },
                {
                    "sent": "But as I go through the talk, I'll try to re late.",
                    "label": 0
                },
                {
                    "sent": "What we do to these?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other methods?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We come from a much very theoretical background, so we'd like to formalize things in our viewpoints on the clustering problem.",
                    "label": 0
                },
                {
                    "sent": "Is really these data points?",
                    "label": 1
                },
                {
                    "sent": "We're going to assume are some random samples that come from some unknown density distribution, so we don't know the density distribution.",
                    "label": 1
                },
                {
                    "sent": "In practice, we won't even know in what space these points live in.",
                    "label": 0
                },
                {
                    "sent": "We're just going to have some approximation of the space and some approximation of the density function.",
                    "label": 0
                },
                {
                    "sent": "And we're going to see what we.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually find out from this.",
                    "label": 0
                },
                {
                    "sent": "So what's the intuitive definition of a cluster?",
                    "label": 1
                },
                {
                    "sent": "Well, suppose we have a 1 dimensional density function like this kind of.",
                    "label": 0
                },
                {
                    "sent": "A natural definition is to say that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are looking for the basins of the peaks of F, so we have our two peaks and we want to say if we follow the gradients at which peak do we end up so kind of a very nice simple.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Definition, but you know, in the best world we would actually have access to the function to the underlying function and the underlying space, but usually we're only going to have finite samples, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question will be can we approximate these basins of attraction from the samples?",
                    "label": 0
                },
                {
                    "sent": "To what extent can we approximate them and what other information can we?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correct, OK, so switching now to topology, because really what this talk is about is applying topological tools to clustering.",
                    "label": 0
                },
                {
                    "sent": "So topology is the study of how spaces are connected and it deals with the classification of spaces.",
                    "label": 1
                },
                {
                    "sent": "So one of my other coworkers, Vin de Silva, likes to say that apology is the branch of mathematics that can tell a donut from a coffee mug and the reason is is because each of them has one hole, and so in the eyes of topology they're exactly the same.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Uh, so.",
                    "label": 0
                },
                {
                    "sent": "In math, there's been a topology goes back 100 hundred and 50 years, and there's a lot of work done, but we deal primarily more computational topology, which is we're trying to find qualitative.",
                    "label": 0
                },
                {
                    "sent": "But rigorous descriptions for large datasets so.",
                    "label": 0
                },
                {
                    "sent": "My background is in geometry where we have some descriptions for shapes and things like this, but these things aren't very robust in the sense that you can change geometry a little bit and.",
                    "label": 0
                },
                {
                    "sent": "All the descriptions that you want to say about them will change a lot.",
                    "label": 0
                },
                {
                    "sent": "And also things need to be very precise with geometry, whereas with computational topology our only assumption is that we know local neighborhood information, so we don't really know what the entire thing looks like.",
                    "label": 1
                },
                {
                    "sent": "But given a point we can say roughly what our neighbor, what's in our neighborhood.",
                    "label": 0
                },
                {
                    "sent": "So we really only have kinda.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proximity information and today I'll be talking about homology, which is 1 very specific invariant that's used in topology.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What it says is basically the idea is that if two things have the same homology, then they may or may not be the same shape, but if they have different Hama.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Geez, we know for certain that they're not the same shape, so in this case if we have a donut.",
                    "label": 0
                },
                {
                    "sent": "Empty or a Taurus rather than the sphere.",
                    "label": 0
                },
                {
                    "sent": "We know that they're not the same because their homology.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are different and at least in low dimension, homology has kind of a nice geometric interpretation in the zero dimension, the homology, the of the dimension of homology is really just the connected components of your space.",
                    "label": 0
                },
                {
                    "sent": "In one dimension it's the number of holes that your space has, and in two dimension it's the number of enclosed spaces or voids so.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The formalisms and the machinery behind.",
                    "label": 0
                },
                {
                    "sent": "Really quite complicated, but today we're only going to be dealing with 0 dimensional homology, which is the connected components, which is, I think, much.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simpler to understand.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're not just going to be dealing with homology, but with persistent technology, which is an extension that goes back from about the year 2000.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to look at is, given some function, we're going to look at the evolution of the topology of this super level sets so.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does this mean?",
                    "label": 0
                },
                {
                    "sent": "This means that we're going to take we're going to have a level, and we're going to start sweeping down and we're going to look at the intersection of this space and our function, and here when we hit this maximum point, we're going to get one connected component here and then.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We move this.",
                    "label": 0
                },
                {
                    "sent": "We move this down.",
                    "label": 0
                },
                {
                    "sent": "This space we look at the intersection.",
                    "label": 0
                },
                {
                    "sent": "It's grows.",
                    "label": 0
                },
                {
                    "sent": "This one is born again.",
                    "label": 0
                },
                {
                    "sent": "So now we have two connected component.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then as we move down this merge.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chiz",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this continues until we get the entire space.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just as a.",
                    "label": 0
                },
                {
                    "sent": "Bit of a tangent is for the vocabulary so that we're on the same page.",
                    "label": 0
                },
                {
                    "sent": "Is this sequence of super level sets is called a filtration and all this filtration means is that for.",
                    "label": 0
                },
                {
                    "sent": "It's parameterized by some.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By some parameter here Alpha, but it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It could be really any parameter it could be in.",
                    "label": 0
                },
                {
                    "sent": "The natural number is anything, but it's all it is.",
                    "label": 0
                },
                {
                    "sent": "Is this increasing sequence of spaces?",
                    "label": 1
                },
                {
                    "sent": "So for any Alpha bigger than beta in this case, since we're going down since our parameters decreasing each space here is bigger than any space here.",
                    "label": 0
                },
                {
                    "sent": "So that's the only requirement, and this is called a filtration.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so since we're going to be dealing with density functions, we were going to assume our density function doesn't actually go to Infinity, so we can cross that off.",
                    "label": 0
                },
                {
                    "sent": "And we know that our density function is non negative, so we can kind of ignore everything on the right here and so we have a finite filtration which is which goes from some maximum thing to 0.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and so if you remember on the previous slides we had these components when new components were born.",
                    "label": 0
                },
                {
                    "sent": "We call that the birth time and when they merge we call that the death time and so in the filtration, there's a point where component appears in a time when it gets merged.",
                    "label": 0
                },
                {
                    "sent": "And I'll explain this a little more.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a second, but just to say that you know the study of filtrations is.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work in math on this and two papers if you're interested into how this relates to persistence is actually the original persistence paper and the follow up which are.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Listed here.",
                    "label": 0
                },
                {
                    "sent": "So going back to this so OK, now we're tracking the.",
                    "label": 0
                },
                {
                    "sent": "Evolution of this apology so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we go down soon before we add this this space, everything here is included here.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This down here, but what we get by tracking these birth and death times is a bar code or rather a bar.",
                    "label": 0
                },
                {
                    "sent": "So this the beginning of the bar is when the component.",
                    "label": 0
                },
                {
                    "sent": "Big came into the space and this is when it became merged and OK, what what's why?",
                    "label": 0
                },
                {
                    "sent": "Why is this good at all?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um well.",
                    "label": 0
                },
                {
                    "sent": "There's a proof proved by coercion, or else we're not in her that if our if our function is close, so say that we have some smooth underlying function, or actually any underlying function that's close to the thing that we're measuring.",
                    "label": 0
                },
                {
                    "sent": "The bar codes will be close as well.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this seems promising.",
                    "label": 0
                },
                {
                    "sent": "So now what we're going to do is we're just going to say the topological definition of a cluster while we have our basins of attraction again, and are so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Peoples butt what we're going to do is we're going to say OK, Now we know these are clusters, but these barcodes are really just the signature of these of these of these peaks, so prominent peaks are going to correspond to prominent, persistent, connected components.",
                    "label": 0
                },
                {
                    "sent": "So if a peak is big enough, we're going to see a long bar because.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We know that these things are stable.",
                    "label": 0
                },
                {
                    "sent": "OK, and so I've been saying that they're close in their stable and I wanted to just say a few words about what this actually means.",
                    "label": 0
                },
                {
                    "sent": "So say we've computed some barcodes by an algorithm that all.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about later, but you know we do.",
                    "label": 0
                },
                {
                    "sent": "We do it on one space.",
                    "label": 0
                },
                {
                    "sent": "We do it on another so we have the blue and the red and now we.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To compare these.",
                    "label": 0
                },
                {
                    "sent": "Well, the first thing we're going to do is we're going to map this to R2, so just the birth time is going to be mapped into the into the Y axis and the the sorry.",
                    "label": 0
                },
                {
                    "sent": "The birth time will be mapped to the X axis and the death time will be mapped to the Y axis.",
                    "label": 0
                },
                {
                    "sent": "So now we have a point now.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why do we do this?",
                    "label": 0
                },
                {
                    "sent": "Well, OK, we if we do this for all the bars we get a bunch of blue and red points.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's great, but now what we're going to do is we.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Superimpose the two.",
                    "label": 0
                },
                {
                    "sent": "And we find a matching a perfect matching.",
                    "label": 0
                },
                {
                    "sent": "Where were allowed to match to the diagonal as well.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the distance we define is called the bottleneck distance and all it says is that over all possible bijections between points blue and red points, we take the the minimum maximum distance.",
                    "label": 0
                },
                {
                    "sent": "So it's the Infinity norm and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once we have this distance, well.",
                    "label": 0
                },
                {
                    "sent": "What we can say is that.",
                    "label": 0
                },
                {
                    "sent": "Under function perturbations under kind of distances in the space, and I'll give a few more precise examples later.",
                    "label": 0
                },
                {
                    "sent": "We can say that this bottleneck distance remains small if our perturbations are small.",
                    "label": 1
                },
                {
                    "sent": "So the point really is that if we have something, we change it a little bit.",
                    "label": 0
                },
                {
                    "sent": "These points really theoretically cannot.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move very far.",
                    "label": 0
                },
                {
                    "sent": "OK, so these.",
                    "label": 0
                },
                {
                    "sent": "Barcodes and persistence diagrams seem kind of strange the first time you see them, but.",
                    "label": 0
                },
                {
                    "sent": "This kind of concept is well known in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Dendrograms are actually a form of barcodes and pro.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distance diagrams, and.",
                    "label": 0
                },
                {
                    "sent": "Specifically, if you think about single linkage clustering, all it really is is a distance filtration and I'll explain why.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean by that.",
                    "label": 0
                },
                {
                    "sent": "So say we have these points and where the distance is their distance in the X axis.",
                    "label": 0
                },
                {
                    "sent": "And we get this dendrogram but just buy some.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Linkage clustering, what we're doing is really we're drawing circles around each point, and when there's an intersection, we connect it.",
                    "label": 0
                },
                {
                    "sent": "This is exactly just a filtration where we connect things based on how far they are, which we usually call a distance.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operation, and so we continue this and then this point gets connected to this component.",
                    "label": 0
                },
                {
                    "sent": "So we connect.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edit and we do this.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then until we've connected everything.",
                    "label": 0
                },
                {
                    "sent": "And so our barcode really is just this collection of vertical pieces.",
                    "label": 0
                },
                {
                    "sent": "This is exactly a barcode and barcode has exactly the same information as a person.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since diagram OK, so a natural question is is well, you know why are we throwing away this information here?",
                    "label": 0
                },
                {
                    "sent": "Because obviously this has more information than just the persistence diagram, because here we know how we're actually merging rather than just how long.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Survive.",
                    "label": 0
                },
                {
                    "sent": "And the question in the answer is well.",
                    "label": 0
                },
                {
                    "sent": "We know that the lengths are stable and this is actually known in single linkage clustering as well, but it's also known in single linkage clustering that the connections themselves are.",
                    "label": 0
                },
                {
                    "sent": "In general, not stable.",
                    "label": 0
                },
                {
                    "sent": "And actually, as it turns out, even over the filtration that we define, which is based on the density function, it's it's not necessarily stable, and in general you really can't hope for any kind of real stability here.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of the connections, OK, so now.",
                    "label": 1
                },
                {
                    "sent": "We're getting actually to the algorithm so.",
                    "label": 0
                },
                {
                    "sent": "I've talked about persistence diagrams and topology and all these things, but where does the where do the actual clusters come in?",
                    "label": 0
                },
                {
                    "sent": "Well, it's.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quite simple.",
                    "label": 0
                },
                {
                    "sent": "So we start off with some space here so it's you have 4 four peaks here, one peak here in a circle like a little crater here and.",
                    "label": 0
                },
                {
                    "sent": "And two.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pics down there.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do is.",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to build some graph on them, and we're going to find some density estimates, and we're just going to do mode seeking, which means from each point we're just going to follow the gradient along the graph until we get.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stock and what we're going to get is a bunch of clusters that don't really look like they make any sense whatsoever.",
                    "label": 0
                },
                {
                    "sent": "But then.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because we know that the long bars correspond to that.",
                    "label": 0
                },
                {
                    "sent": "Things that persist for a long time are going to last.",
                    "label": 0
                },
                {
                    "sent": "We're going to use this information to merge these small clusters together so that we end up with something that makes sense.",
                    "label": 0
                },
                {
                    "sent": "So here we get the five peaks and then this ring here.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so what's the algorithm?",
                    "label": 0
                },
                {
                    "sent": "Our input is a point set with the function value, some graph and a parameter Alpha.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk about.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so it's very simple.",
                    "label": 0
                },
                {
                    "sent": "We sort according to F in decreasing order of F because we want the peaks of our density function and for each point we look at our neighbors at the neighborhood in the graph.",
                    "label": 0
                },
                {
                    "sent": "And if it's if there's no higher neighbour, then it's a new cluster.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we assign it to some pseudo gradient.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter what.",
                    "label": 0
                },
                {
                    "sent": "And then if this point is adjacent to more clusters we look at.",
                    "label": 0
                },
                {
                    "sent": "If these clusters are more than Alpha persistent, and if they're not, we merge it into the oldest adjacent cluster.",
                    "label": 0
                },
                {
                    "sent": "It's very simple, it's very efficient.",
                    "label": 0
                },
                {
                    "sent": "You can implement this using the Union find algorithm, so it runs in.",
                    "label": 0
                },
                {
                    "sent": "Roughly, the size of your graph.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Linear in the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Putting this all together, now we have all of the tools that we really need, so we estimate our density and the first thing we do is we run, run the algorithm with Alpha equals to Infinity.",
                    "label": 0
                },
                {
                    "sent": "And why do why do we do this?",
                    "label": 0
                },
                {
                    "sent": "This is really the standard persistence algorithm, which is how we end up, which is how we compute our persistence diagram.",
                    "label": 0
                },
                {
                    "sent": "So in this case, what you see is.",
                    "label": 0
                },
                {
                    "sent": "You know we have.",
                    "label": 0
                },
                {
                    "sent": "These are the birth times.",
                    "label": 0
                },
                {
                    "sent": "These are the death times and we see we have 123456 prominent clusters and a bunch of noise.",
                    "label": 0
                },
                {
                    "sent": "So you know we can kind of see a gap here, so we're going to pick our Alpha anywhere in between here and then run the algorithm again an we end up.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Star clusters so if we if we didn't do anything.",
                    "label": 0
                },
                {
                    "sent": "So if we picked Alpha equal to 0, this is exactly what we would get.",
                    "label": 0
                },
                {
                    "sent": "Picking Alpha a little bit bigger, we end up with.",
                    "label": 0
                },
                {
                    "sent": "This nice this nice answer.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I've talked a lot about graphs and parameters, but there's a few kind of graphs that are standard choices that we can actually say something theoretically about them, so the need for a graph is really because we need this notion of a neighborhood, because as I said, topology really just depends on.",
                    "label": 0
                },
                {
                    "sent": "This localnet idea of.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Local neighborhood OK so.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are many choices for graphs, but.",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk about 3.",
                    "label": 0
                },
                {
                    "sent": "The first is the Vietoris graph, or also known as just the Delta neighborhood Graph, where the our neighborhood graph it's all.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It really says is that.",
                    "label": 0
                },
                {
                    "sent": "If for some parameter Delta if too.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Points are within that Delta.",
                    "label": 0
                },
                {
                    "sent": "We connect them with an edge and if they are further away, we don't.",
                    "label": 0
                },
                {
                    "sent": "This is very.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the simplest graph.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we can build, it's has it's very simple to compute.",
                    "label": 0
                },
                {
                    "sent": "We just need pairwise distances.",
                    "label": 0
                },
                {
                    "sent": "We can compute in any metric space.",
                    "label": 0
                },
                {
                    "sent": "And theoretically, it's also very nice because we can say a lot about what we end up with.",
                    "label": 0
                },
                {
                    "sent": "In the end, we can actually have provable reconstruction on quite general spaces, and you know, theoretically it's very nice to work with.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another very common thing to use is the K nearest neighbor graph, which is you know you have some points and you basically just.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Connect the K nearest neighbors and you do this everywhere so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is nice because it's also simple to compute.",
                    "label": 0
                },
                {
                    "sent": "Also requires pairwise distances can be computed in any metric space, but it's also sparse.",
                    "label": 0
                },
                {
                    "sent": "The Vietoris rips graph can be very very large for large values of Delta, which can slow things down a lot, but this is sparse theoretically it's not quite as nice to work with.",
                    "label": 0
                },
                {
                    "sent": "We can say a few things about it, but.",
                    "label": 0
                },
                {
                    "sent": "Because it's not the geom.",
                    "label": 0
                },
                {
                    "sent": "It's not quite as obviously a geometric graph.",
                    "label": 0
                },
                {
                    "sent": "It's a little more you have to work a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To say something concrete about it.",
                    "label": 0
                },
                {
                    "sent": "And finally, there's the Delaunay graph which is given some points we simply construct.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any edge that's Adele on edge so I didn't draw the circles, but if.",
                    "label": 0
                },
                {
                    "sent": "The Delani graph is basically all the edges that are in a Delaunay triangulation, which is defined in any dimension, and basically it says that for any circum circle, any for any three points where the triangle is in there, the circum circle is empty.",
                    "label": 0
                },
                {
                    "sent": "So this is geometric processing.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "101 but.",
                    "label": 0
                },
                {
                    "sent": "Then really nice properties of these things is that it is sparse.",
                    "label": 0
                },
                {
                    "sent": "There is very fast computation if we're in low dimensional space, but we need in some sense to be in a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "And it's Canonical so.",
                    "label": 0
                },
                {
                    "sent": "The other two, the Vietoris rips and the K nearest neighbor graph, both require parameter either the size of your neighborhood, the number of neighbors.",
                    "label": 0
                },
                {
                    "sent": "This is Canonical.",
                    "label": 0
                },
                {
                    "sent": "There is for any inputs set of points.",
                    "label": 0
                },
                {
                    "sent": "There's only one possible graph, so when we can build it, it is very nice because it's one less thing we have to experiment across 2.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get some answers OK, now density estimation, I'm just going to go over very quickly.",
                    "label": 0
                },
                {
                    "sent": "It's a huge field of study.",
                    "label": 0
                },
                {
                    "sent": "There's books written on it.",
                    "label": 0
                },
                {
                    "sent": "There's thousands of papers.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But where we only really use two kind of density estimators because?",
                    "label": 0
                },
                {
                    "sent": "I the two most common ones because this is really not our area of expertise.",
                    "label": 0
                },
                {
                    "sent": "First is the Gaussian kernel which takes some bandwidth parameter H, which again is kind of a smoothing parameter.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whereas the other one we used was this distance to measure an.",
                    "label": 0
                },
                {
                    "sent": "I'm afraid I forgot the reference to this, but it was it's work by Frederick shizzle, David Koechner and can't on Merry go at all from INRIA.",
                    "label": 0
                },
                {
                    "sent": "And so all it says is that it's the root mean squared distance.",
                    "label": 0
                },
                {
                    "sent": "To the K nearest neighbors, and they proved there's some very nice stability results about this and it's smooth and it in practice it works very, very nicely.",
                    "label": 0
                },
                {
                    "sent": "So these are the two density estimators we use.",
                    "label": 0
                },
                {
                    "sent": "There's numerous other ones you can use, and so you can really just pick your favorite estimator.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One requirement that we do have is that your that your estimated function value and your true function value.",
                    "label": 0
                },
                {
                    "sent": "You have to be close in the Infinity Norm, so you can't allow single points to be very far off.",
                    "label": 0
                },
                {
                    "sent": "Now this is not ideal because often we have outliers and want some of our current work is now how to deal with that but.",
                    "label": 0
                },
                {
                    "sent": "Actually, in a lot of cases, like for the Gaussian kernel or in these two cases, you can actually show that you know with sufficient sampling you can actually get this condition, because these things converge fast enough that you know with high probability you're not going to get any outliers.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now I wanted to go over some specific guarantee that's, you know.",
                    "label": 0
                },
                {
                    "sent": "I've talked about the algorithm and showed you one toy example that works.",
                    "label": 0
                },
                {
                    "sent": "But you know what's an?",
                    "label": 0
                },
                {
                    "sent": "I've said we can prove a lot of results, theoretical results, but I wanted to give you one example, one concrete example.",
                    "label": 0
                },
                {
                    "sent": "So say we are dealing with a manifold and our function is C Lipschitz.",
                    "label": 0
                },
                {
                    "sent": "If we have an epsilon cover, which means that any points on the manifold is within epsilon of a sample point.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, well on a manifold we can measure distances so we can build a rips graph.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we build the RIPS graph and the guarantee says that.",
                    "label": 0
                },
                {
                    "sent": "So in the soda paper my soda paper from 2009 we showed that this RIPS filtration and the Super level set filtration are close.",
                    "label": 0
                },
                {
                    "sent": "Now in the literature we call this interleaved and I didn't really want to get into this because there's quite a lot of machinery to go over, but it's interleaved with a parameter that depends on the Lipschitz constant of the function and.",
                    "label": 0
                },
                {
                    "sent": "Delta, which is the parameter of our of our rips graph and the only requirement is that Delta has to be bigger than our sampling, so you know if any point on the spaces within epsilon the smallest rips graph we can build so that this works is that epsilon and by this paper at sausage.",
                    "label": 1
                },
                {
                    "sent": "Because these two are close in the sense of interleaved their persistence.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Grams will be close to and what this means is that if we have a sufficient sampling.",
                    "label": 0
                },
                {
                    "sent": "And good estimates of our function values.",
                    "label": 0
                },
                {
                    "sent": "We will get a good approximation of the persistence diagram.",
                    "label": 0
                },
                {
                    "sent": "Now this works for uniform sampling and it's works for any function and pretty much in any space.",
                    "label": 0
                },
                {
                    "sent": "I believe any meta.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "OK, but I've mentioned you know we uniform sampling epsilon covers, but we're sampling from some distribution, so our sampling isn't going to be uniform.",
                    "label": 0
                },
                {
                    "sent": "And our approximation dip.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ends on this C Delta.",
                    "label": 0
                },
                {
                    "sent": "So what does this actually mean?",
                    "label": 0
                },
                {
                    "sent": "So so we have some Gaussian type of thing and we sample, so we get more points near the center than the outside.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know what we need to do is we need to find well sampled regions and.",
                    "label": 0
                },
                {
                    "sent": "We know that our approximation results holds there.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's it's a tradeoff because when we're building the rips parameter, we have this scale that we can choose Delta.",
                    "label": 0
                },
                {
                    "sent": "So if we choose small Delta, our approximation will be good for a large Delta.",
                    "label": 0
                },
                {
                    "sent": "It's the approximation will be less good, but we're going to get more of.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space, so in this case say we choose a very small Delta, so we draw small balls around everything.",
                    "label": 0
                },
                {
                    "sent": "So we're really well sampled only in this super level set.",
                    "label": 0
                },
                {
                    "sent": "So we have a good approximation here and here below it.",
                    "label": 0
                },
                {
                    "sent": "We can only say a few things.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whereas if we increase Delta well now we cover more of this space, but our persistence diagram guarantee goes down.",
                    "label": 0
                },
                {
                    "sent": "So you know it's a tradeoff.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now to actually say some things as well.",
                    "label": 0
                },
                {
                    "sent": "OK, we know our persistence diagram is roughly right, but now we can define a signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "So what we say is we're going to require our persistence diagram to have this structure, so things will either need to be quite far from the diagonal or close the diagonal and some gap in between.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formally.",
                    "label": 0
                },
                {
                    "sent": "Oh, I guess I deleted that but OK.",
                    "label": 0
                },
                {
                    "sent": "So if we have this structure what we can say is that while the number of clusters is stable, because we know that the points can't move around too much, we know that a point from here can't go into here and vice versa.",
                    "label": 0
                },
                {
                    "sent": "And the proof of it is it's a little tedious, but it's quite geometric.",
                    "label": 0
                },
                {
                    "sent": "Basically you just have to decompose your space into some disjoint regions.",
                    "label": 0
                },
                {
                    "sent": "An look at each region.",
                    "label": 0
                },
                {
                    "sent": "What happens to the points and you can say.",
                    "label": 0
                },
                {
                    "sent": "And in the end you can conclude that the number of points in this area.",
                    "label": 0
                },
                {
                    "sent": "Is stable over all these over any kind of perturbations that satisfy.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Certain criteria.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The upshot of this is if the peaks are prominent enough, we will get the right number of clusters.",
                    "label": 1
                },
                {
                    "sent": "So the number of clusters is stable and Additionally the the more samples you have, the smaller the noise so.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we go back so.",
                    "label": 0
                },
                {
                    "sent": "As we.",
                    "label": 0
                },
                {
                    "sent": "As we increase the number of samples we have this this whole area here stays the same or moves around a little bit.",
                    "label": 0
                },
                {
                    "sent": "But this D1 depends on the number of samples we have, so this part goes towards the diagonal, where in the if we had an infinite number of points.",
                    "label": 1
                },
                {
                    "sent": "Basically we could go arbitrarily close to this diagonal.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so yeah, so this part shrinks as the number of points increases, whereas this part stays the same.",
                    "label": 0
                },
                {
                    "sent": "So in principle if we had enough points.",
                    "label": 0
                },
                {
                    "sent": "Any diagram should be.",
                    "label": 0
                },
                {
                    "sent": "Any diagram should be.",
                    "label": 0
                },
                {
                    "sent": "Well separated.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but you know this is kind of nice theoretically, but we don't have usually don't have an infinite number of points.",
                    "label": 0
                },
                {
                    "sent": "So what does this say practically?",
                    "label": 0
                },
                {
                    "sent": "Well, it does give us a sense of how stable the clusters that we actually get our if we know the number of clusters we're looking for, it's very easy to choose a threshold, you can just you just order your points and you cut somewhere when you reach the number of clusters you want.",
                    "label": 1
                },
                {
                    "sent": "And actually I found during my experiments that it can help with the choice of other parameters to, for example, the scale of our RIPS parameter, the scale of our graph.",
                    "label": 0
                },
                {
                    "sent": "If we choose it too large, what we find happens is that we end up with one connected component that's prominent, and everything else is noise, so that's not very interesting.",
                    "label": 0
                },
                {
                    "sent": "Whereas if we choose it too small, we end up with everything that looks like noise, and so often, you know by running it a few times and looking at the persistence diagram, we can kind of say whether we've hits the sweet spots in terms of these.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other parameters depending on the choices that we've made.",
                    "label": 1
                },
                {
                    "sent": "OK, so the last the last theoretical guarantee before I kind of get to the experiments are spatial stability, so I've talked about the number of clusters, the persistence dog, the persistence diagrams, and you know that we're going to get the right number, but.",
                    "label": 1
                },
                {
                    "sent": "I've said nothing about that these things look like the actual basins of attraction.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You know, under the SC Lipschitz assumption, which basically means that our function isn't too crazy.",
                    "label": 0
                },
                {
                    "sent": "Women in clusters having minimum size.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so under small perturbations, these peaks are always going to be part of the same class.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know we have some clusters and these are the peaks in them.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we perturb things a little bit, and so our actual maximum can move a lot.",
                    "label": 0
                },
                {
                    "sent": "Like think about a flat plane, and if you perturb it just a little bit, your actual Maxima can move arbitrarily far, but we have a guarantee that it will be.",
                    "label": 0
                },
                {
                    "sent": "They will still be clustered together.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "That means that if we have different perturbations or different samplings or anything like that, we have a correspondence between the clusters.",
                    "label": 1
                },
                {
                    "sent": "Assuming this well set.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rated condition.",
                    "label": 0
                },
                {
                    "sent": "OK, now the bad news is that the unstable part, the parts that don't fall into that, can be arbitrarily large.",
                    "label": 0
                },
                {
                    "sent": "So we have this counter example of this.",
                    "label": 0
                },
                {
                    "sent": "These two peaks and there's a little Moat around this peak, and so we have.",
                    "label": 0
                },
                {
                    "sent": "These are the two peaks.",
                    "label": 0
                },
                {
                    "sent": "And then there's another little peak here.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does this look like?",
                    "label": 0
                },
                {
                    "sent": "Well, at one ripps parameter, everything outside here gets merged into here.",
                    "label": 0
                },
                {
                    "sent": "Same thing here, but by shifting the values just a little bit we can make everything go to this into this maximum.",
                    "label": 0
                },
                {
                    "sent": "So basically everything here.",
                    "label": 0
                },
                {
                    "sent": "Everything on the outside here, so this part is stable.",
                    "label": 0
                },
                {
                    "sent": "This part is stable.",
                    "label": 0
                },
                {
                    "sent": "Everything outside here is kind of arbitrary into where it goes.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now for some experimental results, so I'll show you three datasets.",
                    "label": 0
                },
                {
                    "sent": "Two are synthetic, inspire two spirals for rings, and then I'll give a biological exam.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well on simulator.",
                    "label": 0
                },
                {
                    "sent": "So R2D example is these two interlock spirals with.",
                    "label": 0
                },
                {
                    "sent": "So this is about 100,000 points with a very noisy.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Density estimate, but actually to compare with spectral clustering we had to downsample quite a bit, so this is the actual data we use.",
                    "label": 0
                },
                {
                    "sent": "So this is the density.",
                    "label": 0
                },
                {
                    "sent": "The point set with the density and the persistence diagram for the rips parameter and you see you get 1, two prominent ones and then some noise, and so this is before merging and after merging and after merging you see you actually do recover the IT actually does go all the way in for a.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the clusters and for the other.",
                    "label": 0
                },
                {
                    "sent": "The nice thing is is that under the different graphs it's actually quite nice as well, so this is again with the RIPS parameter.",
                    "label": 0
                },
                {
                    "sent": "This is with a.",
                    "label": 0
                },
                {
                    "sent": "With a smaller rips parameter.",
                    "label": 0
                },
                {
                    "sent": "So here we decrease the size, the scale size and what we see is we end up with a lot of prominence components here, but we know that since they appear very late we can actually filter them out.",
                    "label": 0
                },
                {
                    "sent": "And when we do this well we end up with basically everything on the outside.",
                    "label": 0
                },
                {
                    "sent": "That's noise that's been added, gets filtered out because all of this noise corresponds to these small connected components.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and finally yeah, so these are the other two graphs, so this is with the nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "So again I mean the persistence diagrams don't look identical, but the general structure is there and this is with Adele on a graph.",
                    "label": 0
                },
                {
                    "sent": "And here I want to point out, you know, you may notice that the delani graph actually has a lot more points near the diagonal, and it seems like a lot more noise.",
                    "label": 0
                },
                {
                    "sent": "But the reason for this is because the Delaunay graph doesn't really have any sense of scale.",
                    "label": 0
                },
                {
                    "sent": "It fills whatever space you have completely.",
                    "label": 0
                },
                {
                    "sent": "So what we had to do is we had to subdivide each edge and re estimate the density at that edge.",
                    "label": 0
                },
                {
                    "sent": "So in Euclidean space this is quite simple to do because you can just pick some point and estimate the density.",
                    "label": 0
                },
                {
                    "sent": "The same you would at any sample point.",
                    "label": 0
                },
                {
                    "sent": "And if it's small, it's small.",
                    "label": 0
                },
                {
                    "sent": "If it's large it's large, but basically you just do this.",
                    "label": 0
                },
                {
                    "sent": "And you end up with the same the same structure to the.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the persistence diagram.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know we like nice examples that look you know that look interesting.",
                    "label": 0
                },
                {
                    "sent": "So we did, these four interlocked rings.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "So there's one ring here, wondering here.",
                    "label": 0
                },
                {
                    "sent": "And then both of these rings.",
                    "label": 0
                },
                {
                    "sent": "Are are interlocked with it, so it's about 12,000 points.",
                    "label": 0
                },
                {
                    "sent": "I think I went up to again.",
                    "label": 0
                },
                {
                    "sent": "Two or three 100,000.",
                    "label": 0
                },
                {
                    "sent": "But because it's in our.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can't see anything.",
                    "label": 0
                },
                {
                    "sent": "So yeah, using the rips graph or I think this is maybe with the nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "Again, you see the four clusters which.",
                    "label": 0
                },
                {
                    "sent": "Nicely enough correspond to the four rings.",
                    "label": 0
                },
                {
                    "sent": "So that's nice.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know we compared it across the different graphs, and so you know for a small rips parameter most most of the noise gets filtered out, which is shown in black here.",
                    "label": 0
                },
                {
                    "sent": "For the nearest neighbor, obviously everything gets connected so you don't get this filtering property, but we still end up with the same.",
                    "label": 0
                },
                {
                    "sent": "With the same four rings in the same before clusters in the persistence diagram and then the davonte again you see this.",
                    "label": 0
                },
                {
                    "sent": "The large number of points by by the diagonal, and you know the same four rings.",
                    "label": 0
                },
                {
                    "sent": "So the reason we actually chose these two examples is because I spent quite a lot of time trying to get these to work with spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "To my frustration, because what ends up happening is all the uniform noise that's been added.",
                    "label": 0
                },
                {
                    "sent": "Completely kills any spectral clustering method because you end up you end up with no sparse cuts and you really can't do much about it.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Yeah, so here's a finally one example where this was an earlier four rings example, which you can kind of see here with a lot more points, but you notice here that the gap isn't quite as prominent, so this was.",
                    "label": 0
                },
                {
                    "sent": "We're trying to figure out, you know, should we use this example because it's it's not a very good indication of a well separated diagram, but it turns out that you know it wasn't our method, it was the data set that actually has this property.",
                    "label": 0
                },
                {
                    "sent": "What I had done was geometrically uniformly sample along the rings, or rather so, each ring had the same number of samples, which means that the larger rings had many, had a lot fewer samples than the inner ring, and so the inner rings were much more prominent.",
                    "label": 0
                },
                {
                    "sent": "And you know the inner part was much more prominent than the outer part, and that's why this this last outer cluster, which was which corresponds to this outer ring, is so not prominent because it's actually not very dense.",
                    "label": 0
                },
                {
                    "sent": "And in this case, here is just a histogram of these points and you see this is the gap that we chose.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we decrease the.",
                    "label": 0
                },
                {
                    "sent": "Decrease the rips parameter.",
                    "label": 0
                },
                {
                    "sent": "The gap actually grows, which is shown here, and you know, here.",
                    "label": 0
                },
                {
                    "sent": "This really just illustrates that there is this correspondence of that.",
                    "label": 0
                },
                {
                    "sent": "You want to find the smallest.",
                    "label": 0
                },
                {
                    "sent": "Rips parameter or kind of the smallest edge lengths.",
                    "label": 0
                },
                {
                    "sent": "To get the best approximation while getting as much of your space as possible, because if you decrease it a little bit too much then then all of these, you know all of these components are going to start coming this way, and so you're going to end up with just annoys.",
                    "label": 0
                },
                {
                    "sent": "A lot of small component.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That don't mean anything OK, so now for the real data example.",
                    "label": 0
                },
                {
                    "sent": "So is comes from biology.",
                    "label": 0
                },
                {
                    "sent": "So what biologists do is they look at protein confirmations.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they don't want to do that many experiments.",
                    "label": 0
                },
                {
                    "sent": "So what they do is they have simulations and these are physical simulations where they actually simulates the forces and everything and.",
                    "label": 0
                },
                {
                    "sent": "You know they they can only do this for very short periods of time picoseconds.",
                    "label": 0
                },
                {
                    "sent": "But the interest the biological phenomena they want to see in protein confirmations is much longer.",
                    "label": 0
                },
                {
                    "sent": "It's on the order of milliseconds.",
                    "label": 0
                },
                {
                    "sent": "So the problem is now they have all these confirmations and you know they have these short trajectories, but they don't.",
                    "label": 0
                },
                {
                    "sent": "They want to find some longer term behave.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know what they want, what they've noticed, though, is that what happens?",
                    "label": 0
                },
                {
                    "sent": "You know, the protein confirmation doesn't just kind of randomly go everywhere.",
                    "label": 0
                },
                {
                    "sent": "It has several kind of fuzzy stable States and then it jumps from one stable state to another and then stays in the stable state for a long time.",
                    "label": 0
                },
                {
                    "sent": "So these are actually called meta.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stable states.",
                    "label": 0
                },
                {
                    "sent": "And the example I'm going to show you was produced by Vijay Pandas Group at Stanford and is the L9 dipeptide molecule which don't ask me what it does.",
                    "label": 0
                },
                {
                    "sent": "But we did have 200,000 confirmations in R22 so it's in 22 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "It has a non Euclidean metric.",
                    "label": 0
                },
                {
                    "sent": "You have to do for any two confirmations.",
                    "label": 0
                },
                {
                    "sent": "You have to find the best rigid transform and then compute the root mean squared distance.",
                    "label": 0
                },
                {
                    "sent": "So it's very non Euclidean.",
                    "label": 0
                },
                {
                    "sent": "It's very high dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But biologists have spent a long time studying this, so they know what the correct projection of this is.",
                    "label": 0
                },
                {
                    "sent": "So this they know that this actual molecule only has 2 degrees of freedom, and this is called a Ramachandran plot, and it's these are two angles of the confirmation, and these are the 200,000.",
                    "label": 0
                },
                {
                    "sent": "Individual confirmations mapped into this, so you know from the biologists we know this is the right answer, and so we ran this on our with our algorithm and this is the persistence diagram we got.",
                    "label": 0
                },
                {
                    "sent": "22 dimensional points projected to two.",
                    "label": 0
                },
                {
                    "sent": "So buy some buy something that they've figured out through the biological mechanisms that this has to be.",
                    "label": 0
                },
                {
                    "sent": "It's map down into this, yeah, so they're not the same.",
                    "label": 0
                },
                {
                    "sent": "The point the distances here are not the same as the distances in the actual confirmation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The actual distances are.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "So so for each two so so this.",
                    "label": 0
                },
                {
                    "sent": "So when we computed this first we spent, you know I think half a day just computing some distances because you need to look at every two points.",
                    "label": 0
                },
                {
                    "sent": "Find the best transform and then compute the distance and so you end as far as I know, there's no faster way to do it so.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So this was the persistence diagram we got, and it doesn't look too promising because it's, I mean there's no really well separated structure, but.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we when we actually cluster, we see there's I'll show you two potential clusterings.",
                    "label": 0
                },
                {
                    "sent": "One is with six clusters and one is with seven, and biologists have spent a lot of time trying to get these six clusters.",
                    "label": 0
                },
                {
                    "sent": "And you know, there's at least two or three papers written on just getting this kind of clustering.",
                    "label": 0
                },
                {
                    "sent": "And we actually got it without too much trouble.",
                    "label": 0
                },
                {
                    "sent": "Usually what ends up happening is because these states are much less frequent than these states.",
                    "label": 0
                },
                {
                    "sent": "This all ends up being merged into one cluster, so they usually get five or four clusters rather than the six that they're actually looking for.",
                    "label": 0
                },
                {
                    "sent": "But you know, we said, well, you know why not seven clusters?",
                    "label": 0
                },
                {
                    "sent": "I mean, the persistence diagram tells us that it's I mean it's just as reasonable as six from the.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Austin's point of view, and so you know, because if we map this here, we see there's kind of four clusters that are, you know, roughly on the order of 10 to 3, then we have kind of another two that are still pretty high, and then you know three that are noise and.",
                    "label": 0
                },
                {
                    "sent": "The actual metric that the biologists use to measure how good their clustering is is something called metastability.",
                    "label": 0
                },
                {
                    "sent": "And so I won't go into what the actual definition of it is, but basically it measures how within these trajectories, how often do you stay within within one cluster.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if you want to what you want to do is you want to maximize the number of clusters, while minimizing how often they jump between clusters, and so you know we took our clustering and we compute it and we saw that you know OK, actually there is no good reason for having six clusters versus 7.",
                    "label": 0
                },
                {
                    "sent": "Even based on their own metric of what it is, and from our point of view, this just makes this makes sense, because from the persistence diagram we don't really know what the right number of clusters is, but there are experts in the field where the data comes from.",
                    "label": 0
                },
                {
                    "sent": "The can say OK, well, we can tell them, you know we can have 456 or seven clusters and it all makes sense.",
                    "label": 0
                },
                {
                    "sent": "Now you have to tell me what.",
                    "label": 0
                },
                {
                    "sent": "You have to give me some additional information so that I can decide what the true number of clusters is.",
                    "label": 0
                },
                {
                    "sent": "In this case it's the metastability and then you can decide that you actually want seven clusters because.",
                    "label": 0
                },
                {
                    "sent": "Let's say for this initial 6 clusters, while they like 6.",
                    "label": 0
                },
                {
                    "sent": "This was because really actually.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the gold standard was a biologist, went and looked at this.",
                    "label": 0
                },
                {
                    "sent": "Andrew lines this way and said these should be the six clusters that we're going to that we want.",
                    "label": 0
                },
                {
                    "sent": "No no no.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stop.",
                    "label": 0
                },
                {
                    "sent": "Yep, so OK.",
                    "label": 0
                },
                {
                    "sent": "The take home message of this talk really is that you know what I've shown you is kind of a very generic practical algorithm, and for us it's kind of important to make it as general as possible, so it's nice that we can.",
                    "label": 0
                },
                {
                    "sent": "Most of what I've said today works in any metric space, so we don't really need a lot of geometry.",
                    "label": 0
                },
                {
                    "sent": "We don't need the manifold, we don't need Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "We can really just work with metric spaces, or even in principle.",
                    "label": 0
                },
                {
                    "sent": "We could even just work with similarities.",
                    "label": 0
                },
                {
                    "sent": "It's very generic, you know we're not experts in density estimation or.",
                    "label": 0
                },
                {
                    "sent": "You know like choosing graph, so you can choose your favorite and just run the algorithm with it.",
                    "label": 0
                },
                {
                    "sent": "It's completely agnostic to.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their choice.",
                    "label": 0
                },
                {
                    "sent": "But the point really is that I think you know, for whatever choices you make, the persistence diagram really is the structure of the function in the space.",
                    "label": 0
                },
                {
                    "sent": "In some sense, it really is, you know, given what your estimate, what you end up estimating and these things.",
                    "label": 0
                },
                {
                    "sent": "This really is what you're looking at.",
                    "label": 0
                },
                {
                    "sent": "There is no kind of additional artifacts or anything like this if you know if you don't like what your what, your persistence diagram looks like, you need to change.",
                    "label": 0
                },
                {
                    "sent": "If some assumptions, maybe a different graph, a different estimator.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you know?",
                    "label": 0
                },
                {
                    "sent": "Look over, perhaps, maybe it's not really a smooth density function that this is sampled from something like.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "And you know the nice part for us is that it does come with theoretical guarantees.",
                    "label": 0
                },
                {
                    "sent": "We have the number of clusters we have, the spatial stability, and it opens a lot of other in.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Listing questions, so I'm just going to go over some.",
                    "label": 0
                },
                {
                    "sent": "Very quickly over some current work that we're doing so you know, I said if there's a gap, what we can do is we can perturb our function so we can introduce noise and what we can do is well, then we can kind of start to detect boundaries and we can start to do kind of soft clustering.",
                    "label": 0
                },
                {
                    "sent": "We can say, OK, well, a point may be unstable, but let's give it a probability that it goes to this cluster of that cluster.",
                    "label": 0
                },
                {
                    "sent": "So you know this is preliminary work, but it.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seems to work very well.",
                    "label": 0
                },
                {
                    "sent": "Other interesting question is how do we use higher dimensional features?",
                    "label": 0
                },
                {
                    "sent": "So I've mentioned that we only use the connected components, but all the topological persistence works in arbitrary dimension, so you know we not only get the number of components, but we can figure out that this is a crater we can figure out that this is a Taurus, you know, but the question is for clustering, how can we actually use this or what?",
                    "label": 0
                },
                {
                    "sent": "What can this actually tell us for real machine learning problems, is I mean?",
                    "label": 0
                },
                {
                    "sent": "Is this information useful at all?",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the other thing is, as I mentioned, we are not restricted to density functions.",
                    "label": 0
                },
                {
                    "sent": "We can use arbitrary functions, and right now I'm working with.",
                    "label": 0
                },
                {
                    "sent": "With someone also at Stanford on doing segmentation using a particular function and the results are actually quite promising as well.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and the code if you're interested in trying it, is available here and are there any questions?",
                    "label": 0
                },
                {
                    "sent": "Yes, so so for the clustering.",
                    "label": 0
                },
                {
                    "sent": "You can use the Union find algorithm which is in the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "But for higher dimensions you need to use the full homology computation which is cubed.",
                    "label": 0
                },
                {
                    "sent": "Roughly.",
                    "label": 0
                },
                {
                    "sent": "To, well, that's to reduce the.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "So the difference between this and discrete Morse theory is that here we're actually we're not requiring our samples to be on the space, it just has to be close to the space.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, this is a little more general.",
                    "label": 0
                },
                {
                    "sent": "But again, I think the result is the same even in discrete Morse theory that you're not going to be able to get any.",
                    "label": 0
                },
                {
                    "sent": "You're in some sense, your choice of gradients in the discrete case is arbitrary, and I might have forgotten to mention it, but the persistence diagram.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter what gradient you choose, the persistence diagram is actually the same, so.",
                    "label": 0
                },
                {
                    "sent": "So you know the problem is, once you actually have to choose the right gradient, estimating the gradient is a much more difficult problem than just estimating the function.",
                    "label": 0
                },
                {
                    "sent": "So where we can we try to avoid it?",
                    "label": 0
                },
                {
                    "sent": "Persistent diagram.",
                    "label": 0
                },
                {
                    "sent": "The answer is true.",
                    "label": 0
                },
                {
                    "sent": "This is a very fine way to visualize the structure function, yes.",
                    "label": 0
                },
                {
                    "sent": "Well, the information is important.",
                    "label": 0
                },
                {
                    "sent": "It seems that if you very different parameters.",
                    "label": 0
                },
                {
                    "sent": "Diagram can change quite a bit, yes.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "Well, you know.",
                    "label": 0
                },
                {
                    "sent": "So the point is that you can.",
                    "label": 0
                },
                {
                    "sent": "There's a stable part in an unstable part, and actually you can track this if your changes are small enough you can.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can in some sense stack them and they're going to change continuously like each point will actually move continuously, so.",
                    "label": 0
                },
                {
                    "sent": "You know one other thing we're trying to define now is say that you have all these different things.",
                    "label": 0
                },
                {
                    "sent": "Can we actually define a distribution on these persistence diagrams and in practice, yes we can.",
                    "label": 0
                },
                {
                    "sent": "But it's not clear from.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's kind of several theoretical questions that are very tricky to answer in terms of you know what are we actually converging to.",
                    "label": 0
                },
                {
                    "sent": "What is is there one distribution function that?",
                    "label": 0
                },
                {
                    "sent": "We're looking at or because you know, for example, if we look at the space so we have some estimated function and we look at the space of all close functions we can't.",
                    "label": 0
                },
                {
                    "sent": "We can't define a uniform sampling on this space because it's infinite dimensional, so you know, you know we can certainly perturb things and see what we get, but theoretically we're not really sure what this means so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's kinda work.",
                    "label": 0
                },
                {
                    "sent": "Any idea how?",
                    "label": 0
                },
                {
                    "sent": "Complicated would be to make this argument incremental, so local changes in data.",
                    "label": 0
                },
                {
                    "sent": "Local local changes in the clustering.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can do this so you can actually make this dynamic.",
                    "label": 0
                },
                {
                    "sent": "It's so you know if you are to change things.",
                    "label": 0
                },
                {
                    "sent": "So with the graph, it's a little more complicated, so if the function is just change, it's very easy.",
                    "label": 0
                },
                {
                    "sent": "If the actual graph changes, you can also do it, but it's a little more tricky like it can be generalized, though I think.",
                    "label": 0
                },
                {
                    "sent": "This would be.",
                    "label": 0
                },
                {
                    "sent": "Step function banning the road.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's but the theoretical tools you need are a little more involved.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "So I mean for the graph it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Yeah no, no.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so as long as so the nice thing is for the for the Euclidean cases, we just use the KD tree to compute distance.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "I mean it's each distance query is log time and each.",
                    "label": 0
                },
                {
                    "sent": "And the space you need is constant.",
                    "label": 0
                },
                {
                    "sent": "Essentially so, but yeah, I mean, you still are constrained by kind of being able to find these neighborhoods.",
                    "label": 0
                },
                {
                    "sent": "So if you have to look over your entire data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it.",
                    "label": 0
                },
                {
                    "sent": "Well it's it's not implemented yet, but we do have.",
                    "label": 0
                },
                {
                    "sent": "I do have the theoretical results.",
                    "label": 0
                },
                {
                    "sent": "Let's say that it works and.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's it's in the process of I don't have any nice pictures to show or a movie.",
                    "label": 0
                },
                {
                    "sent": "So I decided to skip it.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Topological mode analysis tool.",
                    "label": 0
                },
                {
                    "sent": "So far.",
                    "label": 0
                },
                {
                    "sent": "And image segmentation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I thought yeah those are the things we've tried.",
                    "label": 0
                },
                {
                    "sent": "Send yeah, there's there's been a few other things but nothing.",
                    "label": 0
                },
                {
                    "sent": "Nothing really extends no extensive testing, so.",
                    "label": 0
                },
                {
                    "sent": "OK. How do I say thanks?",
                    "label": 0
                }
            ]
        }
    }
}