{
    "id": "pwzfodqey5verlsnlbn6kkqpf5plgw66",
    "title": "Large Scale Rule-Based Reasoning Using a Laptop",
    "info": {
        "author": [
            "Martin Peters, Fachhochschule Dortmund"
        ],
        "published": "July 15, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2015_peters_reasoning/",
    "segmentation": [
        [
            "So my name is Martin Peters.",
            "I'm from the University of Applied Science and Arts in Dortmund and I'm going to talk about large scale rule based reasoning using a laptop.",
            "So when come."
        ],
        [
            "Use of semantic technologies reasoning is still one key feature, and it basically means to infer knowledge that is implicitly given by the existing data.",
            "Nevertheless, reasoning can still be a very challenging task, especially when it comes to reasoning on large data sets with a high performance.",
            "The scalability and poor performance of the reasoning process highly depends on the data set that is used, as well as on the ontology language.",
            "While the ontology language can often be expressed in Rule based way like we are here focusing on in my talk.",
            "Um, recently they have been much research in this area and most of these features research is focusing on parallel reasoning where the approaches can be basically divided into 2 two parts.",
            "I'm the first one is to use multi core processor for reasoning which is very cheap and fast 'cause you can use your CPU like it's in your laptop for example.",
            "And it's also, but it's also limited in terms of the hardware resources that are available, so you're also very limited with respect to the size of the data set your processing.",
            "On the other side, there are quite a few cluster based approaches.",
            "Most of them are using MapReduce for example.",
            "They are very fast and scalable.",
            "For example, we could see that Web Pi was able to scale on 100 billion triples, but it's also very expensive and costly to set up such an architecture just for reason."
        ],
        [
            "Keeping these two parts of reasoning in mind and looking at the data sets we usually have to work with in the real world, and most of the real world data sets are ranging in the size of a couple of 1000 of triples to up to a few of billions of triples.",
            "Multi core processor process on one side are only able most of the time to handle maybe a couple of 100 of millions of triples and then usually you get resource problems on the other side, MapReduce can easily handle this amount of data, but it's also very expensive and costly, so there is some kind of gap and it would be great if we were able to also use multicore processor to handle even larger data sets like they are available in the real world.",
            "In our preview."
        ],
        [
            "Work called scaling parallel rule based reasoning.",
            "We implemented Iriti based forward chaining reasonable and this reason our users a massively parallel hardware for GPU's of graphic cards.",
            "This reason allows us to simply define rules using a rule file and to input RDF data, and it outputs in rich data.",
            "Using this approach, we showed that it's possible to use a single machine to reason on one billion triples.",
            "Nevertheless, we also had to use two GPU's and a total of 192 gigabytes of memory.",
            "1st."
        ],
        [
            "More, we figured out that the memory consumption is still limiting factor for this approach, and so our goal was for current work to identify the resource consuming parts of the reason a process.",
            "To reduce the data that means it needs to be stored and to apply some sort of compression to the data that still needs to be stored.",
            "Stored.",
            "To do this, we also thought about how to swap data from the main memory to the hard disk, which is much cheaper and more available memory.",
            "And before I can start to."
        ],
        [
            "Describe the concepts we introduced for our work.",
            "I first have to introduce the retail algorithm which is based on a written network that gets derived from the input rules.",
            "So here I put two simple rules from the RDF semantics and based on these rules will first have to start to create Alpha and beta nodes.",
            "Each Alpha node is derived by all of the unique rule pattern terms within our rule base.",
            "So we create 12A nodes.",
            "I for one A2 and we connect those 2A nodes by beta one, which then corresponds to the complete rule #2.",
            "Based on this simple routine network.",
            "We can then start to iterate all of our inputs data through the network.",
            "That means that first we have to start using or applying the Alpha matching, which means that we use all of the input triples and match them against all of the triple Alpha nodes, and in this case A1A2 for A1.",
            "You can see that there's actually no condition for the node, cause it corresponds to a rule pattern that only contains variables, so every trip is going to match this Alpha node.",
            "And we create a working memory which contains references to all of our troubles.",
            "For Alpha #2 there is a condition that the predicate needs to be in our GFS domain, and so we create a working memory that holds reference that rule #3 which matches this condition.",
            "After applying Alpha matching, we have to do is a beta matching, which means that we take all of the elements from the working memory memory of other one, match them against all of the elements of working memory of A2."
        ],
        [
            "We check the condition of the beta node.",
            "And this is the case for these two combinations that are matching as a bitter nodes based on this written network, we can now start to fire the rules and to derive new triples and for rule number one we get 3 new triples where one is a duplicate.",
            "So we're going to ignore it.",
            "And for rule #2 we get two more triples.",
            "Based on these new triples, we can now start with the process again to derive new triples until no more triples can be derived and the process can be finished.",
            "Tapola"
        ],
        [
            "This process to run it on a GPU on a massively parallel hardware for Alpha matching.",
            "What we basically do is we create once read for each of the input triples and there's this red.",
            "Then iterate through all of the Alpha nodes on the GPU to see if one triple matches the condition of the Alpha node.",
            "Copy to note this is quite for the beta matchings is quite similar, except that we create one thread for each of the elements of A1.",
            "For example, of the working memory of A1 and this thread.",
            "Then iterate through all of them elements of the working memory of A2.",
            "To see if the combination matches the condition of the beta node.",
            "OK looking."
        ],
        [
            "This process we can basically identify three data structures that are needed within our reasoning process.",
            "The first data structure, as it ripples itself.",
            "The triples are stored within a list of dictionary encoded triples, so we're not using the string representation of triple, but we're using a dictionary corner representation, where every numerical value points to a tune.",
            "Entry was on a dictionary, which makes it much easier to handle the data.",
            "Furthermore, we've got the working memories itself.",
            "Is working memories contain references to the triples list.",
            "And the third data structure is the hash set, and the headset is used to identify duplicate triples.",
            "So every time we derive new triples forms, Richie algorithm will first have to check if this triple is already available within our data set, and if so, we're going to ignore or reject it, and to do so we usually compute hash code from a triple and see if this had on.",
            "This trip is already available in our data set.",
            "To get an idea on how much memory is actually used for these three data."
        ],
        [
            "Just we did a small innovation and use the least Universe benchmark data set and generated about 2000 universities.",
            "And we Furthermore use the English version of DB Pedia.",
            "And apply target S and low def rules.",
            "And first of all, you can see that if just to store the triples it took us about 8 to 11 gigabytes.",
            "Using the numerical representation.",
            "Some more memory was used by the triple hash set and even more memory was used by the working memories of authority algorithm.",
            "So in total we used about 20 to 30 gigabytes just to store these three data structures.",
            "Possibility algorithm, which is already too much memory to be handled with single laptop for example.",
            "Um?"
        ],
        [
            "Our idea to reduce the memory consumption was based on two things.",
            "First of all, we wanted to swap as much memory as possible from the heart from the main memory to the hard disk.",
            "To swap this memory to the hard disk without loss of performance is very important to create self contained data structures that can be read and written within blocks so that you don't have to do any random access on your hard disk.",
            "And the second idea was to apply compression.",
            "So the first thing we did with working memories was that we remove all references within the working memories and instead of using the references to the triples list, we are placing the actual data was in the working memories.",
            "But we are not placing the complete triple was in it, but we are only placing the variable parts of reaching node within the working memory.",
            "So for example for Alpha #2 where the predicates was defined as.",
            "To be in our group's domain, we only store as a subject and subject of matching triples within the working memory."
        ],
        [
            "Pros and more.",
            "Looking at the triple hash set, the triple headset was used to identify duplicate triples and it is randomly accessed and was a very high frequency, so this wouldn't make sense to swap it to the hard disk because it would slow down the process immediately, so we had to find a way to keep in keep it in memory, but to be able to compress it.",
            "And to do this, we also first started to place the complete data into the data structure, so we didn't use reference anymore, but we placed a complete triple within 2 into the headset.",
            "Base."
        ],
        [
            "This new data structure containing three numerical values we applied.",
            "First of all, we applied vertical partitioning.",
            "Vertical partitioning basically means that we don't store one big hash set where all triples are safe, but we create 1 hash set for every predicate that is available within our data set.",
            "So it is possible because many data sets are only described using a couple of predicates.",
            "For examples, University benchmark data set contains only 43 predicates.",
            "While DB Pedia, which is a much larger data set, I think about 3 billion tuples right now has about 53,000 predicates.",
            "And by using multiple hash sets for each, one has set for each predicate, we remove the need to store the predicate explicitly and we are able to reduce the memory consumption of about 30 three person.",
            "So there."
        ],
        [
            "Basically 2 numbers left that can be compressed.",
            "To do this we further applied differential encoding, which means that we are we are first sorting both values, so the first triple for examples of 55 and 35 minutes we replaced the higher number to the left side and the lower numbers right side and we check if it's cheaper to just towards the difference between two values so you can see that on the left side there is the initial values.",
            "Right side instead of starting a 55 in a 35, we're just storing a 55 and 20 because the 20 is the difference between the two values, so it's slowing down lower number.",
            "Based on the."
        ],
        [
            "2 numbers and we further apply a variable byte encoding.",
            "And variable byte encoding means that we're not using 8 by data types to encode each number, cause many of these numbers will only use just a very short portion of of bytes within the actual data type.",
            "For examples of 55 as well as the 20 can both be encoded using a single bite.",
            "And that's exactly what we did here.",
            "We first encodes the lower number on the right side, which is the 20 in this case, and we only use the first 7 bits to encode the numerical value, and we use dates built to indicate if the end of the data value was reached or if the next byte also adds some more data to this value.",
            "The second value is just placed in front of it.",
            "While the two most significant bits are used to identify or to indicate if the differential encoding was used and if the number of the order of numbers was changed or not.",
            "So what we do here is basically that we take the two numbers, we change the encoding, put them together and build a new unique number that still stores the same information, but using much less bytes.",
            "So."
        ],
        [
            "Summarize this process of compression up when we first do it is.",
            "We created a hash set containing three numerical values, where each value was initially decoded using an 8 by data type.",
            "So we've got 24 bytes in total and we reduce this by applying vertical partitioning.",
            "So we're just using 16 bytes for each truthful information, and we further applied differential encoding and variable byte encoding to further reduce memory consumption.",
            "In this case, 22 byte for the first value or three bites for the for the second triple information."
        ],
        [
            "Looking back at our three data structures, first of all, the working memories were restructured to be self contained and that allows us to swap the complete data structure to the hard disk.",
            "So we're not using any main memory for this data structure anymore, wasn't more.",
            "Furthermore, the triple hash set was compressed.",
            "And be cause these are working memories, nor the triple hashtag are using any references.",
            "It was a troubled anymore.",
            "We can also completely swap that rules to the hard disk and we don't have to randomly excess of triples anymore, which allows us to very efficiently store and read data to the hard disk.",
            "Nevertheless, we also introduced quite a lot of complexity, especially to the codes that needs to be executed on the GPU and to address this complexity as well."
        ],
        [
            "Yes, to also apply further optimizations.",
            "We also introduced the concept to generate the code that gets executed on the GPU based on the rule files that are that are provided for the reason of process.",
            "So before we start the voting process, we parse the rule file with.",
            "Further dictionary encodes single rule elements within the rule file and based on this information we create the files that get executed on the GPU.",
            "This allows us to reduce loops within the code.",
            "It allows us to optimize memory access which is very important if you're programming for a GPU.",
            "For example, you have to manage different kinds of memories and you have to be very careful.",
            "When you're executing, accessing, which kind of memory.",
            "It further allows us to reduce the memory paramaters parameters and also to use dictionary encoded values directly within our program code.",
            "So for example, here in this small example you can see that we are comparing the predicate of a triple directly with Norma number, because we exactly know which number is the dictionary encoded value of the predicate, which need to be needs to be compared to here.",
            "We have firms are able to provide specific and optimized code for each of the RTI nodes that gets derived by them.",
            "RTI algorithm so we are able to provide very.",
            "Optimized code for the GPU."
        ],
        [
            "To test our concepts, we used three data sets.",
            "The first one is University benchmark data set, where we generated up to 8000 universities was up to 1 million triples.",
            "We use the English version of DB.",
            "Pedia was about 400 million triples and we used one more real world data set from the medical domain, which is CTD.",
            "We applied the RDF S and low DF rules and used a routine a laptop from 2012, 2012 with 16 gigabytes of memory and 1 gigabyte 1 gigabyte GPU."
        ],
        [
            "And first of all, you can see that we were able to do the reasoning on one billion triples on just a single laptop.",
            "And here you can see the memory consumption of the hash set.",
            "That was the only data structure that needs to be permanently kept in main memory, and for one billion triples, which is the data set from the University benchmark.",
            "With 8000 universities we used about 12 gigabytes to store the triple information within the headset on the laptop.",
            "And we used an average of 6.2 bytes per triple to store all triple information on our laptop.",
            "Instead of 24 bytes, and if we are also considering the overheads that gets introduced by using a headset, so a hash that always has some unused entries, we are using 9.7 bytes but triple in total we were able to reduce the memory consumption of the hedge set of about 75%, and looking at the complete reasoner process, which also includes the working memories as well as the triples itself, we were able to reduce memory consumption of 84%."
        ],
        [
            "Looking at the performance of our reasoning process, we also got really good results.",
            "For example, the University benchmark data set was computed within a maximum throughput of 4 million triples a second.",
            "For the smallest value and was about 1.8 billion seconds.",
            "Million triples a second for the largest data set.",
            "For DP and the medical data set, we also reached the throughput of more than 1 million people or more than 5 seconds to compare it with other work.",
            "For example, web PY used 64 machines using MapReduce and they reached maximum throughput of 2.1 billion triples a second and dynamite, which is stream reasoners at once on a single machine.",
            "Just like our approach and also uses multi core processor reached maximum throughput.",
            "Of 227K triples per second, so we are much faster than sensors.",
            "OK."
        ],
        [
            "Already coming to my end of the presentation, what we did is we identified the resource consuming parts within the RTI algorithm and we tried to reduce the memory consumption and to swap as much data from the main memory to the hard disk without loss of performance.",
            "This was done by restructuring the working memories by also applying a compression to the used hash set for deduplication and we also introduced some code generation.",
            "The code that gets executed on the GPU based on the used rules.",
            "We finally were able to reason 1 billion triples using just a single laptop and reach the maximum throughput of 5 million triples per second.",
            "In the future it would be very interesting to see if this concept can also be applied to stream reasoning, and it would also be interesting to see if we can add some more expressiveness to the rule language that is supporting."
        ],
        [
            "OK, thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my name is Martin Peters.",
                    "label": 0
                },
                {
                    "sent": "I'm from the University of Applied Science and Arts in Dortmund and I'm going to talk about large scale rule based reasoning using a laptop.",
                    "label": 1
                },
                {
                    "sent": "So when come.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use of semantic technologies reasoning is still one key feature, and it basically means to infer knowledge that is implicitly given by the existing data.",
                    "label": 1
                },
                {
                    "sent": "Nevertheless, reasoning can still be a very challenging task, especially when it comes to reasoning on large data sets with a high performance.",
                    "label": 0
                },
                {
                    "sent": "The scalability and poor performance of the reasoning process highly depends on the data set that is used, as well as on the ontology language.",
                    "label": 0
                },
                {
                    "sent": "While the ontology language can often be expressed in Rule based way like we are here focusing on in my talk.",
                    "label": 0
                },
                {
                    "sent": "Um, recently they have been much research in this area and most of these features research is focusing on parallel reasoning where the approaches can be basically divided into 2 two parts.",
                    "label": 0
                },
                {
                    "sent": "I'm the first one is to use multi core processor for reasoning which is very cheap and fast 'cause you can use your CPU like it's in your laptop for example.",
                    "label": 0
                },
                {
                    "sent": "And it's also, but it's also limited in terms of the hardware resources that are available, so you're also very limited with respect to the size of the data set your processing.",
                    "label": 0
                },
                {
                    "sent": "On the other side, there are quite a few cluster based approaches.",
                    "label": 0
                },
                {
                    "sent": "Most of them are using MapReduce for example.",
                    "label": 0
                },
                {
                    "sent": "They are very fast and scalable.",
                    "label": 0
                },
                {
                    "sent": "For example, we could see that Web Pi was able to scale on 100 billion triples, but it's also very expensive and costly to set up such an architecture just for reason.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Keeping these two parts of reasoning in mind and looking at the data sets we usually have to work with in the real world, and most of the real world data sets are ranging in the size of a couple of 1000 of triples to up to a few of billions of triples.",
                    "label": 0
                },
                {
                    "sent": "Multi core processor process on one side are only able most of the time to handle maybe a couple of 100 of millions of triples and then usually you get resource problems on the other side, MapReduce can easily handle this amount of data, but it's also very expensive and costly, so there is some kind of gap and it would be great if we were able to also use multicore processor to handle even larger data sets like they are available in the real world.",
                    "label": 0
                },
                {
                    "sent": "In our preview.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work called scaling parallel rule based reasoning.",
                    "label": 0
                },
                {
                    "sent": "We implemented Iriti based forward chaining reasonable and this reason our users a massively parallel hardware for GPU's of graphic cards.",
                    "label": 0
                },
                {
                    "sent": "This reason allows us to simply define rules using a rule file and to input RDF data, and it outputs in rich data.",
                    "label": 0
                },
                {
                    "sent": "Using this approach, we showed that it's possible to use a single machine to reason on one billion triples.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, we also had to use two GPU's and a total of 192 gigabytes of memory.",
                    "label": 0
                },
                {
                    "sent": "1st.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More, we figured out that the memory consumption is still limiting factor for this approach, and so our goal was for current work to identify the resource consuming parts of the reason a process.",
                    "label": 1
                },
                {
                    "sent": "To reduce the data that means it needs to be stored and to apply some sort of compression to the data that still needs to be stored.",
                    "label": 0
                },
                {
                    "sent": "Stored.",
                    "label": 0
                },
                {
                    "sent": "To do this, we also thought about how to swap data from the main memory to the hard disk, which is much cheaper and more available memory.",
                    "label": 1
                },
                {
                    "sent": "And before I can start to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Describe the concepts we introduced for our work.",
                    "label": 0
                },
                {
                    "sent": "I first have to introduce the retail algorithm which is based on a written network that gets derived from the input rules.",
                    "label": 0
                },
                {
                    "sent": "So here I put two simple rules from the RDF semantics and based on these rules will first have to start to create Alpha and beta nodes.",
                    "label": 0
                },
                {
                    "sent": "Each Alpha node is derived by all of the unique rule pattern terms within our rule base.",
                    "label": 0
                },
                {
                    "sent": "So we create 12A nodes.",
                    "label": 0
                },
                {
                    "sent": "I for one A2 and we connect those 2A nodes by beta one, which then corresponds to the complete rule #2.",
                    "label": 0
                },
                {
                    "sent": "Based on this simple routine network.",
                    "label": 0
                },
                {
                    "sent": "We can then start to iterate all of our inputs data through the network.",
                    "label": 0
                },
                {
                    "sent": "That means that first we have to start using or applying the Alpha matching, which means that we use all of the input triples and match them against all of the triple Alpha nodes, and in this case A1A2 for A1.",
                    "label": 0
                },
                {
                    "sent": "You can see that there's actually no condition for the node, cause it corresponds to a rule pattern that only contains variables, so every trip is going to match this Alpha node.",
                    "label": 0
                },
                {
                    "sent": "And we create a working memory which contains references to all of our troubles.",
                    "label": 0
                },
                {
                    "sent": "For Alpha #2 there is a condition that the predicate needs to be in our GFS domain, and so we create a working memory that holds reference that rule #3 which matches this condition.",
                    "label": 0
                },
                {
                    "sent": "After applying Alpha matching, we have to do is a beta matching, which means that we take all of the elements from the working memory memory of other one, match them against all of the elements of working memory of A2.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We check the condition of the beta node.",
                    "label": 0
                },
                {
                    "sent": "And this is the case for these two combinations that are matching as a bitter nodes based on this written network, we can now start to fire the rules and to derive new triples and for rule number one we get 3 new triples where one is a duplicate.",
                    "label": 0
                },
                {
                    "sent": "So we're going to ignore it.",
                    "label": 0
                },
                {
                    "sent": "And for rule #2 we get two more triples.",
                    "label": 0
                },
                {
                    "sent": "Based on these new triples, we can now start with the process again to derive new triples until no more triples can be derived and the process can be finished.",
                    "label": 0
                },
                {
                    "sent": "Tapola",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This process to run it on a GPU on a massively parallel hardware for Alpha matching.",
                    "label": 1
                },
                {
                    "sent": "What we basically do is we create once read for each of the input triples and there's this red.",
                    "label": 0
                },
                {
                    "sent": "Then iterate through all of the Alpha nodes on the GPU to see if one triple matches the condition of the Alpha node.",
                    "label": 1
                },
                {
                    "sent": "Copy to note this is quite for the beta matchings is quite similar, except that we create one thread for each of the elements of A1.",
                    "label": 0
                },
                {
                    "sent": "For example, of the working memory of A1 and this thread.",
                    "label": 0
                },
                {
                    "sent": "Then iterate through all of them elements of the working memory of A2.",
                    "label": 0
                },
                {
                    "sent": "To see if the combination matches the condition of the beta node.",
                    "label": 0
                },
                {
                    "sent": "OK looking.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This process we can basically identify three data structures that are needed within our reasoning process.",
                    "label": 0
                },
                {
                    "sent": "The first data structure, as it ripples itself.",
                    "label": 0
                },
                {
                    "sent": "The triples are stored within a list of dictionary encoded triples, so we're not using the string representation of triple, but we're using a dictionary corner representation, where every numerical value points to a tune.",
                    "label": 0
                },
                {
                    "sent": "Entry was on a dictionary, which makes it much easier to handle the data.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, we've got the working memories itself.",
                    "label": 0
                },
                {
                    "sent": "Is working memories contain references to the triples list.",
                    "label": 0
                },
                {
                    "sent": "And the third data structure is the hash set, and the headset is used to identify duplicate triples.",
                    "label": 0
                },
                {
                    "sent": "So every time we derive new triples forms, Richie algorithm will first have to check if this triple is already available within our data set, and if so, we're going to ignore or reject it, and to do so we usually compute hash code from a triple and see if this had on.",
                    "label": 0
                },
                {
                    "sent": "This trip is already available in our data set.",
                    "label": 0
                },
                {
                    "sent": "To get an idea on how much memory is actually used for these three data.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just we did a small innovation and use the least Universe benchmark data set and generated about 2000 universities.",
                    "label": 0
                },
                {
                    "sent": "And we Furthermore use the English version of DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "And apply target S and low def rules.",
                    "label": 0
                },
                {
                    "sent": "And first of all, you can see that if just to store the triples it took us about 8 to 11 gigabytes.",
                    "label": 0
                },
                {
                    "sent": "Using the numerical representation.",
                    "label": 0
                },
                {
                    "sent": "Some more memory was used by the triple hash set and even more memory was used by the working memories of authority algorithm.",
                    "label": 0
                },
                {
                    "sent": "So in total we used about 20 to 30 gigabytes just to store these three data structures.",
                    "label": 0
                },
                {
                    "sent": "Possibility algorithm, which is already too much memory to be handled with single laptop for example.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our idea to reduce the memory consumption was based on two things.",
                    "label": 0
                },
                {
                    "sent": "First of all, we wanted to swap as much memory as possible from the heart from the main memory to the hard disk.",
                    "label": 0
                },
                {
                    "sent": "To swap this memory to the hard disk without loss of performance is very important to create self contained data structures that can be read and written within blocks so that you don't have to do any random access on your hard disk.",
                    "label": 1
                },
                {
                    "sent": "And the second idea was to apply compression.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we did with working memories was that we remove all references within the working memories and instead of using the references to the triples list, we are placing the actual data was in the working memories.",
                    "label": 1
                },
                {
                    "sent": "But we are not placing the complete triple was in it, but we are only placing the variable parts of reaching node within the working memory.",
                    "label": 0
                },
                {
                    "sent": "So for example for Alpha #2 where the predicates was defined as.",
                    "label": 0
                },
                {
                    "sent": "To be in our group's domain, we only store as a subject and subject of matching triples within the working memory.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pros and more.",
                    "label": 0
                },
                {
                    "sent": "Looking at the triple hash set, the triple headset was used to identify duplicate triples and it is randomly accessed and was a very high frequency, so this wouldn't make sense to swap it to the hard disk because it would slow down the process immediately, so we had to find a way to keep in keep it in memory, but to be able to compress it.",
                    "label": 0
                },
                {
                    "sent": "And to do this, we also first started to place the complete data into the data structure, so we didn't use reference anymore, but we placed a complete triple within 2 into the headset.",
                    "label": 0
                },
                {
                    "sent": "Base.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This new data structure containing three numerical values we applied.",
                    "label": 0
                },
                {
                    "sent": "First of all, we applied vertical partitioning.",
                    "label": 0
                },
                {
                    "sent": "Vertical partitioning basically means that we don't store one big hash set where all triples are safe, but we create 1 hash set for every predicate that is available within our data set.",
                    "label": 0
                },
                {
                    "sent": "So it is possible because many data sets are only described using a couple of predicates.",
                    "label": 0
                },
                {
                    "sent": "For examples, University benchmark data set contains only 43 predicates.",
                    "label": 0
                },
                {
                    "sent": "While DB Pedia, which is a much larger data set, I think about 3 billion tuples right now has about 53,000 predicates.",
                    "label": 0
                },
                {
                    "sent": "And by using multiple hash sets for each, one has set for each predicate, we remove the need to store the predicate explicitly and we are able to reduce the memory consumption of about 30 three person.",
                    "label": 0
                },
                {
                    "sent": "So there.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically 2 numbers left that can be compressed.",
                    "label": 0
                },
                {
                    "sent": "To do this we further applied differential encoding, which means that we are we are first sorting both values, so the first triple for examples of 55 and 35 minutes we replaced the higher number to the left side and the lower numbers right side and we check if it's cheaper to just towards the difference between two values so you can see that on the left side there is the initial values.",
                    "label": 0
                },
                {
                    "sent": "Right side instead of starting a 55 in a 35, we're just storing a 55 and 20 because the 20 is the difference between the two values, so it's slowing down lower number.",
                    "label": 0
                },
                {
                    "sent": "Based on the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2 numbers and we further apply a variable byte encoding.",
                    "label": 0
                },
                {
                    "sent": "And variable byte encoding means that we're not using 8 by data types to encode each number, cause many of these numbers will only use just a very short portion of of bytes within the actual data type.",
                    "label": 1
                },
                {
                    "sent": "For examples of 55 as well as the 20 can both be encoded using a single bite.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what we did here.",
                    "label": 0
                },
                {
                    "sent": "We first encodes the lower number on the right side, which is the 20 in this case, and we only use the first 7 bits to encode the numerical value, and we use dates built to indicate if the end of the data value was reached or if the next byte also adds some more data to this value.",
                    "label": 0
                },
                {
                    "sent": "The second value is just placed in front of it.",
                    "label": 1
                },
                {
                    "sent": "While the two most significant bits are used to identify or to indicate if the differential encoding was used and if the number of the order of numbers was changed or not.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is basically that we take the two numbers, we change the encoding, put them together and build a new unique number that still stores the same information, but using much less bytes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Summarize this process of compression up when we first do it is.",
                    "label": 0
                },
                {
                    "sent": "We created a hash set containing three numerical values, where each value was initially decoded using an 8 by data type.",
                    "label": 0
                },
                {
                    "sent": "So we've got 24 bytes in total and we reduce this by applying vertical partitioning.",
                    "label": 0
                },
                {
                    "sent": "So we're just using 16 bytes for each truthful information, and we further applied differential encoding and variable byte encoding to further reduce memory consumption.",
                    "label": 0
                },
                {
                    "sent": "In this case, 22 byte for the first value or three bites for the for the second triple information.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looking back at our three data structures, first of all, the working memories were restructured to be self contained and that allows us to swap the complete data structure to the hard disk.",
                    "label": 0
                },
                {
                    "sent": "So we're not using any main memory for this data structure anymore, wasn't more.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, the triple hash set was compressed.",
                    "label": 0
                },
                {
                    "sent": "And be cause these are working memories, nor the triple hashtag are using any references.",
                    "label": 0
                },
                {
                    "sent": "It was a troubled anymore.",
                    "label": 0
                },
                {
                    "sent": "We can also completely swap that rules to the hard disk and we don't have to randomly excess of triples anymore, which allows us to very efficiently store and read data to the hard disk.",
                    "label": 1
                },
                {
                    "sent": "Nevertheless, we also introduced quite a lot of complexity, especially to the codes that needs to be executed on the GPU and to address this complexity as well.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, to also apply further optimizations.",
                    "label": 0
                },
                {
                    "sent": "We also introduced the concept to generate the code that gets executed on the GPU based on the rule files that are that are provided for the reason of process.",
                    "label": 1
                },
                {
                    "sent": "So before we start the voting process, we parse the rule file with.",
                    "label": 0
                },
                {
                    "sent": "Further dictionary encodes single rule elements within the rule file and based on this information we create the files that get executed on the GPU.",
                    "label": 1
                },
                {
                    "sent": "This allows us to reduce loops within the code.",
                    "label": 0
                },
                {
                    "sent": "It allows us to optimize memory access which is very important if you're programming for a GPU.",
                    "label": 0
                },
                {
                    "sent": "For example, you have to manage different kinds of memories and you have to be very careful.",
                    "label": 0
                },
                {
                    "sent": "When you're executing, accessing, which kind of memory.",
                    "label": 1
                },
                {
                    "sent": "It further allows us to reduce the memory paramaters parameters and also to use dictionary encoded values directly within our program code.",
                    "label": 1
                },
                {
                    "sent": "So for example, here in this small example you can see that we are comparing the predicate of a triple directly with Norma number, because we exactly know which number is the dictionary encoded value of the predicate, which need to be needs to be compared to here.",
                    "label": 0
                },
                {
                    "sent": "We have firms are able to provide specific and optimized code for each of the RTI nodes that gets derived by them.",
                    "label": 0
                },
                {
                    "sent": "RTI algorithm so we are able to provide very.",
                    "label": 0
                },
                {
                    "sent": "Optimized code for the GPU.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To test our concepts, we used three data sets.",
                    "label": 0
                },
                {
                    "sent": "The first one is University benchmark data set, where we generated up to 8000 universities was up to 1 million triples.",
                    "label": 0
                },
                {
                    "sent": "We use the English version of DB.",
                    "label": 0
                },
                {
                    "sent": "Pedia was about 400 million triples and we used one more real world data set from the medical domain, which is CTD.",
                    "label": 0
                },
                {
                    "sent": "We applied the RDF S and low DF rules and used a routine a laptop from 2012, 2012 with 16 gigabytes of memory and 1 gigabyte 1 gigabyte GPU.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And first of all, you can see that we were able to do the reasoning on one billion triples on just a single laptop.",
                    "label": 1
                },
                {
                    "sent": "And here you can see the memory consumption of the hash set.",
                    "label": 0
                },
                {
                    "sent": "That was the only data structure that needs to be permanently kept in main memory, and for one billion triples, which is the data set from the University benchmark.",
                    "label": 0
                },
                {
                    "sent": "With 8000 universities we used about 12 gigabytes to store the triple information within the headset on the laptop.",
                    "label": 1
                },
                {
                    "sent": "And we used an average of 6.2 bytes per triple to store all triple information on our laptop.",
                    "label": 0
                },
                {
                    "sent": "Instead of 24 bytes, and if we are also considering the overheads that gets introduced by using a headset, so a hash that always has some unused entries, we are using 9.7 bytes but triple in total we were able to reduce the memory consumption of the hedge set of about 75%, and looking at the complete reasoner process, which also includes the working memories as well as the triples itself, we were able to reduce memory consumption of 84%.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looking at the performance of our reasoning process, we also got really good results.",
                    "label": 0
                },
                {
                    "sent": "For example, the University benchmark data set was computed within a maximum throughput of 4 million triples a second.",
                    "label": 0
                },
                {
                    "sent": "For the smallest value and was about 1.8 billion seconds.",
                    "label": 0
                },
                {
                    "sent": "Million triples a second for the largest data set.",
                    "label": 0
                },
                {
                    "sent": "For DP and the medical data set, we also reached the throughput of more than 1 million people or more than 5 seconds to compare it with other work.",
                    "label": 1
                },
                {
                    "sent": "For example, web PY used 64 machines using MapReduce and they reached maximum throughput of 2.1 billion triples a second and dynamite, which is stream reasoners at once on a single machine.",
                    "label": 1
                },
                {
                    "sent": "Just like our approach and also uses multi core processor reached maximum throughput.",
                    "label": 0
                },
                {
                    "sent": "Of 227K triples per second, so we are much faster than sensors.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Already coming to my end of the presentation, what we did is we identified the resource consuming parts within the RTI algorithm and we tried to reduce the memory consumption and to swap as much data from the main memory to the hard disk without loss of performance.",
                    "label": 1
                },
                {
                    "sent": "This was done by restructuring the working memories by also applying a compression to the used hash set for deduplication and we also introduced some code generation.",
                    "label": 0
                },
                {
                    "sent": "The code that gets executed on the GPU based on the used rules.",
                    "label": 1
                },
                {
                    "sent": "We finally were able to reason 1 billion triples using just a single laptop and reach the maximum throughput of 5 million triples per second.",
                    "label": 0
                },
                {
                    "sent": "In the future it would be very interesting to see if this concept can also be applied to stream reasoning, and it would also be interesting to see if we can add some more expressiveness to the rule language that is supporting.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}