{
    "id": "aayao554jc7fds27krynugfd53hz3a6r",
    "title": "Processing social media data: can we circumvent the Tower of Babel?",
    "info": {
        "author": [
            "Nikola Ljube\u0161i\u0107, Department of Knowledge Technologies, Jo\u017eef Stefan Institute"
        ],
        "published": "Aug. 1, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Social Media"
        ]
    },
    "url": "http://videolectures.net/solomon_ljubesic_tower_of_babel/",
    "segmentation": [
        [
            "Hello, my name is Nicola Lubitsch fabric at the Department of Knowledge Technologist at this Institute and at the Department of Information Communication Sciences at University of Saga.",
            "Today's talk is titled Processing social media.",
            "Later, Kansas circumvent the power of paper, so it's just it's quite a catch it."
        ],
        [
            "So let's see what I meant by that so."
        ],
        [
            "Social media we almost agree is an unprecedented source of social signal, which therefore is."
        ],
        [
            "Very interesting for us as a source of measuring variation.",
            "Measuring variation that happens for social reasons and we consider actually the linguistic variation that we're also interested in to be just for one one type of general social variation."
        ],
        [
            "And when I talk about variation I talk about version inside cultures or languages, but also across cultures and languages.",
            "And actually I consider this to be a continuum, so there is no distinction actually between those two."
        ],
        [
            "And when we talk about the type of people, of course the tower Babel stands for this variation.",
            "So for the difference between people who that are encoded either in the different behavior and different language as well, and the Curse of Babel if transferred to the two of us who do the research in social media."
        ],
        [
            "So it turns out that we want to research this variation that we're interested in, so we are interested in the variable, and we want to collect as many explanatory variables as possible to understand the phenomenon of choice.",
            "We actually have to process the data to get those variables, and this is very hard because of the same variation that we're interested in.",
            "So this is a Catch 22 game we want to process data to be able to analyze it, but it is already complicated process for the exact reason for which we are interested in.",
            "The data in the first place."
        ],
        [
            "So actually what I want to try answering during this talk is kind of circumvent this curse by performing either model adaptations or model or build our models on predictors that are stable across these variations, either minor or measurements."
        ],
        [
            "So let me give an overview of the talk.",
            "So the talk actually comprises a two part."
        ],
        [
            "The first part is focused on linguistic processing, so we probably all know what we consider under linguistic presence is our task like part of speech tagging, passing, named entity linking, etc.",
            "You probably all know how hard it is to process constant data, so one of the first measurements of the complexity of the task was done by Gimpel it.",
            "Oh so they show that when you train and test and also Journal of course you're part of speech.",
            "Accuracy is very high.",
            "It's not the 7%, but when you move to another domain, specially nonstandard data like Twitter, you get an error increase of five times higher, which is quite a problem.",
            "So in this part.",
            "I will try to answer how can we adapt to such extreme differences in the target domain, and Furthermore is the path not to cover all of this minor variation as I call it inside the language, but also major variations across the languages.",
            "So can we consider building cross lingual models or models that will be able to process data regardless of the language that we use?"
        ],
        [
            "And in the second part of the talk, I will focus on the similar problem, but in another area.",
            "The area of user profiling, so predicting various traits of users on social media, lighter gender, user type, etc.",
            "So we need these data enrichment procedures all the time, especially because we want to investigate the dependence of these variables with another with other variables and traditionally how we do it is we rely on the linguistic content, so we analyze the linguistic content and predict where the user is male or female depending on what they talk about or how they form.",
            "Linguistic utterances etc.",
            "But here I'm investigating whether we can build predictive models that won't rely on linguistic signal only, but a little signal which is more general and which could be easier to adapt across languages or across cultures.",
            "So let me start with the first part."
        ],
        [
            "Which is the linguistic processing.",
            "So here I will just set the stage with a very brief history of text processing.",
            "So we have two phases in text processing.",
            "The first phase is actually that we had knowledge rain systems, so the systems relied mostly on handcrafted rules and handcrafted knowledge, and these systems were very hard to adapt to different domains, different languages.",
            "So in this continuum it was very hard to move to any direction actually, and this was the dominant paradigm until the 9th."
        ],
        [
            "So in the 9th, is the switch to the data driven systems into the data driven paradigm.",
            "As you know.",
            "And instead of writing rules, we just get datasets in which we have a specific problem solved and we will build predictive models from this data and adapting to different domains for different languages or cultures is easier.",
            "You simply need data in the target domain, but they also have the options of the main adaptation like unsupervised domain adaptation and."
        ],
        [
            "I in this second paradigm I roughly see two phases or maybe a front face is just coming around the corner, so the first phase was for sure is something that I at least called the Wall Street Journal error where people were using small datasets both for training and testing.",
            "They were actually not so much interested in how they models perform out there, and there are some out there.",
            "This is the typical in vitro evaluation and with the.",
            "With the social media revolution that came and actually help, it became very interesting for the economy as well.",
            "At that point, it was measured.",
            "Actually, these models are heavily underperform in these domains and then the the domain adaptation methods became much more important than previous.",
            "Although of course there was previous work on that as well and there might be a new area or new stage in this.",
            "The level of a type of processing which could be due to neural models.",
            "We are closer and closer to do proper cross lingual processing, so having single models that are able to process data in different domains and different languages."
        ],
        [
            "So the now we talk about how to approach processing minor variation on the mind of preparation and consideration roughly inside a single language or across similar language is one of the variations is nonstandard EXO, the text that is used in user generated content so."
        ],
        [
            "Social media, so there actually three ways of doing this.",
            "The first one is build a normalizer so we will.",
            "You will build a process which will transform the text from the original form into one form that will be easier to process with the standard pipeline, something we also frequently called the Wall Street Journal pipeline.",
            "So those pipelines that are.",
            "Both are journalized pipeline."
        ],
        [
            "Another option is doing domain adaptation, so whatever you whatever type of processing you want to do, for instance part of speech tagging, you simply adapter model to the target domain.",
            "You don't normalize the data 1st and don't apply the standard model but adapt the model.",
            "The two primary ways of doing this, the supervised just add annotated data from the target domain on unsupervised wear.",
            "Nowadays mostly the distributional model someone most well known that.",
            "I will talk about that little bit later on."
        ],
        [
            "And there's of course a further way joint processing.",
            "So this is in general very relevant for natural language processing, as we mostly process data in a pipeline.",
            "So we first deal with one level of abstraction, and then we move to another one, and we propagate the error.",
            "As we all know.",
            "So by doing joint processing, for instance, doing normalization and positive pitching at the same time in a single process, we can gain specific.",
            "Increases in accuracy or whatever evaluation metric we use."
        ],
        [
            "So first I will talk a little bit on our work on normalization.",
            "So what we do we we we do normalization and rather straightforward fashion.",
            "We consider normalization just to be a translation process.",
            "So we want to translate from the non center data standard data and the trick we regularly uses that we don't consider to translate words but characters.",
            "So we generalize much more by translating character by character and we use the best known paradigm.",
            "The phrase based physical machine translation you might have heard of Moses the system.",
            "Which does this very well.",
            "So what we start with is, are these utterances.",
            "So this is an ensemble occurrence and we consider this to be a sequence of 10 or 12.",
            "As we learn how to transform this into this, and given that we use characters, we generalize a lot, so we know that at some point we also want to insert spaces in the process.",
            "And we have the tool on GitHub for that and we did experiments on Slovene historical slowing data and Slovene social media data, but also on the other types of data.",
            "For instance, Swiss that all data and the same paradigm works very well for any type of language variation.",
            "What we've learned from that is that given that you have a noisy model here, so you have a translation model and the language model.",
            "So we want to have the probabilities of specific constellations of transformations.",
            "And we want to have the probabilities in the target language and the simplest trick to do is just to gather as much target language data, which is the standard data is possible and also use multiple language models and let the optimization process to learn how much it should rely on what source of information.",
            "So we regularly get improvements by using multiple language models.",
            "They can even be in different dialects, so there might be variation between the target domain and the language model optimization processes learn.",
            "How much to listen to specific data sources?",
            "And there was also another question whether we should try to normalize token by token.",
            "So word by word or whether we should consider whole utterances when normalizing and to our surprise the result was that as long as your data is not very non normal, as long as it doesn't deviate from the norm significantly, you should actually do token backed up because in that case we have a much smaller search space and the information that you get from the context is not that relevant.",
            "As the increase in the search space you get with wider context.",
            "So these are the results.",
            "We can have a look just at the very reduction.",
            "So we have four datasets, so we have 18th 19th century data and Twitter data which is very nonstandard and which is slightly nonstandard.",
            "All these measurements are character level, Levenshtein distances, normalized ones, so this is the lower the better.",
            "So this is the evolution if you don't do anything on the text is just measures the amount of transformations that needed to be done.",
            "The baseline system just learns token transformations and applies the most frequent opens summations.",
            "By generalizing with season three, you already get very drastic improvements, especially in the domains where you have a lot of work to do, and by adding multiple language models and tuning them you get further improvements and to their adduction.",
            "If you compare our best system with with the initial phase where we didn't do anything, we get this significant reductions from 55 to 93%.",
            "And if you compare best performing systems with the.",
            "Strong baseline, I would call it.",
            "Then we had better reductions of further two 279%, so the reductions are quite high and the system is quite simple further."
        ],
        [
            "Movie took part in in a shared task on translating historical Dutch.",
            "They called it translating organic wallet, normalizing historical Dutch to modern Dutch, and the interesting thing is that we got the 1st place regardless of many neural machine translation models being part of the shared task as well.",
            "In the normalization game, there's not enough data for the neural machine translation program, which is a very strong one in which got huge improvements in machine translation in general.",
            "To outperform this simple models that actually don't.",
            "Abstract from the surface forms, but just learn how to recombine surface forms to get the target domain date but used label data.",
            "Yes, of course, so starting points for the statistical machine translation or paradigm is to have parallel data.",
            "So you have such a trances.",
            "Such pairs of utterances and learn the translation model from that.",
            "But how large Dave are roughly?",
            "20,000 thousand second operating text.",
            "But we also had previous experiments on having a data system, just 1000 tokens for instances for which we knew that they were highly infrequent in the non center data, and then we normalize them and just having 1000 tokens.",
            "It learns quite quickly whether transformations are that you should be doing and then having huge language models because they're cheap.",
            "I mean there for free.",
            "Actually there is data out there in the standard language written, then you can really.",
            "Just use this transformation to produce hypothesis and then for the hypothesis to be pruned and two for the process to pick the best hypothesis given the target language or the normal language standard language.",
            "So this is for the normalization."
        ],
        [
            "Fast and if we go to tagging.",
            "So here I want to showcase our adaptation over standard Tagger on Slovene Twitter and something that I did 2 weeks ago with Barbara Plank and the colleagues on adapting on using their neural Tiger.",
            "So they have this Alice team Tiger, which is considered to be state of the art.",
            "It combines version character embeddings so it generalizes very well and has really good results.",
            "We compared our results that we obtained with.",
            "Additional random fields modeling techniques, so this is just the traditional way of modeling sequences or doing sequential predictions, and both systems used domain adaptation through distributional information, so we use Brown clusters.",
            "So Brown clustering is a technique where you cluster words into hierarchical clusters given their context.",
            "So you want to cluster together words that occur in a large data set in similar contexts, and you do this on standard and non standard data.",
            "With the idea of updating classes like these, so this is the drawing on the swimming pronoun I yes yes yes yes and yes, so this is a hard hard clustering approach.",
            "So one word is adjusting one single cluster, but there is a hierarchical portion of the algorithm, so you do get some generalization from that as well.",
            "And this is the point, is a true example of a cluster, so it manages to realize that all these word forms actually occur in their similar context, and then you enrich your data presentation so that either when yozora US is in there, you encode the path through this binary tree, where these words occur, hoping that when you once occur, the yes or yes form in the context, which is quite.",
            "Informative for quite probable for personal problem that it will actually realize or choose the these hypothesis.",
            "This is actually personal pronoun and very better system is worth embedding, so probably know about them.",
            "So these are the results of the assistance like Word, Two, VEC, Club plastics, etc.",
            "So those distributional.",
            "Nation encoded in a vector of fixed size like 100 or 300 dimensions.",
            "So you encode the whole information of the context in which red occurs in 100.",
            "Then actions.",
            "And when we call, when we look at the results.",
            "So the first part of the results actually.",
            "Testing on standard data so we have a system trained on summer data and then we test it on Saturday to so these are the state of the art results for Slovene and we see the neural system actually does very similarly actually underperform slightly when it doesn't use distribution information from a huge huge collection of text.",
            "And then we give it the huge collection of text to learn the initializations of the word and character embeddings.",
            "Then it is very similar when we move to non center data.",
            "So this is compara bulto.",
            "The results of Kim palatal.",
            "Who trained on worlds?",
            "Adrenaline tested on Twitter.",
            "You see a huge increase in error, so this is what happens if you take the standard model and by Tomlinson our data you don't get, you get really bad results.",
            "So this is where we would.",
            "Something has to be done.",
            "And when we compare it to the neural system it actually performs very similarly.",
            "If it doesn't have any additional information except that standard training data.",
            "But if you give it distribution information which consists of web data and Twitter data.",
            "Then it's significantly outperforms our system, but our system never uses this additional information does this is not.",
            "This is not a fair Harrison.",
            "But here we can see already where neural models can be better.",
            "They can generalize better if we move further more too.",
            "To the.",
            "Experiments where we use.",
            "Training data in domain training data.",
            "So we have some.",
            "We have some Twitter data that we have tag and if we retrain the models on in domain data so on Twitter data we get back in the game so our results are much better than if you just don't do this supervised adaptation of the model.",
            "And if we include distributional information.",
            "So this is a fair comparison.",
            "So in this case our model uses the Brown clusters, the biased model of system uses the word embeddings and again we get similar results although.",
            "The by the stem does perform a little bit better.",
            "At this point we didn't do any sophistical significance testing, but this could be on the verge of statistical significance, and this makes sense because they generalize better.",
            "They have much more information on the word information from those classes, but they also have character embeddings as well in the system so they can run specific rules on the level which we don't exploit.",
            "And but in the end, if you combine standard and nonstandard data to engage and combine them into building the best possible model plus all the additional information we actually get again very similar results.",
            "So here the story could be that actually both traditional methods like CR S and the new methods in case of natural language processing adapt to different domains in a similar manner, so we cannot gain a lot here from neural models, which was actually shown mostly in natural language processing.",
            "Sorry, how do they perform in time in terms of time in terms of time and take it over.",
            "Train this system.",
            "Of course it takes much longer, but I think decoding so tagging is a little bit faster.",
            "In this case it might be also due to my Evelyn my implementation here, so the CRF, given the significant number of features that I use, is slightly slower in applying the model."
        ],
        [
            "So let me go over to mixture of these two problems.",
            "So some work that we currently do with Katia Zupan is that so we did all this adaptations for Slovene, but there's so many languages, many other languages and many other domains on which this still has to be done.",
            "So we asked ourselves a very natural question.",
            "If you have some time resources to do some supervised adaptation.",
            "So annotating data that we will add to the model, or that we will.",
            "Incorporating the model, whether we should invest in producing normalization data or tag data on the task of part of speech tag.",
            "And our hypothesis is from our previous experience, what we have expected is that if you have limited time resources that you should actually opt for normalization.",
            "So breakthrough trainer Normalizer, adapted data normalization and then part of speech tagger through the standard system.",
            "But if you have significant time resources that then you should probably invest your money or your time into producing tagging, training data and improve your Tiger.",
            "So we did something.",
            "Initially."
        ],
        [
            "Paramount, so these are the experiment on the 18th century, later set, so this is historical data actually, not social media later, but they they tend to have similar phenomena.",
            "So on the X axis we have the hours of annotation, so this this is here we measure the accuracy of the tagging process given the hours that will include that we invested in producing additional resources.",
            "These two here are actually two curves.",
            "These two curves are the curse curves.",
            "If we invest into normalization.",
            "And these two curves that picked how accuracy.",
            "I'm corresponds if we invest into taking data and we look always at the models that do unsurprised normalization through Brown tagging.",
            "Unsupervised adaptation through Brown clusters on.",
            "But it's not so important.",
            "The most important thing here is that on 18th century investing in normalization, so transforming this old forms into new forms and then applying just the standard model works always better, at least on this scale that we managed to have time to produce manual energy.",
            "The data always performs better."
        ],
        [
            "If we move to 19th century services having closer to what we have expected, if you have a little amount of time, that new probably should opt for normalization, but if you have significant time resources, you should opt for improving your tech data.",
            "And if."
        ],
        [
            "Move to the L3 domain which is non standard printer later than we again see a similar phenomenon.",
            "So with limited time resources you might be better off with doing normalization, but if you have significant time investments I've able capable of them then you should invest in supervising your thinking model Direct."
        ],
        [
            "And I would just want to finish this linguistic processing part with some points on cross lingual processing.",
            "So this is a new era area of research with limited results.",
            "So at this point I was talking mostly on something that I call minor variations.",
            "So roughly in language variation.",
            "But what if we want to have a model that we train on on floor in hand?",
            "We want to apply to Korean for instance, so this would be pretty sure this would be major variation.",
            "There is a lot of work being done at that the.",
            "2 interesting papers or lines of work.",
            "First of all, the the researchers from Google and what they do.",
            "They build Palestinians that reads text by bite, so there's no preprocessing of the text.",
            "I just read bye bye bye and they what they return in the task of part of speech thing or named entity recognition.",
            "They return spans and annotations so they just say that from bytes from byte want about six there is data which is a person for instance, so they don't rely on any preprocessing, which is nice.",
            "Second, what they do they feed data from different languages through the same STM, and they actually improving.",
            "That's on each separate language.",
            "They perform better if they train one single model, folder, language, languages.",
            "So there my obviously there has to be some as a college here.",
            "Positive interference across languages.",
            "So by having more training data.",
            "But it obviously improves the tagging or the named entity recognition on German because they always, with some similarities between these lines.",
            "How many know?",
            "What about if the languages are very different?",
            "But they actually didn't measure which language improves which which which languages improve which languages.",
            "They just did experiments on 70 languages in case of partners pitching and 10 languages.",
            "I think in case of mended recognition and they in general observed an improvement, but they didn't do some language ablation.",
            "So removing specific languages and measuring which actually languages do is positive interference.",
            "I would assume the local languages.",
            "Actually, in the training data they had both in the European languages, also Asian languages, but they must have helped between each other, not across.",
            "Probably this would be my expectation.",
            "There's another line of work on training multilingual recommending so we already talked about this with embedding so encoding distribution information in dense vector representations.",
            "So the researchers for from Facebook there for instance trained there plus text tool on almost 300 different languages that are covered through it Wikipedia.",
            "But more interesting thing that they do.",
            "Then they're trying to transform those further bindings so those embedding spaces so that there are.",
            "As similar as possible between languages by having some lexical supervision and transforming those spaces so that the lexical similarity is maximum.",
            "This of course can be used in multiple ways.",
            "If you have embedding spaces embedding spaces that are similar across languages, in theory you could train named entity recognition tool on Slovene and then just by feeding representations from English, Tag English data with naming.",
            "On the task of named entity recognition, and this already works roughly on this error on this level of abstraction regarding part of speech saying the results are still quite bad here I just want to show something that I do."
        ],
        [
            "But last week actually to show.",
            "Guess mostly my work, so we had a task, extra lexical prediction task so we had to predict the how abstract or how can create the specific word is in our case in creation.",
            "This was done on a national project so we had 3000 queries on which we had never had Likert scale from 1 to 5 and we had the mean of the.",
            "Response is obtained from people on wall where they put each word on the objectives, completeness, scale and if we use monolingual further beddings is features.",
            "So I think where the metrics from the creation web then we end.",
            "We do cross validation week on, so we build a regression model on the problem and if we compare the continuous values that we have predicted and those that were given by people, we get couple quite high.",
            "Correlation of 0.81.",
            "So this obviously works on single languages, so you can predict various lexical features from robotics on single languages.",
            "But then I tried using those cross lingual models, training my model on creation and applying the same model on all these other 77 languages with quite good results.",
            "For now I don't have any.",
            "And district evolution that I can."
        ],
        [
            "I'll show you a list of words in Slovene, English, and German.",
            "I think so.",
            "This is the start of the list from the most abstract terms, and this is the end of the list of the tail of the list.",
            "The most the most concrete terms, so you can see at least detailing the.",
            "Add in the pale of the list looks quite well and it's similar in all the other languages, so cross lingual processing is still in early stages, but there are significant improvements being performed on actually on a monthly basis.",
            "So so much about linguistic processing, and now I'd like to move."
        ],
        [
            "To the other part, which focuses mostly on user profiling.",
            "So here I want to show 2."
        ],
        [
            "The profiling experiments.",
            "The first one is done on identifying user type, so we wanted to discriminate between private and corporate users on special in Slovene tweets.",
            "But we also played by with creation tweets as I will show later on and the main comparison with it.",
            "We compare the boat bag of words as a strong baseline with some language agnostic features.",
            "So I told already we were interested in building looking for models that would be easier applicable.",
            "Across many languages and in this research we did experiments on time, robustness in space, robustness.",
            "Under time robustness I mean that we train the model on the on time scales on the time span until 2015 and then applied it on data that was published later on.",
            "Of course, we're not using the same user data on both sides and space robustness we consider to be again, if we have minor variations across similar languages, whether we can train a model on slowing data and apply.",
            "On probation and the set."
        ],
        [
            "An experiment was in general prediction, so this was done on the twisted data set which contains user information like gender and their psychological traits on users.",
            "From Twitter, six different languages.",
            "And here again we compared the bag of words baseline with some language with the same language agnostic features as in the first research.",
            "And we also did experiments on cross lingual gender prediction.",
            "Some training on German testing and budgets.",
            "So."
        ],
        [
            "Let's go first over the language agnostic features.",
            "So here I just list the feature types so we have 5 feature types.",
            "In general we had more features than I listed here in both researchers, so the first feature type is the percentage.",
            "So here, which simply encode the percentage of Tweet that have a specific phenomenon which is not strictly related to the language points.",
            "The percentage of tweets that contain URLs.",
            "You could imagine that this is a very strong feature for discriminating between private and corporate users, for instance.",
            "But it also proved to be quite informative for discriminating between male and female users as well.",
            "Another feature example is the percentage of tweets being replies.",
            "Again, you would probably consider women to reply more than men and private users to reply more than corporate users.",
            "Or maybe not the second type of features is just the mean.",
            "So the actually the next three types of features encode distributions across all the tweets of a specific user, so we have the mean, the median and variance.",
            "And types of features are like the arm of posting, so there's a continuous variable, roughly from one from zero to 23, and another one could be the length of the text.",
            "So yes, of course.",
            "In the week.",
            "So yeah there was.",
            "This was one of the features as well and we had also binary feature where the that was published during the weekday or at the weekend as well.",
            "These are just examples of features are not also features and how many were so in case of in case of predicting corporate and private users we had 28 features isn't I'm not mistaking in case of predicting or discriminating between male and females.",
            "I remember that one we had 51 features.",
            "Chris Brown picture it's more difficult problem.",
            "I guess my feeling then yes, and I must be Frank.",
            "I mean the gender prediction task came later, so then we had the ideas of additional features to be honest about it, yeah.",
            "And so, besides these three types of feature that encode the distribution, we have the user level features like the ratio of friends or followers or specific user.",
            "So you would expect for corporate users probably to have more followers than friends, probably.",
            "Or for instance something that we added by undoing gender prediction the intensity of the red color component in RGB color in the user space background.",
            "So there is information on what type of color the user uses in the background in the text, etc.",
            "So we exploit this information as well.",
            "So these are the feature types."
        ],
        [
            "And let's go to the results.",
            "So on discriminating between corporate and private users are baseline, which is the most frequent class baseline.",
            "So in this case we always claim that the correct class is the one of the most frequent plus, which is the private one.",
            "It's 64, then these are all rated F once, so there's RF ones, and these are weighted difference given the number of instances of each class.",
            "If we apply language agnostic features we get.",
            "Of course, is very large improvement from our weak baseline, but the bag of words model still outperformed the linguistic features.",
            "Instead.",
            "What we want is for the baseline is majority.",
            "The baseline is the majority.",
            "Yes, wise, corporate zero.",
            "I don't do something.",
            "The majority baseline the dummy classifier over his claims for the correct answer today would be the more frequent plus, so it never.",
            "It never votes or it never gets a return on the Corporation.",
            "So it always claims that it's a private class.",
            "So the F1 is 0 because the I mean this is it's F1 measure on the corporate class, because it's never.",
            "So you see, how do you?",
            "OK I methodological, I don't understand.",
            "So you define corporate as the target and then do all the measures precision, recall.",
            "And it's a new problem when you define private stuff and then you combine these two F1 measures by doing this weighted F1.",
            "So which takes into account the number of instances from the corporate and from the private.",
            "Group in in your data set on the level of F1 destination.",
            "No, it just combines the reference.",
            "OK, OK, thank you I didn't.",
            "I understand that methodology.",
            "And if we apply the bag of words model, then we get an improvement over the language agnostic features.",
            "And of course if we build an example which takes into account the probability distribution among classes of each of the of the classifiers, then we get some additional improvements.",
            "But you actually the most important thing is that the bag of words works inside the language better than the language model features."
        ],
        [
            "Sony Bank of words.",
            "A sample is assemble learning, which means it uses the model.",
            "How about merging the features?",
            "I haven't done that, but you understand.",
            "Of course, the bag of words has like 5,000,000 features happening because there is a lot of text there and a lot of many different engrams and the language agnostic feature has on the.",
            "Yeah, the question is whether these features will get lost in this huge huge huge dimensional space.",
            "So I would assume that the sample is a safer choice here.",
            "Yeah, yeah for sure.",
            "OK, and but this is where it gets interesting, so we checked for time depends in space dependence.",
            "So we trained on on users or user data before 2015 and tested on the account of different users that was published after or on or after 2015 and what we observe here is that actually are language agnostic.",
            "Features lose more than the bag of words features.",
            "So our assumption was that it should be similar.",
            "But obviously the topics that are covered do not change so much as much as the.",
            "Behavior of the users changed before 2015.",
            "Article 15 and this is not we roughly observe, and we simply observe what's going on.",
            "Social media, the type of users and their interaction.",
            "Changes retire on the after 2015.",
            "You took different users, different users.",
            "I split users in half and for what some users I took only data before 2015.",
            "Of course, if there was an update and if not then I would just move it to the other cloud category and try there and for the other set of users are two data only published in 2015 or later.",
            "Even.",
            "Because what you are claiming now is that users change behavior.",
            "Now the question is.",
            "If those users who are now changed in behavior, how did they behave before 2:15, right?",
            "This is it like they changed behavior?",
            "Or is it just different users?",
            "Yes, yes, that's all set then.",
            "That's the question.",
            "Yeah, absolutely.",
            "Kind of human psyche is changing and how they approached with Saint Francis.",
            "Left wing users left Twitter handwriting users account if it.",
            "Once it is what we recently observed, another Microsoft and the second part is the space dependence as we call it.",
            "So this is when we train the model on Slovene Dayton applied information data here and our assumption was that OK back approach should work because those two languages are very similar.",
            "Furthermore, in bag of words, vectors and choose between this character.",
            "So these models generalize quite well on features like using URLs or.",
            "Or specific features that does happen with features, but here again, the loss on the linguistic features works greater than on the back of roots.",
            "So in this case actually we are obviously we actually observed that in case of similar languages the back of birth still out performs at least the length diagnostic features how they are defined right now.",
            "Of course, both of the models still outperform the most recent classes this line.",
            "So in this case we have a balanced data set, so if one is very low.",
            "As one would expect.",
            "So this is on the use."
        ],
        [
            "Scientification and justice.",
            "A quick quick feature analysis so."
        ],
        [
            "What we learn from this and from this line of research by looking at the features so far its users reply more mention more other users favor other tweets post in various hours so the variance of the posting time is greater and post tweets of various lengths of the variance of the tweet like is also greater.",
            "On the other hand corporate users use more URLs pose during working hours.",
            "Post earlier in the day and post longer treats.",
            "So these are the best performing features.",
            "So let's go over."
        ],
        [
            "But to the General prediction task.",
            "So in this case we had the data set in which we had users tweeting in six different languages.",
            "So these are the six languages and this is the number of instances per language.",
            "So we can see that in the office today to say thank you Ben for producing the data set.",
            "That is quite different number of users if you apply the most frequent class baseline.",
            "These are the results, so we can see in German the data is quite balanced by the impeller in the data set is quite imbalanced, so there's if I remember correctly much more female users than me.",
            "This column includes our results on something we call in language bag of words.",
            "So in this case we do N fold cross validation and we build character file, grams, bag of words, models and test them on the same language.",
            "So this is the this is these are results if we train and test with the symbol.",
            "This column encodes information or the results.",
            "If we do cross lingual back offered.",
            "So we train a model on one of the languages and evaluated on the remaining.",
            "Languages and we just take the average of the results.",
            "So here you can see that in the cross language bag of words, the results are much lower as well looking set, but still quite high and in most cases still throughout from the most frequent cause baseline or in this case from from random classifier.",
            "There are some researchers were very surprised with news that this result, because they did some similar experiments and they didn't get so such good results.",
            "Then we looked into the data and realize that actually more many of these tweets tweet in English as well.",
            "So English language was proven to be actually what they did.",
            "They got rid of all the tweets for each user who didn't use, not the specific language was used, but actually what I was more interested in.",
            "Users that mostly treating these languages.",
            "And if there is additional signal from their language production which is not in this language, I was still OK with that.",
            "And this is actually what help quite probably helped a lot.",
            "And while we got results much better than the baseline results, so this is on the bag of words."
        ],
        [
            "These are the results if we apply our language agnostic features.",
            "In this case, these are the training languages and these are the testing languages, so this is the model that we trained on German and developed in German.",
            "And we publish in bold all the results will be better than the bag of words, either in language or cross language.",
            "Bag of words.",
            "So as you can observe on the diagonal, of course, many times we are not better than the bag of words as you would expect.",
            "So if you train and test in the same language you are.",
            "Bag of words works better, but if we move across languages then a language Gnostic features perform better.",
            "Furthermore, the variance of our results is much much lower.",
            "In case of language agnostic features then on cross lingual bag of words.",
            "In case of cross building will take over as the variance between the six classifier is 10 times higher than that of the results that you can see.",
            "Here you can see that in most cases the classifiers perform quite similarly and the nice thing of course is that they are still quite far away from the most frequent class based.",
            "So these models could be applied to different languages and you could get still much more signal than by claiming higher randomly.",
            "What's what, or just picking the more probable class or?"
        ],
        [
            "And yeah, we did some analysis of features here.",
            "OK, so the blue is blue in this Gray is actually read on my screen.",
            "So what we did actually given that we used an SVM for that before.",
            "So we scaled the data.",
            "We use the scalar for the data.",
            "We just calculated effect sizes between the germinal between the male and the female users by calculating the difference of the mean of these two distributions, the male and female distribution.",
            "In that case you could have a positive value.",
            "This means that the average value is much higher among female users.",
            "If there is a negative value, this means that actually the average value is much higher among male users, and these are the six different languages and what we were interested in.",
            "So which we showed previously experimentally that this works.",
            "These features work.",
            "There is some useful information in these features, but we were interested in seeing whether this is this condition by analyzing specific features, and then we rank the features.",
            "These are again just the top 20 features by the average rank.",
            "Of a univariate test for each language.",
            "So these are the best performing feature across all the languages.",
            "So the best performing feature is actually the percentage of emoji of tweets that contain emojis, and as you can see in all cases this is much more frequently much higher.",
            "These features are much higher in among female users than month, maybe users, and as you can see if you're looking at the whole list in quite a number of cases, the whole whole rows.",
            "Encoded with the same color, the intensity of the color corresponds to the number of course.",
            "So in for the first three rows, you can observe that these features, obviously informative or they aim at female users, while the next two are more informative than male class.",
            "For instance, the percentage of tweets that use that contain URLs as one would expect we want remained one to reference to other people's work, and the percentage of.",
            "This is a weird one, so the percentage of three separate you the percent from Niaz device is much higher among men than women, something that I wouldn't expect.",
            "But this is pleased with this data set says.",
            "Maybe because of this incorporate, of course we didn't discriminate in this data set.",
            "I mean, this is actually not my data set, so there was not discriminate in private and corporate users, so it might be that actually more Mail users for corporate users, I think before incorporate private it was like URL stand, more characteristic of absolutely and this is what we would expect it to be, just that.",
            "It's more like so.",
            "That's what you observe in here is not due to gender, but you do that kind of task.",
            "This will check yes, especially we have gender encoded in this living data set as well, so we should investigate this into something they disagree.",
            "We can remove the corporate users and just have fun certification of that in some social science, say like why men ask so many questions.",
            "Confused.",
            "Is that I think the high check for exactly the question mark raise it.",
            "The question yeah, the percentage of tweets can consider consistent of having?",
            "Since then is regularly Mail features.",
            "This is somewhere interesting and of course the 2nd and the third one.",
            "The mean order retweet count.",
            "So how frequently they retweet and how much red they use in the background was also informative of the general.",
            "Actually there was some research done that used only the six colors you can define on Twitter to build language independent.",
            "The general predictors, so I just use the information from $6 to prepare.",
            "This is a Mail user review which worked a little bit worse than this one because this is much more information, but it actually worked quite well.",
            "Of course in different datasets or one has to have quite some grains of salt there always.",
            "So did this data set have the neutral gender as well, like we don't know everything?",
            "Is there never follow the binary classification, either male or female, or we don't know, so we remove it from the date.",
            "This was their methodology and the data make female in the other day that was collected by a questionnaire from the users.",
            "Or no, no.",
            "Actually, they're interested primarily in the psychological trait of users, so they were looking for phrases in which people define that they took a psychological test and that their psychological traits are these in these.",
            "So this is how they collected the primary set of users psychological test exactly that they took at some point, and they reported about it.",
            "So they caught on this because they were primarily interested in identifying psychological traits from Twitter data and possibly also across languages.",
            "And then the human annotator went through all the users and define specific users being male or female, use their name, the picture and their content to define that no, no.",
            "The psychological traits I, I'm not sure.",
            "Some of them are online questionnaires and you just fill it out and then you post your result on Twitter because you think it's funny.",
            "And then that's how they automatically they collected the.",
            "The keywords you know from the results of the tests are, I think 4 letters.",
            "Is it that in each letter specifies?",
            "I think about your personality.",
            "There's the psychological test of how much introverts and extraverts.",
            "Yeah, 4 dimensions are pink and visual encoding.",
            "Yeah, I mean it's just the psychological model.",
            "I just have one comment in Slovene where we were able to distinguish between private and corporate and their gender.",
            "We saw regularly that males always are closer to corporate users and females closer through the little private users.",
            "So even private mails they behave as corporate users, private females.",
            "They're the typical private.",
            "That also probably wouldn't have corporate users here, right, yeah.",
            "Producers to report on their personalities, but probably not there.",
            "OK, so I want additional support, probably interesting figures, that's wild female female users use more emojis, emoticons, some offerings by men.",
            "So this somehow follows that men's slower adapt to new ways of communication.",
            "So All in all, the languages emoticon service model among men.",
            "And of course the location of the user is more shared by major system.",
            "So this is also highly informative of the main male gender.",
            "Which you would expect there.",
            "Now I'm going to define rhyme in this space because this is so important.",
            "Yes, OK, and at some point with this we also observed some differences.",
            "Of course between these languages and we were interested whether the differences between languages, whether there is some structure, and whether it follows our expectations on the cultural differences between the speakers of these 6 languages.",
            "So what we did, we represented each of the languages or the cultures as a vector of 51 dimensions, and these values are actually those features.",
            "Taxes and we took those and then we did the clustering over those six data points.",
            "So we have to be used in agglomerative hierarchical clustering, just the most."
        ],
        [
            "Come on the end of end organ we got from this pattern is active this and it's quite nice.",
            "I would say because it really pulls off.",
            "We were expecting that the Portuguese and Spanish users are the most similar ones.",
            "Then the French join in, then the Italian at the same similarity threshold.",
            "the German and Dutch ones merge and everybody merges in a big company, so this is an interesting side effect.",
            "Side results.",
            "Inspection.",
            "There's of course a lot of things that we could do."
        ],
        [
            "So just to conclude, on the two main topics.",
            "So first in the first part you're talking about linguistic processing, so we discriminate within a minor variation in major variation.",
            "So minor variation is roughly inside one language or across closely related languages.",
            "So in this case we've shown that unsupervised adaptation, primarily by using distributional information, can can.",
            "I get too far, especially normalizing data, so just trying to transform the data into a standard form also can help, especially if you don't have a lot of resources at your hand.",
            "For major variation, we show that mostly supervised supervised supervision today to two annotated data has to be done to get reasonable results.",
            "We're still not there to be able to build such representations of words or sentences so that we can move across languages.",
            "Online and on the other hand, there's a lot of data like in machine translation.",
            "We've shown that the neural machine translation models obviously encodes the meanings of sentences quite well and decode them on in the other language, but promising directions for the major variation is multilingual learning, so feeding a different data from different languages to the same network.",
            "We've seen positive influence results there, or multilingual representations apply that we talked before, so you really should come up with representations that can be.",
            "A shared among all the libraries on the other topic of user profiling.",
            "For minor variations we shall.",
            "We show that bag of words baselines are still stronger than our language agnostic features as they are defined right now, but for major variation, language agnostic features do outperform backwards, at least on the gender prediction task.",
            "And there is a lot of work to be done on two on both of those fronts, especially in user profiling.",
            "I would say so.",
            "First of all, our features server is fixed, so we.",
            "I'll get over Privy to perform a specific measurement, and then the hope that the distribution of that variable in the second language or in the second culture is the same.",
            "It's roughly if the if more than 45% of the tweets contain URLs that this product corporate users, we could simply do feature adaptation or generalization by calculating the part of the distribution which which encodes more, probably that that the user is.",
            "Operating product so we would need to adapt features for different distributions in different languages in different cultures.",
            "And of course there is also much more data on social media on each user and we should use that.",
            "We shouldn't use only only this 51 features or only the signal that the users tweet.",
            "There's also multi model signal.",
            "There are pictures, images in this.",
            "Probably the next piece of information that we will add to our language.",
            "Independent general predict it's the images that people either share or the profile pictures that they have the correct.",
            "There's a lot of work to be done that it should be done.",
            "In my opinion, because there is too much variation going on social media that we are on one side interested in on the other side, we have to process the data as much as possible before making interesting inspections."
        ],
        [
            "Just a piece of acknowledgment, so most of this work was done inside the youngest project with the people listed here.",
            "That official was them in their principle investigator of this project I used men."
        ],
        [
            "Datasets these datasets would have a primarily variables for me because of the cloud infrastructure, which is very important in my work."
        ],
        [
            "And there's a new venture that we are just starting to play in.",
            "It's the adventure of hate speech and unacceptable this course, so we just got a new project in which we research socially unacceptable build discourse in an interdisciplinary way.",
            "So we combine technological, social, logical, linguistic, and legal perspective of the same problem in the investigation of this highly, highly relevant."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello, my name is Nicola Lubitsch fabric at the Department of Knowledge Technologist at this Institute and at the Department of Information Communication Sciences at University of Saga.",
                    "label": 1
                },
                {
                    "sent": "Today's talk is titled Processing social media.",
                    "label": 0
                },
                {
                    "sent": "Later, Kansas circumvent the power of paper, so it's just it's quite a catch it.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see what I meant by that so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Social media we almost agree is an unprecedented source of social signal, which therefore is.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very interesting for us as a source of measuring variation.",
                    "label": 0
                },
                {
                    "sent": "Measuring variation that happens for social reasons and we consider actually the linguistic variation that we're also interested in to be just for one one type of general social variation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when I talk about variation I talk about version inside cultures or languages, but also across cultures and languages.",
                    "label": 0
                },
                {
                    "sent": "And actually I consider this to be a continuum, so there is no distinction actually between those two.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when we talk about the type of people, of course the tower Babel stands for this variation.",
                    "label": 0
                },
                {
                    "sent": "So for the difference between people who that are encoded either in the different behavior and different language as well, and the Curse of Babel if transferred to the two of us who do the research in social media.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that we want to research this variation that we're interested in, so we are interested in the variable, and we want to collect as many explanatory variables as possible to understand the phenomenon of choice.",
                    "label": 0
                },
                {
                    "sent": "We actually have to process the data to get those variables, and this is very hard because of the same variation that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So this is a Catch 22 game we want to process data to be able to analyze it, but it is already complicated process for the exact reason for which we are interested in.",
                    "label": 1
                },
                {
                    "sent": "The data in the first place.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually what I want to try answering during this talk is kind of circumvent this curse by performing either model adaptations or model or build our models on predictors that are stable across these variations, either minor or measurements.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me give an overview of the talk.",
                    "label": 0
                },
                {
                    "sent": "So the talk actually comprises a two part.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first part is focused on linguistic processing, so we probably all know what we consider under linguistic presence is our task like part of speech tagging, passing, named entity linking, etc.",
                    "label": 1
                },
                {
                    "sent": "You probably all know how hard it is to process constant data, so one of the first measurements of the complexity of the task was done by Gimpel it.",
                    "label": 0
                },
                {
                    "sent": "Oh so they show that when you train and test and also Journal of course you're part of speech.",
                    "label": 0
                },
                {
                    "sent": "Accuracy is very high.",
                    "label": 0
                },
                {
                    "sent": "It's not the 7%, but when you move to another domain, specially nonstandard data like Twitter, you get an error increase of five times higher, which is quite a problem.",
                    "label": 0
                },
                {
                    "sent": "So in this part.",
                    "label": 0
                },
                {
                    "sent": "I will try to answer how can we adapt to such extreme differences in the target domain, and Furthermore is the path not to cover all of this minor variation as I call it inside the language, but also major variations across the languages.",
                    "label": 1
                },
                {
                    "sent": "So can we consider building cross lingual models or models that will be able to process data regardless of the language that we use?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the second part of the talk, I will focus on the similar problem, but in another area.",
                    "label": 0
                },
                {
                    "sent": "The area of user profiling, so predicting various traits of users on social media, lighter gender, user type, etc.",
                    "label": 1
                },
                {
                    "sent": "So we need these data enrichment procedures all the time, especially because we want to investigate the dependence of these variables with another with other variables and traditionally how we do it is we rely on the linguistic content, so we analyze the linguistic content and predict where the user is male or female depending on what they talk about or how they form.",
                    "label": 0
                },
                {
                    "sent": "Linguistic utterances etc.",
                    "label": 0
                },
                {
                    "sent": "But here I'm investigating whether we can build predictive models that won't rely on linguistic signal only, but a little signal which is more general and which could be easier to adapt across languages or across cultures.",
                    "label": 1
                },
                {
                    "sent": "So let me start with the first part.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is the linguistic processing.",
                    "label": 1
                },
                {
                    "sent": "So here I will just set the stage with a very brief history of text processing.",
                    "label": 1
                },
                {
                    "sent": "So we have two phases in text processing.",
                    "label": 1
                },
                {
                    "sent": "The first phase is actually that we had knowledge rain systems, so the systems relied mostly on handcrafted rules and handcrafted knowledge, and these systems were very hard to adapt to different domains, different languages.",
                    "label": 0
                },
                {
                    "sent": "So in this continuum it was very hard to move to any direction actually, and this was the dominant paradigm until the 9th.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the 9th, is the switch to the data driven systems into the data driven paradigm.",
                    "label": 0
                },
                {
                    "sent": "As you know.",
                    "label": 0
                },
                {
                    "sent": "And instead of writing rules, we just get datasets in which we have a specific problem solved and we will build predictive models from this data and adapting to different domains for different languages or cultures is easier.",
                    "label": 1
                },
                {
                    "sent": "You simply need data in the target domain, but they also have the options of the main adaptation like unsupervised domain adaptation and.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I in this second paradigm I roughly see two phases or maybe a front face is just coming around the corner, so the first phase was for sure is something that I at least called the Wall Street Journal error where people were using small datasets both for training and testing.",
                    "label": 0
                },
                {
                    "sent": "They were actually not so much interested in how they models perform out there, and there are some out there.",
                    "label": 0
                },
                {
                    "sent": "This is the typical in vitro evaluation and with the.",
                    "label": 0
                },
                {
                    "sent": "With the social media revolution that came and actually help, it became very interesting for the economy as well.",
                    "label": 0
                },
                {
                    "sent": "At that point, it was measured.",
                    "label": 0
                },
                {
                    "sent": "Actually, these models are heavily underperform in these domains and then the the domain adaptation methods became much more important than previous.",
                    "label": 0
                },
                {
                    "sent": "Although of course there was previous work on that as well and there might be a new area or new stage in this.",
                    "label": 0
                },
                {
                    "sent": "The level of a type of processing which could be due to neural models.",
                    "label": 0
                },
                {
                    "sent": "We are closer and closer to do proper cross lingual processing, so having single models that are able to process data in different domains and different languages.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the now we talk about how to approach processing minor variation on the mind of preparation and consideration roughly inside a single language or across similar language is one of the variations is nonstandard EXO, the text that is used in user generated content so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Social media, so there actually three ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "The first one is build a normalizer so we will.",
                    "label": 0
                },
                {
                    "sent": "You will build a process which will transform the text from the original form into one form that will be easier to process with the standard pipeline, something we also frequently called the Wall Street Journal pipeline.",
                    "label": 1
                },
                {
                    "sent": "So those pipelines that are.",
                    "label": 0
                },
                {
                    "sent": "Both are journalized pipeline.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another option is doing domain adaptation, so whatever you whatever type of processing you want to do, for instance part of speech tagging, you simply adapter model to the target domain.",
                    "label": 0
                },
                {
                    "sent": "You don't normalize the data 1st and don't apply the standard model but adapt the model.",
                    "label": 0
                },
                {
                    "sent": "The two primary ways of doing this, the supervised just add annotated data from the target domain on unsupervised wear.",
                    "label": 0
                },
                {
                    "sent": "Nowadays mostly the distributional model someone most well known that.",
                    "label": 0
                },
                {
                    "sent": "I will talk about that little bit later on.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's of course a further way joint processing.",
                    "label": 0
                },
                {
                    "sent": "So this is in general very relevant for natural language processing, as we mostly process data in a pipeline.",
                    "label": 0
                },
                {
                    "sent": "So we first deal with one level of abstraction, and then we move to another one, and we propagate the error.",
                    "label": 0
                },
                {
                    "sent": "As we all know.",
                    "label": 0
                },
                {
                    "sent": "So by doing joint processing, for instance, doing normalization and positive pitching at the same time in a single process, we can gain specific.",
                    "label": 0
                },
                {
                    "sent": "Increases in accuracy or whatever evaluation metric we use.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first I will talk a little bit on our work on normalization.",
                    "label": 0
                },
                {
                    "sent": "So what we do we we we do normalization and rather straightforward fashion.",
                    "label": 0
                },
                {
                    "sent": "We consider normalization just to be a translation process.",
                    "label": 0
                },
                {
                    "sent": "So we want to translate from the non center data standard data and the trick we regularly uses that we don't consider to translate words but characters.",
                    "label": 0
                },
                {
                    "sent": "So we generalize much more by translating character by character and we use the best known paradigm.",
                    "label": 0
                },
                {
                    "sent": "The phrase based physical machine translation you might have heard of Moses the system.",
                    "label": 0
                },
                {
                    "sent": "Which does this very well.",
                    "label": 0
                },
                {
                    "sent": "So what we start with is, are these utterances.",
                    "label": 0
                },
                {
                    "sent": "So this is an ensemble occurrence and we consider this to be a sequence of 10 or 12.",
                    "label": 0
                },
                {
                    "sent": "As we learn how to transform this into this, and given that we use characters, we generalize a lot, so we know that at some point we also want to insert spaces in the process.",
                    "label": 0
                },
                {
                    "sent": "And we have the tool on GitHub for that and we did experiments on Slovene historical slowing data and Slovene social media data, but also on the other types of data.",
                    "label": 0
                },
                {
                    "sent": "For instance, Swiss that all data and the same paradigm works very well for any type of language variation.",
                    "label": 0
                },
                {
                    "sent": "What we've learned from that is that given that you have a noisy model here, so you have a translation model and the language model.",
                    "label": 0
                },
                {
                    "sent": "So we want to have the probabilities of specific constellations of transformations.",
                    "label": 0
                },
                {
                    "sent": "And we want to have the probabilities in the target language and the simplest trick to do is just to gather as much target language data, which is the standard data is possible and also use multiple language models and let the optimization process to learn how much it should rely on what source of information.",
                    "label": 0
                },
                {
                    "sent": "So we regularly get improvements by using multiple language models.",
                    "label": 0
                },
                {
                    "sent": "They can even be in different dialects, so there might be variation between the target domain and the language model optimization processes learn.",
                    "label": 0
                },
                {
                    "sent": "How much to listen to specific data sources?",
                    "label": 0
                },
                {
                    "sent": "And there was also another question whether we should try to normalize token by token.",
                    "label": 0
                },
                {
                    "sent": "So word by word or whether we should consider whole utterances when normalizing and to our surprise the result was that as long as your data is not very non normal, as long as it doesn't deviate from the norm significantly, you should actually do token backed up because in that case we have a much smaller search space and the information that you get from the context is not that relevant.",
                    "label": 0
                },
                {
                    "sent": "As the increase in the search space you get with wider context.",
                    "label": 0
                },
                {
                    "sent": "So these are the results.",
                    "label": 0
                },
                {
                    "sent": "We can have a look just at the very reduction.",
                    "label": 0
                },
                {
                    "sent": "So we have four datasets, so we have 18th 19th century data and Twitter data which is very nonstandard and which is slightly nonstandard.",
                    "label": 0
                },
                {
                    "sent": "All these measurements are character level, Levenshtein distances, normalized ones, so this is the lower the better.",
                    "label": 0
                },
                {
                    "sent": "So this is the evolution if you don't do anything on the text is just measures the amount of transformations that needed to be done.",
                    "label": 0
                },
                {
                    "sent": "The baseline system just learns token transformations and applies the most frequent opens summations.",
                    "label": 0
                },
                {
                    "sent": "By generalizing with season three, you already get very drastic improvements, especially in the domains where you have a lot of work to do, and by adding multiple language models and tuning them you get further improvements and to their adduction.",
                    "label": 0
                },
                {
                    "sent": "If you compare our best system with with the initial phase where we didn't do anything, we get this significant reductions from 55 to 93%.",
                    "label": 0
                },
                {
                    "sent": "And if you compare best performing systems with the.",
                    "label": 0
                },
                {
                    "sent": "Strong baseline, I would call it.",
                    "label": 0
                },
                {
                    "sent": "Then we had better reductions of further two 279%, so the reductions are quite high and the system is quite simple further.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Movie took part in in a shared task on translating historical Dutch.",
                    "label": 1
                },
                {
                    "sent": "They called it translating organic wallet, normalizing historical Dutch to modern Dutch, and the interesting thing is that we got the 1st place regardless of many neural machine translation models being part of the shared task as well.",
                    "label": 0
                },
                {
                    "sent": "In the normalization game, there's not enough data for the neural machine translation program, which is a very strong one in which got huge improvements in machine translation in general.",
                    "label": 0
                },
                {
                    "sent": "To outperform this simple models that actually don't.",
                    "label": 0
                },
                {
                    "sent": "Abstract from the surface forms, but just learn how to recombine surface forms to get the target domain date but used label data.",
                    "label": 0
                },
                {
                    "sent": "Yes, of course, so starting points for the statistical machine translation or paradigm is to have parallel data.",
                    "label": 0
                },
                {
                    "sent": "So you have such a trances.",
                    "label": 0
                },
                {
                    "sent": "Such pairs of utterances and learn the translation model from that.",
                    "label": 0
                },
                {
                    "sent": "But how large Dave are roughly?",
                    "label": 0
                },
                {
                    "sent": "20,000 thousand second operating text.",
                    "label": 0
                },
                {
                    "sent": "But we also had previous experiments on having a data system, just 1000 tokens for instances for which we knew that they were highly infrequent in the non center data, and then we normalize them and just having 1000 tokens.",
                    "label": 0
                },
                {
                    "sent": "It learns quite quickly whether transformations are that you should be doing and then having huge language models because they're cheap.",
                    "label": 0
                },
                {
                    "sent": "I mean there for free.",
                    "label": 0
                },
                {
                    "sent": "Actually there is data out there in the standard language written, then you can really.",
                    "label": 0
                },
                {
                    "sent": "Just use this transformation to produce hypothesis and then for the hypothesis to be pruned and two for the process to pick the best hypothesis given the target language or the normal language standard language.",
                    "label": 0
                },
                {
                    "sent": "So this is for the normalization.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fast and if we go to tagging.",
                    "label": 0
                },
                {
                    "sent": "So here I want to showcase our adaptation over standard Tagger on Slovene Twitter and something that I did 2 weeks ago with Barbara Plank and the colleagues on adapting on using their neural Tiger.",
                    "label": 0
                },
                {
                    "sent": "So they have this Alice team Tiger, which is considered to be state of the art.",
                    "label": 0
                },
                {
                    "sent": "It combines version character embeddings so it generalizes very well and has really good results.",
                    "label": 0
                },
                {
                    "sent": "We compared our results that we obtained with.",
                    "label": 0
                },
                {
                    "sent": "Additional random fields modeling techniques, so this is just the traditional way of modeling sequences or doing sequential predictions, and both systems used domain adaptation through distributional information, so we use Brown clusters.",
                    "label": 1
                },
                {
                    "sent": "So Brown clustering is a technique where you cluster words into hierarchical clusters given their context.",
                    "label": 0
                },
                {
                    "sent": "So you want to cluster together words that occur in a large data set in similar contexts, and you do this on standard and non standard data.",
                    "label": 0
                },
                {
                    "sent": "With the idea of updating classes like these, so this is the drawing on the swimming pronoun I yes yes yes yes and yes, so this is a hard hard clustering approach.",
                    "label": 0
                },
                {
                    "sent": "So one word is adjusting one single cluster, but there is a hierarchical portion of the algorithm, so you do get some generalization from that as well.",
                    "label": 0
                },
                {
                    "sent": "And this is the point, is a true example of a cluster, so it manages to realize that all these word forms actually occur in their similar context, and then you enrich your data presentation so that either when yozora US is in there, you encode the path through this binary tree, where these words occur, hoping that when you once occur, the yes or yes form in the context, which is quite.",
                    "label": 0
                },
                {
                    "sent": "Informative for quite probable for personal problem that it will actually realize or choose the these hypothesis.",
                    "label": 0
                },
                {
                    "sent": "This is actually personal pronoun and very better system is worth embedding, so probably know about them.",
                    "label": 0
                },
                {
                    "sent": "So these are the results of the assistance like Word, Two, VEC, Club plastics, etc.",
                    "label": 0
                },
                {
                    "sent": "So those distributional.",
                    "label": 0
                },
                {
                    "sent": "Nation encoded in a vector of fixed size like 100 or 300 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So you encode the whole information of the context in which red occurs in 100.",
                    "label": 0
                },
                {
                    "sent": "Then actions.",
                    "label": 0
                },
                {
                    "sent": "And when we call, when we look at the results.",
                    "label": 0
                },
                {
                    "sent": "So the first part of the results actually.",
                    "label": 0
                },
                {
                    "sent": "Testing on standard data so we have a system trained on summer data and then we test it on Saturday to so these are the state of the art results for Slovene and we see the neural system actually does very similarly actually underperform slightly when it doesn't use distribution information from a huge huge collection of text.",
                    "label": 1
                },
                {
                    "sent": "And then we give it the huge collection of text to learn the initializations of the word and character embeddings.",
                    "label": 0
                },
                {
                    "sent": "Then it is very similar when we move to non center data.",
                    "label": 0
                },
                {
                    "sent": "So this is compara bulto.",
                    "label": 0
                },
                {
                    "sent": "The results of Kim palatal.",
                    "label": 0
                },
                {
                    "sent": "Who trained on worlds?",
                    "label": 0
                },
                {
                    "sent": "Adrenaline tested on Twitter.",
                    "label": 0
                },
                {
                    "sent": "You see a huge increase in error, so this is what happens if you take the standard model and by Tomlinson our data you don't get, you get really bad results.",
                    "label": 0
                },
                {
                    "sent": "So this is where we would.",
                    "label": 0
                },
                {
                    "sent": "Something has to be done.",
                    "label": 0
                },
                {
                    "sent": "And when we compare it to the neural system it actually performs very similarly.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't have any additional information except that standard training data.",
                    "label": 0
                },
                {
                    "sent": "But if you give it distribution information which consists of web data and Twitter data.",
                    "label": 0
                },
                {
                    "sent": "Then it's significantly outperforms our system, but our system never uses this additional information does this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not a fair Harrison.",
                    "label": 0
                },
                {
                    "sent": "But here we can see already where neural models can be better.",
                    "label": 0
                },
                {
                    "sent": "They can generalize better if we move further more too.",
                    "label": 0
                },
                {
                    "sent": "To the.",
                    "label": 1
                },
                {
                    "sent": "Experiments where we use.",
                    "label": 0
                },
                {
                    "sent": "Training data in domain training data.",
                    "label": 0
                },
                {
                    "sent": "So we have some.",
                    "label": 0
                },
                {
                    "sent": "We have some Twitter data that we have tag and if we retrain the models on in domain data so on Twitter data we get back in the game so our results are much better than if you just don't do this supervised adaptation of the model.",
                    "label": 1
                },
                {
                    "sent": "And if we include distributional information.",
                    "label": 0
                },
                {
                    "sent": "So this is a fair comparison.",
                    "label": 0
                },
                {
                    "sent": "So in this case our model uses the Brown clusters, the biased model of system uses the word embeddings and again we get similar results although.",
                    "label": 0
                },
                {
                    "sent": "The by the stem does perform a little bit better.",
                    "label": 0
                },
                {
                    "sent": "At this point we didn't do any sophistical significance testing, but this could be on the verge of statistical significance, and this makes sense because they generalize better.",
                    "label": 0
                },
                {
                    "sent": "They have much more information on the word information from those classes, but they also have character embeddings as well in the system so they can run specific rules on the level which we don't exploit.",
                    "label": 0
                },
                {
                    "sent": "And but in the end, if you combine standard and nonstandard data to engage and combine them into building the best possible model plus all the additional information we actually get again very similar results.",
                    "label": 0
                },
                {
                    "sent": "So here the story could be that actually both traditional methods like CR S and the new methods in case of natural language processing adapt to different domains in a similar manner, so we cannot gain a lot here from neural models, which was actually shown mostly in natural language processing.",
                    "label": 0
                },
                {
                    "sent": "Sorry, how do they perform in time in terms of time in terms of time and take it over.",
                    "label": 0
                },
                {
                    "sent": "Train this system.",
                    "label": 0
                },
                {
                    "sent": "Of course it takes much longer, but I think decoding so tagging is a little bit faster.",
                    "label": 0
                },
                {
                    "sent": "In this case it might be also due to my Evelyn my implementation here, so the CRF, given the significant number of features that I use, is slightly slower in applying the model.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me go over to mixture of these two problems.",
                    "label": 0
                },
                {
                    "sent": "So some work that we currently do with Katia Zupan is that so we did all this adaptations for Slovene, but there's so many languages, many other languages and many other domains on which this still has to be done.",
                    "label": 0
                },
                {
                    "sent": "So we asked ourselves a very natural question.",
                    "label": 0
                },
                {
                    "sent": "If you have some time resources to do some supervised adaptation.",
                    "label": 0
                },
                {
                    "sent": "So annotating data that we will add to the model, or that we will.",
                    "label": 0
                },
                {
                    "sent": "Incorporating the model, whether we should invest in producing normalization data or tag data on the task of part of speech tag.",
                    "label": 0
                },
                {
                    "sent": "And our hypothesis is from our previous experience, what we have expected is that if you have limited time resources that you should actually opt for normalization.",
                    "label": 0
                },
                {
                    "sent": "So breakthrough trainer Normalizer, adapted data normalization and then part of speech tagger through the standard system.",
                    "label": 0
                },
                {
                    "sent": "But if you have significant time resources that then you should probably invest your money or your time into producing tagging, training data and improve your Tiger.",
                    "label": 0
                },
                {
                    "sent": "So we did something.",
                    "label": 0
                },
                {
                    "sent": "Initially.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paramount, so these are the experiment on the 18th century, later set, so this is historical data actually, not social media later, but they they tend to have similar phenomena.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis we have the hours of annotation, so this this is here we measure the accuracy of the tagging process given the hours that will include that we invested in producing additional resources.",
                    "label": 0
                },
                {
                    "sent": "These two here are actually two curves.",
                    "label": 0
                },
                {
                    "sent": "These two curves are the curse curves.",
                    "label": 0
                },
                {
                    "sent": "If we invest into normalization.",
                    "label": 1
                },
                {
                    "sent": "And these two curves that picked how accuracy.",
                    "label": 0
                },
                {
                    "sent": "I'm corresponds if we invest into taking data and we look always at the models that do unsurprised normalization through Brown tagging.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised adaptation through Brown clusters on.",
                    "label": 0
                },
                {
                    "sent": "But it's not so important.",
                    "label": 0
                },
                {
                    "sent": "The most important thing here is that on 18th century investing in normalization, so transforming this old forms into new forms and then applying just the standard model works always better, at least on this scale that we managed to have time to produce manual energy.",
                    "label": 0
                },
                {
                    "sent": "The data always performs better.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we move to 19th century services having closer to what we have expected, if you have a little amount of time, that new probably should opt for normalization, but if you have significant time resources, you should opt for improving your tech data.",
                    "label": 0
                },
                {
                    "sent": "And if.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move to the L3 domain which is non standard printer later than we again see a similar phenomenon.",
                    "label": 0
                },
                {
                    "sent": "So with limited time resources you might be better off with doing normalization, but if you have significant time investments I've able capable of them then you should invest in supervising your thinking model Direct.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I would just want to finish this linguistic processing part with some points on cross lingual processing.",
                    "label": 0
                },
                {
                    "sent": "So this is a new era area of research with limited results.",
                    "label": 0
                },
                {
                    "sent": "So at this point I was talking mostly on something that I call minor variations.",
                    "label": 0
                },
                {
                    "sent": "So roughly in language variation.",
                    "label": 0
                },
                {
                    "sent": "But what if we want to have a model that we train on on floor in hand?",
                    "label": 0
                },
                {
                    "sent": "We want to apply to Korean for instance, so this would be pretty sure this would be major variation.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of work being done at that the.",
                    "label": 0
                },
                {
                    "sent": "2 interesting papers or lines of work.",
                    "label": 0
                },
                {
                    "sent": "First of all, the the researchers from Google and what they do.",
                    "label": 0
                },
                {
                    "sent": "They build Palestinians that reads text by bite, so there's no preprocessing of the text.",
                    "label": 0
                },
                {
                    "sent": "I just read bye bye bye and they what they return in the task of part of speech thing or named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "They return spans and annotations so they just say that from bytes from byte want about six there is data which is a person for instance, so they don't rely on any preprocessing, which is nice.",
                    "label": 0
                },
                {
                    "sent": "Second, what they do they feed data from different languages through the same STM, and they actually improving.",
                    "label": 0
                },
                {
                    "sent": "That's on each separate language.",
                    "label": 0
                },
                {
                    "sent": "They perform better if they train one single model, folder, language, languages.",
                    "label": 0
                },
                {
                    "sent": "So there my obviously there has to be some as a college here.",
                    "label": 0
                },
                {
                    "sent": "Positive interference across languages.",
                    "label": 0
                },
                {
                    "sent": "So by having more training data.",
                    "label": 0
                },
                {
                    "sent": "But it obviously improves the tagging or the named entity recognition on German because they always, with some similarities between these lines.",
                    "label": 0
                },
                {
                    "sent": "How many know?",
                    "label": 0
                },
                {
                    "sent": "What about if the languages are very different?",
                    "label": 1
                },
                {
                    "sent": "But they actually didn't measure which language improves which which which languages improve which languages.",
                    "label": 0
                },
                {
                    "sent": "They just did experiments on 70 languages in case of partners pitching and 10 languages.",
                    "label": 0
                },
                {
                    "sent": "I think in case of mended recognition and they in general observed an improvement, but they didn't do some language ablation.",
                    "label": 0
                },
                {
                    "sent": "So removing specific languages and measuring which actually languages do is positive interference.",
                    "label": 0
                },
                {
                    "sent": "I would assume the local languages.",
                    "label": 0
                },
                {
                    "sent": "Actually, in the training data they had both in the European languages, also Asian languages, but they must have helped between each other, not across.",
                    "label": 0
                },
                {
                    "sent": "Probably this would be my expectation.",
                    "label": 0
                },
                {
                    "sent": "There's another line of work on training multilingual recommending so we already talked about this with embedding so encoding distribution information in dense vector representations.",
                    "label": 1
                },
                {
                    "sent": "So the researchers for from Facebook there for instance trained there plus text tool on almost 300 different languages that are covered through it Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "But more interesting thing that they do.",
                    "label": 0
                },
                {
                    "sent": "Then they're trying to transform those further bindings so those embedding spaces so that there are.",
                    "label": 0
                },
                {
                    "sent": "As similar as possible between languages by having some lexical supervision and transforming those spaces so that the lexical similarity is maximum.",
                    "label": 0
                },
                {
                    "sent": "This of course can be used in multiple ways.",
                    "label": 0
                },
                {
                    "sent": "If you have embedding spaces embedding spaces that are similar across languages, in theory you could train named entity recognition tool on Slovene and then just by feeding representations from English, Tag English data with naming.",
                    "label": 0
                },
                {
                    "sent": "On the task of named entity recognition, and this already works roughly on this error on this level of abstraction regarding part of speech saying the results are still quite bad here I just want to show something that I do.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But last week actually to show.",
                    "label": 0
                },
                {
                    "sent": "Guess mostly my work, so we had a task, extra lexical prediction task so we had to predict the how abstract or how can create the specific word is in our case in creation.",
                    "label": 0
                },
                {
                    "sent": "This was done on a national project so we had 3000 queries on which we had never had Likert scale from 1 to 5 and we had the mean of the.",
                    "label": 0
                },
                {
                    "sent": "Response is obtained from people on wall where they put each word on the objectives, completeness, scale and if we use monolingual further beddings is features.",
                    "label": 0
                },
                {
                    "sent": "So I think where the metrics from the creation web then we end.",
                    "label": 0
                },
                {
                    "sent": "We do cross validation week on, so we build a regression model on the problem and if we compare the continuous values that we have predicted and those that were given by people, we get couple quite high.",
                    "label": 0
                },
                {
                    "sent": "Correlation of 0.81.",
                    "label": 0
                },
                {
                    "sent": "So this obviously works on single languages, so you can predict various lexical features from robotics on single languages.",
                    "label": 0
                },
                {
                    "sent": "But then I tried using those cross lingual models, training my model on creation and applying the same model on all these other 77 languages with quite good results.",
                    "label": 0
                },
                {
                    "sent": "For now I don't have any.",
                    "label": 0
                },
                {
                    "sent": "And district evolution that I can.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show you a list of words in Slovene, English, and German.",
                    "label": 0
                },
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "This is the start of the list from the most abstract terms, and this is the end of the list of the tail of the list.",
                    "label": 0
                },
                {
                    "sent": "The most the most concrete terms, so you can see at least detailing the.",
                    "label": 0
                },
                {
                    "sent": "Add in the pale of the list looks quite well and it's similar in all the other languages, so cross lingual processing is still in early stages, but there are significant improvements being performed on actually on a monthly basis.",
                    "label": 0
                },
                {
                    "sent": "So so much about linguistic processing, and now I'd like to move.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the other part, which focuses mostly on user profiling.",
                    "label": 0
                },
                {
                    "sent": "So here I want to show 2.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The profiling experiments.",
                    "label": 0
                },
                {
                    "sent": "The first one is done on identifying user type, so we wanted to discriminate between private and corporate users on special in Slovene tweets.",
                    "label": 0
                },
                {
                    "sent": "But we also played by with creation tweets as I will show later on and the main comparison with it.",
                    "label": 0
                },
                {
                    "sent": "We compare the boat bag of words as a strong baseline with some language agnostic features.",
                    "label": 0
                },
                {
                    "sent": "So I told already we were interested in building looking for models that would be easier applicable.",
                    "label": 0
                },
                {
                    "sent": "Across many languages and in this research we did experiments on time, robustness in space, robustness.",
                    "label": 1
                },
                {
                    "sent": "Under time robustness I mean that we train the model on the on time scales on the time span until 2015 and then applied it on data that was published later on.",
                    "label": 0
                },
                {
                    "sent": "Of course, we're not using the same user data on both sides and space robustness we consider to be again, if we have minor variations across similar languages, whether we can train a model on slowing data and apply.",
                    "label": 0
                },
                {
                    "sent": "On probation and the set.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An experiment was in general prediction, so this was done on the twisted data set which contains user information like gender and their psychological traits on users.",
                    "label": 0
                },
                {
                    "sent": "From Twitter, six different languages.",
                    "label": 0
                },
                {
                    "sent": "And here again we compared the bag of words baseline with some language with the same language agnostic features as in the first research.",
                    "label": 0
                },
                {
                    "sent": "And we also did experiments on cross lingual gender prediction.",
                    "label": 1
                },
                {
                    "sent": "Some training on German testing and budgets.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's go first over the language agnostic features.",
                    "label": 0
                },
                {
                    "sent": "So here I just list the feature types so we have 5 feature types.",
                    "label": 0
                },
                {
                    "sent": "In general we had more features than I listed here in both researchers, so the first feature type is the percentage.",
                    "label": 0
                },
                {
                    "sent": "So here, which simply encode the percentage of Tweet that have a specific phenomenon which is not strictly related to the language points.",
                    "label": 0
                },
                {
                    "sent": "The percentage of tweets that contain URLs.",
                    "label": 1
                },
                {
                    "sent": "You could imagine that this is a very strong feature for discriminating between private and corporate users, for instance.",
                    "label": 0
                },
                {
                    "sent": "But it also proved to be quite informative for discriminating between male and female users as well.",
                    "label": 0
                },
                {
                    "sent": "Another feature example is the percentage of tweets being replies.",
                    "label": 1
                },
                {
                    "sent": "Again, you would probably consider women to reply more than men and private users to reply more than corporate users.",
                    "label": 0
                },
                {
                    "sent": "Or maybe not the second type of features is just the mean.",
                    "label": 0
                },
                {
                    "sent": "So the actually the next three types of features encode distributions across all the tweets of a specific user, so we have the mean, the median and variance.",
                    "label": 0
                },
                {
                    "sent": "And types of features are like the arm of posting, so there's a continuous variable, roughly from one from zero to 23, and another one could be the length of the text.",
                    "label": 0
                },
                {
                    "sent": "So yes, of course.",
                    "label": 0
                },
                {
                    "sent": "In the week.",
                    "label": 0
                },
                {
                    "sent": "So yeah there was.",
                    "label": 0
                },
                {
                    "sent": "This was one of the features as well and we had also binary feature where the that was published during the weekday or at the weekend as well.",
                    "label": 0
                },
                {
                    "sent": "These are just examples of features are not also features and how many were so in case of in case of predicting corporate and private users we had 28 features isn't I'm not mistaking in case of predicting or discriminating between male and females.",
                    "label": 0
                },
                {
                    "sent": "I remember that one we had 51 features.",
                    "label": 0
                },
                {
                    "sent": "Chris Brown picture it's more difficult problem.",
                    "label": 0
                },
                {
                    "sent": "I guess my feeling then yes, and I must be Frank.",
                    "label": 0
                },
                {
                    "sent": "I mean the gender prediction task came later, so then we had the ideas of additional features to be honest about it, yeah.",
                    "label": 0
                },
                {
                    "sent": "And so, besides these three types of feature that encode the distribution, we have the user level features like the ratio of friends or followers or specific user.",
                    "label": 0
                },
                {
                    "sent": "So you would expect for corporate users probably to have more followers than friends, probably.",
                    "label": 0
                },
                {
                    "sent": "Or for instance something that we added by undoing gender prediction the intensity of the red color component in RGB color in the user space background.",
                    "label": 1
                },
                {
                    "sent": "So there is information on what type of color the user uses in the background in the text, etc.",
                    "label": 0
                },
                {
                    "sent": "So we exploit this information as well.",
                    "label": 0
                },
                {
                    "sent": "So these are the feature types.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let's go to the results.",
                    "label": 0
                },
                {
                    "sent": "So on discriminating between corporate and private users are baseline, which is the most frequent class baseline.",
                    "label": 0
                },
                {
                    "sent": "So in this case we always claim that the correct class is the one of the most frequent plus, which is the private one.",
                    "label": 0
                },
                {
                    "sent": "It's 64, then these are all rated F once, so there's RF ones, and these are weighted difference given the number of instances of each class.",
                    "label": 0
                },
                {
                    "sent": "If we apply language agnostic features we get.",
                    "label": 1
                },
                {
                    "sent": "Of course, is very large improvement from our weak baseline, but the bag of words model still outperformed the linguistic features.",
                    "label": 0
                },
                {
                    "sent": "Instead.",
                    "label": 0
                },
                {
                    "sent": "What we want is for the baseline is majority.",
                    "label": 0
                },
                {
                    "sent": "The baseline is the majority.",
                    "label": 0
                },
                {
                    "sent": "Yes, wise, corporate zero.",
                    "label": 0
                },
                {
                    "sent": "I don't do something.",
                    "label": 0
                },
                {
                    "sent": "The majority baseline the dummy classifier over his claims for the correct answer today would be the more frequent plus, so it never.",
                    "label": 0
                },
                {
                    "sent": "It never votes or it never gets a return on the Corporation.",
                    "label": 0
                },
                {
                    "sent": "So it always claims that it's a private class.",
                    "label": 0
                },
                {
                    "sent": "So the F1 is 0 because the I mean this is it's F1 measure on the corporate class, because it's never.",
                    "label": 0
                },
                {
                    "sent": "So you see, how do you?",
                    "label": 0
                },
                {
                    "sent": "OK I methodological, I don't understand.",
                    "label": 0
                },
                {
                    "sent": "So you define corporate as the target and then do all the measures precision, recall.",
                    "label": 0
                },
                {
                    "sent": "And it's a new problem when you define private stuff and then you combine these two F1 measures by doing this weighted F1.",
                    "label": 0
                },
                {
                    "sent": "So which takes into account the number of instances from the corporate and from the private.",
                    "label": 0
                },
                {
                    "sent": "Group in in your data set on the level of F1 destination.",
                    "label": 0
                },
                {
                    "sent": "No, it just combines the reference.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, thank you I didn't.",
                    "label": 0
                },
                {
                    "sent": "I understand that methodology.",
                    "label": 0
                },
                {
                    "sent": "And if we apply the bag of words model, then we get an improvement over the language agnostic features.",
                    "label": 0
                },
                {
                    "sent": "And of course if we build an example which takes into account the probability distribution among classes of each of the of the classifiers, then we get some additional improvements.",
                    "label": 0
                },
                {
                    "sent": "But you actually the most important thing is that the bag of words works inside the language better than the language model features.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sony Bank of words.",
                    "label": 0
                },
                {
                    "sent": "A sample is assemble learning, which means it uses the model.",
                    "label": 0
                },
                {
                    "sent": "How about merging the features?",
                    "label": 0
                },
                {
                    "sent": "I haven't done that, but you understand.",
                    "label": 0
                },
                {
                    "sent": "Of course, the bag of words has like 5,000,000 features happening because there is a lot of text there and a lot of many different engrams and the language agnostic feature has on the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the question is whether these features will get lost in this huge huge huge dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So I would assume that the sample is a safer choice here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah for sure.",
                    "label": 0
                },
                {
                    "sent": "OK, and but this is where it gets interesting, so we checked for time depends in space dependence.",
                    "label": 0
                },
                {
                    "sent": "So we trained on on users or user data before 2015 and tested on the account of different users that was published after or on or after 2015 and what we observe here is that actually are language agnostic.",
                    "label": 0
                },
                {
                    "sent": "Features lose more than the bag of words features.",
                    "label": 0
                },
                {
                    "sent": "So our assumption was that it should be similar.",
                    "label": 0
                },
                {
                    "sent": "But obviously the topics that are covered do not change so much as much as the.",
                    "label": 0
                },
                {
                    "sent": "Behavior of the users changed before 2015.",
                    "label": 0
                },
                {
                    "sent": "Article 15 and this is not we roughly observe, and we simply observe what's going on.",
                    "label": 0
                },
                {
                    "sent": "Social media, the type of users and their interaction.",
                    "label": 0
                },
                {
                    "sent": "Changes retire on the after 2015.",
                    "label": 0
                },
                {
                    "sent": "You took different users, different users.",
                    "label": 0
                },
                {
                    "sent": "I split users in half and for what some users I took only data before 2015.",
                    "label": 0
                },
                {
                    "sent": "Of course, if there was an update and if not then I would just move it to the other cloud category and try there and for the other set of users are two data only published in 2015 or later.",
                    "label": 0
                },
                {
                    "sent": "Even.",
                    "label": 0
                },
                {
                    "sent": "Because what you are claiming now is that users change behavior.",
                    "label": 0
                },
                {
                    "sent": "Now the question is.",
                    "label": 0
                },
                {
                    "sent": "If those users who are now changed in behavior, how did they behave before 2:15, right?",
                    "label": 0
                },
                {
                    "sent": "This is it like they changed behavior?",
                    "label": 0
                },
                {
                    "sent": "Or is it just different users?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, that's all set then.",
                    "label": 0
                },
                {
                    "sent": "That's the question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, absolutely.",
                    "label": 0
                },
                {
                    "sent": "Kind of human psyche is changing and how they approached with Saint Francis.",
                    "label": 0
                },
                {
                    "sent": "Left wing users left Twitter handwriting users account if it.",
                    "label": 0
                },
                {
                    "sent": "Once it is what we recently observed, another Microsoft and the second part is the space dependence as we call it.",
                    "label": 0
                },
                {
                    "sent": "So this is when we train the model on Slovene Dayton applied information data here and our assumption was that OK back approach should work because those two languages are very similar.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, in bag of words, vectors and choose between this character.",
                    "label": 0
                },
                {
                    "sent": "So these models generalize quite well on features like using URLs or.",
                    "label": 0
                },
                {
                    "sent": "Or specific features that does happen with features, but here again, the loss on the linguistic features works greater than on the back of roots.",
                    "label": 0
                },
                {
                    "sent": "So in this case actually we are obviously we actually observed that in case of similar languages the back of birth still out performs at least the length diagnostic features how they are defined right now.",
                    "label": 0
                },
                {
                    "sent": "Of course, both of the models still outperform the most recent classes this line.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have a balanced data set, so if one is very low.",
                    "label": 0
                },
                {
                    "sent": "As one would expect.",
                    "label": 0
                },
                {
                    "sent": "So this is on the use.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scientification and justice.",
                    "label": 0
                },
                {
                    "sent": "A quick quick feature analysis so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we learn from this and from this line of research by looking at the features so far its users reply more mention more other users favor other tweets post in various hours so the variance of the posting time is greater and post tweets of various lengths of the variance of the tweet like is also greater.",
                    "label": 1
                },
                {
                    "sent": "On the other hand corporate users use more URLs pose during working hours.",
                    "label": 0
                },
                {
                    "sent": "Post earlier in the day and post longer treats.",
                    "label": 0
                },
                {
                    "sent": "So these are the best performing features.",
                    "label": 0
                },
                {
                    "sent": "So let's go over.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But to the General prediction task.",
                    "label": 0
                },
                {
                    "sent": "So in this case we had the data set in which we had users tweeting in six different languages.",
                    "label": 0
                },
                {
                    "sent": "So these are the six languages and this is the number of instances per language.",
                    "label": 0
                },
                {
                    "sent": "So we can see that in the office today to say thank you Ben for producing the data set.",
                    "label": 0
                },
                {
                    "sent": "That is quite different number of users if you apply the most frequent class baseline.",
                    "label": 0
                },
                {
                    "sent": "These are the results, so we can see in German the data is quite balanced by the impeller in the data set is quite imbalanced, so there's if I remember correctly much more female users than me.",
                    "label": 0
                },
                {
                    "sent": "This column includes our results on something we call in language bag of words.",
                    "label": 0
                },
                {
                    "sent": "So in this case we do N fold cross validation and we build character file, grams, bag of words, models and test them on the same language.",
                    "label": 0
                },
                {
                    "sent": "So this is the this is these are results if we train and test with the symbol.",
                    "label": 0
                },
                {
                    "sent": "This column encodes information or the results.",
                    "label": 0
                },
                {
                    "sent": "If we do cross lingual back offered.",
                    "label": 0
                },
                {
                    "sent": "So we train a model on one of the languages and evaluated on the remaining.",
                    "label": 0
                },
                {
                    "sent": "Languages and we just take the average of the results.",
                    "label": 0
                },
                {
                    "sent": "So here you can see that in the cross language bag of words, the results are much lower as well looking set, but still quite high and in most cases still throughout from the most frequent cause baseline or in this case from from random classifier.",
                    "label": 0
                },
                {
                    "sent": "There are some researchers were very surprised with news that this result, because they did some similar experiments and they didn't get so such good results.",
                    "label": 0
                },
                {
                    "sent": "Then we looked into the data and realize that actually more many of these tweets tweet in English as well.",
                    "label": 0
                },
                {
                    "sent": "So English language was proven to be actually what they did.",
                    "label": 0
                },
                {
                    "sent": "They got rid of all the tweets for each user who didn't use, not the specific language was used, but actually what I was more interested in.",
                    "label": 0
                },
                {
                    "sent": "Users that mostly treating these languages.",
                    "label": 0
                },
                {
                    "sent": "And if there is additional signal from their language production which is not in this language, I was still OK with that.",
                    "label": 0
                },
                {
                    "sent": "And this is actually what help quite probably helped a lot.",
                    "label": 0
                },
                {
                    "sent": "And while we got results much better than the baseline results, so this is on the bag of words.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are the results if we apply our language agnostic features.",
                    "label": 0
                },
                {
                    "sent": "In this case, these are the training languages and these are the testing languages, so this is the model that we trained on German and developed in German.",
                    "label": 0
                },
                {
                    "sent": "And we publish in bold all the results will be better than the bag of words, either in language or cross language.",
                    "label": 0
                },
                {
                    "sent": "Bag of words.",
                    "label": 0
                },
                {
                    "sent": "So as you can observe on the diagonal, of course, many times we are not better than the bag of words as you would expect.",
                    "label": 0
                },
                {
                    "sent": "So if you train and test in the same language you are.",
                    "label": 0
                },
                {
                    "sent": "Bag of words works better, but if we move across languages then a language Gnostic features perform better.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, the variance of our results is much much lower.",
                    "label": 0
                },
                {
                    "sent": "In case of language agnostic features then on cross lingual bag of words.",
                    "label": 0
                },
                {
                    "sent": "In case of cross building will take over as the variance between the six classifier is 10 times higher than that of the results that you can see.",
                    "label": 0
                },
                {
                    "sent": "Here you can see that in most cases the classifiers perform quite similarly and the nice thing of course is that they are still quite far away from the most frequent class based.",
                    "label": 0
                },
                {
                    "sent": "So these models could be applied to different languages and you could get still much more signal than by claiming higher randomly.",
                    "label": 0
                },
                {
                    "sent": "What's what, or just picking the more probable class or?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And yeah, we did some analysis of features here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the blue is blue in this Gray is actually read on my screen.",
                    "label": 0
                },
                {
                    "sent": "So what we did actually given that we used an SVM for that before.",
                    "label": 0
                },
                {
                    "sent": "So we scaled the data.",
                    "label": 0
                },
                {
                    "sent": "We use the scalar for the data.",
                    "label": 0
                },
                {
                    "sent": "We just calculated effect sizes between the germinal between the male and the female users by calculating the difference of the mean of these two distributions, the male and female distribution.",
                    "label": 0
                },
                {
                    "sent": "In that case you could have a positive value.",
                    "label": 0
                },
                {
                    "sent": "This means that the average value is much higher among female users.",
                    "label": 0
                },
                {
                    "sent": "If there is a negative value, this means that actually the average value is much higher among male users, and these are the six different languages and what we were interested in.",
                    "label": 0
                },
                {
                    "sent": "So which we showed previously experimentally that this works.",
                    "label": 0
                },
                {
                    "sent": "These features work.",
                    "label": 0
                },
                {
                    "sent": "There is some useful information in these features, but we were interested in seeing whether this is this condition by analyzing specific features, and then we rank the features.",
                    "label": 0
                },
                {
                    "sent": "These are again just the top 20 features by the average rank.",
                    "label": 0
                },
                {
                    "sent": "Of a univariate test for each language.",
                    "label": 0
                },
                {
                    "sent": "So these are the best performing feature across all the languages.",
                    "label": 0
                },
                {
                    "sent": "So the best performing feature is actually the percentage of emoji of tweets that contain emojis, and as you can see in all cases this is much more frequently much higher.",
                    "label": 0
                },
                {
                    "sent": "These features are much higher in among female users than month, maybe users, and as you can see if you're looking at the whole list in quite a number of cases, the whole whole rows.",
                    "label": 0
                },
                {
                    "sent": "Encoded with the same color, the intensity of the color corresponds to the number of course.",
                    "label": 0
                },
                {
                    "sent": "So in for the first three rows, you can observe that these features, obviously informative or they aim at female users, while the next two are more informative than male class.",
                    "label": 0
                },
                {
                    "sent": "For instance, the percentage of tweets that use that contain URLs as one would expect we want remained one to reference to other people's work, and the percentage of.",
                    "label": 0
                },
                {
                    "sent": "This is a weird one, so the percentage of three separate you the percent from Niaz device is much higher among men than women, something that I wouldn't expect.",
                    "label": 0
                },
                {
                    "sent": "But this is pleased with this data set says.",
                    "label": 0
                },
                {
                    "sent": "Maybe because of this incorporate, of course we didn't discriminate in this data set.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is actually not my data set, so there was not discriminate in private and corporate users, so it might be that actually more Mail users for corporate users, I think before incorporate private it was like URL stand, more characteristic of absolutely and this is what we would expect it to be, just that.",
                    "label": 0
                },
                {
                    "sent": "It's more like so.",
                    "label": 0
                },
                {
                    "sent": "That's what you observe in here is not due to gender, but you do that kind of task.",
                    "label": 0
                },
                {
                    "sent": "This will check yes, especially we have gender encoded in this living data set as well, so we should investigate this into something they disagree.",
                    "label": 0
                },
                {
                    "sent": "We can remove the corporate users and just have fun certification of that in some social science, say like why men ask so many questions.",
                    "label": 0
                },
                {
                    "sent": "Confused.",
                    "label": 0
                },
                {
                    "sent": "Is that I think the high check for exactly the question mark raise it.",
                    "label": 0
                },
                {
                    "sent": "The question yeah, the percentage of tweets can consider consistent of having?",
                    "label": 0
                },
                {
                    "sent": "Since then is regularly Mail features.",
                    "label": 0
                },
                {
                    "sent": "This is somewhere interesting and of course the 2nd and the third one.",
                    "label": 0
                },
                {
                    "sent": "The mean order retweet count.",
                    "label": 0
                },
                {
                    "sent": "So how frequently they retweet and how much red they use in the background was also informative of the general.",
                    "label": 0
                },
                {
                    "sent": "Actually there was some research done that used only the six colors you can define on Twitter to build language independent.",
                    "label": 0
                },
                {
                    "sent": "The general predictors, so I just use the information from $6 to prepare.",
                    "label": 0
                },
                {
                    "sent": "This is a Mail user review which worked a little bit worse than this one because this is much more information, but it actually worked quite well.",
                    "label": 0
                },
                {
                    "sent": "Of course in different datasets or one has to have quite some grains of salt there always.",
                    "label": 0
                },
                {
                    "sent": "So did this data set have the neutral gender as well, like we don't know everything?",
                    "label": 0
                },
                {
                    "sent": "Is there never follow the binary classification, either male or female, or we don't know, so we remove it from the date.",
                    "label": 0
                },
                {
                    "sent": "This was their methodology and the data make female in the other day that was collected by a questionnaire from the users.",
                    "label": 0
                },
                {
                    "sent": "Or no, no.",
                    "label": 0
                },
                {
                    "sent": "Actually, they're interested primarily in the psychological trait of users, so they were looking for phrases in which people define that they took a psychological test and that their psychological traits are these in these.",
                    "label": 0
                },
                {
                    "sent": "So this is how they collected the primary set of users psychological test exactly that they took at some point, and they reported about it.",
                    "label": 0
                },
                {
                    "sent": "So they caught on this because they were primarily interested in identifying psychological traits from Twitter data and possibly also across languages.",
                    "label": 0
                },
                {
                    "sent": "And then the human annotator went through all the users and define specific users being male or female, use their name, the picture and their content to define that no, no.",
                    "label": 0
                },
                {
                    "sent": "The psychological traits I, I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Some of them are online questionnaires and you just fill it out and then you post your result on Twitter because you think it's funny.",
                    "label": 0
                },
                {
                    "sent": "And then that's how they automatically they collected the.",
                    "label": 0
                },
                {
                    "sent": "The keywords you know from the results of the tests are, I think 4 letters.",
                    "label": 0
                },
                {
                    "sent": "Is it that in each letter specifies?",
                    "label": 0
                },
                {
                    "sent": "I think about your personality.",
                    "label": 0
                },
                {
                    "sent": "There's the psychological test of how much introverts and extraverts.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 4 dimensions are pink and visual encoding.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean it's just the psychological model.",
                    "label": 0
                },
                {
                    "sent": "I just have one comment in Slovene where we were able to distinguish between private and corporate and their gender.",
                    "label": 0
                },
                {
                    "sent": "We saw regularly that males always are closer to corporate users and females closer through the little private users.",
                    "label": 0
                },
                {
                    "sent": "So even private mails they behave as corporate users, private females.",
                    "label": 0
                },
                {
                    "sent": "They're the typical private.",
                    "label": 0
                },
                {
                    "sent": "That also probably wouldn't have corporate users here, right, yeah.",
                    "label": 0
                },
                {
                    "sent": "Producers to report on their personalities, but probably not there.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want additional support, probably interesting figures, that's wild female female users use more emojis, emoticons, some offerings by men.",
                    "label": 0
                },
                {
                    "sent": "So this somehow follows that men's slower adapt to new ways of communication.",
                    "label": 0
                },
                {
                    "sent": "So All in all, the languages emoticon service model among men.",
                    "label": 0
                },
                {
                    "sent": "And of course the location of the user is more shared by major system.",
                    "label": 0
                },
                {
                    "sent": "So this is also highly informative of the main male gender.",
                    "label": 0
                },
                {
                    "sent": "Which you would expect there.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to define rhyme in this space because this is so important.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, and at some point with this we also observed some differences.",
                    "label": 0
                },
                {
                    "sent": "Of course between these languages and we were interested whether the differences between languages, whether there is some structure, and whether it follows our expectations on the cultural differences between the speakers of these 6 languages.",
                    "label": 0
                },
                {
                    "sent": "So what we did, we represented each of the languages or the cultures as a vector of 51 dimensions, and these values are actually those features.",
                    "label": 0
                },
                {
                    "sent": "Taxes and we took those and then we did the clustering over those six data points.",
                    "label": 0
                },
                {
                    "sent": "So we have to be used in agglomerative hierarchical clustering, just the most.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come on the end of end organ we got from this pattern is active this and it's quite nice.",
                    "label": 0
                },
                {
                    "sent": "I would say because it really pulls off.",
                    "label": 0
                },
                {
                    "sent": "We were expecting that the Portuguese and Spanish users are the most similar ones.",
                    "label": 0
                },
                {
                    "sent": "Then the French join in, then the Italian at the same similarity threshold.",
                    "label": 0
                },
                {
                    "sent": "the German and Dutch ones merge and everybody merges in a big company, so this is an interesting side effect.",
                    "label": 0
                },
                {
                    "sent": "Side results.",
                    "label": 0
                },
                {
                    "sent": "Inspection.",
                    "label": 0
                },
                {
                    "sent": "There's of course a lot of things that we could do.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to conclude, on the two main topics.",
                    "label": 0
                },
                {
                    "sent": "So first in the first part you're talking about linguistic processing, so we discriminate within a minor variation in major variation.",
                    "label": 1
                },
                {
                    "sent": "So minor variation is roughly inside one language or across closely related languages.",
                    "label": 0
                },
                {
                    "sent": "So in this case we've shown that unsupervised adaptation, primarily by using distributional information, can can.",
                    "label": 0
                },
                {
                    "sent": "I get too far, especially normalizing data, so just trying to transform the data into a standard form also can help, especially if you don't have a lot of resources at your hand.",
                    "label": 0
                },
                {
                    "sent": "For major variation, we show that mostly supervised supervised supervision today to two annotated data has to be done to get reasonable results.",
                    "label": 0
                },
                {
                    "sent": "We're still not there to be able to build such representations of words or sentences so that we can move across languages.",
                    "label": 0
                },
                {
                    "sent": "Online and on the other hand, there's a lot of data like in machine translation.",
                    "label": 1
                },
                {
                    "sent": "We've shown that the neural machine translation models obviously encodes the meanings of sentences quite well and decode them on in the other language, but promising directions for the major variation is multilingual learning, so feeding a different data from different languages to the same network.",
                    "label": 1
                },
                {
                    "sent": "We've seen positive influence results there, or multilingual representations apply that we talked before, so you really should come up with representations that can be.",
                    "label": 0
                },
                {
                    "sent": "A shared among all the libraries on the other topic of user profiling.",
                    "label": 1
                },
                {
                    "sent": "For minor variations we shall.",
                    "label": 0
                },
                {
                    "sent": "We show that bag of words baselines are still stronger than our language agnostic features as they are defined right now, but for major variation, language agnostic features do outperform backwards, at least on the gender prediction task.",
                    "label": 0
                },
                {
                    "sent": "And there is a lot of work to be done on two on both of those fronts, especially in user profiling.",
                    "label": 0
                },
                {
                    "sent": "I would say so.",
                    "label": 0
                },
                {
                    "sent": "First of all, our features server is fixed, so we.",
                    "label": 0
                },
                {
                    "sent": "I'll get over Privy to perform a specific measurement, and then the hope that the distribution of that variable in the second language or in the second culture is the same.",
                    "label": 0
                },
                {
                    "sent": "It's roughly if the if more than 45% of the tweets contain URLs that this product corporate users, we could simply do feature adaptation or generalization by calculating the part of the distribution which which encodes more, probably that that the user is.",
                    "label": 0
                },
                {
                    "sent": "Operating product so we would need to adapt features for different distributions in different languages in different cultures.",
                    "label": 0
                },
                {
                    "sent": "And of course there is also much more data on social media on each user and we should use that.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't use only only this 51 features or only the signal that the users tweet.",
                    "label": 0
                },
                {
                    "sent": "There's also multi model signal.",
                    "label": 0
                },
                {
                    "sent": "There are pictures, images in this.",
                    "label": 0
                },
                {
                    "sent": "Probably the next piece of information that we will add to our language.",
                    "label": 0
                },
                {
                    "sent": "Independent general predict it's the images that people either share or the profile pictures that they have the correct.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work to be done that it should be done.",
                    "label": 0
                },
                {
                    "sent": "In my opinion, because there is too much variation going on social media that we are on one side interested in on the other side, we have to process the data as much as possible before making interesting inspections.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a piece of acknowledgment, so most of this work was done inside the youngest project with the people listed here.",
                    "label": 0
                },
                {
                    "sent": "That official was them in their principle investigator of this project I used men.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Datasets these datasets would have a primarily variables for me because of the cloud infrastructure, which is very important in my work.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's a new venture that we are just starting to play in.",
                    "label": 0
                },
                {
                    "sent": "It's the adventure of hate speech and unacceptable this course, so we just got a new project in which we research socially unacceptable build discourse in an interdisciplinary way.",
                    "label": 1
                },
                {
                    "sent": "So we combine technological, social, logical, linguistic, and legal perspective of the same problem in the investigation of this highly, highly relevant.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}