{
    "id": "dkof5q7q7s2yll5nlejtefxfcti56vtf",
    "title": "Tutorial on Learning Deep Architectures",
    "info": {
        "author": [
            "Yoshua Bengio, Department of Computer Science and Operations Research, University of Montreal",
            "Yann LeCun, Computer Science Department, New York University (NYU)"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_bengio_lecun_tldar/",
    "segmentation": [
        [
            "Thanks a lot, Russ."
        ],
        [
            "So.",
            "Deep motivations for deep learning.",
            "Should I speak louder?",
            "Is that OK?",
            "So why do we care about investigating deep architectures?",
            "There are a number of motivations.",
            "One of the oldest motivation is that brains seem to have a deeper architecture.",
            "Meaning that information is processed at.",
            "At many levels of representation and abstraction.",
            "And this is particularly obvious in the visual system.",
            "Is elaborate a bit?",
            "Also.",
            "It seems more natural thinking about how humans organize their ideas and think about problems and even solve problems.",
            "Finally, we've uncovered theoretical motivations for deep architectures having to do with their power of representation, and I'll see quickly a few words about that later.",
            "An older motivation that has to do with the original motivation for neural networks, and that has to do with the use of distributed representations.",
            "I'm not going to say much in this presentation about distributed presentation, but it really is an important motivation.",
            "And it means that we can efficiently represent.",
            "Information with a number of of features.",
            "A number of characteristics that doesn't have to be too large an yet one can represent a huge number of possible configurations of inputs.",
            "Finally, a motivation that has come to be important in in recent years and ties in with a lot of work in machine learning in the last few years has to do with multitask learning and the notion that having intermediate representations that are learned.",
            "Can be useful to generalize.",
            "Better to share statistical strength among tasks among outputs or among components of even the same problem.",
            "I didn't, I didn't, I just use different words so.",
            "I said one of the theoretical motivations is that they can be very efficient in terms of representation, and that's what I meant.",
            "I'll say few more words about that.",
            "So oops."
        ],
        [
            "Well.",
            "If you look in the visual system.",
            "You find in the path from the retina to.",
            "Higher levels of processing in the visual cortex.",
            "A kind of feedforward Ann.",
            "Also feedback paths from groups of units that do computations that neuro scientists start to believe that they understand.",
            "Um?",
            "That seemed to take the image and then represent it in different ways.",
            "At the first really interesting level in area V1, images are represented by neurons that look like they're essentially doing edge detection.",
            "Looking at different angles and scales and positions.",
            "And if you look in area V2, it's more difficult to understand what is going on, but it looks like.",
            "The neurons are essentially composing these edge vectors to detect slightly more complicated primitive shapes, combining, say, two edges.",
            "And then if you go higher up, you start seeing.",
            "Neurons that detect higher, higher level abstractions, little parts of objects, or even a simple objects like a face."
        ],
        [
            "Another motivation that I mentioned was looks like the way we process information just by asking people about how they do that and and.",
            "And other ways of investigation suggests that we do organize our thoughts and our beliefs and our understanding of the world around us using multiple levels of abstractions and concepts that are organized hierarchically, where higher level concepts are defined in terms of lower level ones.",
            "And even the way we learn seem to go that way by composing similar things that we've learned into more complicated representations or more complicated abstractions.",
            "It also seems to be the way that we solve problems.",
            "So when engineers solve problems, they typically try to break up the problem into subproblems, and very often the solution involves a sequence of operations, which essentially means composing things."
        ],
        [
            "Now to these theoretical results that I mentioned.",
            "The bullet that I didn't speak enough of.",
            "The notion of depth can be formalized using.",
            "The computer science notions of graphs.",
            "So if you imagine a graph of computation where you start with some inputs and you combine.",
            "You combine quantities associated with nodes with some operation, and you in each node you do some computation using predecessors in the graph, and you end up with some output that defines a graph, and you can talk about the depth of the graph as the number of the longest path from inputs to output.",
            "Of course, if you think about these graphs for representing functions, depending on what I'm allowed to put in these boxes, the computations I can get different kinds of graphs and the the shortest graph that I can get for representing a function.",
            "The smallest graph can be different depending on the types of operations that are allowed.",
            "So here I've used very simple operations in neural networks.",
            "The operations that we allow are linear transformations followed by a non linearity.",
            "And depending on the choice of the linear transformation, you get different operations and this is the typical feature you've seen for neural networks.",
            "But you can.",
            "You can apply this to a lot of.",
            "Classes are functions that we care about in machine learning."
        ],
        [
            "So about the depth of architectures.",
            "There are theoretical arguments.",
            "The most interesting one from Hastad and collaborators 86 and 91 and some of the work that I did with Olivia de Lalo.",
            "That say roughly the following.",
            "If we have two layers of logic gates, so that's the 86 result, or two layers of formal neurons.",
            "That's the 1991 result, or two layers of RBF units.",
            "Then of course so that we know that this is not these results.",
            "We know that we can have these constitute universal approximators, so for example, 2 levels of logic gates when you can represent any logic function, Boolean function, But these results say that with two layers like this, you might be in trouble in the sense that the number of units you need could be.",
            "Exponential in the input size to represent some functions, even though.",
            "That same function could be represented more efficiently with a deeper architecture.",
            "In particular, has that came up with classes of functions that can be representable very compactly with an architecture of depth K. But if you try to represent it with K -- 1 levels, you would need exponential size in the graph.",
            "Of course these are very specific kinds of functions essentially.",
            "Think of neural networks with positive weights.",
            "But still, it's kind of.",
            "Signal that we might want to investigate training deeper architectures."
        ],
        [
            "Before 2006.",
            "Researchers in the neural network community had tried for at least 20 years to train neural networks with more than one or two hidden layer, and they essentially failed.",
            "And one of the things I'm going to talk about in more detail today is.",
            "Explorations to try to understand why.",
            "So the 1st paper that really made a breakthrough here is the paper by Hinton owes endurant a fast learning algorithm for deep belief networks.",
            "2006 and a little bit later in the same year.",
            "Papers came out one from my group and one from Jan's group.",
            "About using similar principles to train.",
            "Deep architectures.",
            "And now I'm going to say a bit more about these algorithms."
        ],
        [
            "First of all, the algorithm that.",
            "Hinton and his collaborators presented is an algorithm for training a so called deep belief network and then.",
            "Tuning it's parameters so that it can be used for a classification task.",
            "The way it goes is like this.",
            "We're going to take our data.",
            "I'm going to use our data to train a simple one layer model called a restricted Boltzmann machine, which I will tell more about later.",
            "But you can think of it as a finding a transformation of the data into factors that are good at explaining the data.",
            "Once we have learned that transformation, how to get from inputs to these factors that we are learning?",
            "We're going to transform the data into that new representation, and we're going to take that.",
            "New representation as data for training a second.",
            "GBM a second restricted Boltzmann machine.",
            "And we can repeat that any number of times.",
            "And then we can add final layer which may now have extra variables for the classification we care about.",
            "And then there is a final step we can forget about the fact that this whole thing is a probabilistic graphical model and just use the forward paths in order to compute probabilities of classes given inputs and then fine tune do gradient descent and find you in all the parameters of this big model to optimize the classification error, minimize classification error, or more precisely.",
            "Maximize the log likelihood of the correct classification given inputs."
        ],
        [
            "One thing we did in this 2006 paper we produce the results with the deep belief networks and we try to see if the principle this principle of greedy layer wise initialization could be applied to other classes of learners and we tried auto encoders.",
            "So an autoencoder is is neural network with.",
            "And input one or more hidden layers and its target is input, so it's trying to reproduce the input.",
            "That's why it's called auto encoder.",
            "We encode the input into some representation and then we try to decode it.",
            "You can see that if the number of units here is larger than here, we might be in trouble and we might learn, learn the identity and not learn anything useful.",
            "And I'll say more about that, but assuming it doesn't learn the identity for some reason, because we put some constraint on that code, then actually does something interesting.",
            "And if you use the same greedy layer wise trick I just showed to initialize a deep neural network, it works not as well as stacked RBM's, but substantially better than training a regular deep neural network, or even a shallow neural network.",
            "So what you do is you train one autoencoder, then you forget about the decoder weights.",
            "Here, you just use the encoder weights here to find a new representation for the data, and then you can train another autoencoder and then again, once you've trained it, you can forget about the decoder part here and you get a.",
            "And then supervised transformation.",
            "That's a bit more nonlinear.",
            "An abstract, and you can go as many layers as you want, and at the end you can add a layer that's equivalent to logistic regression.",
            "And finally, do this supervised fine tuning in the same way that we've done for a deep belief networks."
        ],
        [
            "You can even do the same thing in a purely supervised way.",
            "So instead of training and autoencoder you can train 1 hidden layer neural network to predict the classes of interest and then throw away those weights.",
            "Just use the first hidden layer as a new representation and then train another one hidden layer, neural network and so on and so on.",
            "And in the end you get a deeper network and you can find you in all the parameters with respect to the target.",
            "Classes.",
            "This actually works better than training from the beginning supervised neural network.",
            "But it's always worse than using unsupervised learning methods that I just mentioned.",
            "At least in the experiments we."
        ],
        [
            "Dead.",
            "One thing that we found which.",
            "Isn't something that Jeff Hinton likes, but is what happens?",
            "And I don't like it either.",
            "I'm.",
            "It's not enough to do this sense.",
            "Provide training and get these nice features somehow you need to find choose those features for the task of interest, otherwise you don't get as good performance and this is what this graph shows we train.",
            "One of these stacked.",
            "BMS or autoencoders?",
            "And this is training time and up to here we only do unsupervised learning and but we look at the top layer unsupervised layer as features and we use them with the logistic regression to predict the class and so the features get better and better.",
            "Now if you continue doing it provides learning.",
            "There's still some improvement, but it doesn't help so much.",
            "If at this point now you decide to fine tune in a purely supervised way, all the parameters, then the air drops significantly.",
            "The test error drop significantly and ends up in a better place and will try to understand what is going on there.",
            "Auto encoder.",
            "Auto associator"
        ],
        [
            "Sorry yeah.",
            "Years ago because we didn't think about doing these experiments.",
            "Because nobody ever tried.",
            "Yeah, as far as I know.",
            "The only reason that nobody people tried stacking right until this before now.",
            "Not that I know of.",
            "They didn't realize how well you could work.",
            "So we experimented with a variant of the auto encoder that is working much better than they were in fact works as well or better than carbs for getting these deep networks, and it's called the denoising autoencoder and the idea is instead of reconstructing trying to reconstruct the input, we're going to take the input we're going to corrupt it by setting, say, by setting some of the values in the input to 0.",
            "And so the network only sees partial information about the actual input, but we're going to ask it to reconstruct the original, so that's why it's called the denoising autoencoders.",
            "Trying to undo some stochastic corruption that we're going to insert in the input.",
            "It's very similar to pseudo likelihood in the sense that we're trying to predict we're trying to reconstruct the whole input, so it's like an encoder, but we're also trying to predict the missing things that have been corrupted.",
            "And, um.",
            "It turns out."
        ],
        [
            "That it turns out that you can connect it to a generative model and the training criterion minimizes a variational lower bound on the generative model, so.",
            "To have a model that's similar to an RBM but not quite the same.",
            "And you can also interpret what is going on.",
            "Is learning a vector field.",
            "That that sends examples that are unlikely to.",
            "2 examples that are more likely.",
            "So what you do is you take let's say your data lies in your manifold.",
            "You take an example, you corrupt it randomly.",
            "And that's the encoder.",
            "That's not the encode.",
            "That's just the corruption.",
            "And then the encoder and decoder together.",
            "Try to map the corrupted input input back to places where it likes to see examples, and so you learn this vector field that pushes that points towards high density regions."
        ],
        [
            "You can stack these and get pretty good results.",
            "And here we show a result on oh it's called Infinite mnit switches amnist digits with.",
            "Every large number of transfer translations rotations, so we have a huge data set as big as you want and.",
            "And here we see different experiments with one or three layers using denoising, auto encoders or carbs to stack our deep architectures.",
            "And in this particular case, the best results is online.",
            "Test error the best results are obtained with this denoising between quarter with three layers and the PBM's here and then.",
            "This is the curve you get with purely supervised training and you can see that the slopes are different, suggesting that they're really going and this is on the log scale air.",
            "They're going to a different place and essentially a SIM card into a different error.",
            "Denoising trigger place.",
            "Two other types of data.",
            "Maybe we, I think we've tried on audio data, but.",
            "It's a good question."
        ],
        [
            "One of the motivations is for these things is.",
            "Sharing statistical strength is as I suggested, and you're doing that in two ways.",
            "One way that you're doing that, you're sharing statistical strength by having a deep architecture is is the following.",
            "Imagine the company function computed by one of the one of the features.",
            "Here it's a pain in the composition of the features computers at that level, which are obtained as a composition of features are playing at that level, and so on and so on.",
            "Now you could imagine having a separate.",
            "Function with a separate parametrization, say a separate neural network or whatever for each of the functions at that level, and then you wouldn't be sharing much between all these features by having these intermediate layers that are shared, your share a lot of the parameterization, and that helps a lot to generalize.",
            "Presumably, and in fact I think that's one of the main reasons these things are working in terms of the expressive power and it's connected to the things I mentioned.",
            "The theoretical results I mentioned earlier.",
            "It's also you also get sharing if you think of multitask learning where you're going to use some shared representation and use it for predicting different kinds of things that are interrelated.",
            "For example, our colleagues at NEC.",
            "I've been using these ideas to solve simultaneously multiple NLP tasks, so they're all related to the meaning and the syntax of the underlying input text."
        ],
        [
            "Would you also put different layer at the bottom for the different type of inputs?",
            "Yeah, you could do that.",
            "So one thing."
        ],
        [
            "That we are working on is.",
            "Let's say you are.",
            "You have multimodal input or you have.",
            "So one thing for example on data.",
            "So you have stereo input.",
            "So you can have different groups of inputs and you can process them in parallel and then join.",
            "These representations later on in those stages, just like it happens in the brain actually."
        ],
        [
            "So one question that I'm that I'm personally really interesting and I guess here I'm departing from the tutorial and more telling you about one of the questions I care about, which is why is this unsupervised pretraining trick working so well.",
            "And I've been asking myself this question for at least two years and there are two hypothesis that would be natural to think of in from the machine learning point of view, and one hypothesis is that this trick acts as a regularizer and the other hypothesis is that this trick acts as somehow helping optimization because we're optimizing a highly nonconvex criterion.",
            "There are many local minima, but maybe it just helps us to find better local minimum.",
            "Regarding the regularization hypothesis, the thinking is that, well, we're using unsupervised learning and maybe.",
            "So this acts as a regularizer because it's forcing the parameters to be close to the parameter to parameters of a similar model.",
            "I mean and associated model that is good at predicting X in a sense of maximizing P of X.",
            "And if representations that are good for representing X is in a model of P of X, are good as internal representations for maximizing P of Y given X.",
            "So it wise some class and X is an input then then this trick is going to work.",
            "So that's the assumption, and in fact we did.",
            "Try to test this in the sense that you can.",
            "You can try to train these things where the input distribution is completely unrelated to the thing you're trying to predict, like you can imagine, your inputs are uniform and then you're trying to predict some function here.",
            "And of course it fails completely because the unsupervised pretraining doesn't have anything interesting to do.",
            "On the converse, something like MNIST where there is a representation of the digits where the classes are well separated.",
            "So doing a good job of P of X is naturally separating the classes, so there's a kind of clustering in some space, which is not the image space where the digits are well separated, and then it becomes easy to classify.",
            "So this is the same argument, the cluster hypothesis.",
            "The same argument is being used in some supervised learning.",
            "And then the optimization hypothesis I mentioned already getting better local minima.",
            "But there's also something about the depth.",
            "So why is it harder to optimize deeper networks?",
            "And one hypothesis is maybe?",
            "Somehow the information about the error doesn't.",
            "Is propagated when you compute gradients, but it's not as informative for deeper layers than for layers that are near the target.",
            "And so by having a an algorithm that allows to train at each level and kind of local way helps a bit.",
            "Now clearly this is not enough, because we've seen that with purely supervised greedy.",
            "Layer wise it didn't help as much.",
            "So maybe the two hypothesis are combined in some way.",
            "But it's still.",
            "Yeah, maybe another reason is that the unsupervised learning accounts for things that would otherwise confuse the supervised learning.",
            "Like what for example in vision, right?",
            "There's all these effects going on in the image, and if you just try to learn a supervised learning model, disappoint?",
            "Oh yes, this is a very good point.",
            "So one of the things that we think is going on indeed is that when you learn these representations, the factors that are good at these intermediate layers are essentially capturing the invariants over there.",
            "By separating the factors of creation more precisely, you get to be independent of the variations that are not important for classification.",
            "So let's say you can separate the translations and rotations and the digital identity then.",
            "Say magically you have this in the intermediate presentation, then from there it would be very easy to do classification because you remove remove those or at least separated those very those types of variations.",
            "Yeah.",
            "Possibly in order to.",
            "Yes, absolutely so so.",
            "I've tried.",
            "These algorithms on on sort of a few data mining tasks where there's predicting the wise very difficult given the XN and the data has a lot of noise, an not clear structure an there is no gains aren't interesting at all, if any.",
            "It looks like in those vision tasks that we've been playing with, it really makes the most difference.",
            "These are tasks where humans.",
            "Can typically do 100% correct in many situations, and the underlying statistical structure is very rich, and yet it's difficult for current machine learning algorithms to capture it.",
            "And my hypothesis is because we're trying to learn the really complicated function.",
            "And we really don't know right now in machine learning how to learn these complicated functions.",
            "And maybe these deep architectures are not only able to represent efficiently these complicated functions, but because of those tricks optimizing these classes of functions is easier somehow."
        ],
        [
            "I'm let me come back to this hypothesis hypothesis.",
            "One things we did is trying trying to understand what is going on we plotted.",
            "The trajectory is during learning.",
            "So each of those circles represents one model at some point during training.",
            "And we have something like 500 different initialization, and where we're plugging here is a 2D dimensional reduction of the function computed by the network and the way we do that is we look at the outputs.",
            "On the test set and take that big vector of outputs over a whole test set.",
            "As a representation of the function.",
            "Captured by the model and then we do a dimensionality reduction and what you see is that they all start more or less in the same region in function space, even though it's running initialization and then they move kind of together and then eventually they all go to a separate little trajectory going to a separate local minima.",
            "So in fact this never happens that two different trajectories go to the same place.",
            "So there must be a lot of local minimum and the other thing is if this is random initialization.",
            "If instead of random initialization you initialize with these unsupervised training tricks, then well qualitatively looks similar.",
            "But the big difference is.",
            "It's just goes into completely different region of function space.",
            "They never overlap.",
            "So even though here we have random initialization, you would think covers the space of functions, not at all.",
            "Not at all.",
            "Blood clot, it seems to be much longer.",
            "Is that just a scaling?",
            "It's hard to know this is this is using T Sne and Disney.",
            "Disney can distort space in a strange way.",
            "It could be that this volume in function space is much smaller than this volume, so you can see the volumes preserved.",
            "If you do PCA and.",
            "Yeah, the volume is probably smaller here, but still.",
            "Surprising that you know the trajectory.",
            "Yeah, so there's yeah.",
            "So so one thing that happens here that we can measure is the variance.",
            "So the variance of the solutions here is much smaller than here, and this is probably so.",
            "This is an agreement with the regularization hypothesis that we are constraining using the improvised pre training solutions.",
            "Yeah, thanks.",
            "No, because so I don't show here what happens during unsupervised pretraining, because this is the function.",
            "After you start predicting outputs, so we so this is started.",
            "After you've done is provide training for awhile and then and then you start doing supervised training.",
            "The other one starts from random initialization.",
            "I mean these also start from initialization and then and then do is provide free training and then we're there."
        ],
        [
            "I mentioned the regularization hypothesis.",
            "We tested that an one of the experiments that clearly shows it is if you.",
            "If you decrease the number of hidden units, in other words, you restrict the capacity of the models.",
            "Sufficiently then, at some point this pretraining trick hurts.",
            "So in fact one thing we had seen before is that the the optimal number of hidden units that we need for these networks that are trained with the answer.",
            "My speech writing is usually much larger than the number of units you that would be optimal in terms of translation error 4 without pre training.",
            "So they like to have a lot of capacity and yet they don't overfit.",
            "And if you restricted capacity too much then in fact performance is worse than if you didn't do that.",
            "So this is an agreement with the regularization hypothesis and it seems to contradict the optimization hypothesis."
        ],
        [
            "On the other hand, there's another set of experiments which seem to agree with the optimization hypothesis, and we're going to try to reconcile all these things.",
            "So I showed this graph before.",
            "It shows online error.",
            "Each one here means 1,000,000 updates on this online amnist with infinite infinite number of examples.",
            "And one thing we see here.",
            "With online learning where you have sort of as many examples as you want, training both training error and online error and test error are smaller when using supervised pre training.",
            "Now clearly is the number of examples becomes large training error and online error.",
            "Angela Station error all going through the same place.",
            "If there was only a regularization effect, you would think that as the number of examples goes to Infinity, the regularization effect would disappear.",
            "But On the contrary, So what is going on?",
            "Yeah.",
            "Well, one thing to say here that I think is important is.",
            "What is nice about about these these results?",
            "It tells us something about.",
            "The regular training with gradient descent that even if you have as many parameters as you want, if I if I give you as many examples as you want, somehow they get stuck in sub optimal solution so they don't exploit the information in the training set.",
            "Now of course if you were to go to a purely nonparametric model where optimization is convex, you would be able to get eventually test error to go down.",
            "But then you would maybe pay other prices.",
            "But clearly with these kinds of architectures we can't seem to.",
            "When you don't use the unsupervised pre training, we can't seem to exploit all the information in the training set.",
            "So the explanation I have about why it's working."
        ],
        [
            "Better.",
            "Is has to do with the learning dynamics.",
            "Initially the weights are kind of small and then during training whether using us provides or supervises the weights increase in magnitude and so we started near the origin and then waits grow and at some point you kind of stuck in some regional space.",
            "You can think of as a quadrant and you can.",
            "You can see that visually.",
            "So these are the first layer filters on when you train on images.",
            "After the supervised learning and then after the pre training after the fine tuning.",
            "And if you look visually they look very similar.",
            "So the same unit before and after looks essentially the same.",
            "But yet this fine tuning is quite important and you get quite a big decrease in performance by doing it.",
            "So we're staying in the same quadrant but doing something to go.",
            "To a local minimum.",
            "OK, I guess.",
            "Set 1.",
            "Pussy.",
            "Can I ask a question for weeks when I come out saying yeah, go ahead while he's talking about?",
            "Practice to represent variation images, say so.",
            "Has anyone done a test where instead of just labeling whether it's a five, you also these nuisance factors actually become targets that you want to predict, so the goal is disabled in EC.",
            "Say it's five, it's rotated, 45 degrees is written in italic, and it's going to take one, I don't think.",
            "Those uses factors become relevant, right, right, right, so I didn't do that.",
            "Young did things in face recognition where you also trying to predict the angle rotation and the people at NEC in language modeling try to predict simultaneously all kinds of relevant things about the input like part of speech, chunking, semantic role labeling, labeling and another task I forgot."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks a lot, Russ.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Deep motivations for deep learning.",
                    "label": 1
                },
                {
                    "sent": "Should I speak louder?",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "So why do we care about investigating deep architectures?",
                    "label": 0
                },
                {
                    "sent": "There are a number of motivations.",
                    "label": 0
                },
                {
                    "sent": "One of the oldest motivation is that brains seem to have a deeper architecture.",
                    "label": 0
                },
                {
                    "sent": "Meaning that information is processed at.",
                    "label": 0
                },
                {
                    "sent": "At many levels of representation and abstraction.",
                    "label": 0
                },
                {
                    "sent": "And this is particularly obvious in the visual system.",
                    "label": 0
                },
                {
                    "sent": "Is elaborate a bit?",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "It seems more natural thinking about how humans organize their ideas and think about problems and even solve problems.",
                    "label": 1
                },
                {
                    "sent": "Finally, we've uncovered theoretical motivations for deep architectures having to do with their power of representation, and I'll see quickly a few words about that later.",
                    "label": 0
                },
                {
                    "sent": "An older motivation that has to do with the original motivation for neural networks, and that has to do with the use of distributed representations.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to say much in this presentation about distributed presentation, but it really is an important motivation.",
                    "label": 0
                },
                {
                    "sent": "And it means that we can efficiently represent.",
                    "label": 0
                },
                {
                    "sent": "Information with a number of of features.",
                    "label": 0
                },
                {
                    "sent": "A number of characteristics that doesn't have to be too large an yet one can represent a huge number of possible configurations of inputs.",
                    "label": 0
                },
                {
                    "sent": "Finally, a motivation that has come to be important in in recent years and ties in with a lot of work in machine learning in the last few years has to do with multitask learning and the notion that having intermediate representations that are learned.",
                    "label": 0
                },
                {
                    "sent": "Can be useful to generalize.",
                    "label": 0
                },
                {
                    "sent": "Better to share statistical strength among tasks among outputs or among components of even the same problem.",
                    "label": 0
                },
                {
                    "sent": "I didn't, I didn't, I just use different words so.",
                    "label": 0
                },
                {
                    "sent": "I said one of the theoretical motivations is that they can be very efficient in terms of representation, and that's what I meant.",
                    "label": 0
                },
                {
                    "sent": "I'll say few more words about that.",
                    "label": 0
                },
                {
                    "sent": "So oops.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "If you look in the visual system.",
                    "label": 1
                },
                {
                    "sent": "You find in the path from the retina to.",
                    "label": 0
                },
                {
                    "sent": "Higher levels of processing in the visual cortex.",
                    "label": 0
                },
                {
                    "sent": "A kind of feedforward Ann.",
                    "label": 0
                },
                {
                    "sent": "Also feedback paths from groups of units that do computations that neuro scientists start to believe that they understand.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That seemed to take the image and then represent it in different ways.",
                    "label": 0
                },
                {
                    "sent": "At the first really interesting level in area V1, images are represented by neurons that look like they're essentially doing edge detection.",
                    "label": 0
                },
                {
                    "sent": "Looking at different angles and scales and positions.",
                    "label": 1
                },
                {
                    "sent": "And if you look in area V2, it's more difficult to understand what is going on, but it looks like.",
                    "label": 0
                },
                {
                    "sent": "The neurons are essentially composing these edge vectors to detect slightly more complicated primitive shapes, combining, say, two edges.",
                    "label": 0
                },
                {
                    "sent": "And then if you go higher up, you start seeing.",
                    "label": 1
                },
                {
                    "sent": "Neurons that detect higher, higher level abstractions, little parts of objects, or even a simple objects like a face.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another motivation that I mentioned was looks like the way we process information just by asking people about how they do that and and.",
                    "label": 0
                },
                {
                    "sent": "And other ways of investigation suggests that we do organize our thoughts and our beliefs and our understanding of the world around us using multiple levels of abstractions and concepts that are organized hierarchically, where higher level concepts are defined in terms of lower level ones.",
                    "label": 1
                },
                {
                    "sent": "And even the way we learn seem to go that way by composing similar things that we've learned into more complicated representations or more complicated abstractions.",
                    "label": 0
                },
                {
                    "sent": "It also seems to be the way that we solve problems.",
                    "label": 0
                },
                {
                    "sent": "So when engineers solve problems, they typically try to break up the problem into subproblems, and very often the solution involves a sequence of operations, which essentially means composing things.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to these theoretical results that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "The bullet that I didn't speak enough of.",
                    "label": 0
                },
                {
                    "sent": "The notion of depth can be formalized using.",
                    "label": 0
                },
                {
                    "sent": "The computer science notions of graphs.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine a graph of computation where you start with some inputs and you combine.",
                    "label": 0
                },
                {
                    "sent": "You combine quantities associated with nodes with some operation, and you in each node you do some computation using predecessors in the graph, and you end up with some output that defines a graph, and you can talk about the depth of the graph as the number of the longest path from inputs to output.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you think about these graphs for representing functions, depending on what I'm allowed to put in these boxes, the computations I can get different kinds of graphs and the the shortest graph that I can get for representing a function.",
                    "label": 0
                },
                {
                    "sent": "The smallest graph can be different depending on the types of operations that are allowed.",
                    "label": 0
                },
                {
                    "sent": "So here I've used very simple operations in neural networks.",
                    "label": 0
                },
                {
                    "sent": "The operations that we allow are linear transformations followed by a non linearity.",
                    "label": 0
                },
                {
                    "sent": "And depending on the choice of the linear transformation, you get different operations and this is the typical feature you've seen for neural networks.",
                    "label": 0
                },
                {
                    "sent": "But you can.",
                    "label": 0
                },
                {
                    "sent": "You can apply this to a lot of.",
                    "label": 0
                },
                {
                    "sent": "Classes are functions that we care about in machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So about the depth of architectures.",
                    "label": 0
                },
                {
                    "sent": "There are theoretical arguments.",
                    "label": 0
                },
                {
                    "sent": "The most interesting one from Hastad and collaborators 86 and 91 and some of the work that I did with Olivia de Lalo.",
                    "label": 0
                },
                {
                    "sent": "That say roughly the following.",
                    "label": 0
                },
                {
                    "sent": "If we have two layers of logic gates, so that's the 86 result, or two layers of formal neurons.",
                    "label": 1
                },
                {
                    "sent": "That's the 1991 result, or two layers of RBF units.",
                    "label": 0
                },
                {
                    "sent": "Then of course so that we know that this is not these results.",
                    "label": 0
                },
                {
                    "sent": "We know that we can have these constitute universal approximators, so for example, 2 levels of logic gates when you can represent any logic function, Boolean function, But these results say that with two layers like this, you might be in trouble in the sense that the number of units you need could be.",
                    "label": 0
                },
                {
                    "sent": "Exponential in the input size to represent some functions, even though.",
                    "label": 0
                },
                {
                    "sent": "That same function could be represented more efficiently with a deeper architecture.",
                    "label": 1
                },
                {
                    "sent": "In particular, has that came up with classes of functions that can be representable very compactly with an architecture of depth K. But if you try to represent it with K -- 1 levels, you would need exponential size in the graph.",
                    "label": 0
                },
                {
                    "sent": "Of course these are very specific kinds of functions essentially.",
                    "label": 0
                },
                {
                    "sent": "Think of neural networks with positive weights.",
                    "label": 0
                },
                {
                    "sent": "But still, it's kind of.",
                    "label": 0
                },
                {
                    "sent": "Signal that we might want to investigate training deeper architectures.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before 2006.",
                    "label": 0
                },
                {
                    "sent": "Researchers in the neural network community had tried for at least 20 years to train neural networks with more than one or two hidden layer, and they essentially failed.",
                    "label": 0
                },
                {
                    "sent": "And one of the things I'm going to talk about in more detail today is.",
                    "label": 0
                },
                {
                    "sent": "Explorations to try to understand why.",
                    "label": 0
                },
                {
                    "sent": "So the 1st paper that really made a breakthrough here is the paper by Hinton owes endurant a fast learning algorithm for deep belief networks.",
                    "label": 1
                },
                {
                    "sent": "2006 and a little bit later in the same year.",
                    "label": 0
                },
                {
                    "sent": "Papers came out one from my group and one from Jan's group.",
                    "label": 0
                },
                {
                    "sent": "About using similar principles to train.",
                    "label": 0
                },
                {
                    "sent": "Deep architectures.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to say a bit more about these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, the algorithm that.",
                    "label": 0
                },
                {
                    "sent": "Hinton and his collaborators presented is an algorithm for training a so called deep belief network and then.",
                    "label": 1
                },
                {
                    "sent": "Tuning it's parameters so that it can be used for a classification task.",
                    "label": 0
                },
                {
                    "sent": "The way it goes is like this.",
                    "label": 0
                },
                {
                    "sent": "We're going to take our data.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use our data to train a simple one layer model called a restricted Boltzmann machine, which I will tell more about later.",
                    "label": 0
                },
                {
                    "sent": "But you can think of it as a finding a transformation of the data into factors that are good at explaining the data.",
                    "label": 0
                },
                {
                    "sent": "Once we have learned that transformation, how to get from inputs to these factors that we are learning?",
                    "label": 0
                },
                {
                    "sent": "We're going to transform the data into that new representation, and we're going to take that.",
                    "label": 0
                },
                {
                    "sent": "New representation as data for training a second.",
                    "label": 1
                },
                {
                    "sent": "GBM a second restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "And we can repeat that any number of times.",
                    "label": 0
                },
                {
                    "sent": "And then we can add final layer which may now have extra variables for the classification we care about.",
                    "label": 0
                },
                {
                    "sent": "And then there is a final step we can forget about the fact that this whole thing is a probabilistic graphical model and just use the forward paths in order to compute probabilities of classes given inputs and then fine tune do gradient descent and find you in all the parameters of this big model to optimize the classification error, minimize classification error, or more precisely.",
                    "label": 0
                },
                {
                    "sent": "Maximize the log likelihood of the correct classification given inputs.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing we did in this 2006 paper we produce the results with the deep belief networks and we try to see if the principle this principle of greedy layer wise initialization could be applied to other classes of learners and we tried auto encoders.",
                    "label": 0
                },
                {
                    "sent": "So an autoencoder is is neural network with.",
                    "label": 0
                },
                {
                    "sent": "And input one or more hidden layers and its target is input, so it's trying to reproduce the input.",
                    "label": 0
                },
                {
                    "sent": "That's why it's called auto encoder.",
                    "label": 0
                },
                {
                    "sent": "We encode the input into some representation and then we try to decode it.",
                    "label": 0
                },
                {
                    "sent": "You can see that if the number of units here is larger than here, we might be in trouble and we might learn, learn the identity and not learn anything useful.",
                    "label": 0
                },
                {
                    "sent": "And I'll say more about that, but assuming it doesn't learn the identity for some reason, because we put some constraint on that code, then actually does something interesting.",
                    "label": 0
                },
                {
                    "sent": "And if you use the same greedy layer wise trick I just showed to initialize a deep neural network, it works not as well as stacked RBM's, but substantially better than training a regular deep neural network, or even a shallow neural network.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you train one autoencoder, then you forget about the decoder weights.",
                    "label": 0
                },
                {
                    "sent": "Here, you just use the encoder weights here to find a new representation for the data, and then you can train another autoencoder and then again, once you've trained it, you can forget about the decoder part here and you get a.",
                    "label": 0
                },
                {
                    "sent": "And then supervised transformation.",
                    "label": 0
                },
                {
                    "sent": "That's a bit more nonlinear.",
                    "label": 0
                },
                {
                    "sent": "An abstract, and you can go as many layers as you want, and at the end you can add a layer that's equivalent to logistic regression.",
                    "label": 0
                },
                {
                    "sent": "And finally, do this supervised fine tuning in the same way that we've done for a deep belief networks.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can even do the same thing in a purely supervised way.",
                    "label": 0
                },
                {
                    "sent": "So instead of training and autoencoder you can train 1 hidden layer neural network to predict the classes of interest and then throw away those weights.",
                    "label": 0
                },
                {
                    "sent": "Just use the first hidden layer as a new representation and then train another one hidden layer, neural network and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "And in the end you get a deeper network and you can find you in all the parameters with respect to the target.",
                    "label": 0
                },
                {
                    "sent": "Classes.",
                    "label": 0
                },
                {
                    "sent": "This actually works better than training from the beginning supervised neural network.",
                    "label": 1
                },
                {
                    "sent": "But it's always worse than using unsupervised learning methods that I just mentioned.",
                    "label": 0
                },
                {
                    "sent": "At least in the experiments we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dead.",
                    "label": 0
                },
                {
                    "sent": "One thing that we found which.",
                    "label": 0
                },
                {
                    "sent": "Isn't something that Jeff Hinton likes, but is what happens?",
                    "label": 0
                },
                {
                    "sent": "And I don't like it either.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "It's not enough to do this sense.",
                    "label": 0
                },
                {
                    "sent": "Provide training and get these nice features somehow you need to find choose those features for the task of interest, otherwise you don't get as good performance and this is what this graph shows we train.",
                    "label": 0
                },
                {
                    "sent": "One of these stacked.",
                    "label": 0
                },
                {
                    "sent": "BMS or autoencoders?",
                    "label": 0
                },
                {
                    "sent": "And this is training time and up to here we only do unsupervised learning and but we look at the top layer unsupervised layer as features and we use them with the logistic regression to predict the class and so the features get better and better.",
                    "label": 0
                },
                {
                    "sent": "Now if you continue doing it provides learning.",
                    "label": 0
                },
                {
                    "sent": "There's still some improvement, but it doesn't help so much.",
                    "label": 0
                },
                {
                    "sent": "If at this point now you decide to fine tune in a purely supervised way, all the parameters, then the air drops significantly.",
                    "label": 0
                },
                {
                    "sent": "The test error drop significantly and ends up in a better place and will try to understand what is going on there.",
                    "label": 0
                },
                {
                    "sent": "Auto encoder.",
                    "label": 0
                },
                {
                    "sent": "Auto associator",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry yeah.",
                    "label": 0
                },
                {
                    "sent": "Years ago because we didn't think about doing these experiments.",
                    "label": 0
                },
                {
                    "sent": "Because nobody ever tried.",
                    "label": 0
                },
                {
                    "sent": "Yeah, as far as I know.",
                    "label": 0
                },
                {
                    "sent": "The only reason that nobody people tried stacking right until this before now.",
                    "label": 0
                },
                {
                    "sent": "Not that I know of.",
                    "label": 0
                },
                {
                    "sent": "They didn't realize how well you could work.",
                    "label": 0
                },
                {
                    "sent": "So we experimented with a variant of the auto encoder that is working much better than they were in fact works as well or better than carbs for getting these deep networks, and it's called the denoising autoencoder and the idea is instead of reconstructing trying to reconstruct the input, we're going to take the input we're going to corrupt it by setting, say, by setting some of the values in the input to 0.",
                    "label": 1
                },
                {
                    "sent": "And so the network only sees partial information about the actual input, but we're going to ask it to reconstruct the original, so that's why it's called the denoising autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Trying to undo some stochastic corruption that we're going to insert in the input.",
                    "label": 0
                },
                {
                    "sent": "It's very similar to pseudo likelihood in the sense that we're trying to predict we're trying to reconstruct the whole input, so it's like an encoder, but we're also trying to predict the missing things that have been corrupted.",
                    "label": 0
                },
                {
                    "sent": "And, um.",
                    "label": 0
                },
                {
                    "sent": "It turns out.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That it turns out that you can connect it to a generative model and the training criterion minimizes a variational lower bound on the generative model, so.",
                    "label": 1
                },
                {
                    "sent": "To have a model that's similar to an RBM but not quite the same.",
                    "label": 0
                },
                {
                    "sent": "And you can also interpret what is going on.",
                    "label": 1
                },
                {
                    "sent": "Is learning a vector field.",
                    "label": 0
                },
                {
                    "sent": "That that sends examples that are unlikely to.",
                    "label": 0
                },
                {
                    "sent": "2 examples that are more likely.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you take let's say your data lies in your manifold.",
                    "label": 0
                },
                {
                    "sent": "You take an example, you corrupt it randomly.",
                    "label": 0
                },
                {
                    "sent": "And that's the encoder.",
                    "label": 0
                },
                {
                    "sent": "That's not the encode.",
                    "label": 0
                },
                {
                    "sent": "That's just the corruption.",
                    "label": 0
                },
                {
                    "sent": "And then the encoder and decoder together.",
                    "label": 0
                },
                {
                    "sent": "Try to map the corrupted input input back to places where it likes to see examples, and so you learn this vector field that pushes that points towards high density regions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can stack these and get pretty good results.",
                    "label": 0
                },
                {
                    "sent": "And here we show a result on oh it's called Infinite mnit switches amnist digits with.",
                    "label": 0
                },
                {
                    "sent": "Every large number of transfer translations rotations, so we have a huge data set as big as you want and.",
                    "label": 0
                },
                {
                    "sent": "And here we see different experiments with one or three layers using denoising, auto encoders or carbs to stack our deep architectures.",
                    "label": 0
                },
                {
                    "sent": "And in this particular case, the best results is online.",
                    "label": 0
                },
                {
                    "sent": "Test error the best results are obtained with this denoising between quarter with three layers and the PBM's here and then.",
                    "label": 0
                },
                {
                    "sent": "This is the curve you get with purely supervised training and you can see that the slopes are different, suggesting that they're really going and this is on the log scale air.",
                    "label": 0
                },
                {
                    "sent": "They're going to a different place and essentially a SIM card into a different error.",
                    "label": 0
                },
                {
                    "sent": "Denoising trigger place.",
                    "label": 0
                },
                {
                    "sent": "Two other types of data.",
                    "label": 0
                },
                {
                    "sent": "Maybe we, I think we've tried on audio data, but.",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the motivations is for these things is.",
                    "label": 0
                },
                {
                    "sent": "Sharing statistical strength is as I suggested, and you're doing that in two ways.",
                    "label": 1
                },
                {
                    "sent": "One way that you're doing that, you're sharing statistical strength by having a deep architecture is is the following.",
                    "label": 0
                },
                {
                    "sent": "Imagine the company function computed by one of the one of the features.",
                    "label": 0
                },
                {
                    "sent": "Here it's a pain in the composition of the features computers at that level, which are obtained as a composition of features are playing at that level, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "Now you could imagine having a separate.",
                    "label": 0
                },
                {
                    "sent": "Function with a separate parametrization, say a separate neural network or whatever for each of the functions at that level, and then you wouldn't be sharing much between all these features by having these intermediate layers that are shared, your share a lot of the parameterization, and that helps a lot to generalize.",
                    "label": 0
                },
                {
                    "sent": "Presumably, and in fact I think that's one of the main reasons these things are working in terms of the expressive power and it's connected to the things I mentioned.",
                    "label": 0
                },
                {
                    "sent": "The theoretical results I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "It's also you also get sharing if you think of multitask learning where you're going to use some shared representation and use it for predicting different kinds of things that are interrelated.",
                    "label": 0
                },
                {
                    "sent": "For example, our colleagues at NEC.",
                    "label": 0
                },
                {
                    "sent": "I've been using these ideas to solve simultaneously multiple NLP tasks, so they're all related to the meaning and the syntax of the underlying input text.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would you also put different layer at the bottom for the different type of inputs?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could do that.",
                    "label": 0
                },
                {
                    "sent": "So one thing.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we are working on is.",
                    "label": 0
                },
                {
                    "sent": "Let's say you are.",
                    "label": 0
                },
                {
                    "sent": "You have multimodal input or you have.",
                    "label": 0
                },
                {
                    "sent": "So one thing for example on data.",
                    "label": 0
                },
                {
                    "sent": "So you have stereo input.",
                    "label": 0
                },
                {
                    "sent": "So you can have different groups of inputs and you can process them in parallel and then join.",
                    "label": 0
                },
                {
                    "sent": "These representations later on in those stages, just like it happens in the brain actually.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one question that I'm that I'm personally really interesting and I guess here I'm departing from the tutorial and more telling you about one of the questions I care about, which is why is this unsupervised pretraining trick working so well.",
                    "label": 1
                },
                {
                    "sent": "And I've been asking myself this question for at least two years and there are two hypothesis that would be natural to think of in from the machine learning point of view, and one hypothesis is that this trick acts as a regularizer and the other hypothesis is that this trick acts as somehow helping optimization because we're optimizing a highly nonconvex criterion.",
                    "label": 1
                },
                {
                    "sent": "There are many local minima, but maybe it just helps us to find better local minimum.",
                    "label": 1
                },
                {
                    "sent": "Regarding the regularization hypothesis, the thinking is that, well, we're using unsupervised learning and maybe.",
                    "label": 0
                },
                {
                    "sent": "So this acts as a regularizer because it's forcing the parameters to be close to the parameter to parameters of a similar model.",
                    "label": 0
                },
                {
                    "sent": "I mean and associated model that is good at predicting X in a sense of maximizing P of X.",
                    "label": 1
                },
                {
                    "sent": "And if representations that are good for representing X is in a model of P of X, are good as internal representations for maximizing P of Y given X.",
                    "label": 0
                },
                {
                    "sent": "So it wise some class and X is an input then then this trick is going to work.",
                    "label": 0
                },
                {
                    "sent": "So that's the assumption, and in fact we did.",
                    "label": 0
                },
                {
                    "sent": "Try to test this in the sense that you can.",
                    "label": 0
                },
                {
                    "sent": "You can try to train these things where the input distribution is completely unrelated to the thing you're trying to predict, like you can imagine, your inputs are uniform and then you're trying to predict some function here.",
                    "label": 0
                },
                {
                    "sent": "And of course it fails completely because the unsupervised pretraining doesn't have anything interesting to do.",
                    "label": 0
                },
                {
                    "sent": "On the converse, something like MNIST where there is a representation of the digits where the classes are well separated.",
                    "label": 0
                },
                {
                    "sent": "So doing a good job of P of X is naturally separating the classes, so there's a kind of clustering in some space, which is not the image space where the digits are well separated, and then it becomes easy to classify.",
                    "label": 0
                },
                {
                    "sent": "So this is the same argument, the cluster hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The same argument is being used in some supervised learning.",
                    "label": 1
                },
                {
                    "sent": "And then the optimization hypothesis I mentioned already getting better local minima.",
                    "label": 0
                },
                {
                    "sent": "But there's also something about the depth.",
                    "label": 0
                },
                {
                    "sent": "So why is it harder to optimize deeper networks?",
                    "label": 0
                },
                {
                    "sent": "And one hypothesis is maybe?",
                    "label": 0
                },
                {
                    "sent": "Somehow the information about the error doesn't.",
                    "label": 0
                },
                {
                    "sent": "Is propagated when you compute gradients, but it's not as informative for deeper layers than for layers that are near the target.",
                    "label": 0
                },
                {
                    "sent": "And so by having a an algorithm that allows to train at each level and kind of local way helps a bit.",
                    "label": 0
                },
                {
                    "sent": "Now clearly this is not enough, because we've seen that with purely supervised greedy.",
                    "label": 0
                },
                {
                    "sent": "Layer wise it didn't help as much.",
                    "label": 0
                },
                {
                    "sent": "So maybe the two hypothesis are combined in some way.",
                    "label": 0
                },
                {
                    "sent": "But it's still.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe another reason is that the unsupervised learning accounts for things that would otherwise confuse the supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Like what for example in vision, right?",
                    "label": 0
                },
                {
                    "sent": "There's all these effects going on in the image, and if you just try to learn a supervised learning model, disappoint?",
                    "label": 0
                },
                {
                    "sent": "Oh yes, this is a very good point.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that we think is going on indeed is that when you learn these representations, the factors that are good at these intermediate layers are essentially capturing the invariants over there.",
                    "label": 0
                },
                {
                    "sent": "By separating the factors of creation more precisely, you get to be independent of the variations that are not important for classification.",
                    "label": 0
                },
                {
                    "sent": "So let's say you can separate the translations and rotations and the digital identity then.",
                    "label": 0
                },
                {
                    "sent": "Say magically you have this in the intermediate presentation, then from there it would be very easy to do classification because you remove remove those or at least separated those very those types of variations.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Possibly in order to.",
                    "label": 0
                },
                {
                    "sent": "Yes, absolutely so so.",
                    "label": 0
                },
                {
                    "sent": "I've tried.",
                    "label": 0
                },
                {
                    "sent": "These algorithms on on sort of a few data mining tasks where there's predicting the wise very difficult given the XN and the data has a lot of noise, an not clear structure an there is no gains aren't interesting at all, if any.",
                    "label": 0
                },
                {
                    "sent": "It looks like in those vision tasks that we've been playing with, it really makes the most difference.",
                    "label": 0
                },
                {
                    "sent": "These are tasks where humans.",
                    "label": 0
                },
                {
                    "sent": "Can typically do 100% correct in many situations, and the underlying statistical structure is very rich, and yet it's difficult for current machine learning algorithms to capture it.",
                    "label": 0
                },
                {
                    "sent": "And my hypothesis is because we're trying to learn the really complicated function.",
                    "label": 0
                },
                {
                    "sent": "And we really don't know right now in machine learning how to learn these complicated functions.",
                    "label": 0
                },
                {
                    "sent": "And maybe these deep architectures are not only able to represent efficiently these complicated functions, but because of those tricks optimizing these classes of functions is easier somehow.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm let me come back to this hypothesis hypothesis.",
                    "label": 0
                },
                {
                    "sent": "One things we did is trying trying to understand what is going on we plotted.",
                    "label": 0
                },
                {
                    "sent": "The trajectory is during learning.",
                    "label": 0
                },
                {
                    "sent": "So each of those circles represents one model at some point during training.",
                    "label": 0
                },
                {
                    "sent": "And we have something like 500 different initialization, and where we're plugging here is a 2D dimensional reduction of the function computed by the network and the way we do that is we look at the outputs.",
                    "label": 0
                },
                {
                    "sent": "On the test set and take that big vector of outputs over a whole test set.",
                    "label": 0
                },
                {
                    "sent": "As a representation of the function.",
                    "label": 0
                },
                {
                    "sent": "Captured by the model and then we do a dimensionality reduction and what you see is that they all start more or less in the same region in function space, even though it's running initialization and then they move kind of together and then eventually they all go to a separate little trajectory going to a separate local minima.",
                    "label": 0
                },
                {
                    "sent": "So in fact this never happens that two different trajectories go to the same place.",
                    "label": 0
                },
                {
                    "sent": "So there must be a lot of local minimum and the other thing is if this is random initialization.",
                    "label": 0
                },
                {
                    "sent": "If instead of random initialization you initialize with these unsupervised training tricks, then well qualitatively looks similar.",
                    "label": 0
                },
                {
                    "sent": "But the big difference is.",
                    "label": 0
                },
                {
                    "sent": "It's just goes into completely different region of function space.",
                    "label": 1
                },
                {
                    "sent": "They never overlap.",
                    "label": 0
                },
                {
                    "sent": "So even though here we have random initialization, you would think covers the space of functions, not at all.",
                    "label": 0
                },
                {
                    "sent": "Not at all.",
                    "label": 0
                },
                {
                    "sent": "Blood clot, it seems to be much longer.",
                    "label": 0
                },
                {
                    "sent": "Is that just a scaling?",
                    "label": 0
                },
                {
                    "sent": "It's hard to know this is this is using T Sne and Disney.",
                    "label": 0
                },
                {
                    "sent": "Disney can distort space in a strange way.",
                    "label": 0
                },
                {
                    "sent": "It could be that this volume in function space is much smaller than this volume, so you can see the volumes preserved.",
                    "label": 1
                },
                {
                    "sent": "If you do PCA and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the volume is probably smaller here, but still.",
                    "label": 0
                },
                {
                    "sent": "Surprising that you know the trajectory.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's yeah.",
                    "label": 0
                },
                {
                    "sent": "So so one thing that happens here that we can measure is the variance.",
                    "label": 0
                },
                {
                    "sent": "So the variance of the solutions here is much smaller than here, and this is probably so.",
                    "label": 0
                },
                {
                    "sent": "This is an agreement with the regularization hypothesis that we are constraining using the improvised pre training solutions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks.",
                    "label": 0
                },
                {
                    "sent": "No, because so I don't show here what happens during unsupervised pretraining, because this is the function.",
                    "label": 0
                },
                {
                    "sent": "After you start predicting outputs, so we so this is started.",
                    "label": 0
                },
                {
                    "sent": "After you've done is provide training for awhile and then and then you start doing supervised training.",
                    "label": 0
                },
                {
                    "sent": "The other one starts from random initialization.",
                    "label": 0
                },
                {
                    "sent": "I mean these also start from initialization and then and then do is provide free training and then we're there.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mentioned the regularization hypothesis.",
                    "label": 0
                },
                {
                    "sent": "We tested that an one of the experiments that clearly shows it is if you.",
                    "label": 0
                },
                {
                    "sent": "If you decrease the number of hidden units, in other words, you restrict the capacity of the models.",
                    "label": 1
                },
                {
                    "sent": "Sufficiently then, at some point this pretraining trick hurts.",
                    "label": 0
                },
                {
                    "sent": "So in fact one thing we had seen before is that the the optimal number of hidden units that we need for these networks that are trained with the answer.",
                    "label": 1
                },
                {
                    "sent": "My speech writing is usually much larger than the number of units you that would be optimal in terms of translation error 4 without pre training.",
                    "label": 0
                },
                {
                    "sent": "So they like to have a lot of capacity and yet they don't overfit.",
                    "label": 0
                },
                {
                    "sent": "And if you restricted capacity too much then in fact performance is worse than if you didn't do that.",
                    "label": 0
                },
                {
                    "sent": "So this is an agreement with the regularization hypothesis and it seems to contradict the optimization hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the other hand, there's another set of experiments which seem to agree with the optimization hypothesis, and we're going to try to reconcile all these things.",
                    "label": 0
                },
                {
                    "sent": "So I showed this graph before.",
                    "label": 0
                },
                {
                    "sent": "It shows online error.",
                    "label": 0
                },
                {
                    "sent": "Each one here means 1,000,000 updates on this online amnist with infinite infinite number of examples.",
                    "label": 0
                },
                {
                    "sent": "And one thing we see here.",
                    "label": 1
                },
                {
                    "sent": "With online learning where you have sort of as many examples as you want, training both training error and online error and test error are smaller when using supervised pre training.",
                    "label": 1
                },
                {
                    "sent": "Now clearly is the number of examples becomes large training error and online error.",
                    "label": 0
                },
                {
                    "sent": "Angela Station error all going through the same place.",
                    "label": 0
                },
                {
                    "sent": "If there was only a regularization effect, you would think that as the number of examples goes to Infinity, the regularization effect would disappear.",
                    "label": 0
                },
                {
                    "sent": "But On the contrary, So what is going on?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, one thing to say here that I think is important is.",
                    "label": 0
                },
                {
                    "sent": "What is nice about about these these results?",
                    "label": 0
                },
                {
                    "sent": "It tells us something about.",
                    "label": 0
                },
                {
                    "sent": "The regular training with gradient descent that even if you have as many parameters as you want, if I if I give you as many examples as you want, somehow they get stuck in sub optimal solution so they don't exploit the information in the training set.",
                    "label": 0
                },
                {
                    "sent": "Now of course if you were to go to a purely nonparametric model where optimization is convex, you would be able to get eventually test error to go down.",
                    "label": 0
                },
                {
                    "sent": "But then you would maybe pay other prices.",
                    "label": 0
                },
                {
                    "sent": "But clearly with these kinds of architectures we can't seem to.",
                    "label": 0
                },
                {
                    "sent": "When you don't use the unsupervised pre training, we can't seem to exploit all the information in the training set.",
                    "label": 0
                },
                {
                    "sent": "So the explanation I have about why it's working.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better.",
                    "label": 0
                },
                {
                    "sent": "Is has to do with the learning dynamics.",
                    "label": 0
                },
                {
                    "sent": "Initially the weights are kind of small and then during training whether using us provides or supervises the weights increase in magnitude and so we started near the origin and then waits grow and at some point you kind of stuck in some regional space.",
                    "label": 0
                },
                {
                    "sent": "You can think of as a quadrant and you can.",
                    "label": 0
                },
                {
                    "sent": "You can see that visually.",
                    "label": 0
                },
                {
                    "sent": "So these are the first layer filters on when you train on images.",
                    "label": 0
                },
                {
                    "sent": "After the supervised learning and then after the pre training after the fine tuning.",
                    "label": 0
                },
                {
                    "sent": "And if you look visually they look very similar.",
                    "label": 0
                },
                {
                    "sent": "So the same unit before and after looks essentially the same.",
                    "label": 0
                },
                {
                    "sent": "But yet this fine tuning is quite important and you get quite a big decrease in performance by doing it.",
                    "label": 0
                },
                {
                    "sent": "So we're staying in the same quadrant but doing something to go.",
                    "label": 0
                },
                {
                    "sent": "To a local minimum.",
                    "label": 0
                },
                {
                    "sent": "OK, I guess.",
                    "label": 0
                },
                {
                    "sent": "Set 1.",
                    "label": 0
                },
                {
                    "sent": "Pussy.",
                    "label": 0
                },
                {
                    "sent": "Can I ask a question for weeks when I come out saying yeah, go ahead while he's talking about?",
                    "label": 0
                },
                {
                    "sent": "Practice to represent variation images, say so.",
                    "label": 0
                },
                {
                    "sent": "Has anyone done a test where instead of just labeling whether it's a five, you also these nuisance factors actually become targets that you want to predict, so the goal is disabled in EC.",
                    "label": 0
                },
                {
                    "sent": "Say it's five, it's rotated, 45 degrees is written in italic, and it's going to take one, I don't think.",
                    "label": 0
                },
                {
                    "sent": "Those uses factors become relevant, right, right, right, so I didn't do that.",
                    "label": 0
                },
                {
                    "sent": "Young did things in face recognition where you also trying to predict the angle rotation and the people at NEC in language modeling try to predict simultaneously all kinds of relevant things about the input like part of speech, chunking, semantic role labeling, labeling and another task I forgot.",
                    "label": 0
                }
            ]
        }
    }
}