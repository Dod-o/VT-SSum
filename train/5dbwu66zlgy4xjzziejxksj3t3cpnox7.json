{
    "id": "5dbwu66zlgy4xjzziejxksj3t3cpnox7",
    "title": "Kernel Representations and Kernel Density Estimation",
    "info": {
        "author": [
            "Peter J. Bickel, Department of Statistics, UC Berkeley"
        ],
        "published": "Dec. 18, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/sip08_bickel_krakd/",
    "segmentation": [
        [
            "Pleasure to be here and thanks thanks to the organizers.",
            "So I actually hesitated, but to speak about and and I actually being foolhardy.",
            "Because I'm speaking about something that I only really got involved in about.",
            "Three or four weeks ago and and learn stuff you know the week before I came here and there are really hard results, but I think it's interesting and I think I'd like to expose people to it.",
            "So, so the story is kernel representations of kernel estimation, so.",
            "Let me."
        ],
        [
            "So here's the outline.",
            "First of all, introduce you to what kernel representations in Laplacian.",
            "Kernel matrices are.",
            "I'll talk about application statistics, which have mainly appeared in the machine learning literature in fact.",
            "Then I'll talk about the uses of kernels for clustering.",
            "Some asymptotic switch was done by coaching schemes.",
            "Unnayan, Belkin and Yogi and.",
            "More recent papers.",
            "Owen Adler at all, which I'll focus on.",
            "And then I'll make the connection to density estimation, which which which somehow.",
            "I think maybe people know, but I at least always thought the two are quite different.",
            "You know, these kernel representations and kernel density estimation, but in fact they're not so different.",
            "They're quite closely related.",
            "And so heuristics for scale going to zero heuristics for scaled down to zero with laplacian's connections with large DD being dimension and some future in some future directions.",
            "So."
        ],
        [
            "So let's start with kernel representations.",
            "So the idea is that you take.",
            "Your data, which you always assume is in high dimensional space.",
            "And let's for the time being, suppose that we have a density with respect to big measure in this high dimensional space.",
            "And you have a kernel which is functional Ardito Ardito R symmetric and positive definite.",
            "But the kernel is positive definite.",
            "And then the idea is that you represent you basically in some way.",
            "Thank you.",
            "You're complicating your life because instead of staying in D dimensional space, you now go into infinite dimensional space.",
            "And you make your data point correspond to the function K of X dot.",
            "And the prototypical K that we're all used to is K of XY equals X transpose Y, but in fact the K that's used in general is the Gaussian curve.",
            "And then having done this representation.",
            "So what do you what do you do then well?",
            "There are various analogues to the usual.",
            "Things that we try to do in multivariate analysis that you can now do with these objects as opposed to the original.",
            "Data."
        ],
        [
            "So here's the first application.",
            "They really arose in estimation of functions using splines.",
            "So when you have when you put Bayes priors on Sobolev spaces.",
            "You end up being naturally.",
            "Coming to.",
            "Functions which are.",
            "The linear span of such kernels.",
            "Which can be closed in the inner product.",
            "I mean I should have said this earlier.",
            "The kernel can be thought of as an inner product, right?",
            "It induces?",
            "So you basically have a Hilbert space which you can get by looking at linear combinations of.",
            "Kernels of kernel function, then closing it in that in that in that in that norm.",
            "And and if you do that, for example, soboleff spaces are reproducing kernel Hilbert spaces.",
            "And the tip the critical thing about these things is or one of the critical things is that they have the property that the evaluation functionals are continuous.",
            "So if you take if you there functions there there, there, there, there there function spaces where I want to go send F into F of X.",
            "That's a bounded linear function.",
            "And the Hilbert space is, and therefore there is in fact.",
            "A kernel is a function which represent of that, but it's not necessarily obvious defined.",
            "For instance, even in the case of a subspace.",
            "The other, I guess, the second time that so this is probably very chunky, but in fact I'm sure.",
            "I saw it sort of was involved.",
            "Nicks work which was based on much earlier work of of guess Bravoman and others were he showed that for support vector machines you could really if you wanted to sort of build up you had a given set of variables, but you then wanted to enrich your set of variables by looking at products and things like that that you could.",
            "So suddenly making the set of variables much larger.",
            "But you could actually carry out the the the progression of computationally very efficient way by essentially thinking of kernel representations.",
            "I'm not going to dwell on this because it's not, it's not.",
            "That's not the aspect that I'm."
        ],
        [
            "Interested at the moment.",
            "Then there's a couple back.",
            "I mean, Mike Jordan, Anne Francis Bach, and.",
            "And others.",
            "Proposed things like kernel ICA.",
            "So kernel, ICA independent component analysis is that you basically note that if you're.",
            "A Hilbert space of functions is dense in L2, for example then.",
            "Independence is characterized by simply requiring that the correlation of K of XX&K of YY is 0 for all X&Y.",
            "Whereas to do it in general you have to take this.",
            "You have to take the so called Rennie correlation at the soup.",
            "Overall, F of X&G of Y, but this is enough and then you start to to operate on that and we start to look at empirically.",
            "You then translate the translation of what that what that statement becomes, and then you can do analysis of, well, independent component analysis.",
            "We were trying to find representation of a multivariate vector as a matrix transform a vector independent, but not necessarily identically distributed random variables, but not necessarily Gaussian.",
            "But you can't do it for the Gaussian.",
            "Um?",
            "So so.",
            "So one way of thinking about it is that you're sort of."
        ],
        [
            "This enables you to take your data and code it in a nonlinear way.",
            "So that things like correlation which you know for instance in the in the in the in the usual case, do not characterize independence.",
            "When you code them in this way, now they do character accident benefits.",
            "Um?",
            "Now here's the aspect that I'm most interested in and that has to do with clustering and spectral clustering.",
            "So you form this matrixcare of XI XJ.",
            "Which is an N by N matrix.",
            "But where if in fact you have the usual inner product.",
            "The eigenvalues and eigenvectors of that matrix are the same as the eigenvalues and eigenvectors of the empirical covariance matrix.",
            "I'm assuming mean zero, so I'm not centric.",
            "And then the other matrices which have arisen which which.",
            "In mathematics and in general before that, with out of out of this, when you think about having a graph where you have K of XI extreme weight on the edge, the so called Laplacian, which is which has an interpretation.",
            "I guess as a Laplacian which is you take you take DN which is a diagonal matrix whose entries of the sums of the kxi, XJ and subtract off this cayenne and.",
            "Again, that turns out to be an interesting object to or.",
            "You can then do the so-called normalized Laplacian where you basically divide effectively by the end.",
            "That and the another version of the normalized Laplacian, which has almost, I mean, has the same eigenvalues as the normalized Laplacian and whose eigenfunctions eigenvectors can be related to it in a simple way.",
            "An interesting point is that of course KN is symmetric and positive definite if K is.",
            "But Ellen.",
            "Actually, symmetric positive definite as long as the function itself is just non negative.",
            "Which is a special which is so it's.",
            "OK, So what does one do with these things?"
        ],
        [
            "So the eigen structures of KNLNLN&L until to been used for lot purposes that we need for manifold estimation.",
            "They've been used with graph spectral clustering, so basically trying to divide graphs into components.",
            "They also need for clustering, but in in actually the by an analysis which I really like a lot by Nadler and others.",
            "And there have been a lot of recent contributions, some of which I only became aware of quite recently, not the one looks for Belkin Bousquet paper, which is to appear.",
            "There is a general overview of kernel applications, which is just appearing in the annals.",
            "And.",
            "There are a bunch of other papers which I'll refer to as I go along."
        ],
        [
            "As I mentioned earlier, observation is that I minus LNL until they have the same eigenvalues.",
            "And that the right eigenvectors of L until to is DN to minus 1/2 times the eigenvector of L. NLN is symmetric, so similar flip it for this.",
            "For the left eigenvectors."
        ],
        [
            "OK, now some theory and and some heuristics for cayanan Ln.",
            "And I'm really going to focus primarily on the Laplacian.",
            "Gauci Colonel Anne.",
            "I normalize it in the way we usually normalize it.",
            "I'm forgetting about the 2\u03c0 to the but the over 2, but you just normalize by 1 / W to the D. And then you normalize the kernel.",
            "The empirical matrix by 1 / N. So then you can think of the.",
            "Empirical matrix the KN hat as I call it.",
            "Now you can think of it as a map from L2 of PN to L2 of PN or PN is empirical, which is a fancy way of saying that it's of course a map from RN to RN and it's given by this.",
            "But it's useful to think about it in terms as.",
            "In terms of how to appear to LPN and the function, so the function is the usual way.",
            "The kernel, what you take a function F and you simply apply.",
            "You use the kernel as a kernel of the integral operator.",
            "And what you get is 1 / N summation.",
            "Michaels wanted to end case of Omega X -- X IFXI.",
            "And now here's the sort of observation which I found.",
            "It narrowed also made, but somehow I haven't seen it.",
            "Is that in fact.",
            "If you apply this the function one.",
            "What you just get is the kernel density estimate.",
            "And so, and of course, that the the the.",
            "Integral operator corresponds to a matrix operation.",
            "The Laplacian, the empirical vertical applausi and I focus on the one the non symmetric one is that you simply divide by this P hat some Omega X.",
            "And therefore one gets mapped into one.",
            "Simply divide by by, by the thing, so that's what the normalized Laplacian goes."
        ],
        [
            "OK, so now how is this implemented for clustering?",
            "Well, you compute the K eigenvectors OK of the N eigenvectors of your matrix.",
            "And then you represent.",
            "The I TH observation by the I throw of that matrix.",
            "Right, so it's so you've got a K vector.",
            "And the motivation is that again, because of the fact that in the reproducing kernel Hilbert space, which I haven't dealt on enough, But basically the inner product is by simply evaluating the inner product, gives you the evaluation map.",
            "If you think it through, in fact that if you think of XI's being K of XI dot and you have an eigenvector which is in this reproducing kernel Hilbert space, then the inner product and the reason kernel Hilbert space is indeed.",
            "Yeah, it's actually exactly what you this K throw is exactly what you want.",
            "And then, having gotten those things, you use K means clustering.",
            "You now have a low dimensional representation.",
            "Of your observations and use K means clustering.",
            "And of course what you really would like is to have the ISET equal to 0 for S not equal to T. That is, you would really like to have things separate out, not simply orthogonality by cancellation.",
            "Orthogonality by having disjoint supports.",
            "Right, that's that's that's that's that's that's what we like.",
            "Now, why is this making these you know?",
            "Why does it have any relation to clustering as we?"
        ],
        [
            "Think about it.",
            "Well, first of all, let me go through the theory and heuristics for the fixed Omega case, which I actually think doesn't.",
            "Isn't nearly as.",
            "Transparent as the case when in fact, as we usually do, we let.",
            "W Omega 10 to 0.",
            "Which is the density?",
            "So so if you now look at the population case, that's clear that all that you switch PN into P. The operator is a Hilbert Schmidt operator, the It's self adjoint positive definite.",
            "It has a point spectrum.",
            "With associated eigenfunctions, EIJ dot.",
            "So that's.",
            "OK, and that was.",
            "Oops."
        ],
        [
            "So coaches Kingina showed.",
            "That if you look at the empirical distribution of the eigenvalues.",
            "And you look at the of the population.",
            "The empirical of the matrix, basically.",
            "Normalizing in the way I indicated it, and if you now look at the.",
            "A distribution which assigns mass one to each of the population eigenvalues.",
            "Right of the operator, there's an infinite number of distinct eigenvalues.",
            "Then indeed, the set of.",
            "The measure if you want which you assign to the first K eigenvalues, converges to the measure to the population measure on the 1st K eigenvalues and the.",
            "You have convergence in what my think of the mallows to distance."
        ],
        [
            "And So what happens to Elen?",
            "Well?",
            "Same thing I mean so so.",
            "Well sorry, piece of Omega now tends to the smoothed P. Right smooth by the Colonel P. Elsa Bowman has the same structures as K Super Omega that is.",
            "It's an operator of the same type.",
            "You have the relation between the spectrum of the.",
            "Laplacian.",
            "The two normalized laplacian's.",
            "And again here 2 questions in a show that the spectrum converges to the spec.",
            "However, that's still as far as I'm concerned.",
            "That tells you that you know your method of clustering is consistent.",
            "As N goes to Infinity but doesn't tell you why it's a, it's it's it's, it's it's a good method of clustering or why it makes sense."
        ],
        [
            "You also OK. Eigenvector Convergence is a little bit trickier.",
            "You don't actually have eigenvector convergence.",
            "What you have is and I will return to this bit later.",
            "What you have is you take an eigenfunction from your.",
            "From the limiting operator, you then evaluated the data points.",
            "And that corresponds to the eigenvector.",
            "Corresponding to the JFE largest eigenvalue in the empirical situation.",
            "But it's interesting you don't start.",
            "With the eigenvector in some sense in the empirical case, you start with the eigenvector in the.",
            "Eigenfunction population case and.",
            "Indicate why that's some interest, by the way, how much longer do I have?",
            "20 minutes.",
            "OK."
        ],
        [
            "Right now there's a cluster.",
            "There's a controversy.",
            "I still haven't told you so.",
            "In this recent paper.",
            "They the phone looks Berg.",
            "Belkin and Busquet argue that the normalized Eigen Eigen Eigen normalized policies are better than the Unnormalized Laplacian.",
            "They don't consider the kernel at all.",
            "In this case, you don't explain why clustering occurs, but there's a rather nice.",
            "Discussion in a tutorial in the looks book tutorial.",
            "And then there's a discussion which I like a lot in the paper of Nadler Lafon Koifman Caboki dies.",
            "Were they basically show that?",
            "For the normalized Laplacian.",
            "You think about clustering.",
            "You define a Markov chain essentially.",
            "On your points.",
            "And basically the the normalized Laplacian is giving a Markov kernel.",
            "And then you can relate the eigenvalues.",
            "To sort of spending a cluster is corresponds to to a set where you spend a lot of time.",
            "Compared to the amount of time it takes you for the change to mix.",
            "But he finally does what I am going to spend the rest of the time on, which he considers.",
            "The.",
            "Bandwidth tending to 0.",
            "And then shows that the Markov chain can be related to diffusion.",
            "And then begins to make an argument, which I find much more explicit about why.",
            "Clustering is where clearly these things would be valuable for cluster."
        ],
        [
            "So.",
            "R. I.",
            "Want to be immediately start to look at and as I said, all this is sort of either trivial or or or certainly very loose, but.",
            "Um?",
            "Ask yourself what happens to the population operator.",
            "As you let Omega tend to 0.",
            "Do you like the bandwidth tenses here?",
            "And then what you can see is that the population operator tends to diagonal operator.",
            "It simply multiplies F of X by P of X.",
            "So.",
            "Basically what you can say is that.",
            "The population operator, at least for the kernel, is encoding for the density.",
            "And we of course know that in some naive sense at least.",
            "Statisticians typically, at least I find that it's since clustering is an ill defined.",
            "Activity in any case.",
            "One way of thinking about it is if you're looking for the higher the regions of high.",
            "With high modes.",
            "And what you know bumps is what you're looking for.",
            "So you're looking for bumps in P. So."
        ],
        [
            "From that point of view.",
            "Then I will argue that it's that really.",
            "The eigenvectors or the eigen functions of this limited operator do have something to do with.",
            "With clustering, but unfortunately in a way which is.",
            "Which is not necessarily so, so so nice.",
            "So let me quickly review some of you may not know this.",
            "I'll do it very quickly and it probably goes so fast that only those of you who know it will will realize.",
            "But there's a theory.",
            "That which goes back to Hilbert on these operators.",
            "The diagonal operators are are self adjoint and and.",
            "Positive definite.",
            "The so the.",
            "Scale the spectrum is, as usual defined as a set of all lambdas which don't where you don't have a bounded inverse, 40 minus Lambda I.",
            "The spectrum is compact.",
            "And now here's the analogue of the spectral decomposition.",
            "In general.",
            "The trouble is that in the case of.",
            "The kind of operators will be thinking about which are which.",
            "We have had up to now these these operators given number of distinct eigenvalues, each of which carries finite number of eigenvectors.",
            "Then it's easy to see sort of what the limiting processes, but in general you don't get that.",
            "And in fact in this case the spectrum can be continuous.",
            "And Moreover.",
            "The eigen.",
            "Vectors can converge, can give you.",
            "You know whole infinite dimensional spaces.",
            "And you'll see that exactly what happens here.",
            "Actually.",
            "So so so.",
            "What did the right generalization turns out to be that you think of the eigenvectors corresponding to an eigenvalue as space?",
            "They define linear space and you think of the projection operator corresponding linear space.",
            "The linear space the projection operator in one in correspondence, and now you have a projection valued measure.",
            "So the projection valued measure at the empty set is zero at the full space, it's the identity.",
            "The projection value measurement intersection is the product of the projections.",
            "And you have the Sigma additively pop property.",
            "So if you have sets whose intersection is empty, then ET of the sum, the projection valued measure assigned to the sum is.",
            "This is the sum of the projection bed, and then you have this marvelous if the sets are mutually disjoint.",
            "And then you have this marvelous representation, which which is that you can basically represent your operator.",
            "As an integral.",
            "Over the spectrum.",
            "Of Lambda with respect to this projection valued measure.",
            "If you think about it, that's exactly what you have.",
            "For example, in the case of a Hilbert Schmidt operator, right?",
            "Because you.",
            "You can check it there quite easily.",
            "OK."
        ],
        [
            "Now.",
            "The question is, what is the spectrum like for these things?",
            "Well, turns out that the spectrum for this limiting operator of the kernel is really easy.",
            "Because.",
            "The operator takes F into GF.",
            "So the eigenvalue we know what the eigenvalues of a diagonal operator are there simply the values on the diagonal.",
            "So in this case, the spectrum is simply the range.",
            "Of G, so in our case G is P. So the spectrum is simply.",
            "The the range of the density.",
            "Moreover in this case you also figure out what the.",
            "Projection.",
            "Corresponding to a fixed interval is a LVL.",
            "And what is it?",
            "It simply takes.",
            "You have a LBL, right?",
            "That's the image of something.",
            "Under the density right 'cause it's 'cause the spectrum is the range of the density.",
            "Now you go back.",
            "I give you an interval within the spectrum I go back.",
            "I have a set.",
            "And.",
            "The projection simply takes any function F. And kills it outside of that set.",
            "It just makes it 0 outside of that.",
            "Moreover you have you have, you know, sort of convergence theorems at all.",
            "If you have TNF converging to T of F for every fixed F, then you have the same thing for the projection valued measures, at least for at points were the projection assigned to the single from zero."
        ],
        [
            "Um?",
            "Well this is OK, so now.",
            "It's you can.",
            "It's pretty easy to show.",
            "Now let's think of the situation that we're usually in in statistics, where we let we have N observations were still think about the the population case to begin with.",
            "But we can think of the kernel but evaluated bandwidth Omega N. Will make it intense to 0.",
            "In that case you actually have convergence to this population diagonal operator.",
            "At each stage you have eigenvalues are distinct, but in the limit what you get is the spectrum, which is the whole range of the thing.",
            "And as I said, what you have is you converge to two.",
            "The function killed outside the inverse image of CD.",
            "So."
        ],
        [
            "OK Oh well.",
            "Alright, so I should have had I'll come to some pictures but just continuing in the eye since I wanted to write down something of a theorem.",
            "Although it's a completely trivial theorem.",
            "At this first thing isn't necessary.",
            "By the way, this permute X 1X N that has nothing to do with it.",
            "But if you look at the empirical situation.",
            "And you assume that PF&PF prime PF, as in L1PF prime is an L2PF are bounded.",
            "The kernel is well behaved as usual.",
            "The bandwidth tends to 0.",
            "N times, the bandwidth tends to Infinity.",
            "Then you have the right behavior."
        ],
        [
            "4.",
            "The empirical case also, right?",
            "So the data the data case?",
            "Again, you have convergence by the L2L1 continuity theorems and Taylor expansion.",
            "You have convergence to this operator, which is takes a function and multiplies it by the density."
        ],
        [
            "So what does that mean?",
            "So let's now let's look at a specific case.",
            "So.",
            "You suppose you have just two, so now this is the ideal clustering situation like we have just two distinct blocks.",
            "And the density is constant P1 and P2 in both cases.",
            "Well, in the limit, what you're going to get is basically.",
            "There's going to be eigenvectors which essentially correspond to P1.",
            "And eigenvectors correspond to P2.",
            "And if in fact you ordered and this is important, if you entered the data not in the random order that they were observed, but entered them in order of magnitude.",
            "Then what you will indeed have is that the well I think it's the other way around, but Oh no.",
            "Sorry, that's right.",
            "P2 is the larger one, right?",
            "So that correspond to the larger set.",
            "The larger eigenvalue.",
            "This is only two eigenvalues.",
            "Here P1 and P2, but unfortunately eigenfunction structure is very complicated.",
            "But basically what you will have then is, as you might expect, that the eigenvectors will have.",
            "First block of coordinates be.",
            "Non zero and then zero and the second the eigenvectors corresponding P2 will have the second set of coordinates be non zero in the first set of coordinates be 0.",
            "Well, this is nice in this particular case."
        ],
        [
            "But in general you start to get into trouble because now if you notice right, there are other values.",
            "If you look at the mode of the second one.",
            "Right there, values in the first region.",
            "Which correspond which, which have which have the same value.",
            "And therefore you are going to have eigenvectors.",
            "Remember, you're going to have eigenvector going to mix up of eigenvectors, so to speak.",
            "If you try to look for the eigenvalue which corresponds to the smaller bump, so to speak.",
            "You will also find among it.",
            "Eigenvectors correspond to the middle region.",
            "Of the larger boat.",
            "Um?"
        ],
        [
            "But I claim that, at least if you look at the kernel and it's not clear that that's what you should be doing.",
            "But the kernel matrix.",
            "But this analysis gives reasonable results for the 1 dimensional case.",
            "You first of all, feeding the data in order.",
            "You look at the First Capital in eigenvalue vectors of KN.",
            "Capital N being.",
            "Even reasonably large.",
            "You threshold the coordinates of the eigenvectors, so you get yourself rid of.",
            "Of things which are small but not exactly 0, and then you select K eigenvectors which have constant sign.",
            "Notice you want functions which really map.",
            "You don't want things that that wiggle right?",
            "You really it would be nice to have.",
            "Flat functions.",
            "And therefore you looking at constant sign there sparse right there that support should be should be concentrated.",
            "And which have connected support.",
            "Better yet, I think actually I haven't explored this.",
            "You find you've picked the first N eigenvectors, and you find K elements of the linear span, which somehow are maximally satisfy AB&C.",
            "And of course, one has to think of what do you mean by that, right?",
            "Sparse?",
            "We can think of the Glass Sioux constant sign we understand connected support.",
            "Somehow you have to have some assumption of smoothness effectively in your in your in your in your in your eigenfunction.",
            "Do you want to pick that?"
        ],
        [
            "So here's a clustering example.",
            "In one dimension is really easy.",
            "There's only one hump.",
            "OK. And you feed it in.",
            "There's the histogram.",
            "A 400 observations.",
            "You look at the first eigenvector and it's really great.",
            "Just absolutely Maps out for you.",
            "The Gaussian density.",
            "The second picture shows you what happens if you simply feed in the data randomly.",
            "So when you cluster, that doesn't matter.",
            "But if you actually want to get shapes out of the eigenvectors, it does."
        ],
        [
            "Alright, so here's an example where you have a mixture.",
            ".5 normal 01.5 normal 21 so they overlap somewhat and .5 normal zero 1 + .5 normal 2.25 so it's quite narrow.",
            "And again, as you can see, you feed these things in an order and the eigenvectors are carrying information.",
            "Right one is hanging around zero, the other one too.",
            "But and again, looking for eigenvectors which have constant sign and."
        ],
        [
            "If you look at the clustering just quickly, it's this is a situation where you in fact are doing clustering quite well.",
            "It's not a difficult situation, but you'll see there.",
            "the Blues are the the.",
            "Um?",
            "Correctly classified as normal 01, the Reds are classified as normal, digital .25 and yellows and greens are misclassified, and as you can see, they're not.",
            "It's pretty easy.",
            "Um?",
            "Now this would get the same picture if you had the noisy eigenvectors or the non noisy eigenvectors would still get the same clustering story.",
            "Um?"
        ],
        [
            "OK, now.",
            "The next story, which actually should have.",
            "I'll discuss the theory of this a little bit later.",
            "I'm afraid these slides came to me.",
            "Whoops, I've got 5 to 3 minutes, right?",
            "5 minutes OK.",
            "These slides came to me.",
            "Morning, Ann and.",
            "Unfortunately, there's some some noise in the communications, but here you see the normalized Laplacian eigenvectors.",
            "You do the same thing.",
            "You feed the things in order.",
            "And you instead of looking at the eigenvectors of the.",
            "Colonel, you look at the eigenvectors of Laplacian and you again get structure.",
            "But it's not.",
            "I think it's not really quite as clear in this particular case."
        ],
        [
            "However, as far as the clustering goes, you basically do just as well."
        ],
        [
            "Now in more than one dimension.",
            "As was pointed out by the real difficulty is that you don't know which eigenvectors.",
            "To look at.",
            "Essential.",
            "For kernel matrices, because we don't have necessarily an order notion of order.",
            "And so you have, you know that you're going to look at the things corresponding to the largest eigenvalue.",
            "But some of those are going to come from from points, which may be quite far away.",
            "And there is, yeah, I think it's a problem that one can deal with, but it's someone has to think about and so KN and the unnormalized Laplacian which was shown actually just just in this recent paper by looks for both ski and Bell, can have the same problem.",
            "The normalized laplacian.",
            "However, as was pointed out."
        ],
        [
            "Bye bye.",
            "Boaz nodler is nicer.",
            "Here the instead of it does converge with diagonal operator, but the diagonal operators, the identity.",
            "And because of diagonal operates the identity, you can look at the difference between it.",
            "And divide by Omega and let Omega tend to 0.",
            "And then what you get is in the limit you get a differential operator.",
            "Yes, there it is.",
            "It's epicenter, the Laplacian of F minus some constant times the inner product of the gradient of the logarithm of the density.",
            "And the gradient of the function.",
            "And this again, you say, well, it's still encoding for the density, but it's encoding for the derivative of the density.",
            "Right, so that's what we're sort of looking for.",
            "You're looking for zeros of the density when you're looking for when you're looking for modes.",
            "And so, in fact, this operator with suitable boundary conditions, does have distinct eigenvalues.",
            "And it's in situations which which you know never talked about.",
            "You actually have very quick drop off.",
            "If you actually did, the eigenvectors do sort of paved the way eigenfunctions do sort of behavior where you'd like them too.",
            "But the eigenvectors eigenvectors are in general uninterpretable unless the data are structured, and that's again not so pleasant.",
            "And I really."
        ],
        [
            "OK, so where is?",
            "OK, that's so, so that's.",
            "Um?",
            "Here are some clustering examples.",
            "So this is kernel clustering.",
            "Where you I cheated?",
            "And the data will fit in in order.",
            "In other words, you fit in sample 1 first and sample 2, then sample 3.",
            "And now if you look at the.",
            "This is actually the clustering that you get, but in fact not only this clustering workout, it's clear which which which eigenvectors you should use, and you.",
            "Oh sorry, no no.",
            "I'm getting I'm getting.",
            "I'm getting myself confused here.",
            "No no.",
            "What happens here is that you there's a rule which was proposed by Belkin Sheet.",
            "Yes, Belkin Xi'an bin you where you basically look at eigenvectors which have constant support.",
            "Just use those.",
            "Which has constant sign.",
            "And unfortunately, then, in this particular example, for example, you start to pick up.",
            "Five eigenvectors of matter, but in fact only three.",
            "I'm I'm."
        ],
        [
            "I'm about to finish.",
            "OK well, this is Laplacian clustering and you get.",
            "The same here you get.",
            "So here you get social class.",
            "In clustering is actually giving you the top three eigenvectors actually OK, so passing clustering."
        ],
        [
            "However, the.",
            "Eigenvectors."
        ],
        [
            "Don't actually look very natural, and that's I think consistent with some results of of your Goldbergs and Yankees.",
            "Um?",
            "OK, so so so whatever you try to do is sort of introduce you very, very quickly.",
            "To this area, I think it's an important way of dimension reduction, but there are a lot of things which have to be.",
            "Which remains to be better understood about it.",
            "And.",
            "That's one question.",
            "Oh alright, can I go on for five since we have a little bit of time at the end, can I go on for for five more minutes?",
            "OK, what?",
            "That's the point.",
            "Alright, OK no no.",
            "I I there's one other thing I want to point out OK here so so these are just questions there.",
            "There's theoretical questions about showing that the sample theory converted to limit is an tends to Infinity and W 10 zero error rate results issues.",
            "OK the real big kit questions that are way of rescuing.",
            "I think the kernel matrix for D bigger than one.",
            "But that is the question, what happens when D tends to Infinity?",
            "Right, that's the large P. Large in situation, now there's an."
        ],
        [
            "Teresting result also very recently nerdy.",
            "Now, Carly.",
            "Which seems to say that all of this effort is worthless.",
            "Because Nordin, who carries result, is that if you have a distribution, for example, like the Gaussian, the spherical Gaussian, or something like that, you know you can imagine the covariance matrix isn't the identity.",
            "Then asymptotically, the spectrum.",
            "This is for fixed Omega now.",
            "Is the same as the spectrum of the ordinary covariance matrix, and we know the ordinary covariance matrix is very bad behave.",
            "For large people you know the eigenvectors don't mean anything.",
            "And.",
            "The Laplacian behaves in exactly the same way.",
            "This no no gain in that.",
            "So here is.",
            "This is these benefits up seem to have some applicability, and yet this seems to suggest that in the in the situations we want wants to use them.",
            "For large dimension, this problem as well."
        ],
        [
            "OK, so now I have a.",
            "Different view.",
            "So my view is that somehow one maybe useful way of thinking of high dimensional space data.",
            "Is that what you have is a mixture of low dimensional things, possibly with some noise, but hopefully the noise is relatively symmetric.",
            "And let's say where the total dimension of the stuff is bounded.",
            "But the the ambient dimension is arbitrary."
        ],
        [
            "So here are some things to support this view.",
            "OK, this I'll really flash by is simply look at."
        ],
        [
            "Zip code, digits and you look patches.",
            "You take patches on the images and if you look at the distribution of.",
            "There's a notion, local density, local dimension, that that can be defined.",
            "The local dimension of these things, which are, you know, the patches are 16 by 16.",
            "Sorry, the pattern 4 by 4 so 16 dimensions.",
            "As you can see the local dimension ranges from zero to basically 5.",
            "So somehow there is some evidence that things are really being composed out of out of out of low dimensional structures.",
            "And."
        ],
        [
            "OK, so now here's a real question.",
            "If this view holds, do the eigenvectors of the normalized Laplacian provide?",
            "As Omega intends to zero successful clustering in Peter mental space.",
            "Can this be established asymptotically in a reasonable way?",
            "Can can be adapted to give similar results, which I think it can.",
            "And and more so.",
            "Three and four months, same As for D = 1, can pre clustering in some way, which is equivalent of ordering the data yield low dimensional representations for high dimensional data.",
            "And that's it.",
            "So 5 minutes.",
            "Peter example, you soon and I don't know much about it, but I don't follow people claiming that these groups really quite well.",
            "Yes.",
            "Classy and the normalized reply.",
            "Well, there's been a mixture of claims or people have gotten good results who claim good results for kernel clustering.",
            "But on the whole, I think the empirical as well as the theoretical evidence seems to be that the normalized Laplacian clustering is giving reasonable results, yes?",
            "What you're doing is kernel is putting a matrix from out of state.",
            "Yes, yes.",
            "Justina well, I mean you can ask, I mean.",
            "There's no no no, but notice I actually hear this is this?",
            "I've sort of.",
            "Generally people think of this for fixed bandwidth, right?",
            "But somehow you gain something from fixed bandwidth I. I'm not so convinced that Ann and the point is that when you let the bandwidth get small, you really are encoding for all the information.",
            "Problem occurs.",
            "Indeed, there is a problem.",
            "Curse of dimensionality in the case like the one that the Nordine considered if the dimensions high right if the distribution fills all space, then the cursor dimensionality applies.",
            "However, if indeed things live in a much lower dimension.",
            "When you think they do.",
            "In fact.",
            "The dimension that applies is the intrinsic dimension rather than the ambient dimension, so so so in some sense I think the cursor do internally, maybe someone illusory.",
            "However you still end up with things which apparently you have a density of 100 coordinates or something like that, right?",
            "But what this?",
            "I think what I find most interesting about this is not so much the clustering as big, but the dimension reduction aspect.",
            "I'm sorry.",
            "Versions of the operator, or else in the end to Elwyn and goes funky and WN goes to zero.",
            "I think there is a paper of what's there.",
            "Hi, I'm sorry.",
            "I think there is a paper which deals with the operator convergence.",
            "Of eliminating the M2L's in Darwin Angus.",
            "When thinking I see it, that's that's a paper.",
            "I don't know why, but I think it's a pointwise convergence, so he doesn't like versions of eigenvalues.",
            "And now you vectors.",
            "But so I think it's still open, but at least David yeah.",
            "The above situations too.",
            "OK, I'd be very interested in that reference.",
            "As I said, I'm sort of learning about this area.",
            "There is an issue actually which is not trivial with that be 'cause if you look you do the usual calculation that you do with density estimation, right?",
            "Well, you have to expand the thing.",
            "To a derivative you have to hope that there's a derivative there.",
            "But if there's a derivative there there somehow living in Sobolev space, but you can't hope to prove convergence, strong convergence, it's overlap space.",
            "But what you can prove, I think, is.",
            "Convergence in L2, which is actually probably enough, but I don't know.",
            "I'm just talking wild, but I think it is possible to get results which show that the eigen structure.",
            "Convergence.",
            "Maybe?",
            "What you say is so concerning the high dimensional data, yes.",
            "Dimensionality despond latex and you can apply it to your vendors filled with you in every small neighborhood death run for it.",
            "Well, that's that's exactly that, yeah.",
            "Instead of the national adoption so you can take the log in national space leaking, but like another way of looking at this and you're gonna say where fly out of values that work with children.",
            "Abundance is not the number.",
            "Are you better this?",
            "It could be better.",
            "Patient could be measured.",
            "Try to extend all the story.",
            "So I mean you can see instead of the mixed reduction you you try to adjust it you know or have ended.",
            "Well, I think I think I think it's correct.",
            "In fact there is some empirical work as I recall having in machine learning is the literature again, but it pays off to localize your current, your bandwidth.",
            "However, coming back to your first question about the emptiness of space.",
            "At some level, I think if the space is really empty, and if it's really like the.",
            "The dimension is really large.",
            "Somehow I interpret stones least favorable results and so on.",
            "That's telling you that you're dead, you just can't do anything, because in principle.",
            "So why can we do things?",
            "Cause if in fact things do live on manifolds and which are smooth.",
            "And and and, and let's say you have symmetric error.",
            "That's something that in Ording seems to be able to argue that in fact you can keep.",
            "Symmetric errors of fixed bandwidth.",
            "Sorry no, the bandwidth in anyone dimension goes to 0, but the whole thing is the L2 norm.",
            "This is bounded.",
            "Then you can.",
            "You can argue that in fact the difficulty of the problem is governed not, although you think you operating in 1000 dimensions actually operating in one dimension.",
            "If things are smooth enough.",
            "Can you somehow for strength from?",
            "Take a massive literature income, less information on selecting bandwidth littleberry correctly.",
            "I, I think that I think you I think you can.",
            "The question is how you do it and.",
            "One of the things that I mean we showed we have a little paper bouillion I've little paper we show that.",
            "For.",
            "For regression, right?",
            "If your regression and this and the predictors are actually concentrated on a low dimensional smooth space.",
            "Then if you do actually local regression, you're OK, However.",
            "You cannot pick your bandwidth by any of the usual prescriptions which have to do with the.",
            "You know, which basically tells you that the bandwidth behaves like H to the power D. With these the ambient dimension, you have to pick it either by some cross validation sort of thing or by using the kind of thing that it is possible to construct.",
            "Estimates of local dimension.",
            "And then then again.",
            "But the literature may be relevant.",
            "Xbox."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pleasure to be here and thanks thanks to the organizers.",
                    "label": 0
                },
                {
                    "sent": "So I actually hesitated, but to speak about and and I actually being foolhardy.",
                    "label": 0
                },
                {
                    "sent": "Because I'm speaking about something that I only really got involved in about.",
                    "label": 0
                },
                {
                    "sent": "Three or four weeks ago and and learn stuff you know the week before I came here and there are really hard results, but I think it's interesting and I think I'd like to expose people to it.",
                    "label": 0
                },
                {
                    "sent": "So, so the story is kernel representations of kernel estimation, so.",
                    "label": 1
                },
                {
                    "sent": "Let me.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the outline.",
                    "label": 0
                },
                {
                    "sent": "First of all, introduce you to what kernel representations in Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Kernel matrices are.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about application statistics, which have mainly appeared in the machine learning literature in fact.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk about the uses of kernels for clustering.",
                    "label": 0
                },
                {
                    "sent": "Some asymptotic switch was done by coaching schemes.",
                    "label": 0
                },
                {
                    "sent": "Unnayan, Belkin and Yogi and.",
                    "label": 0
                },
                {
                    "sent": "More recent papers.",
                    "label": 0
                },
                {
                    "sent": "Owen Adler at all, which I'll focus on.",
                    "label": 0
                },
                {
                    "sent": "And then I'll make the connection to density estimation, which which which somehow.",
                    "label": 0
                },
                {
                    "sent": "I think maybe people know, but I at least always thought the two are quite different.",
                    "label": 0
                },
                {
                    "sent": "You know, these kernel representations and kernel density estimation, but in fact they're not so different.",
                    "label": 1
                },
                {
                    "sent": "They're quite closely related.",
                    "label": 0
                },
                {
                    "sent": "And so heuristics for scale going to zero heuristics for scaled down to zero with laplacian's connections with large DD being dimension and some future in some future directions.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with kernel representations.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you take.",
                    "label": 0
                },
                {
                    "sent": "Your data, which you always assume is in high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And let's for the time being, suppose that we have a density with respect to big measure in this high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And you have a kernel which is functional Ardito Ardito R symmetric and positive definite.",
                    "label": 1
                },
                {
                    "sent": "But the kernel is positive definite.",
                    "label": 0
                },
                {
                    "sent": "And then the idea is that you represent you basically in some way.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "You're complicating your life because instead of staying in D dimensional space, you now go into infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And you make your data point correspond to the function K of X dot.",
                    "label": 0
                },
                {
                    "sent": "And the prototypical K that we're all used to is K of XY equals X transpose Y, but in fact the K that's used in general is the Gaussian curve.",
                    "label": 0
                },
                {
                    "sent": "And then having done this representation.",
                    "label": 0
                },
                {
                    "sent": "So what do you what do you do then well?",
                    "label": 0
                },
                {
                    "sent": "There are various analogues to the usual.",
                    "label": 0
                },
                {
                    "sent": "Things that we try to do in multivariate analysis that you can now do with these objects as opposed to the original.",
                    "label": 0
                },
                {
                    "sent": "Data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the first application.",
                    "label": 0
                },
                {
                    "sent": "They really arose in estimation of functions using splines.",
                    "label": 1
                },
                {
                    "sent": "So when you have when you put Bayes priors on Sobolev spaces.",
                    "label": 0
                },
                {
                    "sent": "You end up being naturally.",
                    "label": 0
                },
                {
                    "sent": "Coming to.",
                    "label": 0
                },
                {
                    "sent": "Functions which are.",
                    "label": 0
                },
                {
                    "sent": "The linear span of such kernels.",
                    "label": 0
                },
                {
                    "sent": "Which can be closed in the inner product.",
                    "label": 0
                },
                {
                    "sent": "I mean I should have said this earlier.",
                    "label": 0
                },
                {
                    "sent": "The kernel can be thought of as an inner product, right?",
                    "label": 0
                },
                {
                    "sent": "It induces?",
                    "label": 0
                },
                {
                    "sent": "So you basically have a Hilbert space which you can get by looking at linear combinations of.",
                    "label": 0
                },
                {
                    "sent": "Kernels of kernel function, then closing it in that in that in that in that norm.",
                    "label": 0
                },
                {
                    "sent": "And and if you do that, for example, soboleff spaces are reproducing kernel Hilbert spaces.",
                    "label": 0
                },
                {
                    "sent": "And the tip the critical thing about these things is or one of the critical things is that they have the property that the evaluation functionals are continuous.",
                    "label": 0
                },
                {
                    "sent": "So if you take if you there functions there there, there, there, there there function spaces where I want to go send F into F of X.",
                    "label": 0
                },
                {
                    "sent": "That's a bounded linear function.",
                    "label": 0
                },
                {
                    "sent": "And the Hilbert space is, and therefore there is in fact.",
                    "label": 0
                },
                {
                    "sent": "A kernel is a function which represent of that, but it's not necessarily obvious defined.",
                    "label": 0
                },
                {
                    "sent": "For instance, even in the case of a subspace.",
                    "label": 0
                },
                {
                    "sent": "The other, I guess, the second time that so this is probably very chunky, but in fact I'm sure.",
                    "label": 0
                },
                {
                    "sent": "I saw it sort of was involved.",
                    "label": 0
                },
                {
                    "sent": "Nicks work which was based on much earlier work of of guess Bravoman and others were he showed that for support vector machines you could really if you wanted to sort of build up you had a given set of variables, but you then wanted to enrich your set of variables by looking at products and things like that that you could.",
                    "label": 0
                },
                {
                    "sent": "So suddenly making the set of variables much larger.",
                    "label": 0
                },
                {
                    "sent": "But you could actually carry out the the the progression of computationally very efficient way by essentially thinking of kernel representations.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to dwell on this because it's not, it's not.",
                    "label": 0
                },
                {
                    "sent": "That's not the aspect that I'm.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interested at the moment.",
                    "label": 0
                },
                {
                    "sent": "Then there's a couple back.",
                    "label": 0
                },
                {
                    "sent": "I mean, Mike Jordan, Anne Francis Bach, and.",
                    "label": 0
                },
                {
                    "sent": "And others.",
                    "label": 0
                },
                {
                    "sent": "Proposed things like kernel ICA.",
                    "label": 1
                },
                {
                    "sent": "So kernel, ICA independent component analysis is that you basically note that if you're.",
                    "label": 0
                },
                {
                    "sent": "A Hilbert space of functions is dense in L2, for example then.",
                    "label": 0
                },
                {
                    "sent": "Independence is characterized by simply requiring that the correlation of K of XX&K of YY is 0 for all X&Y.",
                    "label": 1
                },
                {
                    "sent": "Whereas to do it in general you have to take this.",
                    "label": 0
                },
                {
                    "sent": "You have to take the so called Rennie correlation at the soup.",
                    "label": 0
                },
                {
                    "sent": "Overall, F of X&G of Y, but this is enough and then you start to to operate on that and we start to look at empirically.",
                    "label": 0
                },
                {
                    "sent": "You then translate the translation of what that what that statement becomes, and then you can do analysis of, well, independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "We were trying to find representation of a multivariate vector as a matrix transform a vector independent, but not necessarily identically distributed random variables, but not necessarily Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But you can't do it for the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So one way of thinking about it is that you're sort of.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This enables you to take your data and code it in a nonlinear way.",
                    "label": 0
                },
                {
                    "sent": "So that things like correlation which you know for instance in the in the in the in the usual case, do not characterize independence.",
                    "label": 0
                },
                {
                    "sent": "When you code them in this way, now they do character accident benefits.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now here's the aspect that I'm most interested in and that has to do with clustering and spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "So you form this matrixcare of XI XJ.",
                    "label": 0
                },
                {
                    "sent": "Which is an N by N matrix.",
                    "label": 0
                },
                {
                    "sent": "But where if in fact you have the usual inner product.",
                    "label": 0
                },
                {
                    "sent": "The eigenvalues and eigenvectors of that matrix are the same as the eigenvalues and eigenvectors of the empirical covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming mean zero, so I'm not centric.",
                    "label": 0
                },
                {
                    "sent": "And then the other matrices which have arisen which which.",
                    "label": 0
                },
                {
                    "sent": "In mathematics and in general before that, with out of out of this, when you think about having a graph where you have K of XI extreme weight on the edge, the so called Laplacian, which is which has an interpretation.",
                    "label": 0
                },
                {
                    "sent": "I guess as a Laplacian which is you take you take DN which is a diagonal matrix whose entries of the sums of the kxi, XJ and subtract off this cayenne and.",
                    "label": 0
                },
                {
                    "sent": "Again, that turns out to be an interesting object to or.",
                    "label": 0
                },
                {
                    "sent": "You can then do the so-called normalized Laplacian where you basically divide effectively by the end.",
                    "label": 0
                },
                {
                    "sent": "That and the another version of the normalized Laplacian, which has almost, I mean, has the same eigenvalues as the normalized Laplacian and whose eigenfunctions eigenvectors can be related to it in a simple way.",
                    "label": 0
                },
                {
                    "sent": "An interesting point is that of course KN is symmetric and positive definite if K is.",
                    "label": 1
                },
                {
                    "sent": "But Ellen.",
                    "label": 0
                },
                {
                    "sent": "Actually, symmetric positive definite as long as the function itself is just non negative.",
                    "label": 0
                },
                {
                    "sent": "Which is a special which is so it's.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does one do with these things?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the eigen structures of KNLNLN&L until to been used for lot purposes that we need for manifold estimation.",
                    "label": 1
                },
                {
                    "sent": "They've been used with graph spectral clustering, so basically trying to divide graphs into components.",
                    "label": 1
                },
                {
                    "sent": "They also need for clustering, but in in actually the by an analysis which I really like a lot by Nadler and others.",
                    "label": 0
                },
                {
                    "sent": "And there have been a lot of recent contributions, some of which I only became aware of quite recently, not the one looks for Belkin Bousquet paper, which is to appear.",
                    "label": 0
                },
                {
                    "sent": "There is a general overview of kernel applications, which is just appearing in the annals.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There are a bunch of other papers which I'll refer to as I go along.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I mentioned earlier, observation is that I minus LNL until they have the same eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "And that the right eigenvectors of L until to is DN to minus 1/2 times the eigenvector of L. NLN is symmetric, so similar flip it for this.",
                    "label": 0
                },
                {
                    "sent": "For the left eigenvectors.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now some theory and and some heuristics for cayanan Ln.",
                    "label": 1
                },
                {
                    "sent": "And I'm really going to focus primarily on the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Gauci Colonel Anne.",
                    "label": 0
                },
                {
                    "sent": "I normalize it in the way we usually normalize it.",
                    "label": 0
                },
                {
                    "sent": "I'm forgetting about the 2\u03c0 to the but the over 2, but you just normalize by 1 / W to the D. And then you normalize the kernel.",
                    "label": 0
                },
                {
                    "sent": "The empirical matrix by 1 / N. So then you can think of the.",
                    "label": 0
                },
                {
                    "sent": "Empirical matrix the KN hat as I call it.",
                    "label": 0
                },
                {
                    "sent": "Now you can think of it as a map from L2 of PN to L2 of PN or PN is empirical, which is a fancy way of saying that it's of course a map from RN to RN and it's given by this.",
                    "label": 0
                },
                {
                    "sent": "But it's useful to think about it in terms as.",
                    "label": 0
                },
                {
                    "sent": "In terms of how to appear to LPN and the function, so the function is the usual way.",
                    "label": 0
                },
                {
                    "sent": "The kernel, what you take a function F and you simply apply.",
                    "label": 0
                },
                {
                    "sent": "You use the kernel as a kernel of the integral operator.",
                    "label": 1
                },
                {
                    "sent": "And what you get is 1 / N summation.",
                    "label": 0
                },
                {
                    "sent": "Michaels wanted to end case of Omega X -- X IFXI.",
                    "label": 0
                },
                {
                    "sent": "And now here's the sort of observation which I found.",
                    "label": 0
                },
                {
                    "sent": "It narrowed also made, but somehow I haven't seen it.",
                    "label": 0
                },
                {
                    "sent": "Is that in fact.",
                    "label": 0
                },
                {
                    "sent": "If you apply this the function one.",
                    "label": 0
                },
                {
                    "sent": "What you just get is the kernel density estimate.",
                    "label": 1
                },
                {
                    "sent": "And so, and of course, that the the the.",
                    "label": 0
                },
                {
                    "sent": "Integral operator corresponds to a matrix operation.",
                    "label": 0
                },
                {
                    "sent": "The Laplacian, the empirical vertical applausi and I focus on the one the non symmetric one is that you simply divide by this P hat some Omega X.",
                    "label": 0
                },
                {
                    "sent": "And therefore one gets mapped into one.",
                    "label": 0
                },
                {
                    "sent": "Simply divide by by, by the thing, so that's what the normalized Laplacian goes.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now how is this implemented for clustering?",
                    "label": 0
                },
                {
                    "sent": "Well, you compute the K eigenvectors OK of the N eigenvectors of your matrix.",
                    "label": 0
                },
                {
                    "sent": "And then you represent.",
                    "label": 0
                },
                {
                    "sent": "The I TH observation by the I throw of that matrix.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's so you've got a K vector.",
                    "label": 0
                },
                {
                    "sent": "And the motivation is that again, because of the fact that in the reproducing kernel Hilbert space, which I haven't dealt on enough, But basically the inner product is by simply evaluating the inner product, gives you the evaluation map.",
                    "label": 0
                },
                {
                    "sent": "If you think it through, in fact that if you think of XI's being K of XI dot and you have an eigenvector which is in this reproducing kernel Hilbert space, then the inner product and the reason kernel Hilbert space is indeed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's actually exactly what you this K throw is exactly what you want.",
                    "label": 0
                },
                {
                    "sent": "And then, having gotten those things, you use K means clustering.",
                    "label": 0
                },
                {
                    "sent": "You now have a low dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "Of your observations and use K means clustering.",
                    "label": 1
                },
                {
                    "sent": "And of course what you really would like is to have the ISET equal to 0 for S not equal to T. That is, you would really like to have things separate out, not simply orthogonality by cancellation.",
                    "label": 0
                },
                {
                    "sent": "Orthogonality by having disjoint supports.",
                    "label": 0
                },
                {
                    "sent": "Right, that's that's that's that's that's that's what we like.",
                    "label": 0
                },
                {
                    "sent": "Now, why is this making these you know?",
                    "label": 0
                },
                {
                    "sent": "Why does it have any relation to clustering as we?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think about it.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, let me go through the theory and heuristics for the fixed Omega case, which I actually think doesn't.",
                    "label": 0
                },
                {
                    "sent": "Isn't nearly as.",
                    "label": 0
                },
                {
                    "sent": "Transparent as the case when in fact, as we usually do, we let.",
                    "label": 0
                },
                {
                    "sent": "W Omega 10 to 0.",
                    "label": 0
                },
                {
                    "sent": "Which is the density?",
                    "label": 0
                },
                {
                    "sent": "So so if you now look at the population case, that's clear that all that you switch PN into P. The operator is a Hilbert Schmidt operator, the It's self adjoint positive definite.",
                    "label": 0
                },
                {
                    "sent": "It has a point spectrum.",
                    "label": 1
                },
                {
                    "sent": "With associated eigenfunctions, EIJ dot.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "OK, and that was.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So coaches Kingina showed.",
                    "label": 0
                },
                {
                    "sent": "That if you look at the empirical distribution of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And you look at the of the population.",
                    "label": 0
                },
                {
                    "sent": "The empirical of the matrix, basically.",
                    "label": 0
                },
                {
                    "sent": "Normalizing in the way I indicated it, and if you now look at the.",
                    "label": 0
                },
                {
                    "sent": "A distribution which assigns mass one to each of the population eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Right of the operator, there's an infinite number of distinct eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Then indeed, the set of.",
                    "label": 0
                },
                {
                    "sent": "The measure if you want which you assign to the first K eigenvalues, converges to the measure to the population measure on the 1st K eigenvalues and the.",
                    "label": 0
                },
                {
                    "sent": "You have convergence in what my think of the mallows to distance.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what happens to Elen?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "Same thing I mean so so.",
                    "label": 0
                },
                {
                    "sent": "Well sorry, piece of Omega now tends to the smoothed P. Right smooth by the Colonel P. Elsa Bowman has the same structures as K Super Omega that is.",
                    "label": 0
                },
                {
                    "sent": "It's an operator of the same type.",
                    "label": 0
                },
                {
                    "sent": "You have the relation between the spectrum of the.",
                    "label": 1
                },
                {
                    "sent": "Laplacian.",
                    "label": 0
                },
                {
                    "sent": "The two normalized laplacian's.",
                    "label": 0
                },
                {
                    "sent": "And again here 2 questions in a show that the spectrum converges to the spec.",
                    "label": 1
                },
                {
                    "sent": "However, that's still as far as I'm concerned.",
                    "label": 0
                },
                {
                    "sent": "That tells you that you know your method of clustering is consistent.",
                    "label": 0
                },
                {
                    "sent": "As N goes to Infinity but doesn't tell you why it's a, it's it's it's, it's it's a good method of clustering or why it makes sense.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You also OK. Eigenvector Convergence is a little bit trickier.",
                    "label": 0
                },
                {
                    "sent": "You don't actually have eigenvector convergence.",
                    "label": 0
                },
                {
                    "sent": "What you have is and I will return to this bit later.",
                    "label": 0
                },
                {
                    "sent": "What you have is you take an eigenfunction from your.",
                    "label": 0
                },
                {
                    "sent": "From the limiting operator, you then evaluated the data points.",
                    "label": 0
                },
                {
                    "sent": "And that corresponds to the eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to the JFE largest eigenvalue in the empirical situation.",
                    "label": 0
                },
                {
                    "sent": "But it's interesting you don't start.",
                    "label": 0
                },
                {
                    "sent": "With the eigenvector in some sense in the empirical case, you start with the eigenvector in the.",
                    "label": 0
                },
                {
                    "sent": "Eigenfunction population case and.",
                    "label": 0
                },
                {
                    "sent": "Indicate why that's some interest, by the way, how much longer do I have?",
                    "label": 0
                },
                {
                    "sent": "20 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right now there's a cluster.",
                    "label": 0
                },
                {
                    "sent": "There's a controversy.",
                    "label": 0
                },
                {
                    "sent": "I still haven't told you so.",
                    "label": 0
                },
                {
                    "sent": "In this recent paper.",
                    "label": 0
                },
                {
                    "sent": "They the phone looks Berg.",
                    "label": 0
                },
                {
                    "sent": "Belkin and Busquet argue that the normalized Eigen Eigen Eigen normalized policies are better than the Unnormalized Laplacian.",
                    "label": 0
                },
                {
                    "sent": "They don't consider the kernel at all.",
                    "label": 0
                },
                {
                    "sent": "In this case, you don't explain why clustering occurs, but there's a rather nice.",
                    "label": 1
                },
                {
                    "sent": "Discussion in a tutorial in the looks book tutorial.",
                    "label": 0
                },
                {
                    "sent": "And then there's a discussion which I like a lot in the paper of Nadler Lafon Koifman Caboki dies.",
                    "label": 0
                },
                {
                    "sent": "Were they basically show that?",
                    "label": 0
                },
                {
                    "sent": "For the normalized Laplacian.",
                    "label": 0
                },
                {
                    "sent": "You think about clustering.",
                    "label": 0
                },
                {
                    "sent": "You define a Markov chain essentially.",
                    "label": 0
                },
                {
                    "sent": "On your points.",
                    "label": 0
                },
                {
                    "sent": "And basically the the normalized Laplacian is giving a Markov kernel.",
                    "label": 0
                },
                {
                    "sent": "And then you can relate the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "To sort of spending a cluster is corresponds to to a set where you spend a lot of time.",
                    "label": 0
                },
                {
                    "sent": "Compared to the amount of time it takes you for the change to mix.",
                    "label": 0
                },
                {
                    "sent": "But he finally does what I am going to spend the rest of the time on, which he considers.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Bandwidth tending to 0.",
                    "label": 1
                },
                {
                    "sent": "And then shows that the Markov chain can be related to diffusion.",
                    "label": 0
                },
                {
                    "sent": "And then begins to make an argument, which I find much more explicit about why.",
                    "label": 0
                },
                {
                    "sent": "Clustering is where clearly these things would be valuable for cluster.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "R. I.",
                    "label": 0
                },
                {
                    "sent": "Want to be immediately start to look at and as I said, all this is sort of either trivial or or or certainly very loose, but.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Ask yourself what happens to the population operator.",
                    "label": 0
                },
                {
                    "sent": "As you let Omega tend to 0.",
                    "label": 0
                },
                {
                    "sent": "Do you like the bandwidth tenses here?",
                    "label": 0
                },
                {
                    "sent": "And then what you can see is that the population operator tends to diagonal operator.",
                    "label": 0
                },
                {
                    "sent": "It simply multiplies F of X by P of X.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically what you can say is that.",
                    "label": 0
                },
                {
                    "sent": "The population operator, at least for the kernel, is encoding for the density.",
                    "label": 0
                },
                {
                    "sent": "And we of course know that in some naive sense at least.",
                    "label": 0
                },
                {
                    "sent": "Statisticians typically, at least I find that it's since clustering is an ill defined.",
                    "label": 0
                },
                {
                    "sent": "Activity in any case.",
                    "label": 0
                },
                {
                    "sent": "One way of thinking about it is if you're looking for the higher the regions of high.",
                    "label": 0
                },
                {
                    "sent": "With high modes.",
                    "label": 0
                },
                {
                    "sent": "And what you know bumps is what you're looking for.",
                    "label": 0
                },
                {
                    "sent": "So you're looking for bumps in P. So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From that point of view.",
                    "label": 0
                },
                {
                    "sent": "Then I will argue that it's that really.",
                    "label": 0
                },
                {
                    "sent": "The eigenvectors or the eigen functions of this limited operator do have something to do with.",
                    "label": 0
                },
                {
                    "sent": "With clustering, but unfortunately in a way which is.",
                    "label": 0
                },
                {
                    "sent": "Which is not necessarily so, so so nice.",
                    "label": 0
                },
                {
                    "sent": "So let me quickly review some of you may not know this.",
                    "label": 0
                },
                {
                    "sent": "I'll do it very quickly and it probably goes so fast that only those of you who know it will will realize.",
                    "label": 0
                },
                {
                    "sent": "But there's a theory.",
                    "label": 0
                },
                {
                    "sent": "That which goes back to Hilbert on these operators.",
                    "label": 0
                },
                {
                    "sent": "The diagonal operators are are self adjoint and and.",
                    "label": 0
                },
                {
                    "sent": "Positive definite.",
                    "label": 0
                },
                {
                    "sent": "The so the.",
                    "label": 0
                },
                {
                    "sent": "Scale the spectrum is, as usual defined as a set of all lambdas which don't where you don't have a bounded inverse, 40 minus Lambda I.",
                    "label": 0
                },
                {
                    "sent": "The spectrum is compact.",
                    "label": 0
                },
                {
                    "sent": "And now here's the analogue of the spectral decomposition.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "The trouble is that in the case of.",
                    "label": 0
                },
                {
                    "sent": "The kind of operators will be thinking about which are which.",
                    "label": 0
                },
                {
                    "sent": "We have had up to now these these operators given number of distinct eigenvalues, each of which carries finite number of eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Then it's easy to see sort of what the limiting processes, but in general you don't get that.",
                    "label": 0
                },
                {
                    "sent": "And in fact in this case the spectrum can be continuous.",
                    "label": 0
                },
                {
                    "sent": "And Moreover.",
                    "label": 0
                },
                {
                    "sent": "The eigen.",
                    "label": 0
                },
                {
                    "sent": "Vectors can converge, can give you.",
                    "label": 0
                },
                {
                    "sent": "You know whole infinite dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "And you'll see that exactly what happens here.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "So so so.",
                    "label": 0
                },
                {
                    "sent": "What did the right generalization turns out to be that you think of the eigenvectors corresponding to an eigenvalue as space?",
                    "label": 0
                },
                {
                    "sent": "They define linear space and you think of the projection operator corresponding linear space.",
                    "label": 0
                },
                {
                    "sent": "The linear space the projection operator in one in correspondence, and now you have a projection valued measure.",
                    "label": 0
                },
                {
                    "sent": "So the projection valued measure at the empty set is zero at the full space, it's the identity.",
                    "label": 0
                },
                {
                    "sent": "The projection value measurement intersection is the product of the projections.",
                    "label": 0
                },
                {
                    "sent": "And you have the Sigma additively pop property.",
                    "label": 0
                },
                {
                    "sent": "So if you have sets whose intersection is empty, then ET of the sum, the projection valued measure assigned to the sum is.",
                    "label": 0
                },
                {
                    "sent": "This is the sum of the projection bed, and then you have this marvelous if the sets are mutually disjoint.",
                    "label": 0
                },
                {
                    "sent": "And then you have this marvelous representation, which which is that you can basically represent your operator.",
                    "label": 0
                },
                {
                    "sent": "As an integral.",
                    "label": 0
                },
                {
                    "sent": "Over the spectrum.",
                    "label": 0
                },
                {
                    "sent": "Of Lambda with respect to this projection valued measure.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, that's exactly what you have.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case of a Hilbert Schmidt operator, right?",
                    "label": 0
                },
                {
                    "sent": "Because you.",
                    "label": 0
                },
                {
                    "sent": "You can check it there quite easily.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The question is, what is the spectrum like for these things?",
                    "label": 0
                },
                {
                    "sent": "Well, turns out that the spectrum for this limiting operator of the kernel is really easy.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "The operator takes F into GF.",
                    "label": 0
                },
                {
                    "sent": "So the eigenvalue we know what the eigenvalues of a diagonal operator are there simply the values on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So in this case, the spectrum is simply the range.",
                    "label": 0
                },
                {
                    "sent": "Of G, so in our case G is P. So the spectrum is simply.",
                    "label": 1
                },
                {
                    "sent": "The the range of the density.",
                    "label": 1
                },
                {
                    "sent": "Moreover in this case you also figure out what the.",
                    "label": 0
                },
                {
                    "sent": "Projection.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to a fixed interval is a LVL.",
                    "label": 0
                },
                {
                    "sent": "And what is it?",
                    "label": 0
                },
                {
                    "sent": "It simply takes.",
                    "label": 0
                },
                {
                    "sent": "You have a LBL, right?",
                    "label": 0
                },
                {
                    "sent": "That's the image of something.",
                    "label": 0
                },
                {
                    "sent": "Under the density right 'cause it's 'cause the spectrum is the range of the density.",
                    "label": 0
                },
                {
                    "sent": "Now you go back.",
                    "label": 0
                },
                {
                    "sent": "I give you an interval within the spectrum I go back.",
                    "label": 0
                },
                {
                    "sent": "I have a set.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The projection simply takes any function F. And kills it outside of that set.",
                    "label": 0
                },
                {
                    "sent": "It just makes it 0 outside of that.",
                    "label": 0
                },
                {
                    "sent": "Moreover you have you have, you know, sort of convergence theorems at all.",
                    "label": 0
                },
                {
                    "sent": "If you have TNF converging to T of F for every fixed F, then you have the same thing for the projection valued measures, at least for at points were the projection assigned to the single from zero.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well this is OK, so now.",
                    "label": 0
                },
                {
                    "sent": "It's you can.",
                    "label": 0
                },
                {
                    "sent": "It's pretty easy to show.",
                    "label": 0
                },
                {
                    "sent": "Now let's think of the situation that we're usually in in statistics, where we let we have N observations were still think about the the population case to begin with.",
                    "label": 0
                },
                {
                    "sent": "But we can think of the kernel but evaluated bandwidth Omega N. Will make it intense to 0.",
                    "label": 0
                },
                {
                    "sent": "In that case you actually have convergence to this population diagonal operator.",
                    "label": 0
                },
                {
                    "sent": "At each stage you have eigenvalues are distinct, but in the limit what you get is the spectrum, which is the whole range of the thing.",
                    "label": 0
                },
                {
                    "sent": "And as I said, what you have is you converge to two.",
                    "label": 0
                },
                {
                    "sent": "The function killed outside the inverse image of CD.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK Oh well.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I should have had I'll come to some pictures but just continuing in the eye since I wanted to write down something of a theorem.",
                    "label": 0
                },
                {
                    "sent": "Although it's a completely trivial theorem.",
                    "label": 0
                },
                {
                    "sent": "At this first thing isn't necessary.",
                    "label": 0
                },
                {
                    "sent": "By the way, this permute X 1X N that has nothing to do with it.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the empirical situation.",
                    "label": 0
                },
                {
                    "sent": "And you assume that PF&PF prime PF, as in L1PF prime is an L2PF are bounded.",
                    "label": 0
                },
                {
                    "sent": "The kernel is well behaved as usual.",
                    "label": 0
                },
                {
                    "sent": "The bandwidth tends to 0.",
                    "label": 0
                },
                {
                    "sent": "N times, the bandwidth tends to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Then you have the right behavior.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "4.",
                    "label": 0
                },
                {
                    "sent": "The empirical case also, right?",
                    "label": 0
                },
                {
                    "sent": "So the data the data case?",
                    "label": 0
                },
                {
                    "sent": "Again, you have convergence by the L2L1 continuity theorems and Taylor expansion.",
                    "label": 1
                },
                {
                    "sent": "You have convergence to this operator, which is takes a function and multiplies it by the density.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "So let's now let's look at a specific case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You suppose you have just two, so now this is the ideal clustering situation like we have just two distinct blocks.",
                    "label": 0
                },
                {
                    "sent": "And the density is constant P1 and P2 in both cases.",
                    "label": 0
                },
                {
                    "sent": "Well, in the limit, what you're going to get is basically.",
                    "label": 0
                },
                {
                    "sent": "There's going to be eigenvectors which essentially correspond to P1.",
                    "label": 0
                },
                {
                    "sent": "And eigenvectors correspond to P2.",
                    "label": 0
                },
                {
                    "sent": "And if in fact you ordered and this is important, if you entered the data not in the random order that they were observed, but entered them in order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "Then what you will indeed have is that the well I think it's the other way around, but Oh no.",
                    "label": 0
                },
                {
                    "sent": "Sorry, that's right.",
                    "label": 0
                },
                {
                    "sent": "P2 is the larger one, right?",
                    "label": 0
                },
                {
                    "sent": "So that correspond to the larger set.",
                    "label": 0
                },
                {
                    "sent": "The larger eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "This is only two eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Here P1 and P2, but unfortunately eigenfunction structure is very complicated.",
                    "label": 0
                },
                {
                    "sent": "But basically what you will have then is, as you might expect, that the eigenvectors will have.",
                    "label": 0
                },
                {
                    "sent": "First block of coordinates be.",
                    "label": 0
                },
                {
                    "sent": "Non zero and then zero and the second the eigenvectors corresponding P2 will have the second set of coordinates be non zero in the first set of coordinates be 0.",
                    "label": 1
                },
                {
                    "sent": "Well, this is nice in this particular case.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in general you start to get into trouble because now if you notice right, there are other values.",
                    "label": 1
                },
                {
                    "sent": "If you look at the mode of the second one.",
                    "label": 0
                },
                {
                    "sent": "Right there, values in the first region.",
                    "label": 0
                },
                {
                    "sent": "Which correspond which, which have which have the same value.",
                    "label": 0
                },
                {
                    "sent": "And therefore you are going to have eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Remember, you're going to have eigenvector going to mix up of eigenvectors, so to speak.",
                    "label": 0
                },
                {
                    "sent": "If you try to look for the eigenvalue which corresponds to the smaller bump, so to speak.",
                    "label": 1
                },
                {
                    "sent": "You will also find among it.",
                    "label": 0
                },
                {
                    "sent": "Eigenvectors correspond to the middle region.",
                    "label": 0
                },
                {
                    "sent": "Of the larger boat.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I claim that, at least if you look at the kernel and it's not clear that that's what you should be doing.",
                    "label": 0
                },
                {
                    "sent": "But the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "But this analysis gives reasonable results for the 1 dimensional case.",
                    "label": 0
                },
                {
                    "sent": "You first of all, feeding the data in order.",
                    "label": 0
                },
                {
                    "sent": "You look at the First Capital in eigenvalue vectors of KN.",
                    "label": 0
                },
                {
                    "sent": "Capital N being.",
                    "label": 0
                },
                {
                    "sent": "Even reasonably large.",
                    "label": 0
                },
                {
                    "sent": "You threshold the coordinates of the eigenvectors, so you get yourself rid of.",
                    "label": 0
                },
                {
                    "sent": "Of things which are small but not exactly 0, and then you select K eigenvectors which have constant sign.",
                    "label": 0
                },
                {
                    "sent": "Notice you want functions which really map.",
                    "label": 0
                },
                {
                    "sent": "You don't want things that that wiggle right?",
                    "label": 0
                },
                {
                    "sent": "You really it would be nice to have.",
                    "label": 0
                },
                {
                    "sent": "Flat functions.",
                    "label": 0
                },
                {
                    "sent": "And therefore you looking at constant sign there sparse right there that support should be should be concentrated.",
                    "label": 0
                },
                {
                    "sent": "And which have connected support.",
                    "label": 0
                },
                {
                    "sent": "Better yet, I think actually I haven't explored this.",
                    "label": 0
                },
                {
                    "sent": "You find you've picked the first N eigenvectors, and you find K elements of the linear span, which somehow are maximally satisfy AB&C.",
                    "label": 1
                },
                {
                    "sent": "And of course, one has to think of what do you mean by that, right?",
                    "label": 0
                },
                {
                    "sent": "Sparse?",
                    "label": 0
                },
                {
                    "sent": "We can think of the Glass Sioux constant sign we understand connected support.",
                    "label": 0
                },
                {
                    "sent": "Somehow you have to have some assumption of smoothness effectively in your in your in your in your in your eigenfunction.",
                    "label": 0
                },
                {
                    "sent": "Do you want to pick that?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a clustering example.",
                    "label": 0
                },
                {
                    "sent": "In one dimension is really easy.",
                    "label": 0
                },
                {
                    "sent": "There's only one hump.",
                    "label": 0
                },
                {
                    "sent": "OK. And you feed it in.",
                    "label": 0
                },
                {
                    "sent": "There's the histogram.",
                    "label": 0
                },
                {
                    "sent": "A 400 observations.",
                    "label": 0
                },
                {
                    "sent": "You look at the first eigenvector and it's really great.",
                    "label": 0
                },
                {
                    "sent": "Just absolutely Maps out for you.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian density.",
                    "label": 0
                },
                {
                    "sent": "The second picture shows you what happens if you simply feed in the data randomly.",
                    "label": 0
                },
                {
                    "sent": "So when you cluster, that doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "But if you actually want to get shapes out of the eigenvectors, it does.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so here's an example where you have a mixture.",
                    "label": 0
                },
                {
                    "sent": ".5 normal 01.5 normal 21 so they overlap somewhat and .5 normal zero 1 + .5 normal 2.25 so it's quite narrow.",
                    "label": 0
                },
                {
                    "sent": "And again, as you can see, you feed these things in an order and the eigenvectors are carrying information.",
                    "label": 0
                },
                {
                    "sent": "Right one is hanging around zero, the other one too.",
                    "label": 0
                },
                {
                    "sent": "But and again, looking for eigenvectors which have constant sign and.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the clustering just quickly, it's this is a situation where you in fact are doing clustering quite well.",
                    "label": 0
                },
                {
                    "sent": "It's not a difficult situation, but you'll see there.",
                    "label": 0
                },
                {
                    "sent": "the Blues are the the.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Correctly classified as normal 01, the Reds are classified as normal, digital .25 and yellows and greens are misclassified, and as you can see, they're not.",
                    "label": 0
                },
                {
                    "sent": "It's pretty easy.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now this would get the same picture if you had the noisy eigenvectors or the non noisy eigenvectors would still get the same clustering story.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "The next story, which actually should have.",
                    "label": 0
                },
                {
                    "sent": "I'll discuss the theory of this a little bit later.",
                    "label": 0
                },
                {
                    "sent": "I'm afraid these slides came to me.",
                    "label": 0
                },
                {
                    "sent": "Whoops, I've got 5 to 3 minutes, right?",
                    "label": 0
                },
                {
                    "sent": "5 minutes OK.",
                    "label": 0
                },
                {
                    "sent": "These slides came to me.",
                    "label": 0
                },
                {
                    "sent": "Morning, Ann and.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there's some some noise in the communications, but here you see the normalized Laplacian eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "You do the same thing.",
                    "label": 0
                },
                {
                    "sent": "You feed the things in order.",
                    "label": 0
                },
                {
                    "sent": "And you instead of looking at the eigenvectors of the.",
                    "label": 0
                },
                {
                    "sent": "Colonel, you look at the eigenvectors of Laplacian and you again get structure.",
                    "label": 0
                },
                {
                    "sent": "But it's not.",
                    "label": 0
                },
                {
                    "sent": "I think it's not really quite as clear in this particular case.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, as far as the clustering goes, you basically do just as well.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now in more than one dimension.",
                    "label": 0
                },
                {
                    "sent": "As was pointed out by the real difficulty is that you don't know which eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "To look at.",
                    "label": 0
                },
                {
                    "sent": "Essential.",
                    "label": 0
                },
                {
                    "sent": "For kernel matrices, because we don't have necessarily an order notion of order.",
                    "label": 0
                },
                {
                    "sent": "And so you have, you know that you're going to look at the things corresponding to the largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "But some of those are going to come from from points, which may be quite far away.",
                    "label": 0
                },
                {
                    "sent": "And there is, yeah, I think it's a problem that one can deal with, but it's someone has to think about and so KN and the unnormalized Laplacian which was shown actually just just in this recent paper by looks for both ski and Bell, can have the same problem.",
                    "label": 1
                },
                {
                    "sent": "The normalized laplacian.",
                    "label": 0
                },
                {
                    "sent": "However, as was pointed out.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bye bye.",
                    "label": 0
                },
                {
                    "sent": "Boaz nodler is nicer.",
                    "label": 0
                },
                {
                    "sent": "Here the instead of it does converge with diagonal operator, but the diagonal operators, the identity.",
                    "label": 0
                },
                {
                    "sent": "And because of diagonal operates the identity, you can look at the difference between it.",
                    "label": 0
                },
                {
                    "sent": "And divide by Omega and let Omega tend to 0.",
                    "label": 0
                },
                {
                    "sent": "And then what you get is in the limit you get a differential operator.",
                    "label": 1
                },
                {
                    "sent": "Yes, there it is.",
                    "label": 0
                },
                {
                    "sent": "It's epicenter, the Laplacian of F minus some constant times the inner product of the gradient of the logarithm of the density.",
                    "label": 0
                },
                {
                    "sent": "And the gradient of the function.",
                    "label": 0
                },
                {
                    "sent": "And this again, you say, well, it's still encoding for the density, but it's encoding for the derivative of the density.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's what we're sort of looking for.",
                    "label": 0
                },
                {
                    "sent": "You're looking for zeros of the density when you're looking for when you're looking for modes.",
                    "label": 0
                },
                {
                    "sent": "And so, in fact, this operator with suitable boundary conditions, does have distinct eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "And it's in situations which which you know never talked about.",
                    "label": 1
                },
                {
                    "sent": "You actually have very quick drop off.",
                    "label": 0
                },
                {
                    "sent": "If you actually did, the eigenvectors do sort of paved the way eigenfunctions do sort of behavior where you'd like them too.",
                    "label": 1
                },
                {
                    "sent": "But the eigenvectors eigenvectors are in general uninterpretable unless the data are structured, and that's again not so pleasant.",
                    "label": 0
                },
                {
                    "sent": "And I really.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so where is?",
                    "label": 0
                },
                {
                    "sent": "OK, that's so, so that's.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Here are some clustering examples.",
                    "label": 1
                },
                {
                    "sent": "So this is kernel clustering.",
                    "label": 0
                },
                {
                    "sent": "Where you I cheated?",
                    "label": 0
                },
                {
                    "sent": "And the data will fit in in order.",
                    "label": 0
                },
                {
                    "sent": "In other words, you fit in sample 1 first and sample 2, then sample 3.",
                    "label": 0
                },
                {
                    "sent": "And now if you look at the.",
                    "label": 0
                },
                {
                    "sent": "This is actually the clustering that you get, but in fact not only this clustering workout, it's clear which which which eigenvectors you should use, and you.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, no no.",
                    "label": 0
                },
                {
                    "sent": "I'm getting I'm getting.",
                    "label": 0
                },
                {
                    "sent": "I'm getting myself confused here.",
                    "label": 0
                },
                {
                    "sent": "No no.",
                    "label": 0
                },
                {
                    "sent": "What happens here is that you there's a rule which was proposed by Belkin Sheet.",
                    "label": 0
                },
                {
                    "sent": "Yes, Belkin Xi'an bin you where you basically look at eigenvectors which have constant support.",
                    "label": 0
                },
                {
                    "sent": "Just use those.",
                    "label": 0
                },
                {
                    "sent": "Which has constant sign.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately, then, in this particular example, for example, you start to pick up.",
                    "label": 0
                },
                {
                    "sent": "Five eigenvectors of matter, but in fact only three.",
                    "label": 0
                },
                {
                    "sent": "I'm I'm.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm about to finish.",
                    "label": 0
                },
                {
                    "sent": "OK well, this is Laplacian clustering and you get.",
                    "label": 0
                },
                {
                    "sent": "The same here you get.",
                    "label": 0
                },
                {
                    "sent": "So here you get social class.",
                    "label": 0
                },
                {
                    "sent": "In clustering is actually giving you the top three eigenvectors actually OK, so passing clustering.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, the.",
                    "label": 0
                },
                {
                    "sent": "Eigenvectors.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't actually look very natural, and that's I think consistent with some results of of your Goldbergs and Yankees.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so so so whatever you try to do is sort of introduce you very, very quickly.",
                    "label": 0
                },
                {
                    "sent": "To this area, I think it's an important way of dimension reduction, but there are a lot of things which have to be.",
                    "label": 0
                },
                {
                    "sent": "Which remains to be better understood about it.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "That's one question.",
                    "label": 0
                },
                {
                    "sent": "Oh alright, can I go on for five since we have a little bit of time at the end, can I go on for for five more minutes?",
                    "label": 0
                },
                {
                    "sent": "OK, what?",
                    "label": 0
                },
                {
                    "sent": "That's the point.",
                    "label": 0
                },
                {
                    "sent": "Alright, OK no no.",
                    "label": 0
                },
                {
                    "sent": "I I there's one other thing I want to point out OK here so so these are just questions there.",
                    "label": 0
                },
                {
                    "sent": "There's theoretical questions about showing that the sample theory converted to limit is an tends to Infinity and W 10 zero error rate results issues.",
                    "label": 0
                },
                {
                    "sent": "OK the real big kit questions that are way of rescuing.",
                    "label": 0
                },
                {
                    "sent": "I think the kernel matrix for D bigger than one.",
                    "label": 0
                },
                {
                    "sent": "But that is the question, what happens when D tends to Infinity?",
                    "label": 0
                },
                {
                    "sent": "Right, that's the large P. Large in situation, now there's an.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Teresting result also very recently nerdy.",
                    "label": 0
                },
                {
                    "sent": "Now, Carly.",
                    "label": 0
                },
                {
                    "sent": "Which seems to say that all of this effort is worthless.",
                    "label": 0
                },
                {
                    "sent": "Because Nordin, who carries result, is that if you have a distribution, for example, like the Gaussian, the spherical Gaussian, or something like that, you know you can imagine the covariance matrix isn't the identity.",
                    "label": 0
                },
                {
                    "sent": "Then asymptotically, the spectrum.",
                    "label": 0
                },
                {
                    "sent": "This is for fixed Omega now.",
                    "label": 0
                },
                {
                    "sent": "Is the same as the spectrum of the ordinary covariance matrix, and we know the ordinary covariance matrix is very bad behave.",
                    "label": 0
                },
                {
                    "sent": "For large people you know the eigenvectors don't mean anything.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The Laplacian behaves in exactly the same way.",
                    "label": 1
                },
                {
                    "sent": "This no no gain in that.",
                    "label": 0
                },
                {
                    "sent": "So here is.",
                    "label": 0
                },
                {
                    "sent": "This is these benefits up seem to have some applicability, and yet this seems to suggest that in the in the situations we want wants to use them.",
                    "label": 0
                },
                {
                    "sent": "For large dimension, this problem as well.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I have a.",
                    "label": 0
                },
                {
                    "sent": "Different view.",
                    "label": 0
                },
                {
                    "sent": "So my view is that somehow one maybe useful way of thinking of high dimensional space data.",
                    "label": 0
                },
                {
                    "sent": "Is that what you have is a mixture of low dimensional things, possibly with some noise, but hopefully the noise is relatively symmetric.",
                    "label": 0
                },
                {
                    "sent": "And let's say where the total dimension of the stuff is bounded.",
                    "label": 0
                },
                {
                    "sent": "But the the ambient dimension is arbitrary.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some things to support this view.",
                    "label": 0
                },
                {
                    "sent": "OK, this I'll really flash by is simply look at.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zip code, digits and you look patches.",
                    "label": 0
                },
                {
                    "sent": "You take patches on the images and if you look at the distribution of.",
                    "label": 0
                },
                {
                    "sent": "There's a notion, local density, local dimension, that that can be defined.",
                    "label": 0
                },
                {
                    "sent": "The local dimension of these things, which are, you know, the patches are 16 by 16.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the pattern 4 by 4 so 16 dimensions.",
                    "label": 0
                },
                {
                    "sent": "As you can see the local dimension ranges from zero to basically 5.",
                    "label": 0
                },
                {
                    "sent": "So somehow there is some evidence that things are really being composed out of out of out of low dimensional structures.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now here's a real question.",
                    "label": 0
                },
                {
                    "sent": "If this view holds, do the eigenvectors of the normalized Laplacian provide?",
                    "label": 1
                },
                {
                    "sent": "As Omega intends to zero successful clustering in Peter mental space.",
                    "label": 1
                },
                {
                    "sent": "Can this be established asymptotically in a reasonable way?",
                    "label": 1
                },
                {
                    "sent": "Can can be adapted to give similar results, which I think it can.",
                    "label": 0
                },
                {
                    "sent": "And and more so.",
                    "label": 1
                },
                {
                    "sent": "Three and four months, same As for D = 1, can pre clustering in some way, which is equivalent of ordering the data yield low dimensional representations for high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "So 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "Peter example, you soon and I don't know much about it, but I don't follow people claiming that these groups really quite well.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Classy and the normalized reply.",
                    "label": 0
                },
                {
                    "sent": "Well, there's been a mixture of claims or people have gotten good results who claim good results for kernel clustering.",
                    "label": 0
                },
                {
                    "sent": "But on the whole, I think the empirical as well as the theoretical evidence seems to be that the normalized Laplacian clustering is giving reasonable results, yes?",
                    "label": 0
                },
                {
                    "sent": "What you're doing is kernel is putting a matrix from out of state.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Justina well, I mean you can ask, I mean.",
                    "label": 0
                },
                {
                    "sent": "There's no no no, but notice I actually hear this is this?",
                    "label": 0
                },
                {
                    "sent": "I've sort of.",
                    "label": 0
                },
                {
                    "sent": "Generally people think of this for fixed bandwidth, right?",
                    "label": 0
                },
                {
                    "sent": "But somehow you gain something from fixed bandwidth I. I'm not so convinced that Ann and the point is that when you let the bandwidth get small, you really are encoding for all the information.",
                    "label": 0
                },
                {
                    "sent": "Problem occurs.",
                    "label": 0
                },
                {
                    "sent": "Indeed, there is a problem.",
                    "label": 0
                },
                {
                    "sent": "Curse of dimensionality in the case like the one that the Nordine considered if the dimensions high right if the distribution fills all space, then the cursor dimensionality applies.",
                    "label": 0
                },
                {
                    "sent": "However, if indeed things live in a much lower dimension.",
                    "label": 0
                },
                {
                    "sent": "When you think they do.",
                    "label": 0
                },
                {
                    "sent": "In fact.",
                    "label": 0
                },
                {
                    "sent": "The dimension that applies is the intrinsic dimension rather than the ambient dimension, so so so in some sense I think the cursor do internally, maybe someone illusory.",
                    "label": 0
                },
                {
                    "sent": "However you still end up with things which apparently you have a density of 100 coordinates or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "But what this?",
                    "label": 0
                },
                {
                    "sent": "I think what I find most interesting about this is not so much the clustering as big, but the dimension reduction aspect.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Versions of the operator, or else in the end to Elwyn and goes funky and WN goes to zero.",
                    "label": 0
                },
                {
                    "sent": "I think there is a paper of what's there.",
                    "label": 0
                },
                {
                    "sent": "Hi, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "I think there is a paper which deals with the operator convergence.",
                    "label": 0
                },
                {
                    "sent": "Of eliminating the M2L's in Darwin Angus.",
                    "label": 0
                },
                {
                    "sent": "When thinking I see it, that's that's a paper.",
                    "label": 0
                },
                {
                    "sent": "I don't know why, but I think it's a pointwise convergence, so he doesn't like versions of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And now you vectors.",
                    "label": 0
                },
                {
                    "sent": "But so I think it's still open, but at least David yeah.",
                    "label": 0
                },
                {
                    "sent": "The above situations too.",
                    "label": 0
                },
                {
                    "sent": "OK, I'd be very interested in that reference.",
                    "label": 0
                },
                {
                    "sent": "As I said, I'm sort of learning about this area.",
                    "label": 0
                },
                {
                    "sent": "There is an issue actually which is not trivial with that be 'cause if you look you do the usual calculation that you do with density estimation, right?",
                    "label": 0
                },
                {
                    "sent": "Well, you have to expand the thing.",
                    "label": 0
                },
                {
                    "sent": "To a derivative you have to hope that there's a derivative there.",
                    "label": 0
                },
                {
                    "sent": "But if there's a derivative there there somehow living in Sobolev space, but you can't hope to prove convergence, strong convergence, it's overlap space.",
                    "label": 0
                },
                {
                    "sent": "But what you can prove, I think, is.",
                    "label": 0
                },
                {
                    "sent": "Convergence in L2, which is actually probably enough, but I don't know.",
                    "label": 0
                },
                {
                    "sent": "I'm just talking wild, but I think it is possible to get results which show that the eigen structure.",
                    "label": 0
                },
                {
                    "sent": "Convergence.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "What you say is so concerning the high dimensional data, yes.",
                    "label": 0
                },
                {
                    "sent": "Dimensionality despond latex and you can apply it to your vendors filled with you in every small neighborhood death run for it.",
                    "label": 0
                },
                {
                    "sent": "Well, that's that's exactly that, yeah.",
                    "label": 0
                },
                {
                    "sent": "Instead of the national adoption so you can take the log in national space leaking, but like another way of looking at this and you're gonna say where fly out of values that work with children.",
                    "label": 0
                },
                {
                    "sent": "Abundance is not the number.",
                    "label": 0
                },
                {
                    "sent": "Are you better this?",
                    "label": 0
                },
                {
                    "sent": "It could be better.",
                    "label": 0
                },
                {
                    "sent": "Patient could be measured.",
                    "label": 0
                },
                {
                    "sent": "Try to extend all the story.",
                    "label": 0
                },
                {
                    "sent": "So I mean you can see instead of the mixed reduction you you try to adjust it you know or have ended.",
                    "label": 0
                },
                {
                    "sent": "Well, I think I think I think it's correct.",
                    "label": 0
                },
                {
                    "sent": "In fact there is some empirical work as I recall having in machine learning is the literature again, but it pays off to localize your current, your bandwidth.",
                    "label": 0
                },
                {
                    "sent": "However, coming back to your first question about the emptiness of space.",
                    "label": 0
                },
                {
                    "sent": "At some level, I think if the space is really empty, and if it's really like the.",
                    "label": 0
                },
                {
                    "sent": "The dimension is really large.",
                    "label": 0
                },
                {
                    "sent": "Somehow I interpret stones least favorable results and so on.",
                    "label": 0
                },
                {
                    "sent": "That's telling you that you're dead, you just can't do anything, because in principle.",
                    "label": 0
                },
                {
                    "sent": "So why can we do things?",
                    "label": 0
                },
                {
                    "sent": "Cause if in fact things do live on manifolds and which are smooth.",
                    "label": 0
                },
                {
                    "sent": "And and and, and let's say you have symmetric error.",
                    "label": 0
                },
                {
                    "sent": "That's something that in Ording seems to be able to argue that in fact you can keep.",
                    "label": 0
                },
                {
                    "sent": "Symmetric errors of fixed bandwidth.",
                    "label": 0
                },
                {
                    "sent": "Sorry no, the bandwidth in anyone dimension goes to 0, but the whole thing is the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "This is bounded.",
                    "label": 0
                },
                {
                    "sent": "Then you can.",
                    "label": 0
                },
                {
                    "sent": "You can argue that in fact the difficulty of the problem is governed not, although you think you operating in 1000 dimensions actually operating in one dimension.",
                    "label": 0
                },
                {
                    "sent": "If things are smooth enough.",
                    "label": 0
                },
                {
                    "sent": "Can you somehow for strength from?",
                    "label": 0
                },
                {
                    "sent": "Take a massive literature income, less information on selecting bandwidth littleberry correctly.",
                    "label": 0
                },
                {
                    "sent": "I, I think that I think you I think you can.",
                    "label": 0
                },
                {
                    "sent": "The question is how you do it and.",
                    "label": 0
                },
                {
                    "sent": "One of the things that I mean we showed we have a little paper bouillion I've little paper we show that.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For regression, right?",
                    "label": 0
                },
                {
                    "sent": "If your regression and this and the predictors are actually concentrated on a low dimensional smooth space.",
                    "label": 0
                },
                {
                    "sent": "Then if you do actually local regression, you're OK, However.",
                    "label": 0
                },
                {
                    "sent": "You cannot pick your bandwidth by any of the usual prescriptions which have to do with the.",
                    "label": 0
                },
                {
                    "sent": "You know, which basically tells you that the bandwidth behaves like H to the power D. With these the ambient dimension, you have to pick it either by some cross validation sort of thing or by using the kind of thing that it is possible to construct.",
                    "label": 0
                },
                {
                    "sent": "Estimates of local dimension.",
                    "label": 0
                },
                {
                    "sent": "And then then again.",
                    "label": 0
                },
                {
                    "sent": "But the literature may be relevant.",
                    "label": 0
                },
                {
                    "sent": "Xbox.",
                    "label": 0
                }
            ]
        }
    }
}