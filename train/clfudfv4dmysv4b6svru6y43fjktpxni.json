{
    "id": "clfudfv4dmysv4b6svru6y43fjktpxni",
    "title": "Building blocks for semantic search engines: Ranking and compact indexing in entity-relation graphs",
    "info": {
        "author": [
            "Soumen Chakrabarti, Department of Computer Science and Engineering, Indian Institute of Technology Bombay"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Semantic Web->Annotation",
            "Top->Computer Science->Search Engines"
        ]
    },
    "url": "http://videolectures.net/iiia06_chakrabarti_rcier/",
    "segmentation": [
        [
            "Good afternoon actually.",
            "I don't know if it's better to go just before lunch or just after lunch, but hopefully we'll find some fun stuff going on here and keep awake.",
            "So I'm going to talk about semantic search engines and.",
            "In a sense, I feel insecure taking that name in vain.",
            "And who knows what semantic search is in any case."
        ],
        [
            "So if it's first lunch and the title is too long, you know in fewer words ranking and indexing for semantic search.",
            "This is with some of my students and supported by some of the search companies."
        ],
        [
            "So what is semantic search?",
            "They said I'm not quite sure what it is we are groping around trying to feel what it means and for the purpose of today's talk.",
            "I would like to define semantic search as kind of search which has to combine creatively.",
            "String match with the entity match.",
            "So the first kind of strings we see while searching are strings with.",
            "Meaning I say I want to find a flight to Hawaii in under $500 and what I'm looking for is a flight, and that's the kind of entity I want to instantiate that to a specific flight, whereas other strings can perhaps effort to remain uninterpreted, like how I might suffice to match a string without understanding what.",
            "How is.",
            "But flight certainly would not suffice.",
            "So in this talk I'd be talking about is the relations and some of the kinds of relations and searching and ranking would involve issues like word proximity, conductance, properties of graphs which represent the entities and relationships connections.",
            "But it turns out that in spite of relatively modest start, we can approximate many, many information needs quite well and one point I'd like to impress throughout the talk is that.",
            "This is not about warehousing, so in the in the standard database warehousing literature, people have been content to first turn and unstructured corpus of text into relational tables, perhaps with approximate scores, and then subject that relational data to standard structured queries and structured mining.",
            "Now my claim is this will never be enough in the context of the semantic web, because your schema is never complete.",
            "The schema is evolving.",
            "People haven't defined enough types yet, and definitely not enough relations.",
            "And all search has to simultaneously involve some pieces of schema with other parts of unstructured data."
        ],
        [
            "So here's an example.",
            "I have this running corpus born in New York in 1934.",
            "Second was a noted astronaut, etc.",
            "So in this diagram, words are connected by dotted lines to a type hierarchy, and those lines are dotted.",
            "Because I'm uncertain about what to connect where.",
            "For example, without knowing a lot about the context and maybe having a reasonable world model, I cannot figure out if New York is a city or a state, but observe that that may do limited damage cause those nodes converge at region close enough.",
            "So maybe for many query needs, the fact that I'm.",
            "Confused between city and state may not actually matter.",
            "The other nodes.",
            "There are edges which are in this graph, at least certain, and those are the solid edges and those are all his relationships.",
            "So I know that person is an entity and scientists with the person and so on.",
            "And these tokens may also have specific deterministic predicates with them, like 1934 is a sequence of 4 digits.",
            "It also has a digit.",
            "Maybe there's some information extractor or query processing system which understands that 4 digits in sequence preceded by in is indicative of a year, and so I can answer questions of the form.",
            "So ahead of time I just don't know how this compound structure, comprising of the linear text and the nonlinear graph is going to be quick.",
            "For example, one query involving this graph could be.",
            "You know name of physicists to search for intelligent life in the cosmos and we have tools which can translate things like that into soft structured query which says find me an instance of a physicist near cosmos and so on.",
            "Now for that query I would like to exploit the fact that signal is an astronomical is a physicist.",
            "OK, whereas some other query might, so where was second born and then second becomes the stuff you want to match the string.",
            "But where should map to region and that flows down to New York and here's an example where knowing for sure whether New York is a city or state is not important.",
            "You're still answer the question correctly.",
            "Or someone else when was second born and then you know I need to pick on 1934 except in this case at least inside Wordnet I didn't create ahead of time any connection between the string 1934 and a year or time.",
            "I found it out by some device bridging through those surface patterns, so that's sort of the delay of the land."
        ],
        [
            "No, the query class we address in this talk looks like this.",
            "So find a token span W, such as.",
            "You know New Yorkers Sagan, which appears in some context such that W is a mention of an entity East, so the string Carl ET cetera Carl Sagan is a mention of the concept of that specific physicist, so E is that entity in the physicist and is an instance of a type or the answer type given in the query.",
            "So find me a physicist who studied the cosmos.",
            "Which is mostly in this Doc mentioned in red.",
            "And that token span W has to be near in ways that will define gradually in various ways.",
            "Selector string selector strings are in green and those are the things you can kind of match without understanding what they are.",
            "So search intelligent life in Cosmos in one particular query might change in a different query on the same extracted graph, so of course all of these are imprecise.",
            "I can never know for sure whether token spam, WS or mention of East it could be a different 2nd.",
            "I'll never know for sure if you know he's an instance of an A type in the previous chart, we saw that the solid edges told you for sure.",
            "Where the Rays of type B and so on.",
            "But in real life you will not know.",
            "Michael Jordan may be a footballer, might be a basketball player or might be a mathematician.",
            "So in this talk will assume that those links in the graph are reasonably certain and clean.",
            "We will focus on the near part.",
            "You know how to characterize near how to score nearness in spite of these limitations.",
            "The framework is surprisingly powerful.",
            "We tried fitting ranking functions.",
            "For this structure of queries and we found that for the trick question answering benchmark, our system came up with the right answer.",
            "Tokens in the third or fourth position most of the time."
        ],
        [
            "So this talk will have two parts.",
            "In the first part will talk about the near scoring framework.",
            "And if you look at the literature in information retrieval, of course there are query primitives for insisting that towards appear within five tokens of each other, like span queries and so on window queries, but the distance has to be specified exactly X query and export do provide window limits like the distance at most 10 words.",
            "Whether you want an ordered match with the window or you don't.",
            "But this is not learned and the full text contents clause of X squared and X path does allow you to go through some sort of a mapping thesaurus to match related words or narrower terms with the level limit, but still very very.",
            "They're learning free, so as the person who is asking the query of the specify, everything here is my thesaurus.",
            "I'd like to map this particular word to all related words in hardware terms up to L levels deep in the hierarchy.",
            "Now of course other end user you don't want to do that.",
            "There's no known implementation which combines this issue of narrower terms, like physicist, narrowing down to second together with the proximity information that comes on the corpus itself.",
            "So to go back to the picture, there are two things involved in this."
        ],
        [
            "Of the ranking, I'd like to efficiently map from the query into a node in the type hierarchy are like to walk down to different points in the corpus such that those points are strongly activated by things matching in green.",
            "OK, so it's kind of reaching out to the answer from both sides.",
            "From the tech side as well as the type taxonomy said."
        ],
        [
            "So that's the first part and becausw the ranking technology here is nowhere as mature standard information retrieval.",
            "We propose a learning framework for graph proximity."
        ],
        [
            "And the second part of the talk would be about how to index these things so you know if I want to ask, you know, find me a person near theory and relativity as match words.",
            "We could in principle expand out person to every kind of person we know inside word net like physicist, politician, cricketer etc.",
            "Now this is totally impractical and this would result in a large fan out at creating.",
            "On the other hand, if you want to pre annotate the corpus, that's complex too.",
            "So in some recent work by orders union students at University of Washington, they have built a system called the binding engine, and there just annotating three or four kinds of named entities in the corpus and indexing it for quick access leads to a tenfold increase in index size, whereas our target is something like 18,000 eight types like the ordinate internal nodes today, and maybe even more later.",
            "So in the second part of the talk, I'll describe a workload driven indexing query optimization that essentially depends on the extreme skew in the query workload so that only certain types are necessary to index and others are not.",
            "But you have to be careful about that thanks to that long tail."
        ],
        [
            "Business.",
            "So here's the first part.",
            "Scoring and ranking nodes in graphs.",
            "Feel free to interrupt and ask questions."
        ],
        [
            "So discuss 2 flavors of ranking problems or related.",
            "One is a sort of generalization of the other.",
            "In the first part we talk about the query class which we just discussed, which is fine.",
            "Me objects of a specified type near match keywords and type membership is perfect.",
            "OK, we don't.",
            "We don't question that.",
            "Nearest captured through two kinds of things.",
            "Token rareness if I say find scientists near pneumonia, the pneumonia is a much better word than find scientists near experiment.",
            "So rareness and distance between those match tokens and the candidate token, or token spam.",
            "The second kind of ranking is a bit more involved technically.",
            "Here we have a general graph with type edges and nodes with text, and we think of doing random walks on this graph as like an approximation to page rank.",
            "So here is a node which represents a company.",
            "These people work there, so works for edges, and these people receive and send emails.",
            "They write papers, papers are cited by other papers, and emails are sent in responses to emails and so on.",
            "Now if I want to do some kind of semantic search on this sort of graph.",
            "I might say find me a person who is near IBM and XML right?",
            "And to answer those kinds of questions, one usually there have been at least three or four papers which adapt Pagerank like scoring to this graph by saying OK, maybe works for Link has a certain amount of conductivity which is different from person sending email and with those different edge weights I'm going to run a different sort of page rank.",
            "And then rank the nodes in order of that page rank.",
            "Now the problem is that you know to our knowledge no one has been thinking of learning those edge weights based on relevance feedback the hardware and plugging those numbers and then the ranking comes out and they say OK, we feel pretty happy with it.",
            "So one question is how to take the relevance feedback literature from standard information retrieval into a regime where things are not vector space documents but nodes are entities that nodes in a graph and they're being ranked.",
            "So we'll see that in the one be part of the talk."
        ],
        [
            "So you have the first somewhat easier setup for ranking token spots.",
            "So let's say my question is, who was the inventor of television?",
            "Or who invented television?",
            "And because it's a who question some device which we don't talk about here would map the question to person and every token in the corpus which connects through two person is a candidate.",
            "So John Baird is a candidate to Comspan, which you have to score.",
            "Now, in this case the stem invent appears twice close to that.",
            "So think of the corpus as being a big chain graph with some nodes being connected to that type taxonomy.",
            "And the candidate appears at offset zero and then there are nodes of the left and right of it.",
            "So the first occurrence of inventor is at position minus one.",
            "The further occurrences of minus four and television, which turns out to be a rare term is appearing at minus six, and it was roughly think of the situation as maybe every matched green selector sends out some form of energy activation spreading style, and if the word is rare, it has a lot of energy.",
            "Forward is more frequently invent.",
            "It does somewhat lower amounts of energy, and those are radiated out and a certain amount of that energy reaches the candidate.",
            "I could think of maybe adding up those energies OK, but it's probably a bad idea to add up the energy for invent twice, because in that case very common word appearing multiple times near your candidate is going to boost up your score, so that's not typically a very good thing to do in our work.",
            "We have assumed that we take the Max over the same selector, and we add up over different selectors, so we take only the closest talker and serve them at selector, and then we had to work the distinct select.",
            "So the considerations in summer here at the following.",
            "So this idea for selected is rare.",
            "It should have more energy.",
            "The distance from the selector to the match position of course matters, and you might roughly think that the bigger the distance, the smaller the influence, and that's broadly true, but it's not true at very small distance, and we shall see an interesting example of that when we learn it.",
            "We didn't expect it ahead of time, and maybe if you're a linguist, you understand why already, but we didn't.",
            "We didn't know ahead of time.",
            "So we want to actually learn the form of this function.",
            "We don't want to guess that there's been some XML scoring literature where people assumed an exponential decay.",
            "I don't want to assume that it turned out to be a good idea not to assume that.",
            "And if there are many occurrences of 1 selector, we should take the closest one.",
            "In particular, that's been a good choice.",
            "And we generally add over the different selectors, although it could do other things too.",
            "Anything would probably work fine."
        ],
        [
            "So rather than guessing parametric form for the decay function, will just fit explicitly make bins for position minus 32 + 30 and 50 number for each.",
            "So for the moment, suppose we assume that according to the left and occurs to the right are the same, so we collapse them on one side and so we have this W parameters to estimate.",
            "The horizontal axis was that some kind of.",
            "Representation of your index.",
            "Some kind of static?",
            "And is this a fixed."
        ],
        [
            "With one node allocated for each word is just a linear text.",
            "Yes, each document descriptor from another document, but each document is a linear sequence of words.",
            "So this is just a just imports in running text.",
            "Television was invented in 1925.",
            "How?",
            "For each document in apartment, yes.",
            "So we have one node for every word in the corpus, so of course we don't want to represent them explicitly."
        ],
        [
            "So if we believe that left and right are the same and there's no special significance direction, and let's say that we know that beyond uppercase, W words is way too far for you to capture any more dependencies than there, this W parameters would like to.",
            "Estimate so every candidate position is characterized by feature vector, which can create out of its neighborhood if there is a math selector distance J and this is the closest occurrence of that selector, then we set the feature at J to be the energy of.",
            "That's electric now, of course, you could have one selector at position plus five and another selector at minus five.",
            "There's some special cases you might want to add them up, and so on, but broadly speaking, you have the total amount of activation energy at various distances, and we think of it as sort of a multiplicative form.",
            "So if the decay function is characterized by this vector beta, then your activation at the node is basically the inner product over beta and F. OK, because if the selector matches at position J, it's scaled by weight beta J and then added on to the score of the candidates.",
            "So it's a standard inner product space an in that we're trying to express preferences of the form.",
            "Suppose we have some ground truth and I tell you that here are 10 answer context in here.",
            "100 non answer context.",
            "Then sampling from that we can get examples like I like candidate you less than I like Kennedy.",
            "You is relevant.",
            "It's an answer to my question, whereas V is not.",
            "So if I say that you is less preferable to V, what I mean is that.",
            "Later Dot the feature vector of you should be less than or equal to.",
            "Better the feature vector of so this framework has been known in machine learning for awhile, and one of the."
        ],
        [
            "Those mechanisms for estimating beta is the so-called rank SVM algorithm from Thurston Hawkins.",
            "And others, so the idea is to search for that better vector.",
            "In some part of the DII don't want to have the components of bitter blow up to Infinity or something, so I put regularizer like that which controls there to norm of data subject to.",
            "Bitter transporter dot FI minus better transpose FJ has to be less than equal to minus one, so earlier saying it has to be less than equal to 0.",
            "But now we're going to be stricter about it.",
            "That's the Max margin kind of framework, and for every I that I prefer less than J, I'm going to put up a constant of this phone.",
            "Free.",
            "So the soft margin form looks like this.",
            "This is standard transformation from standard hard margin, SVM's too soft margin, SVM's, the better.",
            "See men as before, except I'll include some slack variables and I'm going to forgive violations up to the slack variable.",
            "And then I'm going to charge a penalty for the slacks.",
            "OK, so now I want if I to be defeated by FJ after being given a benefit of SIG.",
            "And if I have to give a lot of benefit, I want to penalize the objective.",
            "So if you eliminate the SS, you can actually write that equation entirely in terms of the betas.",
            "The objective looks sort of the same up to this point.",
            "After this idea is replaced by the Max of zero and one plus Beta Phi minus bitter.",
            "OK.",
            "So this is the classic hinge loss and it turns out that if you insist on the hinge loss, then this is a quadratic program, which takes a long time to solve.",
            "But there's good news this August, there's a paper which talks about training a linear kernel SVM in linear time, so that's improving all the time.",
            "But in practice we have found that it's probably best to replace this hinge loss with a sharp corner.",
            "With a smoother function and then optimize the resulting objective function."
        ],
        [
            "So what kind of loss functions can we use so the hinge loss in the previous chart was this blue line and there's a host of others.",
            "Maybe all of your conversant with all the learning literature, but one commonly used approximation to the hinge loss is a so called log X plus or the soft hinge loss, which is just one log of 1 plus into the power X which asymptotes 20 as well as 2 equal to X on both sides in between.",
            "It's a bit of a nuisance.",
            "You can even use an exponential loss if you are not too unhappy with the right hand side.",
            "Do the Huber loss, which is 0 up to zero, then it's a parabolic Y equal to constant times X squared up to a certain point and after that it's parallel to or equal to X.",
            "So these are all approximations to the true loss, which is basically that you get a point whenever you rank inj correctly with respect to each other and you lose a point if you don't.",
            "Let's see how it does so we can try all kinds of smooth losses.",
            "I've tried the smooth hinge as well as Huber and expand for this particular problem.",
            "All of them work reasonably well, but for the next problem that I would describe, only Huber loss seems to work reasonably well.",
            "The other thing to note is that absolute values in beta don't mean anything becausw.",
            "My ranking function is better.",
            "Dot FI can shift and scale better, however I want.",
            "And the other prior belief about this setup is suppose here in our position zero.",
            "I have my candidate token and a selector or a match happens at position 29 versus 30.",
            "We shouldn't really believe that 29 and 30th at different, so there has to be some smoothness about beta, right?",
            "So we can also ensure that by setting beta of W plus 120 arbitrarily.",
            "Because remember fixed offsets don't matter and then I can try to penalize Addison Betas from.",
            "Differing too much.",
            "We could also force monotonic decrease in beta, but that turns out to be a bad idea."
        ],
        [
            "So finally, our model complexity may be written down as some over J. Cole to want to W better J minus Peter J plus one whole squared, which encourages artisan builders to be close together.",
            "And then plus the loss function, which is an approximation to my training error.",
            "Which we discussed some examples of.",
            "So when we do this, we find that beta J plotted against Jay smooth suitably.",
            "This is the best cross validation point.",
            "Looks like this, so there is a very steady growth up to about four or five.",
            "And then there is a steep decay which is sort of exponential.",
            "OK, down to some noise level up to about 50 tokens.",
            "So why is that so?",
            "Then we started inspecting the actual track document contexts where this match was taking place, and it turns out that, at least for that benchmark, and probably Even so, you know if we wanted to do named entity type searches on the web, most of the time your target type is a named entity.",
            "Which city are, you know, where was, or when was so on, and the context by which you qualify your question also has proper nouns and things like that.",
            "Mount named entities and named entities are almost always separated by action words and prepositions and articles and so on, and that accounts for that gap of three to four.",
            "You have to have some things in between to be able to connect up those two named entities and that accounts for the non monotonic form of the plot.",
            "So of course, if we thought about the linguistic aspect of this, we might have caught that ahead of time.",
            "But this was interesting to see coming straight out of just a list of Trek published positive and negative results for each question, we plug that into the framework and we crank the handle, and this graph comes out.",
            "So that's fairly nice.",
            "So this is the first kind of ranking.",
            "If we then take this ranking function and we plug it into ranking engine.",
            "So what we did is we took leucines indexing token offset framework.",
            "But we wrote our own scoring function on top of it and when you do that.",
            "The mean reciprocal rank of the answer, which is the average overall queries the reciprocal of the first rank, where an answer is found.",
            "So higher the better.",
            "Improves from .16 two .29 so point on a matter of .9 means that on average typically you're finding your answer between rank three and four.",
            "OK.",
            "So that's the story up to this point.",
            "This works if your graph is basically a chain graph, and you can characterize the parameter space is a vector.",
            "The distance between your candidate token and the match position.",
            "But what if your candidate in the match are nodes in a general graph?",
            "So that's the second part.",
            "Any questions on this so far, yes.",
            "How stable is your?",
            "Not parametric estimate for the values of and then.",
            "How about using parametric distribution like a plus song which would kind of have this sure?",
            "It's fairly stable, so we computed this graph for about three or four years of track data and they all looked essentially the same.",
            "Not so now that we know what it looks like.",
            "Sure, we should feel free to try parametric forms for this."
        ],
        [
            "OK, so the second part of scoring the story is that you know we have this really jumbled network and have this project called searching Personal information networks of Spin where there are all these adapters for email and papers and so on which emit this tiny graph.",
            "Let's so each adapter understands a small fragment or twigs which says, you know paper has citation and author and affiliation or an email is has a body to CC and from.",
            "And then there is either a registry of sorts or some learning technology to map it to what we call the PIN schema.",
            "So two and CC and author all of them map to person.",
            "And maybe I have OK.",
            "So maybe paper and email are differentiated, in which case papers and citations both going to paper and email goes to email obviously and authors affiliation goes to a company or organization.",
            "So this is a schema matching thing, and once that's done, any adapter can inject graph data into what we call the PIN review, which is an instance of the PIN schemer.",
            "So the pin to be adheres to the spin schema, so it's a little trickier though, because different.",
            "Others may be importing dirty data or redundant data, and we have another registry of reconcilers which can also inject information of the person of the form that these two person nodes are actually the same person, but I'm not quite sure.",
            "I think I'm about .9 shared that these two nodes are the same, or that this particular email mentions this person, which is an even more unsure position.",
            "So anyway, at this point we have a graph like this and would like to ask queries of the form.",
            "Find me a person who knows a lot about XML.",
            "OK, so how do I do ranking for that kind of query on this graph?",
            "And so the database community on the community have been very active."
        ],
        [
            "In designing systems of this form and.",
            "That's both kind of demonstrated by way of the system we are building, as well as this particular query.",
            "So if you said type equal to person near paper equal to XML, an index.",
            "This is the sort of screen viewed getting our system, and in this particular listing get hardware comes at the top of the list, and if you click on that node you're going to see get hard in the center of the graph, but you also want to explain why get hard is thus favored by the query.",
            "So what's going to happen is our system is also going to bring up a local context around Gerhard which is transferring the biggest amount of page rank into their heart from matched nodes.",
            "For example, here's a.",
            "Paper which talks about an index based XL search engine for querying XML data so that kind of explains why your heart is hard, erect, and you know I'll try live demonstration at the end of the talk you can kind of browse out from here and try to explain why and how those are connected to other Members in the list and so on.",
            "So the question is, what's the ranking model here?",
            "And today most of the database or IR literature in this area has basically said look, we think this is a good ranking function Now this of course you know in the.",
            "They are decades when people design TF IDF NVM 25 that was validated through years and decades of testing and experience in this domain.",
            "It's a more complex domain there.",
            "This nonlinear artifact, the graph, different types of nodes and edges, and we have no idea really how this coding should take place."
        ],
        [
            "So since a lot of people seem to use Pagerank variants for ranking in this sort of graph, we start from that point.",
            "So nodes have entity types, person, paper, email and company etc.",
            "And edges have relation types, load, send, cited.",
            "So on let's say each edge has a type Corti of East and that's taken from some discrete small set of types.",
            "And if edge I2J has type T, then it has a weight which is called better of tea.",
            "So now the parameterization is different, it's not distance on a chain, it's nonlinear an this weight beta.",
            "Let's do a conductance C of ITJ, which is defined according to the usual page rank kind of semantics, namely.",
            "If I'm at node I and I have a blue edge, so these colors represent types of blue edge and two red edges and red weights are three and blue edges 2.",
            "Then I go to this particular J with probability 2, / 2 + 3 + 3.",
            "This is if I'm not bored.",
            "If I'm bored with probability 1 minus Alpha, I will teleport off to an arbitrary place in the graph, so the setup is very close to standard Brennan page page rank, except that nodes have conductance.",
            "Different kind of condiments, so this is sort of the inverse problem in the sense that it's not that I'm given a conductance matrix and have to find the page rank the user by saying they like some nodes more than other nodes is setting up conditions on the final page rank.",
            "I want PETA look particular way I want to satisfy some partial orders, and now you design the C matrix for me.",
            "Nowhere that uses very few parameters, namely one parameter for every edge type."
        ],
        [
            "So to summarize, the matter of conductance, this is what the all the cases of conductance.",
            "So this is written the other way around, so I can do an easy matrix vector multiply in the right way.",
            "The conduct is from I to J is 0.",
            "If I were dead and obviously can't go anywhere.",
            "If I&J are both so called dummy node, then also it's 0.",
            "So we implement teleport through a dummy node.",
            "So in standard page rank teleport means I'm at a node.",
            "I don't want to walk over to a neighbor, so I jump uniformly at random everywhere in the graph.",
            "Not an easy way of doing that is to add a synthetic node called D, so all nodes jump there and then jump back to all nodes in the graph.",
            "So D is that node.",
            "So if.",
            "If I'm at the dummy node and I want to go to a non domino, did I do that with some probability which is guided by the so called teleport vector which is R&R J is the probability of jumping from D2J?",
            "On the other hand, if I am not at a dummy node, but I'm about to go into a dummy node or nodes that are not dead ends, I do that with probability 1 minus Alpha on nodes which I didn't have nowhere else to go.",
            "So I go there with probability one.",
            "Meanwhile, if both ions are ordinary nodes, then I go from I to J with probability, which is Alpha times the weight of the edge divided by the total outgoing width of node I.",
            "So that's my whole conduct.",
            "Instead of it's not important to look at the exact form of the expression there, just ratios of polynomials or issues of betas in various forms.",
            "So."
        ],
        [
            "Problem.",
            "That's easiest to pose is like this, so find me.",
            "A vector of betas, remember 1 beta for each.",
            "Edge type, all of which are more than one, so I can in this connection always afford to scale all the betas because the issue of probabilities will remain the same, so that actual bidders don't matter.",
            "So I want to lower bound it to one Y, because if some edge weight reduced down to zero with changing the.",
            "Topology of the graph and we might be changing its eigen properties.",
            "So we want to ensure that all edges retain their positive weight.",
            "But the issue can be arbitrarily bad and they want to penalize beta for having some model cost which will come to in a minute.",
            "Object towards subject too.",
            "So we know from standard page rank that the President vector P will be solving P = C of beta.",
            "Remember the matrix has better all over it times P, so this is a matrix vector multiplication and there's a hidden one here because there are just I can value is 1C being a stochastic matrix.",
            "And P Furthermore has to satisfy this property if I.",
            "Don't like high as much as like Jade.",
            "NPI has two less than equal to PC, so this form however is very problematic.",
            "'cause it couples CNP soapies available and so we see so we get quadratic constraints so we can't really tolerate that.",
            "Not computationally efficiently anyway.",
            "So the second question is, what is the model cost?",
            "So we might think of what's a parsimonious model.",
            "What's the default hypothesis?",
            "So one belief is that all bitter tea has to be one exactly, so you're thinking that your baseline is the standard page rank and would like to modify standard project minimally to be able to fit the users preferences in the form of a less preferred than J.",
            "The second, slightly more robust viewpoint is that the parsimonious model says all batteries are equal, but I don't know if there are one or not doesn't matter actually.",
            "So in that case we set up your model cost in this pairwise squared difference.",
            "For now, for people who are familiar with VM's, you notice that I've not put a margin here.",
            "I didn't say that Pi have to lose to PJ by a large margin.",
            "And the reason is that unlike in the case of vector space rank learning, here we cannot afford to put an arbitrary margin becausw.",
            "You know there we are basically saying that you know Beta XI has to be less than beta XJ and you can make that happen by scaling up with arbitrarily.",
            "Here, scaling up with arbitrary doesn't have that same effect.",
            "Scaling up all betas will keep the system exactly the same.",
            "So an arbitrary margin may not even be achievable no matter how much you scale better.",
            "So this is actually open to more criticism and I'll be happy to take it offline and discuss this."
        ],
        [
            "So the third thing is to break.",
            "This P equals CP recurrence and we do that by.",
            "Approximating P with this fixed number of matrix powers, so C to the power H, where H is the horizon times PO, which is uniform distribution over the nodes, and that's typically how page rank itself is estimated, and it may not be too bad.",
            "So as page time goes on in Page thank you compute the IT.",
            "Roelofse to the power HP0.",
            "As the sum over J the previous power, premultiplied went on the copy of C. OK, we just apply chain rule of differentiation to that to simultaneously compute the derivative of C to the power HP0 element.",
            "I with respect every edge type.",
            "So if this looks too dense, that's fine.",
            "We're just returning along with our page and computation the derivative of page rank with respect to every edge type and will need that in the next slide."
        ],
        [
            "So finally, what's our optimization or optimization looks like, minimize overall, better grammatical one?",
            "The penalty over betas plus some sort of loss, and in our case, as I said, Huber loss works out well.",
            "The difference between the high score in the zscore so the gradient of the last part looks simply relativity of the Hoover part and then the derivative of that with respect to.",
            "Though it's right and this we've already computed in the previous slide, so you plug all that in and now you get a standard gradient descent type algorithm.",
            "These are polynomial ratios and products of things, so it's not globally convex or anything, so we need some sort of a grid restart to make sure that we're nailing the global optimum."
        ],
        [
            "No, how do you know this thing works well?",
            "You can monitor the.",
            "The residuals of the gradients themselves and just like the residuals of Pagerank themselves decay exponentially.",
            "It's also decay, so as iterations proceed the L Infinity norm between the gradients goes down exponentially.",
            "So that's nice.",
            "That's not too surprising.",
            "Maybe you can also prove something given assumptions about the graph.",
            "And also, as H the horizon increases.",
            "Of course my CPU time goes up because I have to do more iterations.",
            "On the other hand, the gradient becomes more accurate, so our test error decreases.",
            "Welcome to test errors in a moment and also fewer nutritions are required to converge."
        ],
        [
            "So the next thing to ask is what lower optimization services look like.",
            "And remember the two kinds of surface is the true error surface and our approximation through Huber loss.",
            "So in this particular chart we created page rank score using a hidden value of Alpha overtures .7.",
            "So you jump with probability .3 and we walked to the neighborhood probability point.",
            "So now after that you change Alpha and see what kind of training error you're getting.",
            "So of course add the true value.",
            "Don't get any training at all because you're running the same random walk process and your auditing is exactly consistent.",
            "Now that router is given by the blue.",
            "Curve which is unimodal in this particular case, it goes up on both sides.",
            "Unfortunately, the Huber the hinge approximation to the true loss which is used all over in Max margin literature does not track true at all.",
            "At the lower side, whereas sure is increasing who will hinge loss actually goes down.",
            "And this is a big problem because any Newton method which is started here will actually go downwards for as we wanted to climb over that Hill.",
            "Now it turns out that.",
            "Uber is not perfect, but it's at least better than the hinge loss, so with correct design we can tune Hoover to be a little better than the hinge loss.",
            "But this is the Alpha optimization.",
            "Optimization of bidders turns out to be somewhat easier in practice, but it better understanding of the optimization surface.",
            "So here, as iterations proceed in the Newton method, the error goes down and so does Huber loss, whereas hinge rooms around a little bit."
        ],
        [
            "Alright, so the summary of the experimental setup is we create a random walk using hidden edge weights.",
            "And then we sample the partial order and then it see for learning method can recover the edge weights and the partial order on the total order.",
            "And one 20,000 node VLP graph with about 120,000 edges.",
            "We found that only about 100 pairs of training preferences are enough to cut down tester to only 11 out of 2000 and the variance goes down pretty sharply to now.",
            "One thing to be careful about is that the training and test preferences have to be no disjoint if you're not careful, you may actually just be testing the algorithm.",
            "Understand that transitivity holds.",
            "If I like a bigger than B&B would be better than C. But it is better than see that's not the only thing we're interested in learning.",
            "You're also interested in learning the edge weights.",
            "Now the other thing we tried is how robust is this tool training noise?",
            "If I take the training pairs and randomly flip some fraction of them saying you know the trainer thought he was better than be but we make better than a.",
            "Does the learning algorithm stand up to that kind of perturbation?",
            "We found that even at the point that you upset 20% of the training pairs, the tester goes up by only about five person like that.",
            "Meanwhile, the algorithm starts recognizing that the input is noisy and starts cutting down on the model cost part of it.",
            "So it says you know, changing my batteries doesn't seem to matter, therefore I'm going to make them closer to each other and my model cost is going to go down, so that's nice to see."
        ],
        [
            "OK, so this is the ultimate test of this part of the algorithm which is.",
            "You know we inject hidden because run page rank and then sample the preference and see for algorithm can estimate those and likewise we can do it with Alpha.",
            "So here is hidden, but here is estimated beta and as you can see it's sort of like in a straight line with some deviations.",
            "It is easier to inspect if you plot on the Y axis instead of estimated bit other ratio of a similar to hidden beta where you clearly see this upward pressure on the lower edge weights and a downward pressure on the high edge word.",
            "So that's kind of obvious because our prior biases to push them closer together, so the lower weights are being pulled up by the higher weights and the higher words are being pushed down by the lower words.",
            "So if you reduce the model cost, then this effect is going to reduce and similar effects are seen and that parameter is the parameter B which is often written as big C in SVM literature.",
            "That's a penalty for violations of the training set.",
            "So here we see that for low values of.",
            "Training error penalty.",
            "We fit the hidden alphas almost perfectly by the estimated Alphas, but if you are more stringent about fitting the training data with a larger value of B then the system overfits and water the Orphic mean in the context of Alpha.",
            "Remember if Alpha is very close to one then you are walking all the time in the graph.",
            "You know jumping if Alpha is close to zero you are jumping all the time to this algorithm.",
            "It's more advantageous to tune Alpha closer to 1.",
            "Because then you are walking a lot and I can use the edge weights to satisfy your rankings.",
            "If you push Alpha down towards all jumping then I have limited wiggle room right?",
            "So if I want to pay a lot of attention to their training set, then the algorithm hypes up the alphas.",
            "So that things walk more and then it can use the edges to satisfy your preferences.",
            "So overfitting in the Alpha space means the graph goes up like that."
        ],
        [
            "So in the somewhat shorter second portal, we talking about indexing.",
            "But to summarize quickly, the inner product space of ranking is very well explored.",
            "It's a very simple scoring model and still TF IDF NVM 25 took decades to evolve and stabilize.",
            "Learning those weights is very recent, however still evolving.",
            "Meanwhile we have the new class of problems which is ranking in graphs and page rank and friends are just version 0.1.",
            "They basically said the graph by four since it let's rank in this and we're saying.",
            "You know we want some sort of desirable outcome, and we want to fit weights to come up with that outcome.",
            "And since this field is much less mature, we must bootstrap or ranking wisdom via machine learning techniques."
        ],
        [
            "OK, so in the second part we will talk about indexing issues, performance issues.",
            "So type hierarchies are pretty large and deep.",
            "If you're thinking would have open domain searching over arbitrary text, you might have 10s of thousands of internal nodes in your type hierarchy order today has 18,000 internal nouns and a total of over 80,000 nouns.",
            "So runtime 8 type expansion is out of the question.",
            "If you say which scientists found theory of relativity and you want to expand scientists to all scientists known by ordnet, you'd be actually asking 650 queries, so that's completely impractical.",
            "In 60 cities and so on, there are more cities than that, so the opposite approach is to pre index all of this.",
            "So if I see the token sagun I index at the same position, 2nd and physicists and scientists person living thing and so on.",
            "But this is the problem that the index space will now block because you are seeing a lot of words at the same positions of the upper words are actually since it's from word net.",
            "So the extent of blood is clear from this table with the original corpus is about 6 gigabytes.",
            "The gzip corpus is about 1.3, and the stem indexing Lucene is only .91 gigabytes.",
            "However, if I do this upward closed indexing, I get a total index either 4.6 gigabytes, which is almost the original corpus size.",
            "So you might argue that you can't really afford it, especially if you want to store a large portion of the full of the index in RAM for fast.",
            "Query processing."
        ],
        [
            "No, our basic framework is very simple.",
            "We call it the pre generalizing post filter approach so that the full set of data types BA and that's too large to index in full.",
            "So we decide to index only registered subset R. Now let's say a query has a type A.",
            "Let's say which scientist studied wells.",
            "So they type is scientist.",
            "Or unfortunately scientists hasn't been indexed during indexing time.",
            "So we have to walk up to the nearest index data type which we call the generalization orgy.",
            "And in this case, is that happened to living thing.",
            "So now we use G to prove our index to get all occurrences of living thing.",
            "And this is a particular context.",
            "Whales were studied by Cousteau.",
            "Now the problem is that Cousteau was the true candidate.",
            "But because they generalize to Gina, whales are also a candidate.",
            "Now Douglas Adams might think that wells were studying Cousteau while Cousteau are starting the whales for the purpose of.",
            "Answering this query you would like to answer with boost or not.",
            "Well, so there are some elimination to do after you are done with this step.",
            "So we get the best Cape.",
            "User wants K answers.",
            "We get a few more answers because now they're going to be eliminated and then given an index.",
            "We got all these candidates like Wells and Cousteau, and running through each have to eliminate some of them."
        ],
        [
            "So we go to Wales and see it does well connected scientists?",
            "Well, not in the known universe, so wells are cancelled out in kusto kinda survives now.",
            "If at the end of this we are left with fewer than K hits, then we have to actually go back and restart the query with larger K prime which is very expensive.",
            "So we have some ideas on how to pick a conservative K prime, etc.",
            "So I'll.",
            "Not mention that for lack of time."
        ],
        [
            "So the important things in this setup is.",
            "How to pick an art and to pick the subset R we have to know how much index storage R will cost as compared to the whole of the unfortunate thing is, well, first of all there is an exponential number of subsets of a, But even if someone gives me a few candidates, I can't afford to kind of index all of them in turn and see how much how many bytes each of them costs, because that's going to be too much time and the 2nd is.",
            "If I have indexed all of a, that's very good for query processing.",
            "No, that's going to take a particular amount of time to answer a particular query.",
            "If I have indexed subset of a, namely R, I'm going to take more time to solve a query in general.",
            "What is going to be the load factor in query processing time and I have to find the expected bloat over a representative workload so it's a space time tradeoff and the time tradeoff has to be averaged over a workload and the space tradeoff has to be determined for all kinds of candidate hours."
        ],
        [
            "So our problem, the size problem turns out to be easy.",
            "Each token occurrence results in one posting entry in Lucene, and even if index compression is used, we can assume that over the whole corpus that accounts for a constant factor, and so we can roughly estimate that the index size taken by R is going to be the sum over all eight types.",
            "In R, the number of times our token which reaches up to R is found in the corpus.",
            "And this is surprisingly accurate.",
            "So we sampled a bunch of random ours and plotted a scatter of the index size, actually building it painfully against this number, and we found it almost straight line match.",
            "So that problem is easy."
        ],
        [
            "The second problem is a bit more tricky.",
            "We don't have a very good solution.",
            "We have an OK solution, so if R is the whole of a, then the time taken to answer the query is leucines time to actually run through those inverted lists of the corpus count of a times.",
            "The time to scan through each document already chocolates.",
            "So on the other hand, if I couldn't index all of R, then the price to pay for the generalization is that I have to now scan more posting entries.",
            "So I'm multiplying the same T scan with the corpus count of G, which is larger and after that I need to do the post filtering of K prime responses, which will be K prime time, some tea filter, anti filter and T scan can be estimated by actually running and profiling the code.",
            "So the overall blood factor then looks like T scan times, corpus count of G. The larger 1 + K prime times T filter divided by these contain the corpus count of a, so this is not too bad."
        ],
        [
            "Estimate, I mean the the scatter looks nowhere as respectable, but the important observation is this is a poor query scatter.",
            "If you average over multiple queries workload then the line.",
            "If it becomes so much better.",
            "So while observed the estimated ratio for one query is noisy, the average over many questions is much better.",
            "So use those two forms."
        ],
        [
            "And the last question to be answered is worth the workload, and this is where that long tail part comes in again.",
            "So the expected bloat over many queries is just the probability of finding a type in a query times the query bloat of a with respect to the registered set R. And if for query probability we use some sort of a maximum likelihood estimate given counts, that's never going to be good be cause a lot of areas will never be seen in the training data, and you're going to find those as in the test data.",
            "And if you haven't seen an A in the training data and you have given it a probability of zero, your index optimizer is not going to budget for.",
            "Its appearance, and therefore when the query eventually comes along, you are going to have a huge bloat in query processing time.",
            "Because this is an average, exceptions will matter.",
            "OK, so therefore good smoothing of the query."
        ],
        [
            "Ridiculous, sensual, and we're not really doing a great job of that yet, but one easy approach does reasonably well, so we do what's called lidstone smoothing, which is have an additive, smoother, sometimes used in natural language work, and we fit L. So instead of using just the part without L, we slap on a non 0 L and we fit L by calculating the probability of held out data with respect to help.",
            "OK, so here is the Redstone parameter.",
            "And here is the probability of held out test data and that reaches a distinct peak and we hold L at that point.",
            "But we still do uniforms, moving overall unseen.",
            "A types which is probably not the right thing to do."
        ],
        [
            "Anyway, so after you put all that together, you have a workload.",
            "You have a query block for every NR, so I'm going to search for R and this is actually NP hard even when the type taxonomies are tree, which was a bit surprising.",
            "No, so we have a greedy algorithm to solve it.",
            "We start with the roots of a Witcher like Sentinel nodes, and then we add the most profitable a type A star.",
            "So what's the profit?",
            "The profit is the ratio of reduction in bloat of a star and its descendants.",
            "If we add that to the increase in index space because you're also indexing yesterday, so keep sorting by that and add the next one.",
            "It's a little tricky because when you include scientists in art you reduce the bloat of physicists.",
            "And that reduces the desirability of including person, so you have to kind of proceed by downward and upward scans in the ordinate.",
            "Or a type graph efficiently.",
            "And as you do this, you get a continuous tradeoff in the index space and average query bloat, so the more index passive investor less query bloat there is.",
            "It's nice to see that there are very sharp nice so you get most of the benefit of full a very early on.",
            "And here's some interesting anecdotes about what lidstone parameter to use.",
            "If we use a little parameter, which is really, really small, it means you believe that things which you have never seen in the training that are never going to appear in the test data.",
            "But will they do so in cross validated data?",
            "You certainly find for the lowest value of lidstone some test data was really bad in the sense of not finding a generalization close enough, and that led to a huge blow up to the point that algorithm picked up that a type and then it plummets.",
            "So it's important to pick the right listen parameters.",
            "It turns out that this is kind of signal."
        ],
        [
            "That specifically in parameter which led to the largest likelihood of test data is the same list."
        ],
        [
            "Parameter that led to the lowest.",
            "Graph in this family, which is not obvious why that should happen."
        ],
        [
            "So that was for estimated as the algorithm runs estimated numbers.",
            "These are real numbers, observed index size in bytes and average and maximum load over the track query workload and the average float is with just one 4th or 1/5 of the total index size.",
            "We get an average growth of only 1.9, so you're slowing down the queries by factor of two while using index space, which is 151.",
            "Sixth of what you could have.",
            "And the maximum load of course looks really bad.",
            "There are one or two really bad queries out of 500 or 1000 trick queries which blood by a factor of 1000, but that's reality for almost any search engine.",
            "I mean, Google probably has like a timer or something, and once you run out of 1 million stopwords your queries out of the system too."
        ],
        [
            "So to summarize, we could get down from the full type index space of 4.6 gigabytes, down to about 520 megabytes, and that compares favorably with the STEM index itself is actually smaller than their STEM index, so it's less than a factor of two payment over the index space that's already invested an we need a couple of other indexes that I don't mention.",
            "Detail window reachability index to test whether whales can reach up to scientists?",
            "That's very small.",
            "And we need a forward index.",
            "To know that the world is well, I'll take that offline."
        ],
        [
            "So the summary of partners we have prototype which is built around Lucene and IBM's Umm framework.",
            "We're going to release that in open source in a couple of months.",
            "So what's the added functionality beyond just Lucy?",
            "So basically there's another pipeline which maybe I should produce."
        ],
        [
            "Miss Katie diagram here.",
            "So there's this annotator pipeline which takes a corpus and subjects two named entity recognizers and lexical network connectors.",
            "So that creates that linear chain with the graph structure but implicitly not taking as much space as it would if you allocated inode for every word in the corpus or anything like that.",
            "And then there's the first round of indexers which create the STEM index in the reachability in the forward index and maybe temporarily create the Fuller type index.",
            "Wild analysis goes on and then there is a type workload.",
            "Part so if you have a query log or you if you have some prior idea about the distribution of a types in your workload query workload, you pass that in with smooth.",
            "That suitably and from that we have workload driven subset chooser algorithm that I just told you about and in past two we compress the type index to the subset and then threw out over the full index.",
            "And after that there is the query processing part which I haven't talked about today, but the scoring function in the query processor is pluggable and currently is provided by all those in the rank SVM and the smooth loss or the graph ranking approximations and that gives you the final top care tokens or snippets as responses to your query."
        ],
        [
            "So.",
            "And we also have.",
            "Can you feel air which takes well formed questions and Maps them into a type selectors?",
            "If we say what's the distance between Paris and Rome?",
            "It will come up with an award Nets synset ID for distance as a type, and Roman Pariser selectors.",
            "So ongoing work, we're looking at indexing and searching religions other than user relationships and exploring more general notions of graph proximity soft, which I've already mentioned."
        ],
        [
            "So the conclusion is that we can only afford to perform some limited.",
            "Restructuring of the corpus.",
            "It's difficult to anticipate all query needs, and so the structured part has to be attached to an otherwise unstructured corpus, and query systems must handle this combination of a linear corpus together with semantic structure above.",
            "In one shot, we cannot insist on a specific schema, and these two parts of the structure have to coexist.",
            "Also, models of influence in graphs are getting clearer in the machine learning.",
            "But what's not clear is what is a query in this context and what is their response?",
            "Is the unit of response.",
            "One node is enough for a sponsor Twig?",
            "How to rank those things?",
            "These are not clear, ranking graphs is still underexplored.",
            "We know how to rank feature vectors, but you don't know how to rank nodes in a graph all that well.",
            "And finally scheduled scaleable indexing and top K query execution are still major challenges now some extreme special cases of those are being handled.",
            "Hertz Group has done some excellent work on that and there are groups in Microsoft who are working on top query processing in presence of text and structure, but definitely not for general graphs.",
            "So all those are morning needs.",
            "That's another talk.",
            "China's population, but maybe if you.",
            "Some short questions again.",
            "Yes.",
            "Of course, the first part is if you take into account.",
            "Swings.",
            "Provide additional information about.",
            "This is something that has been studied the language language.",
            "Because it works well for words that announce.",
            "Later just fails for hers because for verbs.",
            "The structure of the group imposes that what is important for her by the compliments that might be introduced by positions that are not necessary next liver.",
            "Basically they had invested his integration of nouns.",
            "Works very well to be visible Contacts, information verbs, just phase.",
            "Right, so there's a variety of answers to that.",
            "The first thing is first comment is a big night, but search engines have already reduced expectations.",
            "Such a lot that most people know that only noun oriented queries succeed, right?",
            "So that's one, but our non parametric or pseudo parametric form of benefiting the proximity function is actually one way to fix this a little bit, because in case it's a verb attachment then you know that the function isn't going to reach its maximum very close to zero.",
            "It's going to be further out.",
            "And if the number of such queries increases in the in the training mix, I'm going to learn that the peak is going to shift away.",
            "But the better answer to your question is that if I had the time and NLP tools would help me say run a dependency parse on the corpus ahead of time, then my unit of distance doesn't need to be number of links on the chain, so I could also learn that.",
            "So you could think of diversifying the chain graph to saying uninterpreted dumb distance link, as against.",
            "Prince Lincoln this would become two different teas in my graph ranking framework.",
            "Sing assistant people see if they don't see that.",
            "Speak the way you set up your system.",
            "Is your daddy nude?",
            "And the Alpha being at one, if I'm not mistaken, it's gonna be really useful metrics.",
            "So you're basically searching for the dominant eigenvectors of such methods.",
            "Oh so OK.",
            "So the point is.",
            "Who is this better be?",
            "One is better and speak with you and then you just people like because you said to find the determinant of 1 minus eBay.",
            "Does you know?",
            "So that matrix is typically huge.",
            "I don't want to invert it or find it settlement for one thing.",
            "So there are no problem.",
            "So the problem is that I need that I need the gradient of that inverse.",
            "It's not just the inverse that I want, I want the gradient of the inverse with respect to every better T. That's horrible, so I don't want to handle that problem."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon actually.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's better to go just before lunch or just after lunch, but hopefully we'll find some fun stuff going on here and keep awake.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about semantic search engines and.",
                    "label": 1
                },
                {
                    "sent": "In a sense, I feel insecure taking that name in vain.",
                    "label": 1
                },
                {
                    "sent": "And who knows what semantic search is in any case.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if it's first lunch and the title is too long, you know in fewer words ranking and indexing for semantic search.",
                    "label": 0
                },
                {
                    "sent": "This is with some of my students and supported by some of the search companies.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is semantic search?",
                    "label": 0
                },
                {
                    "sent": "They said I'm not quite sure what it is we are groping around trying to feel what it means and for the purpose of today's talk.",
                    "label": 0
                },
                {
                    "sent": "I would like to define semantic search as kind of search which has to combine creatively.",
                    "label": 0
                },
                {
                    "sent": "String match with the entity match.",
                    "label": 0
                },
                {
                    "sent": "So the first kind of strings we see while searching are strings with.",
                    "label": 0
                },
                {
                    "sent": "Meaning I say I want to find a flight to Hawaii in under $500 and what I'm looking for is a flight, and that's the kind of entity I want to instantiate that to a specific flight, whereas other strings can perhaps effort to remain uninterpreted, like how I might suffice to match a string without understanding what.",
                    "label": 0
                },
                {
                    "sent": "How is.",
                    "label": 0
                },
                {
                    "sent": "But flight certainly would not suffice.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I'd be talking about is the relations and some of the kinds of relations and searching and ranking would involve issues like word proximity, conductance, properties of graphs which represent the entities and relationships connections.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that in spite of relatively modest start, we can approximate many, many information needs quite well and one point I'd like to impress throughout the talk is that.",
                    "label": 0
                },
                {
                    "sent": "This is not about warehousing, so in the in the standard database warehousing literature, people have been content to first turn and unstructured corpus of text into relational tables, perhaps with approximate scores, and then subject that relational data to standard structured queries and structured mining.",
                    "label": 0
                },
                {
                    "sent": "Now my claim is this will never be enough in the context of the semantic web, because your schema is never complete.",
                    "label": 0
                },
                {
                    "sent": "The schema is evolving.",
                    "label": 0
                },
                {
                    "sent": "People haven't defined enough types yet, and definitely not enough relations.",
                    "label": 0
                },
                {
                    "sent": "And all search has to simultaneously involve some pieces of schema with other parts of unstructured data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "I have this running corpus born in New York in 1934.",
                    "label": 1
                },
                {
                    "sent": "Second was a noted astronaut, etc.",
                    "label": 0
                },
                {
                    "sent": "So in this diagram, words are connected by dotted lines to a type hierarchy, and those lines are dotted.",
                    "label": 0
                },
                {
                    "sent": "Because I'm uncertain about what to connect where.",
                    "label": 0
                },
                {
                    "sent": "For example, without knowing a lot about the context and maybe having a reasonable world model, I cannot figure out if New York is a city or a state, but observe that that may do limited damage cause those nodes converge at region close enough.",
                    "label": 0
                },
                {
                    "sent": "So maybe for many query needs, the fact that I'm.",
                    "label": 0
                },
                {
                    "sent": "Confused between city and state may not actually matter.",
                    "label": 0
                },
                {
                    "sent": "The other nodes.",
                    "label": 0
                },
                {
                    "sent": "There are edges which are in this graph, at least certain, and those are the solid edges and those are all his relationships.",
                    "label": 0
                },
                {
                    "sent": "So I know that person is an entity and scientists with the person and so on.",
                    "label": 0
                },
                {
                    "sent": "And these tokens may also have specific deterministic predicates with them, like 1934 is a sequence of 4 digits.",
                    "label": 0
                },
                {
                    "sent": "It also has a digit.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's some information extractor or query processing system which understands that 4 digits in sequence preceded by in is indicative of a year, and so I can answer questions of the form.",
                    "label": 0
                },
                {
                    "sent": "So ahead of time I just don't know how this compound structure, comprising of the linear text and the nonlinear graph is going to be quick.",
                    "label": 0
                },
                {
                    "sent": "For example, one query involving this graph could be.",
                    "label": 0
                },
                {
                    "sent": "You know name of physicists to search for intelligent life in the cosmos and we have tools which can translate things like that into soft structured query which says find me an instance of a physicist near cosmos and so on.",
                    "label": 1
                },
                {
                    "sent": "Now for that query I would like to exploit the fact that signal is an astronomical is a physicist.",
                    "label": 0
                },
                {
                    "sent": "OK, whereas some other query might, so where was second born and then second becomes the stuff you want to match the string.",
                    "label": 0
                },
                {
                    "sent": "But where should map to region and that flows down to New York and here's an example where knowing for sure whether New York is a city or state is not important.",
                    "label": 0
                },
                {
                    "sent": "You're still answer the question correctly.",
                    "label": 0
                },
                {
                    "sent": "Or someone else when was second born and then you know I need to pick on 1934 except in this case at least inside Wordnet I didn't create ahead of time any connection between the string 1934 and a year or time.",
                    "label": 0
                },
                {
                    "sent": "I found it out by some device bridging through those surface patterns, so that's sort of the delay of the land.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, the query class we address in this talk looks like this.",
                    "label": 0
                },
                {
                    "sent": "So find a token span W, such as.",
                    "label": 0
                },
                {
                    "sent": "You know New Yorkers Sagan, which appears in some context such that W is a mention of an entity East, so the string Carl ET cetera Carl Sagan is a mention of the concept of that specific physicist, so E is that entity in the physicist and is an instance of a type or the answer type given in the query.",
                    "label": 0
                },
                {
                    "sent": "So find me a physicist who studied the cosmos.",
                    "label": 0
                },
                {
                    "sent": "Which is mostly in this Doc mentioned in red.",
                    "label": 0
                },
                {
                    "sent": "And that token span W has to be near in ways that will define gradually in various ways.",
                    "label": 0
                },
                {
                    "sent": "Selector string selector strings are in green and those are the things you can kind of match without understanding what they are.",
                    "label": 0
                },
                {
                    "sent": "So search intelligent life in Cosmos in one particular query might change in a different query on the same extracted graph, so of course all of these are imprecise.",
                    "label": 0
                },
                {
                    "sent": "I can never know for sure whether token spam, WS or mention of East it could be a different 2nd.",
                    "label": 0
                },
                {
                    "sent": "I'll never know for sure if you know he's an instance of an A type in the previous chart, we saw that the solid edges told you for sure.",
                    "label": 0
                },
                {
                    "sent": "Where the Rays of type B and so on.",
                    "label": 0
                },
                {
                    "sent": "But in real life you will not know.",
                    "label": 0
                },
                {
                    "sent": "Michael Jordan may be a footballer, might be a basketball player or might be a mathematician.",
                    "label": 0
                },
                {
                    "sent": "So in this talk will assume that those links in the graph are reasonably certain and clean.",
                    "label": 0
                },
                {
                    "sent": "We will focus on the near part.",
                    "label": 0
                },
                {
                    "sent": "You know how to characterize near how to score nearness in spite of these limitations.",
                    "label": 0
                },
                {
                    "sent": "The framework is surprisingly powerful.",
                    "label": 0
                },
                {
                    "sent": "We tried fitting ranking functions.",
                    "label": 0
                },
                {
                    "sent": "For this structure of queries and we found that for the trick question answering benchmark, our system came up with the right answer.",
                    "label": 0
                },
                {
                    "sent": "Tokens in the third or fourth position most of the time.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this talk will have two parts.",
                    "label": 0
                },
                {
                    "sent": "In the first part will talk about the near scoring framework.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the literature in information retrieval, of course there are query primitives for insisting that towards appear within five tokens of each other, like span queries and so on window queries, but the distance has to be specified exactly X query and export do provide window limits like the distance at most 10 words.",
                    "label": 0
                },
                {
                    "sent": "Whether you want an ordered match with the window or you don't.",
                    "label": 0
                },
                {
                    "sent": "But this is not learned and the full text contents clause of X squared and X path does allow you to go through some sort of a mapping thesaurus to match related words or narrower terms with the level limit, but still very very.",
                    "label": 0
                },
                {
                    "sent": "They're learning free, so as the person who is asking the query of the specify, everything here is my thesaurus.",
                    "label": 0
                },
                {
                    "sent": "I'd like to map this particular word to all related words in hardware terms up to L levels deep in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Now of course other end user you don't want to do that.",
                    "label": 0
                },
                {
                    "sent": "There's no known implementation which combines this issue of narrower terms, like physicist, narrowing down to second together with the proximity information that comes on the corpus itself.",
                    "label": 0
                },
                {
                    "sent": "So to go back to the picture, there are two things involved in this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the ranking, I'd like to efficiently map from the query into a node in the type hierarchy are like to walk down to different points in the corpus such that those points are strongly activated by things matching in green.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's kind of reaching out to the answer from both sides.",
                    "label": 0
                },
                {
                    "sent": "From the tech side as well as the type taxonomy said.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the first part and becausw the ranking technology here is nowhere as mature standard information retrieval.",
                    "label": 0
                },
                {
                    "sent": "We propose a learning framework for graph proximity.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second part of the talk would be about how to index these things so you know if I want to ask, you know, find me a person near theory and relativity as match words.",
                    "label": 0
                },
                {
                    "sent": "We could in principle expand out person to every kind of person we know inside word net like physicist, politician, cricketer etc.",
                    "label": 0
                },
                {
                    "sent": "Now this is totally impractical and this would result in a large fan out at creating.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you want to pre annotate the corpus, that's complex too.",
                    "label": 0
                },
                {
                    "sent": "So in some recent work by orders union students at University of Washington, they have built a system called the binding engine, and there just annotating three or four kinds of named entities in the corpus and indexing it for quick access leads to a tenfold increase in index size, whereas our target is something like 18,000 eight types like the ordinate internal nodes today, and maybe even more later.",
                    "label": 0
                },
                {
                    "sent": "So in the second part of the talk, I'll describe a workload driven indexing query optimization that essentially depends on the extreme skew in the query workload so that only certain types are necessary to index and others are not.",
                    "label": 0
                },
                {
                    "sent": "But you have to be careful about that thanks to that long tail.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Business.",
                    "label": 0
                },
                {
                    "sent": "So here's the first part.",
                    "label": 0
                },
                {
                    "sent": "Scoring and ranking nodes in graphs.",
                    "label": 0
                },
                {
                    "sent": "Feel free to interrupt and ask questions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So discuss 2 flavors of ranking problems or related.",
                    "label": 0
                },
                {
                    "sent": "One is a sort of generalization of the other.",
                    "label": 0
                },
                {
                    "sent": "In the first part we talk about the query class which we just discussed, which is fine.",
                    "label": 0
                },
                {
                    "sent": "Me objects of a specified type near match keywords and type membership is perfect.",
                    "label": 0
                },
                {
                    "sent": "OK, we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't question that.",
                    "label": 0
                },
                {
                    "sent": "Nearest captured through two kinds of things.",
                    "label": 0
                },
                {
                    "sent": "Token rareness if I say find scientists near pneumonia, the pneumonia is a much better word than find scientists near experiment.",
                    "label": 0
                },
                {
                    "sent": "So rareness and distance between those match tokens and the candidate token, or token spam.",
                    "label": 0
                },
                {
                    "sent": "The second kind of ranking is a bit more involved technically.",
                    "label": 0
                },
                {
                    "sent": "Here we have a general graph with type edges and nodes with text, and we think of doing random walks on this graph as like an approximation to page rank.",
                    "label": 0
                },
                {
                    "sent": "So here is a node which represents a company.",
                    "label": 0
                },
                {
                    "sent": "These people work there, so works for edges, and these people receive and send emails.",
                    "label": 0
                },
                {
                    "sent": "They write papers, papers are cited by other papers, and emails are sent in responses to emails and so on.",
                    "label": 0
                },
                {
                    "sent": "Now if I want to do some kind of semantic search on this sort of graph.",
                    "label": 0
                },
                {
                    "sent": "I might say find me a person who is near IBM and XML right?",
                    "label": 0
                },
                {
                    "sent": "And to answer those kinds of questions, one usually there have been at least three or four papers which adapt Pagerank like scoring to this graph by saying OK, maybe works for Link has a certain amount of conductivity which is different from person sending email and with those different edge weights I'm going to run a different sort of page rank.",
                    "label": 0
                },
                {
                    "sent": "And then rank the nodes in order of that page rank.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that you know to our knowledge no one has been thinking of learning those edge weights based on relevance feedback the hardware and plugging those numbers and then the ranking comes out and they say OK, we feel pretty happy with it.",
                    "label": 0
                },
                {
                    "sent": "So one question is how to take the relevance feedback literature from standard information retrieval into a regime where things are not vector space documents but nodes are entities that nodes in a graph and they're being ranked.",
                    "label": 0
                },
                {
                    "sent": "So we'll see that in the one be part of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you have the first somewhat easier setup for ranking token spots.",
                    "label": 0
                },
                {
                    "sent": "So let's say my question is, who was the inventor of television?",
                    "label": 0
                },
                {
                    "sent": "Or who invented television?",
                    "label": 0
                },
                {
                    "sent": "And because it's a who question some device which we don't talk about here would map the question to person and every token in the corpus which connects through two person is a candidate.",
                    "label": 0
                },
                {
                    "sent": "So John Baird is a candidate to Comspan, which you have to score.",
                    "label": 0
                },
                {
                    "sent": "Now, in this case the stem invent appears twice close to that.",
                    "label": 0
                },
                {
                    "sent": "So think of the corpus as being a big chain graph with some nodes being connected to that type taxonomy.",
                    "label": 0
                },
                {
                    "sent": "And the candidate appears at offset zero and then there are nodes of the left and right of it.",
                    "label": 0
                },
                {
                    "sent": "So the first occurrence of inventor is at position minus one.",
                    "label": 0
                },
                {
                    "sent": "The further occurrences of minus four and television, which turns out to be a rare term is appearing at minus six, and it was roughly think of the situation as maybe every matched green selector sends out some form of energy activation spreading style, and if the word is rare, it has a lot of energy.",
                    "label": 0
                },
                {
                    "sent": "Forward is more frequently invent.",
                    "label": 0
                },
                {
                    "sent": "It does somewhat lower amounts of energy, and those are radiated out and a certain amount of that energy reaches the candidate.",
                    "label": 0
                },
                {
                    "sent": "I could think of maybe adding up those energies OK, but it's probably a bad idea to add up the energy for invent twice, because in that case very common word appearing multiple times near your candidate is going to boost up your score, so that's not typically a very good thing to do in our work.",
                    "label": 0
                },
                {
                    "sent": "We have assumed that we take the Max over the same selector, and we add up over different selectors, so we take only the closest talker and serve them at selector, and then we had to work the distinct select.",
                    "label": 0
                },
                {
                    "sent": "So the considerations in summer here at the following.",
                    "label": 0
                },
                {
                    "sent": "So this idea for selected is rare.",
                    "label": 0
                },
                {
                    "sent": "It should have more energy.",
                    "label": 0
                },
                {
                    "sent": "The distance from the selector to the match position of course matters, and you might roughly think that the bigger the distance, the smaller the influence, and that's broadly true, but it's not true at very small distance, and we shall see an interesting example of that when we learn it.",
                    "label": 0
                },
                {
                    "sent": "We didn't expect it ahead of time, and maybe if you're a linguist, you understand why already, but we didn't.",
                    "label": 0
                },
                {
                    "sent": "We didn't know ahead of time.",
                    "label": 0
                },
                {
                    "sent": "So we want to actually learn the form of this function.",
                    "label": 0
                },
                {
                    "sent": "We don't want to guess that there's been some XML scoring literature where people assumed an exponential decay.",
                    "label": 0
                },
                {
                    "sent": "I don't want to assume that it turned out to be a good idea not to assume that.",
                    "label": 0
                },
                {
                    "sent": "And if there are many occurrences of 1 selector, we should take the closest one.",
                    "label": 0
                },
                {
                    "sent": "In particular, that's been a good choice.",
                    "label": 0
                },
                {
                    "sent": "And we generally add over the different selectors, although it could do other things too.",
                    "label": 0
                },
                {
                    "sent": "Anything would probably work fine.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So rather than guessing parametric form for the decay function, will just fit explicitly make bins for position minus 32 + 30 and 50 number for each.",
                    "label": 0
                },
                {
                    "sent": "So for the moment, suppose we assume that according to the left and occurs to the right are the same, so we collapse them on one side and so we have this W parameters to estimate.",
                    "label": 0
                },
                {
                    "sent": "The horizontal axis was that some kind of.",
                    "label": 0
                },
                {
                    "sent": "Representation of your index.",
                    "label": 0
                },
                {
                    "sent": "Some kind of static?",
                    "label": 0
                },
                {
                    "sent": "And is this a fixed.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With one node allocated for each word is just a linear text.",
                    "label": 0
                },
                {
                    "sent": "Yes, each document descriptor from another document, but each document is a linear sequence of words.",
                    "label": 0
                },
                {
                    "sent": "So this is just a just imports in running text.",
                    "label": 0
                },
                {
                    "sent": "Television was invented in 1925.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "For each document in apartment, yes.",
                    "label": 0
                },
                {
                    "sent": "So we have one node for every word in the corpus, so of course we don't want to represent them explicitly.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we believe that left and right are the same and there's no special significance direction, and let's say that we know that beyond uppercase, W words is way too far for you to capture any more dependencies than there, this W parameters would like to.",
                    "label": 0
                },
                {
                    "sent": "Estimate so every candidate position is characterized by feature vector, which can create out of its neighborhood if there is a math selector distance J and this is the closest occurrence of that selector, then we set the feature at J to be the energy of.",
                    "label": 0
                },
                {
                    "sent": "That's electric now, of course, you could have one selector at position plus five and another selector at minus five.",
                    "label": 0
                },
                {
                    "sent": "There's some special cases you might want to add them up, and so on, but broadly speaking, you have the total amount of activation energy at various distances, and we think of it as sort of a multiplicative form.",
                    "label": 0
                },
                {
                    "sent": "So if the decay function is characterized by this vector beta, then your activation at the node is basically the inner product over beta and F. OK, because if the selector matches at position J, it's scaled by weight beta J and then added on to the score of the candidates.",
                    "label": 0
                },
                {
                    "sent": "So it's a standard inner product space an in that we're trying to express preferences of the form.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have some ground truth and I tell you that here are 10 answer context in here.",
                    "label": 0
                },
                {
                    "sent": "100 non answer context.",
                    "label": 0
                },
                {
                    "sent": "Then sampling from that we can get examples like I like candidate you less than I like Kennedy.",
                    "label": 0
                },
                {
                    "sent": "You is relevant.",
                    "label": 0
                },
                {
                    "sent": "It's an answer to my question, whereas V is not.",
                    "label": 0
                },
                {
                    "sent": "So if I say that you is less preferable to V, what I mean is that.",
                    "label": 0
                },
                {
                    "sent": "Later Dot the feature vector of you should be less than or equal to.",
                    "label": 0
                },
                {
                    "sent": "Better the feature vector of so this framework has been known in machine learning for awhile, and one of the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those mechanisms for estimating beta is the so-called rank SVM algorithm from Thurston Hawkins.",
                    "label": 0
                },
                {
                    "sent": "And others, so the idea is to search for that better vector.",
                    "label": 0
                },
                {
                    "sent": "In some part of the DII don't want to have the components of bitter blow up to Infinity or something, so I put regularizer like that which controls there to norm of data subject to.",
                    "label": 0
                },
                {
                    "sent": "Bitter transporter dot FI minus better transpose FJ has to be less than equal to minus one, so earlier saying it has to be less than equal to 0.",
                    "label": 0
                },
                {
                    "sent": "But now we're going to be stricter about it.",
                    "label": 0
                },
                {
                    "sent": "That's the Max margin kind of framework, and for every I that I prefer less than J, I'm going to put up a constant of this phone.",
                    "label": 0
                },
                {
                    "sent": "Free.",
                    "label": 0
                },
                {
                    "sent": "So the soft margin form looks like this.",
                    "label": 0
                },
                {
                    "sent": "This is standard transformation from standard hard margin, SVM's too soft margin, SVM's, the better.",
                    "label": 0
                },
                {
                    "sent": "See men as before, except I'll include some slack variables and I'm going to forgive violations up to the slack variable.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to charge a penalty for the slacks.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I want if I to be defeated by FJ after being given a benefit of SIG.",
                    "label": 0
                },
                {
                    "sent": "And if I have to give a lot of benefit, I want to penalize the objective.",
                    "label": 0
                },
                {
                    "sent": "So if you eliminate the SS, you can actually write that equation entirely in terms of the betas.",
                    "label": 0
                },
                {
                    "sent": "The objective looks sort of the same up to this point.",
                    "label": 0
                },
                {
                    "sent": "After this idea is replaced by the Max of zero and one plus Beta Phi minus bitter.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the classic hinge loss and it turns out that if you insist on the hinge loss, then this is a quadratic program, which takes a long time to solve.",
                    "label": 0
                },
                {
                    "sent": "But there's good news this August, there's a paper which talks about training a linear kernel SVM in linear time, so that's improving all the time.",
                    "label": 0
                },
                {
                    "sent": "But in practice we have found that it's probably best to replace this hinge loss with a sharp corner.",
                    "label": 0
                },
                {
                    "sent": "With a smoother function and then optimize the resulting objective function.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what kind of loss functions can we use so the hinge loss in the previous chart was this blue line and there's a host of others.",
                    "label": 0
                },
                {
                    "sent": "Maybe all of your conversant with all the learning literature, but one commonly used approximation to the hinge loss is a so called log X plus or the soft hinge loss, which is just one log of 1 plus into the power X which asymptotes 20 as well as 2 equal to X on both sides in between.",
                    "label": 0
                },
                {
                    "sent": "It's a bit of a nuisance.",
                    "label": 0
                },
                {
                    "sent": "You can even use an exponential loss if you are not too unhappy with the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Do the Huber loss, which is 0 up to zero, then it's a parabolic Y equal to constant times X squared up to a certain point and after that it's parallel to or equal to X.",
                    "label": 0
                },
                {
                    "sent": "So these are all approximations to the true loss, which is basically that you get a point whenever you rank inj correctly with respect to each other and you lose a point if you don't.",
                    "label": 0
                },
                {
                    "sent": "Let's see how it does so we can try all kinds of smooth losses.",
                    "label": 0
                },
                {
                    "sent": "I've tried the smooth hinge as well as Huber and expand for this particular problem.",
                    "label": 0
                },
                {
                    "sent": "All of them work reasonably well, but for the next problem that I would describe, only Huber loss seems to work reasonably well.",
                    "label": 0
                },
                {
                    "sent": "The other thing to note is that absolute values in beta don't mean anything becausw.",
                    "label": 0
                },
                {
                    "sent": "My ranking function is better.",
                    "label": 0
                },
                {
                    "sent": "Dot FI can shift and scale better, however I want.",
                    "label": 0
                },
                {
                    "sent": "And the other prior belief about this setup is suppose here in our position zero.",
                    "label": 0
                },
                {
                    "sent": "I have my candidate token and a selector or a match happens at position 29 versus 30.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't really believe that 29 and 30th at different, so there has to be some smoothness about beta, right?",
                    "label": 0
                },
                {
                    "sent": "So we can also ensure that by setting beta of W plus 120 arbitrarily.",
                    "label": 0
                },
                {
                    "sent": "Because remember fixed offsets don't matter and then I can try to penalize Addison Betas from.",
                    "label": 0
                },
                {
                    "sent": "Differing too much.",
                    "label": 0
                },
                {
                    "sent": "We could also force monotonic decrease in beta, but that turns out to be a bad idea.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally, our model complexity may be written down as some over J. Cole to want to W better J minus Peter J plus one whole squared, which encourages artisan builders to be close together.",
                    "label": 0
                },
                {
                    "sent": "And then plus the loss function, which is an approximation to my training error.",
                    "label": 0
                },
                {
                    "sent": "Which we discussed some examples of.",
                    "label": 0
                },
                {
                    "sent": "So when we do this, we find that beta J plotted against Jay smooth suitably.",
                    "label": 0
                },
                {
                    "sent": "This is the best cross validation point.",
                    "label": 0
                },
                {
                    "sent": "Looks like this, so there is a very steady growth up to about four or five.",
                    "label": 0
                },
                {
                    "sent": "And then there is a steep decay which is sort of exponential.",
                    "label": 0
                },
                {
                    "sent": "OK, down to some noise level up to about 50 tokens.",
                    "label": 0
                },
                {
                    "sent": "So why is that so?",
                    "label": 0
                },
                {
                    "sent": "Then we started inspecting the actual track document contexts where this match was taking place, and it turns out that, at least for that benchmark, and probably Even so, you know if we wanted to do named entity type searches on the web, most of the time your target type is a named entity.",
                    "label": 0
                },
                {
                    "sent": "Which city are, you know, where was, or when was so on, and the context by which you qualify your question also has proper nouns and things like that.",
                    "label": 0
                },
                {
                    "sent": "Mount named entities and named entities are almost always separated by action words and prepositions and articles and so on, and that accounts for that gap of three to four.",
                    "label": 0
                },
                {
                    "sent": "You have to have some things in between to be able to connect up those two named entities and that accounts for the non monotonic form of the plot.",
                    "label": 0
                },
                {
                    "sent": "So of course, if we thought about the linguistic aspect of this, we might have caught that ahead of time.",
                    "label": 0
                },
                {
                    "sent": "But this was interesting to see coming straight out of just a list of Trek published positive and negative results for each question, we plug that into the framework and we crank the handle, and this graph comes out.",
                    "label": 0
                },
                {
                    "sent": "So that's fairly nice.",
                    "label": 0
                },
                {
                    "sent": "So this is the first kind of ranking.",
                    "label": 0
                },
                {
                    "sent": "If we then take this ranking function and we plug it into ranking engine.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we took leucines indexing token offset framework.",
                    "label": 0
                },
                {
                    "sent": "But we wrote our own scoring function on top of it and when you do that.",
                    "label": 0
                },
                {
                    "sent": "The mean reciprocal rank of the answer, which is the average overall queries the reciprocal of the first rank, where an answer is found.",
                    "label": 1
                },
                {
                    "sent": "So higher the better.",
                    "label": 0
                },
                {
                    "sent": "Improves from .16 two .29 so point on a matter of .9 means that on average typically you're finding your answer between rank three and four.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's the story up to this point.",
                    "label": 0
                },
                {
                    "sent": "This works if your graph is basically a chain graph, and you can characterize the parameter space is a vector.",
                    "label": 0
                },
                {
                    "sent": "The distance between your candidate token and the match position.",
                    "label": 0
                },
                {
                    "sent": "But what if your candidate in the match are nodes in a general graph?",
                    "label": 0
                },
                {
                    "sent": "So that's the second part.",
                    "label": 0
                },
                {
                    "sent": "Any questions on this so far, yes.",
                    "label": 0
                },
                {
                    "sent": "How stable is your?",
                    "label": 0
                },
                {
                    "sent": "Not parametric estimate for the values of and then.",
                    "label": 0
                },
                {
                    "sent": "How about using parametric distribution like a plus song which would kind of have this sure?",
                    "label": 0
                },
                {
                    "sent": "It's fairly stable, so we computed this graph for about three or four years of track data and they all looked essentially the same.",
                    "label": 0
                },
                {
                    "sent": "Not so now that we know what it looks like.",
                    "label": 0
                },
                {
                    "sent": "Sure, we should feel free to try parametric forms for this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the second part of scoring the story is that you know we have this really jumbled network and have this project called searching Personal information networks of Spin where there are all these adapters for email and papers and so on which emit this tiny graph.",
                    "label": 0
                },
                {
                    "sent": "Let's so each adapter understands a small fragment or twigs which says, you know paper has citation and author and affiliation or an email is has a body to CC and from.",
                    "label": 0
                },
                {
                    "sent": "And then there is either a registry of sorts or some learning technology to map it to what we call the PIN schema.",
                    "label": 0
                },
                {
                    "sent": "So two and CC and author all of them map to person.",
                    "label": 0
                },
                {
                    "sent": "And maybe I have OK.",
                    "label": 0
                },
                {
                    "sent": "So maybe paper and email are differentiated, in which case papers and citations both going to paper and email goes to email obviously and authors affiliation goes to a company or organization.",
                    "label": 0
                },
                {
                    "sent": "So this is a schema matching thing, and once that's done, any adapter can inject graph data into what we call the PIN review, which is an instance of the PIN schemer.",
                    "label": 0
                },
                {
                    "sent": "So the pin to be adheres to the spin schema, so it's a little trickier though, because different.",
                    "label": 0
                },
                {
                    "sent": "Others may be importing dirty data or redundant data, and we have another registry of reconcilers which can also inject information of the person of the form that these two person nodes are actually the same person, but I'm not quite sure.",
                    "label": 0
                },
                {
                    "sent": "I think I'm about .9 shared that these two nodes are the same, or that this particular email mentions this person, which is an even more unsure position.",
                    "label": 0
                },
                {
                    "sent": "So anyway, at this point we have a graph like this and would like to ask queries of the form.",
                    "label": 0
                },
                {
                    "sent": "Find me a person who knows a lot about XML.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do I do ranking for that kind of query on this graph?",
                    "label": 0
                },
                {
                    "sent": "And so the database community on the community have been very active.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In designing systems of this form and.",
                    "label": 0
                },
                {
                    "sent": "That's both kind of demonstrated by way of the system we are building, as well as this particular query.",
                    "label": 0
                },
                {
                    "sent": "So if you said type equal to person near paper equal to XML, an index.",
                    "label": 0
                },
                {
                    "sent": "This is the sort of screen viewed getting our system, and in this particular listing get hardware comes at the top of the list, and if you click on that node you're going to see get hard in the center of the graph, but you also want to explain why get hard is thus favored by the query.",
                    "label": 0
                },
                {
                    "sent": "So what's going to happen is our system is also going to bring up a local context around Gerhard which is transferring the biggest amount of page rank into their heart from matched nodes.",
                    "label": 0
                },
                {
                    "sent": "For example, here's a.",
                    "label": 0
                },
                {
                    "sent": "Paper which talks about an index based XL search engine for querying XML data so that kind of explains why your heart is hard, erect, and you know I'll try live demonstration at the end of the talk you can kind of browse out from here and try to explain why and how those are connected to other Members in the list and so on.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what's the ranking model here?",
                    "label": 0
                },
                {
                    "sent": "And today most of the database or IR literature in this area has basically said look, we think this is a good ranking function Now this of course you know in the.",
                    "label": 0
                },
                {
                    "sent": "They are decades when people design TF IDF NVM 25 that was validated through years and decades of testing and experience in this domain.",
                    "label": 0
                },
                {
                    "sent": "It's a more complex domain there.",
                    "label": 0
                },
                {
                    "sent": "This nonlinear artifact, the graph, different types of nodes and edges, and we have no idea really how this coding should take place.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So since a lot of people seem to use Pagerank variants for ranking in this sort of graph, we start from that point.",
                    "label": 0
                },
                {
                    "sent": "So nodes have entity types, person, paper, email and company etc.",
                    "label": 0
                },
                {
                    "sent": "And edges have relation types, load, send, cited.",
                    "label": 0
                },
                {
                    "sent": "So on let's say each edge has a type Corti of East and that's taken from some discrete small set of types.",
                    "label": 0
                },
                {
                    "sent": "And if edge I2J has type T, then it has a weight which is called better of tea.",
                    "label": 0
                },
                {
                    "sent": "So now the parameterization is different, it's not distance on a chain, it's nonlinear an this weight beta.",
                    "label": 0
                },
                {
                    "sent": "Let's do a conductance C of ITJ, which is defined according to the usual page rank kind of semantics, namely.",
                    "label": 0
                },
                {
                    "sent": "If I'm at node I and I have a blue edge, so these colors represent types of blue edge and two red edges and red weights are three and blue edges 2.",
                    "label": 0
                },
                {
                    "sent": "Then I go to this particular J with probability 2, / 2 + 3 + 3.",
                    "label": 0
                },
                {
                    "sent": "This is if I'm not bored.",
                    "label": 0
                },
                {
                    "sent": "If I'm bored with probability 1 minus Alpha, I will teleport off to an arbitrary place in the graph, so the setup is very close to standard Brennan page page rank, except that nodes have conductance.",
                    "label": 0
                },
                {
                    "sent": "Different kind of condiments, so this is sort of the inverse problem in the sense that it's not that I'm given a conductance matrix and have to find the page rank the user by saying they like some nodes more than other nodes is setting up conditions on the final page rank.",
                    "label": 0
                },
                {
                    "sent": "I want PETA look particular way I want to satisfy some partial orders, and now you design the C matrix for me.",
                    "label": 0
                },
                {
                    "sent": "Nowhere that uses very few parameters, namely one parameter for every edge type.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to summarize, the matter of conductance, this is what the all the cases of conductance.",
                    "label": 0
                },
                {
                    "sent": "So this is written the other way around, so I can do an easy matrix vector multiply in the right way.",
                    "label": 0
                },
                {
                    "sent": "The conduct is from I to J is 0.",
                    "label": 0
                },
                {
                    "sent": "If I were dead and obviously can't go anywhere.",
                    "label": 0
                },
                {
                    "sent": "If I&J are both so called dummy node, then also it's 0.",
                    "label": 0
                },
                {
                    "sent": "So we implement teleport through a dummy node.",
                    "label": 0
                },
                {
                    "sent": "So in standard page rank teleport means I'm at a node.",
                    "label": 0
                },
                {
                    "sent": "I don't want to walk over to a neighbor, so I jump uniformly at random everywhere in the graph.",
                    "label": 0
                },
                {
                    "sent": "Not an easy way of doing that is to add a synthetic node called D, so all nodes jump there and then jump back to all nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "So D is that node.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "If I'm at the dummy node and I want to go to a non domino, did I do that with some probability which is guided by the so called teleport vector which is R&R J is the probability of jumping from D2J?",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if I am not at a dummy node, but I'm about to go into a dummy node or nodes that are not dead ends, I do that with probability 1 minus Alpha on nodes which I didn't have nowhere else to go.",
                    "label": 0
                },
                {
                    "sent": "So I go there with probability one.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile, if both ions are ordinary nodes, then I go from I to J with probability, which is Alpha times the weight of the edge divided by the total outgoing width of node I.",
                    "label": 0
                },
                {
                    "sent": "So that's my whole conduct.",
                    "label": 0
                },
                {
                    "sent": "Instead of it's not important to look at the exact form of the expression there, just ratios of polynomials or issues of betas in various forms.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "That's easiest to pose is like this, so find me.",
                    "label": 0
                },
                {
                    "sent": "A vector of betas, remember 1 beta for each.",
                    "label": 0
                },
                {
                    "sent": "Edge type, all of which are more than one, so I can in this connection always afford to scale all the betas because the issue of probabilities will remain the same, so that actual bidders don't matter.",
                    "label": 0
                },
                {
                    "sent": "So I want to lower bound it to one Y, because if some edge weight reduced down to zero with changing the.",
                    "label": 0
                },
                {
                    "sent": "Topology of the graph and we might be changing its eigen properties.",
                    "label": 0
                },
                {
                    "sent": "So we want to ensure that all edges retain their positive weight.",
                    "label": 0
                },
                {
                    "sent": "But the issue can be arbitrarily bad and they want to penalize beta for having some model cost which will come to in a minute.",
                    "label": 0
                },
                {
                    "sent": "Object towards subject too.",
                    "label": 0
                },
                {
                    "sent": "So we know from standard page rank that the President vector P will be solving P = C of beta.",
                    "label": 0
                },
                {
                    "sent": "Remember the matrix has better all over it times P, so this is a matrix vector multiplication and there's a hidden one here because there are just I can value is 1C being a stochastic matrix.",
                    "label": 0
                },
                {
                    "sent": "And P Furthermore has to satisfy this property if I.",
                    "label": 0
                },
                {
                    "sent": "Don't like high as much as like Jade.",
                    "label": 0
                },
                {
                    "sent": "NPI has two less than equal to PC, so this form however is very problematic.",
                    "label": 0
                },
                {
                    "sent": "'cause it couples CNP soapies available and so we see so we get quadratic constraints so we can't really tolerate that.",
                    "label": 0
                },
                {
                    "sent": "Not computationally efficiently anyway.",
                    "label": 0
                },
                {
                    "sent": "So the second question is, what is the model cost?",
                    "label": 0
                },
                {
                    "sent": "So we might think of what's a parsimonious model.",
                    "label": 0
                },
                {
                    "sent": "What's the default hypothesis?",
                    "label": 0
                },
                {
                    "sent": "So one belief is that all bitter tea has to be one exactly, so you're thinking that your baseline is the standard page rank and would like to modify standard project minimally to be able to fit the users preferences in the form of a less preferred than J.",
                    "label": 0
                },
                {
                    "sent": "The second, slightly more robust viewpoint is that the parsimonious model says all batteries are equal, but I don't know if there are one or not doesn't matter actually.",
                    "label": 0
                },
                {
                    "sent": "So in that case we set up your model cost in this pairwise squared difference.",
                    "label": 0
                },
                {
                    "sent": "For now, for people who are familiar with VM's, you notice that I've not put a margin here.",
                    "label": 0
                },
                {
                    "sent": "I didn't say that Pi have to lose to PJ by a large margin.",
                    "label": 0
                },
                {
                    "sent": "And the reason is that unlike in the case of vector space rank learning, here we cannot afford to put an arbitrary margin becausw.",
                    "label": 0
                },
                {
                    "sent": "You know there we are basically saying that you know Beta XI has to be less than beta XJ and you can make that happen by scaling up with arbitrarily.",
                    "label": 0
                },
                {
                    "sent": "Here, scaling up with arbitrary doesn't have that same effect.",
                    "label": 0
                },
                {
                    "sent": "Scaling up all betas will keep the system exactly the same.",
                    "label": 0
                },
                {
                    "sent": "So an arbitrary margin may not even be achievable no matter how much you scale better.",
                    "label": 0
                },
                {
                    "sent": "So this is actually open to more criticism and I'll be happy to take it offline and discuss this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the third thing is to break.",
                    "label": 0
                },
                {
                    "sent": "This P equals CP recurrence and we do that by.",
                    "label": 0
                },
                {
                    "sent": "Approximating P with this fixed number of matrix powers, so C to the power H, where H is the horizon times PO, which is uniform distribution over the nodes, and that's typically how page rank itself is estimated, and it may not be too bad.",
                    "label": 0
                },
                {
                    "sent": "So as page time goes on in Page thank you compute the IT.",
                    "label": 0
                },
                {
                    "sent": "Roelofse to the power HP0.",
                    "label": 0
                },
                {
                    "sent": "As the sum over J the previous power, premultiplied went on the copy of C. OK, we just apply chain rule of differentiation to that to simultaneously compute the derivative of C to the power HP0 element.",
                    "label": 0
                },
                {
                    "sent": "I with respect every edge type.",
                    "label": 0
                },
                {
                    "sent": "So if this looks too dense, that's fine.",
                    "label": 0
                },
                {
                    "sent": "We're just returning along with our page and computation the derivative of page rank with respect to every edge type and will need that in the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally, what's our optimization or optimization looks like, minimize overall, better grammatical one?",
                    "label": 0
                },
                {
                    "sent": "The penalty over betas plus some sort of loss, and in our case, as I said, Huber loss works out well.",
                    "label": 0
                },
                {
                    "sent": "The difference between the high score in the zscore so the gradient of the last part looks simply relativity of the Hoover part and then the derivative of that with respect to.",
                    "label": 0
                },
                {
                    "sent": "Though it's right and this we've already computed in the previous slide, so you plug all that in and now you get a standard gradient descent type algorithm.",
                    "label": 0
                },
                {
                    "sent": "These are polynomial ratios and products of things, so it's not globally convex or anything, so we need some sort of a grid restart to make sure that we're nailing the global optimum.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, how do you know this thing works well?",
                    "label": 0
                },
                {
                    "sent": "You can monitor the.",
                    "label": 0
                },
                {
                    "sent": "The residuals of the gradients themselves and just like the residuals of Pagerank themselves decay exponentially.",
                    "label": 0
                },
                {
                    "sent": "It's also decay, so as iterations proceed the L Infinity norm between the gradients goes down exponentially.",
                    "label": 0
                },
                {
                    "sent": "So that's nice.",
                    "label": 0
                },
                {
                    "sent": "That's not too surprising.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can also prove something given assumptions about the graph.",
                    "label": 0
                },
                {
                    "sent": "And also, as H the horizon increases.",
                    "label": 0
                },
                {
                    "sent": "Of course my CPU time goes up because I have to do more iterations.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the gradient becomes more accurate, so our test error decreases.",
                    "label": 0
                },
                {
                    "sent": "Welcome to test errors in a moment and also fewer nutritions are required to converge.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next thing to ask is what lower optimization services look like.",
                    "label": 0
                },
                {
                    "sent": "And remember the two kinds of surface is the true error surface and our approximation through Huber loss.",
                    "label": 0
                },
                {
                    "sent": "So in this particular chart we created page rank score using a hidden value of Alpha overtures .7.",
                    "label": 0
                },
                {
                    "sent": "So you jump with probability .3 and we walked to the neighborhood probability point.",
                    "label": 0
                },
                {
                    "sent": "So now after that you change Alpha and see what kind of training error you're getting.",
                    "label": 0
                },
                {
                    "sent": "So of course add the true value.",
                    "label": 0
                },
                {
                    "sent": "Don't get any training at all because you're running the same random walk process and your auditing is exactly consistent.",
                    "label": 0
                },
                {
                    "sent": "Now that router is given by the blue.",
                    "label": 0
                },
                {
                    "sent": "Curve which is unimodal in this particular case, it goes up on both sides.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the Huber the hinge approximation to the true loss which is used all over in Max margin literature does not track true at all.",
                    "label": 0
                },
                {
                    "sent": "At the lower side, whereas sure is increasing who will hinge loss actually goes down.",
                    "label": 0
                },
                {
                    "sent": "And this is a big problem because any Newton method which is started here will actually go downwards for as we wanted to climb over that Hill.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that.",
                    "label": 0
                },
                {
                    "sent": "Uber is not perfect, but it's at least better than the hinge loss, so with correct design we can tune Hoover to be a little better than the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "But this is the Alpha optimization.",
                    "label": 0
                },
                {
                    "sent": "Optimization of bidders turns out to be somewhat easier in practice, but it better understanding of the optimization surface.",
                    "label": 0
                },
                {
                    "sent": "So here, as iterations proceed in the Newton method, the error goes down and so does Huber loss, whereas hinge rooms around a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so the summary of the experimental setup is we create a random walk using hidden edge weights.",
                    "label": 0
                },
                {
                    "sent": "And then we sample the partial order and then it see for learning method can recover the edge weights and the partial order on the total order.",
                    "label": 0
                },
                {
                    "sent": "And one 20,000 node VLP graph with about 120,000 edges.",
                    "label": 0
                },
                {
                    "sent": "We found that only about 100 pairs of training preferences are enough to cut down tester to only 11 out of 2000 and the variance goes down pretty sharply to now.",
                    "label": 0
                },
                {
                    "sent": "One thing to be careful about is that the training and test preferences have to be no disjoint if you're not careful, you may actually just be testing the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Understand that transitivity holds.",
                    "label": 0
                },
                {
                    "sent": "If I like a bigger than B&B would be better than C. But it is better than see that's not the only thing we're interested in learning.",
                    "label": 0
                },
                {
                    "sent": "You're also interested in learning the edge weights.",
                    "label": 0
                },
                {
                    "sent": "Now the other thing we tried is how robust is this tool training noise?",
                    "label": 0
                },
                {
                    "sent": "If I take the training pairs and randomly flip some fraction of them saying you know the trainer thought he was better than be but we make better than a.",
                    "label": 0
                },
                {
                    "sent": "Does the learning algorithm stand up to that kind of perturbation?",
                    "label": 0
                },
                {
                    "sent": "We found that even at the point that you upset 20% of the training pairs, the tester goes up by only about five person like that.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile, the algorithm starts recognizing that the input is noisy and starts cutting down on the model cost part of it.",
                    "label": 0
                },
                {
                    "sent": "So it says you know, changing my batteries doesn't seem to matter, therefore I'm going to make them closer to each other and my model cost is going to go down, so that's nice to see.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the ultimate test of this part of the algorithm which is.",
                    "label": 0
                },
                {
                    "sent": "You know we inject hidden because run page rank and then sample the preference and see for algorithm can estimate those and likewise we can do it with Alpha.",
                    "label": 1
                },
                {
                    "sent": "So here is hidden, but here is estimated beta and as you can see it's sort of like in a straight line with some deviations.",
                    "label": 1
                },
                {
                    "sent": "It is easier to inspect if you plot on the Y axis instead of estimated bit other ratio of a similar to hidden beta where you clearly see this upward pressure on the lower edge weights and a downward pressure on the high edge word.",
                    "label": 1
                },
                {
                    "sent": "So that's kind of obvious because our prior biases to push them closer together, so the lower weights are being pulled up by the higher weights and the higher words are being pushed down by the lower words.",
                    "label": 0
                },
                {
                    "sent": "So if you reduce the model cost, then this effect is going to reduce and similar effects are seen and that parameter is the parameter B which is often written as big C in SVM literature.",
                    "label": 0
                },
                {
                    "sent": "That's a penalty for violations of the training set.",
                    "label": 0
                },
                {
                    "sent": "So here we see that for low values of.",
                    "label": 0
                },
                {
                    "sent": "Training error penalty.",
                    "label": 0
                },
                {
                    "sent": "We fit the hidden alphas almost perfectly by the estimated Alphas, but if you are more stringent about fitting the training data with a larger value of B then the system overfits and water the Orphic mean in the context of Alpha.",
                    "label": 0
                },
                {
                    "sent": "Remember if Alpha is very close to one then you are walking all the time in the graph.",
                    "label": 0
                },
                {
                    "sent": "You know jumping if Alpha is close to zero you are jumping all the time to this algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's more advantageous to tune Alpha closer to 1.",
                    "label": 1
                },
                {
                    "sent": "Because then you are walking a lot and I can use the edge weights to satisfy your rankings.",
                    "label": 0
                },
                {
                    "sent": "If you push Alpha down towards all jumping then I have limited wiggle room right?",
                    "label": 0
                },
                {
                    "sent": "So if I want to pay a lot of attention to their training set, then the algorithm hypes up the alphas.",
                    "label": 0
                },
                {
                    "sent": "So that things walk more and then it can use the edges to satisfy your preferences.",
                    "label": 0
                },
                {
                    "sent": "So overfitting in the Alpha space means the graph goes up like that.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the somewhat shorter second portal, we talking about indexing.",
                    "label": 0
                },
                {
                    "sent": "But to summarize quickly, the inner product space of ranking is very well explored.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple scoring model and still TF IDF NVM 25 took decades to evolve and stabilize.",
                    "label": 0
                },
                {
                    "sent": "Learning those weights is very recent, however still evolving.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile we have the new class of problems which is ranking in graphs and page rank and friends are just version 0.1.",
                    "label": 0
                },
                {
                    "sent": "They basically said the graph by four since it let's rank in this and we're saying.",
                    "label": 0
                },
                {
                    "sent": "You know we want some sort of desirable outcome, and we want to fit weights to come up with that outcome.",
                    "label": 0
                },
                {
                    "sent": "And since this field is much less mature, we must bootstrap or ranking wisdom via machine learning techniques.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in the second part we will talk about indexing issues, performance issues.",
                    "label": 0
                },
                {
                    "sent": "So type hierarchies are pretty large and deep.",
                    "label": 0
                },
                {
                    "sent": "If you're thinking would have open domain searching over arbitrary text, you might have 10s of thousands of internal nodes in your type hierarchy order today has 18,000 internal nouns and a total of over 80,000 nouns.",
                    "label": 0
                },
                {
                    "sent": "So runtime 8 type expansion is out of the question.",
                    "label": 0
                },
                {
                    "sent": "If you say which scientists found theory of relativity and you want to expand scientists to all scientists known by ordnet, you'd be actually asking 650 queries, so that's completely impractical.",
                    "label": 0
                },
                {
                    "sent": "In 60 cities and so on, there are more cities than that, so the opposite approach is to pre index all of this.",
                    "label": 0
                },
                {
                    "sent": "So if I see the token sagun I index at the same position, 2nd and physicists and scientists person living thing and so on.",
                    "label": 0
                },
                {
                    "sent": "But this is the problem that the index space will now block because you are seeing a lot of words at the same positions of the upper words are actually since it's from word net.",
                    "label": 0
                },
                {
                    "sent": "So the extent of blood is clear from this table with the original corpus is about 6 gigabytes.",
                    "label": 0
                },
                {
                    "sent": "The gzip corpus is about 1.3, and the stem indexing Lucene is only .91 gigabytes.",
                    "label": 0
                },
                {
                    "sent": "However, if I do this upward closed indexing, I get a total index either 4.6 gigabytes, which is almost the original corpus size.",
                    "label": 0
                },
                {
                    "sent": "So you might argue that you can't really afford it, especially if you want to store a large portion of the full of the index in RAM for fast.",
                    "label": 0
                },
                {
                    "sent": "Query processing.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, our basic framework is very simple.",
                    "label": 0
                },
                {
                    "sent": "We call it the pre generalizing post filter approach so that the full set of data types BA and that's too large to index in full.",
                    "label": 0
                },
                {
                    "sent": "So we decide to index only registered subset R. Now let's say a query has a type A.",
                    "label": 0
                },
                {
                    "sent": "Let's say which scientist studied wells.",
                    "label": 0
                },
                {
                    "sent": "So they type is scientist.",
                    "label": 0
                },
                {
                    "sent": "Or unfortunately scientists hasn't been indexed during indexing time.",
                    "label": 0
                },
                {
                    "sent": "So we have to walk up to the nearest index data type which we call the generalization orgy.",
                    "label": 0
                },
                {
                    "sent": "And in this case, is that happened to living thing.",
                    "label": 0
                },
                {
                    "sent": "So now we use G to prove our index to get all occurrences of living thing.",
                    "label": 0
                },
                {
                    "sent": "And this is a particular context.",
                    "label": 0
                },
                {
                    "sent": "Whales were studied by Cousteau.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is that Cousteau was the true candidate.",
                    "label": 0
                },
                {
                    "sent": "But because they generalize to Gina, whales are also a candidate.",
                    "label": 0
                },
                {
                    "sent": "Now Douglas Adams might think that wells were studying Cousteau while Cousteau are starting the whales for the purpose of.",
                    "label": 0
                },
                {
                    "sent": "Answering this query you would like to answer with boost or not.",
                    "label": 0
                },
                {
                    "sent": "Well, so there are some elimination to do after you are done with this step.",
                    "label": 0
                },
                {
                    "sent": "So we get the best Cape.",
                    "label": 0
                },
                {
                    "sent": "User wants K answers.",
                    "label": 0
                },
                {
                    "sent": "We get a few more answers because now they're going to be eliminated and then given an index.",
                    "label": 0
                },
                {
                    "sent": "We got all these candidates like Wells and Cousteau, and running through each have to eliminate some of them.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we go to Wales and see it does well connected scientists?",
                    "label": 0
                },
                {
                    "sent": "Well, not in the known universe, so wells are cancelled out in kusto kinda survives now.",
                    "label": 0
                },
                {
                    "sent": "If at the end of this we are left with fewer than K hits, then we have to actually go back and restart the query with larger K prime which is very expensive.",
                    "label": 0
                },
                {
                    "sent": "So we have some ideas on how to pick a conservative K prime, etc.",
                    "label": 0
                },
                {
                    "sent": "So I'll.",
                    "label": 0
                },
                {
                    "sent": "Not mention that for lack of time.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the important things in this setup is.",
                    "label": 0
                },
                {
                    "sent": "How to pick an art and to pick the subset R we have to know how much index storage R will cost as compared to the whole of the unfortunate thing is, well, first of all there is an exponential number of subsets of a, But even if someone gives me a few candidates, I can't afford to kind of index all of them in turn and see how much how many bytes each of them costs, because that's going to be too much time and the 2nd is.",
                    "label": 0
                },
                {
                    "sent": "If I have indexed all of a, that's very good for query processing.",
                    "label": 0
                },
                {
                    "sent": "No, that's going to take a particular amount of time to answer a particular query.",
                    "label": 0
                },
                {
                    "sent": "If I have indexed subset of a, namely R, I'm going to take more time to solve a query in general.",
                    "label": 0
                },
                {
                    "sent": "What is going to be the load factor in query processing time and I have to find the expected bloat over a representative workload so it's a space time tradeoff and the time tradeoff has to be averaged over a workload and the space tradeoff has to be determined for all kinds of candidate hours.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our problem, the size problem turns out to be easy.",
                    "label": 0
                },
                {
                    "sent": "Each token occurrence results in one posting entry in Lucene, and even if index compression is used, we can assume that over the whole corpus that accounts for a constant factor, and so we can roughly estimate that the index size taken by R is going to be the sum over all eight types.",
                    "label": 0
                },
                {
                    "sent": "In R, the number of times our token which reaches up to R is found in the corpus.",
                    "label": 0
                },
                {
                    "sent": "And this is surprisingly accurate.",
                    "label": 0
                },
                {
                    "sent": "So we sampled a bunch of random ours and plotted a scatter of the index size, actually building it painfully against this number, and we found it almost straight line match.",
                    "label": 0
                },
                {
                    "sent": "So that problem is easy.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second problem is a bit more tricky.",
                    "label": 0
                },
                {
                    "sent": "We don't have a very good solution.",
                    "label": 0
                },
                {
                    "sent": "We have an OK solution, so if R is the whole of a, then the time taken to answer the query is leucines time to actually run through those inverted lists of the corpus count of a times.",
                    "label": 0
                },
                {
                    "sent": "The time to scan through each document already chocolates.",
                    "label": 0
                },
                {
                    "sent": "So on the other hand, if I couldn't index all of R, then the price to pay for the generalization is that I have to now scan more posting entries.",
                    "label": 0
                },
                {
                    "sent": "So I'm multiplying the same T scan with the corpus count of G, which is larger and after that I need to do the post filtering of K prime responses, which will be K prime time, some tea filter, anti filter and T scan can be estimated by actually running and profiling the code.",
                    "label": 0
                },
                {
                    "sent": "So the overall blood factor then looks like T scan times, corpus count of G. The larger 1 + K prime times T filter divided by these contain the corpus count of a, so this is not too bad.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimate, I mean the the scatter looks nowhere as respectable, but the important observation is this is a poor query scatter.",
                    "label": 0
                },
                {
                    "sent": "If you average over multiple queries workload then the line.",
                    "label": 0
                },
                {
                    "sent": "If it becomes so much better.",
                    "label": 0
                },
                {
                    "sent": "So while observed the estimated ratio for one query is noisy, the average over many questions is much better.",
                    "label": 0
                },
                {
                    "sent": "So use those two forms.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last question to be answered is worth the workload, and this is where that long tail part comes in again.",
                    "label": 0
                },
                {
                    "sent": "So the expected bloat over many queries is just the probability of finding a type in a query times the query bloat of a with respect to the registered set R. And if for query probability we use some sort of a maximum likelihood estimate given counts, that's never going to be good be cause a lot of areas will never be seen in the training data, and you're going to find those as in the test data.",
                    "label": 0
                },
                {
                    "sent": "And if you haven't seen an A in the training data and you have given it a probability of zero, your index optimizer is not going to budget for.",
                    "label": 0
                },
                {
                    "sent": "Its appearance, and therefore when the query eventually comes along, you are going to have a huge bloat in query processing time.",
                    "label": 0
                },
                {
                    "sent": "Because this is an average, exceptions will matter.",
                    "label": 0
                },
                {
                    "sent": "OK, so therefore good smoothing of the query.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ridiculous, sensual, and we're not really doing a great job of that yet, but one easy approach does reasonably well, so we do what's called lidstone smoothing, which is have an additive, smoother, sometimes used in natural language work, and we fit L. So instead of using just the part without L, we slap on a non 0 L and we fit L by calculating the probability of held out data with respect to help.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the Redstone parameter.",
                    "label": 0
                },
                {
                    "sent": "And here is the probability of held out test data and that reaches a distinct peak and we hold L at that point.",
                    "label": 0
                },
                {
                    "sent": "But we still do uniforms, moving overall unseen.",
                    "label": 0
                },
                {
                    "sent": "A types which is probably not the right thing to do.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, so after you put all that together, you have a workload.",
                    "label": 0
                },
                {
                    "sent": "You have a query block for every NR, so I'm going to search for R and this is actually NP hard even when the type taxonomies are tree, which was a bit surprising.",
                    "label": 0
                },
                {
                    "sent": "No, so we have a greedy algorithm to solve it.",
                    "label": 0
                },
                {
                    "sent": "We start with the roots of a Witcher like Sentinel nodes, and then we add the most profitable a type A star.",
                    "label": 0
                },
                {
                    "sent": "So what's the profit?",
                    "label": 0
                },
                {
                    "sent": "The profit is the ratio of reduction in bloat of a star and its descendants.",
                    "label": 0
                },
                {
                    "sent": "If we add that to the increase in index space because you're also indexing yesterday, so keep sorting by that and add the next one.",
                    "label": 0
                },
                {
                    "sent": "It's a little tricky because when you include scientists in art you reduce the bloat of physicists.",
                    "label": 0
                },
                {
                    "sent": "And that reduces the desirability of including person, so you have to kind of proceed by downward and upward scans in the ordinate.",
                    "label": 0
                },
                {
                    "sent": "Or a type graph efficiently.",
                    "label": 0
                },
                {
                    "sent": "And as you do this, you get a continuous tradeoff in the index space and average query bloat, so the more index passive investor less query bloat there is.",
                    "label": 0
                },
                {
                    "sent": "It's nice to see that there are very sharp nice so you get most of the benefit of full a very early on.",
                    "label": 0
                },
                {
                    "sent": "And here's some interesting anecdotes about what lidstone parameter to use.",
                    "label": 0
                },
                {
                    "sent": "If we use a little parameter, which is really, really small, it means you believe that things which you have never seen in the training that are never going to appear in the test data.",
                    "label": 0
                },
                {
                    "sent": "But will they do so in cross validated data?",
                    "label": 0
                },
                {
                    "sent": "You certainly find for the lowest value of lidstone some test data was really bad in the sense of not finding a generalization close enough, and that led to a huge blow up to the point that algorithm picked up that a type and then it plummets.",
                    "label": 0
                },
                {
                    "sent": "So it's important to pick the right listen parameters.",
                    "label": 0
                },
                {
                    "sent": "It turns out that this is kind of signal.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That specifically in parameter which led to the largest likelihood of test data is the same list.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameter that led to the lowest.",
                    "label": 0
                },
                {
                    "sent": "Graph in this family, which is not obvious why that should happen.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was for estimated as the algorithm runs estimated numbers.",
                    "label": 0
                },
                {
                    "sent": "These are real numbers, observed index size in bytes and average and maximum load over the track query workload and the average float is with just one 4th or 1/5 of the total index size.",
                    "label": 1
                },
                {
                    "sent": "We get an average growth of only 1.9, so you're slowing down the queries by factor of two while using index space, which is 151.",
                    "label": 0
                },
                {
                    "sent": "Sixth of what you could have.",
                    "label": 0
                },
                {
                    "sent": "And the maximum load of course looks really bad.",
                    "label": 0
                },
                {
                    "sent": "There are one or two really bad queries out of 500 or 1000 trick queries which blood by a factor of 1000, but that's reality for almost any search engine.",
                    "label": 0
                },
                {
                    "sent": "I mean, Google probably has like a timer or something, and once you run out of 1 million stopwords your queries out of the system too.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to summarize, we could get down from the full type index space of 4.6 gigabytes, down to about 520 megabytes, and that compares favorably with the STEM index itself is actually smaller than their STEM index, so it's less than a factor of two payment over the index space that's already invested an we need a couple of other indexes that I don't mention.",
                    "label": 0
                },
                {
                    "sent": "Detail window reachability index to test whether whales can reach up to scientists?",
                    "label": 0
                },
                {
                    "sent": "That's very small.",
                    "label": 0
                },
                {
                    "sent": "And we need a forward index.",
                    "label": 0
                },
                {
                    "sent": "To know that the world is well, I'll take that offline.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the summary of partners we have prototype which is built around Lucene and IBM's Umm framework.",
                    "label": 0
                },
                {
                    "sent": "We're going to release that in open source in a couple of months.",
                    "label": 0
                },
                {
                    "sent": "So what's the added functionality beyond just Lucy?",
                    "label": 0
                },
                {
                    "sent": "So basically there's another pipeline which maybe I should produce.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Miss Katie diagram here.",
                    "label": 0
                },
                {
                    "sent": "So there's this annotator pipeline which takes a corpus and subjects two named entity recognizers and lexical network connectors.",
                    "label": 0
                },
                {
                    "sent": "So that creates that linear chain with the graph structure but implicitly not taking as much space as it would if you allocated inode for every word in the corpus or anything like that.",
                    "label": 0
                },
                {
                    "sent": "And then there's the first round of indexers which create the STEM index in the reachability in the forward index and maybe temporarily create the Fuller type index.",
                    "label": 0
                },
                {
                    "sent": "Wild analysis goes on and then there is a type workload.",
                    "label": 0
                },
                {
                    "sent": "Part so if you have a query log or you if you have some prior idea about the distribution of a types in your workload query workload, you pass that in with smooth.",
                    "label": 0
                },
                {
                    "sent": "That suitably and from that we have workload driven subset chooser algorithm that I just told you about and in past two we compress the type index to the subset and then threw out over the full index.",
                    "label": 0
                },
                {
                    "sent": "And after that there is the query processing part which I haven't talked about today, but the scoring function in the query processor is pluggable and currently is provided by all those in the rank SVM and the smooth loss or the graph ranking approximations and that gives you the final top care tokens or snippets as responses to your query.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And we also have.",
                    "label": 0
                },
                {
                    "sent": "Can you feel air which takes well formed questions and Maps them into a type selectors?",
                    "label": 0
                },
                {
                    "sent": "If we say what's the distance between Paris and Rome?",
                    "label": 0
                },
                {
                    "sent": "It will come up with an award Nets synset ID for distance as a type, and Roman Pariser selectors.",
                    "label": 0
                },
                {
                    "sent": "So ongoing work, we're looking at indexing and searching religions other than user relationships and exploring more general notions of graph proximity soft, which I've already mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the conclusion is that we can only afford to perform some limited.",
                    "label": 0
                },
                {
                    "sent": "Restructuring of the corpus.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to anticipate all query needs, and so the structured part has to be attached to an otherwise unstructured corpus, and query systems must handle this combination of a linear corpus together with semantic structure above.",
                    "label": 0
                },
                {
                    "sent": "In one shot, we cannot insist on a specific schema, and these two parts of the structure have to coexist.",
                    "label": 0
                },
                {
                    "sent": "Also, models of influence in graphs are getting clearer in the machine learning.",
                    "label": 0
                },
                {
                    "sent": "But what's not clear is what is a query in this context and what is their response?",
                    "label": 0
                },
                {
                    "sent": "Is the unit of response.",
                    "label": 0
                },
                {
                    "sent": "One node is enough for a sponsor Twig?",
                    "label": 0
                },
                {
                    "sent": "How to rank those things?",
                    "label": 0
                },
                {
                    "sent": "These are not clear, ranking graphs is still underexplored.",
                    "label": 0
                },
                {
                    "sent": "We know how to rank feature vectors, but you don't know how to rank nodes in a graph all that well.",
                    "label": 0
                },
                {
                    "sent": "And finally scheduled scaleable indexing and top K query execution are still major challenges now some extreme special cases of those are being handled.",
                    "label": 0
                },
                {
                    "sent": "Hertz Group has done some excellent work on that and there are groups in Microsoft who are working on top query processing in presence of text and structure, but definitely not for general graphs.",
                    "label": 0
                },
                {
                    "sent": "So all those are morning needs.",
                    "label": 0
                },
                {
                    "sent": "That's another talk.",
                    "label": 0
                },
                {
                    "sent": "China's population, but maybe if you.",
                    "label": 0
                },
                {
                    "sent": "Some short questions again.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Of course, the first part is if you take into account.",
                    "label": 0
                },
                {
                    "sent": "Swings.",
                    "label": 0
                },
                {
                    "sent": "Provide additional information about.",
                    "label": 0
                },
                {
                    "sent": "This is something that has been studied the language language.",
                    "label": 0
                },
                {
                    "sent": "Because it works well for words that announce.",
                    "label": 0
                },
                {
                    "sent": "Later just fails for hers because for verbs.",
                    "label": 0
                },
                {
                    "sent": "The structure of the group imposes that what is important for her by the compliments that might be introduced by positions that are not necessary next liver.",
                    "label": 0
                },
                {
                    "sent": "Basically they had invested his integration of nouns.",
                    "label": 0
                },
                {
                    "sent": "Works very well to be visible Contacts, information verbs, just phase.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's a variety of answers to that.",
                    "label": 0
                },
                {
                    "sent": "The first thing is first comment is a big night, but search engines have already reduced expectations.",
                    "label": 0
                },
                {
                    "sent": "Such a lot that most people know that only noun oriented queries succeed, right?",
                    "label": 0
                },
                {
                    "sent": "So that's one, but our non parametric or pseudo parametric form of benefiting the proximity function is actually one way to fix this a little bit, because in case it's a verb attachment then you know that the function isn't going to reach its maximum very close to zero.",
                    "label": 0
                },
                {
                    "sent": "It's going to be further out.",
                    "label": 0
                },
                {
                    "sent": "And if the number of such queries increases in the in the training mix, I'm going to learn that the peak is going to shift away.",
                    "label": 0
                },
                {
                    "sent": "But the better answer to your question is that if I had the time and NLP tools would help me say run a dependency parse on the corpus ahead of time, then my unit of distance doesn't need to be number of links on the chain, so I could also learn that.",
                    "label": 0
                },
                {
                    "sent": "So you could think of diversifying the chain graph to saying uninterpreted dumb distance link, as against.",
                    "label": 0
                },
                {
                    "sent": "Prince Lincoln this would become two different teas in my graph ranking framework.",
                    "label": 0
                },
                {
                    "sent": "Sing assistant people see if they don't see that.",
                    "label": 0
                },
                {
                    "sent": "Speak the way you set up your system.",
                    "label": 0
                },
                {
                    "sent": "Is your daddy nude?",
                    "label": 0
                },
                {
                    "sent": "And the Alpha being at one, if I'm not mistaken, it's gonna be really useful metrics.",
                    "label": 0
                },
                {
                    "sent": "So you're basically searching for the dominant eigenvectors of such methods.",
                    "label": 0
                },
                {
                    "sent": "Oh so OK.",
                    "label": 0
                },
                {
                    "sent": "So the point is.",
                    "label": 0
                },
                {
                    "sent": "Who is this better be?",
                    "label": 0
                },
                {
                    "sent": "One is better and speak with you and then you just people like because you said to find the determinant of 1 minus eBay.",
                    "label": 0
                },
                {
                    "sent": "Does you know?",
                    "label": 0
                },
                {
                    "sent": "So that matrix is typically huge.",
                    "label": 0
                },
                {
                    "sent": "I don't want to invert it or find it settlement for one thing.",
                    "label": 0
                },
                {
                    "sent": "So there are no problem.",
                    "label": 0
                },
                {
                    "sent": "So the problem is that I need that I need the gradient of that inverse.",
                    "label": 0
                },
                {
                    "sent": "It's not just the inverse that I want, I want the gradient of the inverse with respect to every better T. That's horrible, so I don't want to handle that problem.",
                    "label": 0
                }
            ]
        }
    }
}