{
    "id": "ttseenjmf3yrfmvjr62wp4pyf3zwi52s",
    "title": "Connections between the Lasso and Support Vector Machines",
    "info": {
        "author": [
            "Martin Jaggi, Applied Mathematics - CMAP, \u00c9cole Polytechnique"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_jaggi_connections/",
    "segmentation": [
        [
            "I'd like to thank the organizers for.",
            "Having me here, it's a great pleasure.",
            "So as this workshop is about.",
            "Uh.",
            "Like compromises on one hand.",
            "And on sparse message, on the other hand.",
            "We'd like to investigate a bit on the connections of these two fields and.",
            "One example of.",
            "Such a connection is.",
            "Actually here if I would."
        ],
        [
            "OK, is this two things?",
            "So they're like to go?",
            "2 problems here, but you want to study which is still supported machine from machine learning.",
            "And then there is this other thing, which is the last so message from from sparse regression.",
            "And this would be the example we will cover in the first part of the talk, and then in a small part at the end of a brief comment about greedy algorithms which look very similar in convex optimization.",
            "On one hand, then on sparse.",
            "Signal processing, on the other hand.",
            "But the main part will be about last Windows VM, Santa Paper, and that is actually on archive and also in the.",
            "In this workshop PDF."
        ],
        [
            "So what are these linear classifiers?",
            "This is not the first time we're going to see this concept in this workshop and on.",
            "Also not the last time we're going to see it, and so the setting is the following.",
            "We given this digital images, for example, this our training data and if such image is represented by a point."
        ],
        [
            "In the dimension space, say.",
            "There are two of these kinds and what we want."
        ],
        [
            "Find this we're going to find the hyperplane.",
            "Which separates these two kinds of points for the best possible."
        ],
        [
            "Margin of separation.",
            "So that's what we call a support vector machine.",
            "And in this talk here, what we?"
        ],
        [
            "Going to assume instead the hyperplanes are going through the origin.",
            "That's not a big restriction, but the advantage of using only such planes is that.",
            "If you do this, you only have to care about one kind of points, so we can just ignore group."
        ],
        [
            "By considering the negatives instead.",
            "So I've taken the blue points and mirrored them at the origin.",
            "Now I have only one one set of points and I'm asking what's the best separating plane for that kind of."
        ],
        [
            "Point set.",
            "So let's think."
        ],
        [
            "You want about this problem, so we want to find the best separating planes."
        ],
        [
            "Here is one shot.",
            "Such candidate of a plane W is its normal vector and yellow area.",
            "Here is the margin of separation which we get by this particular plane.",
            "What I want to argue is that.",
            "Finding the best separation is actually the same as finding the shortest point in this blue area."
        ],
        [
            "Which is the polytope distance problem, so to say.",
            "So the blue areas to come exam of all the points in our input.",
            "And.",
            "We want to find the shortest point here which."
        ],
        [
            "Is given by this W star.",
            "I claim that this W star this is the solution to both problems.",
            "It's both the shortest point in the in the hole, and it's also the vector which best separates the points which gets the best margin.",
            "So let's let's think about how we can formalize this distance problem because for this talk it will be enough to understand the distance problem, not have.",
            "We don't have to worry about the margin maximization too much, which I will explain on the next slide.",
            "Why this is the case.",
            "So the distance problem can be written in two ways, here is."
        ],
        [
            "Love.",
            "The possibilities we can just minimize the Euclidean squared norm.",
            "Over the Hall."
        ],
        [
            "Or here's another way.",
            "We will just say represent our points in the hole by convex combinations.",
            "That's the definition of a convex Hull, so we have some nonnegative weights which I put in a vector X.",
            "The Vector X listing and dimensions.",
            "I'm given N many points and these entries of the vector accepted some to one, so there's a probability vectors.",
            "Accessing the simplex in dimensions and a or my data points so I will collect all my data points in a matrix and every point will be a column of this matrix.",
            "There will be D many rows and then many columns.",
            "And here we are going to use work mainly in this formulation using the context representation 'cause those of you which are familiar to ACMS will will see that the first one is actually related to the primal supported machine.",
            "Looks a bit similar versus this one.",
            "Here is exactly the dual support vector machine and the points which appear with non zero weights in the solution to the problem.",
            "So those which have non 0XI these are the support vectors, those which define the best solution."
        ],
        [
            "So let me explain why we can actually restrict ourselves.",
            "This promise of why is it interesting to work with this formulation?",
            "Here is a small list of of ACM variants which have the property that exactly fit into the settings because they're dual problem is exactly of this form is quadratic minimization over the simplex, so this includes this as invariants.",
            "If you're in a hard margin setting, if you assume that you later separable then.",
            "You you have connect two or just one class or you can have the plane going to the origin or not if it's not going through the origin then there is a slight.",
            "Issue that you the offsets the distance from the engine will also be regularised, but it's not a big deal, but.",
            "The crucial thing here is that you cannot assume your data to be separable by early no function.",
            "They will never be the case in practice.",
            "So one of the core properties with responsible for the success of this kernel, methods that you can deal with outliers.",
            "You can deal with non separable vectors, so that's why we are interested in the soft magic formulations.",
            "Other points are allowed to lie on the wrong side of the plane, but if you're on the wrong side of the plane then it will be punished in the objective function.",
            "And we consider those variants where your distance to the plane.",
            "If you're on the wrong side, is the punished by the square of this distance there."
        ],
        [
            "It's not there in which you just consider the hinge loss which the distance alone without a square.",
            "This we do not cover here because they're not there.",
            "Dual problem doesn't have this form, so just for completeness, here is an example.",
            "It's actually this guy here, so that's the SCM Primal.",
            "If you've probably seen it looks a bit technical, but if you want to take the dual of this quadratic program, you can do that and you can verify that you will get a problem with this form.",
            "More important thing is that."
        ],
        [
            "This formulation is very obvious to kernel eyes, so you can just use a kernel where you you don't know the explicit features viewpoints.",
            "You only know the inner products between pairs of points.",
            "Because that's only what this optimization objective depends on.",
            "So here's the objective.",
            "I hope it can be seen.",
            "It's a bit small, so the objective is only depending on the on the matrix agents voice a, so only the inner product between all pairs of columns.",
            "Which you can just replace by the kernel matrix instead.",
            "And then you will still have the same optimization problem.",
            "So what we want to remember from here is that from now on, this will be our iciam problem and.",
            "Every assume instance will be given by such a matrix A where the data points are the columns."
        ],
        [
            "OK, here is a completely different animal, so it's the lawsuit problem.",
            "This is a regression method.",
            "It's actually a different name, for it is actually the L1 regularised least squares regression.",
            "So you want to approximate.",
            "Some, some right inside B by some linear model AX.",
            "That's a standard squared loss, but what you have is you have a regularization.",
            "You add this term that your Wellness.",
            "They should have small norms.",
            "They should have small complexity to make your model robust, so you add an L1 constraint on your variables.",
            "That's our regularization.",
            "Another effect apart from being more robust, is that you will encourage sparse solutions.",
            "Which is which was very.",
            "Nice and his."
        ],
        [
            "Greatly helped the dismissal to have so many applications.",
            "They are like 2 interpretations of this same problem so you can interpret it as.",
            "Column wise so to say.",
            "If you think about what happens if he here we want to approximate our right inside vector B by few columns of this matrix A in this setting you usually call the matrix the dictionary matrix.",
            "Or you can do the same thing."
        ],
        [
            "Can interpret the rows of the matrix of the same problem and you can think of this as.",
            "Feature selection from because you you wanted each row of the matrix times you expect, or so that's the selection of feature you get from that row will approximate the corresponding B entry from the right hand side.",
            "Just two interpretations of the same problem.",
            "I apologize if this is very basic."
        ],
        [
            "There is 1 technical catch we want to do instead of that one on being some bounded by some value T. I want one home to be bounded by one, so I want to be in DL1 unit ball.",
            "That's my optimization domain.",
            "This is not a restriction becausw you can just re scaling matrix A if you prefer to have some other value on the L1 norm.",
            "So that's our lawsuit problem.",
            "And again, how is it defined?",
            "So any instance is defined by matrix A and the right hand side vector B.",
            "So this already looks quite similar to what we had as dual SVM formulation, so this is the primal assume, and this looks kind of similar to the other one except for the classifier.",
            "We had a simplex domain and here it's now one ball.",
            "And in the classifier we only had this X squared.",
            "We didn't have this additional right inside Vectibix, so that's two differences between these problems.",
            "And now we're trying to just overcome those to relate this."
        ],
        [
            "Would be more together, so we want to.",
            "To see that giving some.",
            "Some primulas two instance, can we actually construct and assume instance of of this form which does the same thing?",
            "So now I'm giving this last time given A&B and the question is can I come up with some matrix, a tiller?",
            "In of whatever this consists such that the optimization problem is the same."
        ],
        [
            "And this direction is actually much easier than the other one, so here it's just easy.",
            "You can just re Parameterise deal one ball by by using this positive coordinates instead or the barycentric coordinates, which is the same thing as we've done for the polytope.",
            "Previously we talked about putting distance, but it's even simpler."
        ],
        [
            "You just write that any vector in that one ball can be written as a bigger vector in the simplex, so this is twice as many coordinates times this matrix which order the vertices of the L1 Volt unit vectors.",
            "Just maybe I'm making this more complicated than S."
        ],
        [
            "Siri so.",
            "Another way to see this for each variable you had previously, which was allowed to be non negative or positive.",
            "Now you have two variables and table positive.",
            "They sum up to one year in a simplex.",
            "I would just plug this in into my last problem instead of this set at one.",
            "I will plug in the simplex and my my larger variable like 2X prime.",
            "This is the same thing and now I've basically I'm done already because now I have an SVM problem nearly the only thing that remains here is that there is a vector B which I need to get rid of.",
            "But this is just translation so I can do this by by."
        ],
        [
            "Just translating each column of my matrix by vector B.",
            "And then if I did find this to be my my classifier matrix, then this is exactly the equivalent problem.",
            "So what do I mean by equivalent?",
            "So I mean that so now you can give me any any feasible vector for the last two there, one born and then you automatically get one or several vectors in the simplex which keep the same objective for the sea.",
            "And on the other way around is even simpler for any vector simplex.",
            "For any classifier vector.",
            "So to see you get exactly 1 back to I1 bowl.",
            "So and this this guy will have the same objective value in the lawsuit.",
            "So it's not only that these two problems have the same.",
            "Optimal solution that is there is clear, but also all feasible solutions correspond sense, so this is nothing.",
            "Nothing fancy.",
            "And.",
            "Just.",
            "To make this."
        ],
        [
            "Another way to see this I'm doing.",
            "This overly complicated so just to see that what we've done previously in this formula source, we have written the last two as a distance problem.",
            "And here is a way, but how?"
        ],
        [
            "Can geometrically see this.",
            "I have my original points a from the last matrix on the right side, the blue ones.",
            "Their negatives are the red, the green ones on the left side."
        ],
        [
            "And I can take the comics after the union of those two, and when I claimed."
        ],
        [
            "And is that the lawsuit problem is actually to compute the distance from this vector to the?"
        ],
        [
            "Dot com itself the union."
        ],
        [
            "And why is that?",
            "I mean, we've already seen it or previous lovers just the geometric way to see it."
        ],
        [
            "What is the set of points AX, 4X India one born?",
            "It's just a the matrix a applied to the other one ball cheese to convicts out of the unit vectors."
        ],
        [
            "It doesn't.",
            "If you have a linear map, it doesn't matter if you do convex Hull after or before the linear map, so you can so."
        ],
        [
            "So you have to decide this.",
            "Actually, the convex Hull of of the Union of Direct sector center negatives.",
            "I hope this makes a bit of sense.",
            "Please interrupt if there are any questions that."
        ],
        [
            "So now the more interesting side is the the other direction.",
            "This is a bit more complicated to now.",
            "Can we do a reduction in the other way given an SVM can be constructed lasu instance which does the same thing.",
            "The problem in this is that if you have a lawsuit and any any point which is part of this problem automatically, it's negative will also be part of the game and this you don't want for classifier.",
            "So in a classifier, when you're giving some training example then it's negative is something completely else, so it's no way that you allowed to use that for your training."
        ],
        [
            "So can we.",
            "Can we find a way around this problem that can we get rid of these negatives and there is a?",
            "A way to do this, but it's a bit.",
            "A bit formula to the reduction is very simple if your ideas are formula, so I will just define my lawsuit instance.",
            "This is translation of my assume points.",
            "So the same matrix now.",
            "So we're not changing the size of the matrix as we've done previously, so now I'm taking the same matrix A and I just translate each column by fixed vector and this vector is exactly is actually the right hand side of the zoo and.",
            "And the question is just how should we choose these translation such that these problems are in a in any sensible way related?",
            "And it turns out."
        ],
        [
            "You want to choose this translation.",
            "In a way that.",
            "The the translation Vector B is a W is weekly separating for the point sets for these points from the classifier.",
            "So what does weekly separately mean?",
            "It means that all points from the set have positive inner product.",
            "This vector then these victories is suitable for our reduction.",
            "So of course, this only works if you can find such a vector, and so I'm just saying that this is not a.",
            "Doesn't have to be the best separation, this is just any separation as long as everybody's very small amount of the right side, it doesn't have to be the maximum margin.",
            "So.",
            "To understand this.",
            "What does it actually mean?"
        ],
        [
            "Then we can again go to gym to feel like, just to give you the intuition.",
            "So here we have the points which we got from our classifier in the beginning.",
            "What we want to do is we want to formalize something like this so that we can find the best possible separation.",
            "But now the problem is we have we have a lawsuit so the negatives for these guys will also be included.",
            "So we want to be we want their negatives to be somewhere where they don't have a negative impact on us.",
            "So we want these negatives to be far away.",
            "From the interesting.",
            "Region where something is happening and."
        ],
        [
            "We will use our weakly separating vector for that."
        ],
        [
            "You just choose the origin, which is the only freedom we have by choosing the selector.",
            "Be as to be far away from this weekly separating plane so we can choose here and the idea."
        ],
        [
            "This is if you do this, then the negatives of these points they will not influence your your problem in a bad way.",
            "So I'll make this formula means, but this is the geometric idea.",
            "I hope this makes any sense, and then the optimization objective will still be the same.",
            "We just we will find the same W star vector will not have any contributions from this evil blue points.",
            "OK."
        ],
        [
            "So let's make this more former.",
            "We're given the classifier we want to construct this lawsuit with the ATL and BTL.",
            "And the result that we get from this construction is that the.",
            "Distance to instance.",
            "Every interesting feasible.",
            "Point in that lawsuit will be positive anyway, so only the positive part of this lawsuit will be appearing in an existing solution.",
            "We can do this would be more formal, it's it's a bit annoying, but you can say that for for any point, which is indeed one ball so feasible for the lawsuit.",
            "There is a point which is positive, not negative entries, which is actually on the simplex, which is the same or better objective for the last two already.",
            "And then what you can do is you can take it and plug it into the SCM and this will give you exactly the same objective objective.",
            "So the other dealer direction is simple, so of course every point for the SCM.",
            "Every point which is already simplex is feasible for the last two anyway, because the simplex is included in the L1 bowl.",
            "So from the SCM Activella series, easy.",
            "But if you are given some strange point of the lawsuit and you can say that there is a corresponding simplex point only using the blue points, which gives you the same or better objective.",
            "And in particular, you will also get that this constructed Lasu now has the same optimal solution of course, and you also have in a sense that all feasible solutions correspond.",
            "Or the interesting feasible solutions correspond for the two problems.",
            "The only thing I didn't tell you yet is how do we find such a really separating vector?",
            "Because otherwise it's pointless, we cannot.",
            "Do the reduction.",
            "And.",
            "The good thing is that for for the soft margin we have two loss, such a weekly separating vector is is is trivial to obtain, it's automatically.",
            "Availability essentially constant vector, so there is no cost to get such a weekly separating vector.",
            "Foreign tools.",
            "If you have a hard margin SVM.",
            "Then you might you have to run some nasty mob device to get such a such a vector, but you don't have to run it until the close to the optimum just until it becomes separating for the first time."
        ],
        [
            "I hope this makes some sense.",
            "So.",
            "What's what can this be used for?",
            "Why?",
            "Why would you want to do this anyway, so?"
        ],
        [
            "The main motivation to look at this was that you can take out any algorithm for one of the two problems and apply to the respective order problem.",
            "So this this find interesting because.",
            "Dimensions for for ACMS has been developing in a very different directions than the optimizers, for for the last two or sparse problems, so I hope this might.",
            "This might help to related to algorithm to see which one is what.",
            "Are these properties which make the arguments being good for for this problem and how do they actually compare?",
            "Like numerically, if you would want to do some experiments, how would they compare when applied to the respective other problem so?",
            "But this is still.",
            "To Do List.",
            "I haven't done any experiments on this set, I'd be.",
            "Interested to hear about this."
        ],
        [
            "The one classified Mr I find interesting is this class of the sub linear time methods which have been developed by Clarkson, Hassan, and Woodruff.",
            "Like three years ago for the SVM and for the last two three years ago I guess.",
            "But they have been developed independently, and the property here is that they take less time then it needs to read the input.",
            "The input matrix has size N * T, but they only take time proportional to N, plus the switch is much smaller.",
            "It will be interesting to see how they.",
            "How do you work on the other pro?"
        ],
        [
            "OK, there is 1.",
            "We can also look at one of the problems individually and see what what can we do for for that thing here now using the."
        ],
        [
            "Production and for the loss of an interesting thing is securitization.",
            "So now you have a lawsuit instance and you can just take the equivalence VM if you like, and you can ask what happens if you kernelized SVM.",
            "And here is what's happening.",
            "So previously you had.",
            "You had this problem on the columns of A and the right inside be, but now these are mapped into kernel space.",
            "So now.",
            "You have aggression problem, which is which is a Steelers squared loss.",
            "But it's actually on the vectors in the kernel space.",
            "So that."
        ],
        [
            "That's your objective and.",
            "Again, it's.",
            "The optimization problem is completely the same, but now it's just determined by the kernel inner products between the columns and also between these right hand side vector B.",
            "And you can solve it by the same algorithms, But the question is here, what can it be used for?",
            "This, I'd still be happy to hear more ideas from you if this could be useful.",
            "So for example, if you use a polynomial kernel, what would would you mean is that?",
            "You will learn this regression not only approximating the.",
            "Director B itself, but you would also use the higher moments of these columns AI to approximate the higher moments of B, which could be useful.",
            "Maybe for some applications?"
        ],
        [
            "OK, for support vector machines we also have some.",
            "Some interesting.",
            "Implications and the one I like most is that.",
            "What are the?",
            "What is the set of support vectors?",
            "But The thing is that by by looking at the query and lawsuit you can say that these are exactly the vectors for which the last Winston says non zero entries.",
            "So the sparsity of these lawsuits is very well studied.",
            "In signal processing there are many results for different classes of matrices A&B and the question is can we use some of these results to say something that how many support vectors do we have because it's the same number of the sparsity there is the same.",
            "It's the number of support tickets we have for our classifiers, and if you would like this number to be small because the cost of using your classifier if it's like kernelized grows exactly with this number of support vectors.",
            "So you want to understand this this number of each letter which.",
            "I hope could be.",
            "Possible here."
        ],
        [
            "And another.",
            "Nice concept which which was previously only introduced for the last two messages to conserve screening rules.",
            "So this means a before you start any optimized for your problem, you just go through your data once and you try to identify some data points which will be guaranteed.",
            "To be 0 anyway, in your optimal solution so you can discard them right away before starting the optimizer.",
            "And this.",
            "So far was not was not done with SPMS, but I mean it's not the magic message.",
            "It will not allow you to do have much much faster training, but it still might be an interesting thing to try and to.",
            "To see if it could help the optimizer."
        ],
        [
            "So you can use these by using our reduction, or you can also do it directly for the SVM without using this equivalence.",
            "And here is the idea if you would want to use directly.",
            "So here is some candidate separating vector from ISBN.",
            "So can I use this to discard some of the points?",
            "Which I will not need anyway.",
            "You can do this."
        ],
        [
            "If you rotate the marching you had here, which is not the perfect one but just some margin, you think about all the possible ways to rotate this margin around your point W. And then."
        ],
        [
            "There this Thursday is."
        ],
        [
            "Red set of points is this cone, which consists of all the points which are outside the margin for any possible locations of this bad margin so.",
            "I don't know the optimum watching, but I know that this is at least smaller than the optimal ones and that those points here they are on the wrong side of all possible placements of the margins, so they will not be support vectors ever, so I can just throw them away right away before starting my optimizer.",
            "We could go."
        ],
        [
            "Go to a very short.",
            "Second point, which has nothing to do with classifiers.",
            "So this is only about sparse methods.",
            "Is a bit related to the lawsuit because we want to study greedy algorithms and we want to.",
            "To investigate it because they look very similar in the optimization side, if you want to minimize some continuous convex function on the left here and on the other hand in this sparse methods which just try to recover as farce solution from some noisy measurements.",
            "So on the right hand side nothing is optimized, you just ask you what's the true solution to.",
            "Two, what is your true signal which when you only got this corrupted measurements which are given by this P and you hope to tell someone close to the linear measurement AX?"
        ],
        [
            "OK, let's let's start with the.",
            "Optimization, so here of course it's my my favorite algorithm.",
            "This is the probably the oldest algorithm for constrained convex optimization.",
            "And it's the front Wolf message from the 50s.",
            "What he does is."
        ],
        [
            "It minimizes the convex function or very constrained domain.",
            "Which no kisses the other one ball, and all function here.",
            "The blue function is just a quadratic.",
            "If we apply this thing to the lawsuit problem.",
            "So what he does is it starts at some point X.",
            "And the iteration goes like this that you think that the original function is still too complicated.",
            "I want to minimize something."
        ],
        [
            "Simpler I do the linear approximation.",
            "To my objective, as my current point and I will minimize this linear function instead over the same domain."
        ],
        [
            "And then the update is that I will update my X Factor a bit towards the linear minimizer of."
        ],
        [
            "Listing.",
            "And then I repeat, so that's the whole algorithm.",
            "And the nice thing is, if your domain is the one ball, then this extreme points where the linear minimum retained.",
            "These are the vertices of course, and these are the unit basis vectors or the negatives if you like.",
            "So the interesting thing here is that."
        ],
        [
            "Which is the yeah, which is the vertex that attains the linear minimum.",
            "That's actually the word tax where the gradient test maximum entry.",
            "If you look at it, it's values.",
            "So here F is our quadratic objective and we are changing towards this coordinate, which is the largest in the gradient which promises to give use the biggest change.",
            "So that's our greedy interpretation of this optimization method.",
            "And The funny thing is that.",
            "The coordinate towards which we change this."
        ],
        [
            "Same as you would select by by the signal processing mass, whatever you do.",
            "You will join the matching pursuit.",
            "This would select exactly the same coordinates I, but then he would do it slightly different.",
            "Update it would change the accordion I to be best possibly aligned with the right inside be.",
            "So it's the same.",
            "It affects the same coordinates, but the update is slightly different, but still interesting to see that they correspond because left side.",
            "It is an optimization method and the right side is a greedy selection.",
            "Methods and."
        ],
        [
            "What what is even more interesting for me is that you can do a fully corrective variant of Frank Wolf.",
            "This is a fancy name of just saying that you re optimize over the corners that you have previously seen, so you've done K steps in each step you have moved towards one of the corners.",
            "And now we have not just moving towards the newest guy, but we are actually.",
            "In each step going to the best.",
            "Called the best entry in the comics after previously seen corners.",
            "That's the second variant.",
            "It is the same convergence guarantees essentially, but now this is actually.",
            "Fully equivalent to the orthogonal matching pursuit where you re optimize over the coordinates which you have seen in the previous matching pursuit steps.",
            "If.",
            "Did one board here is chosen large enough?",
            "There is a small catch here, but I hope that.",
            "Still, there might be some.",
            "So some useful applications of this kind of connections, and maybe there might be other.",
            "All the possibilities where the knowledge of the sparse methods might help us to understand the optimization methods and the other way around, and we actually hoping to to organize the workshop at this year's NIPS conference in December on trying to relate to more between these two kind of let."
        ],
        [
            "And that's all for the moments.",
            "Happy to answer any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to thank the organizers for.",
                    "label": 0
                },
                {
                    "sent": "Having me here, it's a great pleasure.",
                    "label": 0
                },
                {
                    "sent": "So as this workshop is about.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Like compromises on one hand.",
                    "label": 0
                },
                {
                    "sent": "And on sparse message, on the other hand.",
                    "label": 0
                },
                {
                    "sent": "We'd like to investigate a bit on the connections of these two fields and.",
                    "label": 0
                },
                {
                    "sent": "One example of.",
                    "label": 0
                },
                {
                    "sent": "Such a connection is.",
                    "label": 0
                },
                {
                    "sent": "Actually here if I would.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, is this two things?",
                    "label": 0
                },
                {
                    "sent": "So they're like to go?",
                    "label": 0
                },
                {
                    "sent": "2 problems here, but you want to study which is still supported machine from machine learning.",
                    "label": 0
                },
                {
                    "sent": "And then there is this other thing, which is the last so message from from sparse regression.",
                    "label": 0
                },
                {
                    "sent": "And this would be the example we will cover in the first part of the talk, and then in a small part at the end of a brief comment about greedy algorithms which look very similar in convex optimization.",
                    "label": 0
                },
                {
                    "sent": "On one hand, then on sparse.",
                    "label": 0
                },
                {
                    "sent": "Signal processing, on the other hand.",
                    "label": 1
                },
                {
                    "sent": "But the main part will be about last Windows VM, Santa Paper, and that is actually on archive and also in the.",
                    "label": 0
                },
                {
                    "sent": "In this workshop PDF.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what are these linear classifiers?",
                    "label": 0
                },
                {
                    "sent": "This is not the first time we're going to see this concept in this workshop and on.",
                    "label": 0
                },
                {
                    "sent": "Also not the last time we're going to see it, and so the setting is the following.",
                    "label": 0
                },
                {
                    "sent": "We given this digital images, for example, this our training data and if such image is represented by a point.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the dimension space, say.",
                    "label": 0
                },
                {
                    "sent": "There are two of these kinds and what we want.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Find this we're going to find the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Which separates these two kinds of points for the best possible.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Margin of separation.",
                    "label": 0
                },
                {
                    "sent": "So that's what we call a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "And in this talk here, what we?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to assume instead the hyperplanes are going through the origin.",
                    "label": 0
                },
                {
                    "sent": "That's not a big restriction, but the advantage of using only such planes is that.",
                    "label": 0
                },
                {
                    "sent": "If you do this, you only have to care about one kind of points, so we can just ignore group.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By considering the negatives instead.",
                    "label": 0
                },
                {
                    "sent": "So I've taken the blue points and mirrored them at the origin.",
                    "label": 0
                },
                {
                    "sent": "Now I have only one one set of points and I'm asking what's the best separating plane for that kind of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point set.",
                    "label": 0
                },
                {
                    "sent": "So let's think.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You want about this problem, so we want to find the best separating planes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is one shot.",
                    "label": 0
                },
                {
                    "sent": "Such candidate of a plane W is its normal vector and yellow area.",
                    "label": 0
                },
                {
                    "sent": "Here is the margin of separation which we get by this particular plane.",
                    "label": 0
                },
                {
                    "sent": "What I want to argue is that.",
                    "label": 0
                },
                {
                    "sent": "Finding the best separation is actually the same as finding the shortest point in this blue area.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is the polytope distance problem, so to say.",
                    "label": 1
                },
                {
                    "sent": "So the blue areas to come exam of all the points in our input.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We want to find the shortest point here which.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is given by this W star.",
                    "label": 0
                },
                {
                    "sent": "I claim that this W star this is the solution to both problems.",
                    "label": 0
                },
                {
                    "sent": "It's both the shortest point in the in the hole, and it's also the vector which best separates the points which gets the best margin.",
                    "label": 0
                },
                {
                    "sent": "So let's let's think about how we can formalize this distance problem because for this talk it will be enough to understand the distance problem, not have.",
                    "label": 0
                },
                {
                    "sent": "We don't have to worry about the margin maximization too much, which I will explain on the next slide.",
                    "label": 0
                },
                {
                    "sent": "Why this is the case.",
                    "label": 0
                },
                {
                    "sent": "So the distance problem can be written in two ways, here is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Love.",
                    "label": 0
                },
                {
                    "sent": "The possibilities we can just minimize the Euclidean squared norm.",
                    "label": 0
                },
                {
                    "sent": "Over the Hall.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or here's another way.",
                    "label": 0
                },
                {
                    "sent": "We will just say represent our points in the hole by convex combinations.",
                    "label": 1
                },
                {
                    "sent": "That's the definition of a convex Hull, so we have some nonnegative weights which I put in a vector X.",
                    "label": 0
                },
                {
                    "sent": "The Vector X listing and dimensions.",
                    "label": 0
                },
                {
                    "sent": "I'm given N many points and these entries of the vector accepted some to one, so there's a probability vectors.",
                    "label": 0
                },
                {
                    "sent": "Accessing the simplex in dimensions and a or my data points so I will collect all my data points in a matrix and every point will be a column of this matrix.",
                    "label": 0
                },
                {
                    "sent": "There will be D many rows and then many columns.",
                    "label": 0
                },
                {
                    "sent": "And here we are going to use work mainly in this formulation using the context representation 'cause those of you which are familiar to ACMS will will see that the first one is actually related to the primal supported machine.",
                    "label": 0
                },
                {
                    "sent": "Looks a bit similar versus this one.",
                    "label": 0
                },
                {
                    "sent": "Here is exactly the dual support vector machine and the points which appear with non zero weights in the solution to the problem.",
                    "label": 0
                },
                {
                    "sent": "So those which have non 0XI these are the support vectors, those which define the best solution.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me explain why we can actually restrict ourselves.",
                    "label": 0
                },
                {
                    "sent": "This promise of why is it interesting to work with this formulation?",
                    "label": 0
                },
                {
                    "sent": "Here is a small list of of ACM variants which have the property that exactly fit into the settings because they're dual problem is exactly of this form is quadratic minimization over the simplex, so this includes this as invariants.",
                    "label": 1
                },
                {
                    "sent": "If you're in a hard margin setting, if you assume that you later separable then.",
                    "label": 0
                },
                {
                    "sent": "You you have connect two or just one class or you can have the plane going to the origin or not if it's not going through the origin then there is a slight.",
                    "label": 0
                },
                {
                    "sent": "Issue that you the offsets the distance from the engine will also be regularised, but it's not a big deal, but.",
                    "label": 0
                },
                {
                    "sent": "The crucial thing here is that you cannot assume your data to be separable by early no function.",
                    "label": 1
                },
                {
                    "sent": "They will never be the case in practice.",
                    "label": 0
                },
                {
                    "sent": "So one of the core properties with responsible for the success of this kernel, methods that you can deal with outliers.",
                    "label": 0
                },
                {
                    "sent": "You can deal with non separable vectors, so that's why we are interested in the soft magic formulations.",
                    "label": 0
                },
                {
                    "sent": "Other points are allowed to lie on the wrong side of the plane, but if you're on the wrong side of the plane then it will be punished in the objective function.",
                    "label": 0
                },
                {
                    "sent": "And we consider those variants where your distance to the plane.",
                    "label": 0
                },
                {
                    "sent": "If you're on the wrong side, is the punished by the square of this distance there.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not there in which you just consider the hinge loss which the distance alone without a square.",
                    "label": 0
                },
                {
                    "sent": "This we do not cover here because they're not there.",
                    "label": 0
                },
                {
                    "sent": "Dual problem doesn't have this form, so just for completeness, here is an example.",
                    "label": 0
                },
                {
                    "sent": "It's actually this guy here, so that's the SCM Primal.",
                    "label": 0
                },
                {
                    "sent": "If you've probably seen it looks a bit technical, but if you want to take the dual of this quadratic program, you can do that and you can verify that you will get a problem with this form.",
                    "label": 0
                },
                {
                    "sent": "More important thing is that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This formulation is very obvious to kernel eyes, so you can just use a kernel where you you don't know the explicit features viewpoints.",
                    "label": 0
                },
                {
                    "sent": "You only know the inner products between pairs of points.",
                    "label": 0
                },
                {
                    "sent": "Because that's only what this optimization objective depends on.",
                    "label": 0
                },
                {
                    "sent": "So here's the objective.",
                    "label": 0
                },
                {
                    "sent": "I hope it can be seen.",
                    "label": 0
                },
                {
                    "sent": "It's a bit small, so the objective is only depending on the on the matrix agents voice a, so only the inner product between all pairs of columns.",
                    "label": 0
                },
                {
                    "sent": "Which you can just replace by the kernel matrix instead.",
                    "label": 0
                },
                {
                    "sent": "And then you will still have the same optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So what we want to remember from here is that from now on, this will be our iciam problem and.",
                    "label": 0
                },
                {
                    "sent": "Every assume instance will be given by such a matrix A where the data points are the columns.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is a completely different animal, so it's the lawsuit problem.",
                    "label": 0
                },
                {
                    "sent": "This is a regression method.",
                    "label": 0
                },
                {
                    "sent": "It's actually a different name, for it is actually the L1 regularised least squares regression.",
                    "label": 1
                },
                {
                    "sent": "So you want to approximate.",
                    "label": 0
                },
                {
                    "sent": "Some, some right inside B by some linear model AX.",
                    "label": 0
                },
                {
                    "sent": "That's a standard squared loss, but what you have is you have a regularization.",
                    "label": 0
                },
                {
                    "sent": "You add this term that your Wellness.",
                    "label": 0
                },
                {
                    "sent": "They should have small norms.",
                    "label": 0
                },
                {
                    "sent": "They should have small complexity to make your model robust, so you add an L1 constraint on your variables.",
                    "label": 0
                },
                {
                    "sent": "That's our regularization.",
                    "label": 0
                },
                {
                    "sent": "Another effect apart from being more robust, is that you will encourage sparse solutions.",
                    "label": 0
                },
                {
                    "sent": "Which is which was very.",
                    "label": 0
                },
                {
                    "sent": "Nice and his.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Greatly helped the dismissal to have so many applications.",
                    "label": 0
                },
                {
                    "sent": "They are like 2 interpretations of this same problem so you can interpret it as.",
                    "label": 0
                },
                {
                    "sent": "Column wise so to say.",
                    "label": 0
                },
                {
                    "sent": "If you think about what happens if he here we want to approximate our right inside vector B by few columns of this matrix A in this setting you usually call the matrix the dictionary matrix.",
                    "label": 0
                },
                {
                    "sent": "Or you can do the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can interpret the rows of the matrix of the same problem and you can think of this as.",
                    "label": 0
                },
                {
                    "sent": "Feature selection from because you you wanted each row of the matrix times you expect, or so that's the selection of feature you get from that row will approximate the corresponding B entry from the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Just two interpretations of the same problem.",
                    "label": 0
                },
                {
                    "sent": "I apologize if this is very basic.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is 1 technical catch we want to do instead of that one on being some bounded by some value T. I want one home to be bounded by one, so I want to be in DL1 unit ball.",
                    "label": 0
                },
                {
                    "sent": "That's my optimization domain.",
                    "label": 0
                },
                {
                    "sent": "This is not a restriction becausw you can just re scaling matrix A if you prefer to have some other value on the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "So that's our lawsuit problem.",
                    "label": 0
                },
                {
                    "sent": "And again, how is it defined?",
                    "label": 0
                },
                {
                    "sent": "So any instance is defined by matrix A and the right hand side vector B.",
                    "label": 0
                },
                {
                    "sent": "So this already looks quite similar to what we had as dual SVM formulation, so this is the primal assume, and this looks kind of similar to the other one except for the classifier.",
                    "label": 0
                },
                {
                    "sent": "We had a simplex domain and here it's now one ball.",
                    "label": 0
                },
                {
                    "sent": "And in the classifier we only had this X squared.",
                    "label": 0
                },
                {
                    "sent": "We didn't have this additional right inside Vectibix, so that's two differences between these problems.",
                    "label": 0
                },
                {
                    "sent": "And now we're trying to just overcome those to relate this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would be more together, so we want to.",
                    "label": 0
                },
                {
                    "sent": "To see that giving some.",
                    "label": 0
                },
                {
                    "sent": "Some primulas two instance, can we actually construct and assume instance of of this form which does the same thing?",
                    "label": 0
                },
                {
                    "sent": "So now I'm giving this last time given A&B and the question is can I come up with some matrix, a tiller?",
                    "label": 0
                },
                {
                    "sent": "In of whatever this consists such that the optimization problem is the same.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this direction is actually much easier than the other one, so here it's just easy.",
                    "label": 0
                },
                {
                    "sent": "You can just re Parameterise deal one ball by by using this positive coordinates instead or the barycentric coordinates, which is the same thing as we've done for the polytope.",
                    "label": 0
                },
                {
                    "sent": "Previously we talked about putting distance, but it's even simpler.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You just write that any vector in that one ball can be written as a bigger vector in the simplex, so this is twice as many coordinates times this matrix which order the vertices of the L1 Volt unit vectors.",
                    "label": 0
                },
                {
                    "sent": "Just maybe I'm making this more complicated than S.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Siri so.",
                    "label": 0
                },
                {
                    "sent": "Another way to see this for each variable you had previously, which was allowed to be non negative or positive.",
                    "label": 0
                },
                {
                    "sent": "Now you have two variables and table positive.",
                    "label": 0
                },
                {
                    "sent": "They sum up to one year in a simplex.",
                    "label": 0
                },
                {
                    "sent": "I would just plug this in into my last problem instead of this set at one.",
                    "label": 0
                },
                {
                    "sent": "I will plug in the simplex and my my larger variable like 2X prime.",
                    "label": 0
                },
                {
                    "sent": "This is the same thing and now I've basically I'm done already because now I have an SVM problem nearly the only thing that remains here is that there is a vector B which I need to get rid of.",
                    "label": 0
                },
                {
                    "sent": "But this is just translation so I can do this by by.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just translating each column of my matrix by vector B.",
                    "label": 0
                },
                {
                    "sent": "And then if I did find this to be my my classifier matrix, then this is exactly the equivalent problem.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by equivalent?",
                    "label": 0
                },
                {
                    "sent": "So I mean that so now you can give me any any feasible vector for the last two there, one born and then you automatically get one or several vectors in the simplex which keep the same objective for the sea.",
                    "label": 0
                },
                {
                    "sent": "And on the other way around is even simpler for any vector simplex.",
                    "label": 0
                },
                {
                    "sent": "For any classifier vector.",
                    "label": 0
                },
                {
                    "sent": "So to see you get exactly 1 back to I1 bowl.",
                    "label": 0
                },
                {
                    "sent": "So and this this guy will have the same objective value in the lawsuit.",
                    "label": 0
                },
                {
                    "sent": "So it's not only that these two problems have the same.",
                    "label": 0
                },
                {
                    "sent": "Optimal solution that is there is clear, but also all feasible solutions correspond sense, so this is nothing.",
                    "label": 0
                },
                {
                    "sent": "Nothing fancy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "To make this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way to see this I'm doing.",
                    "label": 0
                },
                {
                    "sent": "This overly complicated so just to see that what we've done previously in this formula source, we have written the last two as a distance problem.",
                    "label": 0
                },
                {
                    "sent": "And here is a way, but how?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can geometrically see this.",
                    "label": 0
                },
                {
                    "sent": "I have my original points a from the last matrix on the right side, the blue ones.",
                    "label": 0
                },
                {
                    "sent": "Their negatives are the red, the green ones on the left side.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I can take the comics after the union of those two, and when I claimed.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And is that the lawsuit problem is actually to compute the distance from this vector to the?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dot com itself the union.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And why is that?",
                    "label": 0
                },
                {
                    "sent": "I mean, we've already seen it or previous lovers just the geometric way to see it.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the set of points AX, 4X India one born?",
                    "label": 0
                },
                {
                    "sent": "It's just a the matrix a applied to the other one ball cheese to convicts out of the unit vectors.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "If you have a linear map, it doesn't matter if you do convex Hull after or before the linear map, so you can so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you have to decide this.",
                    "label": 0
                },
                {
                    "sent": "Actually, the convex Hull of of the Union of Direct sector center negatives.",
                    "label": 0
                },
                {
                    "sent": "I hope this makes a bit of sense.",
                    "label": 0
                },
                {
                    "sent": "Please interrupt if there are any questions that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the more interesting side is the the other direction.",
                    "label": 0
                },
                {
                    "sent": "This is a bit more complicated to now.",
                    "label": 0
                },
                {
                    "sent": "Can we do a reduction in the other way given an SVM can be constructed lasu instance which does the same thing.",
                    "label": 1
                },
                {
                    "sent": "The problem in this is that if you have a lawsuit and any any point which is part of this problem automatically, it's negative will also be part of the game and this you don't want for classifier.",
                    "label": 0
                },
                {
                    "sent": "So in a classifier, when you're giving some training example then it's negative is something completely else, so it's no way that you allowed to use that for your training.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So can we.",
                    "label": 0
                },
                {
                    "sent": "Can we find a way around this problem that can we get rid of these negatives and there is a?",
                    "label": 0
                },
                {
                    "sent": "A way to do this, but it's a bit.",
                    "label": 0
                },
                {
                    "sent": "A bit formula to the reduction is very simple if your ideas are formula, so I will just define my lawsuit instance.",
                    "label": 0
                },
                {
                    "sent": "This is translation of my assume points.",
                    "label": 0
                },
                {
                    "sent": "So the same matrix now.",
                    "label": 0
                },
                {
                    "sent": "So we're not changing the size of the matrix as we've done previously, so now I'm taking the same matrix A and I just translate each column by fixed vector and this vector is exactly is actually the right hand side of the zoo and.",
                    "label": 0
                },
                {
                    "sent": "And the question is just how should we choose these translation such that these problems are in a in any sensible way related?",
                    "label": 0
                },
                {
                    "sent": "And it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want to choose this translation.",
                    "label": 0
                },
                {
                    "sent": "In a way that.",
                    "label": 0
                },
                {
                    "sent": "The the translation Vector B is a W is weekly separating for the point sets for these points from the classifier.",
                    "label": 1
                },
                {
                    "sent": "So what does weekly separately mean?",
                    "label": 0
                },
                {
                    "sent": "It means that all points from the set have positive inner product.",
                    "label": 0
                },
                {
                    "sent": "This vector then these victories is suitable for our reduction.",
                    "label": 0
                },
                {
                    "sent": "So of course, this only works if you can find such a vector, and so I'm just saying that this is not a.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have to be the best separation, this is just any separation as long as everybody's very small amount of the right side, it doesn't have to be the maximum margin.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To understand this.",
                    "label": 0
                },
                {
                    "sent": "What does it actually mean?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can again go to gym to feel like, just to give you the intuition.",
                    "label": 0
                },
                {
                    "sent": "So here we have the points which we got from our classifier in the beginning.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is we want to formalize something like this so that we can find the best possible separation.",
                    "label": 0
                },
                {
                    "sent": "But now the problem is we have we have a lawsuit so the negatives for these guys will also be included.",
                    "label": 0
                },
                {
                    "sent": "So we want to be we want their negatives to be somewhere where they don't have a negative impact on us.",
                    "label": 0
                },
                {
                    "sent": "So we want these negatives to be far away.",
                    "label": 0
                },
                {
                    "sent": "From the interesting.",
                    "label": 0
                },
                {
                    "sent": "Region where something is happening and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will use our weakly separating vector for that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You just choose the origin, which is the only freedom we have by choosing the selector.",
                    "label": 0
                },
                {
                    "sent": "Be as to be far away from this weekly separating plane so we can choose here and the idea.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is if you do this, then the negatives of these points they will not influence your your problem in a bad way.",
                    "label": 0
                },
                {
                    "sent": "So I'll make this formula means, but this is the geometric idea.",
                    "label": 0
                },
                {
                    "sent": "I hope this makes any sense, and then the optimization objective will still be the same.",
                    "label": 0
                },
                {
                    "sent": "We just we will find the same W star vector will not have any contributions from this evil blue points.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's make this more former.",
                    "label": 0
                },
                {
                    "sent": "We're given the classifier we want to construct this lawsuit with the ATL and BTL.",
                    "label": 0
                },
                {
                    "sent": "And the result that we get from this construction is that the.",
                    "label": 0
                },
                {
                    "sent": "Distance to instance.",
                    "label": 0
                },
                {
                    "sent": "Every interesting feasible.",
                    "label": 0
                },
                {
                    "sent": "Point in that lawsuit will be positive anyway, so only the positive part of this lawsuit will be appearing in an existing solution.",
                    "label": 0
                },
                {
                    "sent": "We can do this would be more formal, it's it's a bit annoying, but you can say that for for any point, which is indeed one ball so feasible for the lawsuit.",
                    "label": 0
                },
                {
                    "sent": "There is a point which is positive, not negative entries, which is actually on the simplex, which is the same or better objective for the last two already.",
                    "label": 1
                },
                {
                    "sent": "And then what you can do is you can take it and plug it into the SCM and this will give you exactly the same objective objective.",
                    "label": 0
                },
                {
                    "sent": "So the other dealer direction is simple, so of course every point for the SCM.",
                    "label": 0
                },
                {
                    "sent": "Every point which is already simplex is feasible for the last two anyway, because the simplex is included in the L1 bowl.",
                    "label": 0
                },
                {
                    "sent": "So from the SCM Activella series, easy.",
                    "label": 0
                },
                {
                    "sent": "But if you are given some strange point of the lawsuit and you can say that there is a corresponding simplex point only using the blue points, which gives you the same or better objective.",
                    "label": 0
                },
                {
                    "sent": "And in particular, you will also get that this constructed Lasu now has the same optimal solution of course, and you also have in a sense that all feasible solutions correspond.",
                    "label": 0
                },
                {
                    "sent": "Or the interesting feasible solutions correspond for the two problems.",
                    "label": 0
                },
                {
                    "sent": "The only thing I didn't tell you yet is how do we find such a really separating vector?",
                    "label": 0
                },
                {
                    "sent": "Because otherwise it's pointless, we cannot.",
                    "label": 0
                },
                {
                    "sent": "Do the reduction.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The good thing is that for for the soft margin we have two loss, such a weekly separating vector is is is trivial to obtain, it's automatically.",
                    "label": 0
                },
                {
                    "sent": "Availability essentially constant vector, so there is no cost to get such a weekly separating vector.",
                    "label": 0
                },
                {
                    "sent": "Foreign tools.",
                    "label": 0
                },
                {
                    "sent": "If you have a hard margin SVM.",
                    "label": 0
                },
                {
                    "sent": "Then you might you have to run some nasty mob device to get such a such a vector, but you don't have to run it until the close to the optimum just until it becomes separating for the first time.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I hope this makes some sense.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What's what can this be used for?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Why would you want to do this anyway, so?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main motivation to look at this was that you can take out any algorithm for one of the two problems and apply to the respective order problem.",
                    "label": 1
                },
                {
                    "sent": "So this this find interesting because.",
                    "label": 0
                },
                {
                    "sent": "Dimensions for for ACMS has been developing in a very different directions than the optimizers, for for the last two or sparse problems, so I hope this might.",
                    "label": 0
                },
                {
                    "sent": "This might help to related to algorithm to see which one is what.",
                    "label": 0
                },
                {
                    "sent": "Are these properties which make the arguments being good for for this problem and how do they actually compare?",
                    "label": 0
                },
                {
                    "sent": "Like numerically, if you would want to do some experiments, how would they compare when applied to the respective other problem so?",
                    "label": 0
                },
                {
                    "sent": "But this is still.",
                    "label": 0
                },
                {
                    "sent": "To Do List.",
                    "label": 0
                },
                {
                    "sent": "I haven't done any experiments on this set, I'd be.",
                    "label": 0
                },
                {
                    "sent": "Interested to hear about this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The one classified Mr I find interesting is this class of the sub linear time methods which have been developed by Clarkson, Hassan, and Woodruff.",
                    "label": 0
                },
                {
                    "sent": "Like three years ago for the SVM and for the last two three years ago I guess.",
                    "label": 0
                },
                {
                    "sent": "But they have been developed independently, and the property here is that they take less time then it needs to read the input.",
                    "label": 0
                },
                {
                    "sent": "The input matrix has size N * T, but they only take time proportional to N, plus the switch is much smaller.",
                    "label": 0
                },
                {
                    "sent": "It will be interesting to see how they.",
                    "label": 0
                },
                {
                    "sent": "How do you work on the other pro?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, there is 1.",
                    "label": 0
                },
                {
                    "sent": "We can also look at one of the problems individually and see what what can we do for for that thing here now using the.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Production and for the loss of an interesting thing is securitization.",
                    "label": 0
                },
                {
                    "sent": "So now you have a lawsuit instance and you can just take the equivalence VM if you like, and you can ask what happens if you kernelized SVM.",
                    "label": 0
                },
                {
                    "sent": "And here is what's happening.",
                    "label": 0
                },
                {
                    "sent": "So previously you had.",
                    "label": 0
                },
                {
                    "sent": "You had this problem on the columns of A and the right inside be, but now these are mapped into kernel space.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "You have aggression problem, which is which is a Steelers squared loss.",
                    "label": 0
                },
                {
                    "sent": "But it's actually on the vectors in the kernel space.",
                    "label": 0
                },
                {
                    "sent": "So that.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's your objective and.",
                    "label": 0
                },
                {
                    "sent": "Again, it's.",
                    "label": 0
                },
                {
                    "sent": "The optimization problem is completely the same, but now it's just determined by the kernel inner products between the columns and also between these right hand side vector B.",
                    "label": 0
                },
                {
                    "sent": "And you can solve it by the same algorithms, But the question is here, what can it be used for?",
                    "label": 0
                },
                {
                    "sent": "This, I'd still be happy to hear more ideas from you if this could be useful.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you use a polynomial kernel, what would would you mean is that?",
                    "label": 0
                },
                {
                    "sent": "You will learn this regression not only approximating the.",
                    "label": 0
                },
                {
                    "sent": "Director B itself, but you would also use the higher moments of these columns AI to approximate the higher moments of B, which could be useful.",
                    "label": 0
                },
                {
                    "sent": "Maybe for some applications?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, for support vector machines we also have some.",
                    "label": 0
                },
                {
                    "sent": "Some interesting.",
                    "label": 0
                },
                {
                    "sent": "Implications and the one I like most is that.",
                    "label": 0
                },
                {
                    "sent": "What are the?",
                    "label": 0
                },
                {
                    "sent": "What is the set of support vectors?",
                    "label": 1
                },
                {
                    "sent": "But The thing is that by by looking at the query and lawsuit you can say that these are exactly the vectors for which the last Winston says non zero entries.",
                    "label": 0
                },
                {
                    "sent": "So the sparsity of these lawsuits is very well studied.",
                    "label": 0
                },
                {
                    "sent": "In signal processing there are many results for different classes of matrices A&B and the question is can we use some of these results to say something that how many support vectors do we have because it's the same number of the sparsity there is the same.",
                    "label": 0
                },
                {
                    "sent": "It's the number of support tickets we have for our classifiers, and if you would like this number to be small because the cost of using your classifier if it's like kernelized grows exactly with this number of support vectors.",
                    "label": 1
                },
                {
                    "sent": "So you want to understand this this number of each letter which.",
                    "label": 0
                },
                {
                    "sent": "I hope could be.",
                    "label": 0
                },
                {
                    "sent": "Possible here.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And another.",
                    "label": 0
                },
                {
                    "sent": "Nice concept which which was previously only introduced for the last two messages to conserve screening rules.",
                    "label": 0
                },
                {
                    "sent": "So this means a before you start any optimized for your problem, you just go through your data once and you try to identify some data points which will be guaranteed.",
                    "label": 1
                },
                {
                    "sent": "To be 0 anyway, in your optimal solution so you can discard them right away before starting the optimizer.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                },
                {
                    "sent": "So far was not was not done with SPMS, but I mean it's not the magic message.",
                    "label": 0
                },
                {
                    "sent": "It will not allow you to do have much much faster training, but it still might be an interesting thing to try and to.",
                    "label": 0
                },
                {
                    "sent": "To see if it could help the optimizer.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can use these by using our reduction, or you can also do it directly for the SVM without using this equivalence.",
                    "label": 0
                },
                {
                    "sent": "And here is the idea if you would want to use directly.",
                    "label": 0
                },
                {
                    "sent": "So here is some candidate separating vector from ISBN.",
                    "label": 0
                },
                {
                    "sent": "So can I use this to discard some of the points?",
                    "label": 0
                },
                {
                    "sent": "Which I will not need anyway.",
                    "label": 0
                },
                {
                    "sent": "You can do this.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you rotate the marching you had here, which is not the perfect one but just some margin, you think about all the possible ways to rotate this margin around your point W. And then.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There this Thursday is.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Red set of points is this cone, which consists of all the points which are outside the margin for any possible locations of this bad margin so.",
                    "label": 0
                },
                {
                    "sent": "I don't know the optimum watching, but I know that this is at least smaller than the optimal ones and that those points here they are on the wrong side of all possible placements of the margins, so they will not be support vectors ever, so I can just throw them away right away before starting my optimizer.",
                    "label": 0
                },
                {
                    "sent": "We could go.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go to a very short.",
                    "label": 0
                },
                {
                    "sent": "Second point, which has nothing to do with classifiers.",
                    "label": 0
                },
                {
                    "sent": "So this is only about sparse methods.",
                    "label": 0
                },
                {
                    "sent": "Is a bit related to the lawsuit because we want to study greedy algorithms and we want to.",
                    "label": 0
                },
                {
                    "sent": "To investigate it because they look very similar in the optimization side, if you want to minimize some continuous convex function on the left here and on the other hand in this sparse methods which just try to recover as farce solution from some noisy measurements.",
                    "label": 0
                },
                {
                    "sent": "So on the right hand side nothing is optimized, you just ask you what's the true solution to.",
                    "label": 0
                },
                {
                    "sent": "Two, what is your true signal which when you only got this corrupted measurements which are given by this P and you hope to tell someone close to the linear measurement AX?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's let's start with the.",
                    "label": 0
                },
                {
                    "sent": "Optimization, so here of course it's my my favorite algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is the probably the oldest algorithm for constrained convex optimization.",
                    "label": 1
                },
                {
                    "sent": "And it's the front Wolf message from the 50s.",
                    "label": 0
                },
                {
                    "sent": "What he does is.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It minimizes the convex function or very constrained domain.",
                    "label": 0
                },
                {
                    "sent": "Which no kisses the other one ball, and all function here.",
                    "label": 0
                },
                {
                    "sent": "The blue function is just a quadratic.",
                    "label": 0
                },
                {
                    "sent": "If we apply this thing to the lawsuit problem.",
                    "label": 0
                },
                {
                    "sent": "So what he does is it starts at some point X.",
                    "label": 0
                },
                {
                    "sent": "And the iteration goes like this that you think that the original function is still too complicated.",
                    "label": 0
                },
                {
                    "sent": "I want to minimize something.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simpler I do the linear approximation.",
                    "label": 0
                },
                {
                    "sent": "To my objective, as my current point and I will minimize this linear function instead over the same domain.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the update is that I will update my X Factor a bit towards the linear minimizer of.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Listing.",
                    "label": 0
                },
                {
                    "sent": "And then I repeat, so that's the whole algorithm.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is, if your domain is the one ball, then this extreme points where the linear minimum retained.",
                    "label": 0
                },
                {
                    "sent": "These are the vertices of course, and these are the unit basis vectors or the negatives if you like.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing here is that.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is the yeah, which is the vertex that attains the linear minimum.",
                    "label": 0
                },
                {
                    "sent": "That's actually the word tax where the gradient test maximum entry.",
                    "label": 0
                },
                {
                    "sent": "If you look at it, it's values.",
                    "label": 0
                },
                {
                    "sent": "So here F is our quadratic objective and we are changing towards this coordinate, which is the largest in the gradient which promises to give use the biggest change.",
                    "label": 0
                },
                {
                    "sent": "So that's our greedy interpretation of this optimization method.",
                    "label": 0
                },
                {
                    "sent": "And The funny thing is that.",
                    "label": 0
                },
                {
                    "sent": "The coordinate towards which we change this.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same as you would select by by the signal processing mass, whatever you do.",
                    "label": 1
                },
                {
                    "sent": "You will join the matching pursuit.",
                    "label": 1
                },
                {
                    "sent": "This would select exactly the same coordinates I, but then he would do it slightly different.",
                    "label": 0
                },
                {
                    "sent": "Update it would change the accordion I to be best possibly aligned with the right inside be.",
                    "label": 0
                },
                {
                    "sent": "So it's the same.",
                    "label": 0
                },
                {
                    "sent": "It affects the same coordinates, but the update is slightly different, but still interesting to see that they correspond because left side.",
                    "label": 0
                },
                {
                    "sent": "It is an optimization method and the right side is a greedy selection.",
                    "label": 0
                },
                {
                    "sent": "Methods and.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What what is even more interesting for me is that you can do a fully corrective variant of Frank Wolf.",
                    "label": 1
                },
                {
                    "sent": "This is a fancy name of just saying that you re optimize over the corners that you have previously seen, so you've done K steps in each step you have moved towards one of the corners.",
                    "label": 0
                },
                {
                    "sent": "And now we have not just moving towards the newest guy, but we are actually.",
                    "label": 0
                },
                {
                    "sent": "In each step going to the best.",
                    "label": 0
                },
                {
                    "sent": "Called the best entry in the comics after previously seen corners.",
                    "label": 0
                },
                {
                    "sent": "That's the second variant.",
                    "label": 0
                },
                {
                    "sent": "It is the same convergence guarantees essentially, but now this is actually.",
                    "label": 1
                },
                {
                    "sent": "Fully equivalent to the orthogonal matching pursuit where you re optimize over the coordinates which you have seen in the previous matching pursuit steps.",
                    "label": 1
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "Did one board here is chosen large enough?",
                    "label": 0
                },
                {
                    "sent": "There is a small catch here, but I hope that.",
                    "label": 0
                },
                {
                    "sent": "Still, there might be some.",
                    "label": 0
                },
                {
                    "sent": "So some useful applications of this kind of connections, and maybe there might be other.",
                    "label": 0
                },
                {
                    "sent": "All the possibilities where the knowledge of the sparse methods might help us to understand the optimization methods and the other way around, and we actually hoping to to organize the workshop at this year's NIPS conference in December on trying to relate to more between these two kind of let.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's all for the moments.",
                    "label": 0
                },
                {
                    "sent": "Happy to answer any questions.",
                    "label": 0
                }
            ]
        }
    }
}