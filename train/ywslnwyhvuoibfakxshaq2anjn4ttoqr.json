{
    "id": "ywslnwyhvuoibfakxshaq2anjn4ttoqr",
    "title": "The Rate of Convergence of AdaBoost",
    "info": {
        "author": [
            "Cynthia Rudin, Sloan School of Management, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Boosting"
        ]
    },
    "url": "http://videolectures.net/colt2011_rudin_rate/",
    "segmentation": [
        [
            "This talk is about the rate of convergence of Adaboost to minimizers of the exponential loss, and it's work with internal movie Jian Rob Japeri.",
            "The person who really deserves the honor of giving this talk is internal, but unfortunately he wasn't able to make it because of a visa issue."
        ],
        [
            "OK, so so as you know, Adaboost is one of the most popular machine learning algorithms, and it's definitely a very widely used algorithm, and it was named as one of the top 10 algorithms in data mining in 2008."
        ],
        [
            "And it's not only popular, but it's sort of proven.",
            "Its usefulness and the quote I grabbed here is from a pretty well known empirical comparison of many different machine learning algorithms on many different datasets using many different evaluation measures, and they found that Adaboost applied in a certain context, performed the best out of the ones they tried.",
            "So, so clearly we don't need much more of an excuse to study Adaboost convergence properties."
        ],
        [
            "But we we also have the very excellent excuse that even though there is a very large body of work on Adaboost and related boosting algorithms, basic properties of Adaboost, convergence are still not fully understood.",
            "And we're going to focus on one of these important properties here.",
            "Namely, we're going to find convergence rates that hold without any simplifying assumptions.",
            "It's much easier to find a convergence rate for Adaboost if you make some kind of strong assumption, but those assumptions just aren't necessarily true, so the convergence rates that I'm going to present now are really the only ones that hold for Adaboost in the way that it's really used."
        ],
        [
            "OK, so let me give you a little bit of background about Adaboost.",
            "Out of boost is known for its ability to combine a collection of weak classifiers into a strong classifier, but that's not the interpretation we're going to use today.",
            "We're going to use the coordinate descent view of Adaboost, which is that Adaboost is an iterative optimization algorithm.",
            "It minimizes what's called the exponential loss.",
            "And it uses the solution to the minimization problem in order to combine the weak classifiers.",
            "OK, so we start with a collection of examples with binary labels.",
            "And the set of weak hypothesis.",
            "And Adaboost produces a linear combination of the hypothesis.",
            "And the prediction made by this combination is the weighted majority vote where the weights are the lambdas.",
            "And Adaboost chooses those weights by minimizing an upper bound for the misclassification error, which is the exponential loss."
        ],
        [
            "OK, so now if I plug the formula for F into the formula for the exponential loss.",
            "I get this function of the coefficient vector Lambda.",
            "So given the examples and hypothesis, the exponential loss is just a function of the of the vector Lambda."
        ],
        [
            "OK, so Adaboost is a coordinate descent algorithm, and it iteratively updates one component of Linda at a time to move toward the minimizer of the exponential loss, and we're trying to figure out how fast that happens, and most of the time we picture, you know, sliding along one coordinate at a time until we reach a finite minimizer, like like I've shown here.",
            "But this is the kind of Contacts landscape that we usually think about when we talk about.",
            "Gradient descent algorithms the tricky."
        ],
        [
            "Is that sometimes it's possible to get a landscape like this where the where the minimizer is offered Infinity?",
            "And the minimizer is not just any old infinite value, but rather it's.",
            "It's achieved by following following a particular trajectory or Valley out to Infinity here.",
            "So you could follow it that way.",
            "And this means we're not working over a compact space."
        ],
        [
            "OK, now what's known about this problem.",
            "It's known that Adaboost does asymptotically minimize the exponential loss, but it's not clear how fast.",
            "And I should say, of course, that minimizing the exponential loss is not necessarily enough to ensure good generalization, But that's what Adaboost does.",
            "OK, and there are various convergence rates for Adaboost and for variants of Adaboost, but most of them make some kind of additional assumption in order to get a rate.",
            "So for instance, when what's called the weak learning assumption holds, it amounts to assuming that Adaboost takes a big enough coordinate descent step at every iteration, which leads to a very fast convergence rate, exponentially faster than the ones we present here.",
            "But of course, the weak learning assumption doesn't necessarily hold when you're combining a bunch of features.",
            "And another assumption is that a finite finite minimizer exists.",
            "And there are many classic results addressing convergence rates for a coordinate descent when a finite minimizer exists, and in fact it's almost standard to assume a finite minimizer exists and that we're working over a compact space.",
            "But for Adaboost it's very easy to find examples where that's just not true.",
            "And last year Rob posed a formal conjecture that fast rates of convergence hold for Adaboost without any of these untrue assumptions, and that's what we're working on.",
            "And also finding a convergence rate for Adaboost is direct directly relevant for showing how fast Adaboost converges to a Bayes optimal classifier."
        ],
        [
            "OK, So what I'm going to do now is outline two convergence rates.",
            "The first one handles the question of polynomial convergence to within epsilon of a reference solution.",
            "And the second rate talks about how many iterations we need to get within epsilon of optimal.",
            "So they're two different types of rates.",
            "Ann, you should keep in mind that both types of rates are useful.",
            "You can't use the first rate to get the second one because there may not be any reference solution for the optimal loss.",
            "And you can't.",
            "You can't use the second one to get the first one, because the second one in some sense involves the complexity of the solutions that are within epsilon of optimal, and that can be much larger than the complexity of a reference solution.",
            "So basically the first one is in some sense more polynomial than the second one, so you want both types of rates."
        ],
        [
            "OK, and the talk is about Adaboost, but it has messages that are broader and that are relevant to optimization more generally.",
            "So typically convergence rates and convergence rate analysis begin by assuming that there exists a finite minimizer, and when this assumption doesn't hold, the problem of proving convergence rates can be considerably more challenging, but but we give techniques that can be applied even in that case.",
            "And also we've separated two different types of rates, so comparison to reference versus comparison to optimal, and we've shown that the different rather different convergence rates can be possible in each.",
            "And also analysis of convergence rates often, but don't always ignore the constants and we've shown that these constants can be extremely large in the worst case, and that's why both kinds of convergence rates are valuable, because we can sometimes do better with respect to one than the other."
        ],
        [
            "OK, so let's start with the first rate.",
            "The first rate was based on Rob's open problem and that open problem said that we want to find something that looks like this."
        ],
        [
            "So at iteration T, the exponential loss of Adaboost coefficient vector Lambda T. Will be at most epsilon more than that of any parameter vector of L1 norm bounded by be in a number of rounds.",
            "That's at most a polynomial and be in one over epsilon.",
            "Well, log in and be in one over epsilon actually, so that's a bit of a mouthful, so let me let me break it down a little bit.",
            "OK, so interpret this.",
            "You should think about an L1 ball of size B and pick some parameter vector Lambda star whose whose radius is less than B."
        ],
        [
            "And you should think about Lambda Star as being the best solution within the ball of radius B.",
            "So it's a solution that possibly has the lowest exponential loss within the ball."
        ],
        [
            "And now think about the coefficient vector Lambda T at some iteration of Adaboost and we want to make sure that the exponential loss of Lambda T is not too much more than that of Lambda star."
        ],
        [
            "In fact, we don't want it to be more than epsilon away.",
            "And when you're in this circumstance, the conjecture says that when that.",
            "The number of rounds T that it took Adaboost."
        ],
        [
            "To get to this point.",
            "Is it most a polynomial in the log of the number of hypothesis, so log in and then the number of examples M, radius B in one over epsilon?"
        ],
        [
            "OK, now I drew the exponential loss is having a finite minimizer, but it really doesn't need to have one."
        ],
        [
            "And I also drew Lambda T to have a norm less than B and this also doesn't need to be true."
        ],
        [
            "OK, so the result is that if you if you give the reference vector Lambda star.",
            "Then Adam boosts losses within epsilon of Lambda stars loss and at most that many rounds.",
            "And that's our polynomial.",
            "And it doesn't depend on log in or M at all, except maybe through implicitly through the norm of Lambda star, and so the dependence is polynomial in it is polynomial in one over epsilon, it's epsilon to the negative 5th.",
            "But it's still polynomial.",
            "And the best known previous result was exponential in one over epsilon squared, and there's just no comparison.",
            "The new rate is much faster, and so we're going to talk about this dependence on epsilon a lot more.",
            "OK."
        ],
        [
            "And again, this convergence rate considers how long it takes until we get to within epsilon of the solution of some Lambda star.",
            "And the dependence on the norm of Lambda star is actually kind of important.",
            "For instance, it's pretty easy to generate a data set where if you want to, if you want to get to a certain loss value L star, then the number of rounds you need depends directly on the size of the smallest vector achieving elstar, and that's what this lemma says.",
            "It says there are simple datasets for which the number of rounds required to achieve Lascelles star is at least roughly the norm of the smallest solution, achieving Elsa."
        ],
        [
            "And the official version of that lemma is is here.",
            "And the problem with this is that the size of this of this thing in the numerator here this.",
            "The size of the smallest solution achieving elstar can be on the order of two to the UMP, so exponential in the number of examples."
        ],
        [
            "So the official version of that lemma is here.",
            "So if you try to put the theorem in terms of M, you won't get a polynomial in M. So in order to get polynomial dependence, it needs to be in terms of the size of the reference solution Lambda star."
        ],
        [
            "OK, and on the other hand we believe it should be possible to get better dependence on epsilon.",
            "OK, so specifically we conjecture that epsilon that Adaboost achieves last at most the loss of Lambda Star plus epsilon in at most order B squared over epsilon rounds, where the order notation hides only absolute constants."
        ],
        [
            "Alright, so here's an example of why we think the convergence should be that fast, so this is from a real data set and this is epsilon on the Y axis and the number of rounds on the X axis.",
            "And.",
            "The number of rounds is increasing as as one over epsilon, and so in the next result we do get the bound depends on one of our epsilon, so that's that's a very good reason why we think."
        ],
        [
            "One over epsilon.",
            "OK, so for the second convergence rate."
        ],
        [
            "The result is that Adaboost gets within epsilon of the optimal loss, and at most C over epsilon rounds where C only depends on the data.",
            "And Theorem one had dependence epsilon to the negative 5th, so this one is better in terms of epsilon.",
            "And this solution doesn't depend on the best solution within a ball, but it can't be used to prove the conjecture because see the see here.",
            "Can be.",
            "Larger than two to the M in the worst case, and most of the time it's not most of the time it's much smaller, but it depends on the data.",
            "So in the worst case, it could be 2 to the."
        ],
        [
            "OK, so the main tool for proving this, we call the decomposition lemma.",
            "And it says that each of the examples fall into one of two categories, the zero loss at Z or the finite margin set F An let me give you some intuition behind those two sets."
        ],
        [
            "OK, so he."
        ],
        [
            "Are my examples and here's my decision boundary.",
            "And there's the finite margin set.",
            "Examples in the finite margin set never get too far away from the decision boundary, no matter what vector lamb do you use, as long as the loss isn't too large."
        ],
        [
            "And the zero loss that is the other part.",
            "And these examples get they get further and further away from the decision boundary.",
            "So essentially the finite margin set forms the hardcore of the examples and you might not be able to separate them.",
            "Well, no matter what algorithm you choose.",
            "The main part of the learning problem is the optimization on the finite margin set.",
            "And similarly, the zero last set is the easy part.",
            "Those examples can be perfectly separated, so Adaboost separates those and then as its solution grows, it pushes the zero lossett further and further away and its contribution to the exponential loss goes to 0."
        ],
        [
            "OK, so here's the decomposition lemma.",
            "The lemma not only guarantees the partition into ZF, but also explains that solving the optimization problem on all examples decomposes into separately solving an optimization problem on Z and then another one on F. So in other words, we can.",
            "We can completely solve the zero last part without touching the finite margin set and then what's left is the optimization problem on the finite margin set.",
            "Alright, so let me let me just read this so it says for any data set there exists a partition of the training examples and dizzy enough such that these two things hold simultaneously and the first one is that for some gamma there exists a vector 8 plus such that and then this says the margins are at least gamma in Z and this one says that examples enough have zero margins.",
            "So let me repeat."
        ],
        [
            "Again.",
            "There's some number gamma.",
            "Such that all of the examples in the zero last set have margin at least gamma.",
            "So here's the zero lossett.",
            "Emergent at least gamma according to Ada.",
            "Plus and then all the examples in FY right on the decision boundary."
        ],
        [
            "OK, and then the second bit is that the optimal loss considering only examples enough is achieved by some finite ETA star.",
            "OK, and the winning combination is is 8 star plus Infinity times 88 plus."
        ],
        [
            "OK, and this is the same thing I said earlier.",
            "If you ignore the zero law set and you look just at the finite margin set."
        ],
        [
            "Then there's a finite minimizer Ada star on the finite margins."
        ],
        [
            "And so there's a tostar."
        ],
        [
            "And again, the decomposition lemma is the main step in proving Theorem 2."
        ],
        [
            "OK, now as I mentioned, the dependence on epsilon is just what we want in the second theorem.",
            "But of course, that constancy can be really large.",
            "In fact, it can be doubly exponential in the number of training examples.",
            "But that doesn't mean that it's always going to be that way, just it just means that there might be some room for improvement in our analysis.",
            "So in the paper we stayed a conjecture regarding the dependence on M. We think that if the hypothesis take on the values taken, only the values negative 1 zero and one.",
            "Then add a boost converges to within epsilon of the optimal loss within within this many rounds.",
            "OK, so it's only singly exponential, not doubly, and that dependence would be essentially optimal, meaning that we have a lower bound that essentially matches this."
        ],
        [
            "Alright, so to summarize, we presented two rate bounds, one that depends on the size of the best solution within a ball and has dependence epsilon to the negative 5th.",
            "The other one depends only on C over epsilon, but she can be doubly exponential in M. And we in the paper we we have many lower bounds in conjectures that are aimed to show where our analysis is tight and where there might be room for improvement.",
            "So there so there's definitely more exciting work to be done on this problem.",
            "Thank you.",
            "Any questions?",
            "So your analysis is essentially the analysis of minimizing a coordinate wise descent algorithm, and I was wondering if you try to so this is really adapted to the learning for boosting.",
            "Did you were able to extract some fundamental properties of the convex function you want to minimize so that you can actually translate those rates to a more general set up?",
            "Yeah, so that's a really good question.",
            "Yeah, there so.",
            "In the paper, we actually highlight sort of where where the analysis actually applies more generally.",
            "So for instance.",
            "There's this at this very well known old fact that Adaboost stepsize, which is sort of that Delta T there has something to do with its progress, and in fact it has a lot to do with its progress.",
            "It completely determines its progress, and so a lot of our results actually hold whenever that inequality holds, and it holds for Adaboost, but it holds more generally as well, for instance.",
            "So perhaps the opposite question.",
            "So if you're viewing Adaboost, is a optimization last minimization with the dependence in the L1 norm?",
            "How does it compare to other even coordinate methods for minimizing an objective with an L1 constraint or with dependents and L1 and the rate?",
            "OK, so if you have, if you have an L1 constraint, you have a finite minimizer, but I've never in the first part of your analysis where you did do that you did have.",
            "No one constraint.",
            "Oh, you mean to the reference solution with the with the L1 constraint?",
            "So so what's what's the?",
            "I'm sorry?",
            "How does it compare to other methods for minimizing an objective subject to no one constraint?",
            "Let's see, that's a tricky question I. I'm thinking maybe I should handle that one offline because it's a little bit complicated, but.",
            "It's I don't.",
            "So the answer is I know a little bit, but I don't know quite quite the right answer.",
            "So I think maybe I'll take it offline.",
            "Any other questions?",
            "OK, I had this small question actually in your conjecture conjecture on the running time with a depends on B, like the diameter of the L1 bowl and epsilon.",
            "You had different different exponentials like B ^2 versus epsilon and it would seem like if you scale all the losses by certain.",
            "Yeah no, no, this is exponentially.",
            "I don't know 15 slides ago, so it seems like if you scale all."
        ],
        [
            "The payoff by factor of 10.",
            "You'll need epsilon to be divided by 10 right, but with the ball be scaled by 100 or.",
            "Sorry, I mean I would imagine it would be over epsilon not be squared over epsilon I guess.",
            "So sorry, so you're saying you think it should be because this will be my.",
            "I don't know anything about your problem, but that would be my.",
            "My guess?",
            "So you think OK, just because of the scaling scaling works this way?",
            "OK.",
            "I'll take it offline, so thank let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This talk is about the rate of convergence of Adaboost to minimizers of the exponential loss, and it's work with internal movie Jian Rob Japeri.",
                    "label": 0
                },
                {
                    "sent": "The person who really deserves the honor of giving this talk is internal, but unfortunately he wasn't able to make it because of a visa issue.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so as you know, Adaboost is one of the most popular machine learning algorithms, and it's definitely a very widely used algorithm, and it was named as one of the top 10 algorithms in data mining in 2008.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's not only popular, but it's sort of proven.",
                    "label": 0
                },
                {
                    "sent": "Its usefulness and the quote I grabbed here is from a pretty well known empirical comparison of many different machine learning algorithms on many different datasets using many different evaluation measures, and they found that Adaboost applied in a certain context, performed the best out of the ones they tried.",
                    "label": 0
                },
                {
                    "sent": "So, so clearly we don't need much more of an excuse to study Adaboost convergence properties.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we we also have the very excellent excuse that even though there is a very large body of work on Adaboost and related boosting algorithms, basic properties of Adaboost, convergence are still not fully understood.",
                    "label": 1
                },
                {
                    "sent": "And we're going to focus on one of these important properties here.",
                    "label": 0
                },
                {
                    "sent": "Namely, we're going to find convergence rates that hold without any simplifying assumptions.",
                    "label": 0
                },
                {
                    "sent": "It's much easier to find a convergence rate for Adaboost if you make some kind of strong assumption, but those assumptions just aren't necessarily true, so the convergence rates that I'm going to present now are really the only ones that hold for Adaboost in the way that it's really used.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me give you a little bit of background about Adaboost.",
                    "label": 0
                },
                {
                    "sent": "Out of boost is known for its ability to combine a collection of weak classifiers into a strong classifier, but that's not the interpretation we're going to use today.",
                    "label": 1
                },
                {
                    "sent": "We're going to use the coordinate descent view of Adaboost, which is that Adaboost is an iterative optimization algorithm.",
                    "label": 1
                },
                {
                    "sent": "It minimizes what's called the exponential loss.",
                    "label": 0
                },
                {
                    "sent": "And it uses the solution to the minimization problem in order to combine the weak classifiers.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start with a collection of examples with binary labels.",
                    "label": 0
                },
                {
                    "sent": "And the set of weak hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And Adaboost produces a linear combination of the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And the prediction made by this combination is the weighted majority vote where the weights are the lambdas.",
                    "label": 0
                },
                {
                    "sent": "And Adaboost chooses those weights by minimizing an upper bound for the misclassification error, which is the exponential loss.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now if I plug the formula for F into the formula for the exponential loss.",
                    "label": 0
                },
                {
                    "sent": "I get this function of the coefficient vector Lambda.",
                    "label": 0
                },
                {
                    "sent": "So given the examples and hypothesis, the exponential loss is just a function of the of the vector Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so Adaboost is a coordinate descent algorithm, and it iteratively updates one component of Linda at a time to move toward the minimizer of the exponential loss, and we're trying to figure out how fast that happens, and most of the time we picture, you know, sliding along one coordinate at a time until we reach a finite minimizer, like like I've shown here.",
                    "label": 0
                },
                {
                    "sent": "But this is the kind of Contacts landscape that we usually think about when we talk about.",
                    "label": 0
                },
                {
                    "sent": "Gradient descent algorithms the tricky.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that sometimes it's possible to get a landscape like this where the where the minimizer is offered Infinity?",
                    "label": 0
                },
                {
                    "sent": "And the minimizer is not just any old infinite value, but rather it's.",
                    "label": 0
                },
                {
                    "sent": "It's achieved by following following a particular trajectory or Valley out to Infinity here.",
                    "label": 0
                },
                {
                    "sent": "So you could follow it that way.",
                    "label": 0
                },
                {
                    "sent": "And this means we're not working over a compact space.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now what's known about this problem.",
                    "label": 0
                },
                {
                    "sent": "It's known that Adaboost does asymptotically minimize the exponential loss, but it's not clear how fast.",
                    "label": 1
                },
                {
                    "sent": "And I should say, of course, that minimizing the exponential loss is not necessarily enough to ensure good generalization, But that's what Adaboost does.",
                    "label": 0
                },
                {
                    "sent": "OK, and there are various convergence rates for Adaboost and for variants of Adaboost, but most of them make some kind of additional assumption in order to get a rate.",
                    "label": 0
                },
                {
                    "sent": "So for instance, when what's called the weak learning assumption holds, it amounts to assuming that Adaboost takes a big enough coordinate descent step at every iteration, which leads to a very fast convergence rate, exponentially faster than the ones we present here.",
                    "label": 1
                },
                {
                    "sent": "But of course, the weak learning assumption doesn't necessarily hold when you're combining a bunch of features.",
                    "label": 0
                },
                {
                    "sent": "And another assumption is that a finite finite minimizer exists.",
                    "label": 0
                },
                {
                    "sent": "And there are many classic results addressing convergence rates for a coordinate descent when a finite minimizer exists, and in fact it's almost standard to assume a finite minimizer exists and that we're working over a compact space.",
                    "label": 1
                },
                {
                    "sent": "But for Adaboost it's very easy to find examples where that's just not true.",
                    "label": 0
                },
                {
                    "sent": "And last year Rob posed a formal conjecture that fast rates of convergence hold for Adaboost without any of these untrue assumptions, and that's what we're working on.",
                    "label": 0
                },
                {
                    "sent": "And also finding a convergence rate for Adaboost is direct directly relevant for showing how fast Adaboost converges to a Bayes optimal classifier.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what I'm going to do now is outline two convergence rates.",
                    "label": 0
                },
                {
                    "sent": "The first one handles the question of polynomial convergence to within epsilon of a reference solution.",
                    "label": 1
                },
                {
                    "sent": "And the second rate talks about how many iterations we need to get within epsilon of optimal.",
                    "label": 0
                },
                {
                    "sent": "So they're two different types of rates.",
                    "label": 0
                },
                {
                    "sent": "Ann, you should keep in mind that both types of rates are useful.",
                    "label": 0
                },
                {
                    "sent": "You can't use the first rate to get the second one because there may not be any reference solution for the optimal loss.",
                    "label": 0
                },
                {
                    "sent": "And you can't.",
                    "label": 0
                },
                {
                    "sent": "You can't use the second one to get the first one, because the second one in some sense involves the complexity of the solutions that are within epsilon of optimal, and that can be much larger than the complexity of a reference solution.",
                    "label": 0
                },
                {
                    "sent": "So basically the first one is in some sense more polynomial than the second one, so you want both types of rates.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and the talk is about Adaboost, but it has messages that are broader and that are relevant to optimization more generally.",
                    "label": 0
                },
                {
                    "sent": "So typically convergence rates and convergence rate analysis begin by assuming that there exists a finite minimizer, and when this assumption doesn't hold, the problem of proving convergence rates can be considerably more challenging, but but we give techniques that can be applied even in that case.",
                    "label": 0
                },
                {
                    "sent": "And also we've separated two different types of rates, so comparison to reference versus comparison to optimal, and we've shown that the different rather different convergence rates can be possible in each.",
                    "label": 1
                },
                {
                    "sent": "And also analysis of convergence rates often, but don't always ignore the constants and we've shown that these constants can be extremely large in the worst case, and that's why both kinds of convergence rates are valuable, because we can sometimes do better with respect to one than the other.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's start with the first rate.",
                    "label": 0
                },
                {
                    "sent": "The first rate was based on Rob's open problem and that open problem said that we want to find something that looks like this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at iteration T, the exponential loss of Adaboost coefficient vector Lambda T. Will be at most epsilon more than that of any parameter vector of L1 norm bounded by be in a number of rounds.",
                    "label": 1
                },
                {
                    "sent": "That's at most a polynomial and be in one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "Well, log in and be in one over epsilon actually, so that's a bit of a mouthful, so let me let me break it down a little bit.",
                    "label": 0
                },
                {
                    "sent": "OK, so interpret this.",
                    "label": 0
                },
                {
                    "sent": "You should think about an L1 ball of size B and pick some parameter vector Lambda star whose whose radius is less than B.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you should think about Lambda Star as being the best solution within the ball of radius B.",
                    "label": 0
                },
                {
                    "sent": "So it's a solution that possibly has the lowest exponential loss within the ball.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now think about the coefficient vector Lambda T at some iteration of Adaboost and we want to make sure that the exponential loss of Lambda T is not too much more than that of Lambda star.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In fact, we don't want it to be more than epsilon away.",
                    "label": 1
                },
                {
                    "sent": "And when you're in this circumstance, the conjecture says that when that.",
                    "label": 0
                },
                {
                    "sent": "The number of rounds T that it took Adaboost.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get to this point.",
                    "label": 0
                },
                {
                    "sent": "Is it most a polynomial in the log of the number of hypothesis, so log in and then the number of examples M, radius B in one over epsilon?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now I drew the exponential loss is having a finite minimizer, but it really doesn't need to have one.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I also drew Lambda T to have a norm less than B and this also doesn't need to be true.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the result is that if you if you give the reference vector Lambda star.",
                    "label": 1
                },
                {
                    "sent": "Then Adam boosts losses within epsilon of Lambda stars loss and at most that many rounds.",
                    "label": 0
                },
                {
                    "sent": "And that's our polynomial.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't depend on log in or M at all, except maybe through implicitly through the norm of Lambda star, and so the dependence is polynomial in it is polynomial in one over epsilon, it's epsilon to the negative 5th.",
                    "label": 0
                },
                {
                    "sent": "But it's still polynomial.",
                    "label": 0
                },
                {
                    "sent": "And the best known previous result was exponential in one over epsilon squared, and there's just no comparison.",
                    "label": 1
                },
                {
                    "sent": "The new rate is much faster, and so we're going to talk about this dependence on epsilon a lot more.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again, this convergence rate considers how long it takes until we get to within epsilon of the solution of some Lambda star.",
                    "label": 0
                },
                {
                    "sent": "And the dependence on the norm of Lambda star is actually kind of important.",
                    "label": 0
                },
                {
                    "sent": "For instance, it's pretty easy to generate a data set where if you want to, if you want to get to a certain loss value L star, then the number of rounds you need depends directly on the size of the smallest vector achieving elstar, and that's what this lemma says.",
                    "label": 0
                },
                {
                    "sent": "It says there are simple datasets for which the number of rounds required to achieve Lascelles star is at least roughly the norm of the smallest solution, achieving Elsa.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the official version of that lemma is is here.",
                    "label": 0
                },
                {
                    "sent": "And the problem with this is that the size of this of this thing in the numerator here this.",
                    "label": 0
                },
                {
                    "sent": "The size of the smallest solution achieving elstar can be on the order of two to the UMP, so exponential in the number of examples.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the official version of that lemma is here.",
                    "label": 0
                },
                {
                    "sent": "So if you try to put the theorem in terms of M, you won't get a polynomial in M. So in order to get polynomial dependence, it needs to be in terms of the size of the reference solution Lambda star.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and on the other hand we believe it should be possible to get better dependence on epsilon.",
                    "label": 0
                },
                {
                    "sent": "OK, so specifically we conjecture that epsilon that Adaboost achieves last at most the loss of Lambda Star plus epsilon in at most order B squared over epsilon rounds, where the order notation hides only absolute constants.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so here's an example of why we think the convergence should be that fast, so this is from a real data set and this is epsilon on the Y axis and the number of rounds on the X axis.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The number of rounds is increasing as as one over epsilon, and so in the next result we do get the bound depends on one of our epsilon, so that's that's a very good reason why we think.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One over epsilon.",
                    "label": 0
                },
                {
                    "sent": "OK, so for the second convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The result is that Adaboost gets within epsilon of the optimal loss, and at most C over epsilon rounds where C only depends on the data.",
                    "label": 1
                },
                {
                    "sent": "And Theorem one had dependence epsilon to the negative 5th, so this one is better in terms of epsilon.",
                    "label": 0
                },
                {
                    "sent": "And this solution doesn't depend on the best solution within a ball, but it can't be used to prove the conjecture because see the see here.",
                    "label": 1
                },
                {
                    "sent": "Can be.",
                    "label": 0
                },
                {
                    "sent": "Larger than two to the M in the worst case, and most of the time it's not most of the time it's much smaller, but it depends on the data.",
                    "label": 0
                },
                {
                    "sent": "So in the worst case, it could be 2 to the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the main tool for proving this, we call the decomposition lemma.",
                    "label": 0
                },
                {
                    "sent": "And it says that each of the examples fall into one of two categories, the zero loss at Z or the finite margin set F An let me give you some intuition behind those two sets.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so he.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are my examples and here's my decision boundary.",
                    "label": 0
                },
                {
                    "sent": "And there's the finite margin set.",
                    "label": 0
                },
                {
                    "sent": "Examples in the finite margin set never get too far away from the decision boundary, no matter what vector lamb do you use, as long as the loss isn't too large.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the zero loss that is the other part.",
                    "label": 0
                },
                {
                    "sent": "And these examples get they get further and further away from the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "So essentially the finite margin set forms the hardcore of the examples and you might not be able to separate them.",
                    "label": 0
                },
                {
                    "sent": "Well, no matter what algorithm you choose.",
                    "label": 0
                },
                {
                    "sent": "The main part of the learning problem is the optimization on the finite margin set.",
                    "label": 1
                },
                {
                    "sent": "And similarly, the zero last set is the easy part.",
                    "label": 0
                },
                {
                    "sent": "Those examples can be perfectly separated, so Adaboost separates those and then as its solution grows, it pushes the zero lossett further and further away and its contribution to the exponential loss goes to 0.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the decomposition lemma.",
                    "label": 0
                },
                {
                    "sent": "The lemma not only guarantees the partition into ZF, but also explains that solving the optimization problem on all examples decomposes into separately solving an optimization problem on Z and then another one on F. So in other words, we can.",
                    "label": 0
                },
                {
                    "sent": "We can completely solve the zero last part without touching the finite margin set and then what's left is the optimization problem on the finite margin set.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me let me just read this so it says for any data set there exists a partition of the training examples and dizzy enough such that these two things hold simultaneously and the first one is that for some gamma there exists a vector 8 plus such that and then this says the margins are at least gamma in Z and this one says that examples enough have zero margins.",
                    "label": 1
                },
                {
                    "sent": "So let me repeat.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "There's some number gamma.",
                    "label": 0
                },
                {
                    "sent": "Such that all of the examples in the zero last set have margin at least gamma.",
                    "label": 0
                },
                {
                    "sent": "So here's the zero lossett.",
                    "label": 0
                },
                {
                    "sent": "Emergent at least gamma according to Ada.",
                    "label": 0
                },
                {
                    "sent": "Plus and then all the examples in FY right on the decision boundary.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then the second bit is that the optimal loss considering only examples enough is achieved by some finite ETA star.",
                    "label": 0
                },
                {
                    "sent": "OK, and the winning combination is is 8 star plus Infinity times 88 plus.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and this is the same thing I said earlier.",
                    "label": 0
                },
                {
                    "sent": "If you ignore the zero law set and you look just at the finite margin set.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then there's a finite minimizer Ada star on the finite margins.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so there's a tostar.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, the decomposition lemma is the main step in proving Theorem 2.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now as I mentioned, the dependence on epsilon is just what we want in the second theorem.",
                    "label": 0
                },
                {
                    "sent": "But of course, that constancy can be really large.",
                    "label": 0
                },
                {
                    "sent": "In fact, it can be doubly exponential in the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "But that doesn't mean that it's always going to be that way, just it just means that there might be some room for improvement in our analysis.",
                    "label": 0
                },
                {
                    "sent": "So in the paper we stayed a conjecture regarding the dependence on M. We think that if the hypothesis take on the values taken, only the values negative 1 zero and one.",
                    "label": 0
                },
                {
                    "sent": "Then add a boost converges to within epsilon of the optimal loss within within this many rounds.",
                    "label": 1
                },
                {
                    "sent": "OK, so it's only singly exponential, not doubly, and that dependence would be essentially optimal, meaning that we have a lower bound that essentially matches this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so to summarize, we presented two rate bounds, one that depends on the size of the best solution within a ball and has dependence epsilon to the negative 5th.",
                    "label": 1
                },
                {
                    "sent": "The other one depends only on C over epsilon, but she can be doubly exponential in M. And we in the paper we we have many lower bounds in conjectures that are aimed to show where our analysis is tight and where there might be room for improvement.",
                    "label": 0
                },
                {
                    "sent": "So there so there's definitely more exciting work to be done on this problem.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "So your analysis is essentially the analysis of minimizing a coordinate wise descent algorithm, and I was wondering if you try to so this is really adapted to the learning for boosting.",
                    "label": 0
                },
                {
                    "sent": "Did you were able to extract some fundamental properties of the convex function you want to minimize so that you can actually translate those rates to a more general set up?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's a really good question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there so.",
                    "label": 0
                },
                {
                    "sent": "In the paper, we actually highlight sort of where where the analysis actually applies more generally.",
                    "label": 0
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "There's this at this very well known old fact that Adaboost stepsize, which is sort of that Delta T there has something to do with its progress, and in fact it has a lot to do with its progress.",
                    "label": 0
                },
                {
                    "sent": "It completely determines its progress, and so a lot of our results actually hold whenever that inequality holds, and it holds for Adaboost, but it holds more generally as well, for instance.",
                    "label": 0
                },
                {
                    "sent": "So perhaps the opposite question.",
                    "label": 0
                },
                {
                    "sent": "So if you're viewing Adaboost, is a optimization last minimization with the dependence in the L1 norm?",
                    "label": 0
                },
                {
                    "sent": "How does it compare to other even coordinate methods for minimizing an objective with an L1 constraint or with dependents and L1 and the rate?",
                    "label": 0
                },
                {
                    "sent": "OK, so if you have, if you have an L1 constraint, you have a finite minimizer, but I've never in the first part of your analysis where you did do that you did have.",
                    "label": 0
                },
                {
                    "sent": "No one constraint.",
                    "label": 0
                },
                {
                    "sent": "Oh, you mean to the reference solution with the with the L1 constraint?",
                    "label": 0
                },
                {
                    "sent": "So so what's what's the?",
                    "label": 0
                },
                {
                    "sent": "I'm sorry?",
                    "label": 0
                },
                {
                    "sent": "How does it compare to other methods for minimizing an objective subject to no one constraint?",
                    "label": 0
                },
                {
                    "sent": "Let's see, that's a tricky question I. I'm thinking maybe I should handle that one offline because it's a little bit complicated, but.",
                    "label": 0
                },
                {
                    "sent": "It's I don't.",
                    "label": 0
                },
                {
                    "sent": "So the answer is I know a little bit, but I don't know quite quite the right answer.",
                    "label": 0
                },
                {
                    "sent": "So I think maybe I'll take it offline.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, I had this small question actually in your conjecture conjecture on the running time with a depends on B, like the diameter of the L1 bowl and epsilon.",
                    "label": 0
                },
                {
                    "sent": "You had different different exponentials like B ^2 versus epsilon and it would seem like if you scale all the losses by certain.",
                    "label": 0
                },
                {
                    "sent": "Yeah no, no, this is exponentially.",
                    "label": 0
                },
                {
                    "sent": "I don't know 15 slides ago, so it seems like if you scale all.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The payoff by factor of 10.",
                    "label": 0
                },
                {
                    "sent": "You'll need epsilon to be divided by 10 right, but with the ball be scaled by 100 or.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I mean I would imagine it would be over epsilon not be squared over epsilon I guess.",
                    "label": 0
                },
                {
                    "sent": "So sorry, so you're saying you think it should be because this will be my.",
                    "label": 0
                },
                {
                    "sent": "I don't know anything about your problem, but that would be my.",
                    "label": 0
                },
                {
                    "sent": "My guess?",
                    "label": 0
                },
                {
                    "sent": "So you think OK, just because of the scaling scaling works this way?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'll take it offline, so thank let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}