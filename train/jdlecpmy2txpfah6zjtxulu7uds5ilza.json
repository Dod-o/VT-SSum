{
    "id": "jdlecpmy2txpfah6zjtxulu7uds5ilza",
    "title": "Stability and convergence",
    "info": {
        "author": [
            "Nathan Srebro, Toyota Technological Institute at Chicago"
        ],
        "published": "July 28, 2007",
        "recorded": "July 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/srmc07_srebro_nati_sac/",
    "segmentation": [
        [
            "OK, so.",
            "Winner said that we should.",
            "Feedback for the questions we ask may the deducted from the talk, so I think I'll be fine.",
            "So I mean, this should really not be more than half an hour.",
            "And it's also.",
            "Not sure I would call this a talk.",
            "I mean basically this is just stating what we follow following up to some of the talks and discussions yesterday.",
            "Stealing some problems relating to stability that would be interested in looking at started looking at and trying to figure out what can possibly do we what we can possibly maybe say there.",
            "So I have to warn in advance, right?",
            "This is I'm not going to present anything I actually did or any results or anything remotely like that.",
            "And also you should say that some of the questions here are results for discussions.",
            "Also with the shy Benjamin."
        ],
        [
            "OK, so just basically I'm looking.",
            "Don't like putting these set up slides the beginning, but since we already, it's just a it's when I'm going to be.",
            "I'm going to be talking about things, so I guess I'm going to be talking about data coming from ID model.",
            "We have some sample and I'm going to limit myself here, 2 clusterings and minimize some objective function.",
            "And I'm also going to be not considering this.",
            "Not concerned with the problem of generalizing the clustering from one day to another, so I'm assuming the clustering is specified by some centers or something so that whenever clustering of a clustering of the entire space, so you can apply this to any sample and then calculate the objective on that sample.",
            "Not only the same."
        ],
        [
            "From which it actually obtained the clustering.",
            "OK, so.",
            "Latest ability to so.",
            "One thing that we can try asking and this is seems to me that it's been the main thing that we've been talking about yesterday is what does clustering tell us about the source distribution deep, so is this distribution clusterable?",
            "I mean, is this just like some uniform cloud points or do we really have distinct clusters there?",
            "And how many clusters do we have there and so forth and?",
            "In this regard, for example, the work the Shinola tells us that really we understand in some sense if we at least looking at stability just as a binary measure, stable or not, stable, that in the limit it really tells us something we know exactly what it tells us about distribution D and what it tells us is just is the objective function minimizing the OR maximum optimized on the unique unique partitioning?",
            "Or there are multiple partitioning that have the same objective value.",
            "Ann is, as was demonstrated.",
            "This is perhaps.",
            "Not always exactly what we wanted.",
            "It doesn't exactly maybe correspond to our natural view of how many clusters there are, but this we kind of at least can say exactly what the stability here, at least in this.",
            "In this sense, in this Limsup sense tells us about the distribution D, but the way I view stability is really it's not that it tells us something about the situation D, but rather it tells us something about the sample size that we have.",
            "And the relationship between the clustering that we obtained to the true clustering D. So the true clustering of these, so I'm assuming that I'm taking here by ADHD.",
            "Note here the optimized you can think of it as optimizing the clustering objective.",
            "So if you really want to think about K means something about K means, so it's the optimal.",
            "The Caymans objective on the true clustering and always exists and OK, so there might be multiple global minima, in which case we really don't have stability anyway, there's no point in talking about stability for finite sets.",
            "But at least if there is a unique global optimum, than a D is well defined and in the limit we know that we will have stability.",
            "And when we look at stability and finite size on finite sets really how we view it is not OK if the settings on this like intermediate size, then the stability on the finite set tells us something about D, but rather the stability on the finite set tells us something about this finite set in particular.",
            "But the sample size and we can phrase these questions in different ways.",
            "So does that mean we find some structure?",
            "I mean this this structure really exists in the true distribution.",
            "In this regard, I'm not.",
            "I mean, if I have so going back to the all these examples of how stability doesn't tell us something about the true distribution if they have uniform interval, then the clustering I'm going to find if I find look for, say, three clusters here, I'm just going to separate it into three segments like this and OK. You can say, well, maybe this clustering is not very interesting or whatnot.",
            "But it really does represent a clustering on the correct on the full distribution.",
            "However, if I maybe have.",
            "If I have maybe not enough points OK. And I find some clustering here then.",
            "So I'm going to find this clustering, but maybe this structure that I find here doesn't really tell me anything about the correct distribution because it just didn't observe enough points.",
            "And if I did observe enough points, I'll see that.",
            "Actually, I have one cluster here, one cluster here, and one cluster here, maybe an and what I found just doesn't relate to the complete distribution.",
            "So really this is a matter of generalization, and really so you can think of it really.",
            "It's actually question about the sample size.",
            "Do I have enough samples?",
            "Another way to phrase this, which is a bit different and actually like it much listener explaining a bit why is not generalization of the structure itself.",
            "Visualization of the objective value.",
            "So in the first 2 points here, I'm kind of asking.",
            "Are these?",
            "Do these clusters exactly?",
            "These specials exists and relate to the true clusters where defined true is minimizing the objective and the full distribution.",
            "Another similar question we can ask is I have some clustering and I calculated the objective value.",
            "So that means objective clustering on my finite set as well.",
            "If I take the same clustering and calculate the objective value on the true distribution, will I also get something that's similar orbits as low in this type of objective?",
            "Might be interesting to us really for doing things.",
            "Where the OK?",
            "So maybe I'll talk about this.",
            "Another slide I think OK so well, maybe trying to do now is just formalize these two things.",
            "So basically the type of.",
            "So you were talking about your central classroom schemes, where you can treat your data is IID samples.",
            "So you would exclude situations like.",
            "I did you a similarity matrix and you have to find.",
            "Normalized type solution.",
            "OK so.",
            "Lots of like OK so then you have to be a.",
            "It still can apply, but definitely it becomes harder to talk about things here I'm.",
            "What would be, for example, the expected cost?",
            "So you can still talk about in the following way.",
            "You have a distribution over objects and what you have about these objects is just the similarity.",
            "But you can still talk about.",
            "You can still talk about this as a IID data, so you have.",
            "I have some if I want to.",
            "Um?",
            "Formulation.",
            "The dissimilarity matrix.",
            "IID from fun super.",
            "Instances, but the agency only one server from there.",
            "OK, so there may be 2 separate issues here and let me try to see if I understand what your concerns are.",
            "So one issue is it really and this is.",
            "OK, so I'll try to separate distance of two things.",
            "One maybe think that you may be asking about or if you're not concerned about is it really in many cases we don't.",
            "This idea assumption would be problematic because we really have one data set in this data set, doesn't it doesn't really make sense to think about it is coming from some ID's or so.",
            "For example, if we're looking at.",
            "We're looking at the data set, which is either like the entire population or data set, which is all the genes then really.",
            "I mean, that's that's it, and there's no ID source here.",
            "So is this the issue or no?",
            "OK, so that's OK.",
            "So my second guessed about the issue here is that we have a rather than than the only information that I have about instances is pairwise distance information.",
            "OK, but in this case what I would say is not that.",
            "The distance matrix is generated from some IID distribution.",
            "Someday generate the distances independently, but rather that I have some abstract SpaceX where I have some distribution of this abstract under some data that comes ID from this abstract SpaceX and just the only information that I have is the X One X2 for any.",
            "Two points from this abstract space, but it can still talk about having in distribution D / X and the data coming from your ID.",
            "ID copies of this distribution.",
            "I don't see a problem with that.",
            "It does make some of the statements of the trickier, and I'm not going to actually.",
            "Refer directly to this buddy, but I don't see any problem with this setup.",
            "There's no problem with that setup, but you might never get enough data to identify.",
            "Any?",
            "To identify your distribution to some degree.",
            "In your SpaceX, but you might have enough data to identify a partitioning of of the objects based on these eggs.",
            "No, that's that's perfectly fine, right?",
            "So OK, so I'm not trying to identify distributions.",
            "OK, let me OK, maybe I should even put in side.",
            "Probably when I'm trying to get at is that I'm trying to use the ability to figure out to what extend it to what resolution do I actually have?",
            "Support the evidence in the data?",
            "They have, so there might be some very complicated structure in the data, so the true distribution for example might have a cluster here in the cluster.",
            "Here in the cluster here in the cluster here and the cluster here.",
            "So if I get really too few data points on its three data points, I might not have support for anything.",
            "So say I only have support for K plus one only cables.",
            "Hopefully I could tell that the only K = 1 being stable will see in a bit.",
            "It's a bit trickier than that.",
            "If I start having more data, I might have support for two clusters, and I don't know if you want to say that really this data set has two clusters or not.",
            "I mean I. I don't know what your definition of correct number of clusters is.",
            "Again, I I don't believe in this concept of correct number of clusters.",
            "OK, because there are two clusters here and if I have enough data, I'm going to support if it to say that really I'm going to find.",
            "One cluster here in one cluster.",
            "Here in this structure actually does exist in a true data, and so I'm going to say that cake was 1 = 2 is fine here when they have more data, I'm going to find these two clusters and I'm also going to find 5 clusters and then I'm not going to try to use the ability to say is K = 2 better than K + 5 = 5 better than K + 2.",
            "That's in the sense.",
            "Maybe a question about the the true distribution and what your notion of good clustering is that I don't want to get into.",
            "OK, what I'm going to want to be able to say is that if I have enough data.",
            "Then both the clustering into clustering is supported by the data and the clustering to five clustering is supported by the data.",
            "But maybe if I have less data then the clustering into only clustering into clustering and in that sense if I have infinite data then it really I mean really clustering into any number of strings at some point is going to be supported unless I have symmetries that mean that I'm not stable at the limit.",
            "OK so I'm not trying here to say that I can really at some point say everything about the distribution DM.",
            "Just trying to see to say for a specific clustering is that clustering actually supported or is this just artifact of the fact that I have don't have enough data?",
            "Hey."
        ],
        [
            "So in particular, the way it can be face is as follows.",
            "So basically what I want to be able to make a statement like this that if I have stability in other words, if for two randomly drone subset the distance between their clustering is small with high probability, then I want to be able to transform it into saying that I'm very close to the correct clustering that with high probability.",
            "My question in my sample is close to the clustering of the.",
            "Missing parentheses here maybe, but close to the clustering on the entire distribution.",
            "So and again, this is a.",
            "Here is maybe I should add kayson subtext here for you should think about is clustering with some specific number of clusters.",
            "OK, so in this example if I have some intermediate number of examples, presumably this will be stable for two clusters, and so the two cluster structure would generalize, but maybe not stable for five clustering, and so the five cluster structure would not generalize.",
            "And later on the face structure Cluster 5 cluster structure must might start generalizing.",
            "So we want to contrast this with the.",
            "Different notion and this is looking more standard notion of generalization that when we were familiar from the from classification.",
            "This is that our objective function generalizes OK, so this is again if we have stability thing, then we can obtain some statement that says that are this.",
            "We can measure.",
            "This is our objective objective on our on our subset or empirical estimate of the objective function and if we can bound the expected value objective functional entire data set by.",
            "Our empirical estimate plus 70 something that depends on on the stability and maybe also in the sample size so.",
            "This type of statements are easier to obtain, but really want to.",
            "It's type of saving so much less care about because.",
            "Physical conceptually is I view it, unlike in classification and classification, we have the objective where minimizing is really the objective we care about are almost objectively care, but I mean they really objective.",
            "We know exactly what the objective we care about is.",
            "It's the.",
            "It's the expected expected misclassification error specification error over the entire distribution.",
            "OK, so that exactly we can't measure, but at least we can.",
            "We can have a good estimator fit that we can talk about how well an estimator, if it is, and that's our empirical classification error, and so these type of statements really makes sense.",
            "I mean, we have an empirical estimate of the quantity that we are interested in, and we use it to bound the quantity that are really, really interested in.",
            "OK, this isn't classification.",
            "However, I would argue that in clustering.",
            "In many cases, the real objective we're interested in is not minimizing.",
            "This came in the objective.",
            "So I mean, sometimes it is.",
            "I mean, if if we're doing something like facility location or doing vector quantization for compression, then really the K means objective is what we're interested in.",
            "I mean, we want to just be able to approximate the data, and we can we care about how much how many extra bits it's going to take us, or how many extra miles or dollars or whatnot.",
            "But in most cases, in clustering at least, it seems that we're talking about here what we really care about minimizing is this.",
            "It's the distance between the clustering that we obtain and the some true clustering that we have no idea about.",
            "The problem is that this is something you really can't estimate.",
            "I mean, even on our empirical, even in our sample it's not something we can get an easy empirical estimate for.",
            "But it is actually what we're interested in.",
            "So when there's some in a much more complicated situation in which we're really interested in one objective, but because we have no idea how to approach it, then we actually minimize the different objective, and in certain cases we can argue that the optimum of this objective will actually give us the true clustering, or interested in.",
            "For example, if we have data, that's Vic some strong assumptions on our data data that comes from Gaussian mixtures.",
            "What I'm going to say is that.",
            "That minimizing this objective, for example K means is sensible.",
            "If you really want to find this, OK, no, but it's definitely sensible for you.",
            "Find Sadie what what I'm trying to argue is if we have some true underlying clustering and we have some assumptions in Metro\nClustering, for example, that it's the clusters are.",
            "Are Gaussians or even we can make, uh, we don't even need him to be Gaussian.",
            "All we need him to be as have some kind of spherical structure, but say each Gaussian is each each.",
            "This cluster is Gaussian.",
            "Then we know that at least in limit, minimizing the K means objective or minimizing, maximizing the likely relative to a Gaussian mixture model is actually going to give us the correct clustering in the limit.",
            "So in that sense, it's actually it's reasonable to use this objective function.",
            "However, it doesn't necessarily mean that if I get if I don't find the optimum, but I get some kind of like approximation here, it doesn't necessarily tell me something about how close I am to the the.",
            "Correct structure.",
            "Tell me where J actually enters in that line over here?",
            "Yes, so OK it's a bit OK, so the way J the way J interests into a is it A is defined as the minimizer of J over the sample.",
            "OK, but OK.",
            "But what I'm saying is if that's not clear how it enters into what I'm really interested in, an is an example of how getting a very good approximation.",
            "So maybe another issue is.",
            "I mean I don't know what's wrong if you give me an approximation that's within I don't know how to translate this function.",
            "Into this epsilon here.",
            "I mean if you give me something, that's the K means is approximately 203 four and what does it tell me about how similar my clusters are?",
            "What I care about is really is getting most of the points correctly clustered?",
            "What how does doing translate so?",
            ".3 four?",
            "Also, relative measures don't help me and this is what I'm going to show here.",
            "And this is an example of how I can really.",
            "AD AD is your desired gold partition, yes, well, OK.",
            "So easy, it's just the minimizer.",
            "It's the same way except replace S with D, so it's the minimizer off my objective.",
            "You know what you go to?",
            "Partition is no but but Jay, let's say let's say, well, if you get this Oracle and you have access to be and then you can calculate AB.",
            "But still the number of Jay doesn't tell you yes.",
            "OK so so.",
            "So I'll show you know so.",
            "So what I'm saying is not exactly the number what what?",
            "What doesn't tell me OK, is it?",
            "If I know that I I approximate JK to some?",
            "Error on Jay is they can tell you how big my error and Jay is or how far I am from the J of the optimal partition.",
            "OK, save the correct distribution that I can't translate that to how far my how big my clustering error is from this correct gold standard clustering.",
            "So I'm going to show you this in the next slide, so, but basically I mean if I could optimize it then that would be fine.",
            "But what I'm saying is if I only have if I have only approximately optimize it and I have some.",
            "I know by how how close I am to the objective value that doesn't tell me how close I am to the correct clustering.",
            "I still don't get the point, so is your problem that Jay is not really?",
            "Modeling correctly, yeah well.",
            "Doesn't indicate the rate in which the question.",
            "You here you have the difference of two Vin numbers.",
            "Yeah, I think so.",
            "So unless we try to answer your question it models it only in a very limited extent.",
            "It models it because it is not the situation that at the end of the day what I really am going to measure my performance by is J and say if I got a better J value, I'm happier if I gotta hire Jay value unless happy when I'm going to measure my my my performance bias by this measure, how far I am from the gold standard clustering.",
            "However, Jay does capture.",
            "Uh, is suited to what I want.",
            "It does capture what I want, but only in the very limited sense.",
            "Then I know that the optimum of J does actually minimize is actually the optimal for they want.",
            "But the fact that the optimum's are the same doesn't doesn't mean doesn't tell me how how being often one of them corresponds to being off another."
        ],
        [
            "So maybe I'll show this example OK, so consider so this example I should say was originally constructed to show how getting we have polynomial time approximation schemes for 4K means.",
            "How this doesn't actually help us cluster in our sense, so considering you have two clusters in, these clusters are very well separated.",
            "Their 10 standard date, each one of them is spherical Gaussians, and there are 10 standard deviations from each other.",
            "But they live in very high dimensions, so even in D dimensions but.",
            "There is still very, very well separated, so there's a very distinct clusters because there's going to be essentially no overlap between them, so we can calculate now that what the K means objective and if we're talking about likely it's going to be very similar to the Caymans objective here, because they're so well separated, so the K means objective of the true clustering.",
            "I have a contribution of roughly DDS the dimensionality from from each point, so it's going to be roughly times in or the if we look at the normalized by and it's going to be and now I look at the following very silly clustering.",
            "I'm just going to go through customs at the center very close to the origin of very close to the origin.",
            "So between the two clusters, so really have no clustering whatsoever, is just nonsensical clustering.",
            "And now I'm going to get the same cost in each they mentioned.",
            "I'm going to get D. Plus, in the single dimension in which the customers are actually separated, I'm going to get an additional cost of about 25.",
            "So is my dimensionality increases and I would argue that case at least very interested in high dimensional case then.",
            "The ratio between these two costs becomes negligible.",
            "It's going to be 1 + 25 / D. So really, even if even though I have here a very good approximation to the objective value given by a trivial trivial clustering, I really have no approximation whatsoever to what I'm really interested in, which is being able to separate this cloud in this cloud.",
            "OK.",
            "So This is why I would argue that.",
            "I mean, although OK, it's obvious that this maybe, maybe not so obvious, but it's not so difficult to show under certain circumstances that this type of condition implies this type of condition.",
            "The other way doesn't go out, and this is in some sense easier to obtain.",
            "It's still not clear to me exactly how I mean.",
            "There are some very limited ways in which you know to do that, but this type of result is what we're really.",
            "I would really like to obtain.",
            "Yes.",
            "So, so it's true that that that the dynamic range is is vanishingly Steve.",
            "He goes to very high.",
            "Is it?",
            "Isn't it simply a problem of rescaling everything we scaling by what?",
            "Fusion on the real axis where you could slip become more and more so much as well.",
            "How do I scale by?",
            "It's definitely not where we tried to show.",
            "Even if I scale by the value of the objective, that doesn't help me.",
            "So if the scale how?",
            "How do I?",
            "How do I know what to scale by?",
            "At 1:00 this this this range in which your costs live.",
            "This range is with this dimension going to Infinity.",
            "We dominated by yeah yeah.",
            "And maybe it's getting to 0 because it might be a simple rescaling problem.",
            "OK, so I at least thought it might be.",
            "I at least don't know how to get a sensible scaling here.",
            "I want to scale this by so it might be, I mean.",
            "But usually what you do is you take the minimum, you take the maximum annual minimum, maximum, minimum, maximum over what?",
            "Of partitions.",
            "How do I OK?",
            "So you're saying it's only competition so it might it might.",
            "OK so it might be I definitely.",
            "I mean OK so.",
            "So they definitely might be in that might.",
            "I don't know how to do that, but that might be a way to kind of.",
            "Situations rescaling.",
            "Preserve might preserve all the structure in the spectrum of your cost values.",
            "Then you are fine.",
            "The other limit might be that you do a re scaling and the fluctuations contrary to what you expect it amplified send you start OK, then you get a really messy situation where you cannot say anything about the underlying signal, so there is there is some of that.",
            "There is some of the second situation here because if you take a look at this example we have two clusters here and now.",
            "Instead of moving both centers to the middle.",
            "Just move both centers in some random directions.",
            "Each sensor in some other random direction, so this.",
            "This preserves the clustering even though the effect on the objective value is the same.",
            "Right, but.",
            "Yes.",
            "Discribe.",
            "An analogy, find procedure how you have to let.",
            "How do you have to let these two data points stay there?",
            "Clouds move towards each other so that the overlap between the clouds is the same, so that the limit of the goal is going to Infinity is a very tricky limit.",
            "So.",
            "I'm.",
            "You cannot confirm.",
            "The the rate in which the distance.",
            "Changes in the function of the rate of convergence of Blue Jays know, but he is considering the details of Kitty litter.",
            "So.",
            "So, but I think what it is possible, it is definitely possible that we often suggested is definitely possible, that we can obtain.",
            "I'm pretty familiar with book in simplicity, and I don't think that anything they do answer this, but it's definitely possible that you can obtain some kind of.",
            "Data dependent scaling here that I don't know how to define.",
            "Definitely ought to calculate that will somehow imply in a useful way that.",
            "Uh, some kind of relation of this form implies this, so that's definitely possible and would be interesting to me.",
            "I definitely don't know how to obtain this.",
            "Problem.",
            "Yeah, so this is essentially what I'm doing here, right?",
            "Because there's all these irrelevant dimensions.",
            "OK, so as a side note, I should say that even regardless of stability, this is unrelated to stability side nodes, even regardless of stability.",
            "This type of generalizations on J itself, I believe, are possible to obtain an is a function in various parameters.",
            "We don't exactly know.",
            "I don't agree with your face there.",
            "I know what paper you're talking about.",
            "Your paper doesn't exactly show this.",
            "Maybe we'll talk about this.",
            "OK, OK your paper.",
            "I don't know, I know.",
            "But it would be nice to see something concrete result here, but it would be nice to see that in what way can I mean this is going to depend on several parameters of the problem and it's going to be nice to see in what way.",
            "Maybe we can use stability as a replacement for those parameters that can also be interesting.",
            "Again, I'm more interested in getting results.",
            "This form of.",
            "They tell us how stability stability means that we're really close to the goal to the correct clustering, not that objective is made OK, so."
        ],
        [
            "Problem with this is that we can't get this, so in welcome is going to say that."
        ],
        [
            "This is cheating example, but I really don't think it is so if we look at the following.",
            "Pulling data set.",
            "So this is the true distribution.",
            "The distribution has four clusters, three of them each happen with probability roughly 3rd, and then we have another very very far away cluster that only happens with probability epsilon.",
            "So what's going to happen is the stability here is going to be very misleading if we look at K = 3 then.",
            "Um?",
            "So if you look at the stability for K = 3, so this is instability, say.",
            "Then, well, for very low values, it's going to be unstable just because pretty much I mean anything is unstable for very low values.",
            "But then we're going to be able to really identify those three clusters, and we're not going to observe any point from that right cluster.",
            "So at some point we're going to become stable.",
            "But at this point we're stable then we're.",
            "We're not really capturing.",
            "We're not close by anyway to the correct clustering, because OK, for any most reasonable measures of distance between clusterings.",
            "Here, the problem is not so much we're missing here.",
            "Missing this thing.",
            "OK, so that we're missing this thing might only affect our objective by epsilon.",
            "The issue here is that in the optimal clustering here on the correct on the full distribution looks probably something like this with three clusters.",
            "And so all these points list.",
            "But when we have only finite data and we don't observe you at the rightmost cluster, we're going to get a clustering that looks like this.",
            "So all these points that are in these two clusters were going to say that they are in different clusters, even though in the supposed gold standard clustering they are in the same cluster.",
            "So really we're going to have.",
            "I mean this we're going to miss it all, but that's only an epsilon difference.",
            "But really, we're going to have a substantial clustering error versus very substantial agreement error versus our gold center clustering.",
            "So we have not yet convergence sense, but still are.",
            "Stability is very low.",
            "OK, so we can hope to get a statement like we had before just to continue this.",
            "There's also an interesting point here that the stability is not monotone, so if you hope to get something with this form, then OK, this type of relationship is monotone, and if you're saying that stability really implies this type this convergence, then you really should be hoping that stability is going to be on the more data you have, the more stable you're going to.",
            "You're going to be, but in here we see that once we have roughly.",
            "One over epsilon data.",
            "Then suddenly we are going to see data from that remote cluster, and again we're going to be unstable.",
            "Very gracefully go OK with this depends.",
            "OK, so it depends.",
            "It depends where they define this instability here.",
            "But OK, you're right that.",
            "OK so.",
            "Expectations.",
            "Which is smaller?",
            "Sample size is smaller than one over epsilon.",
            "Still get some of the time it will get very if I have.",
            "I don't agree.",
            "If I have substantially less than one over epsilon, then my probability of observing any point from there would be very low.",
            "OK, it's it's not going to be a jump, but OK. Lake of the effect of the sample size depends on epsilon.",
            "How will you detect is perfect, but how effective it is depends on the distance fixed epsilon.",
            "But the increasing the distance you will increase the effects of even one country.",
            "OK, OK, let's not.",
            "Let's not argue if it's going to be a jump or not.",
            "The point is the point.",
            "The point is that there is a regime which is going to be stable, even though we're actually far away from the correct thing.",
            "And then the stability is not going to be monotone going to increase.",
            "And actually at some point it's going to decrease back because we're at some point going to have, assuming there's not perfect symmetry there.",
            "I mean, we know that we will actually have stability in the limit.",
            "OK, so.",
            "Is flawed from the statistically POV because people doing what they call or simulation would call all that stuff.",
            "When you don't see Elyse this rare class today will call it burning face.",
            "And but when I, but when I have, but when I find out data OK, I I mean.",
            "The problem from a scientific investigation POV, if you do botany and you never commit yourself to a theory because there might be, there might be strange creatures on Mars which you haven't detected yet.",
            "But if it you keep in your hypothesis that you never come up with so, so so so so so.",
            "So I agree.",
            "So let's see.",
            "I agree, So what I'm going to do is I'm not going to say I'm not going to finish my talk here and say, OK, Stability doesn't teach us anything about finite sample size and we should just not usability what I want to see is how to correct.",
            "I mean this definition, we cannot expect to get this type of results.",
            "So what we're going to see, what I'm going to suggest in the second, is how to modify this type of statement so that we might be able.",
            "It still will be useful, and we might be able to get this result.",
            "You cannot achieve this.",
            "For very simple probabilistic means because you have you have extremely rare events, which is extremely high loss, right?",
            "Obviously probability theory doesn't tell you a lot about which.",
            "Allstate insurance.",
            "But that's fine, so.",
            "OK. OK, but that's fine.",
            "So let's see how to correct this so that I will be able to get a result.",
            "Specify.",
            "I agree that's why I want to correct it.",
            "So OK.",
            "Yes.",
            "The partition, another number of clusters, then the distance between the stable partition that you find an optimal partition is absolutely no, no, no, because again, we're looking here at clustering with a fixed K, right?",
            "So, so clustering with a fixed K. This is going to be the correct or optimal partition is going to be solid, the solid blue.",
            "Right?",
            "No, but if we fix K, here's three.",
            "Yes, we fix K. Here is 3, so this is going to be the optimal partition.",
            "No, but no but.",
            "Hey sweet sweet sweet.",
            "Well, that's what we're doing, right?",
            "We're doing.",
            "I mean, that's what we're doing here.",
            "We're looking at.",
            "We're trying to figure out if our for clustering model actually is modeling the for clustering in the data.",
            "OK, OK, so as you know him."
        ],
        [
            "Rightfully pointed out we have to fix this and it's not so difficult to fix it because basically all we want all we need to say is that OK, there might be some crazy stuff going on with low probability and we can't expect to handle these crazy stuff that's going on with low probability.",
            "So instead of saying.",
            "Is the simplest fix I could come up with a statically?",
            "It's maybe not as pleasing so I'll be happy to hear other suggestions, But basically what we want to say is that if we have stability then I can't promise you that I'm close to the gold standard partitioning, but at least I'm close to a gold standard partitioning the way I wrote it.",
            "Here is a very similar distribution.",
            "What I really have in mind here is just taking this D prime to be deconditioned on everything except for some rare event.",
            "So everything except for something that happens probability 1 / sqrt N and the way I wrote it here, is just saying that the Cal divergences between defragment D is low.",
            "So in other words, I want to say that if I have stability then I know that maybe I'm not close to the optimal thing on my distribution, but I'm close to the optimal thing at a very similar distribution.",
            "So this type of statement, I believe can be shown, but I mean I don't know how.",
            "OK um.",
            "OK, so.",
            "I guess this is taking a bit more than 20 minutes, but.",
            "I'm OK, so basically this is the first big half.",
            "OK, so of the talk in the main point here, is that really?",
            "I think that as I said in the beginning, the way I view stability is not necessarily telling us something about the correct distribution is of questionable or not by telling us something about our sample size and saying do we have enough evidence to support what we find and what I want to see is being able to get a rigorous characterization of really can we?",
            "Can we turn this into a rigorous statement?"
        ],
        [
            "OK, so the other thing I want to talk about is a bit different and also mentioned it yesterday, but I viewed a bit differently and this is this is not stability of clustering Now really, but stability of the clustering algorithm, But when?",
            "Say here stability.",
            "This is going to be very.",
            "I really want to try to separate these two things completely.",
            "OK, so now I'm assuming that I have a fixed finite set S and I've only been.",
            "I'm only concerned about it.",
            "I'm only concerned whether I'm really successfully optimizing my objective, successfully, optimizing my objective on this set is.",
            "So yesterday we talked about it, talked a bit about this confinement that because our optimization is not perfect, it might affect our view of stability that things might seem less stable than they really are.",
            "When I'm talking about this now, just the bottom half an really.",
            "The issue here is whether we can use the ability to answer the question of once we run the M, did we actually find the global optimum?",
            "So it's very clear.",
            "OK, so basically what I'm.",
            "OK, so basically what we do and this is actually what we do all the time here.",
            "At least I do all the time and I would imagine that many of you do all the time.",
            "Also, you run the many times from different random initializations and now you can look at this instability in this instability now.",
            "So I'm assuming he actually when I found the point M actually converges too.",
            "So there's some issues here of deciding whether two things are actually the same thing and it just didn't let it run long enough.",
            "But let's assume that I'm running kind of incident number of steps of them and they can look at how.",
            "Why do they call it instability?",
            "This is, stability should be here, I guess not equal to.",
            "OK, so what's the if I run them from two different starting points, what's the probability of actually going to the same place right now so?",
            "No, I'm not sure why I wrote it like this, so this would be called the stability and not instability.",
            "OK, OK, so it's very clear that if every time I run am I get to a different place, it cannot possibly be with reasonably high probability.",
            "I'm actually finding the global optimum because if it reasonably high probability of finding the global optimum, then with the square of that probability at least I will be getting to the same global optimum again.",
            "I'm assuming here there's a single global optimum, so it's very clear that if him.",
            "If I'm unstable, I'm things are bad instabilities, necessary conditions, so we know that if you run them and get two different places, we know that we're not doing anything.",
            "The question you can we get abound in the other way, in in, in the census is at least what I'm doing all the time.",
            "Empirically, I'm running him, and if all the time it gets if almost all the time it actually gets the same global optimum.",
            "Sorry gets to the same point.",
            "I'm pretty confident actually and not statistically, but just sort of.",
            "Mentally confident that I did actually succeed in the DM actually succeeded in optimizing it, and I can actually back up back this up also by empirical results.",
            "So we ran last year a huge empirical study of them.",
            "We just ran.",
            "We generated lots of datasets and ran Eminem and various different sizes, and the idea is to try to figure out really when does M find the global optimum.",
            "And since we generate those data sets, we also knew what could cheat a bit and find what the global thing was.",
            "And from that huge experiment I can tell you empirically that really, this does seem to be the case of that experiment.",
            "Did have lots of limitations.",
            "I mean, this is not proof since it was only specific with generating the data, but we did.",
            "There was not a single case out of hundreds of millions of runs in which we got stability but did not actually converge to the global optimum, yes?",
            "OK, so.",
            "OK, so so.",
            "OK, so that's true and you have to be a bit more.",
            "So you have to make the same.",
            "That's true, so you have to make the same type of.",
            "An affair modifications we did before, but again I'm not so that's true.",
            "What?",
            "So now.",
            "How many?",
            "If.",
            "Then you look stable because you.",
            "No, but Epson.",
            "So what I have here many things as many random initializations.",
            "So if they have.",
            "I have a really optimal solutions involve a tiny cluster and I'm doing not doing enough random initializations such will never find something in that cluster.",
            "Then I'm not going to find it, so the reason I'm not so concerned about that and the reason also didn't come up in their experiments is and it can be corrected.",
            "It is slightly more graceful way, so basically we did assume we knew the minimum cluster, the minimum size of cluster and that's necessary is that came up.",
            "So the way we did experience, we actually chose not a single.",
            "If we have K. Clusters we didn't choose K initial centers.",
            "We chose Kalo K or actually Kalo K or one over the size of the smallest cluster log K random random initialization points, which is why this is happened.",
            "So if we knew there was a tiny cluster there, we actually started looking for more initial centers, but so definitely in some theoretical results here.",
            "This would have to come in in some way.",
            "OK, so this is also as far as I know there's no theoretical results in this, and we think we have an interesting.",
            "I definitely don't have any and related to this.",
            "I'm going to use the last five minutes and hopefully will give me to talk about something that's not really really that's not directly related to stability, but it's an open question that I'm very interested now in, and it is somewhat related to stability.",
            "If you look at if you look at this.",
            "So the question is really we know that in the.",
            "The weight related to this, we know that if we look at stability of clustering, but the previous notion of profitability we had, if the data set goes to Infinity will always going to be stable under some mild conditions.",
            "And the question is the same thing happens here.",
            "So if really we have a lot of data is that is the issue just a matter of sample size.",
            "If we have enough enough data, will we actually always converge to the global min?"
        ],
        [
            "So the conjecture is as follows.",
            "So consider and we're going to limit ourselves here really to just if the very simple case.",
            "OK, but make it may claims in this case this holds, so consider learning.",
            "Mixture of Gaussian.",
            "So in this case we talk about a mixture of unit of spherical Gaussian scape and the important thing here, and this is going to be crucial is that data is actually generated according to our model.",
            "So we have data generated as a mixture of K Gaussians and now we're trying to fit a Gaussian mixture model and we do this by.",
            "Minimizing the.",
            "We do this by by maximizing the likelihood.",
            "OK, so we look for the collection of center, so our model is just specified by K center.",
            "So by some vector in R2D times KE an we just want to minimize maximize our likelihood with respect to the centers.",
            "So when OK.",
            "So this is non convex and typically run any local searches is not related particularly to them.",
            "If you want any local search we will get definitely stuck in local minima.",
            "No.",
            "As the amount of data increases as N goes to Infinity, this likelihood just becomes the KL divergent circle divergent, negative scale divergent minus some constant.",
            "In the claim is that if I look at the scale divergent sit still a nonconvex highly nonconvex function.",
            "As we know exactly where the global minimum RN has many global minima, so the global minimum are exactly the true centers, but also on any permutation of them.",
            "So we have many global minima and many different basins in between them.",
            "We have ridges with several points on those ridges, so there are many critical points here.",
            "Many subtle points.",
            "But the claim here is my conjecture is that there for this function there are no local minima, so in the infinite sample limit, if the data actually comes from our model.",
            "It's just that this assumption is very important here.",
            "If the data actually comes from a model class in the infinite limit, we will not have local minima.",
            "We only have global minimum.",
            "Kate.",
            "So it's a good question.",
            "I mean, I, I can tell you, I'm not sure why it should be the case.",
            "I know why I think it's the case.",
            "So my reason is that there are several reasons I think this is the case.",
            "First of all, if we again this, it started out from this big empirical study and what we notice is no matter what the separation is between the.",
            "Clusters or how they're configured if the data if we take the data set large enough, we will actually M will actually converge the global minimum.",
            "So This is why we started to think about this.",
            "And then we looked at some specific examples in which you would expect to get local minima.",
            "So the classic example in which you would expect to get a local minima is if you have.",
            "The true distribution has three clusters like this, but actually you initialize 2.",
            "Oh to one center here in two centers here.",
            "So OK, if everything is completely symmetric here in the saddle point, but if things are not absolutely symmetric, this is actually not.",
            "Even though this is going to be a local minima for any reasonable finite sample size, this is not actually local minima in the infinite sample case, and the reason this is the Infinite Temple case one.",
            "OK, there are two variants of this equation.",
            "If you fix, the priors were done, fix the problem so if you don't fix the priors.",
            "I mean it holds in both cases explanations would be different if you don't fix the price this year, is that one?",
            "Gaussian is completely enough to explain this blob.",
            "You don't get anything else.",
            "Anything more.",
            "Any additional benefit by having another center here.",
            "This is not true for the finite case, but the infinite case you get absolutely nothing by having another center here.",
            "And so this allows this center to very slowly move over here because it's not going to have any pull from these guys and it will have a pull from these guys because these guys are not perfectly explained by this single Gaussian.",
            "Now this is only again, this is only an instant symbol in any reasonable finite sample size.",
            "You will of course have local minima.",
            "Basically have this very, very flat near plateau.",
            "Really quickly again, so come in when you have finite data.",
            "If you elongate your right cluster a little bit OK, so I'm also allow yourself to estimate the covariance then I don't know what happens.",
            "OK, so OK.",
            "So I looked at this quite a bit for spherical Gaussians, both when you even when you either fixed priors or different parts but with only with spherical Gaussians.",
            "In for spherical Gaussians I'm getting getting more and more confidence about this both because of this empirical study.",
            "We also have some numeric studies for small.",
            "Dimensions and small number of clusters.",
            "And because I mean mostly because I couldn't find a counterexample for varying variances.",
            "I have not looked at this enough.",
            "I don't know if a counterexample there, but it didn't look as hard, so I don't know what happens when the variances are also very.",
            "Um?"
        ],
        [
            "So.",
            "OK, OK so as I said, the implications here.",
            "Of course in some sense in the infinite sample, regardless of any separation assumptions or anything like that, M or any local search will converge.",
            "But this is very very.",
            "This is still.",
            "Very loose because I mean we don't know how.",
            "I mean, we're not really going to get there very quickly.",
            "The real reason we're basically interested in is because the real quantity of interest here is the.",
            "The probability under some reasonable initialization, the TM will converge to the true solution, so this is really what we're interested in studying.",
            "OK, an seeing when large enough this is already going to be close enough to one for some reasonable initialization scheme, and the conjecture can just be stated is saying that for any initialization scheme, PN would always converge to one limit, yes.",
            "So yes, what do you mean by an unstable local minima?",
            "They're definitely critical points.",
            "So, but that's not the local minima then.",
            "Look, yeah, so local minimum.",
            "Yeah, I guess so.",
            "I wouldn't know.",
            "I would call that a saddle point because, yeah, so local minima, meaning that it's there's some neighborhood to fit some open neighborhood to fit that.",
            "It's a minimum in that open neighborhood.",
            "OK. OK so OK, so sorry for abusing this for non not exactly stability question but I do think it has some relationships again just to summarize the main.",
            "Two issues that I'm interested in.",
            "One is this issue of stability, not of clustering, but of the local search and the other is trying to establish some some result of this form.",
            "The likelihood landscape of of these three classes, this yes.",
            "Basically, one cluster runs empty, and since it's empty then you can push it in that cluster along it, yes, so depends.",
            "It's not completely, yeah, so it depends.",
            "If you fix the if the prices are fixed or not.",
            "If the prices are not fixed then yes, one of the clusters becomes almost empty.",
            "It's not completely empty, but it has very low responsibility and then it gets very slowly pushed here.",
            "So the way I just analyzed it as I looked, I analyzed the derivatives of it and the rivers that you can show.",
            "You can see the derivatives actually point in this direction.",
            "They're very, very small, but they point in this direction.",
            "Derivatives of the likelihood with this very.",
            "This is very important if you fix the priors, the situation is innocence easier.",
            "'cause of the fixed the priors you just have too much mess in here 'cause you have 2/3 of the mess here where you really don't need only third of the mess here.",
            "And then so this causes it to move over here.",
            "You of course you need.",
            "You need it to be not completely symmetric, right?",
            "If it's completely symmetric, I wouldn't know which one to move, but if it's not completely symmetric then the one that captures this better and as far as will stay here and the other one will want to move here because here there is a really deficiency in mess.",
            "There should be a 2/3 of the mess and I only have 1/3 of the mess.",
            "Talking about.",
            "Sample sizes this notion that maybe you have you have enough data to support by versus 2.",
            "Frightened exactly the opposite.",
            "Oh they have reached a model.",
            "Let's say just for segment.",
            "Then we have equal priority on one to 10 points.",
            "What happens when you have so very little data and so you sorta single data point?",
            "Well, that doesn't support.",
            "Answer exactly.",
            "Sure.",
            "And if you have.",
            "There's no way to be able to reject it, so I think the difference here is in the Asian.",
            "In between the way you're phrasing the question visionix, you really wanting strong evidence that you have on K, right?",
            "You want?",
            "I mean, the way you phrased it.",
            "Now I'm asking how confident MI about K and what I'm asking here is not how confident I am about K. It's how confident I am about the clustering I get for that fixed K. So in particular, in this example, with the five in two.",
            "I'm not asking you to be confident that there are only two rather than five, only five, rather than two.",
            "I'm only asking you how confident you are that given K = 2, This is the correct clustering or the given K = 5.",
            "This is a curve clustering and then even in the Bayesian sense you would get the same type of behavior with Wi-Fi mean you'd get a very concentrated.",
            "The vision of this is different.",
            "In some sense.",
            "Their answers are similar, but because you may be calculating them is more complicated, but I can strike down mathematically what they are, but there you would be saying that if I don't have enough data then my posterior give condition and cake was two.",
            "My posterior will be fairly concentrated, so I'll have a lot of my posterior mass very close in terms of D to some solution, whereas condition and K = 5.",
            "The posterior will be such that was saying a lot of nasty things that are far away in terms of D from any place.",
            "Like OK?",
            "Notion of stability.",
            "Exactly.",
            "Figure out.",
            "Bayesian.",
            "The number for the right K cannot be closed.",
            "'cause there's no.",
            "'cause we don't.",
            "Percent.",
            "Relative setting right from the beginning and so so.",
            "Look at the opposite answer.",
            "Yeah, you keep all hypothesis where you don't have evidence.",
            "So thanks for the talk."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Winner said that we should.",
                    "label": 0
                },
                {
                    "sent": "Feedback for the questions we ask may the deducted from the talk, so I think I'll be fine.",
                    "label": 0
                },
                {
                    "sent": "So I mean, this should really not be more than half an hour.",
                    "label": 0
                },
                {
                    "sent": "And it's also.",
                    "label": 0
                },
                {
                    "sent": "Not sure I would call this a talk.",
                    "label": 0
                },
                {
                    "sent": "I mean basically this is just stating what we follow following up to some of the talks and discussions yesterday.",
                    "label": 0
                },
                {
                    "sent": "Stealing some problems relating to stability that would be interested in looking at started looking at and trying to figure out what can possibly do we what we can possibly maybe say there.",
                    "label": 0
                },
                {
                    "sent": "So I have to warn in advance, right?",
                    "label": 0
                },
                {
                    "sent": "This is I'm not going to present anything I actually did or any results or anything remotely like that.",
                    "label": 1
                },
                {
                    "sent": "And also you should say that some of the questions here are results for discussions.",
                    "label": 0
                },
                {
                    "sent": "Also with the shy Benjamin.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just basically I'm looking.",
                    "label": 0
                },
                {
                    "sent": "Don't like putting these set up slides the beginning, but since we already, it's just a it's when I'm going to be.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be talking about things, so I guess I'm going to be talking about data coming from ID model.",
                    "label": 0
                },
                {
                    "sent": "We have some sample and I'm going to limit myself here, 2 clusterings and minimize some objective function.",
                    "label": 0
                },
                {
                    "sent": "And I'm also going to be not considering this.",
                    "label": 0
                },
                {
                    "sent": "Not concerned with the problem of generalizing the clustering from one day to another, so I'm assuming the clustering is specified by some centers or something so that whenever clustering of a clustering of the entire space, so you can apply this to any sample and then calculate the objective on that sample.",
                    "label": 0
                },
                {
                    "sent": "Not only the same.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From which it actually obtained the clustering.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Latest ability to so.",
                    "label": 0
                },
                {
                    "sent": "One thing that we can try asking and this is seems to me that it's been the main thing that we've been talking about yesterday is what does clustering tell us about the source distribution deep, so is this distribution clusterable?",
                    "label": 0
                },
                {
                    "sent": "I mean, is this just like some uniform cloud points or do we really have distinct clusters there?",
                    "label": 0
                },
                {
                    "sent": "And how many clusters do we have there and so forth and?",
                    "label": 1
                },
                {
                    "sent": "In this regard, for example, the work the Shinola tells us that really we understand in some sense if we at least looking at stability just as a binary measure, stable or not, stable, that in the limit it really tells us something we know exactly what it tells us about distribution D and what it tells us is just is the objective function minimizing the OR maximum optimized on the unique unique partitioning?",
                    "label": 0
                },
                {
                    "sent": "Or there are multiple partitioning that have the same objective value.",
                    "label": 0
                },
                {
                    "sent": "Ann is, as was demonstrated.",
                    "label": 0
                },
                {
                    "sent": "This is perhaps.",
                    "label": 0
                },
                {
                    "sent": "Not always exactly what we wanted.",
                    "label": 0
                },
                {
                    "sent": "It doesn't exactly maybe correspond to our natural view of how many clusters there are, but this we kind of at least can say exactly what the stability here, at least in this.",
                    "label": 0
                },
                {
                    "sent": "In this sense, in this Limsup sense tells us about the distribution D, but the way I view stability is really it's not that it tells us something about the situation D, but rather it tells us something about the sample size that we have.",
                    "label": 0
                },
                {
                    "sent": "And the relationship between the clustering that we obtained to the true clustering D. So the true clustering of these, so I'm assuming that I'm taking here by ADHD.",
                    "label": 0
                },
                {
                    "sent": "Note here the optimized you can think of it as optimizing the clustering objective.",
                    "label": 0
                },
                {
                    "sent": "So if you really want to think about K means something about K means, so it's the optimal.",
                    "label": 0
                },
                {
                    "sent": "The Caymans objective on the true clustering and always exists and OK, so there might be multiple global minima, in which case we really don't have stability anyway, there's no point in talking about stability for finite sets.",
                    "label": 0
                },
                {
                    "sent": "But at least if there is a unique global optimum, than a D is well defined and in the limit we know that we will have stability.",
                    "label": 0
                },
                {
                    "sent": "And when we look at stability and finite size on finite sets really how we view it is not OK if the settings on this like intermediate size, then the stability on the finite set tells us something about D, but rather the stability on the finite set tells us something about this finite set in particular.",
                    "label": 0
                },
                {
                    "sent": "But the sample size and we can phrase these questions in different ways.",
                    "label": 0
                },
                {
                    "sent": "So does that mean we find some structure?",
                    "label": 0
                },
                {
                    "sent": "I mean this this structure really exists in the true distribution.",
                    "label": 0
                },
                {
                    "sent": "In this regard, I'm not.",
                    "label": 0
                },
                {
                    "sent": "I mean, if I have so going back to the all these examples of how stability doesn't tell us something about the true distribution if they have uniform interval, then the clustering I'm going to find if I find look for, say, three clusters here, I'm just going to separate it into three segments like this and OK. You can say, well, maybe this clustering is not very interesting or whatnot.",
                    "label": 0
                },
                {
                    "sent": "But it really does represent a clustering on the correct on the full distribution.",
                    "label": 0
                },
                {
                    "sent": "However, if I maybe have.",
                    "label": 0
                },
                {
                    "sent": "If I have maybe not enough points OK. And I find some clustering here then.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to find this clustering, but maybe this structure that I find here doesn't really tell me anything about the correct distribution because it just didn't observe enough points.",
                    "label": 0
                },
                {
                    "sent": "And if I did observe enough points, I'll see that.",
                    "label": 0
                },
                {
                    "sent": "Actually, I have one cluster here, one cluster here, and one cluster here, maybe an and what I found just doesn't relate to the complete distribution.",
                    "label": 0
                },
                {
                    "sent": "So really this is a matter of generalization, and really so you can think of it really.",
                    "label": 0
                },
                {
                    "sent": "It's actually question about the sample size.",
                    "label": 1
                },
                {
                    "sent": "Do I have enough samples?",
                    "label": 0
                },
                {
                    "sent": "Another way to phrase this, which is a bit different and actually like it much listener explaining a bit why is not generalization of the structure itself.",
                    "label": 1
                },
                {
                    "sent": "Visualization of the objective value.",
                    "label": 0
                },
                {
                    "sent": "So in the first 2 points here, I'm kind of asking.",
                    "label": 0
                },
                {
                    "sent": "Are these?",
                    "label": 0
                },
                {
                    "sent": "Do these clusters exactly?",
                    "label": 0
                },
                {
                    "sent": "These specials exists and relate to the true clusters where defined true is minimizing the objective and the full distribution.",
                    "label": 0
                },
                {
                    "sent": "Another similar question we can ask is I have some clustering and I calculated the objective value.",
                    "label": 0
                },
                {
                    "sent": "So that means objective clustering on my finite set as well.",
                    "label": 0
                },
                {
                    "sent": "If I take the same clustering and calculate the objective value on the true distribution, will I also get something that's similar orbits as low in this type of objective?",
                    "label": 0
                },
                {
                    "sent": "Might be interesting to us really for doing things.",
                    "label": 0
                },
                {
                    "sent": "Where the OK?",
                    "label": 0
                },
                {
                    "sent": "So maybe I'll talk about this.",
                    "label": 0
                },
                {
                    "sent": "Another slide I think OK so well, maybe trying to do now is just formalize these two things.",
                    "label": 0
                },
                {
                    "sent": "So basically the type of.",
                    "label": 0
                },
                {
                    "sent": "So you were talking about your central classroom schemes, where you can treat your data is IID samples.",
                    "label": 0
                },
                {
                    "sent": "So you would exclude situations like.",
                    "label": 0
                },
                {
                    "sent": "I did you a similarity matrix and you have to find.",
                    "label": 0
                },
                {
                    "sent": "Normalized type solution.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Lots of like OK so then you have to be a.",
                    "label": 0
                },
                {
                    "sent": "It still can apply, but definitely it becomes harder to talk about things here I'm.",
                    "label": 0
                },
                {
                    "sent": "What would be, for example, the expected cost?",
                    "label": 0
                },
                {
                    "sent": "So you can still talk about in the following way.",
                    "label": 0
                },
                {
                    "sent": "You have a distribution over objects and what you have about these objects is just the similarity.",
                    "label": 0
                },
                {
                    "sent": "But you can still talk about.",
                    "label": 0
                },
                {
                    "sent": "You can still talk about this as a IID data, so you have.",
                    "label": 0
                },
                {
                    "sent": "I have some if I want to.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Formulation.",
                    "label": 0
                },
                {
                    "sent": "The dissimilarity matrix.",
                    "label": 0
                },
                {
                    "sent": "IID from fun super.",
                    "label": 0
                },
                {
                    "sent": "Instances, but the agency only one server from there.",
                    "label": 0
                },
                {
                    "sent": "OK, so there may be 2 separate issues here and let me try to see if I understand what your concerns are.",
                    "label": 0
                },
                {
                    "sent": "So one issue is it really and this is.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll try to separate distance of two things.",
                    "label": 0
                },
                {
                    "sent": "One maybe think that you may be asking about or if you're not concerned about is it really in many cases we don't.",
                    "label": 0
                },
                {
                    "sent": "This idea assumption would be problematic because we really have one data set in this data set, doesn't it doesn't really make sense to think about it is coming from some ID's or so.",
                    "label": 0
                },
                {
                    "sent": "For example, if we're looking at.",
                    "label": 0
                },
                {
                    "sent": "We're looking at the data set, which is either like the entire population or data set, which is all the genes then really.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's that's it, and there's no ID source here.",
                    "label": 0
                },
                {
                    "sent": "So is this the issue or no?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's OK.",
                    "label": 0
                },
                {
                    "sent": "So my second guessed about the issue here is that we have a rather than than the only information that I have about instances is pairwise distance information.",
                    "label": 0
                },
                {
                    "sent": "OK, but in this case what I would say is not that.",
                    "label": 0
                },
                {
                    "sent": "The distance matrix is generated from some IID distribution.",
                    "label": 0
                },
                {
                    "sent": "Someday generate the distances independently, but rather that I have some abstract SpaceX where I have some distribution of this abstract under some data that comes ID from this abstract SpaceX and just the only information that I have is the X One X2 for any.",
                    "label": 0
                },
                {
                    "sent": "Two points from this abstract space, but it can still talk about having in distribution D / X and the data coming from your ID.",
                    "label": 0
                },
                {
                    "sent": "ID copies of this distribution.",
                    "label": 0
                },
                {
                    "sent": "I don't see a problem with that.",
                    "label": 0
                },
                {
                    "sent": "It does make some of the statements of the trickier, and I'm not going to actually.",
                    "label": 0
                },
                {
                    "sent": "Refer directly to this buddy, but I don't see any problem with this setup.",
                    "label": 0
                },
                {
                    "sent": "There's no problem with that setup, but you might never get enough data to identify.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                },
                {
                    "sent": "To identify your distribution to some degree.",
                    "label": 0
                },
                {
                    "sent": "In your SpaceX, but you might have enough data to identify a partitioning of of the objects based on these eggs.",
                    "label": 0
                },
                {
                    "sent": "No, that's that's perfectly fine, right?",
                    "label": 0
                },
                {
                    "sent": "So OK, so I'm not trying to identify distributions.",
                    "label": 1
                },
                {
                    "sent": "OK, let me OK, maybe I should even put in side.",
                    "label": 0
                },
                {
                    "sent": "Probably when I'm trying to get at is that I'm trying to use the ability to figure out to what extend it to what resolution do I actually have?",
                    "label": 0
                },
                {
                    "sent": "Support the evidence in the data?",
                    "label": 0
                },
                {
                    "sent": "They have, so there might be some very complicated structure in the data, so the true distribution for example might have a cluster here in the cluster.",
                    "label": 0
                },
                {
                    "sent": "Here in the cluster here in the cluster here and the cluster here.",
                    "label": 0
                },
                {
                    "sent": "So if I get really too few data points on its three data points, I might not have support for anything.",
                    "label": 0
                },
                {
                    "sent": "So say I only have support for K plus one only cables.",
                    "label": 0
                },
                {
                    "sent": "Hopefully I could tell that the only K = 1 being stable will see in a bit.",
                    "label": 0
                },
                {
                    "sent": "It's a bit trickier than that.",
                    "label": 0
                },
                {
                    "sent": "If I start having more data, I might have support for two clusters, and I don't know if you want to say that really this data set has two clusters or not.",
                    "label": 1
                },
                {
                    "sent": "I mean I. I don't know what your definition of correct number of clusters is.",
                    "label": 0
                },
                {
                    "sent": "Again, I I don't believe in this concept of correct number of clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, because there are two clusters here and if I have enough data, I'm going to support if it to say that really I'm going to find.",
                    "label": 0
                },
                {
                    "sent": "One cluster here in one cluster.",
                    "label": 0
                },
                {
                    "sent": "Here in this structure actually does exist in a true data, and so I'm going to say that cake was 1 = 2 is fine here when they have more data, I'm going to find these two clusters and I'm also going to find 5 clusters and then I'm not going to try to use the ability to say is K = 2 better than K + 5 = 5 better than K + 2.",
                    "label": 0
                },
                {
                    "sent": "That's in the sense.",
                    "label": 0
                },
                {
                    "sent": "Maybe a question about the the true distribution and what your notion of good clustering is that I don't want to get into.",
                    "label": 0
                },
                {
                    "sent": "OK, what I'm going to want to be able to say is that if I have enough data.",
                    "label": 0
                },
                {
                    "sent": "Then both the clustering into clustering is supported by the data and the clustering to five clustering is supported by the data.",
                    "label": 0
                },
                {
                    "sent": "But maybe if I have less data then the clustering into only clustering into clustering and in that sense if I have infinite data then it really I mean really clustering into any number of strings at some point is going to be supported unless I have symmetries that mean that I'm not stable at the limit.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm not trying here to say that I can really at some point say everything about the distribution DM.",
                    "label": 0
                },
                {
                    "sent": "Just trying to see to say for a specific clustering is that clustering actually supported or is this just artifact of the fact that I have don't have enough data?",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular, the way it can be face is as follows.",
                    "label": 0
                },
                {
                    "sent": "So basically what I want to be able to make a statement like this that if I have stability in other words, if for two randomly drone subset the distance between their clustering is small with high probability, then I want to be able to transform it into saying that I'm very close to the correct clustering that with high probability.",
                    "label": 0
                },
                {
                    "sent": "My question in my sample is close to the clustering of the.",
                    "label": 0
                },
                {
                    "sent": "Missing parentheses here maybe, but close to the clustering on the entire distribution.",
                    "label": 0
                },
                {
                    "sent": "So and again, this is a.",
                    "label": 0
                },
                {
                    "sent": "Here is maybe I should add kayson subtext here for you should think about is clustering with some specific number of clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this example if I have some intermediate number of examples, presumably this will be stable for two clusters, and so the two cluster structure would generalize, but maybe not stable for five clustering, and so the five cluster structure would not generalize.",
                    "label": 0
                },
                {
                    "sent": "And later on the face structure Cluster 5 cluster structure must might start generalizing.",
                    "label": 0
                },
                {
                    "sent": "So we want to contrast this with the.",
                    "label": 0
                },
                {
                    "sent": "Different notion and this is looking more standard notion of generalization that when we were familiar from the from classification.",
                    "label": 0
                },
                {
                    "sent": "This is that our objective function generalizes OK, so this is again if we have stability thing, then we can obtain some statement that says that are this.",
                    "label": 0
                },
                {
                    "sent": "We can measure.",
                    "label": 0
                },
                {
                    "sent": "This is our objective objective on our on our subset or empirical estimate of the objective function and if we can bound the expected value objective functional entire data set by.",
                    "label": 0
                },
                {
                    "sent": "Our empirical estimate plus 70 something that depends on on the stability and maybe also in the sample size so.",
                    "label": 0
                },
                {
                    "sent": "This type of statements are easier to obtain, but really want to.",
                    "label": 0
                },
                {
                    "sent": "It's type of saving so much less care about because.",
                    "label": 0
                },
                {
                    "sent": "Physical conceptually is I view it, unlike in classification and classification, we have the objective where minimizing is really the objective we care about are almost objectively care, but I mean they really objective.",
                    "label": 0
                },
                {
                    "sent": "We know exactly what the objective we care about is.",
                    "label": 0
                },
                {
                    "sent": "It's the.",
                    "label": 0
                },
                {
                    "sent": "It's the expected expected misclassification error specification error over the entire distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so that exactly we can't measure, but at least we can.",
                    "label": 0
                },
                {
                    "sent": "We can have a good estimator fit that we can talk about how well an estimator, if it is, and that's our empirical classification error, and so these type of statements really makes sense.",
                    "label": 0
                },
                {
                    "sent": "I mean, we have an empirical estimate of the quantity that we are interested in, and we use it to bound the quantity that are really, really interested in.",
                    "label": 0
                },
                {
                    "sent": "OK, this isn't classification.",
                    "label": 0
                },
                {
                    "sent": "However, I would argue that in clustering.",
                    "label": 0
                },
                {
                    "sent": "In many cases, the real objective we're interested in is not minimizing.",
                    "label": 0
                },
                {
                    "sent": "This came in the objective.",
                    "label": 0
                },
                {
                    "sent": "So I mean, sometimes it is.",
                    "label": 0
                },
                {
                    "sent": "I mean, if if we're doing something like facility location or doing vector quantization for compression, then really the K means objective is what we're interested in.",
                    "label": 0
                },
                {
                    "sent": "I mean, we want to just be able to approximate the data, and we can we care about how much how many extra bits it's going to take us, or how many extra miles or dollars or whatnot.",
                    "label": 0
                },
                {
                    "sent": "But in most cases, in clustering at least, it seems that we're talking about here what we really care about minimizing is this.",
                    "label": 0
                },
                {
                    "sent": "It's the distance between the clustering that we obtain and the some true clustering that we have no idea about.",
                    "label": 0
                },
                {
                    "sent": "The problem is that this is something you really can't estimate.",
                    "label": 0
                },
                {
                    "sent": "I mean, even on our empirical, even in our sample it's not something we can get an easy empirical estimate for.",
                    "label": 0
                },
                {
                    "sent": "But it is actually what we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So when there's some in a much more complicated situation in which we're really interested in one objective, but because we have no idea how to approach it, then we actually minimize the different objective, and in certain cases we can argue that the optimum of this objective will actually give us the true clustering, or interested in.",
                    "label": 0
                },
                {
                    "sent": "For example, if we have data, that's Vic some strong assumptions on our data data that comes from Gaussian mixtures.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to say is that.",
                    "label": 0
                },
                {
                    "sent": "That minimizing this objective, for example K means is sensible.",
                    "label": 0
                },
                {
                    "sent": "If you really want to find this, OK, no, but it's definitely sensible for you.",
                    "label": 0
                },
                {
                    "sent": "Find Sadie what what I'm trying to argue is if we have some true underlying clustering and we have some assumptions in Metro\nClustering, for example, that it's the clusters are.",
                    "label": 0
                },
                {
                    "sent": "Are Gaussians or even we can make, uh, we don't even need him to be Gaussian.",
                    "label": 0
                },
                {
                    "sent": "All we need him to be as have some kind of spherical structure, but say each Gaussian is each each.",
                    "label": 0
                },
                {
                    "sent": "This cluster is Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Then we know that at least in limit, minimizing the K means objective or minimizing, maximizing the likely relative to a Gaussian mixture model is actually going to give us the correct clustering in the limit.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, it's actually it's reasonable to use this objective function.",
                    "label": 0
                },
                {
                    "sent": "However, it doesn't necessarily mean that if I get if I don't find the optimum, but I get some kind of like approximation here, it doesn't necessarily tell me something about how close I am to the the.",
                    "label": 0
                },
                {
                    "sent": "Correct structure.",
                    "label": 0
                },
                {
                    "sent": "Tell me where J actually enters in that line over here?",
                    "label": 0
                },
                {
                    "sent": "Yes, so OK it's a bit OK, so the way J the way J interests into a is it A is defined as the minimizer of J over the sample.",
                    "label": 0
                },
                {
                    "sent": "OK, but OK.",
                    "label": 0
                },
                {
                    "sent": "But what I'm saying is if that's not clear how it enters into what I'm really interested in, an is an example of how getting a very good approximation.",
                    "label": 0
                },
                {
                    "sent": "So maybe another issue is.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't know what's wrong if you give me an approximation that's within I don't know how to translate this function.",
                    "label": 0
                },
                {
                    "sent": "Into this epsilon here.",
                    "label": 0
                },
                {
                    "sent": "I mean if you give me something, that's the K means is approximately 203 four and what does it tell me about how similar my clusters are?",
                    "label": 0
                },
                {
                    "sent": "What I care about is really is getting most of the points correctly clustered?",
                    "label": 0
                },
                {
                    "sent": "What how does doing translate so?",
                    "label": 0
                },
                {
                    "sent": ".3 four?",
                    "label": 0
                },
                {
                    "sent": "Also, relative measures don't help me and this is what I'm going to show here.",
                    "label": 0
                },
                {
                    "sent": "And this is an example of how I can really.",
                    "label": 0
                },
                {
                    "sent": "AD AD is your desired gold partition, yes, well, OK.",
                    "label": 0
                },
                {
                    "sent": "So easy, it's just the minimizer.",
                    "label": 0
                },
                {
                    "sent": "It's the same way except replace S with D, so it's the minimizer off my objective.",
                    "label": 0
                },
                {
                    "sent": "You know what you go to?",
                    "label": 0
                },
                {
                    "sent": "Partition is no but but Jay, let's say let's say, well, if you get this Oracle and you have access to be and then you can calculate AB.",
                    "label": 0
                },
                {
                    "sent": "But still the number of Jay doesn't tell you yes.",
                    "label": 0
                },
                {
                    "sent": "OK so so.",
                    "label": 0
                },
                {
                    "sent": "So I'll show you know so.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying is not exactly the number what what?",
                    "label": 0
                },
                {
                    "sent": "What doesn't tell me OK, is it?",
                    "label": 0
                },
                {
                    "sent": "If I know that I I approximate JK to some?",
                    "label": 0
                },
                {
                    "sent": "Error on Jay is they can tell you how big my error and Jay is or how far I am from the J of the optimal partition.",
                    "label": 0
                },
                {
                    "sent": "OK, save the correct distribution that I can't translate that to how far my how big my clustering error is from this correct gold standard clustering.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to show you this in the next slide, so, but basically I mean if I could optimize it then that would be fine.",
                    "label": 0
                },
                {
                    "sent": "But what I'm saying is if I only have if I have only approximately optimize it and I have some.",
                    "label": 0
                },
                {
                    "sent": "I know by how how close I am to the objective value that doesn't tell me how close I am to the correct clustering.",
                    "label": 0
                },
                {
                    "sent": "I still don't get the point, so is your problem that Jay is not really?",
                    "label": 0
                },
                {
                    "sent": "Modeling correctly, yeah well.",
                    "label": 0
                },
                {
                    "sent": "Doesn't indicate the rate in which the question.",
                    "label": 0
                },
                {
                    "sent": "You here you have the difference of two Vin numbers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think so.",
                    "label": 0
                },
                {
                    "sent": "So unless we try to answer your question it models it only in a very limited extent.",
                    "label": 0
                },
                {
                    "sent": "It models it because it is not the situation that at the end of the day what I really am going to measure my performance by is J and say if I got a better J value, I'm happier if I gotta hire Jay value unless happy when I'm going to measure my my my performance bias by this measure, how far I am from the gold standard clustering.",
                    "label": 0
                },
                {
                    "sent": "However, Jay does capture.",
                    "label": 0
                },
                {
                    "sent": "Uh, is suited to what I want.",
                    "label": 0
                },
                {
                    "sent": "It does capture what I want, but only in the very limited sense.",
                    "label": 0
                },
                {
                    "sent": "Then I know that the optimum of J does actually minimize is actually the optimal for they want.",
                    "label": 0
                },
                {
                    "sent": "But the fact that the optimum's are the same doesn't doesn't mean doesn't tell me how how being often one of them corresponds to being off another.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So maybe I'll show this example OK, so consider so this example I should say was originally constructed to show how getting we have polynomial time approximation schemes for 4K means.",
                    "label": 0
                },
                {
                    "sent": "How this doesn't actually help us cluster in our sense, so considering you have two clusters in, these clusters are very well separated.",
                    "label": 0
                },
                {
                    "sent": "Their 10 standard date, each one of them is spherical Gaussians, and there are 10 standard deviations from each other.",
                    "label": 0
                },
                {
                    "sent": "But they live in very high dimensions, so even in D dimensions but.",
                    "label": 0
                },
                {
                    "sent": "There is still very, very well separated, so there's a very distinct clusters because there's going to be essentially no overlap between them, so we can calculate now that what the K means objective and if we're talking about likely it's going to be very similar to the Caymans objective here, because they're so well separated, so the K means objective of the true clustering.",
                    "label": 0
                },
                {
                    "sent": "I have a contribution of roughly DDS the dimensionality from from each point, so it's going to be roughly times in or the if we look at the normalized by and it's going to be and now I look at the following very silly clustering.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to go through customs at the center very close to the origin of very close to the origin.",
                    "label": 0
                },
                {
                    "sent": "So between the two clusters, so really have no clustering whatsoever, is just nonsensical clustering.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to get the same cost in each they mentioned.",
                    "label": 0
                },
                {
                    "sent": "I'm going to get D. Plus, in the single dimension in which the customers are actually separated, I'm going to get an additional cost of about 25.",
                    "label": 0
                },
                {
                    "sent": "So is my dimensionality increases and I would argue that case at least very interested in high dimensional case then.",
                    "label": 0
                },
                {
                    "sent": "The ratio between these two costs becomes negligible.",
                    "label": 0
                },
                {
                    "sent": "It's going to be 1 + 25 / D. So really, even if even though I have here a very good approximation to the objective value given by a trivial trivial clustering, I really have no approximation whatsoever to what I'm really interested in, which is being able to separate this cloud in this cloud.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So This is why I would argue that.",
                    "label": 0
                },
                {
                    "sent": "I mean, although OK, it's obvious that this maybe, maybe not so obvious, but it's not so difficult to show under certain circumstances that this type of condition implies this type of condition.",
                    "label": 0
                },
                {
                    "sent": "The other way doesn't go out, and this is in some sense easier to obtain.",
                    "label": 0
                },
                {
                    "sent": "It's still not clear to me exactly how I mean.",
                    "label": 0
                },
                {
                    "sent": "There are some very limited ways in which you know to do that, but this type of result is what we're really.",
                    "label": 0
                },
                {
                    "sent": "I would really like to obtain.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So, so it's true that that that the dynamic range is is vanishingly Steve.",
                    "label": 0
                },
                {
                    "sent": "He goes to very high.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Isn't it simply a problem of rescaling everything we scaling by what?",
                    "label": 0
                },
                {
                    "sent": "Fusion on the real axis where you could slip become more and more so much as well.",
                    "label": 0
                },
                {
                    "sent": "How do I scale by?",
                    "label": 0
                },
                {
                    "sent": "It's definitely not where we tried to show.",
                    "label": 0
                },
                {
                    "sent": "Even if I scale by the value of the objective, that doesn't help me.",
                    "label": 0
                },
                {
                    "sent": "So if the scale how?",
                    "label": 0
                },
                {
                    "sent": "How do I?",
                    "label": 0
                },
                {
                    "sent": "How do I know what to scale by?",
                    "label": 0
                },
                {
                    "sent": "At 1:00 this this this range in which your costs live.",
                    "label": 0
                },
                {
                    "sent": "This range is with this dimension going to Infinity.",
                    "label": 0
                },
                {
                    "sent": "We dominated by yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "And maybe it's getting to 0 because it might be a simple rescaling problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so I at least thought it might be.",
                    "label": 0
                },
                {
                    "sent": "I at least don't know how to get a sensible scaling here.",
                    "label": 1
                },
                {
                    "sent": "I want to scale this by so it might be, I mean.",
                    "label": 0
                },
                {
                    "sent": "But usually what you do is you take the minimum, you take the maximum annual minimum, maximum, minimum, maximum over what?",
                    "label": 0
                },
                {
                    "sent": "Of partitions.",
                    "label": 0
                },
                {
                    "sent": "How do I OK?",
                    "label": 0
                },
                {
                    "sent": "So you're saying it's only competition so it might it might.",
                    "label": 0
                },
                {
                    "sent": "OK so it might be I definitely.",
                    "label": 0
                },
                {
                    "sent": "I mean OK so.",
                    "label": 0
                },
                {
                    "sent": "So they definitely might be in that might.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to do that, but that might be a way to kind of.",
                    "label": 0
                },
                {
                    "sent": "Situations rescaling.",
                    "label": 0
                },
                {
                    "sent": "Preserve might preserve all the structure in the spectrum of your cost values.",
                    "label": 0
                },
                {
                    "sent": "Then you are fine.",
                    "label": 0
                },
                {
                    "sent": "The other limit might be that you do a re scaling and the fluctuations contrary to what you expect it amplified send you start OK, then you get a really messy situation where you cannot say anything about the underlying signal, so there is there is some of that.",
                    "label": 0
                },
                {
                    "sent": "There is some of the second situation here because if you take a look at this example we have two clusters here and now.",
                    "label": 0
                },
                {
                    "sent": "Instead of moving both centers to the middle.",
                    "label": 0
                },
                {
                    "sent": "Just move both centers in some random directions.",
                    "label": 0
                },
                {
                    "sent": "Each sensor in some other random direction, so this.",
                    "label": 0
                },
                {
                    "sent": "This preserves the clustering even though the effect on the objective value is the same.",
                    "label": 0
                },
                {
                    "sent": "Right, but.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Discribe.",
                    "label": 0
                },
                {
                    "sent": "An analogy, find procedure how you have to let.",
                    "label": 0
                },
                {
                    "sent": "How do you have to let these two data points stay there?",
                    "label": 0
                },
                {
                    "sent": "Clouds move towards each other so that the overlap between the clouds is the same, so that the limit of the goal is going to Infinity is a very tricky limit.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "You cannot confirm.",
                    "label": 0
                },
                {
                    "sent": "The the rate in which the distance.",
                    "label": 0
                },
                {
                    "sent": "Changes in the function of the rate of convergence of Blue Jays know, but he is considering the details of Kitty litter.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So, but I think what it is possible, it is definitely possible that we often suggested is definitely possible, that we can obtain.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty familiar with book in simplicity, and I don't think that anything they do answer this, but it's definitely possible that you can obtain some kind of.",
                    "label": 0
                },
                {
                    "sent": "Data dependent scaling here that I don't know how to define.",
                    "label": 0
                },
                {
                    "sent": "Definitely ought to calculate that will somehow imply in a useful way that.",
                    "label": 0
                },
                {
                    "sent": "Uh, some kind of relation of this form implies this, so that's definitely possible and would be interesting to me.",
                    "label": 0
                },
                {
                    "sent": "I definitely don't know how to obtain this.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is essentially what I'm doing here, right?",
                    "label": 0
                },
                {
                    "sent": "Because there's all these irrelevant dimensions.",
                    "label": 1
                },
                {
                    "sent": "OK, so as a side note, I should say that even regardless of stability, this is unrelated to stability side nodes, even regardless of stability.",
                    "label": 1
                },
                {
                    "sent": "This type of generalizations on J itself, I believe, are possible to obtain an is a function in various parameters.",
                    "label": 0
                },
                {
                    "sent": "We don't exactly know.",
                    "label": 0
                },
                {
                    "sent": "I don't agree with your face there.",
                    "label": 0
                },
                {
                    "sent": "I know what paper you're talking about.",
                    "label": 0
                },
                {
                    "sent": "Your paper doesn't exactly show this.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll talk about this.",
                    "label": 0
                },
                {
                    "sent": "OK, OK your paper.",
                    "label": 0
                },
                {
                    "sent": "I don't know, I know.",
                    "label": 1
                },
                {
                    "sent": "But it would be nice to see something concrete result here, but it would be nice to see that in what way can I mean this is going to depend on several parameters of the problem and it's going to be nice to see in what way.",
                    "label": 1
                },
                {
                    "sent": "Maybe we can use stability as a replacement for those parameters that can also be interesting.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm more interested in getting results.",
                    "label": 0
                },
                {
                    "sent": "This form of.",
                    "label": 0
                },
                {
                    "sent": "They tell us how stability stability means that we're really close to the goal to the correct clustering, not that objective is made OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem with this is that we can't get this, so in welcome is going to say that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is cheating example, but I really don't think it is so if we look at the following.",
                    "label": 0
                },
                {
                    "sent": "Pulling data set.",
                    "label": 0
                },
                {
                    "sent": "So this is the true distribution.",
                    "label": 0
                },
                {
                    "sent": "The distribution has four clusters, three of them each happen with probability roughly 3rd, and then we have another very very far away cluster that only happens with probability epsilon.",
                    "label": 0
                },
                {
                    "sent": "So what's going to happen is the stability here is going to be very misleading if we look at K = 3 then.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So if you look at the stability for K = 3, so this is instability, say.",
                    "label": 0
                },
                {
                    "sent": "Then, well, for very low values, it's going to be unstable just because pretty much I mean anything is unstable for very low values.",
                    "label": 0
                },
                {
                    "sent": "But then we're going to be able to really identify those three clusters, and we're not going to observe any point from that right cluster.",
                    "label": 0
                },
                {
                    "sent": "So at some point we're going to become stable.",
                    "label": 0
                },
                {
                    "sent": "But at this point we're stable then we're.",
                    "label": 0
                },
                {
                    "sent": "We're not really capturing.",
                    "label": 0
                },
                {
                    "sent": "We're not close by anyway to the correct clustering, because OK, for any most reasonable measures of distance between clusterings.",
                    "label": 0
                },
                {
                    "sent": "Here, the problem is not so much we're missing here.",
                    "label": 0
                },
                {
                    "sent": "Missing this thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so that we're missing this thing might only affect our objective by epsilon.",
                    "label": 0
                },
                {
                    "sent": "The issue here is that in the optimal clustering here on the correct on the full distribution looks probably something like this with three clusters.",
                    "label": 0
                },
                {
                    "sent": "And so all these points list.",
                    "label": 0
                },
                {
                    "sent": "But when we have only finite data and we don't observe you at the rightmost cluster, we're going to get a clustering that looks like this.",
                    "label": 0
                },
                {
                    "sent": "So all these points that are in these two clusters were going to say that they are in different clusters, even though in the supposed gold standard clustering they are in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "So really we're going to have.",
                    "label": 0
                },
                {
                    "sent": "I mean this we're going to miss it all, but that's only an epsilon difference.",
                    "label": 0
                },
                {
                    "sent": "But really, we're going to have a substantial clustering error versus very substantial agreement error versus our gold center clustering.",
                    "label": 0
                },
                {
                    "sent": "So we have not yet convergence sense, but still are.",
                    "label": 0
                },
                {
                    "sent": "Stability is very low.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can hope to get a statement like we had before just to continue this.",
                    "label": 0
                },
                {
                    "sent": "There's also an interesting point here that the stability is not monotone, so if you hope to get something with this form, then OK, this type of relationship is monotone, and if you're saying that stability really implies this type this convergence, then you really should be hoping that stability is going to be on the more data you have, the more stable you're going to.",
                    "label": 0
                },
                {
                    "sent": "You're going to be, but in here we see that once we have roughly.",
                    "label": 0
                },
                {
                    "sent": "One over epsilon data.",
                    "label": 0
                },
                {
                    "sent": "Then suddenly we are going to see data from that remote cluster, and again we're going to be unstable.",
                    "label": 0
                },
                {
                    "sent": "Very gracefully go OK with this depends.",
                    "label": 0
                },
                {
                    "sent": "OK, so it depends.",
                    "label": 0
                },
                {
                    "sent": "It depends where they define this instability here.",
                    "label": 0
                },
                {
                    "sent": "But OK, you're right that.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Expectations.",
                    "label": 0
                },
                {
                    "sent": "Which is smaller?",
                    "label": 0
                },
                {
                    "sent": "Sample size is smaller than one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "Still get some of the time it will get very if I have.",
                    "label": 0
                },
                {
                    "sent": "I don't agree.",
                    "label": 0
                },
                {
                    "sent": "If I have substantially less than one over epsilon, then my probability of observing any point from there would be very low.",
                    "label": 0
                },
                {
                    "sent": "OK, it's it's not going to be a jump, but OK. Lake of the effect of the sample size depends on epsilon.",
                    "label": 0
                },
                {
                    "sent": "How will you detect is perfect, but how effective it is depends on the distance fixed epsilon.",
                    "label": 0
                },
                {
                    "sent": "But the increasing the distance you will increase the effects of even one country.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, let's not.",
                    "label": 0
                },
                {
                    "sent": "Let's not argue if it's going to be a jump or not.",
                    "label": 0
                },
                {
                    "sent": "The point is the point.",
                    "label": 0
                },
                {
                    "sent": "The point is that there is a regime which is going to be stable, even though we're actually far away from the correct thing.",
                    "label": 0
                },
                {
                    "sent": "And then the stability is not going to be monotone going to increase.",
                    "label": 0
                },
                {
                    "sent": "And actually at some point it's going to decrease back because we're at some point going to have, assuming there's not perfect symmetry there.",
                    "label": 0
                },
                {
                    "sent": "I mean, we know that we will actually have stability in the limit.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Is flawed from the statistically POV because people doing what they call or simulation would call all that stuff.",
                    "label": 0
                },
                {
                    "sent": "When you don't see Elyse this rare class today will call it burning face.",
                    "label": 0
                },
                {
                    "sent": "And but when I, but when I have, but when I find out data OK, I I mean.",
                    "label": 0
                },
                {
                    "sent": "The problem from a scientific investigation POV, if you do botany and you never commit yourself to a theory because there might be, there might be strange creatures on Mars which you haven't detected yet.",
                    "label": 0
                },
                {
                    "sent": "But if it you keep in your hypothesis that you never come up with so, so so so so so.",
                    "label": 0
                },
                {
                    "sent": "So I agree.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                },
                {
                    "sent": "I agree, So what I'm going to do is I'm not going to say I'm not going to finish my talk here and say, OK, Stability doesn't teach us anything about finite sample size and we should just not usability what I want to see is how to correct.",
                    "label": 0
                },
                {
                    "sent": "I mean this definition, we cannot expect to get this type of results.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to see, what I'm going to suggest in the second, is how to modify this type of statement so that we might be able.",
                    "label": 0
                },
                {
                    "sent": "It still will be useful, and we might be able to get this result.",
                    "label": 0
                },
                {
                    "sent": "You cannot achieve this.",
                    "label": 0
                },
                {
                    "sent": "For very simple probabilistic means because you have you have extremely rare events, which is extremely high loss, right?",
                    "label": 0
                },
                {
                    "sent": "Obviously probability theory doesn't tell you a lot about which.",
                    "label": 0
                },
                {
                    "sent": "Allstate insurance.",
                    "label": 0
                },
                {
                    "sent": "But that's fine, so.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, but that's fine.",
                    "label": 0
                },
                {
                    "sent": "So let's see how to correct this so that I will be able to get a result.",
                    "label": 0
                },
                {
                    "sent": "Specify.",
                    "label": 0
                },
                {
                    "sent": "I agree that's why I want to correct it.",
                    "label": 0
                },
                {
                    "sent": "So OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The partition, another number of clusters, then the distance between the stable partition that you find an optimal partition is absolutely no, no, no, because again, we're looking here at clustering with a fixed K, right?",
                    "label": 0
                },
                {
                    "sent": "So, so clustering with a fixed K. This is going to be the correct or optimal partition is going to be solid, the solid blue.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "No, but if we fix K, here's three.",
                    "label": 0
                },
                {
                    "sent": "Yes, we fix K. Here is 3, so this is going to be the optimal partition.",
                    "label": 0
                },
                {
                    "sent": "No, but no but.",
                    "label": 0
                },
                {
                    "sent": "Hey sweet sweet sweet.",
                    "label": 0
                },
                {
                    "sent": "Well, that's what we're doing, right?",
                    "label": 0
                },
                {
                    "sent": "We're doing.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's what we're doing here.",
                    "label": 0
                },
                {
                    "sent": "We're looking at.",
                    "label": 0
                },
                {
                    "sent": "We're trying to figure out if our for clustering model actually is modeling the for clustering in the data.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so as you know him.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rightfully pointed out we have to fix this and it's not so difficult to fix it because basically all we want all we need to say is that OK, there might be some crazy stuff going on with low probability and we can't expect to handle these crazy stuff that's going on with low probability.",
                    "label": 0
                },
                {
                    "sent": "So instead of saying.",
                    "label": 0
                },
                {
                    "sent": "Is the simplest fix I could come up with a statically?",
                    "label": 0
                },
                {
                    "sent": "It's maybe not as pleasing so I'll be happy to hear other suggestions, But basically what we want to say is that if we have stability then I can't promise you that I'm close to the gold standard partitioning, but at least I'm close to a gold standard partitioning the way I wrote it.",
                    "label": 0
                },
                {
                    "sent": "Here is a very similar distribution.",
                    "label": 0
                },
                {
                    "sent": "What I really have in mind here is just taking this D prime to be deconditioned on everything except for some rare event.",
                    "label": 0
                },
                {
                    "sent": "So everything except for something that happens probability 1 / sqrt N and the way I wrote it here, is just saying that the Cal divergences between defragment D is low.",
                    "label": 0
                },
                {
                    "sent": "So in other words, I want to say that if I have stability then I know that maybe I'm not close to the optimal thing on my distribution, but I'm close to the optimal thing at a very similar distribution.",
                    "label": 0
                },
                {
                    "sent": "So this type of statement, I believe can be shown, but I mean I don't know how.",
                    "label": 0
                },
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I guess this is taking a bit more than 20 minutes, but.",
                    "label": 0
                },
                {
                    "sent": "I'm OK, so basically this is the first big half.",
                    "label": 0
                },
                {
                    "sent": "OK, so of the talk in the main point here, is that really?",
                    "label": 0
                },
                {
                    "sent": "I think that as I said in the beginning, the way I view stability is not necessarily telling us something about the correct distribution is of questionable or not by telling us something about our sample size and saying do we have enough evidence to support what we find and what I want to see is being able to get a rigorous characterization of really can we?",
                    "label": 0
                },
                {
                    "sent": "Can we turn this into a rigorous statement?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the other thing I want to talk about is a bit different and also mentioned it yesterday, but I viewed a bit differently and this is this is not stability of clustering Now really, but stability of the clustering algorithm, But when?",
                    "label": 0
                },
                {
                    "sent": "Say here stability.",
                    "label": 0
                },
                {
                    "sent": "This is going to be very.",
                    "label": 0
                },
                {
                    "sent": "I really want to try to separate these two things completely.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm assuming that I have a fixed finite set S and I've only been.",
                    "label": 1
                },
                {
                    "sent": "I'm only concerned about it.",
                    "label": 0
                },
                {
                    "sent": "I'm only concerned whether I'm really successfully optimizing my objective, successfully, optimizing my objective on this set is.",
                    "label": 0
                },
                {
                    "sent": "So yesterday we talked about it, talked a bit about this confinement that because our optimization is not perfect, it might affect our view of stability that things might seem less stable than they really are.",
                    "label": 0
                },
                {
                    "sent": "When I'm talking about this now, just the bottom half an really.",
                    "label": 1
                },
                {
                    "sent": "The issue here is whether we can use the ability to answer the question of once we run the M, did we actually find the global optimum?",
                    "label": 0
                },
                {
                    "sent": "So it's very clear.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically what I'm.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically what we do and this is actually what we do all the time here.",
                    "label": 0
                },
                {
                    "sent": "At least I do all the time and I would imagine that many of you do all the time.",
                    "label": 1
                },
                {
                    "sent": "Also, you run the many times from different random initializations and now you can look at this instability in this instability now.",
                    "label": 0
                },
                {
                    "sent": "So I'm assuming he actually when I found the point M actually converges too.",
                    "label": 0
                },
                {
                    "sent": "So there's some issues here of deciding whether two things are actually the same thing and it just didn't let it run long enough.",
                    "label": 0
                },
                {
                    "sent": "But let's assume that I'm running kind of incident number of steps of them and they can look at how.",
                    "label": 0
                },
                {
                    "sent": "Why do they call it instability?",
                    "label": 0
                },
                {
                    "sent": "This is, stability should be here, I guess not equal to.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the if I run them from two different starting points, what's the probability of actually going to the same place right now so?",
                    "label": 0
                },
                {
                    "sent": "No, I'm not sure why I wrote it like this, so this would be called the stability and not instability.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so it's very clear that if every time I run am I get to a different place, it cannot possibly be with reasonably high probability.",
                    "label": 0
                },
                {
                    "sent": "I'm actually finding the global optimum because if it reasonably high probability of finding the global optimum, then with the square of that probability at least I will be getting to the same global optimum again.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming here there's a single global optimum, so it's very clear that if him.",
                    "label": 0
                },
                {
                    "sent": "If I'm unstable, I'm things are bad instabilities, necessary conditions, so we know that if you run them and get two different places, we know that we're not doing anything.",
                    "label": 0
                },
                {
                    "sent": "The question you can we get abound in the other way, in in, in the census is at least what I'm doing all the time.",
                    "label": 0
                },
                {
                    "sent": "Empirically, I'm running him, and if all the time it gets if almost all the time it actually gets the same global optimum.",
                    "label": 0
                },
                {
                    "sent": "Sorry gets to the same point.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty confident actually and not statistically, but just sort of.",
                    "label": 0
                },
                {
                    "sent": "Mentally confident that I did actually succeed in the DM actually succeeded in optimizing it, and I can actually back up back this up also by empirical results.",
                    "label": 0
                },
                {
                    "sent": "So we ran last year a huge empirical study of them.",
                    "label": 0
                },
                {
                    "sent": "We just ran.",
                    "label": 0
                },
                {
                    "sent": "We generated lots of datasets and ran Eminem and various different sizes, and the idea is to try to figure out really when does M find the global optimum.",
                    "label": 0
                },
                {
                    "sent": "And since we generate those data sets, we also knew what could cheat a bit and find what the global thing was.",
                    "label": 0
                },
                {
                    "sent": "And from that huge experiment I can tell you empirically that really, this does seem to be the case of that experiment.",
                    "label": 0
                },
                {
                    "sent": "Did have lots of limitations.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is not proof since it was only specific with generating the data, but we did.",
                    "label": 0
                },
                {
                    "sent": "There was not a single case out of hundreds of millions of runs in which we got stability but did not actually converge to the global optimum, yes?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's true and you have to be a bit more.",
                    "label": 0
                },
                {
                    "sent": "So you have to make the same.",
                    "label": 0
                },
                {
                    "sent": "That's true, so you have to make the same type of.",
                    "label": 0
                },
                {
                    "sent": "An affair modifications we did before, but again I'm not so that's true.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "How many?",
                    "label": 0
                },
                {
                    "sent": "If.",
                    "label": 0
                },
                {
                    "sent": "Then you look stable because you.",
                    "label": 0
                },
                {
                    "sent": "No, but Epson.",
                    "label": 0
                },
                {
                    "sent": "So what I have here many things as many random initializations.",
                    "label": 0
                },
                {
                    "sent": "So if they have.",
                    "label": 0
                },
                {
                    "sent": "I have a really optimal solutions involve a tiny cluster and I'm doing not doing enough random initializations such will never find something in that cluster.",
                    "label": 0
                },
                {
                    "sent": "Then I'm not going to find it, so the reason I'm not so concerned about that and the reason also didn't come up in their experiments is and it can be corrected.",
                    "label": 0
                },
                {
                    "sent": "It is slightly more graceful way, so basically we did assume we knew the minimum cluster, the minimum size of cluster and that's necessary is that came up.",
                    "label": 0
                },
                {
                    "sent": "So the way we did experience, we actually chose not a single.",
                    "label": 0
                },
                {
                    "sent": "If we have K. Clusters we didn't choose K initial centers.",
                    "label": 0
                },
                {
                    "sent": "We chose Kalo K or actually Kalo K or one over the size of the smallest cluster log K random random initialization points, which is why this is happened.",
                    "label": 0
                },
                {
                    "sent": "So if we knew there was a tiny cluster there, we actually started looking for more initial centers, but so definitely in some theoretical results here.",
                    "label": 0
                },
                {
                    "sent": "This would have to come in in some way.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is also as far as I know there's no theoretical results in this, and we think we have an interesting.",
                    "label": 0
                },
                {
                    "sent": "I definitely don't have any and related to this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use the last five minutes and hopefully will give me to talk about something that's not really really that's not directly related to stability, but it's an open question that I'm very interested now in, and it is somewhat related to stability.",
                    "label": 0
                },
                {
                    "sent": "If you look at if you look at this.",
                    "label": 0
                },
                {
                    "sent": "So the question is really we know that in the.",
                    "label": 0
                },
                {
                    "sent": "The weight related to this, we know that if we look at stability of clustering, but the previous notion of profitability we had, if the data set goes to Infinity will always going to be stable under some mild conditions.",
                    "label": 0
                },
                {
                    "sent": "And the question is the same thing happens here.",
                    "label": 0
                },
                {
                    "sent": "So if really we have a lot of data is that is the issue just a matter of sample size.",
                    "label": 0
                },
                {
                    "sent": "If we have enough enough data, will we actually always converge to the global min?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the conjecture is as follows.",
                    "label": 0
                },
                {
                    "sent": "So consider and we're going to limit ourselves here really to just if the very simple case.",
                    "label": 0
                },
                {
                    "sent": "OK, but make it may claims in this case this holds, so consider learning.",
                    "label": 0
                },
                {
                    "sent": "Mixture of Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So in this case we talk about a mixture of unit of spherical Gaussian scape and the important thing here, and this is going to be crucial is that data is actually generated according to our model.",
                    "label": 0
                },
                {
                    "sent": "So we have data generated as a mixture of K Gaussians and now we're trying to fit a Gaussian mixture model and we do this by.",
                    "label": 1
                },
                {
                    "sent": "Minimizing the.",
                    "label": 0
                },
                {
                    "sent": "We do this by by maximizing the likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, so we look for the collection of center, so our model is just specified by K center.",
                    "label": 0
                },
                {
                    "sent": "So by some vector in R2D times KE an we just want to minimize maximize our likelihood with respect to the centers.",
                    "label": 0
                },
                {
                    "sent": "So when OK.",
                    "label": 0
                },
                {
                    "sent": "So this is non convex and typically run any local searches is not related particularly to them.",
                    "label": 0
                },
                {
                    "sent": "If you want any local search we will get definitely stuck in local minima.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "As the amount of data increases as N goes to Infinity, this likelihood just becomes the KL divergent circle divergent, negative scale divergent minus some constant.",
                    "label": 0
                },
                {
                    "sent": "In the claim is that if I look at the scale divergent sit still a nonconvex highly nonconvex function.",
                    "label": 0
                },
                {
                    "sent": "As we know exactly where the global minimum RN has many global minima, so the global minimum are exactly the true centers, but also on any permutation of them.",
                    "label": 0
                },
                {
                    "sent": "So we have many global minima and many different basins in between them.",
                    "label": 0
                },
                {
                    "sent": "We have ridges with several points on those ridges, so there are many critical points here.",
                    "label": 0
                },
                {
                    "sent": "Many subtle points.",
                    "label": 0
                },
                {
                    "sent": "But the claim here is my conjecture is that there for this function there are no local minima, so in the infinite sample limit, if the data actually comes from our model.",
                    "label": 1
                },
                {
                    "sent": "It's just that this assumption is very important here.",
                    "label": 0
                },
                {
                    "sent": "If the data actually comes from a model class in the infinite limit, we will not have local minima.",
                    "label": 0
                },
                {
                    "sent": "We only have global minimum.",
                    "label": 0
                },
                {
                    "sent": "Kate.",
                    "label": 0
                },
                {
                    "sent": "So it's a good question.",
                    "label": 0
                },
                {
                    "sent": "I mean, I, I can tell you, I'm not sure why it should be the case.",
                    "label": 0
                },
                {
                    "sent": "I know why I think it's the case.",
                    "label": 0
                },
                {
                    "sent": "So my reason is that there are several reasons I think this is the case.",
                    "label": 0
                },
                {
                    "sent": "First of all, if we again this, it started out from this big empirical study and what we notice is no matter what the separation is between the.",
                    "label": 0
                },
                {
                    "sent": "Clusters or how they're configured if the data if we take the data set large enough, we will actually M will actually converge the global minimum.",
                    "label": 0
                },
                {
                    "sent": "So This is why we started to think about this.",
                    "label": 0
                },
                {
                    "sent": "And then we looked at some specific examples in which you would expect to get local minima.",
                    "label": 0
                },
                {
                    "sent": "So the classic example in which you would expect to get a local minima is if you have.",
                    "label": 0
                },
                {
                    "sent": "The true distribution has three clusters like this, but actually you initialize 2.",
                    "label": 0
                },
                {
                    "sent": "Oh to one center here in two centers here.",
                    "label": 0
                },
                {
                    "sent": "So OK, if everything is completely symmetric here in the saddle point, but if things are not absolutely symmetric, this is actually not.",
                    "label": 0
                },
                {
                    "sent": "Even though this is going to be a local minima for any reasonable finite sample size, this is not actually local minima in the infinite sample case, and the reason this is the Infinite Temple case one.",
                    "label": 0
                },
                {
                    "sent": "OK, there are two variants of this equation.",
                    "label": 0
                },
                {
                    "sent": "If you fix, the priors were done, fix the problem so if you don't fix the priors.",
                    "label": 0
                },
                {
                    "sent": "I mean it holds in both cases explanations would be different if you don't fix the price this year, is that one?",
                    "label": 0
                },
                {
                    "sent": "Gaussian is completely enough to explain this blob.",
                    "label": 0
                },
                {
                    "sent": "You don't get anything else.",
                    "label": 0
                },
                {
                    "sent": "Anything more.",
                    "label": 0
                },
                {
                    "sent": "Any additional benefit by having another center here.",
                    "label": 0
                },
                {
                    "sent": "This is not true for the finite case, but the infinite case you get absolutely nothing by having another center here.",
                    "label": 0
                },
                {
                    "sent": "And so this allows this center to very slowly move over here because it's not going to have any pull from these guys and it will have a pull from these guys because these guys are not perfectly explained by this single Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Now this is only again, this is only an instant symbol in any reasonable finite sample size.",
                    "label": 0
                },
                {
                    "sent": "You will of course have local minima.",
                    "label": 0
                },
                {
                    "sent": "Basically have this very, very flat near plateau.",
                    "label": 0
                },
                {
                    "sent": "Really quickly again, so come in when you have finite data.",
                    "label": 0
                },
                {
                    "sent": "If you elongate your right cluster a little bit OK, so I'm also allow yourself to estimate the covariance then I don't know what happens.",
                    "label": 0
                },
                {
                    "sent": "OK, so OK.",
                    "label": 0
                },
                {
                    "sent": "So I looked at this quite a bit for spherical Gaussians, both when you even when you either fixed priors or different parts but with only with spherical Gaussians.",
                    "label": 0
                },
                {
                    "sent": "In for spherical Gaussians I'm getting getting more and more confidence about this both because of this empirical study.",
                    "label": 0
                },
                {
                    "sent": "We also have some numeric studies for small.",
                    "label": 0
                },
                {
                    "sent": "Dimensions and small number of clusters.",
                    "label": 0
                },
                {
                    "sent": "And because I mean mostly because I couldn't find a counterexample for varying variances.",
                    "label": 0
                },
                {
                    "sent": "I have not looked at this enough.",
                    "label": 0
                },
                {
                    "sent": "I don't know if a counterexample there, but it didn't look as hard, so I don't know what happens when the variances are also very.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, OK so as I said, the implications here.",
                    "label": 0
                },
                {
                    "sent": "Of course in some sense in the infinite sample, regardless of any separation assumptions or anything like that, M or any local search will converge.",
                    "label": 0
                },
                {
                    "sent": "But this is very very.",
                    "label": 0
                },
                {
                    "sent": "This is still.",
                    "label": 0
                },
                {
                    "sent": "Very loose because I mean we don't know how.",
                    "label": 0
                },
                {
                    "sent": "I mean, we're not really going to get there very quickly.",
                    "label": 0
                },
                {
                    "sent": "The real reason we're basically interested in is because the real quantity of interest here is the.",
                    "label": 0
                },
                {
                    "sent": "The probability under some reasonable initialization, the TM will converge to the true solution, so this is really what we're interested in studying.",
                    "label": 0
                },
                {
                    "sent": "OK, an seeing when large enough this is already going to be close enough to one for some reasonable initialization scheme, and the conjecture can just be stated is saying that for any initialization scheme, PN would always converge to one limit, yes.",
                    "label": 0
                },
                {
                    "sent": "So yes, what do you mean by an unstable local minima?",
                    "label": 0
                },
                {
                    "sent": "They're definitely critical points.",
                    "label": 0
                },
                {
                    "sent": "So, but that's not the local minima then.",
                    "label": 0
                },
                {
                    "sent": "Look, yeah, so local minimum.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess so.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't know.",
                    "label": 0
                },
                {
                    "sent": "I would call that a saddle point because, yeah, so local minima, meaning that it's there's some neighborhood to fit some open neighborhood to fit that.",
                    "label": 0
                },
                {
                    "sent": "It's a minimum in that open neighborhood.",
                    "label": 0
                },
                {
                    "sent": "OK. OK so OK, so sorry for abusing this for non not exactly stability question but I do think it has some relationships again just to summarize the main.",
                    "label": 0
                },
                {
                    "sent": "Two issues that I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "One is this issue of stability, not of clustering, but of the local search and the other is trying to establish some some result of this form.",
                    "label": 0
                },
                {
                    "sent": "The likelihood landscape of of these three classes, this yes.",
                    "label": 0
                },
                {
                    "sent": "Basically, one cluster runs empty, and since it's empty then you can push it in that cluster along it, yes, so depends.",
                    "label": 0
                },
                {
                    "sent": "It's not completely, yeah, so it depends.",
                    "label": 0
                },
                {
                    "sent": "If you fix the if the prices are fixed or not.",
                    "label": 0
                },
                {
                    "sent": "If the prices are not fixed then yes, one of the clusters becomes almost empty.",
                    "label": 0
                },
                {
                    "sent": "It's not completely empty, but it has very low responsibility and then it gets very slowly pushed here.",
                    "label": 0
                },
                {
                    "sent": "So the way I just analyzed it as I looked, I analyzed the derivatives of it and the rivers that you can show.",
                    "label": 0
                },
                {
                    "sent": "You can see the derivatives actually point in this direction.",
                    "label": 0
                },
                {
                    "sent": "They're very, very small, but they point in this direction.",
                    "label": 0
                },
                {
                    "sent": "Derivatives of the likelihood with this very.",
                    "label": 0
                },
                {
                    "sent": "This is very important if you fix the priors, the situation is innocence easier.",
                    "label": 0
                },
                {
                    "sent": "'cause of the fixed the priors you just have too much mess in here 'cause you have 2/3 of the mess here where you really don't need only third of the mess here.",
                    "label": 0
                },
                {
                    "sent": "And then so this causes it to move over here.",
                    "label": 0
                },
                {
                    "sent": "You of course you need.",
                    "label": 0
                },
                {
                    "sent": "You need it to be not completely symmetric, right?",
                    "label": 0
                },
                {
                    "sent": "If it's completely symmetric, I wouldn't know which one to move, but if it's not completely symmetric then the one that captures this better and as far as will stay here and the other one will want to move here because here there is a really deficiency in mess.",
                    "label": 0
                },
                {
                    "sent": "There should be a 2/3 of the mess and I only have 1/3 of the mess.",
                    "label": 0
                },
                {
                    "sent": "Talking about.",
                    "label": 0
                },
                {
                    "sent": "Sample sizes this notion that maybe you have you have enough data to support by versus 2.",
                    "label": 0
                },
                {
                    "sent": "Frightened exactly the opposite.",
                    "label": 0
                },
                {
                    "sent": "Oh they have reached a model.",
                    "label": 0
                },
                {
                    "sent": "Let's say just for segment.",
                    "label": 0
                },
                {
                    "sent": "Then we have equal priority on one to 10 points.",
                    "label": 0
                },
                {
                    "sent": "What happens when you have so very little data and so you sorta single data point?",
                    "label": 0
                },
                {
                    "sent": "Well, that doesn't support.",
                    "label": 0
                },
                {
                    "sent": "Answer exactly.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "And if you have.",
                    "label": 0
                },
                {
                    "sent": "There's no way to be able to reject it, so I think the difference here is in the Asian.",
                    "label": 0
                },
                {
                    "sent": "In between the way you're phrasing the question visionix, you really wanting strong evidence that you have on K, right?",
                    "label": 0
                },
                {
                    "sent": "You want?",
                    "label": 0
                },
                {
                    "sent": "I mean, the way you phrased it.",
                    "label": 0
                },
                {
                    "sent": "Now I'm asking how confident MI about K and what I'm asking here is not how confident I am about K. It's how confident I am about the clustering I get for that fixed K. So in particular, in this example, with the five in two.",
                    "label": 0
                },
                {
                    "sent": "I'm not asking you to be confident that there are only two rather than five, only five, rather than two.",
                    "label": 0
                },
                {
                    "sent": "I'm only asking you how confident you are that given K = 2, This is the correct clustering or the given K = 5.",
                    "label": 0
                },
                {
                    "sent": "This is a curve clustering and then even in the Bayesian sense you would get the same type of behavior with Wi-Fi mean you'd get a very concentrated.",
                    "label": 0
                },
                {
                    "sent": "The vision of this is different.",
                    "label": 0
                },
                {
                    "sent": "In some sense.",
                    "label": 0
                },
                {
                    "sent": "Their answers are similar, but because you may be calculating them is more complicated, but I can strike down mathematically what they are, but there you would be saying that if I don't have enough data then my posterior give condition and cake was two.",
                    "label": 0
                },
                {
                    "sent": "My posterior will be fairly concentrated, so I'll have a lot of my posterior mass very close in terms of D to some solution, whereas condition and K = 5.",
                    "label": 0
                },
                {
                    "sent": "The posterior will be such that was saying a lot of nasty things that are far away in terms of D from any place.",
                    "label": 0
                },
                {
                    "sent": "Like OK?",
                    "label": 0
                },
                {
                    "sent": "Notion of stability.",
                    "label": 0
                },
                {
                    "sent": "Exactly.",
                    "label": 0
                },
                {
                    "sent": "Figure out.",
                    "label": 0
                },
                {
                    "sent": "Bayesian.",
                    "label": 0
                },
                {
                    "sent": "The number for the right K cannot be closed.",
                    "label": 0
                },
                {
                    "sent": "'cause there's no.",
                    "label": 0
                },
                {
                    "sent": "'cause we don't.",
                    "label": 0
                },
                {
                    "sent": "Percent.",
                    "label": 0
                },
                {
                    "sent": "Relative setting right from the beginning and so so.",
                    "label": 0
                },
                {
                    "sent": "Look at the opposite answer.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you keep all hypothesis where you don't have evidence.",
                    "label": 0
                },
                {
                    "sent": "So thanks for the talk.",
                    "label": 0
                }
            ]
        }
    }
}