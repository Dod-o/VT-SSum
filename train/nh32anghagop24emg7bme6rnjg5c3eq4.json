{
    "id": "nh32anghagop24emg7bme6rnjg5c3eq4",
    "title": "Knowledge Graph Fact Prediction via Knowledge-Enriched Tensor Factorization",
    "info": {
        "author": [
            "Tim Finin, University of Maryland"
        ],
        "published": "Nov. 27, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_finin_graph_fact_prediction/",
    "segmentation": [
        [
            "So I'm happy to be here.",
            "This is work primarily done by anchor Padya, who just received his dissertation last week, submitted his dissertation, but was unable to come today."
        ],
        [
            "And I'll start out with a little motivation.",
            "Something message that you know.",
            "We've all heard a lot, but it's good to keep in mind.",
            "Knowledge graphs of one kind or another had been used for more than 60 years in AI tasks, especially those involving language understanding."
        ],
        [
            "And today, though, it's important to combine knowledge graphs with machine learning to make both of them better."
        ],
        [
            "So that ought to work.",
            "I'll talk about today, I think is.",
            "An illustration of that the two ways knowledge, graphs and machine learning can work together is the first is that.",
            "We can use knowledge graphs to support machine learning applications.",
            "And the second is we can use.",
            "Machine learning to enrich and improve knowledge graphs.",
            "So our Knowledge graph fact prediction has elements of both.",
            "One aspect is that it uses simple meta knowledge extracted from an existing knowledge graph in order to improve the machine learning systems ability to predict relations that aren't in the graph and should be."
        ],
        [
            "So their contributions, I think for this worker, for one is that Knowledge Graph effect prediction system predicts likely relations in a knowledge graph.",
            "An outperforms rescale and similar systems at that task.",
            "Now, one thing we do is we want to identify relations that we believe to hold that are not in the graph or that are in the graph as opposed to producing a ranked list of relations that should be in the graph.",
            "And this is maybe a subtle difference, but some systems like restau, their goal is to produce a ranked list and then you have to figure out a threshold which you think you'll believe they're in there or not.",
            "We want to go straight to the task of fact prediction or link verification as opposed to link ranking.",
            "A third aspect is that we don't require that the Knowledge graph come with a schema.",
            "You know the kind of knowledge graphs we tend to work within.",
            "The semantic web community often have a nice rich schema, but not all knowledge graphs in the world have any kind of schema and instead we have a way of computing relational similarity.",
            "And we use this then in the prediction task and we compute this just from the existing graph without any schema information.",
            "So we've evaluated this system on 8 existing knowledge graph datasets.",
            "Many of them have been used for other fact prediction or link ranking kind of system.",
            "So it makes it easy to compare our results with other results and see when they're better and when they're not, and these existing datasets I'll talk about, but they include things are familiar with like ones that are created from DB pedia or Freebase."
        ],
        [
            "So we'll start with a simple knowledge gripped simple knowledge graph.",
            "Here we got five nodes in a couple of relations and these represent beliefs that are already in a knowledge graph."
        ],
        [
            "And we're going to represent them as a tensor.",
            "Let's consider first the attack relationship and it'll be a simple two by two matrix with entities and the Rose and entities in the columns and a one wherever that relation holds between the two, with the difference between subject and object or object this object."
        ],
        [
            "And so we can continue on this and look at another relation president.",
            "And we get another two by two matrix.",
            "And."
        ],
        [
            "If we continue for this entire so, you know toy graph, we would end up with a tensor that has four of these two by two matrices, one for each relation, and so the idea is that with enough data we might be able to predict, for example, that maybe Michelle Obama is also a citizen in United States, 'cause you might expect in this graph.",
            "That is very likely that if one person is a citizen of the US, their spouse is also a citizen of US.",
            "Doesn't happen all the time, but it's probably very likely."
        ],
        [
            "So what we want to do is learn embeddings for both entities and relations.",
            "And we'll start with this tensor of a size E by E by K, where E is the number of entities and K as a number of relations."
        ],
        [
            "And what we want to do is we want to compute these embedding tensors ER&E transpose that when combined will approximate the original tensor.",
            "So again X is a tensor of size 8 by 8 by KE is the number of entities, case number of relations P is going to be the latent dimension of our embeddings, an E is going to be an entity matrix of size E by P. And R is going to be a contact compact relation tensor of size P by P by K."
        ],
        [
            "So the process will involve learning parameters to minimize the error in reconstructing from the tensor the original tensor from the entity and relation embeddings.",
            "So because Obama President US is a triple list in the original graph, we want to make you know, we hope that when we combine the embeddings for Obama President in US, it will be one or close to 1."
        ],
        [
            "So again, we're kind of building on the work of rescale, and in fact the framework that for the equations will have.",
            "The first 2 components are basically the base model for raskal.",
            "There's a little bit of variation in how we're doing the regularization, but it's basically rescales model.",
            "What's new is the third component, which is the prior information about the graph.",
            "And what we're going to do is we're going to compute a matrix CIJ.",
            "That represents for relation ion relation, J their similarity.",
            "And well, how do we do that?"
        ],
        [
            "Well, well, we'll get to how we do that in a minute, but so this is how we would work at the similarity between president and citizen is a particular value and then will use that to decide.",
            "As part of the embedding process."
        ],
        [
            "So how do we do that?",
            "Well, we experimented with 405 simple.",
            "Ways we could compute these?",
            "Similarity metrics the first one is symmetry, and that's just a number that says.",
            "How often do the two relations I&J for that similarity value share either a subject or an object.",
            "So relations that involve the same entities are likely to be more similar than ones that don't in original graph.",
            "The second one is agency.",
            "This is a little narrower, says how often do the relations I&J share the same subject?",
            "The third one is similar, but for objects patient.",
            "How often do relations I&J share the same object?",
            "The 4th one is what we call transitivity, and it's how often does our subies object the same as arsham are subject subject and then reverse transitivity is like that, except it goes in the other direction, so these are very simple.",
            "Oh, thanks and it's easy to compute this if you're given a graph over over all the pairs of relations."
        ],
        [
            "So as we'll see, when we well, when I show show some of the results.",
            "Our experiments showed that the transitivity relation of those, 5 models.",
            "Seem to give us the did give us the best results, so so that's the one we're using when we recommend using.",
            "And it's especially true for graphs derived from DB pedia in freebase, which tend to have a low density as opposed to some of the other graphs.",
            "I'll mention that we evaluate it against just because their value they're used for evaluating lots of similar kind of relation predicting algorithms.",
            "What we show here is a heat map of the.",
            "Relation similarity matrix for transitivity for the Win 18 RR data set.",
            "And if you look at the paper, especially if you look at the long paper in the Journal preprints available.",
            "You'll see a lot of other heat Maps for.",
            "All combinations."
        ],
        [
            "So here are the 8 datasets we looked at and.",
            "Most of these, I think, except for DB10K.",
            "Are ones that other people generated and have been used as standard models for link prediction or link ranking?",
            "Now there's two of these things, kinship and you M LS.",
            "Notice that the last column we show the graph density.",
            "Which is related to, you know how?",
            "What's the average degree of links coming in and out of a of a node and kinship and Umm, less are little unusual with respect together because their density is very high.",
            "Freebase 15237 is much lower but still not as low as some of the other ones we looked at, including the base 10K, which is a data set we created and Freebase 13.",
            "So when we evaluate things, we're going to use.",
            "Not hits it.",
            "Hits a 10 or hits at one like rescale.",
            "Often uses, but we're going to use area under the curve because that's also the main metric that rescale, and a similar systems have used and we wanted to be able to compare."
        ],
        [
            "Work with those.",
            "So here's just an overview of, well, you know the details of the area under the curve metrics for the two systems to rescale.",
            "Best, Callan is non negative variant and four models that we created.",
            "So each of these models represents a slightly different equation that we experimented with.",
            "And the last one, which is a model that has.",
            "Is quadratic and has a constrained factor is best overall.",
            "It's not always the best, but where it's not the best.",
            "It's pretty.",
            "It's pretty close compared to other models.",
            "But you notice that it still has trouble comparing doing well on some of the really high density.",
            "Gray"
        ],
        [
            "Yes.",
            "If we look at this in a graphic form for the high density graphs, kinship and Umm, less.",
            "Um, almost all these systems did about this.",
            "All these models that about the same as rescale.",
            "They didn't necessarily do better.",
            "What is kinship?",
            "Kinship is a data graph of think 104 entities.",
            "Each one represents a person and their family relationships between them, and almost every node is connected to almost every other node, so it's kind of an unusual graph when you compare it to something like the graph you get with DB pedia or Freebase or wiki data."
        ],
        [
            "When we look at the low density knowledge graphs and blacked out some of the other ones here, we can see that the quadratic cost constrained model we have.",
            "Does do better than rest Cal with the exception of Freebase 15237 where SQL still does a little bit better?",
            "An Freebase 15237 has a relatively high.",
            "At.",
            "Density compared to the others."
        ],
        [
            "So when we compare this to a different kind of approach, neural systems like tranzit, we see that this our system there are quadratically constrained on these datasets.",
            "We we tested did much better than tranzit"
        ],
        [
            "So, so that's a basic system for future work.",
            "It's we're going to experiment with some additional relational similarity models.",
            "We just had five rather simple ones that might improve things, and then we're looking at various kinds of applications of it for natural language processing, filtering out perhaps mistakes for taking an existing knowledge graph, and trying to identify relations that are suspect, and finally 4.",
            "Reasoning over a cyber security graph we've constructed out of cyber threat intelligence feeds and sticks NVD, another semi structured cyber security data."
        ],
        [
            "So the conclusions we reach our that this is a this is a novel tensor factorization approach for knowledge graph fact prediction.",
            "Again fact prediction as opposed to just coming up with a list of rank facts that should be in the graph or could be in the graph.",
            "We're going further and making a decision about whether it is or isn't in the graph.",
            "And it gives state of the art results on many graphs in particular graphs that have low density, which are ones that we think are kind of more interesting and used in the Community, is possible if you wanted to use our system, you could experiment with your particular graph and you might find that the linear model works better than the quadratic model, for example.",
            "Ugh."
        ],
        [
            "Code and datasets are available in GitHub.",
            "We've got also in a GitHub repository the Journal of Web Semantics, preprint, as well as the eswick Journal direct paper.",
            "And if you have hard detailed questions, you should address them to Doctor Ron Carpathia as of this week, joined Phillips Research America in Cambridge, MA.",
            "So I'll stop there and see if there are any questions.",
            "Text seem very interesting.",
            "Have you made any experiment you mentioned information extraction so trying to find systematic errors out of information, instruction pipelines, and have you any results yet?",
            "Or is just for future work?",
            "We've done a little bit of work on it, but we haven't really applied this to try to identify errors in a language understanding system.",
            "So you mentioned because basically what you mean is that you can use this this approach to extract from resources, for example for lexical resources and then you can eventually predict something from information instruction or you want to directly to learn from information extraction, validated graphs and then.",
            "Yeah, so you know if you wanted to start with a subset of wiki data, you could predict some relations that probably hold that aren't there, and then if you were running on natural language, understanding system and you've already put you produce the graph, you could run it on that and you might find some links in there that themselves, although there in the graph or we have kind of a low likelihood and they might be suspected.",
            "It was great.",
            "Thanks for the talk.",
            "You mentioned that you are using different formulas to combine the different measures that you have in your table.",
            "You mentioned that you're using different combinations, so how did you come up with those?",
            "That's the kind of hard question that Honker has to answer, but he came up with the four four models.",
            "Whether the model was regularised or not, and whether it was linear, quadratic.",
            "And evaluated all four and overall the one that was.",
            "Quadratic and constrained with for regularization.",
            "Did best, but on some graphs, maybe the linear one would do with you a little better, but overall you know that would be the one that we would tend to use.",
            "I have questions about the confusion.",
            "You said that you assume about this schema of the graph, but then your thanks are relational tensor, an entity sensors.",
            "You have to know the list of relations list of entities in the graph.",
            "Before you do that, right right?",
            "So it's depending on the schema of the graph.",
            "Wherever I mean if I gave you a Neo 4 J graph, you could find all the all the relations that were in it and there would be no schema.",
            "By by by having no schema, I'm thinking of know semantic schema.",
            "For example, in the semantic Web you might know that you might have a lot of type information.",
            "You might have domain and range constraints.",
            "You might have all kinds of semantic relations as part of your schema.",
            "But all we do is we take a look at the instances in the graph and the labeled relations between them, like president of or works for whatever.",
            "Yeah, and so I think of it is running on on the instances of a regular semantic web graph and just ignoring all the metadata.",
            "Only one slide I left out because it was right about time.",
            "Had to do with the time and it runs about the same speed and has about the same space.",
            "Constraint says rescale and it's a lot faster I think than the neural network techniques, so that's that's another advantage of this approach.",
            "OK, let's thank him again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm happy to be here.",
                    "label": 0
                },
                {
                    "sent": "This is work primarily done by anchor Padya, who just received his dissertation last week, submitted his dissertation, but was unable to come today.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll start out with a little motivation.",
                    "label": 0
                },
                {
                    "sent": "Something message that you know.",
                    "label": 0
                },
                {
                    "sent": "We've all heard a lot, but it's good to keep in mind.",
                    "label": 0
                },
                {
                    "sent": "Knowledge graphs of one kind or another had been used for more than 60 years in AI tasks, especially those involving language understanding.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And today, though, it's important to combine knowledge graphs with machine learning to make both of them better.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that ought to work.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about today, I think is.",
                    "label": 0
                },
                {
                    "sent": "An illustration of that the two ways knowledge, graphs and machine learning can work together is the first is that.",
                    "label": 0
                },
                {
                    "sent": "We can use knowledge graphs to support machine learning applications.",
                    "label": 0
                },
                {
                    "sent": "And the second is we can use.",
                    "label": 0
                },
                {
                    "sent": "Machine learning to enrich and improve knowledge graphs.",
                    "label": 1
                },
                {
                    "sent": "So our Knowledge graph fact prediction has elements of both.",
                    "label": 0
                },
                {
                    "sent": "One aspect is that it uses simple meta knowledge extracted from an existing knowledge graph in order to improve the machine learning systems ability to predict relations that aren't in the graph and should be.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So their contributions, I think for this worker, for one is that Knowledge Graph effect prediction system predicts likely relations in a knowledge graph.",
                    "label": 1
                },
                {
                    "sent": "An outperforms rescale and similar systems at that task.",
                    "label": 1
                },
                {
                    "sent": "Now, one thing we do is we want to identify relations that we believe to hold that are not in the graph or that are in the graph as opposed to producing a ranked list of relations that should be in the graph.",
                    "label": 0
                },
                {
                    "sent": "And this is maybe a subtle difference, but some systems like restau, their goal is to produce a ranked list and then you have to figure out a threshold which you think you'll believe they're in there or not.",
                    "label": 0
                },
                {
                    "sent": "We want to go straight to the task of fact prediction or link verification as opposed to link ranking.",
                    "label": 0
                },
                {
                    "sent": "A third aspect is that we don't require that the Knowledge graph come with a schema.",
                    "label": 0
                },
                {
                    "sent": "You know the kind of knowledge graphs we tend to work within.",
                    "label": 0
                },
                {
                    "sent": "The semantic web community often have a nice rich schema, but not all knowledge graphs in the world have any kind of schema and instead we have a way of computing relational similarity.",
                    "label": 0
                },
                {
                    "sent": "And we use this then in the prediction task and we compute this just from the existing graph without any schema information.",
                    "label": 0
                },
                {
                    "sent": "So we've evaluated this system on 8 existing knowledge graph datasets.",
                    "label": 0
                },
                {
                    "sent": "Many of them have been used for other fact prediction or link ranking kind of system.",
                    "label": 0
                },
                {
                    "sent": "So it makes it easy to compare our results with other results and see when they're better and when they're not, and these existing datasets I'll talk about, but they include things are familiar with like ones that are created from DB pedia or Freebase.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we'll start with a simple knowledge gripped simple knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "Here we got five nodes in a couple of relations and these represent beliefs that are already in a knowledge graph.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're going to represent them as a tensor.",
                    "label": 0
                },
                {
                    "sent": "Let's consider first the attack relationship and it'll be a simple two by two matrix with entities and the Rose and entities in the columns and a one wherever that relation holds between the two, with the difference between subject and object or object this object.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we can continue on this and look at another relation president.",
                    "label": 0
                },
                {
                    "sent": "And we get another two by two matrix.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we continue for this entire so, you know toy graph, we would end up with a tensor that has four of these two by two matrices, one for each relation, and so the idea is that with enough data we might be able to predict, for example, that maybe Michelle Obama is also a citizen in United States, 'cause you might expect in this graph.",
                    "label": 1
                },
                {
                    "sent": "That is very likely that if one person is a citizen of the US, their spouse is also a citizen of US.",
                    "label": 0
                },
                {
                    "sent": "Doesn't happen all the time, but it's probably very likely.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we want to do is learn embeddings for both entities and relations.",
                    "label": 0
                },
                {
                    "sent": "And we'll start with this tensor of a size E by E by K, where E is the number of entities and K as a number of relations.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we want to do is we want to compute these embedding tensors ER&E transpose that when combined will approximate the original tensor.",
                    "label": 0
                },
                {
                    "sent": "So again X is a tensor of size 8 by 8 by KE is the number of entities, case number of relations P is going to be the latent dimension of our embeddings, an E is going to be an entity matrix of size E by P. And R is going to be a contact compact relation tensor of size P by P by K.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the process will involve learning parameters to minimize the error in reconstructing from the tensor the original tensor from the entity and relation embeddings.",
                    "label": 0
                },
                {
                    "sent": "So because Obama President US is a triple list in the original graph, we want to make you know, we hope that when we combine the embeddings for Obama President in US, it will be one or close to 1.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, we're kind of building on the work of rescale, and in fact the framework that for the equations will have.",
                    "label": 0
                },
                {
                    "sent": "The first 2 components are basically the base model for raskal.",
                    "label": 1
                },
                {
                    "sent": "There's a little bit of variation in how we're doing the regularization, but it's basically rescales model.",
                    "label": 0
                },
                {
                    "sent": "What's new is the third component, which is the prior information about the graph.",
                    "label": 1
                },
                {
                    "sent": "And what we're going to do is we're going to compute a matrix CIJ.",
                    "label": 0
                },
                {
                    "sent": "That represents for relation ion relation, J their similarity.",
                    "label": 0
                },
                {
                    "sent": "And well, how do we do that?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, well, we'll get to how we do that in a minute, but so this is how we would work at the similarity between president and citizen is a particular value and then will use that to decide.",
                    "label": 0
                },
                {
                    "sent": "As part of the embedding process.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, we experimented with 405 simple.",
                    "label": 0
                },
                {
                    "sent": "Ways we could compute these?",
                    "label": 0
                },
                {
                    "sent": "Similarity metrics the first one is symmetry, and that's just a number that says.",
                    "label": 0
                },
                {
                    "sent": "How often do the two relations I&J for that similarity value share either a subject or an object.",
                    "label": 1
                },
                {
                    "sent": "So relations that involve the same entities are likely to be more similar than ones that don't in original graph.",
                    "label": 0
                },
                {
                    "sent": "The second one is agency.",
                    "label": 0
                },
                {
                    "sent": "This is a little narrower, says how often do the relations I&J share the same subject?",
                    "label": 1
                },
                {
                    "sent": "The third one is similar, but for objects patient.",
                    "label": 0
                },
                {
                    "sent": "How often do relations I&J share the same object?",
                    "label": 1
                },
                {
                    "sent": "The 4th one is what we call transitivity, and it's how often does our subies object the same as arsham are subject subject and then reverse transitivity is like that, except it goes in the other direction, so these are very simple.",
                    "label": 0
                },
                {
                    "sent": "Oh, thanks and it's easy to compute this if you're given a graph over over all the pairs of relations.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we'll see, when we well, when I show show some of the results.",
                    "label": 0
                },
                {
                    "sent": "Our experiments showed that the transitivity relation of those, 5 models.",
                    "label": 0
                },
                {
                    "sent": "Seem to give us the did give us the best results, so so that's the one we're using when we recommend using.",
                    "label": 0
                },
                {
                    "sent": "And it's especially true for graphs derived from DB pedia in freebase, which tend to have a low density as opposed to some of the other graphs.",
                    "label": 1
                },
                {
                    "sent": "I'll mention that we evaluate it against just because their value they're used for evaluating lots of similar kind of relation predicting algorithms.",
                    "label": 0
                },
                {
                    "sent": "What we show here is a heat map of the.",
                    "label": 1
                },
                {
                    "sent": "Relation similarity matrix for transitivity for the Win 18 RR data set.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the paper, especially if you look at the long paper in the Journal preprints available.",
                    "label": 0
                },
                {
                    "sent": "You'll see a lot of other heat Maps for.",
                    "label": 0
                },
                {
                    "sent": "All combinations.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the 8 datasets we looked at and.",
                    "label": 0
                },
                {
                    "sent": "Most of these, I think, except for DB10K.",
                    "label": 0
                },
                {
                    "sent": "Are ones that other people generated and have been used as standard models for link prediction or link ranking?",
                    "label": 0
                },
                {
                    "sent": "Now there's two of these things, kinship and you M LS.",
                    "label": 0
                },
                {
                    "sent": "Notice that the last column we show the graph density.",
                    "label": 0
                },
                {
                    "sent": "Which is related to, you know how?",
                    "label": 0
                },
                {
                    "sent": "What's the average degree of links coming in and out of a of a node and kinship and Umm, less are little unusual with respect together because their density is very high.",
                    "label": 0
                },
                {
                    "sent": "Freebase 15237 is much lower but still not as low as some of the other ones we looked at, including the base 10K, which is a data set we created and Freebase 13.",
                    "label": 0
                },
                {
                    "sent": "So when we evaluate things, we're going to use.",
                    "label": 0
                },
                {
                    "sent": "Not hits it.",
                    "label": 0
                },
                {
                    "sent": "Hits a 10 or hits at one like rescale.",
                    "label": 0
                },
                {
                    "sent": "Often uses, but we're going to use area under the curve because that's also the main metric that rescale, and a similar systems have used and we wanted to be able to compare.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work with those.",
                    "label": 0
                },
                {
                    "sent": "So here's just an overview of, well, you know the details of the area under the curve metrics for the two systems to rescale.",
                    "label": 0
                },
                {
                    "sent": "Best, Callan is non negative variant and four models that we created.",
                    "label": 0
                },
                {
                    "sent": "So each of these models represents a slightly different equation that we experimented with.",
                    "label": 0
                },
                {
                    "sent": "And the last one, which is a model that has.",
                    "label": 0
                },
                {
                    "sent": "Is quadratic and has a constrained factor is best overall.",
                    "label": 1
                },
                {
                    "sent": "It's not always the best, but where it's not the best.",
                    "label": 0
                },
                {
                    "sent": "It's pretty.",
                    "label": 0
                },
                {
                    "sent": "It's pretty close compared to other models.",
                    "label": 0
                },
                {
                    "sent": "But you notice that it still has trouble comparing doing well on some of the really high density.",
                    "label": 0
                },
                {
                    "sent": "Gray",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "If we look at this in a graphic form for the high density graphs, kinship and Umm, less.",
                    "label": 1
                },
                {
                    "sent": "Um, almost all these systems did about this.",
                    "label": 0
                },
                {
                    "sent": "All these models that about the same as rescale.",
                    "label": 0
                },
                {
                    "sent": "They didn't necessarily do better.",
                    "label": 0
                },
                {
                    "sent": "What is kinship?",
                    "label": 0
                },
                {
                    "sent": "Kinship is a data graph of think 104 entities.",
                    "label": 0
                },
                {
                    "sent": "Each one represents a person and their family relationships between them, and almost every node is connected to almost every other node, so it's kind of an unusual graph when you compare it to something like the graph you get with DB pedia or Freebase or wiki data.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When we look at the low density knowledge graphs and blacked out some of the other ones here, we can see that the quadratic cost constrained model we have.",
                    "label": 1
                },
                {
                    "sent": "Does do better than rest Cal with the exception of Freebase 15237 where SQL still does a little bit better?",
                    "label": 0
                },
                {
                    "sent": "An Freebase 15237 has a relatively high.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Density compared to the others.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we compare this to a different kind of approach, neural systems like tranzit, we see that this our system there are quadratically constrained on these datasets.",
                    "label": 0
                },
                {
                    "sent": "We we tested did much better than tranzit",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so that's a basic system for future work.",
                    "label": 1
                },
                {
                    "sent": "It's we're going to experiment with some additional relational similarity models.",
                    "label": 1
                },
                {
                    "sent": "We just had five rather simple ones that might improve things, and then we're looking at various kinds of applications of it for natural language processing, filtering out perhaps mistakes for taking an existing knowledge graph, and trying to identify relations that are suspect, and finally 4.",
                    "label": 0
                },
                {
                    "sent": "Reasoning over a cyber security graph we've constructed out of cyber threat intelligence feeds and sticks NVD, another semi structured cyber security data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the conclusions we reach our that this is a this is a novel tensor factorization approach for knowledge graph fact prediction.",
                    "label": 1
                },
                {
                    "sent": "Again fact prediction as opposed to just coming up with a list of rank facts that should be in the graph or could be in the graph.",
                    "label": 0
                },
                {
                    "sent": "We're going further and making a decision about whether it is or isn't in the graph.",
                    "label": 0
                },
                {
                    "sent": "And it gives state of the art results on many graphs in particular graphs that have low density, which are ones that we think are kind of more interesting and used in the Community, is possible if you wanted to use our system, you could experiment with your particular graph and you might find that the linear model works better than the quadratic model, for example.",
                    "label": 0
                },
                {
                    "sent": "Ugh.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Code and datasets are available in GitHub.",
                    "label": 0
                },
                {
                    "sent": "We've got also in a GitHub repository the Journal of Web Semantics, preprint, as well as the eswick Journal direct paper.",
                    "label": 0
                },
                {
                    "sent": "And if you have hard detailed questions, you should address them to Doctor Ron Carpathia as of this week, joined Phillips Research America in Cambridge, MA.",
                    "label": 0
                },
                {
                    "sent": "So I'll stop there and see if there are any questions.",
                    "label": 0
                },
                {
                    "sent": "Text seem very interesting.",
                    "label": 0
                },
                {
                    "sent": "Have you made any experiment you mentioned information extraction so trying to find systematic errors out of information, instruction pipelines, and have you any results yet?",
                    "label": 0
                },
                {
                    "sent": "Or is just for future work?",
                    "label": 0
                },
                {
                    "sent": "We've done a little bit of work on it, but we haven't really applied this to try to identify errors in a language understanding system.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned because basically what you mean is that you can use this this approach to extract from resources, for example for lexical resources and then you can eventually predict something from information instruction or you want to directly to learn from information extraction, validated graphs and then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you know if you wanted to start with a subset of wiki data, you could predict some relations that probably hold that aren't there, and then if you were running on natural language, understanding system and you've already put you produce the graph, you could run it on that and you might find some links in there that themselves, although there in the graph or we have kind of a low likelihood and they might be suspected.",
                    "label": 0
                },
                {
                    "sent": "It was great.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that you are using different formulas to combine the different measures that you have in your table.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that you're using different combinations, so how did you come up with those?",
                    "label": 0
                },
                {
                    "sent": "That's the kind of hard question that Honker has to answer, but he came up with the four four models.",
                    "label": 0
                },
                {
                    "sent": "Whether the model was regularised or not, and whether it was linear, quadratic.",
                    "label": 0
                },
                {
                    "sent": "And evaluated all four and overall the one that was.",
                    "label": 0
                },
                {
                    "sent": "Quadratic and constrained with for regularization.",
                    "label": 0
                },
                {
                    "sent": "Did best, but on some graphs, maybe the linear one would do with you a little better, but overall you know that would be the one that we would tend to use.",
                    "label": 0
                },
                {
                    "sent": "I have questions about the confusion.",
                    "label": 0
                },
                {
                    "sent": "You said that you assume about this schema of the graph, but then your thanks are relational tensor, an entity sensors.",
                    "label": 0
                },
                {
                    "sent": "You have to know the list of relations list of entities in the graph.",
                    "label": 0
                },
                {
                    "sent": "Before you do that, right right?",
                    "label": 0
                },
                {
                    "sent": "So it's depending on the schema of the graph.",
                    "label": 0
                },
                {
                    "sent": "Wherever I mean if I gave you a Neo 4 J graph, you could find all the all the relations that were in it and there would be no schema.",
                    "label": 0
                },
                {
                    "sent": "By by by having no schema, I'm thinking of know semantic schema.",
                    "label": 0
                },
                {
                    "sent": "For example, in the semantic Web you might know that you might have a lot of type information.",
                    "label": 0
                },
                {
                    "sent": "You might have domain and range constraints.",
                    "label": 0
                },
                {
                    "sent": "You might have all kinds of semantic relations as part of your schema.",
                    "label": 0
                },
                {
                    "sent": "But all we do is we take a look at the instances in the graph and the labeled relations between them, like president of or works for whatever.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and so I think of it is running on on the instances of a regular semantic web graph and just ignoring all the metadata.",
                    "label": 0
                },
                {
                    "sent": "Only one slide I left out because it was right about time.",
                    "label": 0
                },
                {
                    "sent": "Had to do with the time and it runs about the same speed and has about the same space.",
                    "label": 0
                },
                {
                    "sent": "Constraint says rescale and it's a lot faster I think than the neural network techniques, so that's that's another advantage of this approach.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank him again.",
                    "label": 0
                }
            ]
        }
    }
}