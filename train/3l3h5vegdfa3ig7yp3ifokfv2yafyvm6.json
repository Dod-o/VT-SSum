{
    "id": "3l3h5vegdfa3ig7yp3ifokfv2yafyvm6",
    "title": "Parallel streaming decision trees",
    "info": {
        "author": [
            "Yossi Richter, IBM Haifa Research Lab"
        ],
        "published": "Sept. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Data Mining->Temporal & Streams Mining"
        ]
    },
    "url": "http://videolectures.net/icml08_richter_psdt/",
    "segmentation": [
        [
            "OK, so so I'm you'll see.",
            "Actually, I'm just presenting the work of two of my colleagues.",
            "Have been criminal adjunct of where all from the Machine Learning Group in the IBM Haifa lab and I'm going to talk about parallel streaming decision trees."
        ],
        [
            "OK, so I guess that most of the talks today I've got are going to be about SVM and the question is why using decision trees and the two main reason is that first it's quite easy to test using decision trees and also it's quite easy to understand exactly what's going on.",
            "The main drawback is that when we have large data sets such as the data set in this competition, basically it's quite hard to train the decision freeze."
        ],
        [
            "So just to mention some of the previous works on decision trees, when we have a single processor, then we have the slick algorithm which basically pre sorts on each of the features and stores everything in the on the disk.",
            "Summation I have been used both by so both is just sampling the data, sampling some of the data in deciding accordingly an clouds basically builds statistics or histograms over the data.",
            "So using a more compact representation.",
            "Um?",
            "Three approaches in parallel computing have has been used, so first vertical parallelism, which means that we basically divide the features, the different features between the processors that we have.",
            "Task parallelism means that for basically divide the open nodes in decision tree for which we have to make a decision between the processors that we have and hybrid methods have been used also that combined.",
            "Previous two approaches.",
            "Two approaches have been used also for streaming, so one is mini mini batch which says that we look at some of the data and then we decide what to do with the current open notes that we have in a decision tree which want to split for example and then we just disregard this data and we look at.",
            "Some other data in the stream in order to decide the next iteration.",
            "The work that I'm going to present is actually parallel variant of mini batch.",
            "The cloud is a variant of clouds that does the same for streaming and also represented data by well uses a more compact representation by just building histograms and other statistics over the data."
        ],
        [
            "OK, so as I said, what we're going to do is something which is very similar to mini batch but just in parallel an this is the basic outline.",
            "So we have the data and we're going to split the data between the different processors that we have on the system.",
            "Now each processor is going to.",
            "Maintain a histogram for each one of the open nodes in the decision tree.",
            "For each one of the labels an for each one of the features.",
            "After some time we just merge all the histograms in the master node and then the master node can make a decision where to split given all the open nodes and decision tree.",
            "We then iterate.",
            "We we passed the current tree to the processors that we have we again.",
            "Forgot about the data that we had before and we use some other data, again partitioned between the processors that we have.",
            "Each one of them again maintains a lot of histograms and then we're going to do the same thing.",
            "We're going to marriage, decide what to do next, an iterate.",
            "No."
        ],
        [
            "OK. Another picture from from another view of the same thing.",
            "This is the timeline we have the master node.",
            "It begins by initializing the root of the decision tree.",
            "Then the workers are the one that's building the histograms.",
            "We take some of the data in.",
            "We build the histograms again for a histogram for each of the open nodes in the tree for each label and for each feature.",
            "Then we merge the histograms.",
            "Everything is collected in the master node.",
            "We will compute the node splits.",
            "We transfer the the current tree to all the workers and again we use other portion of the data again to build histograms and then this is and then we iterate."
        ],
        [
            "So how do we build what's touristic that we use to build histograms online?",
            "So histograms of course just a pair of a list of pairs.",
            "Each pair is composed of a value and counter.",
            "And we do the following so we have a maximum threshold on the number of bins that we would like to hold an whenever we see a new data point if its value is one of the values that we currently have in one of the bins, then we just increment its counter.",
            "Otherwise we create a new bin whose value is the value of the new point and its counter is 1.",
            "And then we do a merge between bins.",
            "So what we do is we take the two closest bins in the histogram that we currently have.",
            "We merge them into a single bin.",
            "The value is just a weighted average Ann and the counter is just the sum of the counters that original bins had."
        ],
        [
            "How do we merge two histograms when we go from the workers to the master?",
            "We basically do the same thing, so merging two histograms is done as following will take the two histograms.",
            "We just concatenate their bins and then we consecutive consecutively do merging operations over a bit.",
            "So again we take the two closest bins will merge them together and we continue until the number of bins falls beneath the threshold that we have."
        ],
        [
            "So just to see, well, a few empirical examples.",
            "So here we used 50 bins for histograms and we sampled thousand data points.",
            "So we see three distributions.",
            "The first one is the.",
            "This is the normal distribution, the second one is person distribution if I recall correctly, with a pair a matter of two and below we see mixture of two of two Gaussians.",
            "So I hope you can see the red curve and we see that.",
            "Our histograms are pretty aligned with the actual distribution.",
            "It's a very good approximation."
        ],
        [
            "OK, once the decision tree is constructed, we also do a pruning stage and we do the following.",
            "So for each node of the tree we store the following number which we denote by C node.",
            "It's the number of samples that didn't fold in the majority class plus one.",
            "So if this number is small, either the.",
            "It's here either becaused just very few samples for in this node, or very few samples for outside of the majority class.",
            "Then we can just prune out all the subtree beneath that node.",
            "Another thing we do is an everything well.",
            "I didn't mention that everything goes bottom up.",
            "Another thing we do is that we store.",
            "Counter which is called see both, which is defined recursively.",
            "It's 2 plus.",
            "See both of the Left child in the right child and so basically it gives us the, well, it's.",
            "It's defined recursively an actually just sums over the sea node, all the subtree.",
            "So if for certain node, if C node is below, see both.",
            "It basically means that.",
            "Within the subtree under this node we don't really manage to cluster better than what we do in the current node, and therefore we can just prune out the subtree that's beneath this node.",
            "So we do both things and we go bottom up and prune the tree."
        ],
        [
            "OK, so just before I go to describe some of the experimental results, this algorithm is the decision tree algorithm is actually part of the large scale machine learning toolbox that we have developed in IBM.",
            "Each includes algorithms for classification, clustering, regression feature extraction, and actually more at this time.",
            "It's it can.",
            "It can be.",
            "It works on a very wide range of parallel systems going from your dual-core laptop to IBM supercomputers.",
            "It includes a very intuitive API, so you can define your own algorithms.",
            "And the main thing is that it's free and it's available from Alpha works."
        ],
        [
            "OK, so."
        ],
        [
            "Alright, it's just passing people's.",
            "It's just what?",
            "Two questions.",
            "Good question.",
            "I'll have to get back with with an answer.",
            "Just using some.",
            "No, we actually we used thinking for this experiment, we used eight cores.",
            "Yeah yeah yeah ownership yeah yes, but actually I think that.",
            "But we had other experiments not for this competition that we used.",
            "We used it on blue gene, which is probably.",
            "Writing.",
            "Right?",
            "Yeah.",
            "OK."
        ],
        [
            "So before we go to the data sets of this competition, let's see what happens with some of the datasets from UCI.",
            "So a few data sets, some of them were already partitioned between the train and the test and four daughters.",
            "We just did 10 fold cross validation.",
            "So in the last in the two right columns we can see the error rate that we got for a standard tree decision tree.",
            "Ann.",
            "For the streaming parallel decision tree and it's actually quite easy to see that there is not a significant statistical difference between the results."
        ],
        [
            "If we take a look at the size of the trees, then we can see again in these two columns we get the error that we get before and after the pruning.",
            "Again, quite easy to see that there there is no statistical difference, but there is a difference in the size of the tree.",
            "An average we managed to using the quite simple you risztics that we employed, we managed to reduce the size of free by about 32%."
        ],
        [
            "OK, so now we move to the data sets of this computation and I'm just going to focus on the Alpha and beta data sets.",
            "The first thing we would like to have is strong scalability in the number of processors.",
            "So if we put more processors in the system would like to see that we actually managed to reduce the runtime and the Convention.",
            "Conventional measure is the speed up, which is the runtime on one processor compared with the runtime on an processors.",
            "So let's focus on this diagram.",
            "We can see that, well, the upper curves are for more examples so.",
            "We we we did a few tests in in the first ones.",
            "We just sample some of the data from the Alpha data set and the last one I guess is just the whole data set and we can see that the larger the data is, the better our speed up.",
            "So for example, if you see if you take a look at the upper curve, we see that for eight processors we get a speedup, which is about five and a half, which means that we basically really utilized the number of processors that we have."
        ],
        [
            "Another thing that we would like to have is called weak scalability with respect to the data size, so would like of course the runtime to depend on the data size but not grow too fast.",
            "So just know that this scale is logarithmic, so the.",
            "So the curves here are actually very concave, which is exactly what we want.",
            "So weak scalability with respect to the data size.",
            "It runs really fast."
        ],
        [
            "Just another diagram trying to characterize the speed up as a function of the size of the input, and here we characterize the size of the output of the input as the product of the number of examples times dimension.",
            "Again, this line is the just the linear regression and can again see that the speedup is quite impressive."
        ],
        [
            "OK, so just to summarize, represented an efficient algorithm for parallel streaming decision trees.",
            "Results are as good As for a single node, and we achieve all the two goals in parallel computing, which is strong scalability in the number of processors and weak scalability in the size of data set.",
            "And the ongoing work we have here is trying to analyze this problem or the construction of decision trees more analytically and prove analytically that the algorithm indeed constructs a decision tree which is epsilon four.",
            "Well at most epsilon far in some metric from the actual tree.",
            "Which is built on a single note."
        ],
        [
            "That's it, thank you.",
            "Yes.",
            "A little bit about how.",
            "How would Informance was compared to the other?",
            "And then there are some metrics that tree was not closed right?",
            "Because?",
            "Some of the other ones.",
            "This classification.",
            "Classification accuracy versus.",
            "I'm actually excellent question.",
            "I guess that the organizers have much better knowledge of this because this is just what I got from a large, so they have all the metrics I guess.",
            "For classification, 'cause we actually use over there.",
            "Right, right, right?",
            "So it doesn't, right?",
            "South.",
            "Right, so I'll just have to get back to you after asking a lot.",
            "Make some other figures just for classification, but so far.",
            "So I guess one of the concerns with sort of final decision tree is that it's kind of like speeding up a slower algorithm so.",
            "Is there anyway that you can address that concern?",
            "I mean yeah.",
            "How fast was this algorithm compared to other algorithms fast in terms of runtime, so we can."
        ],
        [
            "Go back to the diagram that we had here, so this is the runtime an.",
            "So we I guess the larger data set for about this size an if you look at the eight nodes, which is the upper curve or actually the lower lower curve then basically you get something which is I don't know.",
            "This is in seconds, so.",
            "About 3 hours.",
            "Oh, actually.",
            "Yeah.",
            "Well.",
            "Yeah, actually right below yeah, so more like an hour, yeah?",
            "So actually, I don't really know the runtimes for the other competitors, so.",
            "And this is for the Alpha.",
            "An icy similar runtime for the better so.",
            "Do you know for like, the the linear systems or the others?",
            "On the outside.",
            "Is this device?",
            "So this is actually curious too.",
            "So.",
            "Smart phone.",
            "SPM also.",
            "You might want to project this, yeah?",
            "Is gonna be.",
            "SPDT 25 5th St.",
            "Gather round.",
            "Yonder.",
            "Yes, I can just unplug this.",
            "Right, OK?",
            "Yeah, it's.",
            "OK. Because in the algorithm it reminds me of the Den, stream or discount, nor the other streaming now going.",
            "Did you consider having a clustering algorithm as a preprocessing step for for the decision tree?",
            "I don't think so.",
            "So actually, this runtime on actually on a single node for the decision trees is essentially what you run on your machine, right?",
            "If not, we didn't know if that is what you submitted, I see.",
            "It's just about an hour for that last Mii system, probably the same.",
            "See.",
            "Course yes.",
            "Other questions.",
            "OK, so thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so I'm you'll see.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm just presenting the work of two of my colleagues.",
                    "label": 0
                },
                {
                    "sent": "Have been criminal adjunct of where all from the Machine Learning Group in the IBM Haifa lab and I'm going to talk about parallel streaming decision trees.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I guess that most of the talks today I've got are going to be about SVM and the question is why using decision trees and the two main reason is that first it's quite easy to test using decision trees and also it's quite easy to understand exactly what's going on.",
                    "label": 0
                },
                {
                    "sent": "The main drawback is that when we have large data sets such as the data set in this competition, basically it's quite hard to train the decision freeze.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to mention some of the previous works on decision trees, when we have a single processor, then we have the slick algorithm which basically pre sorts on each of the features and stores everything in the on the disk.",
                    "label": 0
                },
                {
                    "sent": "Summation I have been used both by so both is just sampling the data, sampling some of the data in deciding accordingly an clouds basically builds statistics or histograms over the data.",
                    "label": 0
                },
                {
                    "sent": "So using a more compact representation.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Three approaches in parallel computing have has been used, so first vertical parallelism, which means that we basically divide the features, the different features between the processors that we have.",
                    "label": 0
                },
                {
                    "sent": "Task parallelism means that for basically divide the open nodes in decision tree for which we have to make a decision between the processors that we have and hybrid methods have been used also that combined.",
                    "label": 0
                },
                {
                    "sent": "Previous two approaches.",
                    "label": 0
                },
                {
                    "sent": "Two approaches have been used also for streaming, so one is mini mini batch which says that we look at some of the data and then we decide what to do with the current open notes that we have in a decision tree which want to split for example and then we just disregard this data and we look at.",
                    "label": 0
                },
                {
                    "sent": "Some other data in the stream in order to decide the next iteration.",
                    "label": 0
                },
                {
                    "sent": "The work that I'm going to present is actually parallel variant of mini batch.",
                    "label": 0
                },
                {
                    "sent": "The cloud is a variant of clouds that does the same for streaming and also represented data by well uses a more compact representation by just building histograms and other statistics over the data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so as I said, what we're going to do is something which is very similar to mini batch but just in parallel an this is the basic outline.",
                    "label": 0
                },
                {
                    "sent": "So we have the data and we're going to split the data between the different processors that we have on the system.",
                    "label": 0
                },
                {
                    "sent": "Now each processor is going to.",
                    "label": 0
                },
                {
                    "sent": "Maintain a histogram for each one of the open nodes in the decision tree.",
                    "label": 1
                },
                {
                    "sent": "For each one of the labels an for each one of the features.",
                    "label": 0
                },
                {
                    "sent": "After some time we just merge all the histograms in the master node and then the master node can make a decision where to split given all the open nodes and decision tree.",
                    "label": 0
                },
                {
                    "sent": "We then iterate.",
                    "label": 0
                },
                {
                    "sent": "We we passed the current tree to the processors that we have we again.",
                    "label": 0
                },
                {
                    "sent": "Forgot about the data that we had before and we use some other data, again partitioned between the processors that we have.",
                    "label": 0
                },
                {
                    "sent": "Each one of them again maintains a lot of histograms and then we're going to do the same thing.",
                    "label": 0
                },
                {
                    "sent": "We're going to marriage, decide what to do next, an iterate.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Another picture from from another view of the same thing.",
                    "label": 0
                },
                {
                    "sent": "This is the timeline we have the master node.",
                    "label": 0
                },
                {
                    "sent": "It begins by initializing the root of the decision tree.",
                    "label": 1
                },
                {
                    "sent": "Then the workers are the one that's building the histograms.",
                    "label": 0
                },
                {
                    "sent": "We take some of the data in.",
                    "label": 0
                },
                {
                    "sent": "We build the histograms again for a histogram for each of the open nodes in the tree for each label and for each feature.",
                    "label": 0
                },
                {
                    "sent": "Then we merge the histograms.",
                    "label": 0
                },
                {
                    "sent": "Everything is collected in the master node.",
                    "label": 0
                },
                {
                    "sent": "We will compute the node splits.",
                    "label": 1
                },
                {
                    "sent": "We transfer the the current tree to all the workers and again we use other portion of the data again to build histograms and then this is and then we iterate.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we build what's touristic that we use to build histograms online?",
                    "label": 0
                },
                {
                    "sent": "So histograms of course just a pair of a list of pairs.",
                    "label": 1
                },
                {
                    "sent": "Each pair is composed of a value and counter.",
                    "label": 0
                },
                {
                    "sent": "And we do the following so we have a maximum threshold on the number of bins that we would like to hold an whenever we see a new data point if its value is one of the values that we currently have in one of the bins, then we just increment its counter.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we create a new bin whose value is the value of the new point and its counter is 1.",
                    "label": 0
                },
                {
                    "sent": "And then we do a merge between bins.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we take the two closest bins in the histogram that we currently have.",
                    "label": 1
                },
                {
                    "sent": "We merge them into a single bin.",
                    "label": 0
                },
                {
                    "sent": "The value is just a weighted average Ann and the counter is just the sum of the counters that original bins had.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we merge two histograms when we go from the workers to the master?",
                    "label": 0
                },
                {
                    "sent": "We basically do the same thing, so merging two histograms is done as following will take the two histograms.",
                    "label": 1
                },
                {
                    "sent": "We just concatenate their bins and then we consecutive consecutively do merging operations over a bit.",
                    "label": 1
                },
                {
                    "sent": "So again we take the two closest bins will merge them together and we continue until the number of bins falls beneath the threshold that we have.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to see, well, a few empirical examples.",
                    "label": 0
                },
                {
                    "sent": "So here we used 50 bins for histograms and we sampled thousand data points.",
                    "label": 1
                },
                {
                    "sent": "So we see three distributions.",
                    "label": 0
                },
                {
                    "sent": "The first one is the.",
                    "label": 0
                },
                {
                    "sent": "This is the normal distribution, the second one is person distribution if I recall correctly, with a pair a matter of two and below we see mixture of two of two Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So I hope you can see the red curve and we see that.",
                    "label": 0
                },
                {
                    "sent": "Our histograms are pretty aligned with the actual distribution.",
                    "label": 0
                },
                {
                    "sent": "It's a very good approximation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, once the decision tree is constructed, we also do a pruning stage and we do the following.",
                    "label": 0
                },
                {
                    "sent": "So for each node of the tree we store the following number which we denote by C node.",
                    "label": 0
                },
                {
                    "sent": "It's the number of samples that didn't fold in the majority class plus one.",
                    "label": 1
                },
                {
                    "sent": "So if this number is small, either the.",
                    "label": 1
                },
                {
                    "sent": "It's here either becaused just very few samples for in this node, or very few samples for outside of the majority class.",
                    "label": 0
                },
                {
                    "sent": "Then we can just prune out all the subtree beneath that node.",
                    "label": 0
                },
                {
                    "sent": "Another thing we do is an everything well.",
                    "label": 0
                },
                {
                    "sent": "I didn't mention that everything goes bottom up.",
                    "label": 0
                },
                {
                    "sent": "Another thing we do is that we store.",
                    "label": 0
                },
                {
                    "sent": "Counter which is called see both, which is defined recursively.",
                    "label": 1
                },
                {
                    "sent": "It's 2 plus.",
                    "label": 1
                },
                {
                    "sent": "See both of the Left child in the right child and so basically it gives us the, well, it's.",
                    "label": 0
                },
                {
                    "sent": "It's defined recursively an actually just sums over the sea node, all the subtree.",
                    "label": 0
                },
                {
                    "sent": "So if for certain node, if C node is below, see both.",
                    "label": 1
                },
                {
                    "sent": "It basically means that.",
                    "label": 0
                },
                {
                    "sent": "Within the subtree under this node we don't really manage to cluster better than what we do in the current node, and therefore we can just prune out the subtree that's beneath this node.",
                    "label": 0
                },
                {
                    "sent": "So we do both things and we go bottom up and prune the tree.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just before I go to describe some of the experimental results, this algorithm is the decision tree algorithm is actually part of the large scale machine learning toolbox that we have developed in IBM.",
                    "label": 1
                },
                {
                    "sent": "Each includes algorithms for classification, clustering, regression feature extraction, and actually more at this time.",
                    "label": 0
                },
                {
                    "sent": "It's it can.",
                    "label": 0
                },
                {
                    "sent": "It can be.",
                    "label": 0
                },
                {
                    "sent": "It works on a very wide range of parallel systems going from your dual-core laptop to IBM supercomputers.",
                    "label": 0
                },
                {
                    "sent": "It includes a very intuitive API, so you can define your own algorithms.",
                    "label": 1
                },
                {
                    "sent": "And the main thing is that it's free and it's available from Alpha works.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, it's just passing people's.",
                    "label": 0
                },
                {
                    "sent": "It's just what?",
                    "label": 0
                },
                {
                    "sent": "Two questions.",
                    "label": 0
                },
                {
                    "sent": "Good question.",
                    "label": 0
                },
                {
                    "sent": "I'll have to get back with with an answer.",
                    "label": 0
                },
                {
                    "sent": "Just using some.",
                    "label": 0
                },
                {
                    "sent": "No, we actually we used thinking for this experiment, we used eight cores.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah ownership yeah yes, but actually I think that.",
                    "label": 0
                },
                {
                    "sent": "But we had other experiments not for this competition that we used.",
                    "label": 0
                },
                {
                    "sent": "We used it on blue gene, which is probably.",
                    "label": 0
                },
                {
                    "sent": "Writing.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before we go to the data sets of this competition, let's see what happens with some of the datasets from UCI.",
                    "label": 0
                },
                {
                    "sent": "So a few data sets, some of them were already partitioned between the train and the test and four daughters.",
                    "label": 0
                },
                {
                    "sent": "We just did 10 fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "So in the last in the two right columns we can see the error rate that we got for a standard tree decision tree.",
                    "label": 0
                },
                {
                    "sent": "Ann.",
                    "label": 0
                },
                {
                    "sent": "For the streaming parallel decision tree and it's actually quite easy to see that there is not a significant statistical difference between the results.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we take a look at the size of the trees, then we can see again in these two columns we get the error that we get before and after the pruning.",
                    "label": 0
                },
                {
                    "sent": "Again, quite easy to see that there there is no statistical difference, but there is a difference in the size of the tree.",
                    "label": 0
                },
                {
                    "sent": "An average we managed to using the quite simple you risztics that we employed, we managed to reduce the size of free by about 32%.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now we move to the data sets of this computation and I'm just going to focus on the Alpha and beta data sets.",
                    "label": 0
                },
                {
                    "sent": "The first thing we would like to have is strong scalability in the number of processors.",
                    "label": 1
                },
                {
                    "sent": "So if we put more processors in the system would like to see that we actually managed to reduce the runtime and the Convention.",
                    "label": 0
                },
                {
                    "sent": "Conventional measure is the speed up, which is the runtime on one processor compared with the runtime on an processors.",
                    "label": 0
                },
                {
                    "sent": "So let's focus on this diagram.",
                    "label": 0
                },
                {
                    "sent": "We can see that, well, the upper curves are for more examples so.",
                    "label": 0
                },
                {
                    "sent": "We we we did a few tests in in the first ones.",
                    "label": 0
                },
                {
                    "sent": "We just sample some of the data from the Alpha data set and the last one I guess is just the whole data set and we can see that the larger the data is, the better our speed up.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you see if you take a look at the upper curve, we see that for eight processors we get a speedup, which is about five and a half, which means that we basically really utilized the number of processors that we have.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing that we would like to have is called weak scalability with respect to the data size, so would like of course the runtime to depend on the data size but not grow too fast.",
                    "label": 0
                },
                {
                    "sent": "So just know that this scale is logarithmic, so the.",
                    "label": 0
                },
                {
                    "sent": "So the curves here are actually very concave, which is exactly what we want.",
                    "label": 0
                },
                {
                    "sent": "So weak scalability with respect to the data size.",
                    "label": 1
                },
                {
                    "sent": "It runs really fast.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just another diagram trying to characterize the speed up as a function of the size of the input, and here we characterize the size of the output of the input as the product of the number of examples times dimension.",
                    "label": 0
                },
                {
                    "sent": "Again, this line is the just the linear regression and can again see that the speedup is quite impressive.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to summarize, represented an efficient algorithm for parallel streaming decision trees.",
                    "label": 1
                },
                {
                    "sent": "Results are as good As for a single node, and we achieve all the two goals in parallel computing, which is strong scalability in the number of processors and weak scalability in the size of data set.",
                    "label": 0
                },
                {
                    "sent": "And the ongoing work we have here is trying to analyze this problem or the construction of decision trees more analytically and prove analytically that the algorithm indeed constructs a decision tree which is epsilon four.",
                    "label": 0
                },
                {
                    "sent": "Well at most epsilon far in some metric from the actual tree.",
                    "label": 0
                },
                {
                    "sent": "Which is built on a single note.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "A little bit about how.",
                    "label": 0
                },
                {
                    "sent": "How would Informance was compared to the other?",
                    "label": 0
                },
                {
                    "sent": "And then there are some metrics that tree was not closed right?",
                    "label": 0
                },
                {
                    "sent": "Because?",
                    "label": 0
                },
                {
                    "sent": "Some of the other ones.",
                    "label": 0
                },
                {
                    "sent": "This classification.",
                    "label": 0
                },
                {
                    "sent": "Classification accuracy versus.",
                    "label": 0
                },
                {
                    "sent": "I'm actually excellent question.",
                    "label": 0
                },
                {
                    "sent": "I guess that the organizers have much better knowledge of this because this is just what I got from a large, so they have all the metrics I guess.",
                    "label": 0
                },
                {
                    "sent": "For classification, 'cause we actually use over there.",
                    "label": 0
                },
                {
                    "sent": "Right, right, right?",
                    "label": 0
                },
                {
                    "sent": "So it doesn't, right?",
                    "label": 0
                },
                {
                    "sent": "South.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'll just have to get back to you after asking a lot.",
                    "label": 0
                },
                {
                    "sent": "Make some other figures just for classification, but so far.",
                    "label": 0
                },
                {
                    "sent": "So I guess one of the concerns with sort of final decision tree is that it's kind of like speeding up a slower algorithm so.",
                    "label": 0
                },
                {
                    "sent": "Is there anyway that you can address that concern?",
                    "label": 0
                },
                {
                    "sent": "I mean yeah.",
                    "label": 0
                },
                {
                    "sent": "How fast was this algorithm compared to other algorithms fast in terms of runtime, so we can.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back to the diagram that we had here, so this is the runtime an.",
                    "label": 0
                },
                {
                    "sent": "So we I guess the larger data set for about this size an if you look at the eight nodes, which is the upper curve or actually the lower lower curve then basically you get something which is I don't know.",
                    "label": 0
                },
                {
                    "sent": "This is in seconds, so.",
                    "label": 0
                },
                {
                    "sent": "About 3 hours.",
                    "label": 0
                },
                {
                    "sent": "Oh, actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually right below yeah, so more like an hour, yeah?",
                    "label": 0
                },
                {
                    "sent": "So actually, I don't really know the runtimes for the other competitors, so.",
                    "label": 0
                },
                {
                    "sent": "And this is for the Alpha.",
                    "label": 0
                },
                {
                    "sent": "An icy similar runtime for the better so.",
                    "label": 0
                },
                {
                    "sent": "Do you know for like, the the linear systems or the others?",
                    "label": 0
                },
                {
                    "sent": "On the outside.",
                    "label": 0
                },
                {
                    "sent": "Is this device?",
                    "label": 0
                },
                {
                    "sent": "So this is actually curious too.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Smart phone.",
                    "label": 0
                },
                {
                    "sent": "SPM also.",
                    "label": 0
                },
                {
                    "sent": "You might want to project this, yeah?",
                    "label": 0
                },
                {
                    "sent": "Is gonna be.",
                    "label": 0
                },
                {
                    "sent": "SPDT 25 5th St.",
                    "label": 0
                },
                {
                    "sent": "Gather round.",
                    "label": 0
                },
                {
                    "sent": "Yonder.",
                    "label": 0
                },
                {
                    "sent": "Yes, I can just unplug this.",
                    "label": 0
                },
                {
                    "sent": "Right, OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's.",
                    "label": 0
                },
                {
                    "sent": "OK. Because in the algorithm it reminds me of the Den, stream or discount, nor the other streaming now going.",
                    "label": 0
                },
                {
                    "sent": "Did you consider having a clustering algorithm as a preprocessing step for for the decision tree?",
                    "label": 0
                },
                {
                    "sent": "I don't think so.",
                    "label": 0
                },
                {
                    "sent": "So actually, this runtime on actually on a single node for the decision trees is essentially what you run on your machine, right?",
                    "label": 0
                },
                {
                    "sent": "If not, we didn't know if that is what you submitted, I see.",
                    "label": 0
                },
                {
                    "sent": "It's just about an hour for that last Mii system, probably the same.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "Course yes.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, so thank you.",
                    "label": 0
                }
            ]
        }
    }
}