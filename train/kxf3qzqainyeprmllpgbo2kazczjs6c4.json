{
    "id": "kxf3qzqainyeprmllpgbo2kazczjs6c4",
    "title": "Modeling semantic plausibility and the influence of visual context during on-line sentence comprehension",
    "info": {
        "author": [
            "Matthew Crocker, Department of Computational Psycholinguistics, Saarland University"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/mlcs07_crocker_msp/",
    "segmentation": [
        [
            "Thanks, thanks very much by the way and I would like to thank the organizers for inviting me here.",
            "Thank him a little less for putting me in the last slot.",
            "Thanks to all of you for sticking it out.",
            "Yeah, so I'm going to talk about modeling semantic plausibility and visual context in online sentence comprehension.",
            "Basically two fairly different pieces of work here that I'm going to try and glue together.",
            "It also reflects."
        ],
        [
            "Joint work with you missed that.",
            "These people very important peak.",
            "Novella Marshall Mayberry and re copado."
        ],
        [
            "So the general area of research is a little bit different than some of the stuff we've been hearing about.",
            "Especially today is to investigate the processes that underlie the human capacity to understand language primary."
        ],
        [
            "At the sentence level, so I'm interested in how the language processor works.",
            "What are its architectures and mechanisms that."
        ],
        [
            "Did underlie this?",
            "And of course, how can we model this computationally?",
            "Both understanding that is how people actually arrive at some sort of meaning, or some sort of representation which they can convert into action which I'm going to call competence in a very.",
            "Sort of generic sense of the term competence as opposed to Chomsky said, and also performance, right?",
            "That is that we want to be able to capture people's behavior."
        ],
        [
            "Another point that especially interested in is how?",
            "Language understanding sentence comprehension interacts with other cognitive systems and also in fact the environment around."
        ],
        [
            "Yes."
        ],
        [
            "So.",
            "I'm going to sort of divide this up into three parts.",
            "I'm going to say a little bit to begin with about sort of what I think is a sort of current state of human sentence comprehension."
        ],
        [
            "Research and then going to talk about the development of a wide coverage model of semantic plausibility and how we can integrate that into a."
        ],
        [
            "Full sentence comprehension.",
            "And finally, I'll say a little bit, or maybe quite a lot about visually situated language understanding and attention."
        ],
        [
            "So."
        ],
        [
            "Traditional approaches in psycholinguistics.",
            "Have sort of or at least in sentence processing research have focused on.",
            "Largely on reading studies, that is, we look at in highly controlled conditions.",
            "The amount of time people spend on particular regions of a sentence in particular, usually looking at particular sort of ambiguous constructions, either using self paced reading or eye tracking and reading."
        ],
        [
            "So by measuring reading times, we measure processing complexity of some sort, which we then try to relate to complexity in our models.",
            "The result is that the psycholinguistic theories that are around.",
            "Tend to emphasize a largely purely linguistic processing, lexical syntactic to some extent semantic.",
            "And these theories try to explain processing complexity.",
            "The result, though I think."
        ],
        [
            "Is that there's an emphasis in these theories on the weaknesses of human language comprehension."
        ],
        [
            "And Furthermore, is in general a failure to situate human language processing within our world more?"
        ],
        [
            "Finally."
        ],
        [
            "And these are things I want to address, so some sense what underlies a lot of the work I do is what I've called the Performance Paradox, which I think people in computational linguistics, especially nowadays, are pretty aware of.",
            "But psycholinguistics maybe less so, and that is how is it that people understand language so accurately, accurately, and effortlessly, given its complexity and ambiguity.",
            "We understand this incrementally, basically word by word, sometimes even at finer grained levels of incrementality than that in real time.",
            "In fact, we don't just keep up with the input word stream more often even anticipating what's going to come next week and rapidly resolve ambiguities and revise missing."
        ],
        [
            "Reputations and I think two factors which are involved in this.",
            "I'm calling them a solution, maybe isn't the right thing to say, but I think 2 crucial contributors to why people can do this is that we optimize based on both our long-term linguistic experience and knowledge, and that we can adaptively exploit our immediate linguistic and non linguistic context."
        ],
        [
            "So we get a picture that looks something."
        ],
        [
            "Like this when we kind of broaden our view a little bit where we're building sentence processing models.",
            "Which on the one hand have to be competent.",
            "OK, that is have some sort of reasonable linguistic coverage, reasonable linguistic complexity, and models which can arrive at some sort of a useful interpretation of."
        ],
        [
            "Parents.",
            "Of course, we also want to explain performance or human behavior that we want to be able to explain why we see elevated reading times in certain situations, why we get certain blips in our event related potentials, patterns of visual attention, which I'll talk about more a little bit later on, imaging all kinds of experimental paradigms where we want to be able to explain observed behaviors, or."
        ],
        [
            "Neural activity.",
            "We also increasingly need to explain how these models interact with other cognitive systems such as visual processes, memory."
        ],
        [
            "Mention.",
            "And in some sense, well, there's a catchall category over here.",
            "What I've just called adaptation that is, how do people in some sense adapt to their linguistic experience, adapt to the immediate context?",
            "What's going on in their environment, and the tasks in which they are engaged?",
            "We increasingly know that all of these different things play quite a strong role, and yet our theories really ignore a lot of them."
        ],
        [
            "If Martin can do it, so can I.",
            "So what are some of the challenges here?",
            "Well, other sort of hinted at before we want to account not just for the garden path sentences that we spend a lot of time studying, but for garden variety language.",
            "The fact that people can read a 40 word sentence out of the Wall Street Journal effortlessly right, despite the fact that it probably 2/3 of the words in there are category ambiguous and so on."
        ],
        [
            "OK, no problem at all.",
            "Semantics, as I say, today we don't really have very good models of semantics that is plausibility, selectional restrictions.",
            "How priming works, associates, and more generally, kinds of inferences that we might want to even make."
        ],
        [
            "How to use linguistic and non linguistic context in some sort of a reasonable way, such as exploiting common?"
        ],
        [
            "Sound visual environment so on.",
            "And also creating, especially at the level of sentence processing, really concrete quantitative predictions for these kinds of multiple behavioral and neuroscientific measures that were.",
            "Using two info."
        ],
        [
            "Armor theory development."
        ],
        [
            "OK, well I think probabilistic sentence processing is kind of become.",
            "In models of sentence processing, the sort of favored way to go and it addresses some of these problems.",
            "So firstly at an empirical level, there's tons of evidence for the importance of frequency information, right?",
            "We all know that happens at all levels of linguistic representation from lexical right up through to structural, and possibly even semantics.",
            "And probabilistic mechanisms provide a natural way to capture these kinds of frequence."
        ],
        [
            "Preferences.",
            "Something we also learn from computational linguistics is that we can use probabilities as a way of expressing preferences and biases where we haven't really got a clue about how they work.",
            "Underlyingly we can kind of use it as a way of approximating things that we don't really know what to do with yet, and that can be quite used."
        ],
        [
            "Building models.",
            "We have good techniques with these kinds of models for training.",
            "We know we can get systems with fairly large coverage so."
        ],
        [
            "We're doing fairly well and we got a uniform explanation of ambiguity resolution at all different levels of lingua."
        ],
        [
            "Processing Furthermore, there's some interesting kinds of linking hypothesis we can start to construct with these probabilistic models.",
            "We can do ranking and re ranking with them because we're getting sort of clear sort of rankings of different alternative hypothesis.",
            "But there's other sorts of information theoretic complexity measures such as surprise, surprise or entropy reduction and so on which have been shown to correlate quite nicely with certain kinds of behaviors.",
            "No, I'm not."
        ],
        [
            "Anymore about those here."
        ],
        [
            "So one way you can work then just generally when you're building these probabilistic models, is to sort of take sort of a rational analysis.",
            "Kind of an approach here where you at the very top level identify some sort of goal for your system.",
            "That is to say, optimize accurate."
        ],
        [
            "Interpretation.",
            "From there you can express that formally in terms of some sort of a function like, let's maximize the probability of sentence interpretations as we're going along understanding a sentence."
        ],
        [
            "And from there you go to some sort of realization of that.",
            "Where you say, well, how do we actually estimate this probability really?",
            "Anyway, maybe we'll estimate it from corpora and assume that they represent our linguistic experience at some level, and so on."
        ],
        [
            "So this sort of follows a standard sort of marry an approach to how we might build these kinds of models."
        ],
        [
            "And this has been done successfully in various levels of of sentence processing, language processing research.",
            "For example, in lexical category decembe disambiguation, we can maximize the probability of a given word sequence and part of speech sequence.",
            "And this has been shown to predict well not only to we can implement this as a standard bigram part of speech model.",
            "This is kind of nice because on the one hand it has fairly high accuracy in part of speech tagging and on the other hand it's been shown to kind of model exactly some of the behaviors that people exhibit in resolving lexical category ambiguities."
        ],
        [
            "We can sort of go up to the next level and apply the same ideas to sentence processing where we simply use probabilistic context free grammars or related kinds of grammatical models.",
            "And as each word in the sentence comes in, we evaluate all possible syntactic structures and constantly sort of favoring the most probable one.",
            "OK, this work, based on ideas from dangerous keys Paper 96 and implemented in a wide coverage model by Torsten Branson myself."
        ],
        [
            "Little later on.",
            "So how does that model work and how far do we get with?"
        ],
        [
            "Well, here's just a simple example with reduced relative clause.",
            "You have something like the man race to the station was innocent.",
            "This is a lot like the classic horse race.",
            "Past the barn fell kind."
        ],
        [
            "Example.",
            "The way the parts would work is it would come along and say we have the man raced, raced it, turns out, happens to be intransitive.",
            "Preference, so that's what the."
        ],
        [
            "So I means there so when we get 2 the system thinks OK.",
            "I think 2 actually starts in infinitival complement.",
            "Here the man raced to help the old woman or something like that."
        ],
        [
            "You get the following noun phrase.",
            "You realize that probably wasn't right."
        ],
        [
            "And you realize this is probably a prepositional phrase, but because the verb can be intransitive, you're quite happy to continue pursuing a main clause interpretation of this sentence OK. And when you finally get the copula.",
            "You don't know what to do with it.",
            "OK, at this point the parser breaks down.",
            "Effectively, it's been Garden path here and it tries to attach the was sort of the last edge in the parse.",
            "So here we've managed to predict a garden path."
        ],
        [
            "Contrast.",
            "If you just change a couple of the."
        ],
        [
            "Kids and you replace Raceway."
        ],
        [
            "Held then."
        ],
        [
            "When held is reached, we see it's very similar, except that this time it believes that held is preferably a transitive verb.",
            "That's what the little T means on the tag."
        ],
        [
            "Beneath held.",
            "And so as soon as it gets the preposition after held, it realizes.",
            "Well wait, if there's a preposition after this verb, it's probably actually a past participle, because I was expecting a direct object.",
            "At isn't going to start a direct object, and so it can immediately use this sort of probabilistic information to recover retag the verb.",
            "Build the correct syntactic structure."
        ],
        [
            "And move on."
        ],
        [
            "Ultimately, getting the correct person not be garden path and exactly these kinds of probabilistic use have been shown by people like Mary L. McDonald and others too.",
            "Mitigator modulate the difficulty of these kinds of garden path constructions so probabilistic models do this kind of thing really very."
        ],
        [
            "Nicely."
        ],
        [
            "But one thing they don't do well is some."
        ],
        [
            "Antics, at least not yet.",
            "And So what do I mean?",
            "Well, suppose we have something like the doctor cured.",
            "OK, we're pretty happy building a main clause analysis for this."
        ],
        [
            "And only when something like by the.",
            "Surgeon, I don't know comes up, do we realize that we have to re analyze this?",
            "But"
        ],
        [
            "Suppose the first word were something like patient.",
            "We had the patient cared well in this case.",
            "We may even prefer the second syntactic structure here.",
            "The bottom one, the reduced relative clause, or if not, we certainly find it a lot easier to move to it once we get this continuation of the sentence."
        ],
        [
            "So how might we do that?",
            "Well, it seems."
        ],
        [
            "Is pretty intuitive, but to do it what we need is to look a little bit more closely at the information that's available to us.",
            "So in particular I'm going to suggest we need to do is look at verb argument, role relations in modeling some sort of notion of the plausibility of a verb, argument, role relation.",
            "That is, we need to assess how likely is it for patients to cure people.",
            "How likely is it for patients to be the healer of a curing event?",
            "OK."
        ],
        [
            "So.",
            "What we want then is to build some sort of a graded model of verb argument plausibility, something that we can ideally automatically induce from some sort of semantically annotated corpus.",
            "You might ideally want to do it from using some."
        ],
        [
            "Unsupervised methods.",
            "And I'm going to show how we can evaluate that model in the 1st place by looking at how well it predicts human plausibility judgment data."
        ],
        [
            "And then I'm going to see how we how it works when we try to integrate it into a model of incremental parsing and see how well it predicts reading time data."
        ],
        [
            "OK, and I'm going to very briefly present some findings there, but.",
            "Relations between the model and."
        ],
        [
            "Various reading time studies."
        ],
        [
            "So.",
            "The model looks roughly like this.",
            "What we're doing is the basic claim is that we can assess the plausibility of some role argument or verb argument roll triple.",
            "As the probability of that role, argument, verb, verb, sense, and the grammatical function.",
            "Which we then break down using the chain rule OK into the expression that you see sort of spread over 2 lines."
        ],
        [
            "OK, so we've got these five features now.",
            "Anybody who's built probabilistic models of this sort realizes you're going to lots of problems with sparse data, and so of course we use smoothing.",
            "The obvious technique, little less interesting, is to use good Turing smoothing, at least for the first 4 terms, and then for the final term, which is in some sense the most interesting and important one.",
            "We use a class based moving method.",
            "OK to generalize from word tokens to word classes."
        ],
        [
            "So there's two resources we can use here.",
            "One is prop bank right?",
            "Sort of notionally a semantically annotated version of the Wall Street Journal, which has roughly entries that look like it's on the top.",
            "There we have some notion of cure and Doctor gets annotated as the R0 rolark zeros usually reserved for sort of agent like role fillers.",
            "We have the verb cured and then we have the patient which is annotated with Arg, one which is usually for the patient or the."
        ],
        [
            "Team role.",
            "We also have frame net which is a much more richly annotated corpus.",
            "Unfortunately, it's also smaller.",
            "OK, so in frame net we actually have semantic frames, like the healing frame, which when it's applied to a sentence like the doctor cleared, the patient will annotate the doctor with a healer role and the patient with a patient role.",
            "And gives us in some sense rather more accurate and consistent role.",
            "Annotations and prop bank.",
            "As I say, it's a little bit smaller.",
            "And so we actually looked at both of these corpora when we were trying to induce us.",
            "The probabilistic model.",
            "We basically I'm only going to talk about frame net because it outperformed Prop Bank across the board.",
            "So the problem is of course that we have a lot of unseen verb role argument triples for estimating this prob."
        ],
        [
            "Ability OK?",
            "And So what we do is class based smoothing.",
            "So for nouns we actually just use word Nets.",
            "Lowest level since set ontologies.",
            "But for verbs we actually decide to induce our own set of classes OK, and we did this using sort of an information theoretic soft clustering algorithms, information distortion information, information bottleneck.",
            "These are soft clustering algorithms, which means that verbs can appear in more than one class.",
            "And we did this as I said on the frame net corpus, which has about 50 or 60,000 propositions covering about 2000 different verbs, I should say frame it for people who don't know bout it is basically a hand selected corpus so it also doesn't have a natural distribution in the sense of something like, say, Wall Street Journal or BNC.",
            "It's actually a set of sensors which were taken from BNC because they were good examples of possible semantic."
        ],
        [
            "Frames OK. And the intuition is that we want to be able to go from sort of instances in the frame net corpus, where we have, for example, a curing event with a doctor.",
            "Or sorry healing event where we have curing doctor and the Doctor as the healer, role or.",
            "The heal event where we have a physician also occupying the healer, role and so on.",
            "OK, and we want to be able to generalize such that if physician occurs as say, the subject of cure instead of heal, that we can kind of count that as a similar sort of instance when we're trying to deal."
        ],
        [
            "Unseen data.",
            "So the."
        ],
        [
            "Verb classes, then?",
            "Well, we're trying to induce classes of semantically similar verbs semantically similar in the sense that."
        ],
        [
            "Assign some more kinds of roles, and those roles have similar kinds of fillers.",
            "I've."
        ],
        [
            "They said we use proper anchor frame that I'm just going to talk about frame net and we did the clustering based on a number of features.",
            "The argument had the role label verb sense."
        ],
        [
            "Tactic path and path roll.",
            "I already said a little bit about the soft clustering algo."
        ],
        [
            "Them."
        ],
        [
            "And just to give you an idea, classes look something like this.",
            "OK, we get 13 classes.",
            "We actually get all kinds of different possible classifications.",
            "The one that worked best was one that where we had 13 possible classes, and this is just an example of a couple of 'em.",
            "OK, so we have a what looks like a move class and communicate class, and these are quite nice when we did for Prop Bank we had four classes and most of the verbs were in one of those classes, so it was really not very useful and very."
        ],
        [
            "OK.",
            "So how well this model, once we sort of created these classes for doing smooth looks for doing smoothing, how well did it work on predicting human judgments?",
            "Well, we tested this on two different datasets.",
            "One is from Ken McRae ET al in JAMA 1998, which gives us 100 data points of good and bad fillers for agent and patient, OK?"
        ],
        [
            "Examples look like this.",
            "You say basically, how likely is it for a doctor to cure somebody right?",
            "Versus how likely is it for a doctor to be cured by somebody and then you get a rating from one to seven?",
            "OK, on a 7 point scale so.",
            "Doctor is a good healer, but not so great a patient.",
            "And.",
            "Doctor, Doctor in the bottom row should be patient, patient.",
            "OK, we say how likely is it for a patient to heal somebody and it's very poor, but how likely is a patient to be healed?",
            "And it's very very good.",
            "OK so sorry for the typo there.",
            "And it's exactly these kinds of judgments which we want to see if we can predict with our probabilistic model."
        ],
        [
            "And so.",
            "I'm sorry I should say something else.",
            "We also used conducted our own judgment study.",
            "OK, one problem with the McRae data set is that we have no seen triples.",
            "OK, there's no instance of cure Doctor Healer.",
            "Which we have seen in the in the frame.",
            "Net corpus.",
            "OK, so we're trying to be trying to predict human judgments for cases that we've never seen at all, so we decided to also conduct our own study where we had 25% seen examples from the Prop bank corpus, 25% seen in the frame net corpus, and then the rest were unseen, and we also had a bigger spread in terms of the kinds of roles that we were considering here, so it's a slightly more ballots and larger data set, and what I've shown here is.",
            "How well we do at predicting these human judgments on both of these datasets.",
            "So we get a significant correlation on McRae data set where we have 64 OK coverage is not bad.",
            "87.5 by coverage, what I mean is that we have seen that verb with assigning that role before.",
            "OK, if we've never seen that particular verb assigning a particular role independent where we've seen the argument before, then we don't even try, OK. For our own data set, we have a much larger N much better coverage and also more highly significant correlation as well.",
            "So the model does OK.",
            "I should say, by the way, if you're trying to come up with an upper baseline for this, if you see how well human judge judgments correlate with the average of other human judgment scores, you get a correlation of about .7.",
            "So this is a hard thing to get.",
            "If you want to consider that as another baseline."
        ],
        [
            "I should I also just want to briefly say we've also there's a bunch of other models out there in the computational linguistics literature which try to do this kind of thing.",
            "OK, role labeling models and also models of selectional preferences which have been around for a long time, starting with.",
            "Resnick's model OK, and when we sort of adapted those models to try and do our task, they don't do very well.",
            "The role labelers are almost hopeless, they rely a lot on syntactic features and what's particularly difficult for them in these datasets is that our datasets aren't natural text, where usually you're going to see good role fillers, right?",
            "We've got 5050 good role fillers and bad fillers, bad role fillers, and that those bad role fillers are really tough.",
            "For these other kinds of models to get right."
        ],
        [
            "So.",
            "Yeah, so just to sort of.",
            "Point out, I don't want to say much more about it that we've actually sort of compared this architecture with some of the existing models that are trying to do similar things and at least for this kind of data, are modeled as."
        ],
        [
            "Better."
        ],
        [
            "OK.",
            "So now that we've got a model which seems to make predictions which correlate well with human judgement data, how do we go about integrating this into a probabilistic parser?",
            "And we do a fairly simple kind of integration.",
            "Here we have a syntactic parsing model which is an incremental probabilistic parser, the one developed by Brian Roark in his PhD thesis.",
            "It's ahead lexicalized parser.",
            "But only had lexicalized, that is, it doesn't do head head dependencies or anything of that sort, doesn't actually tried that, it doesn't actually help perform."
        ],
        [
            "So the parser anyway.",
            "And we add to that our own semantic model.",
            "OK."
        ],
        [
            "And there's a couple of parameters that we also have to decide on when we do this.",
            "So basically we have the syntactic model shown at the bottom.",
            "It parses a sentence, creates a ranking that Paris also gets the parses.",
            "I should say get handed off to the semantic model, and it does its own ranking based purely on the thematic role assignments, which are implied by that parse OK, and then we have to decide on based on the semantic ranking in the syntactic ranking.",
            "First, how we're going to come up with a global ranking that is combined these two.",
            "In some way.",
            "And then Additionally we have to come up with some measure of.",
            "Processing difficulty, that is, when do we predict difficulty or not?",
            "So there's a couple of parameters we need to establish here, one is.",
            "The interpolation factor for combining these two values to get a global ranking.",
            "It turns out that the best parameter there is basically to entirely go with the syntactic.",
            "Bias, that is, you have a factor of interpolation factor of 1.",
            "Times the syntactic ranking and zero times a semantic you can have that as low as .8.",
            "It doesn't really matter, but there's a huge preference for going with the syntactic parse for your initial ranking.",
            "OK.",
            "The difficulty prediction actually has two components to it.",
            "One is called going to conflict in, the other going to revision.",
            "Explain that with an example."
        ],
        [
            "Here so."
        ],
        [
            "Suppose you get a what's called a noun an MPs compliment.",
            "Ambiguity, like the critic wrote the painting had been blah blah blah.",
            "OK, so when you encounter the painting, you don't know quite what you're supposed to do.",
            "It can either be a direct object of the verb, or it can be the subject of an embedded sentence."
        ],
        [
            "OK.",
            "So we have these two possible structures which we could build up to."
        ],
        [
            "Encountered that noun phrase.",
            "And so when we reach the painting, we have a point of conflict.",
            "That is, we have an ambiguity and we have two things taking us in different directions here.",
            "On the one hand, the syntactic parser is going to, say, build the structure on the left.",
            "OK, it's the most likely syntactic parse.",
            "On the other hand, the semantic system is going to say.",
            "Build the one on the right whi well, because paintings are really bad things to try and right.",
            "OK, so painting is a bad role filler for the left hand structure and so we calculate the conflict cost.",
            "We tried various functions but this is the one that works best.",
            "Best in the end as simply the difference between the.",
            "Rank assigned to the best parse by the syntactic system and with our interpolation function.",
            "That's going to be one.",
            "OK, this is going to have the highest rank minus the rank which the semantic system assigns that same parts.",
            "So basically we're going to see where the semantic system says the parts on the left should be ranked and subtract that.",
            "Take the absolute number and it gives us some notion of the degree of conflict between these two systems and."
        ],
        [
            "We're going to use.",
            "Revision happens when you get to the region in green that is had been OK and you realize what you have to do and there's some cost, presumably with having to switch from one interpretation to another.",
            "And again, you could imagine much more fine grained cost functions, but I think because of the noise, the noisiness, and the probabilities that are generated by our model were actually best.",
            "If we just go to a very fixed kind of a cost function which says.",
            "You have a cost of 1 if and only if there's a semantic change.",
            "Non monotonic semantic change.",
            "That is, you have to destroy some aspect of the interpretation of the rules that you've assigned, and Furthermore that the revised semantics has a lower probability than the semantics you had before.",
            "OK. That is, it's only if you have to switch to a semantic interpretation that is worse than what you had for some reason.",
            "OK, so those are the two functions that I'm going to talk to you about.",
            "As I say, we explored several others, but these actually perform best on our development set."
        ],
        [
            "So how?"
        ],
        [
            "We evaluate it now well."
        ],
        [
            "We looked at a study by."
        ],
        [
            "I Sue Garnsey ET al where they manipulated exactly these kinds of phenomena.",
            "That is, she looked at NPS.",
            "Ambiguous sentences with good and bad objects.",
            "She also did another manipulation, which is kind of nice, namely subcategorization bias of the verb itself.",
            "OK, so we have good objects, like the critic wrote the book.",
            "OK, we have that with a direct with a direct object bias verb like wrote an S complement bias verb like argued.",
            "And then you have bad objects, like the critic wrote or argued the painting OK, and the pattern reading pattern that garnsey gets is shown in the graph in the bottom here.",
            "So what we see is for the painting.",
            "So sorry for the book, which is a good object, right?",
            "We get a low reading time, OK?",
            "Down the bottom left there where it's for a bad object like painting.",
            "We get a very elevated reading time, so this is effectively the cost in the sort of conflict right where we are in the ambiguous region, but semantics is biasing us one way or the other, and then we see the cost of revisions showing up when we get to the embedded verb verb phrase right where there's a lower cost.",
            "In the case of the.",
            "Bad objects, presumably because we've already done the work of starting to move towards that analysis.",
            "If not, having are in fact finished it, whereas is a very very high revision cost when we were committed to.",
            "The direct object or NP complement analysis.",
            "OK, so how does the model do when we take the predictions of the model based on our cost functions and sort of do the usual kinds of scaling and normalizing so that they look nice."
        ],
        [
            "On this graph, we get roughly the same pattern of behavior here.",
            "OK, basically the same kind of interaction on the two regions."
        ],
        [
            "And if we look at what happens for the the sentence complement bias verbs, again, we've got a fairly nice.",
            "Fit least qualitatively between our predictions and those.",
            "Of the currency data.",
            "OK, we actually."
        ],
        [
            "Then did some poo."
        ],
        [
            "I mean, you're dealing with relatively few numbers of data points, OK?",
            "Because we don't model each individual items reading times, we model the average reading times OK."
        ],
        [
            "And so we pull data from 2 MPs studies and I'll report those, and in fact we also looked at a bunch of other experiments as well, namely a couple of so-called NP0 ambiguities.",
            "That's examples, like while the man walked the dog barked versus, well, man, what a man walked.",
            "The car drove past or something like that right where the dog versus the car or better, worse, compliments.",
            "Main clause reduced relative clause ambiguities.",
            "We looked at a couple of experiments there as well as PP attachment, and I'll just report the pooled correlations."
        ],
        [
            "For these studies, so we pulled the two NPS studies together where we have 10 or 12 data points.",
            "We get R .688 and when we pull all eight studies together where we have 36 data points, we get our of .7 OK.",
            "Both significant correlations.",
            "So again, the model seems to do fairly well at predicting or generating predictions which correlate with the reading time behavior that we see in these eight study."
        ],
        [
            "So.",
            "Probabilistic models of plausibility provided good account of human judgment finding.",
            "Crucially, they can distinguish good and bad role fillers and not just rely on the fact that most of the time in natural texture."
        ],
        [
            "Good ones.",
            "And in general this fits nicely into the probabilistic framework where we have wide coverage and good performance probabilistic models, which are both rational on the one hand and experience based on the other, can account for the kinds of general lexical category assignment, syntactic.",
            "Analysis and disambiguation behaviors.",
            "And finally, now with this kind of model can also account for thematic role assignment plausibility effects."
        ],
        [
            "OK."
        ],
        [
            "So now I want to shift gears and look at something really quite different which is.",
            "What about trying to model the immediate context so in some sense what I've talked about so far is modeling assumptions our long-term linguistic experience and knowledge of language up to the semantic level.",
            "And now I want to look at what how do we exploit information that's in our immediate context."
        ],
        [
            "So.",
            "There's a whole range of."
        ],
        [
            "Experiments in the cycle.",
            "Linguistic literature since about 1995, which have shown just how close the temporal interaction is between language understanding and visual attention."
        ],
        [
            "OK, basically what we see are utterance mediated eye movements as people listen to utterances which are talking about related scenes.",
            "For example, preferential eye movements trigger looks to mention scene entities.",
            "But what we also see is sort of the compositional interpretation of the unfolding utterance causing anticipatory eye movements to objects which hasn't even been mentioned yet.",
            "Very subtle factors like international cues in the speech can also modulate this kind of visual attention effects.",
            "Sort of contrastive stress or stress about which informs us about word order.",
            "And Furthermore, there's a bit of evidence quite a bit, I think.",
            "Telling us that the scene itself actually influences online language understanding, so it's not just that the tension in the scene is revealing language.",
            "Understanding process is.",
            "It's actually fundamentally informing those."
        ],
        [
            "Processes.",
            "So.",
            "I'm."
        ],
        [
            "Say a very little bit about German because I'm going to talk about a couple of German experiments here.",
            "OK, I suspect most people know this.",
            "I'll go through it very quickly.",
            "German allows subject, verb, object and object group subject word order in its main cause."
        ],
        [
            "Sentences.",
            "It also has case marking.",
            "OK, so if I say Dad has a as rabbit, that's nominative.",
            "But if I say Dean has an, it's an accusative form.",
            "Therefore likely an object.",
            "In contrast, feminine and neuter nouns like Princess, in which is a feminine.",
            "The article doesn't indicate case.",
            "OK, so in this case it's.",
            "It's ambiguous as to whether or not the Princess."
        ],
        [
            "It is nominative or accusative."
        ],
        [
            "What do these studies look like?",
            "Well, people wear a head mounted eye tracker and they look at a very simple clip art visual scene like this.",
            "And then they hear one of two."
        ],
        [
            "Pence is either.",
            "The rabbit eats soon.",
            "The cabbage in a subject verb, object, word order, or the rabbit eat soon.",
            "The Fox in an object, verb subject word order.",
            "The only difference is the case marking.",
            "On the 1st noun phrase OK, First version we know we've got a an SVO.",
            "In the second, we know there's a.",
            "And OVS.",
            "And if we look at the."
        ],
        [
            "Movements right after the verb but before they reach the final noun phrase.",
            "What we see are anticipatory eye movements.",
            "OK, so if we look at the image that people are staring at, then in the first condition.",
            "The SVO condition as soon as people have heard the verb eat."
        ],
        [
            "They're already looking at the cabbage OK, because they know rabbits eat cabbage.",
            "In contrast.",
            "In the second OVS version, they're already using.",
            "Case marking information combined with applause instead of the semantics of the verb 2."
        ],
        [
            "Direct more attention.",
            "Towards the Fox than to the cabbage?",
            "OK, that is we see an increase in looks away from the cabbage and towards the Fox."
        ],
        [
            "OK, so we got a continuous stream of eye movements where you know people first look at the rabbit of course, and then as they hear the verb, they start anticipating either the subject or object of the."
        ],
        [
            "Intense.",
            "We usually just pick out a region and count up the eye."
        ],
        [
            "I fixations in that region.",
            "We get something like this.",
            "We actually don't get quite the pattern that we'd ideally like, but we see in the SVO case we get a lot of looks to the patient, which is the cabbage.",
            "Whereas in the OVS case we gotta drop in those looks, an increase in looks to the Fox, even though it doesn't quite fully override a main effect of looking at the cabbage."
        ],
        [
            "So we then did another study where we said, OK, well, that's nice.",
            "We can see how people use these case marking cues.",
            "Right and combine that with compositional sentence interpretation to direct these eye movements in the scene.",
            "But what if we take away the case marking cues and we put the necessary information to disambiguate the sentence in the scene?",
            "And what do I mean by that?",
            "Well, here we've got the Princess, and it's ambiguous as to whether or not that's a subject or the object of the sentence.",
            "But as soon as you hear the verb as either washes or paints, the events in the scene tell you whether or not she's the agent or the patient.",
            "And so our question was.",
            "Can people use that information as soon as they hear the verb to workout a?",
            "What the right word order is for the sentence and be to anticipate the correct role filler and so our expectation is hopefully in the SVO condition."
        ],
        [
            "Immediately after hearing the verb, we should we should see looks over at the pirate."
        ],
        [
            "But if the verb is mild, the obvious condition we should see look to the fencer."
        ],
        [
            "And sure enough, that's exactly the pattern of eye movements that we get here, OK?",
            "So what we see here then is very clear effect of scene information being used immediately.",
            "The reason these characters in these events are sort of so unusual is that we don't want stereotypical information about who are likely role fillers for this particular verb to be confounding this, so it's exactly the opposite of the first study I talked about, where stereotypical eaters of rabbits versus ease of rabbits or whatever come into play.",
            "So look at two very different kinds of information.",
            "Revealing themselves here and in fact."
        ],
        [
            "We decided to see what happens if we picked those two kinds of info."
        ],
        [
            "Nation sources against each other.",
            "OK, so we know that people can you sort of stereotypical and selectional constraints.",
            "On the one hand, we know that rabbits are very likely to cabbage, or we can use depicted event information where we say, well, we can see that the fencers painting the Princess.",
            "And so we decided to in a single study, workout a weather in a single study.",
            "People can use both of these kinds of information sources and B.",
            "What happens when they pull you in opposite directions?",
            "Do you have a preference for using one information source over?"
        ],
        [
            "Other.",
            "So the stimuli look even more odd.",
            "OK, these are all object, verb subject sentences.",
            "OK, so they start with an accusative template and he's the central character and he's the patient of two events.",
            "OK, and this is the case where either the scene or knowledge tells you who the agent should be.",
            "So if we say, serves.",
            "Then the scene tells you that the."
        ],
        [
            "Detective is probably the role filler 'cause he's depicted as doing this serving event.",
            "But if you say."
        ],
        [
            "If you hear jinxes, well, there's nobody jinxing in the scene.",
            "But you know that the Wizard is a very likely agent of this event, based on stereotypical information.",
            "And So what?"
        ],
        [
            "Found here was what you would hope to get, which is.",
            "Namely people could use either information source.",
            "OK, that is here they did indeed look more at the detective before they heard it, and here they looked more at the wizard."
        ],
        [
            "The interesting case."
        ],
        [
            "Case I'm not.",
            "That's not interesting, But the interesting cases when we pick these two sources against each other.",
            "So here we replace those two different verbs with the same verb, namely spies on.",
            "And in this condition, spies on.",
            "Is referring to the wizard because he's depicted as doing a spying event?",
            "OK, whereas in this condition spies on is referring the detective and he's the stereotypical role filler for this kind of an event.",
            "OK, these stimuli were also fully counterbalanced.",
            "I can go into the design details if you want to afterwards."
        ],
        [
            "And so when we look at what happens after the verb here, OK, we're not expecting any kind of an interaction.",
            "It's gotta be gotta be doing the same thing in both sentences, right?",
            "'cause the sentence is of the same up to that point, the question is."
        ],
        [
            "Which way did they go?",
            "And the answer is they go for the depicted.",
            "OK, that is when they hear spies on, we get more looks to the wizard than to the detective here.",
            "So this seems to be a preference here for relying on the information which is in some sense staring you in the face versus your general knowledge about."
        ],
        [
            "Who's likely to spy on people?"
        ],
        [
            "OK, So what do we have here?",
            "Well, the rapid use of linguistic and scene information we know can guide interpretation and direct visual."
        ],
        [
            "And Furthermore, this anticipation anticipation of role fillers seems to preferentially rely on depicted events over stored knowledge, at least in this."
        ],
        [
            "Setting OK. And.",
            "Roughly what this actually I think I would skip to the next."
        ],
        [
            "5.",
            "The picture that we think this sketch."
        ],
        [
            "This one looks like this.",
            "You have the utterance."
        ],
        [
            "Basically driving attention."
        ],
        [
            "2.",
            "Individual scene information in the scene."
        ],
        [
            "Then informing utterance comprehension.",
            "So during utterance processing we're getting incremental interpretation based on all kinds of linguistic constraints, and also forward inferencing, that is, generation of expectations about what's likely to be mentioned next and this interpretation."
        ],
        [
            "Drives your attention towards relevant objects in the scene, either referentially, that is, you look at objects which have been mentioned.",
            "If you hear Princess, you look at the Princess, but also these anticipatory eye movements.",
            "That is, if you have forward inference, is telling you you're probably going to hear the fencer next.",
            "Then you start looking at the fencer before it's even mentioned."
        ],
        [
            "OK. And Furthermore, if you have reference to something like an event, then by directing your attention to the event in the scene, you can extract the propositional information.",
            "That's concerned with that event and use that to actually inform and possibly even revise the interpretation of the utterance that you've built so far.",
            "OK, and one reason we think that you might see this priority of relying on what's in the immediate scene, and this is very speculative."
        ],
        [
            "Over your stored knowledge is that it could have some sort of."
        ],
        [
            "Basis in language acquisition.",
            "In some sense, it could be a holdover strategy from language acquisition where we're trying to constantly ground what we hear in terms of nouns and events with what's going on in the scene around us.",
            "And maybe this somehow just sticks later in life, even though presumably we're not seeing such unusual or unseen kinds of events with respect to our language anymore, OK?"
        ],
        [
            "5 minutes to say something."
        ],
        [
            "How we might model this so to deal with the modeling?",
            "What we need is some kind of a of an architecture which can integrate two or sort of multiple information sources simultaneously, which on the one hand is also experienced based so they can learn stereotypical role fillers.",
            "But as I say can also learn to adapt to some sort of immediate context if it's there."
        ],
        [
            "OK, one might use probabilistic models here perfectly reasonable.",
            "We wanted to avoid that because we wanted to avoid having to stipulate too much about how you combine these different information sources.",
            "We were afraid that with probabilistic account, we'd have to really be giving that away.",
            "So we rather dis."
        ],
        [
            "To do is build a simple feedforward simple recurrent network."
        ],
        [
            "Basically, expand it a little.",
            "OK, so simple recurrent networks look like this.",
            "I think I'm going to assume most people have seen this before.",
            "Words are presented one at a time.",
            "They get passed through a hidden unit layer to reveal some sort of an interpretation or produce some sort of interpretation, and that hidden layer at each time step gets copied back some context layer, which can then also inform processing at the next word.",
            "OK, that's what's used to sort of build up some representation of the sentence overtime."
        ],
        [
            "So.",
            "In the architecture that we build called CIA.",
            "Net for the coordinated interplay account.",
            "What we did is we started off with.",
            "A simple recurrent network architecture like this."
        ],
        [
            "Yes.",
            "And we added to it some sort of a representation of the scene.",
            "In fact, we did quite a number of simulations using just this architecture, but in the most recent one."
        ],
        [
            "We also have.",
            "And attentional gating vector here?",
            "And what this does is it basically masks the events that are in the visual scene and directs attention to one event over another OK?",
            "Some sense amplifying the importance of that event for comprehension."
        ],
        [
            "So the goal here, then, is to model a whole bunch of things right.",
            "We're trying to model experience, that is, make sure it learns things like stereotypical role fillers, model the immediate use of scene information if it's available, work perfectly well if the scene is not there, and construct an interpretation of the sentence in the absence of the scene model, the priority of the scene if stereotypical knowledge and seen information conflict with each other.",
            "And of course model the general kinds of anticipatory behavior."
        ],
        [
            "We see in the experiments.",
            "So."
        ],
        [
            "Slightly more detailed look at the architecture.",
            "More colorful is."
        ],
        [
            "Well.",
            "Again, simply the simple recurrent network architecture.",
            "Here we have a representation of the scene at the top.",
            "OK."
        ],
        [
            "So.",
            "What you see in the scene at the top sort of corresponds to this image here, right?",
            "We have a wizard spying on a pilot, and then we have a detective serving the pilot, and then we have this gating vector, which is in some sense telling us.",
            "Gets multiplied elementwise to the left of.",
            "Enter the right event and effectively, then shifts attention from one to the other."
        ],
        [
            "And this is trained with back prop through time.",
            "A few details.",
            "The two events actually get fed into the hidden layer through shared weights.",
            "That avoids it sort of relying on positional information in the network to solve the problem.",
            "It's trained in a supervised manner on the interpretation that is during training we tell it what the correct output interpretation should be.",
            "This sort of case role representation of the meaning, but the attentional vector is unsupervised, so it has to learn how to adapt that attentional vector to optimize getting the right interpretation."
        ],
        [
            "OK."
        ],
        [
            "In terms of the training data, what we did is."
        ],
        [
            "Constructed grammar which effectively covers the experimental stimuli from two of the experiments that I presented, the depicted events study and then this other study where we have compared stereotypical knowledge versus event depicted knowledge.",
            "The idea was to come up with a single grammar and lexicon which sort of covered both of these so that we weren't, in some sense, fitting independently to the two experimental datasets.",
            "We also kept out exactly those conflicting examples.",
            "That is, examples where the verb.",
            "On the one hand identified a stereotypical role filler, but at the same time a role filler that was depicted was depicted OK. 'cause what we want to do is see if we can actually predict this preference to rely on the depicted event information, and so we don't explicitly train on any of those kinds of instances.",
            "It never sees a conflict case during training."
        ],
        [
            "We gave it a scene as context in about 50% of the cases.",
            "I think in something like 5% of the cases the scene was totally irrelevant to what was being said in the sentence as well.",
            "So a little bit of noise."
        ],
        [
            "There as well.",
            "I should say in terms of the overall performance, the network does basically perfectly, that is, in the sense that at the end of a sentence it always has the right.",
            "Meaning meaning representation.",
            "What we're interested in evaluating here is what's it doing during processing.",
            "That is, does it have the right anticipatory behaviors during processing and so?"
        ],
        [
            "This is a look at the attentional vector.",
            "OK the gating vector.",
            "And.",
            "Red and blue are the unambiguous cases.",
            "OK, So what this is showing is that as we move through the sentence, if the verb.",
            "Unambiguously identifies the stereotypical agent.",
            "Then we see attention going upwards, which represents a shift in attention towards the stereotypical event.",
            "If, on the other hand, it unambiguously identifies a depicted agent, that's the blue line, then attention shifts downwards and that represents attention towards the depicted role filler.",
            "OK. Just in case, is the pink in the green lines here and there?",
            "What we're seeing is as soon as the verb gets reached.",
            "We're seeing a shift in attention to the scene, so this is the conflict conditions and we see that right away at the verb.",
            "It's predicting this shift in attention towards the depicted as opposed to the stereotypical OK. Of course, that persists a little.",
            "And of course, finally, when it here is the actual disambiguating down at the end, which says, I'm either the depicted one or on the stereotypical one, it gets the right.",
            "Shifting attention there."
        ],
        [
            "Another thing we can look at is actually what role?"
        ],
        [
            "Filler is the network predicting in its output, meaning representation.",
            "OK, so this is the normalized Euclidean of the output, subject to the actual depicted.",
            "Agent, whether that's the stereotypical one or the one that's involved in the event, OK. Um?",
            "And here again, what you see is at the verb.",
            "You actually get an interesting effect here, which is.",
            "And across the board preference at the verb to actually predict the stereotypical stereotypical character.",
            "The reason that happens at the verb is because the attention gaming vector hasn't yet had a chance to modulate the event.",
            "That can only happen at the subsequent at the next time step.",
            "OK, so this actually makes a prediction that.",
            "We could in theory look at experimentally, which says that immediately at the verb you've actually got a biased towards predicting a stereotypical role filler.",
            "But as soon as you sort of use your visual attention to examine the relevant part of the scene that flips and you actually have a preference too.",
            "Fill the role with the depicted agent OK, which you see, then showing up on the adverb.",
            "OK, where the blue?",
            "The biggest case is red and blue aren't interesting, but the pink and the.",
            "And the green you see a slight bias at the verb to tip towards.",
            "Predicting the depicted role filler.",
            "OK, so.",
            "It seems to capture basically all the phenomena that we're interested in, in addition to predicting.",
            "Without ever having been trained on it, this bias towards using.",
            "Depicted event role fillers over stereotypical role fillers."
        ],
        [
            "When the two conflict.",
            "OK."
        ],
        [
            "20"
        ],
        [
            "Say this."
        ],
        [
            "I don't think so.",
            "OK, So what are some?"
        ],
        [
            "General comments or directions so I.",
            "What I'm generally trying to sell here is the idea that models of language processing and ultimately language acquisition really need to be situated.",
            "That is, we can only go so far by by sort of trying to circumscribe all of those problems and investigate them.",
            "While ignoring all the systems to which there intercont."
        ],
        [
            "Acted in intertwined in the case of sentence processing, we need to do this looking beyond simple lexical and syntactic processing.",
            "In terms of the challenges well?",
            "Clearly the kinds of machine learning methods I've talked about here are very simple off the shelf kinds of machine learning techniques.",
            "I have no doubt that there are better, better models for learning semantic classes and modeling semantic model, possibly even unsupervised.",
            "Techniques and more cognitive plausible, cognitively plausible kinds of supervised techniques.",
            "Challenges in terms of connecting our models with grounding in the environment.",
            "I mean, this is already come up in Michael Franks talk earlier on.",
            "Building models which are better grounded in terms of theories of acquisition, situated acquisition and also treating the environment not just as something as a separate notion, but something which is fundamentally part of what it means to understand the sentence that is by grounding and interpretation with what's in the world around us.",
            "We don't just do normal interpretation, it's in some sense and enriched notion of comprehension because it's augmented by the actual properties of what's going on in the invisible in the visual environment.",
            "A huge challenge I think as we try to integrate these multiple kinds of cues and information sources from the environment.",
            "Again, it came up in Michael Franks talk is the real time integration of these kinds of multimodal informational cues.",
            "That is, how do we take speech information scene information, gesture, joint attention and so on, and combine all of these in some sort of real time way such that we can identify the correlations which are necessary to do the kinds of machine learning that we need.",
            "I'll stop there."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks, thanks very much by the way and I would like to thank the organizers for inviting me here.",
                    "label": 0
                },
                {
                    "sent": "Thank him a little less for putting me in the last slot.",
                    "label": 0
                },
                {
                    "sent": "Thanks to all of you for sticking it out.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm going to talk about modeling semantic plausibility and visual context in online sentence comprehension.",
                    "label": 1
                },
                {
                    "sent": "Basically two fairly different pieces of work here that I'm going to try and glue together.",
                    "label": 0
                },
                {
                    "sent": "It also reflects.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joint work with you missed that.",
                    "label": 0
                },
                {
                    "sent": "These people very important peak.",
                    "label": 0
                },
                {
                    "sent": "Novella Marshall Mayberry and re copado.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the general area of research is a little bit different than some of the stuff we've been hearing about.",
                    "label": 0
                },
                {
                    "sent": "Especially today is to investigate the processes that underlie the human capacity to understand language primary.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the sentence level, so I'm interested in how the language processor works.",
                    "label": 0
                },
                {
                    "sent": "What are its architectures and mechanisms that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did underlie this?",
                    "label": 0
                },
                {
                    "sent": "And of course, how can we model this computationally?",
                    "label": 1
                },
                {
                    "sent": "Both understanding that is how people actually arrive at some sort of meaning, or some sort of representation which they can convert into action which I'm going to call competence in a very.",
                    "label": 0
                },
                {
                    "sent": "Sort of generic sense of the term competence as opposed to Chomsky said, and also performance, right?",
                    "label": 0
                },
                {
                    "sent": "That is that we want to be able to capture people's behavior.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another point that especially interested in is how?",
                    "label": 0
                },
                {
                    "sent": "Language understanding sentence comprehension interacts with other cognitive systems and also in fact the environment around.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of divide this up into three parts.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say a little bit to begin with about sort of what I think is a sort of current state of human sentence comprehension.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Research and then going to talk about the development of a wide coverage model of semantic plausibility and how we can integrate that into a.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Full sentence comprehension.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'll say a little bit, or maybe quite a lot about visually situated language understanding and attention.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Traditional approaches in psycholinguistics.",
                    "label": 0
                },
                {
                    "sent": "Have sort of or at least in sentence processing research have focused on.",
                    "label": 0
                },
                {
                    "sent": "Largely on reading studies, that is, we look at in highly controlled conditions.",
                    "label": 0
                },
                {
                    "sent": "The amount of time people spend on particular regions of a sentence in particular, usually looking at particular sort of ambiguous constructions, either using self paced reading or eye tracking and reading.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So by measuring reading times, we measure processing complexity of some sort, which we then try to relate to complexity in our models.",
                    "label": 0
                },
                {
                    "sent": "The result is that the psycholinguistic theories that are around.",
                    "label": 0
                },
                {
                    "sent": "Tend to emphasize a largely purely linguistic processing, lexical syntactic to some extent semantic.",
                    "label": 1
                },
                {
                    "sent": "And these theories try to explain processing complexity.",
                    "label": 1
                },
                {
                    "sent": "The result, though I think.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that there's an emphasis in these theories on the weaknesses of human language comprehension.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Furthermore, is in general a failure to situate human language processing within our world more?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these are things I want to address, so some sense what underlies a lot of the work I do is what I've called the Performance Paradox, which I think people in computational linguistics, especially nowadays, are pretty aware of.",
                    "label": 0
                },
                {
                    "sent": "But psycholinguistics maybe less so, and that is how is it that people understand language so accurately, accurately, and effortlessly, given its complexity and ambiguity.",
                    "label": 1
                },
                {
                    "sent": "We understand this incrementally, basically word by word, sometimes even at finer grained levels of incrementality than that in real time.",
                    "label": 0
                },
                {
                    "sent": "In fact, we don't just keep up with the input word stream more often even anticipating what's going to come next week and rapidly resolve ambiguities and revise missing.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reputations and I think two factors which are involved in this.",
                    "label": 0
                },
                {
                    "sent": "I'm calling them a solution, maybe isn't the right thing to say, but I think 2 crucial contributors to why people can do this is that we optimize based on both our long-term linguistic experience and knowledge, and that we can adaptively exploit our immediate linguistic and non linguistic context.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we get a picture that looks something.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like this when we kind of broaden our view a little bit where we're building sentence processing models.",
                    "label": 1
                },
                {
                    "sent": "Which on the one hand have to be competent.",
                    "label": 0
                },
                {
                    "sent": "OK, that is have some sort of reasonable linguistic coverage, reasonable linguistic complexity, and models which can arrive at some sort of a useful interpretation of.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parents.",
                    "label": 0
                },
                {
                    "sent": "Of course, we also want to explain performance or human behavior that we want to be able to explain why we see elevated reading times in certain situations, why we get certain blips in our event related potentials, patterns of visual attention, which I'll talk about more a little bit later on, imaging all kinds of experimental paradigms where we want to be able to explain observed behaviors, or.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Neural activity.",
                    "label": 0
                },
                {
                    "sent": "We also increasingly need to explain how these models interact with other cognitive systems such as visual processes, memory.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mention.",
                    "label": 0
                },
                {
                    "sent": "And in some sense, well, there's a catchall category over here.",
                    "label": 0
                },
                {
                    "sent": "What I've just called adaptation that is, how do people in some sense adapt to their linguistic experience, adapt to the immediate context?",
                    "label": 0
                },
                {
                    "sent": "What's going on in their environment, and the tasks in which they are engaged?",
                    "label": 0
                },
                {
                    "sent": "We increasingly know that all of these different things play quite a strong role, and yet our theories really ignore a lot of them.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If Martin can do it, so can I.",
                    "label": 0
                },
                {
                    "sent": "So what are some of the challenges here?",
                    "label": 0
                },
                {
                    "sent": "Well, other sort of hinted at before we want to account not just for the garden path sentences that we spend a lot of time studying, but for garden variety language.",
                    "label": 1
                },
                {
                    "sent": "The fact that people can read a 40 word sentence out of the Wall Street Journal effortlessly right, despite the fact that it probably 2/3 of the words in there are category ambiguous and so on.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, no problem at all.",
                    "label": 0
                },
                {
                    "sent": "Semantics, as I say, today we don't really have very good models of semantics that is plausibility, selectional restrictions.",
                    "label": 1
                },
                {
                    "sent": "How priming works, associates, and more generally, kinds of inferences that we might want to even make.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to use linguistic and non linguistic context in some sort of a reasonable way, such as exploiting common?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sound visual environment so on.",
                    "label": 1
                },
                {
                    "sent": "And also creating, especially at the level of sentence processing, really concrete quantitative predictions for these kinds of multiple behavioral and neuroscientific measures that were.",
                    "label": 1
                },
                {
                    "sent": "Using two info.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Armor theory development.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, well I think probabilistic sentence processing is kind of become.",
                    "label": 1
                },
                {
                    "sent": "In models of sentence processing, the sort of favored way to go and it addresses some of these problems.",
                    "label": 0
                },
                {
                    "sent": "So firstly at an empirical level, there's tons of evidence for the importance of frequency information, right?",
                    "label": 1
                },
                {
                    "sent": "We all know that happens at all levels of linguistic representation from lexical right up through to structural, and possibly even semantics.",
                    "label": 0
                },
                {
                    "sent": "And probabilistic mechanisms provide a natural way to capture these kinds of frequence.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Preferences.",
                    "label": 0
                },
                {
                    "sent": "Something we also learn from computational linguistics is that we can use probabilities as a way of expressing preferences and biases where we haven't really got a clue about how they work.",
                    "label": 1
                },
                {
                    "sent": "Underlyingly we can kind of use it as a way of approximating things that we don't really know what to do with yet, and that can be quite used.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Building models.",
                    "label": 0
                },
                {
                    "sent": "We have good techniques with these kinds of models for training.",
                    "label": 1
                },
                {
                    "sent": "We know we can get systems with fairly large coverage so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're doing fairly well and we got a uniform explanation of ambiguity resolution at all different levels of lingua.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Processing Furthermore, there's some interesting kinds of linking hypothesis we can start to construct with these probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "We can do ranking and re ranking with them because we're getting sort of clear sort of rankings of different alternative hypothesis.",
                    "label": 0
                },
                {
                    "sent": "But there's other sorts of information theoretic complexity measures such as surprise, surprise or entropy reduction and so on which have been shown to correlate quite nicely with certain kinds of behaviors.",
                    "label": 0
                },
                {
                    "sent": "No, I'm not.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anymore about those here.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way you can work then just generally when you're building these probabilistic models, is to sort of take sort of a rational analysis.",
                    "label": 0
                },
                {
                    "sent": "Kind of an approach here where you at the very top level identify some sort of goal for your system.",
                    "label": 0
                },
                {
                    "sent": "That is to say, optimize accurate.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interpretation.",
                    "label": 0
                },
                {
                    "sent": "From there you can express that formally in terms of some sort of a function like, let's maximize the probability of sentence interpretations as we're going along understanding a sentence.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And from there you go to some sort of realization of that.",
                    "label": 0
                },
                {
                    "sent": "Where you say, well, how do we actually estimate this probability really?",
                    "label": 0
                },
                {
                    "sent": "Anyway, maybe we'll estimate it from corpora and assume that they represent our linguistic experience at some level, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this sort of follows a standard sort of marry an approach to how we might build these kinds of models.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this has been done successfully in various levels of of sentence processing, language processing research.",
                    "label": 0
                },
                {
                    "sent": "For example, in lexical category decembe disambiguation, we can maximize the probability of a given word sequence and part of speech sequence.",
                    "label": 0
                },
                {
                    "sent": "And this has been shown to predict well not only to we can implement this as a standard bigram part of speech model.",
                    "label": 0
                },
                {
                    "sent": "This is kind of nice because on the one hand it has fairly high accuracy in part of speech tagging and on the other hand it's been shown to kind of model exactly some of the behaviors that people exhibit in resolving lexical category ambiguities.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can sort of go up to the next level and apply the same ideas to sentence processing where we simply use probabilistic context free grammars or related kinds of grammatical models.",
                    "label": 0
                },
                {
                    "sent": "And as each word in the sentence comes in, we evaluate all possible syntactic structures and constantly sort of favoring the most probable one.",
                    "label": 1
                },
                {
                    "sent": "OK, this work, based on ideas from dangerous keys Paper 96 and implemented in a wide coverage model by Torsten Branson myself.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little later on.",
                    "label": 0
                },
                {
                    "sent": "So how does that model work and how far do we get with?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, here's just a simple example with reduced relative clause.",
                    "label": 1
                },
                {
                    "sent": "You have something like the man race to the station was innocent.",
                    "label": 1
                },
                {
                    "sent": "This is a lot like the classic horse race.",
                    "label": 0
                },
                {
                    "sent": "Past the barn fell kind.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example.",
                    "label": 0
                },
                {
                    "sent": "The way the parts would work is it would come along and say we have the man raced, raced it, turns out, happens to be intransitive.",
                    "label": 1
                },
                {
                    "sent": "Preference, so that's what the.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I means there so when we get 2 the system thinks OK.",
                    "label": 0
                },
                {
                    "sent": "I think 2 actually starts in infinitival complement.",
                    "label": 0
                },
                {
                    "sent": "Here the man raced to help the old woman or something like that.",
                    "label": 1
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get the following noun phrase.",
                    "label": 0
                },
                {
                    "sent": "You realize that probably wasn't right.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you realize this is probably a prepositional phrase, but because the verb can be intransitive, you're quite happy to continue pursuing a main clause interpretation of this sentence OK. And when you finally get the copula.",
                    "label": 0
                },
                {
                    "sent": "You don't know what to do with it.",
                    "label": 0
                },
                {
                    "sent": "OK, at this point the parser breaks down.",
                    "label": 0
                },
                {
                    "sent": "Effectively, it's been Garden path here and it tries to attach the was sort of the last edge in the parse.",
                    "label": 0
                },
                {
                    "sent": "So here we've managed to predict a garden path.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Contrast.",
                    "label": 0
                },
                {
                    "sent": "If you just change a couple of the.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kids and you replace Raceway.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Held then.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When held is reached, we see it's very similar, except that this time it believes that held is preferably a transitive verb.",
                    "label": 0
                },
                {
                    "sent": "That's what the little T means on the tag.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Beneath held.",
                    "label": 0
                },
                {
                    "sent": "And so as soon as it gets the preposition after held, it realizes.",
                    "label": 0
                },
                {
                    "sent": "Well wait, if there's a preposition after this verb, it's probably actually a past participle, because I was expecting a direct object.",
                    "label": 0
                },
                {
                    "sent": "At isn't going to start a direct object, and so it can immediately use this sort of probabilistic information to recover retag the verb.",
                    "label": 0
                },
                {
                    "sent": "Build the correct syntactic structure.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And move on.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ultimately, getting the correct person not be garden path and exactly these kinds of probabilistic use have been shown by people like Mary L. McDonald and others too.",
                    "label": 0
                },
                {
                    "sent": "Mitigator modulate the difficulty of these kinds of garden path constructions so probabilistic models do this kind of thing really very.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nicely.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But one thing they don't do well is some.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Antics, at least not yet.",
                    "label": 0
                },
                {
                    "sent": "And So what do I mean?",
                    "label": 0
                },
                {
                    "sent": "Well, suppose we have something like the doctor cured.",
                    "label": 1
                },
                {
                    "sent": "OK, we're pretty happy building a main clause analysis for this.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And only when something like by the.",
                    "label": 0
                },
                {
                    "sent": "Surgeon, I don't know comes up, do we realize that we have to re analyze this?",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose the first word were something like patient.",
                    "label": 0
                },
                {
                    "sent": "We had the patient cared well in this case.",
                    "label": 0
                },
                {
                    "sent": "We may even prefer the second syntactic structure here.",
                    "label": 0
                },
                {
                    "sent": "The bottom one, the reduced relative clause, or if not, we certainly find it a lot easier to move to it once we get this continuation of the sentence.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how might we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, it seems.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is pretty intuitive, but to do it what we need is to look a little bit more closely at the information that's available to us.",
                    "label": 0
                },
                {
                    "sent": "So in particular I'm going to suggest we need to do is look at verb argument, role relations in modeling some sort of notion of the plausibility of a verb, argument, role relation.",
                    "label": 0
                },
                {
                    "sent": "That is, we need to assess how likely is it for patients to cure people.",
                    "label": 0
                },
                {
                    "sent": "How likely is it for patients to be the healer of a curing event?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we want then is to build some sort of a graded model of verb argument plausibility, something that we can ideally automatically induce from some sort of semantically annotated corpus.",
                    "label": 1
                },
                {
                    "sent": "You might ideally want to do it from using some.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unsupervised methods.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to show how we can evaluate that model in the 1st place by looking at how well it predicts human plausibility judgment data.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I'm going to see how we how it works when we try to integrate it into a model of incremental parsing and see how well it predicts reading time data.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and I'm going to very briefly present some findings there, but.",
                    "label": 0
                },
                {
                    "sent": "Relations between the model and.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Various reading time studies.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The model looks roughly like this.",
                    "label": 0
                },
                {
                    "sent": "What we're doing is the basic claim is that we can assess the plausibility of some role argument or verb argument roll triple.",
                    "label": 0
                },
                {
                    "sent": "As the probability of that role, argument, verb, verb, sense, and the grammatical function.",
                    "label": 1
                },
                {
                    "sent": "Which we then break down using the chain rule OK into the expression that you see sort of spread over 2 lines.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we've got these five features now.",
                    "label": 0
                },
                {
                    "sent": "Anybody who's built probabilistic models of this sort realizes you're going to lots of problems with sparse data, and so of course we use smoothing.",
                    "label": 0
                },
                {
                    "sent": "The obvious technique, little less interesting, is to use good Turing smoothing, at least for the first 4 terms, and then for the final term, which is in some sense the most interesting and important one.",
                    "label": 0
                },
                {
                    "sent": "We use a class based moving method.",
                    "label": 0
                },
                {
                    "sent": "OK to generalize from word tokens to word classes.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's two resources we can use here.",
                    "label": 0
                },
                {
                    "sent": "One is prop bank right?",
                    "label": 0
                },
                {
                    "sent": "Sort of notionally a semantically annotated version of the Wall Street Journal, which has roughly entries that look like it's on the top.",
                    "label": 1
                },
                {
                    "sent": "There we have some notion of cure and Doctor gets annotated as the R0 rolark zeros usually reserved for sort of agent like role fillers.",
                    "label": 0
                },
                {
                    "sent": "We have the verb cured and then we have the patient which is annotated with Arg, one which is usually for the patient or the.",
                    "label": 1
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Team role.",
                    "label": 0
                },
                {
                    "sent": "We also have frame net which is a much more richly annotated corpus.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it's also smaller.",
                    "label": 0
                },
                {
                    "sent": "OK, so in frame net we actually have semantic frames, like the healing frame, which when it's applied to a sentence like the doctor cleared, the patient will annotate the doctor with a healer role and the patient with a patient role.",
                    "label": 1
                },
                {
                    "sent": "And gives us in some sense rather more accurate and consistent role.",
                    "label": 0
                },
                {
                    "sent": "Annotations and prop bank.",
                    "label": 0
                },
                {
                    "sent": "As I say, it's a little bit smaller.",
                    "label": 0
                },
                {
                    "sent": "And so we actually looked at both of these corpora when we were trying to induce us.",
                    "label": 0
                },
                {
                    "sent": "The probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "We basically I'm only going to talk about frame net because it outperformed Prop Bank across the board.",
                    "label": 0
                },
                {
                    "sent": "So the problem is of course that we have a lot of unseen verb role argument triples for estimating this prob.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ability OK?",
                    "label": 0
                },
                {
                    "sent": "And So what we do is class based smoothing.",
                    "label": 0
                },
                {
                    "sent": "So for nouns we actually just use word Nets.",
                    "label": 0
                },
                {
                    "sent": "Lowest level since set ontologies.",
                    "label": 0
                },
                {
                    "sent": "But for verbs we actually decide to induce our own set of classes OK, and we did this using sort of an information theoretic soft clustering algorithms, information distortion information, information bottleneck.",
                    "label": 0
                },
                {
                    "sent": "These are soft clustering algorithms, which means that verbs can appear in more than one class.",
                    "label": 0
                },
                {
                    "sent": "And we did this as I said on the frame net corpus, which has about 50 or 60,000 propositions covering about 2000 different verbs, I should say frame it for people who don't know bout it is basically a hand selected corpus so it also doesn't have a natural distribution in the sense of something like, say, Wall Street Journal or BNC.",
                    "label": 0
                },
                {
                    "sent": "It's actually a set of sensors which were taken from BNC because they were good examples of possible semantic.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Frames OK. And the intuition is that we want to be able to go from sort of instances in the frame net corpus, where we have, for example, a curing event with a doctor.",
                    "label": 0
                },
                {
                    "sent": "Or sorry healing event where we have curing doctor and the Doctor as the healer, role or.",
                    "label": 0
                },
                {
                    "sent": "The heal event where we have a physician also occupying the healer, role and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, and we want to be able to generalize such that if physician occurs as say, the subject of cure instead of heal, that we can kind of count that as a similar sort of instance when we're trying to deal.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unseen data.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Verb classes, then?",
                    "label": 0
                },
                {
                    "sent": "Well, we're trying to induce classes of semantically similar verbs semantically similar in the sense that.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assign some more kinds of roles, and those roles have similar kinds of fillers.",
                    "label": 0
                },
                {
                    "sent": "I've.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They said we use proper anchor frame that I'm just going to talk about frame net and we did the clustering based on a number of features.",
                    "label": 0
                },
                {
                    "sent": "The argument had the role label verb sense.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tactic path and path roll.",
                    "label": 0
                },
                {
                    "sent": "I already said a little bit about the soft clustering algo.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Them.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just to give you an idea, classes look something like this.",
                    "label": 0
                },
                {
                    "sent": "OK, we get 13 classes.",
                    "label": 0
                },
                {
                    "sent": "We actually get all kinds of different possible classifications.",
                    "label": 0
                },
                {
                    "sent": "The one that worked best was one that where we had 13 possible classes, and this is just an example of a couple of 'em.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a what looks like a move class and communicate class, and these are quite nice when we did for Prop Bank we had four classes and most of the verbs were in one of those classes, so it was really not very useful and very.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how well this model, once we sort of created these classes for doing smooth looks for doing smoothing, how well did it work on predicting human judgments?",
                    "label": 0
                },
                {
                    "sent": "Well, we tested this on two different datasets.",
                    "label": 0
                },
                {
                    "sent": "One is from Ken McRae ET al in JAMA 1998, which gives us 100 data points of good and bad fillers for agent and patient, OK?",
                    "label": 1
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples look like this.",
                    "label": 0
                },
                {
                    "sent": "You say basically, how likely is it for a doctor to cure somebody right?",
                    "label": 0
                },
                {
                    "sent": "Versus how likely is it for a doctor to be cured by somebody and then you get a rating from one to seven?",
                    "label": 0
                },
                {
                    "sent": "OK, on a 7 point scale so.",
                    "label": 0
                },
                {
                    "sent": "Doctor is a good healer, but not so great a patient.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Doctor, Doctor in the bottom row should be patient, patient.",
                    "label": 0
                },
                {
                    "sent": "OK, we say how likely is it for a patient to heal somebody and it's very poor, but how likely is a patient to be healed?",
                    "label": 0
                },
                {
                    "sent": "And it's very very good.",
                    "label": 0
                },
                {
                    "sent": "OK so sorry for the typo there.",
                    "label": 0
                },
                {
                    "sent": "And it's exactly these kinds of judgments which we want to see if we can predict with our probabilistic model.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I should say something else.",
                    "label": 0
                },
                {
                    "sent": "We also used conducted our own judgment study.",
                    "label": 0
                },
                {
                    "sent": "OK, one problem with the McRae data set is that we have no seen triples.",
                    "label": 0
                },
                {
                    "sent": "OK, there's no instance of cure Doctor Healer.",
                    "label": 0
                },
                {
                    "sent": "Which we have seen in the in the frame.",
                    "label": 0
                },
                {
                    "sent": "Net corpus.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're trying to be trying to predict human judgments for cases that we've never seen at all, so we decided to also conduct our own study where we had 25% seen examples from the Prop bank corpus, 25% seen in the frame net corpus, and then the rest were unseen, and we also had a bigger spread in terms of the kinds of roles that we were considering here, so it's a slightly more ballots and larger data set, and what I've shown here is.",
                    "label": 0
                },
                {
                    "sent": "How well we do at predicting these human judgments on both of these datasets.",
                    "label": 0
                },
                {
                    "sent": "So we get a significant correlation on McRae data set where we have 64 OK coverage is not bad.",
                    "label": 0
                },
                {
                    "sent": "87.5 by coverage, what I mean is that we have seen that verb with assigning that role before.",
                    "label": 0
                },
                {
                    "sent": "OK, if we've never seen that particular verb assigning a particular role independent where we've seen the argument before, then we don't even try, OK. For our own data set, we have a much larger N much better coverage and also more highly significant correlation as well.",
                    "label": 0
                },
                {
                    "sent": "So the model does OK.",
                    "label": 0
                },
                {
                    "sent": "I should say, by the way, if you're trying to come up with an upper baseline for this, if you see how well human judge judgments correlate with the average of other human judgment scores, you get a correlation of about .7.",
                    "label": 0
                },
                {
                    "sent": "So this is a hard thing to get.",
                    "label": 0
                },
                {
                    "sent": "If you want to consider that as another baseline.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I should I also just want to briefly say we've also there's a bunch of other models out there in the computational linguistics literature which try to do this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "OK, role labeling models and also models of selectional preferences which have been around for a long time, starting with.",
                    "label": 0
                },
                {
                    "sent": "Resnick's model OK, and when we sort of adapted those models to try and do our task, they don't do very well.",
                    "label": 0
                },
                {
                    "sent": "The role labelers are almost hopeless, they rely a lot on syntactic features and what's particularly difficult for them in these datasets is that our datasets aren't natural text, where usually you're going to see good role fillers, right?",
                    "label": 0
                },
                {
                    "sent": "We've got 5050 good role fillers and bad fillers, bad role fillers, and that those bad role fillers are really tough.",
                    "label": 0
                },
                {
                    "sent": "For these other kinds of models to get right.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so just to sort of.",
                    "label": 0
                },
                {
                    "sent": "Point out, I don't want to say much more about it that we've actually sort of compared this architecture with some of the existing models that are trying to do similar things and at least for this kind of data, are modeled as.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now that we've got a model which seems to make predictions which correlate well with human judgement data, how do we go about integrating this into a probabilistic parser?",
                    "label": 0
                },
                {
                    "sent": "And we do a fairly simple kind of integration.",
                    "label": 0
                },
                {
                    "sent": "Here we have a syntactic parsing model which is an incremental probabilistic parser, the one developed by Brian Roark in his PhD thesis.",
                    "label": 1
                },
                {
                    "sent": "It's ahead lexicalized parser.",
                    "label": 0
                },
                {
                    "sent": "But only had lexicalized, that is, it doesn't do head head dependencies or anything of that sort, doesn't actually tried that, it doesn't actually help perform.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the parser anyway.",
                    "label": 0
                },
                {
                    "sent": "And we add to that our own semantic model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's a couple of parameters that we also have to decide on when we do this.",
                    "label": 0
                },
                {
                    "sent": "So basically we have the syntactic model shown at the bottom.",
                    "label": 1
                },
                {
                    "sent": "It parses a sentence, creates a ranking that Paris also gets the parses.",
                    "label": 0
                },
                {
                    "sent": "I should say get handed off to the semantic model, and it does its own ranking based purely on the thematic role assignments, which are implied by that parse OK, and then we have to decide on based on the semantic ranking in the syntactic ranking.",
                    "label": 1
                },
                {
                    "sent": "First, how we're going to come up with a global ranking that is combined these two.",
                    "label": 0
                },
                {
                    "sent": "In some way.",
                    "label": 0
                },
                {
                    "sent": "And then Additionally we have to come up with some measure of.",
                    "label": 0
                },
                {
                    "sent": "Processing difficulty, that is, when do we predict difficulty or not?",
                    "label": 1
                },
                {
                    "sent": "So there's a couple of parameters we need to establish here, one is.",
                    "label": 0
                },
                {
                    "sent": "The interpolation factor for combining these two values to get a global ranking.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the best parameter there is basically to entirely go with the syntactic.",
                    "label": 0
                },
                {
                    "sent": "Bias, that is, you have a factor of interpolation factor of 1.",
                    "label": 0
                },
                {
                    "sent": "Times the syntactic ranking and zero times a semantic you can have that as low as .8.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter, but there's a huge preference for going with the syntactic parse for your initial ranking.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "The difficulty prediction actually has two components to it.",
                    "label": 0
                },
                {
                    "sent": "One is called going to conflict in, the other going to revision.",
                    "label": 0
                },
                {
                    "sent": "Explain that with an example.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here so.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suppose you get a what's called a noun an MPs compliment.",
                    "label": 0
                },
                {
                    "sent": "Ambiguity, like the critic wrote the painting had been blah blah blah.",
                    "label": 1
                },
                {
                    "sent": "OK, so when you encounter the painting, you don't know quite what you're supposed to do.",
                    "label": 0
                },
                {
                    "sent": "It can either be a direct object of the verb, or it can be the subject of an embedded sentence.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we have these two possible structures which we could build up to.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Encountered that noun phrase.",
                    "label": 0
                },
                {
                    "sent": "And so when we reach the painting, we have a point of conflict.",
                    "label": 0
                },
                {
                    "sent": "That is, we have an ambiguity and we have two things taking us in different directions here.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, the syntactic parser is going to, say, build the structure on the left.",
                    "label": 0
                },
                {
                    "sent": "OK, it's the most likely syntactic parse.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the semantic system is going to say.",
                    "label": 0
                },
                {
                    "sent": "Build the one on the right whi well, because paintings are really bad things to try and right.",
                    "label": 0
                },
                {
                    "sent": "OK, so painting is a bad role filler for the left hand structure and so we calculate the conflict cost.",
                    "label": 0
                },
                {
                    "sent": "We tried various functions but this is the one that works best.",
                    "label": 0
                },
                {
                    "sent": "Best in the end as simply the difference between the.",
                    "label": 0
                },
                {
                    "sent": "Rank assigned to the best parse by the syntactic system and with our interpolation function.",
                    "label": 0
                },
                {
                    "sent": "That's going to be one.",
                    "label": 0
                },
                {
                    "sent": "OK, this is going to have the highest rank minus the rank which the semantic system assigns that same parts.",
                    "label": 0
                },
                {
                    "sent": "So basically we're going to see where the semantic system says the parts on the left should be ranked and subtract that.",
                    "label": 0
                },
                {
                    "sent": "Take the absolute number and it gives us some notion of the degree of conflict between these two systems and.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to use.",
                    "label": 0
                },
                {
                    "sent": "Revision happens when you get to the region in green that is had been OK and you realize what you have to do and there's some cost, presumably with having to switch from one interpretation to another.",
                    "label": 0
                },
                {
                    "sent": "And again, you could imagine much more fine grained cost functions, but I think because of the noise, the noisiness, and the probabilities that are generated by our model were actually best.",
                    "label": 0
                },
                {
                    "sent": "If we just go to a very fixed kind of a cost function which says.",
                    "label": 0
                },
                {
                    "sent": "You have a cost of 1 if and only if there's a semantic change.",
                    "label": 0
                },
                {
                    "sent": "Non monotonic semantic change.",
                    "label": 0
                },
                {
                    "sent": "That is, you have to destroy some aspect of the interpretation of the rules that you've assigned, and Furthermore that the revised semantics has a lower probability than the semantics you had before.",
                    "label": 0
                },
                {
                    "sent": "OK. That is, it's only if you have to switch to a semantic interpretation that is worse than what you had for some reason.",
                    "label": 0
                },
                {
                    "sent": "OK, so those are the two functions that I'm going to talk to you about.",
                    "label": 0
                },
                {
                    "sent": "As I say, we explored several others, but these actually perform best on our development set.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how?",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We evaluate it now well.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We looked at a study by.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I Sue Garnsey ET al where they manipulated exactly these kinds of phenomena.",
                    "label": 0
                },
                {
                    "sent": "That is, she looked at NPS.",
                    "label": 0
                },
                {
                    "sent": "Ambiguous sentences with good and bad objects.",
                    "label": 0
                },
                {
                    "sent": "She also did another manipulation, which is kind of nice, namely subcategorization bias of the verb itself.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have good objects, like the critic wrote the book.",
                    "label": 1
                },
                {
                    "sent": "OK, we have that with a direct with a direct object bias verb like wrote an S complement bias verb like argued.",
                    "label": 0
                },
                {
                    "sent": "And then you have bad objects, like the critic wrote or argued the painting OK, and the pattern reading pattern that garnsey gets is shown in the graph in the bottom here.",
                    "label": 0
                },
                {
                    "sent": "So what we see is for the painting.",
                    "label": 1
                },
                {
                    "sent": "So sorry for the book, which is a good object, right?",
                    "label": 1
                },
                {
                    "sent": "We get a low reading time, OK?",
                    "label": 0
                },
                {
                    "sent": "Down the bottom left there where it's for a bad object like painting.",
                    "label": 0
                },
                {
                    "sent": "We get a very elevated reading time, so this is effectively the cost in the sort of conflict right where we are in the ambiguous region, but semantics is biasing us one way or the other, and then we see the cost of revisions showing up when we get to the embedded verb verb phrase right where there's a lower cost.",
                    "label": 0
                },
                {
                    "sent": "In the case of the.",
                    "label": 0
                },
                {
                    "sent": "Bad objects, presumably because we've already done the work of starting to move towards that analysis.",
                    "label": 0
                },
                {
                    "sent": "If not, having are in fact finished it, whereas is a very very high revision cost when we were committed to.",
                    "label": 0
                },
                {
                    "sent": "The direct object or NP complement analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so how does the model do when we take the predictions of the model based on our cost functions and sort of do the usual kinds of scaling and normalizing so that they look nice.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this graph, we get roughly the same pattern of behavior here.",
                    "label": 0
                },
                {
                    "sent": "OK, basically the same kind of interaction on the two regions.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we look at what happens for the the sentence complement bias verbs, again, we've got a fairly nice.",
                    "label": 0
                },
                {
                    "sent": "Fit least qualitatively between our predictions and those.",
                    "label": 0
                },
                {
                    "sent": "Of the currency data.",
                    "label": 0
                },
                {
                    "sent": "OK, we actually.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then did some poo.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean, you're dealing with relatively few numbers of data points, OK?",
                    "label": 0
                },
                {
                    "sent": "Because we don't model each individual items reading times, we model the average reading times OK.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we pull data from 2 MPs studies and I'll report those, and in fact we also looked at a bunch of other experiments as well, namely a couple of so-called NP0 ambiguities.",
                    "label": 0
                },
                {
                    "sent": "That's examples, like while the man walked the dog barked versus, well, man, what a man walked.",
                    "label": 0
                },
                {
                    "sent": "The car drove past or something like that right where the dog versus the car or better, worse, compliments.",
                    "label": 0
                },
                {
                    "sent": "Main clause reduced relative clause ambiguities.",
                    "label": 0
                },
                {
                    "sent": "We looked at a couple of experiments there as well as PP attachment, and I'll just report the pooled correlations.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For these studies, so we pulled the two NPS studies together where we have 10 or 12 data points.",
                    "label": 1
                },
                {
                    "sent": "We get R .688 and when we pull all eight studies together where we have 36 data points, we get our of .7 OK.",
                    "label": 0
                },
                {
                    "sent": "Both significant correlations.",
                    "label": 0
                },
                {
                    "sent": "So again, the model seems to do fairly well at predicting or generating predictions which correlate with the reading time behavior that we see in these eight study.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic models of plausibility provided good account of human judgment finding.",
                    "label": 0
                },
                {
                    "sent": "Crucially, they can distinguish good and bad role fillers and not just rely on the fact that most of the time in natural texture.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good ones.",
                    "label": 0
                },
                {
                    "sent": "And in general this fits nicely into the probabilistic framework where we have wide coverage and good performance probabilistic models, which are both rational on the one hand and experience based on the other, can account for the kinds of general lexical category assignment, syntactic.",
                    "label": 0
                },
                {
                    "sent": "Analysis and disambiguation behaviors.",
                    "label": 0
                },
                {
                    "sent": "And finally, now with this kind of model can also account for thematic role assignment plausibility effects.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I want to shift gears and look at something really quite different which is.",
                    "label": 0
                },
                {
                    "sent": "What about trying to model the immediate context so in some sense what I've talked about so far is modeling assumptions our long-term linguistic experience and knowledge of language up to the semantic level.",
                    "label": 0
                },
                {
                    "sent": "And now I want to look at what how do we exploit information that's in our immediate context.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's a whole range of.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiments in the cycle.",
                    "label": 0
                },
                {
                    "sent": "Linguistic literature since about 1995, which have shown just how close the temporal interaction is between language understanding and visual attention.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, basically what we see are utterance mediated eye movements as people listen to utterances which are talking about related scenes.",
                    "label": 0
                },
                {
                    "sent": "For example, preferential eye movements trigger looks to mention scene entities.",
                    "label": 0
                },
                {
                    "sent": "But what we also see is sort of the compositional interpretation of the unfolding utterance causing anticipatory eye movements to objects which hasn't even been mentioned yet.",
                    "label": 0
                },
                {
                    "sent": "Very subtle factors like international cues in the speech can also modulate this kind of visual attention effects.",
                    "label": 1
                },
                {
                    "sent": "Sort of contrastive stress or stress about which informs us about word order.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, there's a bit of evidence quite a bit, I think.",
                    "label": 1
                },
                {
                    "sent": "Telling us that the scene itself actually influences online language understanding, so it's not just that the tension in the scene is revealing language.",
                    "label": 0
                },
                {
                    "sent": "Understanding process is.",
                    "label": 0
                },
                {
                    "sent": "It's actually fundamentally informing those.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Processes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say a very little bit about German because I'm going to talk about a couple of German experiments here.",
                    "label": 0
                },
                {
                    "sent": "OK, I suspect most people know this.",
                    "label": 0
                },
                {
                    "sent": "I'll go through it very quickly.",
                    "label": 0
                },
                {
                    "sent": "German allows subject, verb, object and object group subject word order in its main cause.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sentences.",
                    "label": 0
                },
                {
                    "sent": "It also has case marking.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I say Dad has a as rabbit, that's nominative.",
                    "label": 0
                },
                {
                    "sent": "But if I say Dean has an, it's an accusative form.",
                    "label": 0
                },
                {
                    "sent": "Therefore likely an object.",
                    "label": 0
                },
                {
                    "sent": "In contrast, feminine and neuter nouns like Princess, in which is a feminine.",
                    "label": 0
                },
                {
                    "sent": "The article doesn't indicate case.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case it's.",
                    "label": 0
                },
                {
                    "sent": "It's ambiguous as to whether or not the Princess.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is nominative or accusative.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do these studies look like?",
                    "label": 0
                },
                {
                    "sent": "Well, people wear a head mounted eye tracker and they look at a very simple clip art visual scene like this.",
                    "label": 0
                },
                {
                    "sent": "And then they hear one of two.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pence is either.",
                    "label": 0
                },
                {
                    "sent": "The rabbit eats soon.",
                    "label": 0
                },
                {
                    "sent": "The cabbage in a subject verb, object, word order, or the rabbit eat soon.",
                    "label": 0
                },
                {
                    "sent": "The Fox in an object, verb subject word order.",
                    "label": 0
                },
                {
                    "sent": "The only difference is the case marking.",
                    "label": 0
                },
                {
                    "sent": "On the 1st noun phrase OK, First version we know we've got a an SVO.",
                    "label": 0
                },
                {
                    "sent": "In the second, we know there's a.",
                    "label": 0
                },
                {
                    "sent": "And OVS.",
                    "label": 0
                },
                {
                    "sent": "And if we look at the.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Movements right after the verb but before they reach the final noun phrase.",
                    "label": 0
                },
                {
                    "sent": "What we see are anticipatory eye movements.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we look at the image that people are staring at, then in the first condition.",
                    "label": 0
                },
                {
                    "sent": "The SVO condition as soon as people have heard the verb eat.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They're already looking at the cabbage OK, because they know rabbits eat cabbage.",
                    "label": 0
                },
                {
                    "sent": "In contrast.",
                    "label": 0
                },
                {
                    "sent": "In the second OVS version, they're already using.",
                    "label": 0
                },
                {
                    "sent": "Case marking information combined with applause instead of the semantics of the verb 2.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Direct more attention.",
                    "label": 0
                },
                {
                    "sent": "Towards the Fox than to the cabbage?",
                    "label": 1
                },
                {
                    "sent": "OK, that is we see an increase in looks away from the cabbage and towards the Fox.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we got a continuous stream of eye movements where you know people first look at the rabbit of course, and then as they hear the verb, they start anticipating either the subject or object of the.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intense.",
                    "label": 0
                },
                {
                    "sent": "We usually just pick out a region and count up the eye.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I fixations in that region.",
                    "label": 0
                },
                {
                    "sent": "We get something like this.",
                    "label": 0
                },
                {
                    "sent": "We actually don't get quite the pattern that we'd ideally like, but we see in the SVO case we get a lot of looks to the patient, which is the cabbage.",
                    "label": 0
                },
                {
                    "sent": "Whereas in the OVS case we gotta drop in those looks, an increase in looks to the Fox, even though it doesn't quite fully override a main effect of looking at the cabbage.",
                    "label": 1
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we then did another study where we said, OK, well, that's nice.",
                    "label": 0
                },
                {
                    "sent": "We can see how people use these case marking cues.",
                    "label": 0
                },
                {
                    "sent": "Right and combine that with compositional sentence interpretation to direct these eye movements in the scene.",
                    "label": 0
                },
                {
                    "sent": "But what if we take away the case marking cues and we put the necessary information to disambiguate the sentence in the scene?",
                    "label": 0
                },
                {
                    "sent": "And what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "Well, here we've got the Princess, and it's ambiguous as to whether or not that's a subject or the object of the sentence.",
                    "label": 0
                },
                {
                    "sent": "But as soon as you hear the verb as either washes or paints, the events in the scene tell you whether or not she's the agent or the patient.",
                    "label": 0
                },
                {
                    "sent": "And so our question was.",
                    "label": 0
                },
                {
                    "sent": "Can people use that information as soon as they hear the verb to workout a?",
                    "label": 0
                },
                {
                    "sent": "What the right word order is for the sentence and be to anticipate the correct role filler and so our expectation is hopefully in the SVO condition.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Immediately after hearing the verb, we should we should see looks over at the pirate.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if the verb is mild, the obvious condition we should see look to the fencer.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And sure enough, that's exactly the pattern of eye movements that we get here, OK?",
                    "label": 0
                },
                {
                    "sent": "So what we see here then is very clear effect of scene information being used immediately.",
                    "label": 0
                },
                {
                    "sent": "The reason these characters in these events are sort of so unusual is that we don't want stereotypical information about who are likely role fillers for this particular verb to be confounding this, so it's exactly the opposite of the first study I talked about, where stereotypical eaters of rabbits versus ease of rabbits or whatever come into play.",
                    "label": 0
                },
                {
                    "sent": "So look at two very different kinds of information.",
                    "label": 0
                },
                {
                    "sent": "Revealing themselves here and in fact.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We decided to see what happens if we picked those two kinds of info.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation sources against each other.",
                    "label": 0
                },
                {
                    "sent": "OK, so we know that people can you sort of stereotypical and selectional constraints.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, we know that rabbits are very likely to cabbage, or we can use depicted event information where we say, well, we can see that the fencers painting the Princess.",
                    "label": 0
                },
                {
                    "sent": "And so we decided to in a single study, workout a weather in a single study.",
                    "label": 0
                },
                {
                    "sent": "People can use both of these kinds of information sources and B.",
                    "label": 0
                },
                {
                    "sent": "What happens when they pull you in opposite directions?",
                    "label": 1
                },
                {
                    "sent": "Do you have a preference for using one information source over?",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other.",
                    "label": 0
                },
                {
                    "sent": "So the stimuli look even more odd.",
                    "label": 0
                },
                {
                    "sent": "OK, these are all object, verb subject sentences.",
                    "label": 0
                },
                {
                    "sent": "OK, so they start with an accusative template and he's the central character and he's the patient of two events.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is the case where either the scene or knowledge tells you who the agent should be.",
                    "label": 0
                },
                {
                    "sent": "So if we say, serves.",
                    "label": 0
                },
                {
                    "sent": "Then the scene tells you that the.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detective is probably the role filler 'cause he's depicted as doing this serving event.",
                    "label": 0
                },
                {
                    "sent": "But if you say.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you hear jinxes, well, there's nobody jinxing in the scene.",
                    "label": 0
                },
                {
                    "sent": "But you know that the Wizard is a very likely agent of this event, based on stereotypical information.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Found here was what you would hope to get, which is.",
                    "label": 0
                },
                {
                    "sent": "Namely people could use either information source.",
                    "label": 0
                },
                {
                    "sent": "OK, that is here they did indeed look more at the detective before they heard it, and here they looked more at the wizard.",
                    "label": 0
                }
            ]
        },
        "clip_139": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The interesting case.",
                    "label": 0
                }
            ]
        },
        "clip_140": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case I'm not.",
                    "label": 0
                },
                {
                    "sent": "That's not interesting, But the interesting cases when we pick these two sources against each other.",
                    "label": 0
                },
                {
                    "sent": "So here we replace those two different verbs with the same verb, namely spies on.",
                    "label": 0
                },
                {
                    "sent": "And in this condition, spies on.",
                    "label": 0
                },
                {
                    "sent": "Is referring to the wizard because he's depicted as doing a spying event?",
                    "label": 0
                },
                {
                    "sent": "OK, whereas in this condition spies on is referring the detective and he's the stereotypical role filler for this kind of an event.",
                    "label": 0
                },
                {
                    "sent": "OK, these stimuli were also fully counterbalanced.",
                    "label": 0
                },
                {
                    "sent": "I can go into the design details if you want to afterwards.",
                    "label": 0
                }
            ]
        },
        "clip_141": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_142": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so when we look at what happens after the verb here, OK, we're not expecting any kind of an interaction.",
                    "label": 0
                },
                {
                    "sent": "It's gotta be gotta be doing the same thing in both sentences, right?",
                    "label": 0
                },
                {
                    "sent": "'cause the sentence is of the same up to that point, the question is.",
                    "label": 0
                }
            ]
        },
        "clip_143": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which way did they go?",
                    "label": 0
                },
                {
                    "sent": "And the answer is they go for the depicted.",
                    "label": 0
                },
                {
                    "sent": "OK, that is when they hear spies on, we get more looks to the wizard than to the detective here.",
                    "label": 0
                },
                {
                    "sent": "So this seems to be a preference here for relying on the information which is in some sense staring you in the face versus your general knowledge about.",
                    "label": 0
                }
            ]
        },
        "clip_144": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who's likely to spy on people?",
                    "label": 0
                }
            ]
        },
        "clip_145": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what do we have here?",
                    "label": 0
                },
                {
                    "sent": "Well, the rapid use of linguistic and scene information we know can guide interpretation and direct visual.",
                    "label": 0
                }
            ]
        },
        "clip_146": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Furthermore, this anticipation anticipation of role fillers seems to preferentially rely on depicted events over stored knowledge, at least in this.",
                    "label": 0
                }
            ]
        },
        "clip_147": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Setting OK. And.",
                    "label": 0
                },
                {
                    "sent": "Roughly what this actually I think I would skip to the next.",
                    "label": 0
                }
            ]
        },
        "clip_148": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "5.",
                    "label": 0
                },
                {
                    "sent": "The picture that we think this sketch.",
                    "label": 0
                }
            ]
        },
        "clip_149": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one looks like this.",
                    "label": 0
                },
                {
                    "sent": "You have the utterance.",
                    "label": 0
                }
            ]
        },
        "clip_150": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically driving attention.",
                    "label": 0
                }
            ]
        },
        "clip_151": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Individual scene information in the scene.",
                    "label": 0
                }
            ]
        },
        "clip_152": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then informing utterance comprehension.",
                    "label": 0
                },
                {
                    "sent": "So during utterance processing we're getting incremental interpretation based on all kinds of linguistic constraints, and also forward inferencing, that is, generation of expectations about what's likely to be mentioned next and this interpretation.",
                    "label": 0
                }
            ]
        },
        "clip_153": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Drives your attention towards relevant objects in the scene, either referentially, that is, you look at objects which have been mentioned.",
                    "label": 0
                },
                {
                    "sent": "If you hear Princess, you look at the Princess, but also these anticipatory eye movements.",
                    "label": 0
                },
                {
                    "sent": "That is, if you have forward inference, is telling you you're probably going to hear the fencer next.",
                    "label": 0
                },
                {
                    "sent": "Then you start looking at the fencer before it's even mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_154": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. And Furthermore, if you have reference to something like an event, then by directing your attention to the event in the scene, you can extract the propositional information.",
                    "label": 0
                },
                {
                    "sent": "That's concerned with that event and use that to actually inform and possibly even revise the interpretation of the utterance that you've built so far.",
                    "label": 0
                },
                {
                    "sent": "OK, and one reason we think that you might see this priority of relying on what's in the immediate scene, and this is very speculative.",
                    "label": 0
                }
            ]
        },
        "clip_155": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over your stored knowledge is that it could have some sort of.",
                    "label": 0
                }
            ]
        },
        "clip_156": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basis in language acquisition.",
                    "label": 0
                },
                {
                    "sent": "In some sense, it could be a holdover strategy from language acquisition where we're trying to constantly ground what we hear in terms of nouns and events with what's going on in the scene around us.",
                    "label": 0
                },
                {
                    "sent": "And maybe this somehow just sticks later in life, even though presumably we're not seeing such unusual or unseen kinds of events with respect to our language anymore, OK?",
                    "label": 0
                }
            ]
        },
        "clip_157": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "5 minutes to say something.",
                    "label": 0
                }
            ]
        },
        "clip_158": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we might model this so to deal with the modeling?",
                    "label": 0
                },
                {
                    "sent": "What we need is some kind of a of an architecture which can integrate two or sort of multiple information sources simultaneously, which on the one hand is also experienced based so they can learn stereotypical role fillers.",
                    "label": 0
                },
                {
                    "sent": "But as I say can also learn to adapt to some sort of immediate context if it's there.",
                    "label": 0
                }
            ]
        },
        "clip_159": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, one might use probabilistic models here perfectly reasonable.",
                    "label": 0
                },
                {
                    "sent": "We wanted to avoid that because we wanted to avoid having to stipulate too much about how you combine these different information sources.",
                    "label": 0
                },
                {
                    "sent": "We were afraid that with probabilistic account, we'd have to really be giving that away.",
                    "label": 0
                },
                {
                    "sent": "So we rather dis.",
                    "label": 0
                }
            ]
        },
        "clip_160": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do is build a simple feedforward simple recurrent network.",
                    "label": 0
                }
            ]
        },
        "clip_161": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, expand it a little.",
                    "label": 0
                },
                {
                    "sent": "OK, so simple recurrent networks look like this.",
                    "label": 0
                },
                {
                    "sent": "I think I'm going to assume most people have seen this before.",
                    "label": 0
                },
                {
                    "sent": "Words are presented one at a time.",
                    "label": 0
                },
                {
                    "sent": "They get passed through a hidden unit layer to reveal some sort of an interpretation or produce some sort of interpretation, and that hidden layer at each time step gets copied back some context layer, which can then also inform processing at the next word.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what's used to sort of build up some representation of the sentence overtime.",
                    "label": 0
                }
            ]
        },
        "clip_162": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the architecture that we build called CIA.",
                    "label": 0
                },
                {
                    "sent": "Net for the coordinated interplay account.",
                    "label": 0
                },
                {
                    "sent": "What we did is we started off with.",
                    "label": 0
                },
                {
                    "sent": "A simple recurrent network architecture like this.",
                    "label": 0
                }
            ]
        },
        "clip_163": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And we added to it some sort of a representation of the scene.",
                    "label": 0
                },
                {
                    "sent": "In fact, we did quite a number of simulations using just this architecture, but in the most recent one.",
                    "label": 0
                }
            ]
        },
        "clip_164": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also have.",
                    "label": 0
                },
                {
                    "sent": "And attentional gating vector here?",
                    "label": 1
                },
                {
                    "sent": "And what this does is it basically masks the events that are in the visual scene and directs attention to one event over another OK?",
                    "label": 1
                },
                {
                    "sent": "Some sense amplifying the importance of that event for comprehension.",
                    "label": 0
                }
            ]
        },
        "clip_165": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the goal here, then, is to model a whole bunch of things right.",
                    "label": 0
                },
                {
                    "sent": "We're trying to model experience, that is, make sure it learns things like stereotypical role fillers, model the immediate use of scene information if it's available, work perfectly well if the scene is not there, and construct an interpretation of the sentence in the absence of the scene model, the priority of the scene if stereotypical knowledge and seen information conflict with each other.",
                    "label": 1
                },
                {
                    "sent": "And of course model the general kinds of anticipatory behavior.",
                    "label": 0
                }
            ]
        },
        "clip_166": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We see in the experiments.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_167": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slightly more detailed look at the architecture.",
                    "label": 0
                },
                {
                    "sent": "More colorful is.",
                    "label": 0
                }
            ]
        },
        "clip_168": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Again, simply the simple recurrent network architecture.",
                    "label": 0
                },
                {
                    "sent": "Here we have a representation of the scene at the top.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_169": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What you see in the scene at the top sort of corresponds to this image here, right?",
                    "label": 0
                },
                {
                    "sent": "We have a wizard spying on a pilot, and then we have a detective serving the pilot, and then we have this gating vector, which is in some sense telling us.",
                    "label": 0
                },
                {
                    "sent": "Gets multiplied elementwise to the left of.",
                    "label": 0
                },
                {
                    "sent": "Enter the right event and effectively, then shifts attention from one to the other.",
                    "label": 0
                }
            ]
        },
        "clip_170": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_171": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is trained with back prop through time.",
                    "label": 0
                },
                {
                    "sent": "A few details.",
                    "label": 0
                },
                {
                    "sent": "The two events actually get fed into the hidden layer through shared weights.",
                    "label": 0
                },
                {
                    "sent": "That avoids it sort of relying on positional information in the network to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "It's trained in a supervised manner on the interpretation that is during training we tell it what the correct output interpretation should be.",
                    "label": 0
                },
                {
                    "sent": "This sort of case role representation of the meaning, but the attentional vector is unsupervised, so it has to learn how to adapt that attentional vector to optimize getting the right interpretation.",
                    "label": 0
                }
            ]
        },
        "clip_172": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_173": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of the training data, what we did is.",
                    "label": 0
                }
            ]
        },
        "clip_174": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Constructed grammar which effectively covers the experimental stimuli from two of the experiments that I presented, the depicted events study and then this other study where we have compared stereotypical knowledge versus event depicted knowledge.",
                    "label": 0
                },
                {
                    "sent": "The idea was to come up with a single grammar and lexicon which sort of covered both of these so that we weren't, in some sense, fitting independently to the two experimental datasets.",
                    "label": 0
                },
                {
                    "sent": "We also kept out exactly those conflicting examples.",
                    "label": 0
                },
                {
                    "sent": "That is, examples where the verb.",
                    "label": 0
                },
                {
                    "sent": "On the one hand identified a stereotypical role filler, but at the same time a role filler that was depicted was depicted OK. 'cause what we want to do is see if we can actually predict this preference to rely on the depicted event information, and so we don't explicitly train on any of those kinds of instances.",
                    "label": 0
                },
                {
                    "sent": "It never sees a conflict case during training.",
                    "label": 0
                }
            ]
        },
        "clip_175": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We gave it a scene as context in about 50% of the cases.",
                    "label": 0
                },
                {
                    "sent": "I think in something like 5% of the cases the scene was totally irrelevant to what was being said in the sentence as well.",
                    "label": 0
                },
                {
                    "sent": "So a little bit of noise.",
                    "label": 0
                }
            ]
        },
        "clip_176": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There as well.",
                    "label": 0
                },
                {
                    "sent": "I should say in terms of the overall performance, the network does basically perfectly, that is, in the sense that at the end of a sentence it always has the right.",
                    "label": 0
                },
                {
                    "sent": "Meaning meaning representation.",
                    "label": 0
                },
                {
                    "sent": "What we're interested in evaluating here is what's it doing during processing.",
                    "label": 0
                },
                {
                    "sent": "That is, does it have the right anticipatory behaviors during processing and so?",
                    "label": 0
                }
            ]
        },
        "clip_177": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a look at the attentional vector.",
                    "label": 0
                },
                {
                    "sent": "OK the gating vector.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Red and blue are the unambiguous cases.",
                    "label": 0
                },
                {
                    "sent": "OK, So what this is showing is that as we move through the sentence, if the verb.",
                    "label": 0
                },
                {
                    "sent": "Unambiguously identifies the stereotypical agent.",
                    "label": 0
                },
                {
                    "sent": "Then we see attention going upwards, which represents a shift in attention towards the stereotypical event.",
                    "label": 0
                },
                {
                    "sent": "If, on the other hand, it unambiguously identifies a depicted agent, that's the blue line, then attention shifts downwards and that represents attention towards the depicted role filler.",
                    "label": 0
                },
                {
                    "sent": "OK. Just in case, is the pink in the green lines here and there?",
                    "label": 0
                },
                {
                    "sent": "What we're seeing is as soon as the verb gets reached.",
                    "label": 0
                },
                {
                    "sent": "We're seeing a shift in attention to the scene, so this is the conflict conditions and we see that right away at the verb.",
                    "label": 0
                },
                {
                    "sent": "It's predicting this shift in attention towards the depicted as opposed to the stereotypical OK. Of course, that persists a little.",
                    "label": 0
                },
                {
                    "sent": "And of course, finally, when it here is the actual disambiguating down at the end, which says, I'm either the depicted one or on the stereotypical one, it gets the right.",
                    "label": 0
                },
                {
                    "sent": "Shifting attention there.",
                    "label": 0
                }
            ]
        },
        "clip_178": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing we can look at is actually what role?",
                    "label": 0
                }
            ]
        },
        "clip_179": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Filler is the network predicting in its output, meaning representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the normalized Euclidean of the output, subject to the actual depicted.",
                    "label": 1
                },
                {
                    "sent": "Agent, whether that's the stereotypical one or the one that's involved in the event, OK. Um?",
                    "label": 0
                },
                {
                    "sent": "And here again, what you see is at the verb.",
                    "label": 0
                },
                {
                    "sent": "You actually get an interesting effect here, which is.",
                    "label": 0
                },
                {
                    "sent": "And across the board preference at the verb to actually predict the stereotypical stereotypical character.",
                    "label": 0
                },
                {
                    "sent": "The reason that happens at the verb is because the attention gaming vector hasn't yet had a chance to modulate the event.",
                    "label": 0
                },
                {
                    "sent": "That can only happen at the subsequent at the next time step.",
                    "label": 0
                },
                {
                    "sent": "OK, so this actually makes a prediction that.",
                    "label": 0
                },
                {
                    "sent": "We could in theory look at experimentally, which says that immediately at the verb you've actually got a biased towards predicting a stereotypical role filler.",
                    "label": 0
                },
                {
                    "sent": "But as soon as you sort of use your visual attention to examine the relevant part of the scene that flips and you actually have a preference too.",
                    "label": 0
                },
                {
                    "sent": "Fill the role with the depicted agent OK, which you see, then showing up on the adverb.",
                    "label": 0
                },
                {
                    "sent": "OK, where the blue?",
                    "label": 0
                },
                {
                    "sent": "The biggest case is red and blue aren't interesting, but the pink and the.",
                    "label": 1
                },
                {
                    "sent": "And the green you see a slight bias at the verb to tip towards.",
                    "label": 0
                },
                {
                    "sent": "Predicting the depicted role filler.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 1
                },
                {
                    "sent": "It seems to capture basically all the phenomena that we're interested in, in addition to predicting.",
                    "label": 0
                },
                {
                    "sent": "Without ever having been trained on it, this bias towards using.",
                    "label": 0
                },
                {
                    "sent": "Depicted event role fillers over stereotypical role fillers.",
                    "label": 0
                }
            ]
        },
        "clip_180": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When the two conflict.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_181": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "20",
                    "label": 0
                }
            ]
        },
        "clip_182": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Say this.",
                    "label": 0
                }
            ]
        },
        "clip_183": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't think so.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are some?",
                    "label": 0
                }
            ]
        },
        "clip_184": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "General comments or directions so I.",
                    "label": 0
                },
                {
                    "sent": "What I'm generally trying to sell here is the idea that models of language processing and ultimately language acquisition really need to be situated.",
                    "label": 0
                },
                {
                    "sent": "That is, we can only go so far by by sort of trying to circumscribe all of those problems and investigate them.",
                    "label": 0
                },
                {
                    "sent": "While ignoring all the systems to which there intercont.",
                    "label": 0
                }
            ]
        },
        "clip_185": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Acted in intertwined in the case of sentence processing, we need to do this looking beyond simple lexical and syntactic processing.",
                    "label": 0
                },
                {
                    "sent": "In terms of the challenges well?",
                    "label": 0
                },
                {
                    "sent": "Clearly the kinds of machine learning methods I've talked about here are very simple off the shelf kinds of machine learning techniques.",
                    "label": 0
                },
                {
                    "sent": "I have no doubt that there are better, better models for learning semantic classes and modeling semantic model, possibly even unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Techniques and more cognitive plausible, cognitively plausible kinds of supervised techniques.",
                    "label": 0
                },
                {
                    "sent": "Challenges in terms of connecting our models with grounding in the environment.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is already come up in Michael Franks talk earlier on.",
                    "label": 0
                },
                {
                    "sent": "Building models which are better grounded in terms of theories of acquisition, situated acquisition and also treating the environment not just as something as a separate notion, but something which is fundamentally part of what it means to understand the sentence that is by grounding and interpretation with what's in the world around us.",
                    "label": 0
                },
                {
                    "sent": "We don't just do normal interpretation, it's in some sense and enriched notion of comprehension because it's augmented by the actual properties of what's going on in the invisible in the visual environment.",
                    "label": 0
                },
                {
                    "sent": "A huge challenge I think as we try to integrate these multiple kinds of cues and information sources from the environment.",
                    "label": 0
                },
                {
                    "sent": "Again, it came up in Michael Franks talk is the real time integration of these kinds of multimodal informational cues.",
                    "label": 0
                },
                {
                    "sent": "That is, how do we take speech information scene information, gesture, joint attention and so on, and combine all of these in some sort of real time way such that we can identify the correlations which are necessary to do the kinds of machine learning that we need.",
                    "label": 0
                },
                {
                    "sent": "I'll stop there.",
                    "label": 0
                }
            ]
        }
    }
}