{
    "id": "u3kod2kqutkyjb2pagqantv5n6yyij7i",
    "title": "Tensor principal component analysis",
    "info": {
        "author": [
            "David Steurer, Department of Computer Science, Cornell University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_steurer_component_analysis/",
    "segmentation": [
        [
            "This joint work with Sam Hopkins and Jonathan Jonathan she at Cornell."
        ],
        [
            "OK, so.",
            "Principal component analysis is very basic tool in analyzing data.",
            "Suppose you have some some matrix where we imagine that the entries contain maybe noisy correlations between between two variables and and what we want to do is we want to find a linear combination of the variables that that is sort of a most interesting that maximizes variance and so so it just boils down to maximizing this quadratic form overall unit vectors.",
            "So so so.",
            "One reason why this is so so popular and so useful is because you can.",
            "You can solve this problem efficiently, because the optimal solution is just the top eigenvector of a.",
            "And however, in in many important applications, it turns out that sort of slight variance of this principle of this basic principle component analysis would would give better statistical guarantees.",
            "And sort of popular variants are very, you know, restrict that are the kind of directions that you care about too sparse directions so this a little bit related to to sparse coding as we heard about, so it's usually called sparse PCA.",
            "Another variant, which is the one that we'll study here, is that.",
            "Imagine that you don't have just pairwise correlations information about pairwise correlations, but you also have informations about higher order correlations.",
            "And then you you know you.",
            "You could hope that you you can exploit.",
            "Those higher order correlations and incorporate those in order to find interesting directions in your data.",
            "Um, but sort of 1.",
            "One thing all of these variants share is that they become computationally very challenging, so in particular you know the like.",
            "In the worst case, you know they become NP hard and hard to approximate, but but what's really?",
            "You know, the the real question is how?",
            "What's the computational complexity in the stochastic setting where you know because we care about its statistical guarantees and and there are in many cases the complexity is just unclear.",
            "And so that's the general context of the kind of problems that we're dealing with."
        ],
        [
            "The concrete problem that we look at is model for a stochastic model.",
            "For tensor principal component analysis proposed by Montanari and Richard.",
            "So this is a variant of PCA where you try to exploit higher order correlations.",
            "And so this in this model, you are given a kaitens or a.",
            "And that has particular is generated in a particular way, so it's generated by.",
            "Combining.",
            "A signal term which is just a rank one tensor for some unit vector V. And and this is combined with with some noise and the noise here will just be.",
            "A tensor where each entry is independent, standard color, standard Gaussian.",
            "And then the you know they're combined with some with some weight, which which is the signal to noise ratio.",
            "So you know the more weight you give.",
            "Is the larger towel.",
            "This parameter is the more visible the signal is and the question is for what values of Tau and K are you able to recover?",
            "The signal V. And the question is posted for information theoretic point.",
            "When the fuel is also computationally so.",
            "This is the very stylized model for this for this problem.",
            "But it turns out to be quite interesting.",
            "And.",
            "So it's also instructive to look at what's the maximum likelihood estimation problem for.",
            "For this.",
            "For this question, and this turns out to be just to maximize this homogeneous degree K polynomial.",
            "Overall, all unit vectors X very analogous to.",
            "Two to the Center PC, a Yaqui.",
            "So for K = 2 This is exactly the the user problem that we have to solve for for PCA.",
            "And in that case, you know it's computationally efficient.",
            "Just some eigenvalue computation.",
            "And but for larger K, this becomes NP hard even for and.",
            "In particular the case K = 2 three, it seems to capture already the the difficulty of the problem and.",
            "In particular, I know it's NP hard for this, but as I said, this doesn't mean anything for for the stochastic model that we will look at here.",
            "And so in this talk we will focus on the case cake with three.",
            "We try to understand what happens and just explaining this picture here.",
            "So this is a three tensor, so you know 3 dimensional.",
            "Array of of Just of numbers and interior.",
            "This is a picture how we hope that the solution lens that the landscape of this optimization problem looks like somehow you know you have to seek a polynomial and you look at it.",
            "Overall unit over the unit sphere, Euclidean unit sphere and you hope that maybe the function looks like this and this spike here is.",
            "Tells you the location of this Spike tells you, gives you a vector that is close to me.",
            "OK, so that is the list of hope.",
            "But of course solving this problem is is is is is.",
            "That's how to solve this problem efficiently as that's the main question."
        ],
        [
            "OK, So what are the previous results?",
            "So the results are by montinari enrich other couple a couple of years ago.",
            "So first you know the the first question is when is it information theoretically possible to recover vector close too close to the signal?",
            "And this turns out to be.",
            "For for, for, for you know it's possible as long as so if you if you solve this maximum likelihood estimation problem, then you recover a vector close to the signal as long as the signal to noise ratio is at least ruden.",
            "Maybe it's instructive to see this briefly, so for every unit vector X, if you look at the contribution.",
            "Of the Z term to this polynomial, it's standard Gaussian.",
            "OK, so so so.",
            "So there's sort of noise at every point X.",
            "The noise contributes a standard Gaussian to the functions you're trying to maximize, and now we are trying to maximize this function.",
            "Overall unit vectors, and if you discretize this fear, you have two to the end points.",
            "So you want that for all of these two to the endpoints.",
            "The contribution of the center.",
            "Of this center Gaussians should be less than a towel, and you know for if you choose to to be root N, then you know it means that the standard they stand Gaussian would have to be root N deviations away from away from its mean, and that probability is is 2 to the minus N which which allows you to do the Union bound here so so this routine is actually interesting thing about the routine is that it doesn't depend on the order of the tensor at all, so it's the same routine.",
            "Right answer for all values of.",
            "All orders of tenders.",
            "But the question that sort of now the main question is when can you do this when you can do recovery efficiently and want to now in Richard they proposed a very simple algorithm and so the algorithm is just to look at the three tensor and think of it and look at sort of the.",
            "Slices think of it as a collection of N matrices, each of dimension N by N, and then arrange those in matrices.",
            "AN squared is 1 big N squared by N matrix and so so just reshape the tensor is a matrix and now you can look at the top right singular value of of this vector, and that's the algorithm that they proposed to find the signal.",
            "It turns out that they were able to show that for non trivial signal to noise ratio, this this algorithm indeed recovers vector close to the signal.",
            "And then so they notice something interesting in the notice.",
            "Is that actually the empirical performance?",
            "If you actually run this algorithm on on instances generated in this way, it turns out that they that.",
            "This is significantly smaller noise signal to noise ratio is enough for the algorithm to work, and so there's this gap between the theoretical analysis and empirical performance.",
            "And usually you know there are many reasons why such a gap can happen.",
            "You know either you know you don't run the algorithm on the algorithm that you analyze.",
            "You know you do.",
            "Some worst case analysis, but then you don't run the algorithm on real life instances.",
            "But actually here this is not the case.",
            "You know we analyzed the algorithm exactly on the kind of instances that we actually run the algorithm.",
            "Another reason why there could be this kind of gap is that.",
            "Some of the topics on kicking OK, but actually this is also not the case, so you can run these algorithms for huge values of N, so so that's you know that's very unlikely to be an issue.",
            "You know.",
            "Finally, should be that the analysis the theoretical analysis not typed, but also this.",
            "In many ways, it's not.",
            "It's not the correct answer, so so turns out that the the analysis is tight in in many ways, and so there's something Mr Mysterious going on and we will sort of get back to this mystery in after I show you after I tell you about.",
            "The results of this work.",
            "Um?"
        ],
        [
            "So, So what we do is we we.",
            "We study.",
            "We studied this.",
            "So sarcastic model forward pencil PCA in from the point of view of a very general algorithmic framework called the sum of squares method.",
            "This is this has been.",
            "Discovered by shore and further developed by Perillo and Losier.",
            "In the 2000s and and so this method has been studied in in many.",
            "Scientific disciplines also has been studied for approximation algorithms quite a lot, and in the last couple of years it turned out to be very powerful approach to unsupervised learning, both as a way to get new kind of algorithms, but also as a way to understand limitations of algorithms.",
            "In the resources in service, at least, this one work in this at least one more work in this conference about the software's algorithm.",
            "And and what we show is that indeed, you know algorithms based on this method allow you to close the gap between the theoretical analysis and empirical performance that montinari in Richard observed.",
            "So you can get the algorithms that work for this kind of signal to noise ratio.",
            "And indeed, the our analysis also allows us to show that sort of a slight variant of the machinery in Richard algorithm is enough to detest.",
            "Has this kind of this kind of theoretical guarantees.",
            "Um?",
            "Villa.",
            "You're sort of asking questions, so some of the algorithms that we study here they're quite slow, and so it's interesting to ask what how fast can you actually solve this problem?",
            "And it turns out that you can use knowledge about the eigen values that come from the theoretical analysis in order to speed up.",
            "Some of the algorithms quite significantly so that they run in nearly linear time.",
            "Which is the best that you can hope for in this context.",
            "Finally, we also.",
            "Study lower bounds for this problem.",
            "In particular, we show that.",
            "It's not possible to get better recovery guarantees so so this signal to noise ratio is optimal as long as.",
            "For for certain, as long as you algorithm is based on a certain broad set of techniques in this process of techniques is.",
            "Captured by this decree 4 summer squares proof system.",
            "If you.",
            "If you've heard about that.",
            "OK, so finally."
        ],
        [
            "Let's resolve this mystery that I mentioned before.",
            "So so this is our general approach to to try to understand these kind of difficult optimization problems and the approaches.",
            "Relaxation and grounding.",
            "So relaxation is A is a tractable optimization problem.",
            "This associated with let's say so this this this you know, a priority intractable optimization problem in this direction should give you some upper bound on the optimal value here.",
            "And together with the realization that there's usually a rounding and the rounding, what it doesn't do is allows you to transform a solution.",
            "For the relaxed optimization problem into a solution to the original optimization problem without changing the objective value by too much, so that's the the kind of thing that you usually try to have, and it turns out that this this this approach it fails for in the context of the relaxation that want to now in Richard study.",
            "So in the in the in the critical parameter regime, it turns out that the optimal value of their relaxation is very far from the optimal value of the optimization problem.",
            "That the.",
            "Of this Emily optimization problem, however, it's still possible to extract to extract, and if you look at an optimal solution to their relaxation, it's still possible at this empirically to extract an optimal solution, and so this you know this kind of discrepancy, it means that there's no rounding in this sense possible.",
            "No, because the you know there's a huge integrality gap, so this discrepancy between the values is usually called integrality gap, but it's still possible to to recover the solution.",
            "And so this is a.",
            "Require some explanation and and it turns out that the the explanation is that there are some 2nd order effect in the optimal value that.",
            "Cloud enables their recovery in the sense that if you mention solving this relaxation, then there are some solutions where whose value is pretty close to the optimal value, but they don't give any information about the the signal.",
            "But then if you if you if you start.",
            "Sort of, you know, solving the relaxation up to a second order term, then suddenly the optimal solutions there they will become close to the actual solution that you care about.",
            "And in this situation is very troublesome because somehow we know if this would be a common phenomenon.",
            "We would have to reconsider a lot of our, you know, thinking about relaxations because usually we you know if you have some integrality gap for relaxation, we.",
            "We consider that relaxation and try to look for other relaxations, but now if somehow it's you know if it's like a common phenomenon, that second order effects in relaxations allow you to get to still get algorithms that would be somehow troublesome and but it turns out that."
        ],
        [
            "You can resolve this issue a little bit here, and it turns out that if you consider a stronger relaxations, regulations that come from this summer squares method then this second order effect you know translates to a first order effect and there you don't have to worry about.",
            "The 2nd order effects.",
            "So let me so this is a technical description of how this squares algorithm works.",
            "It's fairly simple, but let me, but it's a bit technical.",
            "Let me skip it here.",
            "And let me.",
            "Um?",
            "Come to the come to the conclusion."
        ],
        [
            "So.",
            "So so so I didn't explain it but but I I want to, but I think that's an important message that.",
            "In in, in this in this in this context.",
            "The general line of work and the message is that the sum of squares algorithm it.",
            "It gives a new way of thinking about spectral algorithms in the following sense.",
            "So usually you have spectral algorithm.",
            "There is a single matrix that is naturally associated with your problem.",
            "Either you know it's your input matrix.",
            "If you have, like for PCA or its graph Laplacian.",
            "If you have a graph problem.",
            "In both the sum of squares method.",
            "Gives you it's it.",
            "It allows you to associate a whole hierarchy of increasingly more interesting families of matrices with a single problem.",
            "And then the question is, you know, can you and I think there's a lot of opportunities to exploit this richer, richer families of matrices in order to get better algorithms?",
            "And you have some complete open questions.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This joint work with Sam Hopkins and Jonathan Jonathan she at Cornell.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Principal component analysis is very basic tool in analyzing data.",
                    "label": 1
                },
                {
                    "sent": "Suppose you have some some matrix where we imagine that the entries contain maybe noisy correlations between between two variables and and what we want to do is we want to find a linear combination of the variables that that is sort of a most interesting that maximizes variance and so so it just boils down to maximizing this quadratic form overall unit vectors.",
                    "label": 0
                },
                {
                    "sent": "So so so.",
                    "label": 0
                },
                {
                    "sent": "One reason why this is so so popular and so useful is because you can.",
                    "label": 0
                },
                {
                    "sent": "You can solve this problem efficiently, because the optimal solution is just the top eigenvector of a.",
                    "label": 1
                },
                {
                    "sent": "And however, in in many important applications, it turns out that sort of slight variance of this principle of this basic principle component analysis would would give better statistical guarantees.",
                    "label": 1
                },
                {
                    "sent": "And sort of popular variants are very, you know, restrict that are the kind of directions that you care about too sparse directions so this a little bit related to to sparse coding as we heard about, so it's usually called sparse PCA.",
                    "label": 0
                },
                {
                    "sent": "Another variant, which is the one that we'll study here, is that.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you don't have just pairwise correlations information about pairwise correlations, but you also have informations about higher order correlations.",
                    "label": 0
                },
                {
                    "sent": "And then you you know you.",
                    "label": 0
                },
                {
                    "sent": "You could hope that you you can exploit.",
                    "label": 0
                },
                {
                    "sent": "Those higher order correlations and incorporate those in order to find interesting directions in your data.",
                    "label": 0
                },
                {
                    "sent": "Um, but sort of 1.",
                    "label": 0
                },
                {
                    "sent": "One thing all of these variants share is that they become computationally very challenging, so in particular you know the like.",
                    "label": 1
                },
                {
                    "sent": "In the worst case, you know they become NP hard and hard to approximate, but but what's really?",
                    "label": 0
                },
                {
                    "sent": "You know, the the real question is how?",
                    "label": 0
                },
                {
                    "sent": "What's the computational complexity in the stochastic setting where you know because we care about its statistical guarantees and and there are in many cases the complexity is just unclear.",
                    "label": 0
                },
                {
                    "sent": "And so that's the general context of the kind of problems that we're dealing with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The concrete problem that we look at is model for a stochastic model.",
                    "label": 1
                },
                {
                    "sent": "For tensor principal component analysis proposed by Montanari and Richard.",
                    "label": 1
                },
                {
                    "sent": "So this is a variant of PCA where you try to exploit higher order correlations.",
                    "label": 0
                },
                {
                    "sent": "And so this in this model, you are given a kaitens or a.",
                    "label": 0
                },
                {
                    "sent": "And that has particular is generated in a particular way, so it's generated by.",
                    "label": 0
                },
                {
                    "sent": "Combining.",
                    "label": 0
                },
                {
                    "sent": "A signal term which is just a rank one tensor for some unit vector V. And and this is combined with with some noise and the noise here will just be.",
                    "label": 0
                },
                {
                    "sent": "A tensor where each entry is independent, standard color, standard Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And then the you know they're combined with some with some weight, which which is the signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "So you know the more weight you give.",
                    "label": 0
                },
                {
                    "sent": "Is the larger towel.",
                    "label": 0
                },
                {
                    "sent": "This parameter is the more visible the signal is and the question is for what values of Tau and K are you able to recover?",
                    "label": 0
                },
                {
                    "sent": "The signal V. And the question is posted for information theoretic point.",
                    "label": 0
                },
                {
                    "sent": "When the fuel is also computationally so.",
                    "label": 0
                },
                {
                    "sent": "This is the very stylized model for this for this problem.",
                    "label": 0
                },
                {
                    "sent": "But it turns out to be quite interesting.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "So it's also instructive to look at what's the maximum likelihood estimation problem for.",
                    "label": 0
                },
                {
                    "sent": "For this.",
                    "label": 0
                },
                {
                    "sent": "For this question, and this turns out to be just to maximize this homogeneous degree K polynomial.",
                    "label": 1
                },
                {
                    "sent": "Overall, all unit vectors X very analogous to.",
                    "label": 1
                },
                {
                    "sent": "Two to the Center PC, a Yaqui.",
                    "label": 0
                },
                {
                    "sent": "So for K = 2 This is exactly the the user problem that we have to solve for for PCA.",
                    "label": 0
                },
                {
                    "sent": "And in that case, you know it's computationally efficient.",
                    "label": 1
                },
                {
                    "sent": "Just some eigenvalue computation.",
                    "label": 0
                },
                {
                    "sent": "And but for larger K, this becomes NP hard even for and.",
                    "label": 0
                },
                {
                    "sent": "In particular the case K = 2 three, it seems to capture already the the difficulty of the problem and.",
                    "label": 0
                },
                {
                    "sent": "In particular, I know it's NP hard for this, but as I said, this doesn't mean anything for for the stochastic model that we will look at here.",
                    "label": 0
                },
                {
                    "sent": "And so in this talk we will focus on the case cake with three.",
                    "label": 0
                },
                {
                    "sent": "We try to understand what happens and just explaining this picture here.",
                    "label": 0
                },
                {
                    "sent": "So this is a three tensor, so you know 3 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Array of of Just of numbers and interior.",
                    "label": 0
                },
                {
                    "sent": "This is a picture how we hope that the solution lens that the landscape of this optimization problem looks like somehow you know you have to seek a polynomial and you look at it.",
                    "label": 0
                },
                {
                    "sent": "Overall unit over the unit sphere, Euclidean unit sphere and you hope that maybe the function looks like this and this spike here is.",
                    "label": 0
                },
                {
                    "sent": "Tells you the location of this Spike tells you, gives you a vector that is close to me.",
                    "label": 0
                },
                {
                    "sent": "OK, so that is the list of hope.",
                    "label": 0
                },
                {
                    "sent": "But of course solving this problem is is is is is.",
                    "label": 0
                },
                {
                    "sent": "That's how to solve this problem efficiently as that's the main question.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what are the previous results?",
                    "label": 1
                },
                {
                    "sent": "So the results are by montinari enrich other couple a couple of years ago.",
                    "label": 0
                },
                {
                    "sent": "So first you know the the first question is when is it information theoretically possible to recover vector close too close to the signal?",
                    "label": 0
                },
                {
                    "sent": "And this turns out to be.",
                    "label": 0
                },
                {
                    "sent": "For for, for, for you know it's possible as long as so if you if you solve this maximum likelihood estimation problem, then you recover a vector close to the signal as long as the signal to noise ratio is at least ruden.",
                    "label": 1
                },
                {
                    "sent": "Maybe it's instructive to see this briefly, so for every unit vector X, if you look at the contribution.",
                    "label": 0
                },
                {
                    "sent": "Of the Z term to this polynomial, it's standard Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, so so so.",
                    "label": 0
                },
                {
                    "sent": "So there's sort of noise at every point X.",
                    "label": 1
                },
                {
                    "sent": "The noise contributes a standard Gaussian to the functions you're trying to maximize, and now we are trying to maximize this function.",
                    "label": 0
                },
                {
                    "sent": "Overall unit vectors, and if you discretize this fear, you have two to the end points.",
                    "label": 0
                },
                {
                    "sent": "So you want that for all of these two to the endpoints.",
                    "label": 0
                },
                {
                    "sent": "The contribution of the center.",
                    "label": 0
                },
                {
                    "sent": "Of this center Gaussians should be less than a towel, and you know for if you choose to to be root N, then you know it means that the standard they stand Gaussian would have to be root N deviations away from away from its mean, and that probability is is 2 to the minus N which which allows you to do the Union bound here so so this routine is actually interesting thing about the routine is that it doesn't depend on the order of the tensor at all, so it's the same routine.",
                    "label": 0
                },
                {
                    "sent": "Right answer for all values of.",
                    "label": 0
                },
                {
                    "sent": "All orders of tenders.",
                    "label": 0
                },
                {
                    "sent": "But the question that sort of now the main question is when can you do this when you can do recovery efficiently and want to now in Richard they proposed a very simple algorithm and so the algorithm is just to look at the three tensor and think of it and look at sort of the.",
                    "label": 0
                },
                {
                    "sent": "Slices think of it as a collection of N matrices, each of dimension N by N, and then arrange those in matrices.",
                    "label": 0
                },
                {
                    "sent": "AN squared is 1 big N squared by N matrix and so so just reshape the tensor is a matrix and now you can look at the top right singular value of of this vector, and that's the algorithm that they proposed to find the signal.",
                    "label": 0
                },
                {
                    "sent": "It turns out that they were able to show that for non trivial signal to noise ratio, this this algorithm indeed recovers vector close to the signal.",
                    "label": 0
                },
                {
                    "sent": "And then so they notice something interesting in the notice.",
                    "label": 1
                },
                {
                    "sent": "Is that actually the empirical performance?",
                    "label": 0
                },
                {
                    "sent": "If you actually run this algorithm on on instances generated in this way, it turns out that they that.",
                    "label": 0
                },
                {
                    "sent": "This is significantly smaller noise signal to noise ratio is enough for the algorithm to work, and so there's this gap between the theoretical analysis and empirical performance.",
                    "label": 0
                },
                {
                    "sent": "And usually you know there are many reasons why such a gap can happen.",
                    "label": 0
                },
                {
                    "sent": "You know either you know you don't run the algorithm on the algorithm that you analyze.",
                    "label": 0
                },
                {
                    "sent": "You know you do.",
                    "label": 0
                },
                {
                    "sent": "Some worst case analysis, but then you don't run the algorithm on real life instances.",
                    "label": 0
                },
                {
                    "sent": "But actually here this is not the case.",
                    "label": 0
                },
                {
                    "sent": "You know we analyzed the algorithm exactly on the kind of instances that we actually run the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Another reason why there could be this kind of gap is that.",
                    "label": 0
                },
                {
                    "sent": "Some of the topics on kicking OK, but actually this is also not the case, so you can run these algorithms for huge values of N, so so that's you know that's very unlikely to be an issue.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 1
                },
                {
                    "sent": "Finally, should be that the analysis the theoretical analysis not typed, but also this.",
                    "label": 1
                },
                {
                    "sent": "In many ways, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not the correct answer, so so turns out that the the analysis is tight in in many ways, and so there's something Mr Mysterious going on and we will sort of get back to this mystery in after I show you after I tell you about.",
                    "label": 0
                },
                {
                    "sent": "The results of this work.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, So what we do is we we.",
                    "label": 0
                },
                {
                    "sent": "We study.",
                    "label": 0
                },
                {
                    "sent": "We studied this.",
                    "label": 0
                },
                {
                    "sent": "So sarcastic model forward pencil PCA in from the point of view of a very general algorithmic framework called the sum of squares method.",
                    "label": 0
                },
                {
                    "sent": "This is this has been.",
                    "label": 0
                },
                {
                    "sent": "Discovered by shore and further developed by Perillo and Losier.",
                    "label": 0
                },
                {
                    "sent": "In the 2000s and and so this method has been studied in in many.",
                    "label": 0
                },
                {
                    "sent": "Scientific disciplines also has been studied for approximation algorithms quite a lot, and in the last couple of years it turned out to be very powerful approach to unsupervised learning, both as a way to get new kind of algorithms, but also as a way to understand limitations of algorithms.",
                    "label": 0
                },
                {
                    "sent": "In the resources in service, at least, this one work in this at least one more work in this conference about the software's algorithm.",
                    "label": 0
                },
                {
                    "sent": "And and what we show is that indeed, you know algorithms based on this method allow you to close the gap between the theoretical analysis and empirical performance that montinari in Richard observed.",
                    "label": 1
                },
                {
                    "sent": "So you can get the algorithms that work for this kind of signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "And indeed, the our analysis also allows us to show that sort of a slight variant of the machinery in Richard algorithm is enough to detest.",
                    "label": 0
                },
                {
                    "sent": "Has this kind of this kind of theoretical guarantees.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Villa.",
                    "label": 0
                },
                {
                    "sent": "You're sort of asking questions, so some of the algorithms that we study here they're quite slow, and so it's interesting to ask what how fast can you actually solve this problem?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that you can use knowledge about the eigen values that come from the theoretical analysis in order to speed up.",
                    "label": 1
                },
                {
                    "sent": "Some of the algorithms quite significantly so that they run in nearly linear time.",
                    "label": 0
                },
                {
                    "sent": "Which is the best that you can hope for in this context.",
                    "label": 1
                },
                {
                    "sent": "Finally, we also.",
                    "label": 1
                },
                {
                    "sent": "Study lower bounds for this problem.",
                    "label": 1
                },
                {
                    "sent": "In particular, we show that.",
                    "label": 0
                },
                {
                    "sent": "It's not possible to get better recovery guarantees so so this signal to noise ratio is optimal as long as.",
                    "label": 0
                },
                {
                    "sent": "For for certain, as long as you algorithm is based on a certain broad set of techniques in this process of techniques is.",
                    "label": 1
                },
                {
                    "sent": "Captured by this decree 4 summer squares proof system.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "If you've heard about that.",
                    "label": 0
                },
                {
                    "sent": "OK, so finally.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's resolve this mystery that I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "So so this is our general approach to to try to understand these kind of difficult optimization problems and the approaches.",
                    "label": 0
                },
                {
                    "sent": "Relaxation and grounding.",
                    "label": 0
                },
                {
                    "sent": "So relaxation is A is a tractable optimization problem.",
                    "label": 0
                },
                {
                    "sent": "This associated with let's say so this this this you know, a priority intractable optimization problem in this direction should give you some upper bound on the optimal value here.",
                    "label": 1
                },
                {
                    "sent": "And together with the realization that there's usually a rounding and the rounding, what it doesn't do is allows you to transform a solution.",
                    "label": 0
                },
                {
                    "sent": "For the relaxed optimization problem into a solution to the original optimization problem without changing the objective value by too much, so that's the the kind of thing that you usually try to have, and it turns out that this this this approach it fails for in the context of the relaxation that want to now in Richard study.",
                    "label": 0
                },
                {
                    "sent": "So in the in the in the critical parameter regime, it turns out that the optimal value of their relaxation is very far from the optimal value of the optimization problem.",
                    "label": 1
                },
                {
                    "sent": "That the.",
                    "label": 0
                },
                {
                    "sent": "Of this Emily optimization problem, however, it's still possible to extract to extract, and if you look at an optimal solution to their relaxation, it's still possible at this empirically to extract an optimal solution, and so this you know this kind of discrepancy, it means that there's no rounding in this sense possible.",
                    "label": 0
                },
                {
                    "sent": "No, because the you know there's a huge integrality gap, so this discrepancy between the values is usually called integrality gap, but it's still possible to to recover the solution.",
                    "label": 0
                },
                {
                    "sent": "And so this is a.",
                    "label": 0
                },
                {
                    "sent": "Require some explanation and and it turns out that the the explanation is that there are some 2nd order effect in the optimal value that.",
                    "label": 0
                },
                {
                    "sent": "Cloud enables their recovery in the sense that if you mention solving this relaxation, then there are some solutions where whose value is pretty close to the optimal value, but they don't give any information about the the signal.",
                    "label": 0
                },
                {
                    "sent": "But then if you if you if you start.",
                    "label": 0
                },
                {
                    "sent": "Sort of, you know, solving the relaxation up to a second order term, then suddenly the optimal solutions there they will become close to the actual solution that you care about.",
                    "label": 0
                },
                {
                    "sent": "And in this situation is very troublesome because somehow we know if this would be a common phenomenon.",
                    "label": 0
                },
                {
                    "sent": "We would have to reconsider a lot of our, you know, thinking about relaxations because usually we you know if you have some integrality gap for relaxation, we.",
                    "label": 0
                },
                {
                    "sent": "We consider that relaxation and try to look for other relaxations, but now if somehow it's you know if it's like a common phenomenon, that second order effects in relaxations allow you to get to still get algorithms that would be somehow troublesome and but it turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can resolve this issue a little bit here, and it turns out that if you consider a stronger relaxations, regulations that come from this summer squares method then this second order effect you know translates to a first order effect and there you don't have to worry about.",
                    "label": 0
                },
                {
                    "sent": "The 2nd order effects.",
                    "label": 0
                },
                {
                    "sent": "So let me so this is a technical description of how this squares algorithm works.",
                    "label": 0
                },
                {
                    "sent": "It's fairly simple, but let me, but it's a bit technical.",
                    "label": 0
                },
                {
                    "sent": "Let me skip it here.",
                    "label": 0
                },
                {
                    "sent": "And let me.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Come to the come to the conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So so so I didn't explain it but but I I want to, but I think that's an important message that.",
                    "label": 0
                },
                {
                    "sent": "In in, in this in this in this context.",
                    "label": 0
                },
                {
                    "sent": "The general line of work and the message is that the sum of squares algorithm it.",
                    "label": 0
                },
                {
                    "sent": "It gives a new way of thinking about spectral algorithms in the following sense.",
                    "label": 1
                },
                {
                    "sent": "So usually you have spectral algorithm.",
                    "label": 0
                },
                {
                    "sent": "There is a single matrix that is naturally associated with your problem.",
                    "label": 1
                },
                {
                    "sent": "Either you know it's your input matrix.",
                    "label": 0
                },
                {
                    "sent": "If you have, like for PCA or its graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "If you have a graph problem.",
                    "label": 0
                },
                {
                    "sent": "In both the sum of squares method.",
                    "label": 1
                },
                {
                    "sent": "Gives you it's it.",
                    "label": 0
                },
                {
                    "sent": "It allows you to associate a whole hierarchy of increasingly more interesting families of matrices with a single problem.",
                    "label": 1
                },
                {
                    "sent": "And then the question is, you know, can you and I think there's a lot of opportunities to exploit this richer, richer families of matrices in order to get better algorithms?",
                    "label": 0
                },
                {
                    "sent": "And you have some complete open questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}