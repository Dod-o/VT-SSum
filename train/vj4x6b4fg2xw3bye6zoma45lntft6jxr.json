{
    "id": "vj4x6b4fg2xw3bye6zoma45lntft6jxr",
    "title": "Entropy-based Variational Scheme for Fast Bayes Learning of Gaussian Mixture",
    "info": {
        "author": [
            "Francisco Escolano, Department of Science of the Computation and Artificial Intelligence, University of Alicante"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_escolano_ebv/",
    "segmentation": [
        [
            "This is a work of learning Gaussian mixtures using our.",
            "Bayes learning approach, and in this case we are incorporating elements from information theory."
        ],
        [
            "So.",
            "Everybody, I think in the in the room is aware of the.",
            "Applications and Gaussian mixtures in computer vision and pattern recognition, and.",
            "Boom got some issues.",
            "Are recurrent topic in this field and.",
            "Traditional methods associated to the EM algorithm have evolved to incorporate it elements.",
            "Information theoretic elements like NDL principle in parallel for development of variational Bayes, which is also information theoretic base.",
            "So this is some examples of use of Gaussian mixture for semente."
        ],
        [
            "Channel 4.",
            "Mountain image material.",
            "So."
        ],
        [
            "In variational Bayes, the idea is that we have to get this.",
            "This posterior and this posterior depends on a set of.",
            "Parameters.",
            "Hidden variables.",
            "And this integral, which cannot be done so usually.",
            "One thing to do is to try to approximate this.",
            "Posterior.",
            "By means of another posterior, and this is done through the minimization of the variational."
        ],
        [
            "Energy variational free energy is nothing but the Cooper labor divergences between the.",
            "Approximating distribution and the distribution and this term, the likelihood term.",
            "So we are looking for this minimizing.",
            "That part because this is this, is independent of these variables.",
            "So the idea is to do that using an EM based approach as."
        ],
        [
            "Many of you must, as you know, and you interleave the process of estimating the parameters of the model.",
            "And estimating the hidden variables and this estimation can be done using."
        ],
        [
            "Mean field approximations on these kinds of things.",
            "So when you have a basin mixture.",
            "You have this kind of of posterior.",
            "And where you have the typical hidden variables where indicating that example belongs to a given kernel, and what is more important is that in the."
        ],
        [
            "The.",
            "Bayes variational, Bayes approach the model estimation of the order of the model is implicit.",
            "And for instance, in a recent paper for concession, Apollo San Lucas.",
            "We assume, for instance, fixed number of components.",
            "F and called free components an.",
            "Fixed components an another number of components called free components.",
            "We can be that can be modified.",
            "The idea is that during the optimization process of finding the Gaussian mixture for your data.",
            "You may have a number of variable number of kernels and the idea is to find finally the optimal number of kernels for your data.",
            "So the idea of this paper is to OK to make this this separation between these coefficients of the free components and the fixed components and.",
            "OK, this part of the formulation and this.",
            "Finally the what is the?",
            "The factorization of the of the approximate approximation of the posterior.",
            "We depends on beta, which is related to Alpha.",
            "That is better the probability of a given component to be free or not free during the process."
        ],
        [
            "So.",
            "This is the usual.",
            "Shape of the of the.",
            "Densities.",
            "These multinomial and this is something like."
        ],
        [
            "This is priorities comes from the normal.",
            "The priority of the covariance comes from the wizard distribution and so on.",
            "So.",
            "The idea is to perform this AM algorithm.",
            "In the update, the coefficients of the three components and if we detect that one of the three components is 0, the weight is 0.",
            "We remove that component is something like that."
        ],
        [
            "So.",
            "The basic element of this paper is the.",
            "Is that the standard by Asian?",
            "Variational based method is used for training K equal 2 model.",
            "That is start with one kernel and split into two kernels and speed consists of removing the original component and replaced by two kernels which are set up with the same covariance matrix.",
            "Bad, that means plays in the opposite directions along their maximum variability direction.",
            "That is, along the maximum negative XR, which is OK is quite simple.",
            "And the problem with this method is that the critical point is the amount of splits needs until convergence.",
            "At each iteration of."
        ],
        [
            "This algorithm performing.",
            "These variational based M. Get the older the existing kernels are split.",
            "If after splitting, one realized that this is not.",
            "Has a zero wait, the kernel is removed and this process finished.",
            "When you cannot spit.",
            "Cannot anymore so.",
            "So the number of components increases an then a new set of of spring test starts in next iteration.",
            "This means that if the algorithm stops with K kernels, number of splits has been a quadratic number of them.",
            "So the basic thing in this in Likas algorithm is that OK you can split kernels an all the kernels and test all of them.",
            "But our hypothesis in this work is that is not.",
            "You don't need to do that.",
            "To realize what is the best split.",
            "Actually, what we are going to do is to split one kernel preparation, not all kernels.",
            "OK, so in order to do that we need to define what is a good kernel for splitting and what is a bad kernel for splitting.",
            "OK."
        ],
        [
            "So and reduce this complexity of splittings.",
            "In the process to a linear complexity.",
            "So this is the basic idea we select for splitting the kernel that we think that this less Gaussian that is Colonel is has is a line and contains several samples, and if this samples form bimodality.",
            "Of course this kernel is not Gaussian.",
            "So the idea is to measure the gaussianity of the kernels in order to decide to make a partition of the kernel or not."
        ],
        [
            "OK.",
            "But or main idea is one is paper iteration.",
            "This there basically yeah.",
            "So if we go to the second Gibbs theorem, we know that Gaussian variables have the maximum entropy among all the variables with equal variance.",
            "This means that this is the maximum entropy a kernel can have.",
            "In while in the process.",
            "So given kernel cannot have.",
            "And entropy greater than the Gaussian entropy.",
            "Negotin entropy so.",
            "We define the Gaussian deficiency as.",
            "The difference between the measure entropy of this kernel an."
        ],
        [
            "The maximum entropy of the kernel normalized by the maximum entropy kernel.",
            "This means that the Gaussian deficiency is close to 0.",
            "If you are Gaussian.",
            "Cause this and this tends to be the same thing.",
            "An goes to one if you are not Gaussian.",
            "OK, so we have.",
            "We measure all the kernels we have.",
            "During the process and decide OK, we have to split this one or this one or this one?",
            "OK, measure the Gaussian deficiency an split.",
            "The one with the largest deficiency."
        ],
        [
            "This is the worst kind of OK.",
            "This is including a recent paper or paper, intersessional neural networks and the main problem of this approach is the fact that in a multidimensional context.",
            "Imagine that we are working with Gaussians.",
            "In 15 dimensions, or more than 15 dimensions.",
            "At this time, the problem is committing the entropy is a very hard problem.",
            "So in practice we cannot.",
            "We can only use this approach if we have two 3 dimensional Gaussians.",
            "But what we introduce is the use of bypass entropy estimators.",
            "Bypass entropy estimators or estimators of entropy that give you an approximation of entropy without estimating the PDF.",
            "OK, so if you need to know the entropy of something or multiple dimensions, you can use a bypass entrance tomator.",
            "So we can.",
            "Some of them will knowns are the entropy graphs we have.",
            "Test them better traffic graphs gives you the range entropy and then you have to accept led to the Shannon entropy.",
            "We have done this task in the in the past but recently this.",
            "Estimator which is based on the cell is entropy that does not need to make that extrapolation.",
            "So this is the estimator.",
            "The bypass estimator wages in this paper because or."
        ],
        [
            "Idea is to make this approach.",
            "Applicable 2 high dimensional data, so this is the estimation of entropy following the learning at all approach.",
            "So this K is is our nearest neighbor approach.",
            "K is the number of neighbors, Ann is number of samples.",
            "This is the gamma function and this is the volume of unit ball an this is the.",
            "These are the logarithms.",
            "Is a sum of logarithms of these epsilons without the distance between a sample and the K nearest network nearest neighbor.",
            "OK, so this estimator.",
            "IS has a very good very nice.",
            "Behavior in order to experiment about feature selection.",
            "When we consider a small number of samples.",
            "And a very high number of dimensions.",
            "For instance, we have tested this estimator form for.",
            "Thousands of dimensions 6000 dimensions.",
            "The problem of this estimator is that you want to find you want to maximize it.",
            "Use on doesn't my?",
            "What is the exact number is higher than the other?",
            "But if you want to find exactly this, this estimator is critical when N is small, that is what you have a lot of dimensions and very very few.",
            "Samples anyway.",
            "This is the approach we follow."
        ],
        [
            "And these are some results.",
            "This special interesting result Becausw in this kind of mixture the NDL approach of your engine fails from if you run the code.",
            "30 times it fails a most of most of the time because finds local minimum an you know our previous approach.",
            "Using range entropy is is.",
            "The final result is very nice, and here also the final result is very is very good.",
            "And remember that we are doing that with.",
            "With an order of magnitude less than likos paper, so we are reducing."
        ],
        [
            "We are improving the efficiency of the algorithm.",
            "OK, there are some other cases of mixtures.",
            "And while we need mixtures of higher dimensions, very high dimensions, for instance because sometimes we try to.",
            "To find generative model for manifolds and manifolds are projection on graphs and other objects with many dimensions and you have can.",
            "Do you want to have a mixture model of a manifold you need some of these elements is why we are pursuing methods working in high dimensions."
        ],
        [
            "OK.",
            "So with the wind data for instance, or this is our 30 dimensions and only this.",
            "Small number of samples.",
            "We are getting this.",
            "This recognition rate.",
            "Anne.",
            "I said before that the entropy estimation performs well we with thousands of dimensions."
        ],
        [
            "But what is not in the paper that we have done recently is that trying to move this approach is not to the liquors approach back to the traditional based approach to the Bishop approach, and without considering that some component and fixed and some components are not fixed, the order of the model is implicit in K. The number of K. So the optimal number of K. Is found in this approach so."
        ],
        [
            "To that end.",
            "Replace the simplistic partition splitting method in the liquors approach.",
            "Because we are inserting these rules has to be applied to the partition after partition you have to the pre the weight of the previous kernel must be the way the sum of the first the new kernel first, new kernel and the second wind kennel and so on.",
            "So we do that."
        ],
        [
            "We obtain this kind of moving.",
            "This kind of coefficient.",
            "They come from a delaportas at all paper we have used in our in our work in.",
            "And last year, and basically what we're doing is to make the weights of the.",
            "Of the new.",
            "Kernels dependent of variables following a better distribution.",
            "Moving also the average is based on these variables and also not only in the main the direction of the main body ability, but in all directions but proportionally to the again vector.",
            "They value you have."
        ],
        [
            "So I split is something like that.",
            "We're not moving in the direction of maximum ability, but we're moving the candles in all directions, some directions, more direction less, but All in all directions so."
        ],
        [
            "We consider the classical base by Isha aberrational based approach introduces split methods retain this Gaussian deficiency, two in each iteration of the classical method.",
            "Remove only the worst kernel will not only reduce the complexity of the process, but improve the performance of the method.",
            "Respect to casino.",
            "Colors and liquors and figures on Jane, so some examples is."
        ],
        [
            "Last, like in this case this this case.",
            "Is the overlap case?",
            "As before, performs very bad in?",
            "In Figure 1, Jane and performance also very bad in in in the other, in the other method, but the most significant, I think is is this method is the old faithful data set data set where you have here something like mixer and this is captured by our approach but is not captured by the other previous approach.",
            "So the idea is that we are incorporating.",
            "Two elements.",
            "A method for selecting.",
            "What is the worst kernel at a given iteration to split?",
            "A method for the splitting.",
            "Ant to be aware that this kind of experiments.",
            "Have good results in 1315 dimensions.",
            "So if you have this kind of the dimensionality, you can do your learning of your mixtures in that.",
            "In that case is so this."
        ],
        [
            "The basic idea of this approach and the root of the approach or the basic element of the approaches that they bypass interpeace tomato.",
            "Which is different and more accurate than the entropy graphs method.",
            "So it will depend on the case.",
            "You have.",
            "Take care becausw.",
            "Entropy estimators become critical.",
            "We have a very low number of samples.",
            "Invasion Basware model or the selection is implicit.",
            "It is possible to reduce both complexity complexity at least by one order of magnitude, and it is also possible to improve the accuracy of the results.",
            "For future work we are improving this building rule becauses in this 15 rule we are."
        ],
        [
            "The variance of the the two new kernels is the same of the regional variance, so we have to improve that point."
        ],
        [
            "And.",
            "OK, and the overestimation of the learning goal estimator when a large number of dimension is considered.",
            "I'm very busy.",
            "Few samples is is are available.",
            "But it's not.",
            "It's not so critical.",
            "Bet maybe is number the number of sample is very very low.",
            "OK, so imagine 2 two to find the optimal number of clusters you have only 100 of patterns of 15 dimensions.",
            "So there's."
        ],
        [
            "References this is the paper we are comparing with and these are paper."
        ],
        [
            "Sure.",
            "With the previous method and OK so some.",
            "This paper about the.",
            "Entropy graphs with each other method and.",
            "And this is the paper about Learning Co, which is public."
        ],
        [
            "Teen selfie sticks and it's very easy to implement this.",
            "This entropy estimator or.",
            "And there are some.",
            "Differences where you can use Gaussian mixtures for learning shapes, so that's all thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a work of learning Gaussian mixtures using our.",
                    "label": 0
                },
                {
                    "sent": "Bayes learning approach, and in this case we are incorporating elements from information theory.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Everybody, I think in the in the room is aware of the.",
                    "label": 0
                },
                {
                    "sent": "Applications and Gaussian mixtures in computer vision and pattern recognition, and.",
                    "label": 0
                },
                {
                    "sent": "Boom got some issues.",
                    "label": 0
                },
                {
                    "sent": "Are recurrent topic in this field and.",
                    "label": 1
                },
                {
                    "sent": "Traditional methods associated to the EM algorithm have evolved to incorporate it elements.",
                    "label": 1
                },
                {
                    "sent": "Information theoretic elements like NDL principle in parallel for development of variational Bayes, which is also information theoretic base.",
                    "label": 0
                },
                {
                    "sent": "So this is some examples of use of Gaussian mixture for semente.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Channel 4.",
                    "label": 0
                },
                {
                    "sent": "Mountain image material.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In variational Bayes, the idea is that we have to get this.",
                    "label": 1
                },
                {
                    "sent": "This posterior and this posterior depends on a set of.",
                    "label": 0
                },
                {
                    "sent": "Parameters.",
                    "label": 0
                },
                {
                    "sent": "Hidden variables.",
                    "label": 0
                },
                {
                    "sent": "And this integral, which cannot be done so usually.",
                    "label": 0
                },
                {
                    "sent": "One thing to do is to try to approximate this.",
                    "label": 0
                },
                {
                    "sent": "Posterior.",
                    "label": 0
                },
                {
                    "sent": "By means of another posterior, and this is done through the minimization of the variational.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Energy variational free energy is nothing but the Cooper labor divergences between the.",
                    "label": 1
                },
                {
                    "sent": "Approximating distribution and the distribution and this term, the likelihood term.",
                    "label": 0
                },
                {
                    "sent": "So we are looking for this minimizing.",
                    "label": 1
                },
                {
                    "sent": "That part because this is this, is independent of these variables.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to do that using an EM based approach as.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Many of you must, as you know, and you interleave the process of estimating the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "And estimating the hidden variables and this estimation can be done using.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mean field approximations on these kinds of things.",
                    "label": 0
                },
                {
                    "sent": "So when you have a basin mixture.",
                    "label": 0
                },
                {
                    "sent": "You have this kind of of posterior.",
                    "label": 0
                },
                {
                    "sent": "And where you have the typical hidden variables where indicating that example belongs to a given kernel, and what is more important is that in the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Bayes variational, Bayes approach the model estimation of the order of the model is implicit.",
                    "label": 0
                },
                {
                    "sent": "And for instance, in a recent paper for concession, Apollo San Lucas.",
                    "label": 0
                },
                {
                    "sent": "We assume, for instance, fixed number of components.",
                    "label": 0
                },
                {
                    "sent": "F and called free components an.",
                    "label": 0
                },
                {
                    "sent": "Fixed components an another number of components called free components.",
                    "label": 1
                },
                {
                    "sent": "We can be that can be modified.",
                    "label": 0
                },
                {
                    "sent": "The idea is that during the optimization process of finding the Gaussian mixture for your data.",
                    "label": 0
                },
                {
                    "sent": "You may have a number of variable number of kernels and the idea is to find finally the optimal number of kernels for your data.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this paper is to OK to make this this separation between these coefficients of the free components and the fixed components and.",
                    "label": 1
                },
                {
                    "sent": "OK, this part of the formulation and this.",
                    "label": 0
                },
                {
                    "sent": "Finally the what is the?",
                    "label": 0
                },
                {
                    "sent": "The factorization of the of the approximate approximation of the posterior.",
                    "label": 0
                },
                {
                    "sent": "We depends on beta, which is related to Alpha.",
                    "label": 0
                },
                {
                    "sent": "That is better the probability of a given component to be free or not free during the process.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the usual.",
                    "label": 0
                },
                {
                    "sent": "Shape of the of the.",
                    "label": 0
                },
                {
                    "sent": "Densities.",
                    "label": 0
                },
                {
                    "sent": "These multinomial and this is something like.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is priorities comes from the normal.",
                    "label": 0
                },
                {
                    "sent": "The priority of the covariance comes from the wizard distribution and so on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The idea is to perform this AM algorithm.",
                    "label": 0
                },
                {
                    "sent": "In the update, the coefficients of the three components and if we detect that one of the three components is 0, the weight is 0.",
                    "label": 1
                },
                {
                    "sent": "We remove that component is something like that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The basic element of this paper is the.",
                    "label": 0
                },
                {
                    "sent": "Is that the standard by Asian?",
                    "label": 0
                },
                {
                    "sent": "Variational based method is used for training K equal 2 model.",
                    "label": 1
                },
                {
                    "sent": "That is start with one kernel and split into two kernels and speed consists of removing the original component and replaced by two kernels which are set up with the same covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "Bad, that means plays in the opposite directions along their maximum variability direction.",
                    "label": 0
                },
                {
                    "sent": "That is, along the maximum negative XR, which is OK is quite simple.",
                    "label": 0
                },
                {
                    "sent": "And the problem with this method is that the critical point is the amount of splits needs until convergence.",
                    "label": 0
                },
                {
                    "sent": "At each iteration of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This algorithm performing.",
                    "label": 0
                },
                {
                    "sent": "These variational based M. Get the older the existing kernels are split.",
                    "label": 0
                },
                {
                    "sent": "If after splitting, one realized that this is not.",
                    "label": 0
                },
                {
                    "sent": "Has a zero wait, the kernel is removed and this process finished.",
                    "label": 0
                },
                {
                    "sent": "When you cannot spit.",
                    "label": 0
                },
                {
                    "sent": "Cannot anymore so.",
                    "label": 0
                },
                {
                    "sent": "So the number of components increases an then a new set of of spring test starts in next iteration.",
                    "label": 1
                },
                {
                    "sent": "This means that if the algorithm stops with K kernels, number of splits has been a quadratic number of them.",
                    "label": 1
                },
                {
                    "sent": "So the basic thing in this in Likas algorithm is that OK you can split kernels an all the kernels and test all of them.",
                    "label": 0
                },
                {
                    "sent": "But our hypothesis in this work is that is not.",
                    "label": 1
                },
                {
                    "sent": "You don't need to do that.",
                    "label": 0
                },
                {
                    "sent": "To realize what is the best split.",
                    "label": 0
                },
                {
                    "sent": "Actually, what we are going to do is to split one kernel preparation, not all kernels.",
                    "label": 0
                },
                {
                    "sent": "OK, so in order to do that we need to define what is a good kernel for splitting and what is a bad kernel for splitting.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So and reduce this complexity of splittings.",
                    "label": 0
                },
                {
                    "sent": "In the process to a linear complexity.",
                    "label": 1
                },
                {
                    "sent": "So this is the basic idea we select for splitting the kernel that we think that this less Gaussian that is Colonel is has is a line and contains several samples, and if this samples form bimodality.",
                    "label": 0
                },
                {
                    "sent": "Of course this kernel is not Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to measure the gaussianity of the kernels in order to decide to make a partition of the kernel or not.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But or main idea is one is paper iteration.",
                    "label": 0
                },
                {
                    "sent": "This there basically yeah.",
                    "label": 0
                },
                {
                    "sent": "So if we go to the second Gibbs theorem, we know that Gaussian variables have the maximum entropy among all the variables with equal variance.",
                    "label": 1
                },
                {
                    "sent": "This means that this is the maximum entropy a kernel can have.",
                    "label": 0
                },
                {
                    "sent": "In while in the process.",
                    "label": 0
                },
                {
                    "sent": "So given kernel cannot have.",
                    "label": 0
                },
                {
                    "sent": "And entropy greater than the Gaussian entropy.",
                    "label": 0
                },
                {
                    "sent": "Negotin entropy so.",
                    "label": 0
                },
                {
                    "sent": "We define the Gaussian deficiency as.",
                    "label": 0
                },
                {
                    "sent": "The difference between the measure entropy of this kernel an.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The maximum entropy of the kernel normalized by the maximum entropy kernel.",
                    "label": 1
                },
                {
                    "sent": "This means that the Gaussian deficiency is close to 0.",
                    "label": 0
                },
                {
                    "sent": "If you are Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Cause this and this tends to be the same thing.",
                    "label": 0
                },
                {
                    "sent": "An goes to one if you are not Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have.",
                    "label": 0
                },
                {
                    "sent": "We measure all the kernels we have.",
                    "label": 1
                },
                {
                    "sent": "During the process and decide OK, we have to split this one or this one or this one?",
                    "label": 0
                },
                {
                    "sent": "OK, measure the Gaussian deficiency an split.",
                    "label": 0
                },
                {
                    "sent": "The one with the largest deficiency.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the worst kind of OK.",
                    "label": 0
                },
                {
                    "sent": "This is including a recent paper or paper, intersessional neural networks and the main problem of this approach is the fact that in a multidimensional context.",
                    "label": 1
                },
                {
                    "sent": "Imagine that we are working with Gaussians.",
                    "label": 0
                },
                {
                    "sent": "In 15 dimensions, or more than 15 dimensions.",
                    "label": 0
                },
                {
                    "sent": "At this time, the problem is committing the entropy is a very hard problem.",
                    "label": 0
                },
                {
                    "sent": "So in practice we cannot.",
                    "label": 0
                },
                {
                    "sent": "We can only use this approach if we have two 3 dimensional Gaussians.",
                    "label": 0
                },
                {
                    "sent": "But what we introduce is the use of bypass entropy estimators.",
                    "label": 0
                },
                {
                    "sent": "Bypass entropy estimators or estimators of entropy that give you an approximation of entropy without estimating the PDF.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you need to know the entropy of something or multiple dimensions, you can use a bypass entrance tomator.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "Some of them will knowns are the entropy graphs we have.",
                    "label": 1
                },
                {
                    "sent": "Test them better traffic graphs gives you the range entropy and then you have to accept led to the Shannon entropy.",
                    "label": 0
                },
                {
                    "sent": "We have done this task in the in the past but recently this.",
                    "label": 0
                },
                {
                    "sent": "Estimator which is based on the cell is entropy that does not need to make that extrapolation.",
                    "label": 0
                },
                {
                    "sent": "So this is the estimator.",
                    "label": 0
                },
                {
                    "sent": "The bypass estimator wages in this paper because or.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Idea is to make this approach.",
                    "label": 0
                },
                {
                    "sent": "Applicable 2 high dimensional data, so this is the estimation of entropy following the learning at all approach.",
                    "label": 1
                },
                {
                    "sent": "So this K is is our nearest neighbor approach.",
                    "label": 0
                },
                {
                    "sent": "K is the number of neighbors, Ann is number of samples.",
                    "label": 0
                },
                {
                    "sent": "This is the gamma function and this is the volume of unit ball an this is the.",
                    "label": 1
                },
                {
                    "sent": "These are the logarithms.",
                    "label": 0
                },
                {
                    "sent": "Is a sum of logarithms of these epsilons without the distance between a sample and the K nearest network nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "OK, so this estimator.",
                    "label": 0
                },
                {
                    "sent": "IS has a very good very nice.",
                    "label": 0
                },
                {
                    "sent": "Behavior in order to experiment about feature selection.",
                    "label": 0
                },
                {
                    "sent": "When we consider a small number of samples.",
                    "label": 0
                },
                {
                    "sent": "And a very high number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "For instance, we have tested this estimator form for.",
                    "label": 0
                },
                {
                    "sent": "Thousands of dimensions 6000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "The problem of this estimator is that you want to find you want to maximize it.",
                    "label": 0
                },
                {
                    "sent": "Use on doesn't my?",
                    "label": 0
                },
                {
                    "sent": "What is the exact number is higher than the other?",
                    "label": 0
                },
                {
                    "sent": "But if you want to find exactly this, this estimator is critical when N is small, that is what you have a lot of dimensions and very very few.",
                    "label": 0
                },
                {
                    "sent": "Samples anyway.",
                    "label": 0
                },
                {
                    "sent": "This is the approach we follow.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are some results.",
                    "label": 0
                },
                {
                    "sent": "This special interesting result Becausw in this kind of mixture the NDL approach of your engine fails from if you run the code.",
                    "label": 0
                },
                {
                    "sent": "30 times it fails a most of most of the time because finds local minimum an you know our previous approach.",
                    "label": 0
                },
                {
                    "sent": "Using range entropy is is.",
                    "label": 0
                },
                {
                    "sent": "The final result is very nice, and here also the final result is very is very good.",
                    "label": 0
                },
                {
                    "sent": "And remember that we are doing that with.",
                    "label": 0
                },
                {
                    "sent": "With an order of magnitude less than likos paper, so we are reducing.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are improving the efficiency of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, there are some other cases of mixtures.",
                    "label": 0
                },
                {
                    "sent": "And while we need mixtures of higher dimensions, very high dimensions, for instance because sometimes we try to.",
                    "label": 0
                },
                {
                    "sent": "To find generative model for manifolds and manifolds are projection on graphs and other objects with many dimensions and you have can.",
                    "label": 0
                },
                {
                    "sent": "Do you want to have a mixture model of a manifold you need some of these elements is why we are pursuing methods working in high dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So with the wind data for instance, or this is our 30 dimensions and only this.",
                    "label": 0
                },
                {
                    "sent": "Small number of samples.",
                    "label": 0
                },
                {
                    "sent": "We are getting this.",
                    "label": 0
                },
                {
                    "sent": "This recognition rate.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I said before that the entropy estimation performs well we with thousands of dimensions.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what is not in the paper that we have done recently is that trying to move this approach is not to the liquors approach back to the traditional based approach to the Bishop approach, and without considering that some component and fixed and some components are not fixed, the order of the model is implicit in K. The number of K. So the optimal number of K. Is found in this approach so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To that end.",
                    "label": 0
                },
                {
                    "sent": "Replace the simplistic partition splitting method in the liquors approach.",
                    "label": 0
                },
                {
                    "sent": "Because we are inserting these rules has to be applied to the partition after partition you have to the pre the weight of the previous kernel must be the way the sum of the first the new kernel first, new kernel and the second wind kennel and so on.",
                    "label": 0
                },
                {
                    "sent": "So we do that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We obtain this kind of moving.",
                    "label": 0
                },
                {
                    "sent": "This kind of coefficient.",
                    "label": 0
                },
                {
                    "sent": "They come from a delaportas at all paper we have used in our in our work in.",
                    "label": 0
                },
                {
                    "sent": "And last year, and basically what we're doing is to make the weights of the.",
                    "label": 0
                },
                {
                    "sent": "Of the new.",
                    "label": 0
                },
                {
                    "sent": "Kernels dependent of variables following a better distribution.",
                    "label": 0
                },
                {
                    "sent": "Moving also the average is based on these variables and also not only in the main the direction of the main body ability, but in all directions but proportionally to the again vector.",
                    "label": 0
                },
                {
                    "sent": "They value you have.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I split is something like that.",
                    "label": 0
                },
                {
                    "sent": "We're not moving in the direction of maximum ability, but we're moving the candles in all directions, some directions, more direction less, but All in all directions so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We consider the classical base by Isha aberrational based approach introduces split methods retain this Gaussian deficiency, two in each iteration of the classical method.",
                    "label": 0
                },
                {
                    "sent": "Remove only the worst kernel will not only reduce the complexity of the process, but improve the performance of the method.",
                    "label": 0
                },
                {
                    "sent": "Respect to casino.",
                    "label": 0
                },
                {
                    "sent": "Colors and liquors and figures on Jane, so some examples is.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last, like in this case this this case.",
                    "label": 0
                },
                {
                    "sent": "Is the overlap case?",
                    "label": 0
                },
                {
                    "sent": "As before, performs very bad in?",
                    "label": 0
                },
                {
                    "sent": "In Figure 1, Jane and performance also very bad in in in the other, in the other method, but the most significant, I think is is this method is the old faithful data set data set where you have here something like mixer and this is captured by our approach but is not captured by the other previous approach.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we are incorporating.",
                    "label": 0
                },
                {
                    "sent": "Two elements.",
                    "label": 0
                },
                {
                    "sent": "A method for selecting.",
                    "label": 0
                },
                {
                    "sent": "What is the worst kernel at a given iteration to split?",
                    "label": 0
                },
                {
                    "sent": "A method for the splitting.",
                    "label": 0
                },
                {
                    "sent": "Ant to be aware that this kind of experiments.",
                    "label": 0
                },
                {
                    "sent": "Have good results in 1315 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So if you have this kind of the dimensionality, you can do your learning of your mixtures in that.",
                    "label": 0
                },
                {
                    "sent": "In that case is so this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The basic idea of this approach and the root of the approach or the basic element of the approaches that they bypass interpeace tomato.",
                    "label": 0
                },
                {
                    "sent": "Which is different and more accurate than the entropy graphs method.",
                    "label": 0
                },
                {
                    "sent": "So it will depend on the case.",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "Take care becausw.",
                    "label": 0
                },
                {
                    "sent": "Entropy estimators become critical.",
                    "label": 0
                },
                {
                    "sent": "We have a very low number of samples.",
                    "label": 0
                },
                {
                    "sent": "Invasion Basware model or the selection is implicit.",
                    "label": 0
                },
                {
                    "sent": "It is possible to reduce both complexity complexity at least by one order of magnitude, and it is also possible to improve the accuracy of the results.",
                    "label": 0
                },
                {
                    "sent": "For future work we are improving this building rule becauses in this 15 rule we are.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The variance of the the two new kernels is the same of the regional variance, so we have to improve that point.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "OK, and the overestimation of the learning goal estimator when a large number of dimension is considered.",
                    "label": 1
                },
                {
                    "sent": "I'm very busy.",
                    "label": 1
                },
                {
                    "sent": "Few samples is is are available.",
                    "label": 0
                },
                {
                    "sent": "But it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not so critical.",
                    "label": 0
                },
                {
                    "sent": "Bet maybe is number the number of sample is very very low.",
                    "label": 0
                },
                {
                    "sent": "OK, so imagine 2 two to find the optimal number of clusters you have only 100 of patterns of 15 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So there's.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "References this is the paper we are comparing with and these are paper.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "With the previous method and OK so some.",
                    "label": 0
                },
                {
                    "sent": "This paper about the.",
                    "label": 0
                },
                {
                    "sent": "Entropy graphs with each other method and.",
                    "label": 0
                },
                {
                    "sent": "And this is the paper about Learning Co, which is public.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teen selfie sticks and it's very easy to implement this.",
                    "label": 0
                },
                {
                    "sent": "This entropy estimator or.",
                    "label": 0
                },
                {
                    "sent": "And there are some.",
                    "label": 0
                },
                {
                    "sent": "Differences where you can use Gaussian mixtures for learning shapes, so that's all thank you.",
                    "label": 0
                }
            ]
        }
    }
}