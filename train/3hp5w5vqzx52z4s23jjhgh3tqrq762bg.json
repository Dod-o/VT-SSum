{
    "id": "3hp5w5vqzx52z4s23jjhgh3tqrq762bg",
    "title": "Precomputing Search Features for Fast and Accurate Query Classification",
    "info": {
        "author": [
            "Arnd Christian K\u00f6nig, Microsoft Research"
        ],
        "published": "Feb. 22, 2010",
        "recorded": "February 2010",
        "category": [
            "Top->Computer Science->Semantic Web->Annotation",
            "Top->Computer Science->Web Search"
        ]
    },
    "url": "http://videolectures.net/wsdm2010_konig_psf/",
    "segmentation": [
        [
            "I'm going to be talking about for classification next."
        ],
        [
            "And as most of us know, for classification, sort of ubiquitous combat search now, specially in scenarios where the account that we're trying to retrieve is actually not on the web document, but things like ads or structured comments from the vertical, like in this example with laptops."
        ],
        [
            "They are also very important in terms of load optimization.",
            "Socially posters for outbound per classification often act acts as a gatekeeper at the science which cruise gets admitted to a vertical English."
        ],
        [
            "Give this, there's a large body of work required.",
            "Classification allows it, which works roughly in the walls of text classification, meaning that you're currently single words, and the Kurds engrams in the query are used as features.",
            "Now this is a problem with this request, queries tend to be so short, so on average we have 3 words which doesn't give a lot of signals.",
            "And when you combine that with a large vocabulary size that are relevant these days, you can have various cars and very large.",
            "Which is difficult to classification.",
            "They also had more difficult to generalize across.",
            "Now there are a lot of approaches that are going to actually overcoming specialty and one class of them is the use of post relocation to give your short introduction."
        ],
        [
            "That is the coolest person submitted to web search engine who treat the results within these results.",
            "We identify relevant sub components such as the title and protects capturing key terms, and then we generate features based on the subcomponents in the courage."
        ],
        [
            "It has been shown that these can result in very nice improvements in the overall accuracy of three classification.",
            "Already come with the problem we have is latency.",
            "Basically what you're doing here is running full blast search first before you can even start before your classification and all the crossing the path subsequently.",
            "So but also known is that any increase in latency are actually something that users are very sensitive.",
            "So even very small increases result in.",
            "Much decreased you satisfaction of a search engine and much higher rates of query abandonment, so this is not something you can take lightly and you're talking with this talks really.",
            "How can you realize the benefits of these supposed to people features and do it in a way that is actually errors?"
        ],
        [
            "Stockbridge, roughly falls in the category of post retrieval techniques, but it differs in a couple of important ways and I'll go through these next questions.",
            "First thing is that we use features that are based on the incidence of tags in the documents returned from the query.",
            "Can you give an example?",
            "Consider the task of classifying queries which have problems instead, so that either meant to research or by the property.",
            "Now the tags that we used here are categories, product, securing web pages.",
            "So in this case the cameras, speakers at TV's etc and we say pages stacked with one of these product categories.",
            "If it contains a single instance of that product in the page test.",
            "So now when I first comes in, you know the usual thing with the search results.",
            "We identify the categories and pages and now we compute for each of these categories the ratio of tag pages in the result and these tag ratios then becomes the features using classification and in the case of product intent we can, for example determine that the words no like snapshots, tentacool, Coker with occurrences of a grand camera into documents.",
            "And use that to classify.",
            "This is some nice prices.",
            "For one, we have a smaller feature space until fully gain some generalizations performance through that little thing is that we actually have a lot less information to store because we really only care about these ratios.",
            "For a small set of paths will also show that this generalizes to different corpora that we could do more examples first.",
            "Wanted to corpus of sponsored search fits, so these are the keywords that advertises specified that needs to be matched in order for an ad to display.",
            "I'm giving a certain search for now.",
            "What do you say?",
            "Judgments and locking in focus with every bit places.",
            "Use document and the advertising team if it, as it happened, the intuition why this is useful is you can think of the advertisers, or at least along can be specialized in different areas, so some of them will advertise for business entities, some retail and something very different things.",
            "What this means is sort of these advertisers.",
            "Allergies give you sort of public model and then turn the tag range hoods.",
            "Give me the model of the distribution of including 50 stuff, sorry.",
            "It's for you.",
            "Mother much more straightforward examples to use.",
            "Wikipedia and the use of the most recent Wikipedia facts."
        ],
        [
            "We use the active Pikachu is stab.",
            "Lish is indexed fast.",
            "We have an offline and online thing.",
            "Thanks for giving up.",
            "In fact for this.",
            "No worries, and personally packages and to retrieve the semantics we used a slightly differently.",
            "We don't involve the search engine, but instead we just use very simple containment semantics, no ranking, and there's a very interesting trade over the month because we don't use a search engine we can actually do a few computation for a very large set of queries.",
            "Assembly interacting with the docking ports, so we scale up edit this way.",
            "Some of the quality of the resulting tags and the tradeoff that you have here is that search engines are typically aimed at a small but highly relevant result set, whereas we have a very large result set that we consider which may be less relevant, but get also allows us to generate this tags based on a lot more evidence and at least some experiments that we ran indicate that we actually come out ahead with regards to the overall usefulness of these tags when compared.",
            "To the ones generated by a search engine and a small constraint on the number of documents.",
            "Now in the online phase, a query comes in.",
            "We retrieve all relevant tag ratios from our store, generate features from them, and run these through bird classified.",
            "The index structures that we use are actually based on structures that are used for broad broad match indexing in sponsored search retrieval.",
            "There very fast.",
            "I'm not going to talk about them in more detail.",
            "Instead what I'm going to talk about, uh, three questions that this approach opens up.",
            "The first one is given these ratios.",
            "How do we generate features from them and the 2nd and the next two questions are really based on the fact that we now have limited storage to deal with.",
            "It's impossible to precompute all the ratios for all possible queries that people can enter, so the questions are for which queries do we do?",
            "We pre compute ratios and how do we deal with the fact that a query comes in and we haven't precomputed the ratios for this specific word?",
            "Let me start with features first, so a very simple approach could be to say, well, given that we have for any query we have these ratios, let's just use them as features.",
            "There's two problems with that.",
            "One is we occasionally have, especially with long queries.",
            "Queries that give us very small result sets or empty result sets in turn resulting in in racials that we can't really use.",
            "Also, we have problems with queries where we haven't precomputed the racial slur.",
            "And the approach we used to address this is through the use of so called back operations, and the idea is very similar to back off in the context of language modeling where you estimate the frequency of a word given the large context by the frequency of the words given.",
            "The much smaller context of the large context hasn't occurred often.",
            "So what we do here is instead of the ratio for a long query, we use ratios for a smaller query containing a subset of the words.",
            "This can actually help us gain robustness.",
            "For example, if we have a query Canon camera SD 2.",
            "Where S D2 is actually not a model that they do by using just the query Canon camera, we might still get reactions that tell us that this query has product intent."
        ],
        [
            "To give an idea of the benefits we can gain here, but what I have here is a graph that shows the accuracy in classification based on the query results for the input query versus the accuracy for results obtained by back of ratios.",
            "Various size on the right side and you can see as soon as we start using backup ratios we have a huge jump in overall accuracy."
        ],
        [
            "The overall framework that we now use for is that foreign given query.",
            "We have a set of precomputed tag ratios.",
            "We retrieve the query and all subqueries from the tag racial storm.",
            "And now we grouped them according to how similar they are to the query.",
            "And generate features for each of these groups and the way we generate features basically by aggregating over every group and every tag therein and computing aggregate function functions over those ratios or things like the sum Max min, than the deviation, etc."
        ],
        [
            "So in the example of the query, earlier Canon Cameras Day 2, the first group would contain just single word queries, Canon Camera and SD 2, then exclude 2 words, queries, etc."
        ],
        [
            "Finally, regarding selecting queries to precompute, the issue is that even with medium size for Project Wikipedia, we very quickly have vocabulary that exceeds 10s of millions of words and huge amount of keyword combinations that we can't enumerate or precompute.",
            "So we use some pruning logic.",
            "The 1st first part is that we only limit ourselves to short queries.",
            "2nd is that we require significant correlation.",
            "That is, we only use tag ratios that are significantly different from the expected ratio we expect to see based on the frequency of the tag on the corpus.",
            "And finally we only use ratios that are based on some supporting evidence, which means somewhat large results.",
            "What that does to quality out get to in a second in the experimental evaluation.",
            "But there's a very nice intuition why this actually proves well when you consider the distribution of words and corpora.",
            "So what we know is that words and word combinations in text corpora tend to be distributed roughly according to a power law, which means that most words are very rare, hence the support condition will tune out most of these words.",
            "And now for the more frequent words because.",
            "If we consider a word that has no semantic relation to an incoming to attack, so we assume that the distribution of tags and that words result is random, then because this word is now frequent, we expect the incidents of these tags to be roughly similar in roughly the same as the expected incidents.",
            "So you have a tight concentration, which in turn means that the correlation condition prunes out a lot of the remaining keywords and keyword combinations."
        ],
        [
            "Hey, really, quickly on the experimental evaluation, we evaluated this on a whole bunch of tasks.",
            "I'm going to talk about three of them.",
            "The first one is identifying consumer electronic words.",
            "The corpus we used here is Wikipedia.",
            "The tags, the entity categories that I had in the beginning.",
            "Now if we only use Ngram features, we get 93% accuracy.",
            "Augmenting these with dictionaries of brands, models, product types, product attributes, we get slight increases.",
            "Where's the tag ratios?",
            "Give us 95.6 and the combination of tag ratios and the original ngram features get us up to 96%.",
            "Second task is identifying retail queries.",
            "Here we used slightly different purpose.",
            "We use Wikipedia in the Wikipedia categories and we use the sponsored bits corpus and advertiser IDs for the most frequent advertisers.",
            "The interesting thing about this corpus is that it's very large, so we're looking at 300,000 labeled examples and the more examples we have, the better the engram techniques N gram based techniques typically work.",
            "In fact, if you have extremely large numbers of examples.",
            "They often tend to outperform a lot of other techniques.",
            "And again, what we see here is that the accuracy using only the ngram based techniques is significantly improved when we combine them with the tag ratios and we see the same thing for health related queries where we have a corpus of 800,000 labeled examples."
        ],
        [
            "And finally, one quick word on the pruning efficiency.",
            "So we evaluated using the URL three earlier tasks.",
            "The accuracy for using single word queries and back of only using the combination of single word queries and the query keyword ratio combination selected by a pruning rules.",
            "And finally, because we had limited training set, we could do this for all queries in the training sets and all the subsets of of them.",
            "So this is basically corresponds to precomputing everything, which is something we couldn't do in practice.",
            "And the results were very interesting.",
            "First of all, the pruning results in extremely large reduction in this space operations.",
            "So even if we only consider frequent keywords, we still get less than than a percent.",
            "And.",
            "In a in combination with that we get a very slight degradation in the classification accuracy, which means that both the indicative ratios tend to work well in terms of not getting us too bad classification accuracy, but it was what was also surprising is that the single key words also tend to work very well here, so in a lot of cases it actually surprises to only using the free words single keywords for the task, and you're gonna do fine."
        ],
        [
            "Thank you very much.",
            "So let me maybe ask you a quick question.",
            "Hey, could you please provide some statistics regarding the number of categories considered.",
            "So OK, let me repeat the question.",
            "No, actually you had them nevermind tag categories.",
            "So for the.",
            "Numbers are in the paper, if I recall correctly.",
            "We used both for Wikipedia and for the advertiser tag corpus.",
            "We used 5000 and 10,000 different categories and a much smaller number of product categories.",
            "In the task where we had to identify consumer electronics, I think it was only about 150 different product groups.",
            "And the number of queries were basically corresponded to the number of training examples, so between 30K and 800K.",
            "Any additional questions?",
            "123 gone OK.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to be talking about for classification next.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as most of us know, for classification, sort of ubiquitous combat search now, specially in scenarios where the account that we're trying to retrieve is actually not on the web document, but things like ads or structured comments from the vertical, like in this example with laptops.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They are also very important in terms of load optimization.",
                    "label": 0
                },
                {
                    "sent": "Socially posters for outbound per classification often act acts as a gatekeeper at the science which cruise gets admitted to a vertical English.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give this, there's a large body of work required.",
                    "label": 0
                },
                {
                    "sent": "Classification allows it, which works roughly in the walls of text classification, meaning that you're currently single words, and the Kurds engrams in the query are used as features.",
                    "label": 0
                },
                {
                    "sent": "Now this is a problem with this request, queries tend to be so short, so on average we have 3 words which doesn't give a lot of signals.",
                    "label": 0
                },
                {
                    "sent": "And when you combine that with a large vocabulary size that are relevant these days, you can have various cars and very large.",
                    "label": 1
                },
                {
                    "sent": "Which is difficult to classification.",
                    "label": 0
                },
                {
                    "sent": "They also had more difficult to generalize across.",
                    "label": 1
                },
                {
                    "sent": "Now there are a lot of approaches that are going to actually overcoming specialty and one class of them is the use of post relocation to give your short introduction.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is the coolest person submitted to web search engine who treat the results within these results.",
                    "label": 0
                },
                {
                    "sent": "We identify relevant sub components such as the title and protects capturing key terms, and then we generate features based on the subcomponents in the courage.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It has been shown that these can result in very nice improvements in the overall accuracy of three classification.",
                    "label": 1
                },
                {
                    "sent": "Already come with the problem we have is latency.",
                    "label": 0
                },
                {
                    "sent": "Basically what you're doing here is running full blast search first before you can even start before your classification and all the crossing the path subsequently.",
                    "label": 0
                },
                {
                    "sent": "So but also known is that any increase in latency are actually something that users are very sensitive.",
                    "label": 1
                },
                {
                    "sent": "So even very small increases result in.",
                    "label": 1
                },
                {
                    "sent": "Much decreased you satisfaction of a search engine and much higher rates of query abandonment, so this is not something you can take lightly and you're talking with this talks really.",
                    "label": 0
                },
                {
                    "sent": "How can you realize the benefits of these supposed to people features and do it in a way that is actually errors?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stockbridge, roughly falls in the category of post retrieval techniques, but it differs in a couple of important ways and I'll go through these next questions.",
                    "label": 0
                },
                {
                    "sent": "First thing is that we use features that are based on the incidence of tags in the documents returned from the query.",
                    "label": 1
                },
                {
                    "sent": "Can you give an example?",
                    "label": 0
                },
                {
                    "sent": "Consider the task of classifying queries which have problems instead, so that either meant to research or by the property.",
                    "label": 0
                },
                {
                    "sent": "Now the tags that we used here are categories, product, securing web pages.",
                    "label": 0
                },
                {
                    "sent": "So in this case the cameras, speakers at TV's etc and we say pages stacked with one of these product categories.",
                    "label": 0
                },
                {
                    "sent": "If it contains a single instance of that product in the page test.",
                    "label": 0
                },
                {
                    "sent": "So now when I first comes in, you know the usual thing with the search results.",
                    "label": 0
                },
                {
                    "sent": "We identify the categories and pages and now we compute for each of these categories the ratio of tag pages in the result and these tag ratios then becomes the features using classification and in the case of product intent we can, for example determine that the words no like snapshots, tentacool, Coker with occurrences of a grand camera into documents.",
                    "label": 0
                },
                {
                    "sent": "And use that to classify.",
                    "label": 0
                },
                {
                    "sent": "This is some nice prices.",
                    "label": 1
                },
                {
                    "sent": "For one, we have a smaller feature space until fully gain some generalizations performance through that little thing is that we actually have a lot less information to store because we really only care about these ratios.",
                    "label": 0
                },
                {
                    "sent": "For a small set of paths will also show that this generalizes to different corpora that we could do more examples first.",
                    "label": 0
                },
                {
                    "sent": "Wanted to corpus of sponsored search fits, so these are the keywords that advertises specified that needs to be matched in order for an ad to display.",
                    "label": 0
                },
                {
                    "sent": "I'm giving a certain search for now.",
                    "label": 0
                },
                {
                    "sent": "What do you say?",
                    "label": 0
                },
                {
                    "sent": "Judgments and locking in focus with every bit places.",
                    "label": 0
                },
                {
                    "sent": "Use document and the advertising team if it, as it happened, the intuition why this is useful is you can think of the advertisers, or at least along can be specialized in different areas, so some of them will advertise for business entities, some retail and something very different things.",
                    "label": 0
                },
                {
                    "sent": "What this means is sort of these advertisers.",
                    "label": 0
                },
                {
                    "sent": "Allergies give you sort of public model and then turn the tag range hoods.",
                    "label": 0
                },
                {
                    "sent": "Give me the model of the distribution of including 50 stuff, sorry.",
                    "label": 0
                },
                {
                    "sent": "It's for you.",
                    "label": 0
                },
                {
                    "sent": "Mother much more straightforward examples to use.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia and the use of the most recent Wikipedia facts.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use the active Pikachu is stab.",
                    "label": 0
                },
                {
                    "sent": "Lish is indexed fast.",
                    "label": 0
                },
                {
                    "sent": "We have an offline and online thing.",
                    "label": 0
                },
                {
                    "sent": "Thanks for giving up.",
                    "label": 0
                },
                {
                    "sent": "In fact for this.",
                    "label": 0
                },
                {
                    "sent": "No worries, and personally packages and to retrieve the semantics we used a slightly differently.",
                    "label": 0
                },
                {
                    "sent": "We don't involve the search engine, but instead we just use very simple containment semantics, no ranking, and there's a very interesting trade over the month because we don't use a search engine we can actually do a few computation for a very large set of queries.",
                    "label": 0
                },
                {
                    "sent": "Assembly interacting with the docking ports, so we scale up edit this way.",
                    "label": 0
                },
                {
                    "sent": "Some of the quality of the resulting tags and the tradeoff that you have here is that search engines are typically aimed at a small but highly relevant result set, whereas we have a very large result set that we consider which may be less relevant, but get also allows us to generate this tags based on a lot more evidence and at least some experiments that we ran indicate that we actually come out ahead with regards to the overall usefulness of these tags when compared.",
                    "label": 0
                },
                {
                    "sent": "To the ones generated by a search engine and a small constraint on the number of documents.",
                    "label": 0
                },
                {
                    "sent": "Now in the online phase, a query comes in.",
                    "label": 0
                },
                {
                    "sent": "We retrieve all relevant tag ratios from our store, generate features from them, and run these through bird classified.",
                    "label": 0
                },
                {
                    "sent": "The index structures that we use are actually based on structures that are used for broad broad match indexing in sponsored search retrieval.",
                    "label": 0
                },
                {
                    "sent": "There very fast.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about them in more detail.",
                    "label": 0
                },
                {
                    "sent": "Instead what I'm going to talk about, uh, three questions that this approach opens up.",
                    "label": 0
                },
                {
                    "sent": "The first one is given these ratios.",
                    "label": 0
                },
                {
                    "sent": "How do we generate features from them and the 2nd and the next two questions are really based on the fact that we now have limited storage to deal with.",
                    "label": 1
                },
                {
                    "sent": "It's impossible to precompute all the ratios for all possible queries that people can enter, so the questions are for which queries do we do?",
                    "label": 0
                },
                {
                    "sent": "We pre compute ratios and how do we deal with the fact that a query comes in and we haven't precomputed the ratios for this specific word?",
                    "label": 0
                },
                {
                    "sent": "Let me start with features first, so a very simple approach could be to say, well, given that we have for any query we have these ratios, let's just use them as features.",
                    "label": 0
                },
                {
                    "sent": "There's two problems with that.",
                    "label": 0
                },
                {
                    "sent": "One is we occasionally have, especially with long queries.",
                    "label": 0
                },
                {
                    "sent": "Queries that give us very small result sets or empty result sets in turn resulting in in racials that we can't really use.",
                    "label": 0
                },
                {
                    "sent": "Also, we have problems with queries where we haven't precomputed the racial slur.",
                    "label": 0
                },
                {
                    "sent": "And the approach we used to address this is through the use of so called back operations, and the idea is very similar to back off in the context of language modeling where you estimate the frequency of a word given the large context by the frequency of the words given.",
                    "label": 0
                },
                {
                    "sent": "The much smaller context of the large context hasn't occurred often.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is instead of the ratio for a long query, we use ratios for a smaller query containing a subset of the words.",
                    "label": 0
                },
                {
                    "sent": "This can actually help us gain robustness.",
                    "label": 0
                },
                {
                    "sent": "For example, if we have a query Canon camera SD 2.",
                    "label": 0
                },
                {
                    "sent": "Where S D2 is actually not a model that they do by using just the query Canon camera, we might still get reactions that tell us that this query has product intent.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To give an idea of the benefits we can gain here, but what I have here is a graph that shows the accuracy in classification based on the query results for the input query versus the accuracy for results obtained by back of ratios.",
                    "label": 0
                },
                {
                    "sent": "Various size on the right side and you can see as soon as we start using backup ratios we have a huge jump in overall accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The overall framework that we now use for is that foreign given query.",
                    "label": 0
                },
                {
                    "sent": "We have a set of precomputed tag ratios.",
                    "label": 0
                },
                {
                    "sent": "We retrieve the query and all subqueries from the tag racial storm.",
                    "label": 0
                },
                {
                    "sent": "And now we grouped them according to how similar they are to the query.",
                    "label": 0
                },
                {
                    "sent": "And generate features for each of these groups and the way we generate features basically by aggregating over every group and every tag therein and computing aggregate function functions over those ratios or things like the sum Max min, than the deviation, etc.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the example of the query, earlier Canon Cameras Day 2, the first group would contain just single word queries, Canon Camera and SD 2, then exclude 2 words, queries, etc.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, regarding selecting queries to precompute, the issue is that even with medium size for Project Wikipedia, we very quickly have vocabulary that exceeds 10s of millions of words and huge amount of keyword combinations that we can't enumerate or precompute.",
                    "label": 1
                },
                {
                    "sent": "So we use some pruning logic.",
                    "label": 1
                },
                {
                    "sent": "The 1st first part is that we only limit ourselves to short queries.",
                    "label": 0
                },
                {
                    "sent": "2nd is that we require significant correlation.",
                    "label": 0
                },
                {
                    "sent": "That is, we only use tag ratios that are significantly different from the expected ratio we expect to see based on the frequency of the tag on the corpus.",
                    "label": 0
                },
                {
                    "sent": "And finally we only use ratios that are based on some supporting evidence, which means somewhat large results.",
                    "label": 0
                },
                {
                    "sent": "What that does to quality out get to in a second in the experimental evaluation.",
                    "label": 0
                },
                {
                    "sent": "But there's a very nice intuition why this actually proves well when you consider the distribution of words and corpora.",
                    "label": 0
                },
                {
                    "sent": "So what we know is that words and word combinations in text corpora tend to be distributed roughly according to a power law, which means that most words are very rare, hence the support condition will tune out most of these words.",
                    "label": 0
                },
                {
                    "sent": "And now for the more frequent words because.",
                    "label": 0
                },
                {
                    "sent": "If we consider a word that has no semantic relation to an incoming to attack, so we assume that the distribution of tags and that words result is random, then because this word is now frequent, we expect the incidents of these tags to be roughly similar in roughly the same as the expected incidents.",
                    "label": 1
                },
                {
                    "sent": "So you have a tight concentration, which in turn means that the correlation condition prunes out a lot of the remaining keywords and keyword combinations.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey, really, quickly on the experimental evaluation, we evaluated this on a whole bunch of tasks.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about three of them.",
                    "label": 0
                },
                {
                    "sent": "The first one is identifying consumer electronic words.",
                    "label": 0
                },
                {
                    "sent": "The corpus we used here is Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "The tags, the entity categories that I had in the beginning.",
                    "label": 0
                },
                {
                    "sent": "Now if we only use Ngram features, we get 93% accuracy.",
                    "label": 0
                },
                {
                    "sent": "Augmenting these with dictionaries of brands, models, product types, product attributes, we get slight increases.",
                    "label": 0
                },
                {
                    "sent": "Where's the tag ratios?",
                    "label": 0
                },
                {
                    "sent": "Give us 95.6 and the combination of tag ratios and the original ngram features get us up to 96%.",
                    "label": 0
                },
                {
                    "sent": "Second task is identifying retail queries.",
                    "label": 0
                },
                {
                    "sent": "Here we used slightly different purpose.",
                    "label": 0
                },
                {
                    "sent": "We use Wikipedia in the Wikipedia categories and we use the sponsored bits corpus and advertiser IDs for the most frequent advertisers.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing about this corpus is that it's very large, so we're looking at 300,000 labeled examples and the more examples we have, the better the engram techniques N gram based techniques typically work.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you have extremely large numbers of examples.",
                    "label": 0
                },
                {
                    "sent": "They often tend to outperform a lot of other techniques.",
                    "label": 0
                },
                {
                    "sent": "And again, what we see here is that the accuracy using only the ngram based techniques is significantly improved when we combine them with the tag ratios and we see the same thing for health related queries where we have a corpus of 800,000 labeled examples.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, one quick word on the pruning efficiency.",
                    "label": 0
                },
                {
                    "sent": "So we evaluated using the URL three earlier tasks.",
                    "label": 0
                },
                {
                    "sent": "The accuracy for using single word queries and back of only using the combination of single word queries and the query keyword ratio combination selected by a pruning rules.",
                    "label": 0
                },
                {
                    "sent": "And finally, because we had limited training set, we could do this for all queries in the training sets and all the subsets of of them.",
                    "label": 1
                },
                {
                    "sent": "So this is basically corresponds to precomputing everything, which is something we couldn't do in practice.",
                    "label": 0
                },
                {
                    "sent": "And the results were very interesting.",
                    "label": 0
                },
                {
                    "sent": "First of all, the pruning results in extremely large reduction in this space operations.",
                    "label": 1
                },
                {
                    "sent": "So even if we only consider frequent keywords, we still get less than than a percent.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In a in combination with that we get a very slight degradation in the classification accuracy, which means that both the indicative ratios tend to work well in terms of not getting us too bad classification accuracy, but it was what was also surprising is that the single key words also tend to work very well here, so in a lot of cases it actually surprises to only using the free words single keywords for the task, and you're gonna do fine.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So let me maybe ask you a quick question.",
                    "label": 0
                },
                {
                    "sent": "Hey, could you please provide some statistics regarding the number of categories considered.",
                    "label": 0
                },
                {
                    "sent": "So OK, let me repeat the question.",
                    "label": 0
                },
                {
                    "sent": "No, actually you had them nevermind tag categories.",
                    "label": 0
                },
                {
                    "sent": "So for the.",
                    "label": 0
                },
                {
                    "sent": "Numbers are in the paper, if I recall correctly.",
                    "label": 0
                },
                {
                    "sent": "We used both for Wikipedia and for the advertiser tag corpus.",
                    "label": 0
                },
                {
                    "sent": "We used 5000 and 10,000 different categories and a much smaller number of product categories.",
                    "label": 0
                },
                {
                    "sent": "In the task where we had to identify consumer electronics, I think it was only about 150 different product groups.",
                    "label": 0
                },
                {
                    "sent": "And the number of queries were basically corresponded to the number of training examples, so between 30K and 800K.",
                    "label": 0
                },
                {
                    "sent": "Any additional questions?",
                    "label": 0
                },
                {
                    "sent": "123 gone OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}