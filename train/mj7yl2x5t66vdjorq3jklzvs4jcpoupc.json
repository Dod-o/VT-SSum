{
    "id": "mj7yl2x5t66vdjorq3jklzvs4jcpoupc",
    "title": "Reinforcement Learning",
    "info": {
        "author": [
            "Satinder Singh, Electrical Engineering and Computer Science Department, University of Michigan"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_singh_reinforcement_learning/",
    "segmentation": [
        [
            "So that was saying thank you for coming.",
            "I know you meant if you've been here for many days and you've heard from many great talks.",
            "I've been here for since the beginning of the reinforcement learning one, and it's been fantastic.",
            "So thanks, Joel and join and others who organized this fantastic summer school.",
            "So I'm actually going to pick up on particular on Nando's talk.",
            "I didn't know what I was going to talk about until I heard Nando, and I said, OK, I know what I'm going to talk about now.",
            "And I'm going to present a slightly different point of view or take take on a very similar similar questions, and I'm calling this sort of steps towards continual learning an I have two affiliations people have been explaining that affiliation with Professor University of Michigan.",
            "I'm also the chief scientist at a startup I Co founded with Peter Stone and Mark Ring called coached I an.",
            "In fact CODA is a continual learning company to all the work I'm going to talk about has been done.",
            "At universities, so nothing, none of this is from code at the moment.",
            "OK."
        ],
        [
            "So.",
            "The words continual learning might mean something intuitively to many people.",
            "I want to break it down into what precisely I mean by continual learning, and this will be connect also to what Nando talked about, but in slightly different ways of slicing it.",
            "So when I say continue learning when you want to continue learning agents, I mean agents that can learn new skills overtime from experience as one of the first elements.",
            "Now this might seem like.",
            "Well, what's so radical or sort of interesting about that RL is about learning options or skills, but let's break it down a little bit more.",
            "Learning new skills overtime means I have to store those skills somewhere I have to retrieve them sensibly and hardly any and continually overtime.",
            "Right, and hardly any IRL system or agent does that at the moment.",
            "So this is already just.",
            "This ability is an interesting challenge to take seriously and work on it.",
            "Learn new knowledge, overtime and by knowledge I mean option conditional prediction skill, conditional predictions.",
            "We want to make predictions about the world conditioned on behavior.",
            "Again, do this overtime.",
            "We have our work.",
            "Another work that builds, models, bills, predictions but again do it overtime and integrate that in such a way that we can reuse and incorporate these skills and knowledge that we've acquired overtime.",
            "To help learn more complex skills and knowledge.",
            "Right this developmental, this incremental aspect of learning new skills and learning new knowledge and and then reusing it overtime is a really important challenge that you know people are working on and you want to do it in a way that's scalable and without catastrophic forgetting.",
            "And things of that sort that you heard about, presumably in the deep learning part of the summer school as well.",
            "Another really important question Andrew touched on this one as well is the idea of intrinsic motivation right?",
            "If you have an agent that's going to be doing this kind of continual learning, then what should drive its behavior?",
            "What should an agent do?",
            "In such a setting.",
            "Write reinforcement learning comes with a task.",
            "Continual learning doesn't come with a task.",
            "Will not necessarily come with the task, so you can think of intrinsic motivation to drive experience in the absence, or perhaps more accurately, too long.",
            "A delay in extrinsic.",
            "In extrinsic rewards, extrinsic drives or extrinsic motivation.",
            "So this is a fundamental question in continual learning as well, and by the way, again, to connect to work you've heard about one particularly salient source of intrinsic rewards are the humans are humans or other agents experienced agents.",
            "Right, you heard about imitation.",
            "The drive to imitate, and so on.",
            "As particularly salient examples of where intrinsic motivation might come from.",
            "And finally, of course, the overall goal is still the is still the PRL goal right?",
            "Which is to building agents that can become increasingly competent overtime, not just in terms of the amount of skill and knowledge they have, but also in term in terms of how well they're doing.",
            "Accumulating the extrinsic reward when it is available.",
            "So this is the continual learning problem.",
            "In one slide, right?",
            "This is a problem that I think so.",
            "So you know one thing you could be asking is sort of, you know, how is this different from other things in AI and RL and so on.",
            "So this is a very RL centered view of a alright so very RL inspired RL based view of how to think about the overall AI question as opposed to you know vision is an important problem.",
            "Or speech is an important problem or language is an important problem, which of course they are extremely important problems, but it's a different way of slicing up their problem, and this is a.",
            "This is a continual learning way of slicing up the problem, so I want to make sure that at least what this this framework is about.",
            "This setting is about is reasonably clear.",
            "Any questions about this, particularly how it relates to, are you think this certainly is coming from RL?",
            "The 11 difference or a difference in emphasis is an emphasis on knowledge.",
            "Right, not just on behavior, of course.",
            "Knowledge for good behavior and the intrinsic motivation.",
            "So it certainly comes from RL, but emphasizes these things.",
            "Any questions about this?",
            "By the way, I notice when I was sitting in the audience that sometimes people have questions.",
            "They just raise their hand, but it's hard to see that you raise your hand.",
            "So if you have a question, just blurt something out so I can.",
            "I can hear it and then.",
            "And then know that you want to ask a question.",
            "OK, yes.",
            "The system you have in mind like something concrete.",
            "There sort of like where might this be powerful, yes.",
            "Right, so you know again I'll harken back to something non they talked about.",
            "You know kids in toys as a as a you know you leave a kid in a room with things.",
            "Without extrinsic task, I'll come back in half an hour.",
            "What will they do?",
            "It will come to understand the world there in by playing around in their world by intrinsic drives to explore, to destroy, to, to build the climb on things, to kick things, to do.",
            "Depending on the child, do all kinds of different things.",
            "They build a fundamental understanding of the world in that way.",
            "So to the extent that you want to build truly robust AI's.",
            "Right, as opposed to a eyes to solve very well defined specific problems.",
            "Not that that's a undesirable or easy thing to do either, but to the extent that you want to build artificial intelligences that are able to.",
            "Really robustly understand the world and behavior.",
            "To that extent, we can't build in knowledge.",
            "To that extent, knowledge has to be driven by has to be learned from experience and be intrinsically motivated in the sense of being driven to learning useful things.",
            "So that's the setting that this comes from, yes.",
            "On page that knowledge yeah.",
            "So option conditional predictions are just behavior conditional predictions.",
            "What will I see?",
            "Or will there be a door when I walk out of the left corridor here?",
            "How far will the door be?",
            "How much time will it be before I see the sunlight if I walk by a certain path?",
            "If I pick up the phone and call my wife, is she going to be able to like you do respond?",
            "What's the property that should respond?",
            "These option conditional predictions?",
            "The option is the behavior I'm doing and predictions are certain features of the future that I want to predict.",
            "Sounds a bit like planning is that.",
            "Yes, very interesting question is how does all this relate to planning?",
            "So how do you use predictions to plan how to use knowledge to plan is a really interesting question.",
            "I'm going to show you a really simple."
        ],
        [
            "Example from my work more than a decade ago, 2004 a child's playroom.",
            "This is a paper at Nips, so let me just say this is in my judgment, sort of now by now you know for young people in ancient, maybe many of you weren't even born.",
            "9 gesture.",
            "Ancient example of a continued learning demonstration, so this is a.",
            "This is a toy world.",
            "The child's playroom, which has objects in it.",
            "Objects that have things like that are bells.",
            "There is a monkey toy monkey hand, so you can imagine that there is a.",
            "The agent is looking at this from the top where we're getting a bird's eye view, but the agent has an IA focus.",
            "And the agencies, what's where the eye is agent is a hand.",
            "They didn't consent what's under the hand and the agent has this marker.",
            "This thing with crosshairs called a marker and it's like an indexical representation.",
            "Can remember where it was.",
            "And then there are these buttons.",
            "The blue button to turn music on the red button to turn music off.",
            "It can push something into the ball and roll the ball, hit the ball and roll the ball.",
            "We rose the ball into the Bell.",
            "The Bell rings.",
            "The light switch with turn the light on and off the lights off.",
            "You can't see the colors.",
            "Things of that sort right?",
            "So the agent is put in this world without any extrinsic task.",
            "Perhaps it has to learn to achieve mastery over this little really trivial contrived toy world.",
            "And so I've already described some of the primitive actions, the hand and the I can be moved.",
            "I think the I can be moved North, South East, West the I can be moved to the marker.",
            "The market move to the eye.",
            "The hand can be moved to the things of that sort.",
            "And I already described some of the things one can do in the room, and I think the.",
            "What you want the system to be able to do is somehow learn all the things you can do in this world, how to turn the music on, how to ring the Bell, how to kick the ball, how to how to make the the toy monkey cry out.",
            "Which is I don't know why I use that phrase when writing this paper, but it's been called a monkey world since since we built this build this little domain.",
            "But but but the ultimate, the hardest skill to learn is to make the toy cry out, which requires having the lights off and the music playing and the Bell ringing, so loud noise and the lights off makes the monkey cry out.",
            "OK, so that's the setting."
        ],
        [
            "I don't know if anybody talked about options.",
            "I know many of you know enough about options.",
            "Options are basically temporally extended behaviors and there's a nice very nice mathematical formalism and Joiner who's here at McGill was one of the leaders in developing this option sort of work OK."
        ],
        [
            "Don't talk much about that.",
            "So now let me bring in the continual learning elements and tell you how we sort of solved this very simple problem in this sort of continuing way.",
            "So what is the intrinsic reward here?",
            "The intrinsic reward was error was proportional to error in prediction of a salient events.",
            "Salient event were like turning on and off sounds, turning on and off music, turning on and off things that they were salient events.",
            "And the agent was going to learn options.",
            "Driven by this intrinsic reward to achieve those salient events was also going to learn to predict those salient events.",
            "There's an error in that prediction that was intrinsically rewarding.",
            "OK, so every time you experience a new salient event, you allocate a data structure to build an option for that.",
            "Sailing event.",
            "Build option meaning.",
            "Build a model.",
            "Build a prediction model and build a behavior model builder policy.",
            "OK, and so you every time you experience new sailing event, you're allocating sort of a skill for it and a knowledge for it.",
            "OK, and we were using this really simple idea, which is also something we talked about, which is all these options that we have initiated were being learned all the time in parallel from all the behavior that was being generated and things are called interruption learning methods and we use those to do these sort of things and then because we had these options that predicted what will happen when the option finished, we could do planning to connect back to planning.",
            "We could do planning with those option models to try to make value function updates to our behavior, so here's the picture.",
            "I want to paint for you.",
            "There's a behavior policy.",
            "That's being learned with the intrinsic reward.",
            "Their option and option models that are being learned driven by salient events.",
            "The behavior policies reward is error in prediction of assailant event.",
            "So we never encounter the sale independent, didn't predict.",
            "It feels rewarded.",
            "OK, and the Q function, the value function for the behavior policy is being used in a dynamic.",
            "Will be learning Edina.",
            "Like way you're learning from the steps in the world and you're learning from your option models that you're predicting so I don't know if anybody talked about diner, but it's a simple idea of mixing model free and model based model based reinforcement learning."
        ],
        [
            "OK, so here's a visualization of what happens.",
            "So the X axis is time, and these different sort of.",
            "Graphs are for different salient events.",
            "Ellon means light on Elof means light off.",
            "S on mean sound on.",
            "Music on music of Things of that sort every tick is the occurrence of a salient event.",
            "The height of the vertical bar is the intrinsic reward obtained when that salient event happens.",
            "So if the height is very is tiny, is this kind of height, then that means there was no intrinsic reward.",
            "So what you see is very early on intrinsically.",
            "Turning light on and off is a very simple skill.",
            "You just have to hit the light switch so lots of lights on and off are happening and it's getting surprised by them because it's every new state is different.",
            "So get surprised in this state you know when the ball is in a different place, a different state.",
            "The buttons are in a different place, different states or get surprised and it learns that.",
            "Then at some point, once it knows how to turn the lights on and off, it gets bored by that and tries to learn and get gets to learn how to kick the ball into the Bell to ring the Bell.",
            "That sound on.",
            "And then music on.",
            "Right now, of course, once it turns the music on.",
            "Then if it able to also turn the light on, that was a different state, so it gets surprised again.",
            "Right, so that's why the light on off intrinsic rewards don't go to zero overtime because it once it learns new skills, it can generate new states that hasn't experienced before.",
            "Because now it can be in a state where the music is also on.",
            "Yes.",
            "Yet all body short partial amount of error, so this means it got very surprised.",
            "And then it means good.",
            "So the reward is proportional to the error in prediction.",
            "So the height of the bar is better in prediction.",
            "OK, the this bar this little bar is mean just sailing event occur but there is no reward.",
            "So you can see a natural developmental progression.",
            "Easy things get learned.",
            "First they make it possible to learn harder things.",
            "Right and then they make it possible to learn still harder things in this progression happens overtime."
        ],
        [
            "And so there's a hierarchy of reusable skills right to the primitive.",
            "Actions were things like secured to random objects, cigar lighter random object, move marker.",
            "Do I do marker move, hand up and down North, South East, West?",
            "These are the primitive skills that were built in and then it can turn the light on.",
            "It cannot turn light off once it learns turns light on it can see the color of the light switches in turn music on.",
            "Once it can turn music on and turn the ring the Bell and turn the light off.",
            "It can then activate the toy.",
            "So this automatic hierarchy of things happen in a very simple, simple way, OK?"
        ],
        [
            "And you've seen graphs like this from Nando yesterday, which is a graph that basically says look learning all of these things in parallel.",
            "Ends up achieving much faster learning than only being only learning from intrinsic reward, where the intrinsics for extrinsic reward with extrinsic reward.",
            "Here was the hardest skill, which was to make the monkey activate the monkey toy monkey starts clapping or crying or something like that.",
            "OK, so you see this benefit that Nanna was talking about in all the same ways, exactly the same ways, but now I've given you a very concrete simple example of how to do this kind of continual learning and it's very simple domain."
        ],
        [
            "OK, so I want to wrap up this very simple example.",
            "I'm going to build on it a little bit more so in this very simple example we learn skills and options.",
            "We learn new knowledge, informed predictions, we reuse those skills to learn.",
            "We reuse previously, learn skills to learn more complex skills and the agent got more competent overtime at extrinsic reward.",
            "Now there are lots of caveats.",
            "This is this is, this predates all the deep learning, excitement, right?",
            "So it's extremely contrived.",
            "Domain intrinsic motivations were hardwired about hardwired salient events.",
            "Very limited form of intrinsic reward.",
            "Everything was going to look up tables, so there was no catastrophic forgetting issues and it was, you know, didn't scale very well.",
            "You can imagine so, but there were lots of people working on this sort of thing.",
            "At that time.",
            "Schmidhuber Kaplan.",
            "Oh dear through Sebastian tune and.",
            "Forget Bollerslev first name and many others, yes.",
            "So this was literally look up table.",
            "We're learning a mapping.",
            "We're learning a policy and look up table.",
            "We're learning the initiation set and look up table.",
            "We're learning the termination setting look up table.",
            "We're learning a prediction model option prediction model, look up tables.",
            "Everything was gonna look up table because it's tiny domain.",
            "We could do it, so there was no interference with options.",
            "There was no, no no, no catastrophic.",
            "Forgetting.",
            "All the interesting issues that come up with neural Nets weren't present in this early work.",
            "But but I think the pieces are all there, right?",
            "You're learning options, you're learning options in parallel, early learning of options leads to situations where you can learn harder options.",
            "And because you're building these option conditional predictions, you can plan.",
            "You can do both model based and model free learning.",
            "And piece it together to accelerate the learning of new tasks.",
            "Yes, is this accepted?",
            "That's by having different sub sub policies for skills.",
            "Or does it?",
            "Extensional that's value single node that worked out.",
            "Great question right?",
            "Both of them will have advantages and disadvantages.",
            "So you can certainly go that route.",
            "These were certainly completely disjoint options, right?",
            "You could you could imagine still doing that for neural Nets.",
            "Or you could imagine doing it in a joint way and then have to deal with interference.",
            "Things of that sort.",
            "But you might get better generalization faster learning, so you have their pros and cons.",
            "The tradeoffs with these things, right?",
            "OK."
        ],
        [
            "So now I'm going to take a extended diversion to really focus in on this question of where rewards come from.",
            "We've done some work on this that I think is.",
            "Lay some basic foundations of this that I want to share with you, sort of.",
            "Where do rewards come from and really focus on that specific question.",
            "This is joint work with a lot of people, including Andy, Bartow, Recloose and my students, Nattapong and Jonathan and Georgia."
        ],
        [
            "OK, so I want to explain a very simple idea to you, which I've used different words overtime to describe it.",
            "I'm going to call it here.",
            "Parameters, preferences confound.",
            "So.",
            "Again, the starting point of reinforcement learning is somebody gives you a reward function.",
            "When the continuing setting is not clear where the reward function comes from.",
            "But at least something is clear.",
            "The robot or the agent or the AI is acting on somebody's behalf.",
            "Well, it's not clear whether it's an individual human being or society, but let's ignore that question because that's a very interesting question in its own right.",
            "But imagine for now that there is this agent designer.",
            "The human being.",
            "That has preferences over the agents behavior like the robot doesn't have preferences.",
            "The human designer, the agent designer has preferences over robot behavior.",
            "And those preferences can be captured in the form of a reward function.",
            "Let's call that the objective reward function.",
            "So the question is, what should the agents reward function be?",
            "Because we are building our agent, so it's going to use rewards.",
            "To determine its behavior.",
            "So now there are two reward functions.",
            "The agent designers reward function.",
            "And the agents reward function.",
            "And our L confounds those two."
        ],
        [
            "One notion of reward function.",
            "The objective reward is a notion of preferences or evaluation.",
            "How good is the agents behavior for the agent designer?",
            "The second notion of reward function is a guidance reward function.",
            "There parameters to the agent behavior.",
            "If I take your favorite RL algorithm.",
            "And I give it a reward function.",
            "It will generate a behavior if I change the reward function a little different behavior or different distribution or behaviors.",
            "So there are two reward functions is an evaluation of preferences notion of reward function to call objective reward function and then a subjective or an internal reward function that guides the agent's behavior and in standard PRL this is confounded.",
            "But they don't have to be right, and so and so we have so."
        ],
        [
            "By the way, this you've seen this picture before.",
            "I imagine this is the this is the standard view of RL in which the rewards come from the environment.",
            "This is a more sensible view of an Organism and maybe we should build artificial agents this way, in which the reward comes from a critic inside the inside the agent.",
            "OK, so I think you've seen this view, so I won't spend much time on it, OK?",
            "There are lots of approaches for designing reward.",
            "Yes.",
            "Yes.",
            "No, in this figure I'm only showing the internal reward function because I'm not showing the human designer in this figure at all.",
            "This is just the agent, the artificial agent.",
            "Did you reference a publication?",
            "Yes, this actually.",
            "This is this in Sacrament barcode book.",
            "Now I'm not sure there is there is a. I have a paper called where the rewards come from.",
            "If you look for that, you'll find it.",
            "But I don't think it's new to that paper.",
            "Inverse reinforcement learning is a very interesting idea.",
            "I'll come back to that in the next part of this reward shaping."
        ],
        [
            "Lots of people work on that preference elicitation mechanism.",
            "Design lots of approaches to designing rewards."
        ],
        [
            "So here is the formal thing I want to communicate to you, and I won't spend terribly much time on it.",
            "It's called the optimal reward problem.",
            "So the optimal reward problem takes this confounds seriously.",
            "There are now two reward functions.",
            "The thing that's given.",
            "Is the objective reward function or the agent designers reward function or suburban?",
            "You don't get to muck around with that, because that's the agent designers reward function.",
            "And then you have a internal reward function or some I.",
            "Which is the agent's reward function.",
            "Right, and you give that the agent is G and it's parameterized by.",
            "This are so by and you can give it whatever you want and hear all the other parameters of the agent.",
            "So maybe it's doing Q learning with the neural net and all with certain learning rate.",
            "All the other parameters are in Theta or sub.",
            "I is the reward parameterisations and you put in some environment in an environment envy and it produces an interaction it trajectory H from that environment and this agent.",
            "The utility of this trajectory H or history H to the agent is use of my of H which sums the intrinsic reward.",
            "The utility of that same trajectory to the objective agent which is the human is of course some of the objective rewards.",
            "So the optimal reward problem is to define the reward function.",
            "Are I star so that if you give it to the agent?",
            "It generates a trajectory that optimizes the human designers objective utility.",
            "So this is the optimal rewards problem.",
            "Like I've now set up a mathematic mathematical problem that in some sense explains where rewards should come from.",
            "And we have written a paper and I'm in which you know, we look considered natural agents where would?",
            "So who is the agent designer in natural agents?",
            "Evolution, evolutionary objective reward function is passing on of jeans, right procreation?",
            "But evolution has reached inside our brain and designed an internal reward function.",
            "Had designed an internal reward function that makes us very successful given our particular the rest of our agent architecture.",
            "So the difficult challenge in solving the optimal reward problem is the choice of optimal reward is a fundamentally function of the agent, and it's bound and its capabilities.",
            "Different agents you would give a different reward function too depending on what the rest of the agent architecture is, is crucially a function of the rest of the parameters Theta.",
            "Of the agent.",
            "OK, so instead of a mathematical problem, how do you solve this problem?",
            "But if you could solve this problem, it would derive the best reward function for you.",
            "So The thing is, you want to design to really do AI to really build a continual learning agent.",
            "What you have to do is somehow.",
            "Define a reward function at just the right level.",
            "If it's too sparse and too infrequent, much more like the objective reward.",
            "Then the agent will not succeed in its in its lifelong learning experience.",
            "On the other hand, you can make it too precise, two detail.",
            "But then the agent will be inflexible.",
            "It won't be able to deal with the variation of environment could find itself in.",
            "So somehow to the right level of reward.",
            "That allows the agent to be most successful over the distribution of environments it could find itself in.",
            "So it's ready to learn the reward that makes it ready to learn.",
            "Effectively, in the distribution of environments in so designing a good reward function may actually be.",
            "And underexplored way of learning to learn.",
            "Anyway, sorry, just trying to connect to Nando."
        ],
        [
            "OK. Look at time.",
            "Let me give it again.",
            "I'll give a toy example predating neural net days or not, that's that's inaccurate, predating the deep learning craze.",
            "And then I'm going to give you a deep learning example in a minute.",
            "OK, so here's a toy world.",
            "Very, very simple world in which there's an agent shown by a circle, and all there is in the agent in the room are two things, a worm.",
            "That the agent could eat and get small amount of measly reward, or could pick up the worm and go to the fishing hole at the bottom and get fish and eat fish, which is nicely rewarding.",
            "The agent is going to be Q learning agent.",
            "So what are what?",
            "Should you give the agent?",
            "The objective reward is the the worm has a very tiny amount of positive reward 0.04.",
            "But Fisher that order one.",
            "So that's the objective reward order.",
            "What should you give the agent now if the agent is a look up table Q learner with arbitrary large amounts of time to learn.",
            "It's clear if the agent is unbounded.",
            "What you should do is clear given the objective reward function after all, you'll end up optimizing it.",
            "So we're going to make the agent."
        ],
        [
            "Bounded?",
            "To simulate the notion of real world agents, even though this is a toy problem, the agent can be unbounded is going down at what kind of boundary we're going to.",
            "We're going to look at bounds of finite lifetimes.",
            "So the agent may have only a small amount of life time to learn.",
            "So now if you solve for the optimal reward problem, you get this kind of behavior in which the X axis here is the lifetime of the agent.",
            "So each point is a different agent with different amount of lifetime.",
            "If the agent lifetime is very short.",
            "The best reward turns out to be to slightly prefer if they don't like time is very short.",
            "It doesn't have time to learn to fish because the fish and actually go to the worm find the worm, pick it up, not eat it, and keep carrying it, not eating it to the fish and catch the fish and then eat the fish.",
            "That's a more complicated process to learn, right?",
            "So very short lifetime.",
            "The red bar is a reward for the for the internal reward for eating the warm, so to speak, and the blue one is for eating the fish or speak so very short lifetime you have a slight preference over you.",
            "Give a slight preference to the war man, you say fish.",
            "You can't even get to the fish.",
            "You never pick up the fish.",
            "If you are middling lifetime.",
            "Then it learns to say eating fish is really bad.",
            "Because you don't want to be distracted by eating the fish.",
            "If you happen to eat fish.",
            "You'll get distracted by that and you will not even pick up.",
            "Get enough food from the worm reward from the world so it says eating worm is really good.",
            "Eating fish is really bad and now if you have any with any cross, the larger lifetime now it has enough time.",
            "To have a sensible reward function which is eating fish is really good.",
            "Eating worms is really bad.",
            "Yes, hang on.",
            "Let me let me finish this train of thought.",
            "Then I'll answer questions.",
            "So here's the performance curve you get again with lifetimes, with short enough lifetime basically.",
            "With the intrinsic reward, it learns to eat the slight difference about 3 up to this point.",
            "This is the point at which can learn to fish with the intrinsic reward, the red curve.",
            "This is the point it would learn to fish with the extrinsic reward.",
            "So because of bounds.",
            "An adapting reward it can learn to fish earlier by adapting the internal intrinsic reward.",
            "So I've kept everything else fixed.",
            "It's the same Q learning algorithm, same hyperparameters.",
            "I'm showing you the objective utility on the Y axis.",
            "I'm showing you the effect of tuning if you like the internal reward functions, we can learn to fish much faster, so you can learn, so that's an accurate statement.",
            "It can learn to fish more effectively in the horizon if the lifetime is shorter.",
            "With intrinsic rewards, then with the objective yes, go ahead.",
            "But I guess it still learning and it doesn't know it to look up table it it sees it every state is it's mark off.",
            "It doesn't.",
            "It doesn't have a model number, it doesn't have a model number in Q.",
            "Learning in this one.",
            "Group that just disappeared.",
            "What were the two?",
            "Yeah, the two curves were preference.",
            "The internal reward.",
            "The magnitude of internal reward for eating the worm.",
            "If you like, there's the red one and eating the fish, which is a blue one.",
            "So it's learning.",
            "It was a short lifetime.",
            "It learns to.",
            "Have slight preference for the worm, but crucial one is a middle lifetime because the short lifetime it can't learn very much anyway.",
            "The short light with the middle lifetime, it really says I shouldn't be distracted by the fish so it gives it a large negative reward for eating the fish.",
            "What's the weather?",
            "The Y axis is magnitude.",
            "They're two different things.",
            "The X axis is lifetime X axis lifetime.",
            "the Y axis is the reward coefficient.",
            "So it's positive high positive for the warm, the red one and blue.",
            "Is the coefficient for eating the fish?",
            "This is like.",
            "In the optimal reward function, exactly right.",
            "Thank you for helping clarify that.",
            "Yes, coefficients for the optimal the reward function is a function of some features of state.",
            "And this is the coefficient for that.",
            "I'm just giving you a quick intuitive picture.",
            "OK, I want to so now."
        ],
        [
            "Anne.",
            "Let me skip this."
        ],
        [
            "So now let's Fast forward to the deep learning days, and so we did in 2010.",
            "We came up with an algorithm for solving the optimal rewards problem, and the algorithm is a very straightforward idea.",
            "I won't even spend much time on it.",
            "Again, the perspective is very simple.",
            "There is an objective utility that we care about.",
            "We're going to treat the internal reward function as a parameter isation of a policy.",
            "The internal reward function.",
            "Given an RL agent, will generate behavior in an environment.",
            "So if I change the reward function, I'm changing the behavior of the agent.",
            "So it is a parameterisation of a policy.",
            "Non stationary policy, but policy nevertheless.",
            "Right, so I can do policy gradients.",
            "With this I'm going to do a gradient of the objective utility with respect to the internal reward parameters.",
            "So what I need to do is do a gradient procedure through.",
            "The algorithm that the agent is going to use to convert the intrinsic rewards to behavior.",
            "As long as that process is differentiable, I can differentiate through it.",
            "To adapt my internal reward parameters to climb the gradient with respect to the objective utility.",
            "Make sense I have an objective utility.",
            "You've all seen policy gradient.",
            "Peterbilt gave a great talk on policy gradients.",
            "Right now I have but but reward parameterisations are typically thought of as neural net parameters.",
            "Here I'm thinking of sorry policy parameters that are typically thought of as neural net parameters.",
            "Here I'm thinking of the policy parameters as the internal reward function.",
            "Which is going to be translated by some procedure into behavior.",
            "As long as that procedure is differentiable, I can do policy gradients to learn good reward functions.",
            "Yes.",
            "This is like the design."
        ],
        [
            "Is messing up with the with is looking internally at the agent, so there is another agent go against opening up.",
            "See at the Valley.",
            "Or yeah, yeah, I mean except except that is a meta algorithm, right?",
            "It's like a policy gradient algorithm, meta algorithm.",
            "It's the meta algorithms inside the agent outside the agent.",
            "It's really it's computational.",
            "It's tweaking the.",
            "So what we literally."
        ],
        [
            "I did and I don't want to show you've seen these things before."
        ],
        [
            "Going to show you this picture and then show you some results.",
            "So what we did is we did UCT or look ahead search.",
            "For planning an Atari games.",
            "So not learning I'm doing planning right?",
            "And so you see down there looking at search is a differentiable procedure and you can very scalably differentiate through it.",
            "Just turns out what you do to do value backups.",
            "You can do gradient backups.",
            "So you can do gradient backups through.",
            "You look at search for UCT.",
            "So what we did is basically we gave it instead of having hardwired features for reward functions, we gave it a usual frames as inputs to a neural net whose output was the reward function to use with UCT.",
            "So let me say that again because unusual setting.",
            "We're going to give it images of the current situation.",
            "We're going to pass it through a neural net whose output is going to be the reward function.",
            "To assign to the current situation to any situations you feed in.",
            "And that is going to be a reward function.",
            "That UCT is going to use to do its planning.",
            "And we're going to backdrop to the whole thing through UCT and through the neural net to learn the reward function.",
            "Anne Anne."
        ],
        [
            "Straightforward, but what I wanted to show you was.",
            "That we tried this in 25 Atari games and we did the ratio of the performance with and without this intrinsic reward and the ratio is more than one.",
            "One is the vertical line and in most games it led to improve performance for UCD.",
            "But this was a little bit of cheating because we were giving extra because you see, is compute hungry and of course to compute the intrinsic reward.",
            "Also takes computation when we correct for that to have a more fair apples to apples comparison."
        ],
        [
            "Change a little bit, but still we get a win in most in most cases.",
            "Here we balance computation because you can do deeper UCT, wider UCT.",
            "For the time you would take to compute the reward function at every every step, so balanced out the time you still get a win, yes.",
            "Yeah, sorry, didn't nobody's talked about you City.",
            "OK, UCT is basically look ahead search so.",
            "So one way to plan would be on a current situation.",
            "You take actions in the real world you so you you simulate actions and you simulate next states.",
            "You could get to and you build out this tree.",
            "So look ahead search would be building our history and backing up values, but you can't do that because the branching factor is too large and you can't go very deep if you use it is a clever way and troubles here.",
            "I saw him walk in, he's one of the inventors of the basic idea behind UCT.",
            "It's it's you sort of banded bounds to help determine.",
            "Which actions are worth exploring in this tree so you can get a much more efficient look at search algorithm?",
            "OK. All."
        ],
        [
            "So that was a quick quick.",
            "Introduction to the notion of optimal rewards and how to use them in any kind of agent in which we.",
            "In which we would have differentiable procedures that map reward functions to behavior, right?",
            "Any, any, any procedure that Maps rewards to behavior in a differential way.",
            "You could then use idea of policy gradients to derive reward functions for by taking great inspected the objective utility.",
            "OK, so now I'm going to take a few minutes to talk about the theoretical piece of work that I'm actually quite excited about, yes.",
            "Right in the in the.",
            "In the.",
            "Car games work.",
            "The the input to the reward function was frames, right?",
            "So we gave the last four frames and output.",
            "The scalar just tells you what is the reward associated with this situation.",
            "That's right where we have this neural net that takes observations.",
            "Gives it a reward that is then used by the UCT procedure in conducting its look ahead search.",
            "We're doing we doing policy.",
            "We doing gradients with respect to the objective function reward function.",
            "We care about true UCT through this neural net.",
            "So what's the only thing that's being learned is the reward function?",
            "Because you see, is a fixed planning procedure.",
            "So what we're showing is that.",
            "That repeated planning can be improved.",
            "By adapting the reward function.",
            "And the performance gain that was showing you was after learning a good reward function.",
            "Yes.",
            "On the get the actual result, right?",
            "Learning as you are generating no.",
            "I think if I remember correctly, it's been a few years now.",
            "We learned the reward function and then we compared the performance giving.",
            "Giving straight UCT more enough computation to match the time it takes to take an action with the learned reward function in the signal.",
            "Now you are.",
            "The agent actually.",
            "Well, so you have effectively like doubled."
        ],
        [
            "So so could I improvement just because you're more trainable parameters and the more powerful so in the you see there are no trainable parameters.",
            "You see the planning planning algorithm.",
            "Right, so I then use a learning procedure to gradients tinana reward function that's fixed.",
            "So now again there are no trainable parameters.",
            "Now compare these two agents I have trained to reward function one agent.",
            "The algorithms are the same.",
            "It's the same agent.",
            "UCT is one has a learned reward.",
            "Function 1 uses the objective reward function.",
            "And I'm comparing the performance of these two except to make it fair, the one that's using the objective reward function because it doesn't have to computer award function is expensive way through a forward pass through a neural net.",
            "I give it more computation, but you know I'm at the computation per time step.",
            "Yes.",
            "That should.",
            "Yes.",
            "Good.",
            "Good.",
            "Or do you think that because you have these two levels of reward that can handle now you completely right?",
            "All the work that we've done in optimizing rewards in this way is all for planning.",
            "There's a good reason, right planning is a fixed procedure.",
            "To do this for Q learning or learning algorithm would be a much more interesting challenge, because exactly you're exactly right.",
            "In fact, I would theorem.",
            "I never wrote in my head which is.",
            "It's pretty clear that the optimal reward should be a function of the entire history.",
            "You can't be even if the underlying observation is Markov, the optimal reward will have to be a function of history will have to be non Markov exactly.",
            "So there's a theorem in my head that I've never really settled down writing.",
            "I don't think I can think of a clever use of it, but but anyway, but that's I think."
        ],
        [
            "Very right?",
            "OK, I want to take.",
            "10 minutes and describe some work that I'm quite excited by.",
            "Do a little bit of theory.",
            "Java did a lot of death theories.",
            "I wanted to connect to Java little bit by doing a bit more theory.",
            "And this is going to connect to.",
            "Inverse reinforcement learning.",
            "Except I'm going to bring it one step closer to continual learning broadly.",
            "A small step by looking at this sort of what I'm going to call repeated inverse reinforcement learning.",
            "OK, this is work by non Zhang and Karima mean.",
            "Student in the stock."
        ],
        [
            "So inverse RL.",
            "Inverse RLI know.",
            "You've heard brief bits about it, so inverse RL is the forward problem in reinforcement learning is you're given a reward function.",
            "You produce a behavior.",
            "The inverse problem is given up.",
            "Sorry for problem is given a reward function, produce the optimal behavior.",
            "The inverse problem is given up.",
            "Given a optimal behavior, an optimal behavior in further reward function.",
            "That's the inverse reinforcement learning problem.",
            "Beautiful, intellectually, very beautiful problem, defined band rowing and Stuart Russell and Peter Beale Andrew have done some."
        ],
        [
            "At work with this, the bad news about inverse reinforcement learning that it's fundamentally defined in that the forward problem is many to one, many reward functions will yield the same optimal behavior.",
            "And those of you know if they can discrete math classes.",
            "When you invert a many to one function, you get one too many.",
            "It's not a function.",
            "Right, so you can't impose problem.",
            "OK, so for example give you a concrete example.",
            "Here's a gridworld.",
            "You observe the behavior indexed by the shown by the actions.",
            "Well, here are two possible reward functions, right?",
            "One possible reward function is the big social reward, the bottom.",
            "Left corner and then trying to get there.",
            "Or there could be a reward in the two blue places and it picks up the reward along the way to the bottom and you can imagine gazillions of such reward functions that will all lead to this kind of behavior.",
            "So it's fundamentally imposed OK."
        ],
        [
            "So then how does?",
            "How does inverse RL even get off the ground so the way inverse RL get off the ground and I won't spend too much time on it is basically you can very nicely and elegantly define the space of reward functions.",
            "That are consistent with that behavior.",
            "So it turns out there's a set of linear constraints defined here, and I won't spend time on this.",
            "Which basically define all the reward functions.",
            "For which the observed behavior is optimal in that particular environment.",
            "So that's nice.",
            "That is a space defined by a set of linear functions.",
            "So, but how do you use it then?",
            "In reality, the way use it is you add a heuristic and you get some point in that space.",
            "And then you can use it to generate behavior and this works a lot of people use it, it's great.",
            "But I'm interested for today in this scientific problem of actually inferring a reward function.",
            "Truly in for in the reward function.",
            "And I'm going to set it up in this lifelong learning setting."
        ],
        [
            "As follows.",
            "So I'm going to connect it to AI safety to lifelong learning to continue learning.",
            "In this one slide.",
            "So let's imagine now that this robot is going to act on behalf of a human over the lifetime of the human.",
            "And the human has.",
            "A intrinsic reward function which is complicated.",
            "It involves things like I don't want to harm humans.",
            "No breaking of laws, cost considerations, social norms, general preferences, all kinds of things that a human internally has a sense of reward function.",
            "It's very hard to communicate all of that explicitly in a good reward function.",
            "To the agent.",
            "Write the whole premise of inverse RL.",
            "It's difficult to communicate reward functions, but it's easy to show behavior good behavior.",
            "OK, so here is the amended setting.",
            "I'm going to have this lifelong learning setting where the agent is going to experience a sequence of environments or tasks.",
            "What I want is the agent to become better and better at doing what the human would do in those environments.",
            "Using inverse reinforcement learning.",
            "So I'm going to define this new environment or task with a environment ecity sort of a think of it as a.",
            "It controlled Markov processes, actions, dynamics, everything is defined.",
            "And I'd define a task specific reward function like go get me food.",
            "Now you tell the agent to go get your food, and that's the only reward function you give.",
            "You might cook your cat.",
            "But you say no, no, I didn't mean you cook my cat.",
            "It's bad to go cats things that sort right.",
            "So that's the intrinsic rewards you like your pets.",
            "You like your children.",
            "All this kind of stuff you don't want to give.",
            "You don't want to give all these explicit reward functions to the agent.",
            "So every task is it is it comes in environment and pass.",
            "Specifically work with me.",
            "Go cook me food, clean my house, whatever.",
            "If you have a household robot and so the assumption is that the human beings behavior will be with respect to the sum and this is an arbitrary well, not an arbitrary choice, but I'm going to assume that it's the sum of the humans behavior is going to be the sum of the task specific reward and its intrinsic and the intrinsic human reward.",
            "So the humans behavior and environment eastep TR safety will be optimal with respect to our sub T plus Theta star.",
            "So the agent is going to know everything except datastar.",
            "It doesn't know Datastar, you'll know he sceptile are subte Yeah, have we not seen this notation?",
            "Exercise of state space?",
            "Is action space piece of T is a transition dynamics transition matrices are shipped is a vector of task specific rewards for states.",
            "Data Star is the intrinsic reward for states.",
            "Gammas are discount factor, so now this fully defines the MDP from which the human would generate behavior.",
            "And I want the robot or the agent two as quickly as possible.",
            "Come too.",
            "Do behave as the human would behave.",
            "OK, so the question is the mathematical question is can we learn datastar from optimal demonstrations on a few tasks or more importantly, and more relevantly can be generalized to new tasks.",
            "Yes.",
            "Negative one to plus one.",
            "So I'm going to assume that rewards are bounded.",
            "Different task.",
            "It had really widely.",
            "I'm going to come to that in just one second.",
            "I'm going to address that exactly in one second.",
            "Can't be one second 'cause once it comes over in a minute or two.",
            "Something I wanted to say I totally forgot I'll come back.",
            "Right, the crucial thing is to generalize as quickly as possible, hopefully."
        ],
        [
            "So now I'm going to address Joel's question, so let's look a little bit more carefully at the notion of unidentified ability of reward function.",
            "There really two types of identifiability, representational and identify ability and welchol, behavioral or experimental infallibility.",
            "One of them is uninteresting of the sort that Joel just asked about, which should be ignored.",
            "Not jewels question, but the idea that rewards can be scaled and the second one is more interesting, which is the environmental and should be dealt with.",
            "So let me be on my."
        ],
        [
            "Precise we defined this notion of what I would call behavioral equivalence.",
            "Two reward functions are behaviorally equivalent.",
            "If there is no no no control Markov process.",
            "In which they lead to different behavior.",
            "So if I take a reward function and add a constant to it.",
            "It won't change their behavior in any environment.",
            "No matter what the dynamics of the environment.",
            "And depending on certain other things, if I multiply by scalar, it won't change anything.",
            "So I don't care about identifying the reward functions except to identify which behavioral equivalent class it belongs to.",
            "So Gerald, that's the answer to your question.",
            "Right when I say identify reward function, all I want to identify is some Canonical reward function in a behavioral equivalent class.",
            "So all reward functions that lead to the same behavior in any possible environment or in the same class.",
            "And I shouldn't care about identifying one of them, yes.",
            "I may not care about any possible environment, but I might have some subset of environments that I care about, so right right now, so you can define behavioral equivalence with respect to a set of environments.",
            "I'm just saying any environment right now, but the notion of being equivalence will be will depend on the environments you care about Babel equivalence over.",
            "It could be a Singleton.",
            "Could be a Singleton.",
            "OK, so let's do 2 quick.",
            "Yes, quick question.",
            "No, the yes you can write for a subset of environment.",
            "There might be a subset of environments where there are nontrivial changes.",
            "Reward functions that don't change behavior.",
            "But if you truly allow any environment, then the kinds of behavior equivalences.",
            "Are only trivial.",
            "Trivial incentive, you can write down what that space contains, but if you constrain the class of environment then you can get nontrivial classes."
        ],
        [
            "Good question, OK?",
            "So I'm going to do 2 settings.",
            "The first setting is that just for theoretical reasons, you can imagine a setting where the agent gets to it.",
            "Present, choose the environment, choose the task and say here's the task human tell me what you would do.",
            "That's a very powerful agent.",
            "Right, get it.",
            "Choose the environment for the human.",
            "If you do that, it turns out it can be very efficiently learn.",
            "A good approximation to Theta star.",
            "Basically, you can do preference elicitation if that makes sense to people, right?",
            "If you truly, the agent can truly choose the environment, then the agent can choose wacky environments and really learn in parallel, and you can get this log arhythmic performance in just a quick, very, very efficiently learn the optimal optimal reward function, yes?",
            "Yes, so good.",
            "I'm assuming that every reward function has been canonicalized.",
            "That is, I've picked a Canonical element.",
            "So for example what we literally do is imagine that take an arbitrary state fix its true reward function to be 0.",
            "And scale everything accordingly.",
            "So we picked the Canonical.",
            "We implicitly ameda Canonical assumption of reward functions.",
            "It matters how you do that right?",
            "Not for, not for this theory doesn't really matter.",
            "Can you speak in skilled about credit so like you can just scale one up?",
            "Yeah, but again you're right.",
            "But the IT matters in the sense that.",
            "This is a worst case result overall canonicalization's.",
            "Let's put it that way.",
            "This is a very trivial trivial result."
        ],
        [
            "And I won't.",
            "I won't go through the proof, but I want to get some more interesting things.",
            "Then the issue is suppose the agent is not this powerful.",
            "So now now it's a much more interesting setting.",
            "Something else nature is choosing the tasks that the agent has to do.",
            "So now I'll just give you one insight and show them and give the main theorem and then you're welcome to look at the paper.",
            "So now the following insight right?",
            "Nature is choosing tasks.",
            "Now, one of two things will happen, and so the setting now is not.",
            "I'm not going to learn the truth Data Star because now nature is choosing tasks I may never take me to.",
            "Interesting places to learn Theta star.",
            "So now my measure is how often do I make a mistake, so the setting is nature gives me a nice FTR subte.",
            "The agent proposes a policy.",
            "The human being looks at policy and says yeah, very good policy.",
            "Or no bad policy.",
            "Here's what I would have done.",
            "Every time the human being has to show a show, a demonstration.",
            "I think of that as they age and making a mistake.",
            "What I want to do is bound the number of mistakes the agent would do.",
            "Settings clear later chooses task.",
            "Something chooses task.",
            "You can choose tasks adversarially.",
            "Every time a task is chosen, the agent says this is what I'm going to do.",
            "Human being says great.",
            "Because it's what I would do or close to what I would do or says no, no, I wouldn't do that.",
            "I would do this every time the human says no, no, do this.",
            "That's a mistake.",
            "We want to count the number of mistakes.",
            "OK, so I'll just give you 1 inside of the present, the main theorem and be done.",
            "The inside is.",
            "If the.",
            "Environments are chosen in such a way.",
            "That the agents behavior is right behavior.",
            "For example, is the same environment every time.",
            "Then the agent will not learn more about Datastar, but it's not going to make mistakes either.",
            "So every time the agent makes a mistake.",
            "The proof mechanism shows that it learns enough.",
            "About the truth datastar to make an improvement and make fewer mistakes later.",
            "So it doesn't make a mistake.",
            "Great, no mistake.",
            "If it makes a mistake, it shrinks the volume of possible rewards enough that you can get a nice result out of it, and I won't.",
            "I won't spend more time on this, and basically we have a result that."
        ],
        [
            "It looks like.",
            "It's basically the ellipsoid algorithm applied to this.",
            "For those of you, that's meaningful.",
            "Volume shrinks in the total amount of the total number of mistakes is order D squared, logged by epsilon.",
            "Epsilon is the sort of how suboptimal it is when you come before you count a mistake and D is the dimension of the reward space.",
            "OK."
        ],
        [
            "I will skip all."
        ],
        [
            "This.",
            "Now I'm going to switch to a different topic that also connects with Nando.",
            "You can see that I really made an effort to follow up on Lander.",
            "In my talk.",
            "And we talk about some work that's going to appear.",
            "Acnl 2017 but we did this work a year and a half ago.",
            "Great thanks to the reviewers helping make it better or iterations.",
            "Anne.",
            "So I."
        ],
        [
            "Don't need to motivate zero shot learning ladder.",
            "Great job.",
            "Rapid generalization is key to continue learning.",
            "The setting I'm going to be in is setting where tasks the motivating setting is.",
            "Imagine a household robot.",
            "You go to Amazon.com.",
            "Your robot comes to your house.",
            "And you're going to ask it to do things he's going to learn to do.",
            "Things are going to communicate using language.",
            "Very simple natural language in this setting.",
            "Going to give it asks for my clothes then then make dinner.",
            "Then go sit in a corner.",
            "Whatever.",
            "Something like that right?",
            "And."
        ],
        [
            "OK.",
            "I'm thinking about what to say here to make it fast.",
            "Here's a gridworld and it has objects in it.",
            "Anna task might be visit the cow, pick up the diamond, hit all the rocks, pick up all the eggs.",
            "So now the sequence of instructions he construction has basically.",
            "At most three things, a noun.",
            "A verb.",
            "An account.",
            "Could be all, could be two.",
            "Pick up two eggs.",
            "Things of that sort.",
            "Ann to make the task interesting, there's sometimes random events occur, like somebody.",
            "Somebody rings the doorbell, or in this case some magic box appears, and when the magic box appears, you can get a nice reward by interrupting whatever it is that you're doing.",
            "An opening that box.",
            "So that's the setting the setting is.",
            "You give a list of instructions.",
            "The instructions of nouns, verbs and counts.",
            "And there's some background random thing that you have to maintain that if some background event random happen happens, you interrupt whatever you're doing and go deal with it.",
            "So that's the setting OK?"
        ],
        [
            "So what are the challenges here?",
            "Well?",
            "There's a combinatorial number of tasks, so just training all subtasks isn't really possible, so have to generalize to unseen subtasks.",
            "You also have to generalize to unseen lists of subtasks of tasks.",
            "You also have to decide when something is done before you can move on to the next.",
            "Right, so for translate natural language to what the behavior should be, you have to learn when that thing is done.",
            "And we also have to delete deal with really delayed rewards because in our setting we only get rewarded the entire list of instructions is finished.",
            "So it made it harder.",
            "Right?",
            "So again, the agent is going to be given a list of instructions is going to do its thing.",
            "Either it does the instructions exactly correctly.",
            "Consider plus one if it doesn't, it gets minus one or something like that.",
            "So really, really challenging problem.",
            "But we want to be able to do is train it on some lists and then show that can generalize to unseen lists.",
            "OK, so."
        ],
        [
            "To build a hierarchical architecture, it has a multi task controller.",
            "And a meta controller.",
            "The multi task controllers job is takes arguments from American Troller subgoal arguments and produces actions and produces an estimate if it's done or not is it has a terminated or not as the as the sub task is being given finished or not.",
            "And of course, everyone gets observation.",
            "This multitask controller gets the raw observations and the meta controller gets their observation, but also the list of instructions which is.",
            "Sure, which is denoted.",
            "Here is a gold box.",
            "Anna."
        ],
        [
            "Data Controller has to decide what subgoals given.",
            "Also remember it has to somehow keep track of where it is, what it's doing, when it's done, move on to the next.",
            "Thing is to learn all that kind of fun stuff."
        ],
        [
            "So a subgoal is composed of multiple arguments, right?",
            "Nouns and verbs things."
        ],
        [
            "Sort, so here is the multi task controller architecture.",
            "It's a it's.",
            "Let me just finish this."
        ],
        [
            "Index observation and subcode arguments as input produces a primitive action, and predicts, with the current state is terminal not.",
            "It takes the subgoal arguments.",
            "And basically writes weights that are a function of the subgoal arguments.",
            "And those weights that are predicted by the subgoal arguments are used to generate action observation, so you've seen architectures like this.",
            "It's an adaptation of standard ideas into this the other."
        ],
        [
            "Sort of thing that we had to do to make it work is we use what's called analogy making regularization which basically says we enforce the subgoal argument representations so that.",
            "The difference between the subgoal representations of pick up A and visit a is close to the difference between pick up B and visit be.",
            "And similarly the difference between visit being visit day is close to pick up being pick up a.",
            "So we.",
            "Injected a objective function."
        ],
        [
            "Which I'm going to briefly flash up that.",
            "Encourages Subgoal argument representations that.",
            "Generalize well.",
            "OK, I'm the deep learning people use this all the time.",
            "It's a mother all the time, but there's an idea that we've incorporated."
        ],
        [
            "OK, so the multi task controller is trained first.",
            "On simple subtasks with using PRL objective analogy, making objective Anna termination prediction objective.",
            "So if there are objective, these three objectives are combined and we trained the multi task controller on a subset of all possible sub tasks.",
            "After"
        ],
        [
            "Train the multi task controller.",
            "Then will train the meta controller.",
            "Now on lists of subtasks.",
            "OK."
        ],
        [
            "The Meta Controller architecture is is slightly different.",
            "It stores the list of instructions in memory.",
            "It has a memory pointer.",
            "Which manipulates as an explicit action.",
            "But in order to do temporal abstraction, one of the tasks, one of the things that the multi meter controller does is it basically has an output explicit output that.",
            "Predicts whether it should update the sub task or not.",
            "An if it if it."
        ],
        [
            "It doesn't need to."
        ],
        [
            "If it needs to update then it is back proper's entire architecture and if it doesn't need to."
        ],
        [
            "Update that it has a similar architecture.",
            "So I know I'm being very fast, but all the details are in the paper what I?"
        ],
        [
            "I'll show you is basically how does it do?",
            "So here's 0 short generalization, so let's take a random agent first.",
            "Done following Nando's advice.",
            "Here is the observation, so it's in Minecraft world.",
            "Here is a top down view.",
            "The green box shows what the agent the Meta Controller, thinks the point of the meta controller.",
            "The green box on top there is what some arguments have been given to the sub task multitask controller.",
            "OK, so now here it's going too fast.",
            "So our agents play on training instructions.",
            "Let's see this one.",
            "So pick up two pig.",
            "It's going to go to the pig.",
            "Pick it up.",
            "Now it has transformed.",
            "3 cats will go to the cat.",
            "Will transform it into something else.",
            "Go find now a magic box appeared.",
            "The random event happened on top even though the pointer still says transform 3 cash.",
            "The task above changes to transform box.",
            "It does that, keeping the pointer transform.",
            "3 cats goes back and finishes that task.",
            "It has to find the cat first process recat, then it'll go visit a horse.",
            "And then we'll go with the sheep.",
            "OK, so that's how it does on and then I'm going to show you generalizations longer instructions.",
            "So go faster so you can see what it's doing is maintaining.",
            "By the way, the meta controller has to take say something like.",
            "Pick up three things and internally keep track of how many it's picked up.",
            "Right, and we use recurrent architectures in the meta controller in order to keep learn how to count.",
            "And keep track of these sort of things so you can see the arguments being given to the multi task control on top where the meta controller thinks it's at right now and this is partial observe ability right?",
            "Because of all that, because Minecraft domain with the first person view.",
            "And there you go.",
            "That's the task.",
            "OK, and I'm going to take questions if there are questions, yes.",
            "Previous slide that's alright."
        ],
        [
            "Which slide?",
            "The you want to ask meta controller.",
            "About the multi task controller.",
            "Higher level controller, but the higher level controller gets the observations.",
            "Has this right?",
            "The weights depends on the.",
            "Current subgoal, that it has already given.",
            "The.",
            "The output of the multi task controller, whether it thinks the task is completed or not.",
            "These are the subtasks arguments it producing.",
            "It has representation of the current current Golar Sepetys coming out from there.",
            "It has an explicit output that says am I going to update my sub task or not?",
            "If it chooses to update then it does backdrop to this if."
        ],
        [
            "Chooses not to update then it doesn't update the connection breaks between.",
            "We stick with the previous subgoal, so it learns whether to update the subgoal or not.",
            "And so we get very fast, rapid, you know, 0 short generalization to unseen tasks.",
            "By training on a subset of the tasks, how we're doing for time, I have one more thing to talk about.",
            "So that's the background task.",
            "The background task is when a box appears.",
            "It has the opportunity to earn some reward by interrupting whatever it's doing and going in.",
            "Touching that the idea is to model the notion that there is something that you need to maintain.",
            "So you might be given a task of, you know, for my clothes household robot you know cook my dinner, but there's a background task.",
            "Keep the baby safe the baby is crying you go attend the baby no matter whenever they whatever else you're doing baby comes first kind of thing as an example, motivation.",
            "Yes.",
            "Fails, or when it doesn't.",
            "So there are many ways it can fail it can.",
            "It can move on to the next subgoal before it's finished.",
            "You can change this up task arguments before the previous task is finished.",
            "Counting can fail.",
            "You know it needs to pick up three things, but it only picks up two or Project 4.",
            "Accounting can fail because meta control, as has the sub task controller only.",
            "We never stay the sub task controller pick up three things.",
            "You only say pick up pig so they pick up three pigs, then electric pick up pig, wait for termination, pick up pig, wait for termination.",
            "Pick up pig right has to do all that so it can fail in many.",
            "You can fail because.",
            "The moves on before things finished.",
            "Things of that sort.",
            "Yes, yes, I know this is June Jones work in June is great at making things work.",
            "Yeah, it wasn't.",
            "It wasn't a trivial thing to make it work, yes?",
            "Right, so during training.",
            "We only see a subset of the subtasks.",
            "And we only see short lists.",
            "So in the 0 short case we will see lists much much longer.",
            "Then"
        ],
        [
            "Let me much much longer than we never trained in more than five instructions, so here you're seeing like 20 or something, right?",
            "So it transfers to much longer sets of instructions unseen instructions.",
            "Right, so it's getting zero shot by basically learning how to transfer to manage the list.",
            "American to manage the list.",
            "And by learning how to transform using analogy, making regularization an instruction to the correct sub task, sub task arguments.",
            "So just that put together and how to interrupt and come back how to interrupt and still keep maintaining your pointer.",
            "So when all of that is put together nicely, then all of this zero shot generalization happens.",
            "Right?",
            "You know all of these pieces have come."
        ],
        [
            "OK, I'm going to present work that's really hard off the press.",
            "It's not, it's under review.",
            "That I'm very excited about actually is going to be archived just next week.",
            "Again, this is work by June on value prediction networks.",
            "I'm very excited by this idea.",
            "It builds quite quite closely on the predictor on work by David Silver.",
            "The predict on work was limited to policy evaluation.",
            "We wanted to do optimal control and so we took the predictor an idea and extend it to upper control.",
            "Let me not connect before I talk about the ID."
        ],
        [
            "Let me connect it to continue learning story.",
            "I guess that's the next slide."
        ],
        [
            "So.",
            "We know that observation prediction model is extremely hard to build right.",
            "If I ask you to close your eyes and tell me what observation you will see.",
            "Then you open your eyes.",
            "You have a really hard time producing a great pixel based observation.",
            "You have a pretty easy time answering certain questions about the observation.",
            "How far is the wall between 20 and 30 feet?",
            "I don't know.",
            "I'm making it up right.",
            "What's the color of the wall?",
            "If you close your eyes, you know you'll get some more reasonable color.",
            "How many screens that are probably have can answer that question.",
            "What are the exact pixels everywhere?",
            "You won't be able to tell, right?",
            "So we can't build.",
            "We know we can't build well.",
            "Maybe we can in certain settings, but in general we won't be able to build good observation prediction models if planning.",
            "Which we have to plan.",
            "In the end, build AI.",
            "We have to plan.",
            "Like in some fashion, we have to look ahead in some fashion inside our head.",
            "Inside, the agent has to look ahead inside the tent.",
            "In some fashion.",
            "So now.",
            "And let's call that very general sense of planning.",
            "So if planning requires observation prediction models, we're in trouble.",
            "On the other hand, we know we can make all kinds of predictions and many different temporal scales.",
            "Right, you heard that from Nando you heard from other people's work, a lot of people were working on.",
            "On making long term.",
            "Predictions about various things.",
            "So the question is, can you plan with those predictions without?",
            "Without doing observation prediction model more traditional sense of model building a forward model right.",
            "Your current situation.",
            "You predict the next observation you update your state and you roll that forward.",
            "That's hard.",
            "So how can we plan?",
            "So that's the question again, heavily inspired by Silver Atolls predictor on where we extended to optimal control."
        ],
        [
            "The idea.",
            "Let me do this.",
            "Actually we should full slide.",
            "So we have a core module.",
            "That gets iterated to do planning.",
            "What's the core module?",
            "The core module takes an observation or some set of observations and produces an internal set of predictions internal state.",
            "Not not the sense of MDP state, but internal neural network state.",
            "Right encodes the input.",
            "Then that encoding.",
            "Along with an option, not necessarily a primitive action, but some option predicts the reward for that option all along the way, so it's an extended behavior reward along the way, and the effective discount.",
            "This comes from the predictor work.",
            "If you've seen the product on work, you see something very similar and you predict sort of the number of steps that option will take in effect.",
            "And also you predict the value.",
            "Associated with that.",
            "With that internal status, so you're predicting three things are gamma and V. And then you do a transition that takes.",
            "Internal state option to next internal state.",
            "If you have these four pieces.",
            "The difference between the predictor work there wasn't in action.",
            "There wasn't an option.",
            "If you have these four pieces, then you can roll it forward.",
            "Without ever predicting observations.",
            "You start with some observation.",
            "The current observation.",
            "You build an internal state.",
            "You say, what if I were to take optional one?",
            "It will be actually be a tree, right?",
            "Because you can do many different options, but I'm showing you one roll out you consider optional one you get your new internal state.",
            "You predict the reward, discount and value of that state.",
            "You say?",
            "Well, what if I take another option from this state?",
            "How will I transform my internal state to the internal state?",
            "And make these predictions.",
            "If you can do that.",
            "Then you can plan well.",
            "Then you can at least look forward.",
            "Look ahead, yes.",
            "Function you're predicting the reward and the discount along the option.",
            "Don't think of it as sorry I OK, I see what you mean.",
            "Like the horizon I'm looking at think of it is just the effective discount.",
            "If it takes 5 steps as Gamora the five.",
            "Effectively, we're predicting how long those options take.",
            "Excuse me good catch.",
            "Yeah, I shouldn't call it discount which is called it sort of effective discount effective discount.",
            "Yes.",
            "Pieces fit together the.",
            "The core module is what's repeated, so you start with observations to get the first internal state, and then you don't need anymore observation.",
            "This argama V is the prediction there, so it's sort of.",
            "I've taken the core module and made vertical.",
            "Right, that gets repeated.",
            "And so now you."
        ],
        [
            "Do planning if you've learned such a model, you can do planning.",
            "You can do look ahead search.",
            "Right, you can say what if I take this option, but if I take that option, what if I take that option and you can build our tree?",
            "And you can do UCT if you want it to.",
            "This should have more clever than full lookahead search.",
            "We don't use it yet, but we do some kind of incomplete search through plan because like any planning thing, you know if you build a tree you can only go so deep in a tree.",
            "So you want to go deep, but carefully chosen trajectories.",
            "So any kind of any kind of search algorithm could be adapted here to do planning, and because these steps are options, they can take multiple steps.",
            "Right, so they're already already going very deep in time."
        ],
        [
            "How do you learn this?",
            "Well, learning is again this diagram.",
            "Look familiar.",
            "If you look at the predictor on work, except that options here, right there actions in here.",
            "So what you do is you basically roll it out and there are multiple parameters.",
            "How much do you roll it out?",
            "How deep do you predict?",
            "And how do you?",
            "What planning do you do at the end to get a terminal value?",
            "Right because?",
            "And then you can learn by looking at things that actually happen, learning the reward, and the effective discount is supervised learning.",
            "Learning the value is is a Q learning multi step Q learning like update.",
            "The targets of multi step Q learning are produced by.",
            "Planning at the end to get better performance even though there are many, many, there are not many, but there are multiple parameters here that you have to.",
            "You can use an exploit.",
            "Yes.",
            "Are you doing also like consistency?",
            "Yes, we're doing consistency.",
            "We're making sure that just like predictor on right one step into multiple estimates are constrained to be, you get error functions, so it's the same idea, really, it's really predictor undone with options, so you can plan.",
            "So if you know what I'm sorry.",
            "I know most of you probably don't know what the product number.",
            "It's really cool work.",
            "We've just taken that idea and really shown how to do planning with it.",
            "OK."
        ],
        [
            "So let me in the few minutes remaining, let me let me.",
            "I can go 5 minutes past something.",
            "Yes, let me show you some results in that storm.",
            "So here's a nice domain.",
            "It's basically like traveling salesman.",
            "You have to collect things so you start the green.",
            "Is the agent the blue other rewarding gold Nuggets?",
            "The task is to go pick up as many gold Nuggets as you can in a finite horizon.",
            "Deterministic, our stochastic you can.",
            "We did both.",
            "But basically you want to find the path that has a pick up the most gold Nuggets in the whatever time your lifetime is.",
            "OK so here just to show you is a that you know this is not a easy task to do right?",
            "Because you have to look at the visual observation and figure out a plan, a path.",
            "Because if you make the wrong steps at the beginning, you already sort of.",
            "You may already have lost value.",
            "So you really have to plan the right path right from the beginning.",
            "Anyway, here's what is an example of a path that DQ and will do it as 20 steps, and here's a different path that VPN, which is this value prediction network does and you can see that you know this looked at unreasonable, right?",
            "It comes picks this up because those are picks that up.",
            "And this is quite different path.",
            "But it turns out this has one more gold nugget it picks up compared to the compared to the DQ and I will show you more."
        ],
        [
            "Here is.",
            "The other cool thing this, by the way, was repeated planning."
        ],
        [
            "You you take the action then you plan plan a path like the first action repeated planning right?",
            "That's how VPN and EQ and will do it.",
            "Here is."
        ],
        [
            "Just looking at a full plan that can produce.",
            "So you can.",
            "Once you've built this thing, you can basically in a deterministic environment anyway, which is this one is.",
            "You can just say tell me what you would do in the entire future.",
            "And you can see the VPN plan for 20 steps versus the VPN plan for 12 steps.",
            "So it's sensitive to side of how deeply you plan an picks up pics.",
            "Pretty good paths.",
            "This again is not a not an easy problem too."
        ],
        [
            "So looks easy.",
            "We compared it against many different things.",
            "We compared against actually learning an observation prediction model.",
            "That's opn.",
            "This is a simple environment that you can actually learn decent observation prediction models and plan using observation prediction models.",
            "We also did VPN by the way.",
            "The number in parentheses is the amount of look at how many steps you look at 1, three or five.",
            "And you can see the crucial thing to take away is VPN beats Opn which observation prediction model but also the deeper the look ahead, the better it does so.",
            "Five is only slightly better than three.",
            "I'd far tell you what."
        ],
        [
            "Options where the options were go straight until you get to a choice point.",
            "So go right until you get to Choicepoint go up.",
            "When we get to Choicepoint, go left those with the options OK."
        ],
        [
            "We also did some Atari game work, so we tried many different games.",
            "The blue curve is is VPN's performance in DQN is the DQ and performance.",
            "And in all games, but two.",
            "I think alien and.",
            "And Pacman it led to an improvement performance over over over.",
            "OK, I'm going to abrupt stop that.",
            "I'm going to stop, take a few minutes for questions, but you have a question right there.",
            "We don't learn options accident question.",
            "These were hard wired options were learning to plan given hard wired options.",
            "We're not discovering options.",
            "That would be really cool.",
            "And we're working on it, but that's a really cool problem.",
            "Yes, Diana has some work on discovering options.",
            "Looking at the internal structure of the agents and made this question before.",
            "Steps to do that, but we can think value iteration as two agents one, but give your readers were all there and then we are training an intrinsic.",
            "Why not?",
            "Are you saying why not give the optimal value as the optimal?",
            "Why isn't the optimal value function the optimal internal reward even more?",
            "Training of our reactive agents.",
            "But can I answer the interesting question first?",
            "Why isn't the optimal value function the optimal internal reward?",
            "Anybody wants to help me answer that?",
            "Why isn't the optimal value function of the optimal internal reward?",
            "We had terrible choice of reward.",
            "Hawaii.",
            "You're double counting rewards no, no.",
            "I'm going to cheap.",
            "That's not the point.",
            "Sorry, that's not the question.",
            "Here's the answer.",
            "Baskets good, that's that's the start of the answer right?",
            "Which is that if you care about a single task, of course you can solve the task.",
            "When you're done.",
            "The point of a reward function is to make an agent that's good at learning good at behaving across tasks.",
            "Which is what I meant when I said in the very beginning, right?",
            "The real challenge is what is the right level divine?",
            "The reward function?",
            "You know there is the objective evolutionary reward function.",
            "Procreate.",
            "But sharing a Cup of coffee right now, how does that affected by that reward function?",
            "I don't know.",
            "Right?",
            "We have much more proxamol reward functions, but two proximal would be too constraining because very inflexible.",
            "And so to get the flexibility of behavior, we have the ability to adapt to new environments requires the sort of just the right reward function.",
            "And that's what evolution presumably did, or at least wanted one.",
            "But that wasn't your question.",
            "What is your question?",
            "So.",
            "It's like.",
            "I assisted him by iteration, so if you used by literation and you get the finally the value, this is like training and agent where for where they can only decide what to do instead of extending a little quality.",
            "What I say is that this isn't like this already happening.",
            "If we allow ourselves to get into the values of internal agent.",
            "So my question then is what about the communication cost?",
            "Meaning this is the designer and this is the.",
            "Agent there I'm participation for further robustness, yes.",
            "Yes, yes.",
            "This offer separation.",
            "So there's a whole body of work I do that I haven't talked about, which is multi agent problems in which communication costs player also have a line of work.",
            "If I were to give a talk on safety in AI, I would talk about work on querying.",
            "How should it robot query human being and human being is distracted and as a cost to communicate all kinds of work that multi agent systems people are doing that go in that direction.",
            "So yes so important questions.",
            "I.",
            "Time to eat and we come back.",
            "It's not going to be safety and AI, but it's going to be safe.",
            "MDP is going to send these, yes?",
            "Phil Thomas, right after the coffee break, I think we're going to see.",
            "Thanks, ginger.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was saying thank you for coming.",
                    "label": 0
                },
                {
                    "sent": "I know you meant if you've been here for many days and you've heard from many great talks.",
                    "label": 0
                },
                {
                    "sent": "I've been here for since the beginning of the reinforcement learning one, and it's been fantastic.",
                    "label": 0
                },
                {
                    "sent": "So thanks, Joel and join and others who organized this fantastic summer school.",
                    "label": 0
                },
                {
                    "sent": "So I'm actually going to pick up on particular on Nando's talk.",
                    "label": 0
                },
                {
                    "sent": "I didn't know what I was going to talk about until I heard Nando, and I said, OK, I know what I'm going to talk about now.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to present a slightly different point of view or take take on a very similar similar questions, and I'm calling this sort of steps towards continual learning an I have two affiliations people have been explaining that affiliation with Professor University of Michigan.",
                    "label": 0
                },
                {
                    "sent": "I'm also the chief scientist at a startup I Co founded with Peter Stone and Mark Ring called coached I an.",
                    "label": 0
                },
                {
                    "sent": "In fact CODA is a continual learning company to all the work I'm going to talk about has been done.",
                    "label": 0
                },
                {
                    "sent": "At universities, so nothing, none of this is from code at the moment.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The words continual learning might mean something intuitively to many people.",
                    "label": 0
                },
                {
                    "sent": "I want to break it down into what precisely I mean by continual learning, and this will be connect also to what Nando talked about, but in slightly different ways of slicing it.",
                    "label": 0
                },
                {
                    "sent": "So when I say continue learning when you want to continue learning agents, I mean agents that can learn new skills overtime from experience as one of the first elements.",
                    "label": 0
                },
                {
                    "sent": "Now this might seem like.",
                    "label": 0
                },
                {
                    "sent": "Well, what's so radical or sort of interesting about that RL is about learning options or skills, but let's break it down a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Learning new skills overtime means I have to store those skills somewhere I have to retrieve them sensibly and hardly any and continually overtime.",
                    "label": 0
                },
                {
                    "sent": "Right, and hardly any IRL system or agent does that at the moment.",
                    "label": 0
                },
                {
                    "sent": "So this is already just.",
                    "label": 0
                },
                {
                    "sent": "This ability is an interesting challenge to take seriously and work on it.",
                    "label": 0
                },
                {
                    "sent": "Learn new knowledge, overtime and by knowledge I mean option conditional prediction skill, conditional predictions.",
                    "label": 0
                },
                {
                    "sent": "We want to make predictions about the world conditioned on behavior.",
                    "label": 0
                },
                {
                    "sent": "Again, do this overtime.",
                    "label": 0
                },
                {
                    "sent": "We have our work.",
                    "label": 0
                },
                {
                    "sent": "Another work that builds, models, bills, predictions but again do it overtime and integrate that in such a way that we can reuse and incorporate these skills and knowledge that we've acquired overtime.",
                    "label": 0
                },
                {
                    "sent": "To help learn more complex skills and knowledge.",
                    "label": 0
                },
                {
                    "sent": "Right this developmental, this incremental aspect of learning new skills and learning new knowledge and and then reusing it overtime is a really important challenge that you know people are working on and you want to do it in a way that's scalable and without catastrophic forgetting.",
                    "label": 0
                },
                {
                    "sent": "And things of that sort that you heard about, presumably in the deep learning part of the summer school as well.",
                    "label": 0
                },
                {
                    "sent": "Another really important question Andrew touched on this one as well is the idea of intrinsic motivation right?",
                    "label": 0
                },
                {
                    "sent": "If you have an agent that's going to be doing this kind of continual learning, then what should drive its behavior?",
                    "label": 0
                },
                {
                    "sent": "What should an agent do?",
                    "label": 0
                },
                {
                    "sent": "In such a setting.",
                    "label": 0
                },
                {
                    "sent": "Write reinforcement learning comes with a task.",
                    "label": 0
                },
                {
                    "sent": "Continual learning doesn't come with a task.",
                    "label": 0
                },
                {
                    "sent": "Will not necessarily come with the task, so you can think of intrinsic motivation to drive experience in the absence, or perhaps more accurately, too long.",
                    "label": 0
                },
                {
                    "sent": "A delay in extrinsic.",
                    "label": 0
                },
                {
                    "sent": "In extrinsic rewards, extrinsic drives or extrinsic motivation.",
                    "label": 0
                },
                {
                    "sent": "So this is a fundamental question in continual learning as well, and by the way, again, to connect to work you've heard about one particularly salient source of intrinsic rewards are the humans are humans or other agents experienced agents.",
                    "label": 0
                },
                {
                    "sent": "Right, you heard about imitation.",
                    "label": 0
                },
                {
                    "sent": "The drive to imitate, and so on.",
                    "label": 0
                },
                {
                    "sent": "As particularly salient examples of where intrinsic motivation might come from.",
                    "label": 0
                },
                {
                    "sent": "And finally, of course, the overall goal is still the is still the PRL goal right?",
                    "label": 0
                },
                {
                    "sent": "Which is to building agents that can become increasingly competent overtime, not just in terms of the amount of skill and knowledge they have, but also in term in terms of how well they're doing.",
                    "label": 0
                },
                {
                    "sent": "Accumulating the extrinsic reward when it is available.",
                    "label": 0
                },
                {
                    "sent": "So this is the continual learning problem.",
                    "label": 0
                },
                {
                    "sent": "In one slide, right?",
                    "label": 0
                },
                {
                    "sent": "This is a problem that I think so.",
                    "label": 0
                },
                {
                    "sent": "So you know one thing you could be asking is sort of, you know, how is this different from other things in AI and RL and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is a very RL centered view of a alright so very RL inspired RL based view of how to think about the overall AI question as opposed to you know vision is an important problem.",
                    "label": 0
                },
                {
                    "sent": "Or speech is an important problem or language is an important problem, which of course they are extremely important problems, but it's a different way of slicing up their problem, and this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a continual learning way of slicing up the problem, so I want to make sure that at least what this this framework is about.",
                    "label": 0
                },
                {
                    "sent": "This setting is about is reasonably clear.",
                    "label": 0
                },
                {
                    "sent": "Any questions about this, particularly how it relates to, are you think this certainly is coming from RL?",
                    "label": 0
                },
                {
                    "sent": "The 11 difference or a difference in emphasis is an emphasis on knowledge.",
                    "label": 0
                },
                {
                    "sent": "Right, not just on behavior, of course.",
                    "label": 0
                },
                {
                    "sent": "Knowledge for good behavior and the intrinsic motivation.",
                    "label": 0
                },
                {
                    "sent": "So it certainly comes from RL, but emphasizes these things.",
                    "label": 0
                },
                {
                    "sent": "Any questions about this?",
                    "label": 0
                },
                {
                    "sent": "By the way, I notice when I was sitting in the audience that sometimes people have questions.",
                    "label": 0
                },
                {
                    "sent": "They just raise their hand, but it's hard to see that you raise your hand.",
                    "label": 0
                },
                {
                    "sent": "So if you have a question, just blurt something out so I can.",
                    "label": 0
                },
                {
                    "sent": "I can hear it and then.",
                    "label": 0
                },
                {
                    "sent": "And then know that you want to ask a question.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                },
                {
                    "sent": "The system you have in mind like something concrete.",
                    "label": 0
                },
                {
                    "sent": "There sort of like where might this be powerful, yes.",
                    "label": 0
                },
                {
                    "sent": "Right, so you know again I'll harken back to something non they talked about.",
                    "label": 0
                },
                {
                    "sent": "You know kids in toys as a as a you know you leave a kid in a room with things.",
                    "label": 0
                },
                {
                    "sent": "Without extrinsic task, I'll come back in half an hour.",
                    "label": 0
                },
                {
                    "sent": "What will they do?",
                    "label": 0
                },
                {
                    "sent": "It will come to understand the world there in by playing around in their world by intrinsic drives to explore, to destroy, to, to build the climb on things, to kick things, to do.",
                    "label": 0
                },
                {
                    "sent": "Depending on the child, do all kinds of different things.",
                    "label": 0
                },
                {
                    "sent": "They build a fundamental understanding of the world in that way.",
                    "label": 0
                },
                {
                    "sent": "So to the extent that you want to build truly robust AI's.",
                    "label": 0
                },
                {
                    "sent": "Right, as opposed to a eyes to solve very well defined specific problems.",
                    "label": 0
                },
                {
                    "sent": "Not that that's a undesirable or easy thing to do either, but to the extent that you want to build artificial intelligences that are able to.",
                    "label": 0
                },
                {
                    "sent": "Really robustly understand the world and behavior.",
                    "label": 0
                },
                {
                    "sent": "To that extent, we can't build in knowledge.",
                    "label": 0
                },
                {
                    "sent": "To that extent, knowledge has to be driven by has to be learned from experience and be intrinsically motivated in the sense of being driven to learning useful things.",
                    "label": 0
                },
                {
                    "sent": "So that's the setting that this comes from, yes.",
                    "label": 0
                },
                {
                    "sent": "On page that knowledge yeah.",
                    "label": 0
                },
                {
                    "sent": "So option conditional predictions are just behavior conditional predictions.",
                    "label": 0
                },
                {
                    "sent": "What will I see?",
                    "label": 0
                },
                {
                    "sent": "Or will there be a door when I walk out of the left corridor here?",
                    "label": 0
                },
                {
                    "sent": "How far will the door be?",
                    "label": 0
                },
                {
                    "sent": "How much time will it be before I see the sunlight if I walk by a certain path?",
                    "label": 0
                },
                {
                    "sent": "If I pick up the phone and call my wife, is she going to be able to like you do respond?",
                    "label": 0
                },
                {
                    "sent": "What's the property that should respond?",
                    "label": 0
                },
                {
                    "sent": "These option conditional predictions?",
                    "label": 0
                },
                {
                    "sent": "The option is the behavior I'm doing and predictions are certain features of the future that I want to predict.",
                    "label": 0
                },
                {
                    "sent": "Sounds a bit like planning is that.",
                    "label": 0
                },
                {
                    "sent": "Yes, very interesting question is how does all this relate to planning?",
                    "label": 0
                },
                {
                    "sent": "So how do you use predictions to plan how to use knowledge to plan is a really interesting question.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you a really simple.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example from my work more than a decade ago, 2004 a child's playroom.",
                    "label": 1
                },
                {
                    "sent": "This is a paper at Nips, so let me just say this is in my judgment, sort of now by now you know for young people in ancient, maybe many of you weren't even born.",
                    "label": 0
                },
                {
                    "sent": "9 gesture.",
                    "label": 0
                },
                {
                    "sent": "Ancient example of a continued learning demonstration, so this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a toy world.",
                    "label": 0
                },
                {
                    "sent": "The child's playroom, which has objects in it.",
                    "label": 0
                },
                {
                    "sent": "Objects that have things like that are bells.",
                    "label": 0
                },
                {
                    "sent": "There is a monkey toy monkey hand, so you can imagine that there is a.",
                    "label": 0
                },
                {
                    "sent": "The agent is looking at this from the top where we're getting a bird's eye view, but the agent has an IA focus.",
                    "label": 0
                },
                {
                    "sent": "And the agencies, what's where the eye is agent is a hand.",
                    "label": 0
                },
                {
                    "sent": "They didn't consent what's under the hand and the agent has this marker.",
                    "label": 0
                },
                {
                    "sent": "This thing with crosshairs called a marker and it's like an indexical representation.",
                    "label": 0
                },
                {
                    "sent": "Can remember where it was.",
                    "label": 0
                },
                {
                    "sent": "And then there are these buttons.",
                    "label": 0
                },
                {
                    "sent": "The blue button to turn music on the red button to turn music off.",
                    "label": 0
                },
                {
                    "sent": "It can push something into the ball and roll the ball, hit the ball and roll the ball.",
                    "label": 0
                },
                {
                    "sent": "We rose the ball into the Bell.",
                    "label": 0
                },
                {
                    "sent": "The Bell rings.",
                    "label": 0
                },
                {
                    "sent": "The light switch with turn the light on and off the lights off.",
                    "label": 0
                },
                {
                    "sent": "You can't see the colors.",
                    "label": 0
                },
                {
                    "sent": "Things of that sort right?",
                    "label": 0
                },
                {
                    "sent": "So the agent is put in this world without any extrinsic task.",
                    "label": 0
                },
                {
                    "sent": "Perhaps it has to learn to achieve mastery over this little really trivial contrived toy world.",
                    "label": 0
                },
                {
                    "sent": "And so I've already described some of the primitive actions, the hand and the I can be moved.",
                    "label": 0
                },
                {
                    "sent": "I think the I can be moved North, South East, West the I can be moved to the marker.",
                    "label": 0
                },
                {
                    "sent": "The market move to the eye.",
                    "label": 0
                },
                {
                    "sent": "The hand can be moved to the things of that sort.",
                    "label": 0
                },
                {
                    "sent": "And I already described some of the things one can do in the room, and I think the.",
                    "label": 0
                },
                {
                    "sent": "What you want the system to be able to do is somehow learn all the things you can do in this world, how to turn the music on, how to ring the Bell, how to kick the ball, how to how to make the the toy monkey cry out.",
                    "label": 0
                },
                {
                    "sent": "Which is I don't know why I use that phrase when writing this paper, but it's been called a monkey world since since we built this build this little domain.",
                    "label": 0
                },
                {
                    "sent": "But but but the ultimate, the hardest skill to learn is to make the toy cry out, which requires having the lights off and the music playing and the Bell ringing, so loud noise and the lights off makes the monkey cry out.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the setting.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't know if anybody talked about options.",
                    "label": 0
                },
                {
                    "sent": "I know many of you know enough about options.",
                    "label": 0
                },
                {
                    "sent": "Options are basically temporally extended behaviors and there's a nice very nice mathematical formalism and Joiner who's here at McGill was one of the leaders in developing this option sort of work OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't talk much about that.",
                    "label": 0
                },
                {
                    "sent": "So now let me bring in the continual learning elements and tell you how we sort of solved this very simple problem in this sort of continuing way.",
                    "label": 0
                },
                {
                    "sent": "So what is the intrinsic reward here?",
                    "label": 0
                },
                {
                    "sent": "The intrinsic reward was error was proportional to error in prediction of a salient events.",
                    "label": 0
                },
                {
                    "sent": "Salient event were like turning on and off sounds, turning on and off music, turning on and off things that they were salient events.",
                    "label": 0
                },
                {
                    "sent": "And the agent was going to learn options.",
                    "label": 0
                },
                {
                    "sent": "Driven by this intrinsic reward to achieve those salient events was also going to learn to predict those salient events.",
                    "label": 0
                },
                {
                    "sent": "There's an error in that prediction that was intrinsically rewarding.",
                    "label": 0
                },
                {
                    "sent": "OK, so every time you experience a new salient event, you allocate a data structure to build an option for that.",
                    "label": 0
                },
                {
                    "sent": "Sailing event.",
                    "label": 0
                },
                {
                    "sent": "Build option meaning.",
                    "label": 0
                },
                {
                    "sent": "Build a model.",
                    "label": 0
                },
                {
                    "sent": "Build a prediction model and build a behavior model builder policy.",
                    "label": 0
                },
                {
                    "sent": "OK, and so you every time you experience new sailing event, you're allocating sort of a skill for it and a knowledge for it.",
                    "label": 0
                },
                {
                    "sent": "OK, and we were using this really simple idea, which is also something we talked about, which is all these options that we have initiated were being learned all the time in parallel from all the behavior that was being generated and things are called interruption learning methods and we use those to do these sort of things and then because we had these options that predicted what will happen when the option finished, we could do planning to connect back to planning.",
                    "label": 0
                },
                {
                    "sent": "We could do planning with those option models to try to make value function updates to our behavior, so here's the picture.",
                    "label": 0
                },
                {
                    "sent": "I want to paint for you.",
                    "label": 0
                },
                {
                    "sent": "There's a behavior policy.",
                    "label": 0
                },
                {
                    "sent": "That's being learned with the intrinsic reward.",
                    "label": 0
                },
                {
                    "sent": "Their option and option models that are being learned driven by salient events.",
                    "label": 0
                },
                {
                    "sent": "The behavior policies reward is error in prediction of assailant event.",
                    "label": 0
                },
                {
                    "sent": "So we never encounter the sale independent, didn't predict.",
                    "label": 0
                },
                {
                    "sent": "It feels rewarded.",
                    "label": 0
                },
                {
                    "sent": "OK, and the Q function, the value function for the behavior policy is being used in a dynamic.",
                    "label": 0
                },
                {
                    "sent": "Will be learning Edina.",
                    "label": 0
                },
                {
                    "sent": "Like way you're learning from the steps in the world and you're learning from your option models that you're predicting so I don't know if anybody talked about diner, but it's a simple idea of mixing model free and model based model based reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's a visualization of what happens.",
                    "label": 0
                },
                {
                    "sent": "So the X axis is time, and these different sort of.",
                    "label": 0
                },
                {
                    "sent": "Graphs are for different salient events.",
                    "label": 0
                },
                {
                    "sent": "Ellon means light on Elof means light off.",
                    "label": 0
                },
                {
                    "sent": "S on mean sound on.",
                    "label": 0
                },
                {
                    "sent": "Music on music of Things of that sort every tick is the occurrence of a salient event.",
                    "label": 0
                },
                {
                    "sent": "The height of the vertical bar is the intrinsic reward obtained when that salient event happens.",
                    "label": 0
                },
                {
                    "sent": "So if the height is very is tiny, is this kind of height, then that means there was no intrinsic reward.",
                    "label": 0
                },
                {
                    "sent": "So what you see is very early on intrinsically.",
                    "label": 0
                },
                {
                    "sent": "Turning light on and off is a very simple skill.",
                    "label": 0
                },
                {
                    "sent": "You just have to hit the light switch so lots of lights on and off are happening and it's getting surprised by them because it's every new state is different.",
                    "label": 0
                },
                {
                    "sent": "So get surprised in this state you know when the ball is in a different place, a different state.",
                    "label": 0
                },
                {
                    "sent": "The buttons are in a different place, different states or get surprised and it learns that.",
                    "label": 0
                },
                {
                    "sent": "Then at some point, once it knows how to turn the lights on and off, it gets bored by that and tries to learn and get gets to learn how to kick the ball into the Bell to ring the Bell.",
                    "label": 0
                },
                {
                    "sent": "That sound on.",
                    "label": 0
                },
                {
                    "sent": "And then music on.",
                    "label": 0
                },
                {
                    "sent": "Right now, of course, once it turns the music on.",
                    "label": 0
                },
                {
                    "sent": "Then if it able to also turn the light on, that was a different state, so it gets surprised again.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's why the light on off intrinsic rewards don't go to zero overtime because it once it learns new skills, it can generate new states that hasn't experienced before.",
                    "label": 0
                },
                {
                    "sent": "Because now it can be in a state where the music is also on.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yet all body short partial amount of error, so this means it got very surprised.",
                    "label": 0
                },
                {
                    "sent": "And then it means good.",
                    "label": 0
                },
                {
                    "sent": "So the reward is proportional to the error in prediction.",
                    "label": 0
                },
                {
                    "sent": "So the height of the bar is better in prediction.",
                    "label": 0
                },
                {
                    "sent": "OK, the this bar this little bar is mean just sailing event occur but there is no reward.",
                    "label": 0
                },
                {
                    "sent": "So you can see a natural developmental progression.",
                    "label": 0
                },
                {
                    "sent": "Easy things get learned.",
                    "label": 0
                },
                {
                    "sent": "First they make it possible to learn harder things.",
                    "label": 0
                },
                {
                    "sent": "Right and then they make it possible to learn still harder things in this progression happens overtime.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so there's a hierarchy of reusable skills right to the primitive.",
                    "label": 1
                },
                {
                    "sent": "Actions were things like secured to random objects, cigar lighter random object, move marker.",
                    "label": 0
                },
                {
                    "sent": "Do I do marker move, hand up and down North, South East, West?",
                    "label": 0
                },
                {
                    "sent": "These are the primitive skills that were built in and then it can turn the light on.",
                    "label": 0
                },
                {
                    "sent": "It cannot turn light off once it learns turns light on it can see the color of the light switches in turn music on.",
                    "label": 0
                },
                {
                    "sent": "Once it can turn music on and turn the ring the Bell and turn the light off.",
                    "label": 0
                },
                {
                    "sent": "It can then activate the toy.",
                    "label": 0
                },
                {
                    "sent": "So this automatic hierarchy of things happen in a very simple, simple way, OK?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you've seen graphs like this from Nando yesterday, which is a graph that basically says look learning all of these things in parallel.",
                    "label": 0
                },
                {
                    "sent": "Ends up achieving much faster learning than only being only learning from intrinsic reward, where the intrinsics for extrinsic reward with extrinsic reward.",
                    "label": 0
                },
                {
                    "sent": "Here was the hardest skill, which was to make the monkey activate the monkey toy monkey starts clapping or crying or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so you see this benefit that Nanna was talking about in all the same ways, exactly the same ways, but now I've given you a very concrete simple example of how to do this kind of continual learning and it's very simple domain.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I want to wrap up this very simple example.",
                    "label": 0
                },
                {
                    "sent": "I'm going to build on it a little bit more so in this very simple example we learn skills and options.",
                    "label": 0
                },
                {
                    "sent": "We learn new knowledge, informed predictions, we reuse those skills to learn.",
                    "label": 0
                },
                {
                    "sent": "We reuse previously, learn skills to learn more complex skills and the agent got more competent overtime at extrinsic reward.",
                    "label": 0
                },
                {
                    "sent": "Now there are lots of caveats.",
                    "label": 0
                },
                {
                    "sent": "This is this is, this predates all the deep learning, excitement, right?",
                    "label": 0
                },
                {
                    "sent": "So it's extremely contrived.",
                    "label": 0
                },
                {
                    "sent": "Domain intrinsic motivations were hardwired about hardwired salient events.",
                    "label": 0
                },
                {
                    "sent": "Very limited form of intrinsic reward.",
                    "label": 0
                },
                {
                    "sent": "Everything was going to look up tables, so there was no catastrophic forgetting issues and it was, you know, didn't scale very well.",
                    "label": 0
                },
                {
                    "sent": "You can imagine so, but there were lots of people working on this sort of thing.",
                    "label": 0
                },
                {
                    "sent": "At that time.",
                    "label": 0
                },
                {
                    "sent": "Schmidhuber Kaplan.",
                    "label": 0
                },
                {
                    "sent": "Oh dear through Sebastian tune and.",
                    "label": 0
                },
                {
                    "sent": "Forget Bollerslev first name and many others, yes.",
                    "label": 0
                },
                {
                    "sent": "So this was literally look up table.",
                    "label": 0
                },
                {
                    "sent": "We're learning a mapping.",
                    "label": 0
                },
                {
                    "sent": "We're learning a policy and look up table.",
                    "label": 0
                },
                {
                    "sent": "We're learning the initiation set and look up table.",
                    "label": 0
                },
                {
                    "sent": "We're learning the termination setting look up table.",
                    "label": 0
                },
                {
                    "sent": "We're learning a prediction model option prediction model, look up tables.",
                    "label": 0
                },
                {
                    "sent": "Everything was gonna look up table because it's tiny domain.",
                    "label": 0
                },
                {
                    "sent": "We could do it, so there was no interference with options.",
                    "label": 0
                },
                {
                    "sent": "There was no, no no, no catastrophic.",
                    "label": 0
                },
                {
                    "sent": "Forgetting.",
                    "label": 0
                },
                {
                    "sent": "All the interesting issues that come up with neural Nets weren't present in this early work.",
                    "label": 0
                },
                {
                    "sent": "But but I think the pieces are all there, right?",
                    "label": 0
                },
                {
                    "sent": "You're learning options, you're learning options in parallel, early learning of options leads to situations where you can learn harder options.",
                    "label": 0
                },
                {
                    "sent": "And because you're building these option conditional predictions, you can plan.",
                    "label": 0
                },
                {
                    "sent": "You can do both model based and model free learning.",
                    "label": 0
                },
                {
                    "sent": "And piece it together to accelerate the learning of new tasks.",
                    "label": 0
                },
                {
                    "sent": "Yes, is this accepted?",
                    "label": 0
                },
                {
                    "sent": "That's by having different sub sub policies for skills.",
                    "label": 0
                },
                {
                    "sent": "Or does it?",
                    "label": 0
                },
                {
                    "sent": "Extensional that's value single node that worked out.",
                    "label": 0
                },
                {
                    "sent": "Great question right?",
                    "label": 0
                },
                {
                    "sent": "Both of them will have advantages and disadvantages.",
                    "label": 0
                },
                {
                    "sent": "So you can certainly go that route.",
                    "label": 0
                },
                {
                    "sent": "These were certainly completely disjoint options, right?",
                    "label": 0
                },
                {
                    "sent": "You could you could imagine still doing that for neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Or you could imagine doing it in a joint way and then have to deal with interference.",
                    "label": 0
                },
                {
                    "sent": "Things of that sort.",
                    "label": 0
                },
                {
                    "sent": "But you might get better generalization faster learning, so you have their pros and cons.",
                    "label": 0
                },
                {
                    "sent": "The tradeoffs with these things, right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to take a extended diversion to really focus in on this question of where rewards come from.",
                    "label": 0
                },
                {
                    "sent": "We've done some work on this that I think is.",
                    "label": 0
                },
                {
                    "sent": "Lay some basic foundations of this that I want to share with you, sort of.",
                    "label": 0
                },
                {
                    "sent": "Where do rewards come from and really focus on that specific question.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with a lot of people, including Andy, Bartow, Recloose and my students, Nattapong and Jonathan and Georgia.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I want to explain a very simple idea to you, which I've used different words overtime to describe it.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call it here.",
                    "label": 0
                },
                {
                    "sent": "Parameters, preferences confound.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Again, the starting point of reinforcement learning is somebody gives you a reward function.",
                    "label": 0
                },
                {
                    "sent": "When the continuing setting is not clear where the reward function comes from.",
                    "label": 0
                },
                {
                    "sent": "But at least something is clear.",
                    "label": 0
                },
                {
                    "sent": "The robot or the agent or the AI is acting on somebody's behalf.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not clear whether it's an individual human being or society, but let's ignore that question because that's a very interesting question in its own right.",
                    "label": 0
                },
                {
                    "sent": "But imagine for now that there is this agent designer.",
                    "label": 0
                },
                {
                    "sent": "The human being.",
                    "label": 0
                },
                {
                    "sent": "That has preferences over the agents behavior like the robot doesn't have preferences.",
                    "label": 0
                },
                {
                    "sent": "The human designer, the agent designer has preferences over robot behavior.",
                    "label": 0
                },
                {
                    "sent": "And those preferences can be captured in the form of a reward function.",
                    "label": 0
                },
                {
                    "sent": "Let's call that the objective reward function.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what should the agents reward function be?",
                    "label": 0
                },
                {
                    "sent": "Because we are building our agent, so it's going to use rewards.",
                    "label": 0
                },
                {
                    "sent": "To determine its behavior.",
                    "label": 0
                },
                {
                    "sent": "So now there are two reward functions.",
                    "label": 0
                },
                {
                    "sent": "The agent designers reward function.",
                    "label": 0
                },
                {
                    "sent": "And the agents reward function.",
                    "label": 0
                },
                {
                    "sent": "And our L confounds those two.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One notion of reward function.",
                    "label": 0
                },
                {
                    "sent": "The objective reward is a notion of preferences or evaluation.",
                    "label": 0
                },
                {
                    "sent": "How good is the agents behavior for the agent designer?",
                    "label": 0
                },
                {
                    "sent": "The second notion of reward function is a guidance reward function.",
                    "label": 0
                },
                {
                    "sent": "There parameters to the agent behavior.",
                    "label": 0
                },
                {
                    "sent": "If I take your favorite RL algorithm.",
                    "label": 0
                },
                {
                    "sent": "And I give it a reward function.",
                    "label": 0
                },
                {
                    "sent": "It will generate a behavior if I change the reward function a little different behavior or different distribution or behaviors.",
                    "label": 0
                },
                {
                    "sent": "So there are two reward functions is an evaluation of preferences notion of reward function to call objective reward function and then a subjective or an internal reward function that guides the agent's behavior and in standard PRL this is confounded.",
                    "label": 0
                },
                {
                    "sent": "But they don't have to be right, and so and so we have so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the way, this you've seen this picture before.",
                    "label": 0
                },
                {
                    "sent": "I imagine this is the this is the standard view of RL in which the rewards come from the environment.",
                    "label": 0
                },
                {
                    "sent": "This is a more sensible view of an Organism and maybe we should build artificial agents this way, in which the reward comes from a critic inside the inside the agent.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think you've seen this view, so I won't spend much time on it, OK?",
                    "label": 0
                },
                {
                    "sent": "There are lots of approaches for designing reward.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "No, in this figure I'm only showing the internal reward function because I'm not showing the human designer in this figure at all.",
                    "label": 0
                },
                {
                    "sent": "This is just the agent, the artificial agent.",
                    "label": 0
                },
                {
                    "sent": "Did you reference a publication?",
                    "label": 0
                },
                {
                    "sent": "Yes, this actually.",
                    "label": 0
                },
                {
                    "sent": "This is this in Sacrament barcode book.",
                    "label": 0
                },
                {
                    "sent": "Now I'm not sure there is there is a. I have a paper called where the rewards come from.",
                    "label": 0
                },
                {
                    "sent": "If you look for that, you'll find it.",
                    "label": 0
                },
                {
                    "sent": "But I don't think it's new to that paper.",
                    "label": 0
                },
                {
                    "sent": "Inverse reinforcement learning is a very interesting idea.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to that in the next part of this reward shaping.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lots of people work on that preference elicitation mechanism.",
                    "label": 0
                },
                {
                    "sent": "Design lots of approaches to designing rewards.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the formal thing I want to communicate to you, and I won't spend terribly much time on it.",
                    "label": 0
                },
                {
                    "sent": "It's called the optimal reward problem.",
                    "label": 1
                },
                {
                    "sent": "So the optimal reward problem takes this confounds seriously.",
                    "label": 0
                },
                {
                    "sent": "There are now two reward functions.",
                    "label": 0
                },
                {
                    "sent": "The thing that's given.",
                    "label": 0
                },
                {
                    "sent": "Is the objective reward function or the agent designers reward function or suburban?",
                    "label": 0
                },
                {
                    "sent": "You don't get to muck around with that, because that's the agent designers reward function.",
                    "label": 0
                },
                {
                    "sent": "And then you have a internal reward function or some I.",
                    "label": 0
                },
                {
                    "sent": "Which is the agent's reward function.",
                    "label": 0
                },
                {
                    "sent": "Right, and you give that the agent is G and it's parameterized by.",
                    "label": 0
                },
                {
                    "sent": "This are so by and you can give it whatever you want and hear all the other parameters of the agent.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's doing Q learning with the neural net and all with certain learning rate.",
                    "label": 0
                },
                {
                    "sent": "All the other parameters are in Theta or sub.",
                    "label": 0
                },
                {
                    "sent": "I is the reward parameterisations and you put in some environment in an environment envy and it produces an interaction it trajectory H from that environment and this agent.",
                    "label": 0
                },
                {
                    "sent": "The utility of this trajectory H or history H to the agent is use of my of H which sums the intrinsic reward.",
                    "label": 0
                },
                {
                    "sent": "The utility of that same trajectory to the objective agent which is the human is of course some of the objective rewards.",
                    "label": 0
                },
                {
                    "sent": "So the optimal reward problem is to define the reward function.",
                    "label": 0
                },
                {
                    "sent": "Are I star so that if you give it to the agent?",
                    "label": 0
                },
                {
                    "sent": "It generates a trajectory that optimizes the human designers objective utility.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimal rewards problem.",
                    "label": 0
                },
                {
                    "sent": "Like I've now set up a mathematic mathematical problem that in some sense explains where rewards should come from.",
                    "label": 0
                },
                {
                    "sent": "And we have written a paper and I'm in which you know, we look considered natural agents where would?",
                    "label": 0
                },
                {
                    "sent": "So who is the agent designer in natural agents?",
                    "label": 0
                },
                {
                    "sent": "Evolution, evolutionary objective reward function is passing on of jeans, right procreation?",
                    "label": 0
                },
                {
                    "sent": "But evolution has reached inside our brain and designed an internal reward function.",
                    "label": 0
                },
                {
                    "sent": "Had designed an internal reward function that makes us very successful given our particular the rest of our agent architecture.",
                    "label": 0
                },
                {
                    "sent": "So the difficult challenge in solving the optimal reward problem is the choice of optimal reward is a fundamentally function of the agent, and it's bound and its capabilities.",
                    "label": 0
                },
                {
                    "sent": "Different agents you would give a different reward function too depending on what the rest of the agent architecture is, is crucially a function of the rest of the parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "Of the agent.",
                    "label": 0
                },
                {
                    "sent": "OK, so instead of a mathematical problem, how do you solve this problem?",
                    "label": 0
                },
                {
                    "sent": "But if you could solve this problem, it would derive the best reward function for you.",
                    "label": 0
                },
                {
                    "sent": "So The thing is, you want to design to really do AI to really build a continual learning agent.",
                    "label": 0
                },
                {
                    "sent": "What you have to do is somehow.",
                    "label": 0
                },
                {
                    "sent": "Define a reward function at just the right level.",
                    "label": 0
                },
                {
                    "sent": "If it's too sparse and too infrequent, much more like the objective reward.",
                    "label": 0
                },
                {
                    "sent": "Then the agent will not succeed in its in its lifelong learning experience.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you can make it too precise, two detail.",
                    "label": 0
                },
                {
                    "sent": "But then the agent will be inflexible.",
                    "label": 0
                },
                {
                    "sent": "It won't be able to deal with the variation of environment could find itself in.",
                    "label": 0
                },
                {
                    "sent": "So somehow to the right level of reward.",
                    "label": 0
                },
                {
                    "sent": "That allows the agent to be most successful over the distribution of environments it could find itself in.",
                    "label": 0
                },
                {
                    "sent": "So it's ready to learn the reward that makes it ready to learn.",
                    "label": 0
                },
                {
                    "sent": "Effectively, in the distribution of environments in so designing a good reward function may actually be.",
                    "label": 0
                },
                {
                    "sent": "And underexplored way of learning to learn.",
                    "label": 0
                },
                {
                    "sent": "Anyway, sorry, just trying to connect to Nando.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Look at time.",
                    "label": 0
                },
                {
                    "sent": "Let me give it again.",
                    "label": 0
                },
                {
                    "sent": "I'll give a toy example predating neural net days or not, that's that's inaccurate, predating the deep learning craze.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to give you a deep learning example in a minute.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a toy world.",
                    "label": 0
                },
                {
                    "sent": "Very, very simple world in which there's an agent shown by a circle, and all there is in the agent in the room are two things, a worm.",
                    "label": 0
                },
                {
                    "sent": "That the agent could eat and get small amount of measly reward, or could pick up the worm and go to the fishing hole at the bottom and get fish and eat fish, which is nicely rewarding.",
                    "label": 0
                },
                {
                    "sent": "The agent is going to be Q learning agent.",
                    "label": 0
                },
                {
                    "sent": "So what are what?",
                    "label": 0
                },
                {
                    "sent": "Should you give the agent?",
                    "label": 0
                },
                {
                    "sent": "The objective reward is the the worm has a very tiny amount of positive reward 0.04.",
                    "label": 0
                },
                {
                    "sent": "But Fisher that order one.",
                    "label": 0
                },
                {
                    "sent": "So that's the objective reward order.",
                    "label": 0
                },
                {
                    "sent": "What should you give the agent now if the agent is a look up table Q learner with arbitrary large amounts of time to learn.",
                    "label": 0
                },
                {
                    "sent": "It's clear if the agent is unbounded.",
                    "label": 0
                },
                {
                    "sent": "What you should do is clear given the objective reward function after all, you'll end up optimizing it.",
                    "label": 0
                },
                {
                    "sent": "So we're going to make the agent.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bounded?",
                    "label": 0
                },
                {
                    "sent": "To simulate the notion of real world agents, even though this is a toy problem, the agent can be unbounded is going down at what kind of boundary we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at bounds of finite lifetimes.",
                    "label": 0
                },
                {
                    "sent": "So the agent may have only a small amount of life time to learn.",
                    "label": 0
                },
                {
                    "sent": "So now if you solve for the optimal reward problem, you get this kind of behavior in which the X axis here is the lifetime of the agent.",
                    "label": 0
                },
                {
                    "sent": "So each point is a different agent with different amount of lifetime.",
                    "label": 0
                },
                {
                    "sent": "If the agent lifetime is very short.",
                    "label": 0
                },
                {
                    "sent": "The best reward turns out to be to slightly prefer if they don't like time is very short.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have time to learn to fish because the fish and actually go to the worm find the worm, pick it up, not eat it, and keep carrying it, not eating it to the fish and catch the fish and then eat the fish.",
                    "label": 0
                },
                {
                    "sent": "That's a more complicated process to learn, right?",
                    "label": 0
                },
                {
                    "sent": "So very short lifetime.",
                    "label": 0
                },
                {
                    "sent": "The red bar is a reward for the for the internal reward for eating the warm, so to speak, and the blue one is for eating the fish or speak so very short lifetime you have a slight preference over you.",
                    "label": 0
                },
                {
                    "sent": "Give a slight preference to the war man, you say fish.",
                    "label": 0
                },
                {
                    "sent": "You can't even get to the fish.",
                    "label": 0
                },
                {
                    "sent": "You never pick up the fish.",
                    "label": 0
                },
                {
                    "sent": "If you are middling lifetime.",
                    "label": 0
                },
                {
                    "sent": "Then it learns to say eating fish is really bad.",
                    "label": 0
                },
                {
                    "sent": "Because you don't want to be distracted by eating the fish.",
                    "label": 0
                },
                {
                    "sent": "If you happen to eat fish.",
                    "label": 0
                },
                {
                    "sent": "You'll get distracted by that and you will not even pick up.",
                    "label": 0
                },
                {
                    "sent": "Get enough food from the worm reward from the world so it says eating worm is really good.",
                    "label": 0
                },
                {
                    "sent": "Eating fish is really bad and now if you have any with any cross, the larger lifetime now it has enough time.",
                    "label": 0
                },
                {
                    "sent": "To have a sensible reward function which is eating fish is really good.",
                    "label": 0
                },
                {
                    "sent": "Eating worms is really bad.",
                    "label": 0
                },
                {
                    "sent": "Yes, hang on.",
                    "label": 0
                },
                {
                    "sent": "Let me let me finish this train of thought.",
                    "label": 0
                },
                {
                    "sent": "Then I'll answer questions.",
                    "label": 0
                },
                {
                    "sent": "So here's the performance curve you get again with lifetimes, with short enough lifetime basically.",
                    "label": 0
                },
                {
                    "sent": "With the intrinsic reward, it learns to eat the slight difference about 3 up to this point.",
                    "label": 0
                },
                {
                    "sent": "This is the point at which can learn to fish with the intrinsic reward, the red curve.",
                    "label": 0
                },
                {
                    "sent": "This is the point it would learn to fish with the extrinsic reward.",
                    "label": 0
                },
                {
                    "sent": "So because of bounds.",
                    "label": 0
                },
                {
                    "sent": "An adapting reward it can learn to fish earlier by adapting the internal intrinsic reward.",
                    "label": 0
                },
                {
                    "sent": "So I've kept everything else fixed.",
                    "label": 0
                },
                {
                    "sent": "It's the same Q learning algorithm, same hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "I'm showing you the objective utility on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "I'm showing you the effect of tuning if you like the internal reward functions, we can learn to fish much faster, so you can learn, so that's an accurate statement.",
                    "label": 0
                },
                {
                    "sent": "It can learn to fish more effectively in the horizon if the lifetime is shorter.",
                    "label": 0
                },
                {
                    "sent": "With intrinsic rewards, then with the objective yes, go ahead.",
                    "label": 0
                },
                {
                    "sent": "But I guess it still learning and it doesn't know it to look up table it it sees it every state is it's mark off.",
                    "label": 0
                },
                {
                    "sent": "It doesn't.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have a model number, it doesn't have a model number in Q.",
                    "label": 0
                },
                {
                    "sent": "Learning in this one.",
                    "label": 0
                },
                {
                    "sent": "Group that just disappeared.",
                    "label": 0
                },
                {
                    "sent": "What were the two?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the two curves were preference.",
                    "label": 0
                },
                {
                    "sent": "The internal reward.",
                    "label": 0
                },
                {
                    "sent": "The magnitude of internal reward for eating the worm.",
                    "label": 0
                },
                {
                    "sent": "If you like, there's the red one and eating the fish, which is a blue one.",
                    "label": 0
                },
                {
                    "sent": "So it's learning.",
                    "label": 0
                },
                {
                    "sent": "It was a short lifetime.",
                    "label": 0
                },
                {
                    "sent": "It learns to.",
                    "label": 0
                },
                {
                    "sent": "Have slight preference for the worm, but crucial one is a middle lifetime because the short lifetime it can't learn very much anyway.",
                    "label": 0
                },
                {
                    "sent": "The short light with the middle lifetime, it really says I shouldn't be distracted by the fish so it gives it a large negative reward for eating the fish.",
                    "label": 0
                },
                {
                    "sent": "What's the weather?",
                    "label": 0
                },
                {
                    "sent": "The Y axis is magnitude.",
                    "label": 0
                },
                {
                    "sent": "They're two different things.",
                    "label": 0
                },
                {
                    "sent": "The X axis is lifetime X axis lifetime.",
                    "label": 0
                },
                {
                    "sent": "the Y axis is the reward coefficient.",
                    "label": 0
                },
                {
                    "sent": "So it's positive high positive for the warm, the red one and blue.",
                    "label": 0
                },
                {
                    "sent": "Is the coefficient for eating the fish?",
                    "label": 0
                },
                {
                    "sent": "This is like.",
                    "label": 0
                },
                {
                    "sent": "In the optimal reward function, exactly right.",
                    "label": 0
                },
                {
                    "sent": "Thank you for helping clarify that.",
                    "label": 0
                },
                {
                    "sent": "Yes, coefficients for the optimal the reward function is a function of some features of state.",
                    "label": 0
                },
                {
                    "sent": "And this is the coefficient for that.",
                    "label": 0
                },
                {
                    "sent": "I'm just giving you a quick intuitive picture.",
                    "label": 0
                },
                {
                    "sent": "OK, I want to so now.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Let me skip this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's Fast forward to the deep learning days, and so we did in 2010.",
                    "label": 0
                },
                {
                    "sent": "We came up with an algorithm for solving the optimal rewards problem, and the algorithm is a very straightforward idea.",
                    "label": 0
                },
                {
                    "sent": "I won't even spend much time on it.",
                    "label": 0
                },
                {
                    "sent": "Again, the perspective is very simple.",
                    "label": 0
                },
                {
                    "sent": "There is an objective utility that we care about.",
                    "label": 0
                },
                {
                    "sent": "We're going to treat the internal reward function as a parameter isation of a policy.",
                    "label": 0
                },
                {
                    "sent": "The internal reward function.",
                    "label": 0
                },
                {
                    "sent": "Given an RL agent, will generate behavior in an environment.",
                    "label": 0
                },
                {
                    "sent": "So if I change the reward function, I'm changing the behavior of the agent.",
                    "label": 0
                },
                {
                    "sent": "So it is a parameterisation of a policy.",
                    "label": 0
                },
                {
                    "sent": "Non stationary policy, but policy nevertheless.",
                    "label": 0
                },
                {
                    "sent": "Right, so I can do policy gradients.",
                    "label": 0
                },
                {
                    "sent": "With this I'm going to do a gradient of the objective utility with respect to the internal reward parameters.",
                    "label": 0
                },
                {
                    "sent": "So what I need to do is do a gradient procedure through.",
                    "label": 0
                },
                {
                    "sent": "The algorithm that the agent is going to use to convert the intrinsic rewards to behavior.",
                    "label": 0
                },
                {
                    "sent": "As long as that process is differentiable, I can differentiate through it.",
                    "label": 0
                },
                {
                    "sent": "To adapt my internal reward parameters to climb the gradient with respect to the objective utility.",
                    "label": 0
                },
                {
                    "sent": "Make sense I have an objective utility.",
                    "label": 0
                },
                {
                    "sent": "You've all seen policy gradient.",
                    "label": 1
                },
                {
                    "sent": "Peterbilt gave a great talk on policy gradients.",
                    "label": 0
                },
                {
                    "sent": "Right now I have but but reward parameterisations are typically thought of as neural net parameters.",
                    "label": 0
                },
                {
                    "sent": "Here I'm thinking of sorry policy parameters that are typically thought of as neural net parameters.",
                    "label": 0
                },
                {
                    "sent": "Here I'm thinking of the policy parameters as the internal reward function.",
                    "label": 0
                },
                {
                    "sent": "Which is going to be translated by some procedure into behavior.",
                    "label": 0
                },
                {
                    "sent": "As long as that procedure is differentiable, I can do policy gradients to learn good reward functions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "This is like the design.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is messing up with the with is looking internally at the agent, so there is another agent go against opening up.",
                    "label": 0
                },
                {
                    "sent": "See at the Valley.",
                    "label": 0
                },
                {
                    "sent": "Or yeah, yeah, I mean except except that is a meta algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "It's like a policy gradient algorithm, meta algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's the meta algorithms inside the agent outside the agent.",
                    "label": 0
                },
                {
                    "sent": "It's really it's computational.",
                    "label": 0
                },
                {
                    "sent": "It's tweaking the.",
                    "label": 0
                },
                {
                    "sent": "So what we literally.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I did and I don't want to show you've seen these things before.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to show you this picture and then show you some results.",
                    "label": 0
                },
                {
                    "sent": "So what we did is we did UCT or look ahead search.",
                    "label": 0
                },
                {
                    "sent": "For planning an Atari games.",
                    "label": 0
                },
                {
                    "sent": "So not learning I'm doing planning right?",
                    "label": 0
                },
                {
                    "sent": "And so you see down there looking at search is a differentiable procedure and you can very scalably differentiate through it.",
                    "label": 0
                },
                {
                    "sent": "Just turns out what you do to do value backups.",
                    "label": 0
                },
                {
                    "sent": "You can do gradient backups.",
                    "label": 0
                },
                {
                    "sent": "So you can do gradient backups through.",
                    "label": 0
                },
                {
                    "sent": "You look at search for UCT.",
                    "label": 0
                },
                {
                    "sent": "So what we did is basically we gave it instead of having hardwired features for reward functions, we gave it a usual frames as inputs to a neural net whose output was the reward function to use with UCT.",
                    "label": 0
                },
                {
                    "sent": "So let me say that again because unusual setting.",
                    "label": 0
                },
                {
                    "sent": "We're going to give it images of the current situation.",
                    "label": 0
                },
                {
                    "sent": "We're going to pass it through a neural net whose output is going to be the reward function.",
                    "label": 0
                },
                {
                    "sent": "To assign to the current situation to any situations you feed in.",
                    "label": 0
                },
                {
                    "sent": "And that is going to be a reward function.",
                    "label": 0
                },
                {
                    "sent": "That UCT is going to use to do its planning.",
                    "label": 0
                },
                {
                    "sent": "And we're going to backdrop to the whole thing through UCT and through the neural net to learn the reward function.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straightforward, but what I wanted to show you was.",
                    "label": 0
                },
                {
                    "sent": "That we tried this in 25 Atari games and we did the ratio of the performance with and without this intrinsic reward and the ratio is more than one.",
                    "label": 0
                },
                {
                    "sent": "One is the vertical line and in most games it led to improve performance for UCD.",
                    "label": 0
                },
                {
                    "sent": "But this was a little bit of cheating because we were giving extra because you see, is compute hungry and of course to compute the intrinsic reward.",
                    "label": 0
                },
                {
                    "sent": "Also takes computation when we correct for that to have a more fair apples to apples comparison.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Change a little bit, but still we get a win in most in most cases.",
                    "label": 0
                },
                {
                    "sent": "Here we balance computation because you can do deeper UCT, wider UCT.",
                    "label": 0
                },
                {
                    "sent": "For the time you would take to compute the reward function at every every step, so balanced out the time you still get a win, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sorry, didn't nobody's talked about you City.",
                    "label": 0
                },
                {
                    "sent": "OK, UCT is basically look ahead search so.",
                    "label": 0
                },
                {
                    "sent": "So one way to plan would be on a current situation.",
                    "label": 0
                },
                {
                    "sent": "You take actions in the real world you so you you simulate actions and you simulate next states.",
                    "label": 0
                },
                {
                    "sent": "You could get to and you build out this tree.",
                    "label": 0
                },
                {
                    "sent": "So look ahead search would be building our history and backing up values, but you can't do that because the branching factor is too large and you can't go very deep if you use it is a clever way and troubles here.",
                    "label": 0
                },
                {
                    "sent": "I saw him walk in, he's one of the inventors of the basic idea behind UCT.",
                    "label": 0
                },
                {
                    "sent": "It's it's you sort of banded bounds to help determine.",
                    "label": 0
                },
                {
                    "sent": "Which actions are worth exploring in this tree so you can get a much more efficient look at search algorithm?",
                    "label": 0
                },
                {
                    "sent": "OK. All.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was a quick quick.",
                    "label": 0
                },
                {
                    "sent": "Introduction to the notion of optimal rewards and how to use them in any kind of agent in which we.",
                    "label": 0
                },
                {
                    "sent": "In which we would have differentiable procedures that map reward functions to behavior, right?",
                    "label": 0
                },
                {
                    "sent": "Any, any, any procedure that Maps rewards to behavior in a differential way.",
                    "label": 0
                },
                {
                    "sent": "You could then use idea of policy gradients to derive reward functions for by taking great inspected the objective utility.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to take a few minutes to talk about the theoretical piece of work that I'm actually quite excited about, yes.",
                    "label": 0
                },
                {
                    "sent": "Right in the in the.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "Car games work.",
                    "label": 0
                },
                {
                    "sent": "The the input to the reward function was frames, right?",
                    "label": 0
                },
                {
                    "sent": "So we gave the last four frames and output.",
                    "label": 0
                },
                {
                    "sent": "The scalar just tells you what is the reward associated with this situation.",
                    "label": 0
                },
                {
                    "sent": "That's right where we have this neural net that takes observations.",
                    "label": 0
                },
                {
                    "sent": "Gives it a reward that is then used by the UCT procedure in conducting its look ahead search.",
                    "label": 0
                },
                {
                    "sent": "We're doing we doing policy.",
                    "label": 0
                },
                {
                    "sent": "We doing gradients with respect to the objective function reward function.",
                    "label": 0
                },
                {
                    "sent": "We care about true UCT through this neural net.",
                    "label": 0
                },
                {
                    "sent": "So what's the only thing that's being learned is the reward function?",
                    "label": 0
                },
                {
                    "sent": "Because you see, is a fixed planning procedure.",
                    "label": 0
                },
                {
                    "sent": "So what we're showing is that.",
                    "label": 0
                },
                {
                    "sent": "That repeated planning can be improved.",
                    "label": 0
                },
                {
                    "sent": "By adapting the reward function.",
                    "label": 0
                },
                {
                    "sent": "And the performance gain that was showing you was after learning a good reward function.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "On the get the actual result, right?",
                    "label": 0
                },
                {
                    "sent": "Learning as you are generating no.",
                    "label": 0
                },
                {
                    "sent": "I think if I remember correctly, it's been a few years now.",
                    "label": 0
                },
                {
                    "sent": "We learned the reward function and then we compared the performance giving.",
                    "label": 0
                },
                {
                    "sent": "Giving straight UCT more enough computation to match the time it takes to take an action with the learned reward function in the signal.",
                    "label": 0
                },
                {
                    "sent": "Now you are.",
                    "label": 0
                },
                {
                    "sent": "The agent actually.",
                    "label": 0
                },
                {
                    "sent": "Well, so you have effectively like doubled.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so could I improvement just because you're more trainable parameters and the more powerful so in the you see there are no trainable parameters.",
                    "label": 0
                },
                {
                    "sent": "You see the planning planning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Right, so I then use a learning procedure to gradients tinana reward function that's fixed.",
                    "label": 0
                },
                {
                    "sent": "So now again there are no trainable parameters.",
                    "label": 0
                },
                {
                    "sent": "Now compare these two agents I have trained to reward function one agent.",
                    "label": 0
                },
                {
                    "sent": "The algorithms are the same.",
                    "label": 0
                },
                {
                    "sent": "It's the same agent.",
                    "label": 0
                },
                {
                    "sent": "UCT is one has a learned reward.",
                    "label": 0
                },
                {
                    "sent": "Function 1 uses the objective reward function.",
                    "label": 0
                },
                {
                    "sent": "And I'm comparing the performance of these two except to make it fair, the one that's using the objective reward function because it doesn't have to computer award function is expensive way through a forward pass through a neural net.",
                    "label": 0
                },
                {
                    "sent": "I give it more computation, but you know I'm at the computation per time step.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "That should.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "Or do you think that because you have these two levels of reward that can handle now you completely right?",
                    "label": 0
                },
                {
                    "sent": "All the work that we've done in optimizing rewards in this way is all for planning.",
                    "label": 0
                },
                {
                    "sent": "There's a good reason, right planning is a fixed procedure.",
                    "label": 0
                },
                {
                    "sent": "To do this for Q learning or learning algorithm would be a much more interesting challenge, because exactly you're exactly right.",
                    "label": 0
                },
                {
                    "sent": "In fact, I would theorem.",
                    "label": 0
                },
                {
                    "sent": "I never wrote in my head which is.",
                    "label": 0
                },
                {
                    "sent": "It's pretty clear that the optimal reward should be a function of the entire history.",
                    "label": 0
                },
                {
                    "sent": "You can't be even if the underlying observation is Markov, the optimal reward will have to be a function of history will have to be non Markov exactly.",
                    "label": 0
                },
                {
                    "sent": "So there's a theorem in my head that I've never really settled down writing.",
                    "label": 0
                },
                {
                    "sent": "I don't think I can think of a clever use of it, but but anyway, but that's I think.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very right?",
                    "label": 0
                },
                {
                    "sent": "OK, I want to take.",
                    "label": 0
                },
                {
                    "sent": "10 minutes and describe some work that I'm quite excited by.",
                    "label": 0
                },
                {
                    "sent": "Do a little bit of theory.",
                    "label": 0
                },
                {
                    "sent": "Java did a lot of death theories.",
                    "label": 0
                },
                {
                    "sent": "I wanted to connect to Java little bit by doing a bit more theory.",
                    "label": 0
                },
                {
                    "sent": "And this is going to connect to.",
                    "label": 0
                },
                {
                    "sent": "Inverse reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Except I'm going to bring it one step closer to continual learning broadly.",
                    "label": 0
                },
                {
                    "sent": "A small step by looking at this sort of what I'm going to call repeated inverse reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "OK, this is work by non Zhang and Karima mean.",
                    "label": 0
                },
                {
                    "sent": "Student in the stock.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So inverse RL.",
                    "label": 0
                },
                {
                    "sent": "Inverse RLI know.",
                    "label": 0
                },
                {
                    "sent": "You've heard brief bits about it, so inverse RL is the forward problem in reinforcement learning is you're given a reward function.",
                    "label": 0
                },
                {
                    "sent": "You produce a behavior.",
                    "label": 0
                },
                {
                    "sent": "The inverse problem is given up.",
                    "label": 0
                },
                {
                    "sent": "Sorry for problem is given a reward function, produce the optimal behavior.",
                    "label": 0
                },
                {
                    "sent": "The inverse problem is given up.",
                    "label": 0
                },
                {
                    "sent": "Given a optimal behavior, an optimal behavior in further reward function.",
                    "label": 0
                },
                {
                    "sent": "That's the inverse reinforcement learning problem.",
                    "label": 1
                },
                {
                    "sent": "Beautiful, intellectually, very beautiful problem, defined band rowing and Stuart Russell and Peter Beale Andrew have done some.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At work with this, the bad news about inverse reinforcement learning that it's fundamentally defined in that the forward problem is many to one, many reward functions will yield the same optimal behavior.",
                    "label": 0
                },
                {
                    "sent": "And those of you know if they can discrete math classes.",
                    "label": 0
                },
                {
                    "sent": "When you invert a many to one function, you get one too many.",
                    "label": 0
                },
                {
                    "sent": "It's not a function.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can't impose problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example give you a concrete example.",
                    "label": 0
                },
                {
                    "sent": "Here's a gridworld.",
                    "label": 0
                },
                {
                    "sent": "You observe the behavior indexed by the shown by the actions.",
                    "label": 0
                },
                {
                    "sent": "Well, here are two possible reward functions, right?",
                    "label": 0
                },
                {
                    "sent": "One possible reward function is the big social reward, the bottom.",
                    "label": 0
                },
                {
                    "sent": "Left corner and then trying to get there.",
                    "label": 0
                },
                {
                    "sent": "Or there could be a reward in the two blue places and it picks up the reward along the way to the bottom and you can imagine gazillions of such reward functions that will all lead to this kind of behavior.",
                    "label": 0
                },
                {
                    "sent": "So it's fundamentally imposed OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then how does?",
                    "label": 0
                },
                {
                    "sent": "How does inverse RL even get off the ground so the way inverse RL get off the ground and I won't spend too much time on it is basically you can very nicely and elegantly define the space of reward functions.",
                    "label": 0
                },
                {
                    "sent": "That are consistent with that behavior.",
                    "label": 0
                },
                {
                    "sent": "So it turns out there's a set of linear constraints defined here, and I won't spend time on this.",
                    "label": 0
                },
                {
                    "sent": "Which basically define all the reward functions.",
                    "label": 0
                },
                {
                    "sent": "For which the observed behavior is optimal in that particular environment.",
                    "label": 0
                },
                {
                    "sent": "So that's nice.",
                    "label": 0
                },
                {
                    "sent": "That is a space defined by a set of linear functions.",
                    "label": 0
                },
                {
                    "sent": "So, but how do you use it then?",
                    "label": 0
                },
                {
                    "sent": "In reality, the way use it is you add a heuristic and you get some point in that space.",
                    "label": 0
                },
                {
                    "sent": "And then you can use it to generate behavior and this works a lot of people use it, it's great.",
                    "label": 0
                },
                {
                    "sent": "But I'm interested for today in this scientific problem of actually inferring a reward function.",
                    "label": 0
                },
                {
                    "sent": "Truly in for in the reward function.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to set it up in this lifelong learning setting.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As follows.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to connect it to AI safety to lifelong learning to continue learning.",
                    "label": 0
                },
                {
                    "sent": "In this one slide.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine now that this robot is going to act on behalf of a human over the lifetime of the human.",
                    "label": 0
                },
                {
                    "sent": "And the human has.",
                    "label": 0
                },
                {
                    "sent": "A intrinsic reward function which is complicated.",
                    "label": 0
                },
                {
                    "sent": "It involves things like I don't want to harm humans.",
                    "label": 0
                },
                {
                    "sent": "No breaking of laws, cost considerations, social norms, general preferences, all kinds of things that a human internally has a sense of reward function.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to communicate all of that explicitly in a good reward function.",
                    "label": 0
                },
                {
                    "sent": "To the agent.",
                    "label": 0
                },
                {
                    "sent": "Write the whole premise of inverse RL.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to communicate reward functions, but it's easy to show behavior good behavior.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the amended setting.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have this lifelong learning setting where the agent is going to experience a sequence of environments or tasks.",
                    "label": 0
                },
                {
                    "sent": "What I want is the agent to become better and better at doing what the human would do in those environments.",
                    "label": 0
                },
                {
                    "sent": "Using inverse reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to define this new environment or task with a environment ecity sort of a think of it as a.",
                    "label": 0
                },
                {
                    "sent": "It controlled Markov processes, actions, dynamics, everything is defined.",
                    "label": 0
                },
                {
                    "sent": "And I'd define a task specific reward function like go get me food.",
                    "label": 0
                },
                {
                    "sent": "Now you tell the agent to go get your food, and that's the only reward function you give.",
                    "label": 0
                },
                {
                    "sent": "You might cook your cat.",
                    "label": 0
                },
                {
                    "sent": "But you say no, no, I didn't mean you cook my cat.",
                    "label": 0
                },
                {
                    "sent": "It's bad to go cats things that sort right.",
                    "label": 0
                },
                {
                    "sent": "So that's the intrinsic rewards you like your pets.",
                    "label": 0
                },
                {
                    "sent": "You like your children.",
                    "label": 0
                },
                {
                    "sent": "All this kind of stuff you don't want to give.",
                    "label": 0
                },
                {
                    "sent": "You don't want to give all these explicit reward functions to the agent.",
                    "label": 0
                },
                {
                    "sent": "So every task is it is it comes in environment and pass.",
                    "label": 0
                },
                {
                    "sent": "Specifically work with me.",
                    "label": 0
                },
                {
                    "sent": "Go cook me food, clean my house, whatever.",
                    "label": 0
                },
                {
                    "sent": "If you have a household robot and so the assumption is that the human beings behavior will be with respect to the sum and this is an arbitrary well, not an arbitrary choice, but I'm going to assume that it's the sum of the humans behavior is going to be the sum of the task specific reward and its intrinsic and the intrinsic human reward.",
                    "label": 0
                },
                {
                    "sent": "So the humans behavior and environment eastep TR safety will be optimal with respect to our sub T plus Theta star.",
                    "label": 0
                },
                {
                    "sent": "So the agent is going to know everything except datastar.",
                    "label": 0
                },
                {
                    "sent": "It doesn't know Datastar, you'll know he sceptile are subte Yeah, have we not seen this notation?",
                    "label": 0
                },
                {
                    "sent": "Exercise of state space?",
                    "label": 0
                },
                {
                    "sent": "Is action space piece of T is a transition dynamics transition matrices are shipped is a vector of task specific rewards for states.",
                    "label": 0
                },
                {
                    "sent": "Data Star is the intrinsic reward for states.",
                    "label": 0
                },
                {
                    "sent": "Gammas are discount factor, so now this fully defines the MDP from which the human would generate behavior.",
                    "label": 0
                },
                {
                    "sent": "And I want the robot or the agent two as quickly as possible.",
                    "label": 0
                },
                {
                    "sent": "Come too.",
                    "label": 0
                },
                {
                    "sent": "Do behave as the human would behave.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is the mathematical question is can we learn datastar from optimal demonstrations on a few tasks or more importantly, and more relevantly can be generalized to new tasks.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Negative one to plus one.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to assume that rewards are bounded.",
                    "label": 0
                },
                {
                    "sent": "Different task.",
                    "label": 0
                },
                {
                    "sent": "It had really widely.",
                    "label": 0
                },
                {
                    "sent": "I'm going to come to that in just one second.",
                    "label": 0
                },
                {
                    "sent": "I'm going to address that exactly in one second.",
                    "label": 0
                },
                {
                    "sent": "Can't be one second 'cause once it comes over in a minute or two.",
                    "label": 0
                },
                {
                    "sent": "Something I wanted to say I totally forgot I'll come back.",
                    "label": 0
                },
                {
                    "sent": "Right, the crucial thing is to generalize as quickly as possible, hopefully.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'm going to address Joel's question, so let's look a little bit more carefully at the notion of unidentified ability of reward function.",
                    "label": 0
                },
                {
                    "sent": "There really two types of identifiability, representational and identify ability and welchol, behavioral or experimental infallibility.",
                    "label": 1
                },
                {
                    "sent": "One of them is uninteresting of the sort that Joel just asked about, which should be ignored.",
                    "label": 0
                },
                {
                    "sent": "Not jewels question, but the idea that rewards can be scaled and the second one is more interesting, which is the environmental and should be dealt with.",
                    "label": 1
                },
                {
                    "sent": "So let me be on my.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Precise we defined this notion of what I would call behavioral equivalence.",
                    "label": 0
                },
                {
                    "sent": "Two reward functions are behaviorally equivalent.",
                    "label": 0
                },
                {
                    "sent": "If there is no no no control Markov process.",
                    "label": 0
                },
                {
                    "sent": "In which they lead to different behavior.",
                    "label": 0
                },
                {
                    "sent": "So if I take a reward function and add a constant to it.",
                    "label": 0
                },
                {
                    "sent": "It won't change their behavior in any environment.",
                    "label": 0
                },
                {
                    "sent": "No matter what the dynamics of the environment.",
                    "label": 0
                },
                {
                    "sent": "And depending on certain other things, if I multiply by scalar, it won't change anything.",
                    "label": 0
                },
                {
                    "sent": "So I don't care about identifying the reward functions except to identify which behavioral equivalent class it belongs to.",
                    "label": 0
                },
                {
                    "sent": "So Gerald, that's the answer to your question.",
                    "label": 0
                },
                {
                    "sent": "Right when I say identify reward function, all I want to identify is some Canonical reward function in a behavioral equivalent class.",
                    "label": 0
                },
                {
                    "sent": "So all reward functions that lead to the same behavior in any possible environment or in the same class.",
                    "label": 0
                },
                {
                    "sent": "And I shouldn't care about identifying one of them, yes.",
                    "label": 0
                },
                {
                    "sent": "I may not care about any possible environment, but I might have some subset of environments that I care about, so right right now, so you can define behavioral equivalence with respect to a set of environments.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying any environment right now, but the notion of being equivalence will be will depend on the environments you care about Babel equivalence over.",
                    "label": 0
                },
                {
                    "sent": "It could be a Singleton.",
                    "label": 0
                },
                {
                    "sent": "Could be a Singleton.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's do 2 quick.",
                    "label": 0
                },
                {
                    "sent": "Yes, quick question.",
                    "label": 0
                },
                {
                    "sent": "No, the yes you can write for a subset of environment.",
                    "label": 0
                },
                {
                    "sent": "There might be a subset of environments where there are nontrivial changes.",
                    "label": 0
                },
                {
                    "sent": "Reward functions that don't change behavior.",
                    "label": 0
                },
                {
                    "sent": "But if you truly allow any environment, then the kinds of behavior equivalences.",
                    "label": 0
                },
                {
                    "sent": "Are only trivial.",
                    "label": 0
                },
                {
                    "sent": "Trivial incentive, you can write down what that space contains, but if you constrain the class of environment then you can get nontrivial classes.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good question, OK?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to do 2 settings.",
                    "label": 0
                },
                {
                    "sent": "The first setting is that just for theoretical reasons, you can imagine a setting where the agent gets to it.",
                    "label": 0
                },
                {
                    "sent": "Present, choose the environment, choose the task and say here's the task human tell me what you would do.",
                    "label": 0
                },
                {
                    "sent": "That's a very powerful agent.",
                    "label": 0
                },
                {
                    "sent": "Right, get it.",
                    "label": 0
                },
                {
                    "sent": "Choose the environment for the human.",
                    "label": 0
                },
                {
                    "sent": "If you do that, it turns out it can be very efficiently learn.",
                    "label": 0
                },
                {
                    "sent": "A good approximation to Theta star.",
                    "label": 0
                },
                {
                    "sent": "Basically, you can do preference elicitation if that makes sense to people, right?",
                    "label": 0
                },
                {
                    "sent": "If you truly, the agent can truly choose the environment, then the agent can choose wacky environments and really learn in parallel, and you can get this log arhythmic performance in just a quick, very, very efficiently learn the optimal optimal reward function, yes?",
                    "label": 0
                },
                {
                    "sent": "Yes, so good.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming that every reward function has been canonicalized.",
                    "label": 0
                },
                {
                    "sent": "That is, I've picked a Canonical element.",
                    "label": 0
                },
                {
                    "sent": "So for example what we literally do is imagine that take an arbitrary state fix its true reward function to be 0.",
                    "label": 0
                },
                {
                    "sent": "And scale everything accordingly.",
                    "label": 0
                },
                {
                    "sent": "So we picked the Canonical.",
                    "label": 0
                },
                {
                    "sent": "We implicitly ameda Canonical assumption of reward functions.",
                    "label": 0
                },
                {
                    "sent": "It matters how you do that right?",
                    "label": 0
                },
                {
                    "sent": "Not for, not for this theory doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "Can you speak in skilled about credit so like you can just scale one up?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but again you're right.",
                    "label": 0
                },
                {
                    "sent": "But the IT matters in the sense that.",
                    "label": 0
                },
                {
                    "sent": "This is a worst case result overall canonicalization's.",
                    "label": 0
                },
                {
                    "sent": "Let's put it that way.",
                    "label": 0
                },
                {
                    "sent": "This is a very trivial trivial result.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I won't.",
                    "label": 0
                },
                {
                    "sent": "I won't go through the proof, but I want to get some more interesting things.",
                    "label": 0
                },
                {
                    "sent": "Then the issue is suppose the agent is not this powerful.",
                    "label": 0
                },
                {
                    "sent": "So now now it's a much more interesting setting.",
                    "label": 0
                },
                {
                    "sent": "Something else nature is choosing the tasks that the agent has to do.",
                    "label": 0
                },
                {
                    "sent": "So now I'll just give you one insight and show them and give the main theorem and then you're welcome to look at the paper.",
                    "label": 0
                },
                {
                    "sent": "So now the following insight right?",
                    "label": 0
                },
                {
                    "sent": "Nature is choosing tasks.",
                    "label": 0
                },
                {
                    "sent": "Now, one of two things will happen, and so the setting now is not.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to learn the truth Data Star because now nature is choosing tasks I may never take me to.",
                    "label": 0
                },
                {
                    "sent": "Interesting places to learn Theta star.",
                    "label": 0
                },
                {
                    "sent": "So now my measure is how often do I make a mistake, so the setting is nature gives me a nice FTR subte.",
                    "label": 0
                },
                {
                    "sent": "The agent proposes a policy.",
                    "label": 0
                },
                {
                    "sent": "The human being looks at policy and says yeah, very good policy.",
                    "label": 0
                },
                {
                    "sent": "Or no bad policy.",
                    "label": 0
                },
                {
                    "sent": "Here's what I would have done.",
                    "label": 0
                },
                {
                    "sent": "Every time the human being has to show a show, a demonstration.",
                    "label": 0
                },
                {
                    "sent": "I think of that as they age and making a mistake.",
                    "label": 0
                },
                {
                    "sent": "What I want to do is bound the number of mistakes the agent would do.",
                    "label": 0
                },
                {
                    "sent": "Settings clear later chooses task.",
                    "label": 0
                },
                {
                    "sent": "Something chooses task.",
                    "label": 0
                },
                {
                    "sent": "You can choose tasks adversarially.",
                    "label": 0
                },
                {
                    "sent": "Every time a task is chosen, the agent says this is what I'm going to do.",
                    "label": 0
                },
                {
                    "sent": "Human being says great.",
                    "label": 0
                },
                {
                    "sent": "Because it's what I would do or close to what I would do or says no, no, I wouldn't do that.",
                    "label": 0
                },
                {
                    "sent": "I would do this every time the human says no, no, do this.",
                    "label": 0
                },
                {
                    "sent": "That's a mistake.",
                    "label": 0
                },
                {
                    "sent": "We want to count the number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll just give you 1 inside of the present, the main theorem and be done.",
                    "label": 0
                },
                {
                    "sent": "The inside is.",
                    "label": 0
                },
                {
                    "sent": "If the.",
                    "label": 0
                },
                {
                    "sent": "Environments are chosen in such a way.",
                    "label": 0
                },
                {
                    "sent": "That the agents behavior is right behavior.",
                    "label": 0
                },
                {
                    "sent": "For example, is the same environment every time.",
                    "label": 0
                },
                {
                    "sent": "Then the agent will not learn more about Datastar, but it's not going to make mistakes either.",
                    "label": 0
                },
                {
                    "sent": "So every time the agent makes a mistake.",
                    "label": 0
                },
                {
                    "sent": "The proof mechanism shows that it learns enough.",
                    "label": 0
                },
                {
                    "sent": "About the truth datastar to make an improvement and make fewer mistakes later.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't make a mistake.",
                    "label": 0
                },
                {
                    "sent": "Great, no mistake.",
                    "label": 0
                },
                {
                    "sent": "If it makes a mistake, it shrinks the volume of possible rewards enough that you can get a nice result out of it, and I won't.",
                    "label": 0
                },
                {
                    "sent": "I won't spend more time on this, and basically we have a result that.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It looks like.",
                    "label": 0
                },
                {
                    "sent": "It's basically the ellipsoid algorithm applied to this.",
                    "label": 1
                },
                {
                    "sent": "For those of you, that's meaningful.",
                    "label": 0
                },
                {
                    "sent": "Volume shrinks in the total amount of the total number of mistakes is order D squared, logged by epsilon.",
                    "label": 0
                },
                {
                    "sent": "Epsilon is the sort of how suboptimal it is when you come before you count a mistake and D is the dimension of the reward space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will skip all.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to switch to a different topic that also connects with Nando.",
                    "label": 0
                },
                {
                    "sent": "You can see that I really made an effort to follow up on Lander.",
                    "label": 0
                },
                {
                    "sent": "In my talk.",
                    "label": 0
                },
                {
                    "sent": "And we talk about some work that's going to appear.",
                    "label": 0
                },
                {
                    "sent": "Acnl 2017 but we did this work a year and a half ago.",
                    "label": 0
                },
                {
                    "sent": "Great thanks to the reviewers helping make it better or iterations.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Don't need to motivate zero shot learning ladder.",
                    "label": 0
                },
                {
                    "sent": "Great job.",
                    "label": 0
                },
                {
                    "sent": "Rapid generalization is key to continue learning.",
                    "label": 1
                },
                {
                    "sent": "The setting I'm going to be in is setting where tasks the motivating setting is.",
                    "label": 0
                },
                {
                    "sent": "Imagine a household robot.",
                    "label": 0
                },
                {
                    "sent": "You go to Amazon.com.",
                    "label": 0
                },
                {
                    "sent": "Your robot comes to your house.",
                    "label": 1
                },
                {
                    "sent": "And you're going to ask it to do things he's going to learn to do.",
                    "label": 0
                },
                {
                    "sent": "Things are going to communicate using language.",
                    "label": 0
                },
                {
                    "sent": "Very simple natural language in this setting.",
                    "label": 0
                },
                {
                    "sent": "Going to give it asks for my clothes then then make dinner.",
                    "label": 0
                },
                {
                    "sent": "Then go sit in a corner.",
                    "label": 0
                },
                {
                    "sent": "Whatever.",
                    "label": 0
                },
                {
                    "sent": "Something like that right?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm thinking about what to say here to make it fast.",
                    "label": 0
                },
                {
                    "sent": "Here's a gridworld and it has objects in it.",
                    "label": 0
                },
                {
                    "sent": "Anna task might be visit the cow, pick up the diamond, hit all the rocks, pick up all the eggs.",
                    "label": 1
                },
                {
                    "sent": "So now the sequence of instructions he construction has basically.",
                    "label": 0
                },
                {
                    "sent": "At most three things, a noun.",
                    "label": 0
                },
                {
                    "sent": "A verb.",
                    "label": 0
                },
                {
                    "sent": "An account.",
                    "label": 0
                },
                {
                    "sent": "Could be all, could be two.",
                    "label": 0
                },
                {
                    "sent": "Pick up two eggs.",
                    "label": 0
                },
                {
                    "sent": "Things of that sort.",
                    "label": 0
                },
                {
                    "sent": "Ann to make the task interesting, there's sometimes random events occur, like somebody.",
                    "label": 0
                },
                {
                    "sent": "Somebody rings the doorbell, or in this case some magic box appears, and when the magic box appears, you can get a nice reward by interrupting whatever it is that you're doing.",
                    "label": 0
                },
                {
                    "sent": "An opening that box.",
                    "label": 0
                },
                {
                    "sent": "So that's the setting the setting is.",
                    "label": 0
                },
                {
                    "sent": "You give a list of instructions.",
                    "label": 1
                },
                {
                    "sent": "The instructions of nouns, verbs and counts.",
                    "label": 0
                },
                {
                    "sent": "And there's some background random thing that you have to maintain that if some background event random happen happens, you interrupt whatever you're doing and go deal with it.",
                    "label": 0
                },
                {
                    "sent": "So that's the setting OK?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the challenges here?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "There's a combinatorial number of tasks, so just training all subtasks isn't really possible, so have to generalize to unseen subtasks.",
                    "label": 1
                },
                {
                    "sent": "You also have to generalize to unseen lists of subtasks of tasks.",
                    "label": 0
                },
                {
                    "sent": "You also have to decide when something is done before you can move on to the next.",
                    "label": 1
                },
                {
                    "sent": "Right, so for translate natural language to what the behavior should be, you have to learn when that thing is done.",
                    "label": 0
                },
                {
                    "sent": "And we also have to delete deal with really delayed rewards because in our setting we only get rewarded the entire list of instructions is finished.",
                    "label": 0
                },
                {
                    "sent": "So it made it harder.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 1
                },
                {
                    "sent": "So again, the agent is going to be given a list of instructions is going to do its thing.",
                    "label": 0
                },
                {
                    "sent": "Either it does the instructions exactly correctly.",
                    "label": 0
                },
                {
                    "sent": "Consider plus one if it doesn't, it gets minus one or something like that.",
                    "label": 0
                },
                {
                    "sent": "So really, really challenging problem.",
                    "label": 0
                },
                {
                    "sent": "But we want to be able to do is train it on some lists and then show that can generalize to unseen lists.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To build a hierarchical architecture, it has a multi task controller.",
                    "label": 0
                },
                {
                    "sent": "And a meta controller.",
                    "label": 0
                },
                {
                    "sent": "The multi task controllers job is takes arguments from American Troller subgoal arguments and produces actions and produces an estimate if it's done or not is it has a terminated or not as the as the sub task is being given finished or not.",
                    "label": 1
                },
                {
                    "sent": "And of course, everyone gets observation.",
                    "label": 0
                },
                {
                    "sent": "This multitask controller gets the raw observations and the meta controller gets their observation, but also the list of instructions which is.",
                    "label": 1
                },
                {
                    "sent": "Sure, which is denoted.",
                    "label": 0
                },
                {
                    "sent": "Here is a gold box.",
                    "label": 0
                },
                {
                    "sent": "Anna.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data Controller has to decide what subgoals given.",
                    "label": 0
                },
                {
                    "sent": "Also remember it has to somehow keep track of where it is, what it's doing, when it's done, move on to the next.",
                    "label": 0
                },
                {
                    "sent": "Thing is to learn all that kind of fun stuff.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a subgoal is composed of multiple arguments, right?",
                    "label": 0
                },
                {
                    "sent": "Nouns and verbs things.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort, so here is the multi task controller architecture.",
                    "label": 1
                },
                {
                    "sent": "It's a it's.",
                    "label": 0
                },
                {
                    "sent": "Let me just finish this.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Index observation and subcode arguments as input produces a primitive action, and predicts, with the current state is terminal not.",
                    "label": 0
                },
                {
                    "sent": "It takes the subgoal arguments.",
                    "label": 0
                },
                {
                    "sent": "And basically writes weights that are a function of the subgoal arguments.",
                    "label": 0
                },
                {
                    "sent": "And those weights that are predicted by the subgoal arguments are used to generate action observation, so you've seen architectures like this.",
                    "label": 0
                },
                {
                    "sent": "It's an adaptation of standard ideas into this the other.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of thing that we had to do to make it work is we use what's called analogy making regularization which basically says we enforce the subgoal argument representations so that.",
                    "label": 1
                },
                {
                    "sent": "The difference between the subgoal representations of pick up A and visit a is close to the difference between pick up B and visit be.",
                    "label": 0
                },
                {
                    "sent": "And similarly the difference between visit being visit day is close to pick up being pick up a.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "Injected a objective function.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which I'm going to briefly flash up that.",
                    "label": 0
                },
                {
                    "sent": "Encourages Subgoal argument representations that.",
                    "label": 0
                },
                {
                    "sent": "Generalize well.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm the deep learning people use this all the time.",
                    "label": 0
                },
                {
                    "sent": "It's a mother all the time, but there's an idea that we've incorporated.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the multi task controller is trained first.",
                    "label": 0
                },
                {
                    "sent": "On simple subtasks with using PRL objective analogy, making objective Anna termination prediction objective.",
                    "label": 1
                },
                {
                    "sent": "So if there are objective, these three objectives are combined and we trained the multi task controller on a subset of all possible sub tasks.",
                    "label": 0
                },
                {
                    "sent": "After",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Train the multi task controller.",
                    "label": 0
                },
                {
                    "sent": "Then will train the meta controller.",
                    "label": 1
                },
                {
                    "sent": "Now on lists of subtasks.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Meta Controller architecture is is slightly different.",
                    "label": 1
                },
                {
                    "sent": "It stores the list of instructions in memory.",
                    "label": 0
                },
                {
                    "sent": "It has a memory pointer.",
                    "label": 0
                },
                {
                    "sent": "Which manipulates as an explicit action.",
                    "label": 0
                },
                {
                    "sent": "But in order to do temporal abstraction, one of the tasks, one of the things that the multi meter controller does is it basically has an output explicit output that.",
                    "label": 0
                },
                {
                    "sent": "Predicts whether it should update the sub task or not.",
                    "label": 0
                },
                {
                    "sent": "An if it if it.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It doesn't need to.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If it needs to update then it is back proper's entire architecture and if it doesn't need to.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Update that it has a similar architecture.",
                    "label": 0
                },
                {
                    "sent": "So I know I'm being very fast, but all the details are in the paper what I?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show you is basically how does it do?",
                    "label": 0
                },
                {
                    "sent": "So here's 0 short generalization, so let's take a random agent first.",
                    "label": 0
                },
                {
                    "sent": "Done following Nando's advice.",
                    "label": 0
                },
                {
                    "sent": "Here is the observation, so it's in Minecraft world.",
                    "label": 0
                },
                {
                    "sent": "Here is a top down view.",
                    "label": 0
                },
                {
                    "sent": "The green box shows what the agent the Meta Controller, thinks the point of the meta controller.",
                    "label": 0
                },
                {
                    "sent": "The green box on top there is what some arguments have been given to the sub task multitask controller.",
                    "label": 0
                },
                {
                    "sent": "OK, so now here it's going too fast.",
                    "label": 0
                },
                {
                    "sent": "So our agents play on training instructions.",
                    "label": 0
                },
                {
                    "sent": "Let's see this one.",
                    "label": 0
                },
                {
                    "sent": "So pick up two pig.",
                    "label": 0
                },
                {
                    "sent": "It's going to go to the pig.",
                    "label": 0
                },
                {
                    "sent": "Pick it up.",
                    "label": 0
                },
                {
                    "sent": "Now it has transformed.",
                    "label": 0
                },
                {
                    "sent": "3 cats will go to the cat.",
                    "label": 0
                },
                {
                    "sent": "Will transform it into something else.",
                    "label": 0
                },
                {
                    "sent": "Go find now a magic box appeared.",
                    "label": 0
                },
                {
                    "sent": "The random event happened on top even though the pointer still says transform 3 cash.",
                    "label": 0
                },
                {
                    "sent": "The task above changes to transform box.",
                    "label": 0
                },
                {
                    "sent": "It does that, keeping the pointer transform.",
                    "label": 0
                },
                {
                    "sent": "3 cats goes back and finishes that task.",
                    "label": 0
                },
                {
                    "sent": "It has to find the cat first process recat, then it'll go visit a horse.",
                    "label": 0
                },
                {
                    "sent": "And then we'll go with the sheep.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how it does on and then I'm going to show you generalizations longer instructions.",
                    "label": 0
                },
                {
                    "sent": "So go faster so you can see what it's doing is maintaining.",
                    "label": 0
                },
                {
                    "sent": "By the way, the meta controller has to take say something like.",
                    "label": 0
                },
                {
                    "sent": "Pick up three things and internally keep track of how many it's picked up.",
                    "label": 0
                },
                {
                    "sent": "Right, and we use recurrent architectures in the meta controller in order to keep learn how to count.",
                    "label": 0
                },
                {
                    "sent": "And keep track of these sort of things so you can see the arguments being given to the multi task control on top where the meta controller thinks it's at right now and this is partial observe ability right?",
                    "label": 0
                },
                {
                    "sent": "Because of all that, because Minecraft domain with the first person view.",
                    "label": 0
                },
                {
                    "sent": "And there you go.",
                    "label": 0
                },
                {
                    "sent": "That's the task.",
                    "label": 0
                },
                {
                    "sent": "OK, and I'm going to take questions if there are questions, yes.",
                    "label": 0
                },
                {
                    "sent": "Previous slide that's alright.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which slide?",
                    "label": 0
                },
                {
                    "sent": "The you want to ask meta controller.",
                    "label": 1
                },
                {
                    "sent": "About the multi task controller.",
                    "label": 0
                },
                {
                    "sent": "Higher level controller, but the higher level controller gets the observations.",
                    "label": 0
                },
                {
                    "sent": "Has this right?",
                    "label": 0
                },
                {
                    "sent": "The weights depends on the.",
                    "label": 1
                },
                {
                    "sent": "Current subgoal, that it has already given.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The output of the multi task controller, whether it thinks the task is completed or not.",
                    "label": 0
                },
                {
                    "sent": "These are the subtasks arguments it producing.",
                    "label": 0
                },
                {
                    "sent": "It has representation of the current current Golar Sepetys coming out from there.",
                    "label": 0
                },
                {
                    "sent": "It has an explicit output that says am I going to update my sub task or not?",
                    "label": 0
                },
                {
                    "sent": "If it chooses to update then it does backdrop to this if.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chooses not to update then it doesn't update the connection breaks between.",
                    "label": 0
                },
                {
                    "sent": "We stick with the previous subgoal, so it learns whether to update the subgoal or not.",
                    "label": 0
                },
                {
                    "sent": "And so we get very fast, rapid, you know, 0 short generalization to unseen tasks.",
                    "label": 0
                },
                {
                    "sent": "By training on a subset of the tasks, how we're doing for time, I have one more thing to talk about.",
                    "label": 0
                },
                {
                    "sent": "So that's the background task.",
                    "label": 0
                },
                {
                    "sent": "The background task is when a box appears.",
                    "label": 0
                },
                {
                    "sent": "It has the opportunity to earn some reward by interrupting whatever it's doing and going in.",
                    "label": 0
                },
                {
                    "sent": "Touching that the idea is to model the notion that there is something that you need to maintain.",
                    "label": 0
                },
                {
                    "sent": "So you might be given a task of, you know, for my clothes household robot you know cook my dinner, but there's a background task.",
                    "label": 0
                },
                {
                    "sent": "Keep the baby safe the baby is crying you go attend the baby no matter whenever they whatever else you're doing baby comes first kind of thing as an example, motivation.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Fails, or when it doesn't.",
                    "label": 0
                },
                {
                    "sent": "So there are many ways it can fail it can.",
                    "label": 0
                },
                {
                    "sent": "It can move on to the next subgoal before it's finished.",
                    "label": 0
                },
                {
                    "sent": "You can change this up task arguments before the previous task is finished.",
                    "label": 0
                },
                {
                    "sent": "Counting can fail.",
                    "label": 0
                },
                {
                    "sent": "You know it needs to pick up three things, but it only picks up two or Project 4.",
                    "label": 0
                },
                {
                    "sent": "Accounting can fail because meta control, as has the sub task controller only.",
                    "label": 0
                },
                {
                    "sent": "We never stay the sub task controller pick up three things.",
                    "label": 0
                },
                {
                    "sent": "You only say pick up pig so they pick up three pigs, then electric pick up pig, wait for termination, pick up pig, wait for termination.",
                    "label": 0
                },
                {
                    "sent": "Pick up pig right has to do all that so it can fail in many.",
                    "label": 0
                },
                {
                    "sent": "You can fail because.",
                    "label": 0
                },
                {
                    "sent": "The moves on before things finished.",
                    "label": 0
                },
                {
                    "sent": "Things of that sort.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, I know this is June Jones work in June is great at making things work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it wasn't.",
                    "label": 0
                },
                {
                    "sent": "It wasn't a trivial thing to make it work, yes?",
                    "label": 0
                },
                {
                    "sent": "Right, so during training.",
                    "label": 0
                },
                {
                    "sent": "We only see a subset of the subtasks.",
                    "label": 0
                },
                {
                    "sent": "And we only see short lists.",
                    "label": 0
                },
                {
                    "sent": "So in the 0 short case we will see lists much much longer.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me much much longer than we never trained in more than five instructions, so here you're seeing like 20 or something, right?",
                    "label": 0
                },
                {
                    "sent": "So it transfers to much longer sets of instructions unseen instructions.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's getting zero shot by basically learning how to transfer to manage the list.",
                    "label": 0
                },
                {
                    "sent": "American to manage the list.",
                    "label": 0
                },
                {
                    "sent": "And by learning how to transform using analogy, making regularization an instruction to the correct sub task, sub task arguments.",
                    "label": 0
                },
                {
                    "sent": "So just that put together and how to interrupt and come back how to interrupt and still keep maintaining your pointer.",
                    "label": 0
                },
                {
                    "sent": "So when all of that is put together nicely, then all of this zero shot generalization happens.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "You know all of these pieces have come.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to present work that's really hard off the press.",
                    "label": 0
                },
                {
                    "sent": "It's not, it's under review.",
                    "label": 0
                },
                {
                    "sent": "That I'm very excited about actually is going to be archived just next week.",
                    "label": 0
                },
                {
                    "sent": "Again, this is work by June on value prediction networks.",
                    "label": 0
                },
                {
                    "sent": "I'm very excited by this idea.",
                    "label": 0
                },
                {
                    "sent": "It builds quite quite closely on the predictor on work by David Silver.",
                    "label": 0
                },
                {
                    "sent": "The predict on work was limited to policy evaluation.",
                    "label": 0
                },
                {
                    "sent": "We wanted to do optimal control and so we took the predictor an idea and extend it to upper control.",
                    "label": 0
                },
                {
                    "sent": "Let me not connect before I talk about the ID.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me connect it to continue learning story.",
                    "label": 0
                },
                {
                    "sent": "I guess that's the next slide.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We know that observation prediction model is extremely hard to build right.",
                    "label": 0
                },
                {
                    "sent": "If I ask you to close your eyes and tell me what observation you will see.",
                    "label": 0
                },
                {
                    "sent": "Then you open your eyes.",
                    "label": 0
                },
                {
                    "sent": "You have a really hard time producing a great pixel based observation.",
                    "label": 0
                },
                {
                    "sent": "You have a pretty easy time answering certain questions about the observation.",
                    "label": 0
                },
                {
                    "sent": "How far is the wall between 20 and 30 feet?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I'm making it up right.",
                    "label": 0
                },
                {
                    "sent": "What's the color of the wall?",
                    "label": 0
                },
                {
                    "sent": "If you close your eyes, you know you'll get some more reasonable color.",
                    "label": 0
                },
                {
                    "sent": "How many screens that are probably have can answer that question.",
                    "label": 0
                },
                {
                    "sent": "What are the exact pixels everywhere?",
                    "label": 0
                },
                {
                    "sent": "You won't be able to tell, right?",
                    "label": 0
                },
                {
                    "sent": "So we can't build.",
                    "label": 0
                },
                {
                    "sent": "We know we can't build well.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can in certain settings, but in general we won't be able to build good observation prediction models if planning.",
                    "label": 0
                },
                {
                    "sent": "Which we have to plan.",
                    "label": 0
                },
                {
                    "sent": "In the end, build AI.",
                    "label": 0
                },
                {
                    "sent": "We have to plan.",
                    "label": 0
                },
                {
                    "sent": "Like in some fashion, we have to look ahead in some fashion inside our head.",
                    "label": 0
                },
                {
                    "sent": "Inside, the agent has to look ahead inside the tent.",
                    "label": 0
                },
                {
                    "sent": "In some fashion.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "And let's call that very general sense of planning.",
                    "label": 0
                },
                {
                    "sent": "So if planning requires observation prediction models, we're in trouble.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we know we can make all kinds of predictions and many different temporal scales.",
                    "label": 0
                },
                {
                    "sent": "Right, you heard that from Nando you heard from other people's work, a lot of people were working on.",
                    "label": 0
                },
                {
                    "sent": "On making long term.",
                    "label": 0
                },
                {
                    "sent": "Predictions about various things.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can you plan with those predictions without?",
                    "label": 0
                },
                {
                    "sent": "Without doing observation prediction model more traditional sense of model building a forward model right.",
                    "label": 0
                },
                {
                    "sent": "Your current situation.",
                    "label": 0
                },
                {
                    "sent": "You predict the next observation you update your state and you roll that forward.",
                    "label": 0
                },
                {
                    "sent": "That's hard.",
                    "label": 0
                },
                {
                    "sent": "So how can we plan?",
                    "label": 0
                },
                {
                    "sent": "So that's the question again, heavily inspired by Silver Atolls predictor on where we extended to optimal control.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea.",
                    "label": 0
                },
                {
                    "sent": "Let me do this.",
                    "label": 0
                },
                {
                    "sent": "Actually we should full slide.",
                    "label": 0
                },
                {
                    "sent": "So we have a core module.",
                    "label": 0
                },
                {
                    "sent": "That gets iterated to do planning.",
                    "label": 0
                },
                {
                    "sent": "What's the core module?",
                    "label": 0
                },
                {
                    "sent": "The core module takes an observation or some set of observations and produces an internal set of predictions internal state.",
                    "label": 0
                },
                {
                    "sent": "Not not the sense of MDP state, but internal neural network state.",
                    "label": 0
                },
                {
                    "sent": "Right encodes the input.",
                    "label": 0
                },
                {
                    "sent": "Then that encoding.",
                    "label": 0
                },
                {
                    "sent": "Along with an option, not necessarily a primitive action, but some option predicts the reward for that option all along the way, so it's an extended behavior reward along the way, and the effective discount.",
                    "label": 0
                },
                {
                    "sent": "This comes from the predictor work.",
                    "label": 0
                },
                {
                    "sent": "If you've seen the product on work, you see something very similar and you predict sort of the number of steps that option will take in effect.",
                    "label": 0
                },
                {
                    "sent": "And also you predict the value.",
                    "label": 0
                },
                {
                    "sent": "Associated with that.",
                    "label": 0
                },
                {
                    "sent": "With that internal status, so you're predicting three things are gamma and V. And then you do a transition that takes.",
                    "label": 0
                },
                {
                    "sent": "Internal state option to next internal state.",
                    "label": 0
                },
                {
                    "sent": "If you have these four pieces.",
                    "label": 0
                },
                {
                    "sent": "The difference between the predictor work there wasn't in action.",
                    "label": 0
                },
                {
                    "sent": "There wasn't an option.",
                    "label": 0
                },
                {
                    "sent": "If you have these four pieces, then you can roll it forward.",
                    "label": 0
                },
                {
                    "sent": "Without ever predicting observations.",
                    "label": 0
                },
                {
                    "sent": "You start with some observation.",
                    "label": 0
                },
                {
                    "sent": "The current observation.",
                    "label": 0
                },
                {
                    "sent": "You build an internal state.",
                    "label": 0
                },
                {
                    "sent": "You say, what if I were to take optional one?",
                    "label": 0
                },
                {
                    "sent": "It will be actually be a tree, right?",
                    "label": 0
                },
                {
                    "sent": "Because you can do many different options, but I'm showing you one roll out you consider optional one you get your new internal state.",
                    "label": 0
                },
                {
                    "sent": "You predict the reward, discount and value of that state.",
                    "label": 0
                },
                {
                    "sent": "You say?",
                    "label": 0
                },
                {
                    "sent": "Well, what if I take another option from this state?",
                    "label": 0
                },
                {
                    "sent": "How will I transform my internal state to the internal state?",
                    "label": 0
                },
                {
                    "sent": "And make these predictions.",
                    "label": 0
                },
                {
                    "sent": "If you can do that.",
                    "label": 0
                },
                {
                    "sent": "Then you can plan well.",
                    "label": 0
                },
                {
                    "sent": "Then you can at least look forward.",
                    "label": 0
                },
                {
                    "sent": "Look ahead, yes.",
                    "label": 0
                },
                {
                    "sent": "Function you're predicting the reward and the discount along the option.",
                    "label": 0
                },
                {
                    "sent": "Don't think of it as sorry I OK, I see what you mean.",
                    "label": 0
                },
                {
                    "sent": "Like the horizon I'm looking at think of it is just the effective discount.",
                    "label": 0
                },
                {
                    "sent": "If it takes 5 steps as Gamora the five.",
                    "label": 0
                },
                {
                    "sent": "Effectively, we're predicting how long those options take.",
                    "label": 0
                },
                {
                    "sent": "Excuse me good catch.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I shouldn't call it discount which is called it sort of effective discount effective discount.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Pieces fit together the.",
                    "label": 0
                },
                {
                    "sent": "The core module is what's repeated, so you start with observations to get the first internal state, and then you don't need anymore observation.",
                    "label": 0
                },
                {
                    "sent": "This argama V is the prediction there, so it's sort of.",
                    "label": 0
                },
                {
                    "sent": "I've taken the core module and made vertical.",
                    "label": 0
                },
                {
                    "sent": "Right, that gets repeated.",
                    "label": 0
                },
                {
                    "sent": "And so now you.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do planning if you've learned such a model, you can do planning.",
                    "label": 0
                },
                {
                    "sent": "You can do look ahead search.",
                    "label": 0
                },
                {
                    "sent": "Right, you can say what if I take this option, but if I take that option, what if I take that option and you can build our tree?",
                    "label": 0
                },
                {
                    "sent": "And you can do UCT if you want it to.",
                    "label": 0
                },
                {
                    "sent": "This should have more clever than full lookahead search.",
                    "label": 0
                },
                {
                    "sent": "We don't use it yet, but we do some kind of incomplete search through plan because like any planning thing, you know if you build a tree you can only go so deep in a tree.",
                    "label": 0
                },
                {
                    "sent": "So you want to go deep, but carefully chosen trajectories.",
                    "label": 0
                },
                {
                    "sent": "So any kind of any kind of search algorithm could be adapted here to do planning, and because these steps are options, they can take multiple steps.",
                    "label": 0
                },
                {
                    "sent": "Right, so they're already already going very deep in time.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do you learn this?",
                    "label": 0
                },
                {
                    "sent": "Well, learning is again this diagram.",
                    "label": 0
                },
                {
                    "sent": "Look familiar.",
                    "label": 0
                },
                {
                    "sent": "If you look at the predictor on work, except that options here, right there actions in here.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you basically roll it out and there are multiple parameters.",
                    "label": 0
                },
                {
                    "sent": "How much do you roll it out?",
                    "label": 0
                },
                {
                    "sent": "How deep do you predict?",
                    "label": 0
                },
                {
                    "sent": "And how do you?",
                    "label": 0
                },
                {
                    "sent": "What planning do you do at the end to get a terminal value?",
                    "label": 0
                },
                {
                    "sent": "Right because?",
                    "label": 0
                },
                {
                    "sent": "And then you can learn by looking at things that actually happen, learning the reward, and the effective discount is supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Learning the value is is a Q learning multi step Q learning like update.",
                    "label": 0
                },
                {
                    "sent": "The targets of multi step Q learning are produced by.",
                    "label": 0
                },
                {
                    "sent": "Planning at the end to get better performance even though there are many, many, there are not many, but there are multiple parameters here that you have to.",
                    "label": 0
                },
                {
                    "sent": "You can use an exploit.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Are you doing also like consistency?",
                    "label": 0
                },
                {
                    "sent": "Yes, we're doing consistency.",
                    "label": 0
                },
                {
                    "sent": "We're making sure that just like predictor on right one step into multiple estimates are constrained to be, you get error functions, so it's the same idea, really, it's really predictor undone with options, so you can plan.",
                    "label": 0
                },
                {
                    "sent": "So if you know what I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "I know most of you probably don't know what the product number.",
                    "label": 0
                },
                {
                    "sent": "It's really cool work.",
                    "label": 0
                },
                {
                    "sent": "We've just taken that idea and really shown how to do planning with it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me in the few minutes remaining, let me let me.",
                    "label": 0
                },
                {
                    "sent": "I can go 5 minutes past something.",
                    "label": 0
                },
                {
                    "sent": "Yes, let me show you some results in that storm.",
                    "label": 0
                },
                {
                    "sent": "So here's a nice domain.",
                    "label": 0
                },
                {
                    "sent": "It's basically like traveling salesman.",
                    "label": 0
                },
                {
                    "sent": "You have to collect things so you start the green.",
                    "label": 0
                },
                {
                    "sent": "Is the agent the blue other rewarding gold Nuggets?",
                    "label": 0
                },
                {
                    "sent": "The task is to go pick up as many gold Nuggets as you can in a finite horizon.",
                    "label": 0
                },
                {
                    "sent": "Deterministic, our stochastic you can.",
                    "label": 0
                },
                {
                    "sent": "We did both.",
                    "label": 0
                },
                {
                    "sent": "But basically you want to find the path that has a pick up the most gold Nuggets in the whatever time your lifetime is.",
                    "label": 0
                },
                {
                    "sent": "OK so here just to show you is a that you know this is not a easy task to do right?",
                    "label": 0
                },
                {
                    "sent": "Because you have to look at the visual observation and figure out a plan, a path.",
                    "label": 0
                },
                {
                    "sent": "Because if you make the wrong steps at the beginning, you already sort of.",
                    "label": 0
                },
                {
                    "sent": "You may already have lost value.",
                    "label": 0
                },
                {
                    "sent": "So you really have to plan the right path right from the beginning.",
                    "label": 0
                },
                {
                    "sent": "Anyway, here's what is an example of a path that DQ and will do it as 20 steps, and here's a different path that VPN, which is this value prediction network does and you can see that you know this looked at unreasonable, right?",
                    "label": 0
                },
                {
                    "sent": "It comes picks this up because those are picks that up.",
                    "label": 0
                },
                {
                    "sent": "And this is quite different path.",
                    "label": 0
                },
                {
                    "sent": "But it turns out this has one more gold nugget it picks up compared to the compared to the DQ and I will show you more.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is.",
                    "label": 0
                },
                {
                    "sent": "The other cool thing this, by the way, was repeated planning.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You you take the action then you plan plan a path like the first action repeated planning right?",
                    "label": 0
                },
                {
                    "sent": "That's how VPN and EQ and will do it.",
                    "label": 0
                },
                {
                    "sent": "Here is.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just looking at a full plan that can produce.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "Once you've built this thing, you can basically in a deterministic environment anyway, which is this one is.",
                    "label": 0
                },
                {
                    "sent": "You can just say tell me what you would do in the entire future.",
                    "label": 0
                },
                {
                    "sent": "And you can see the VPN plan for 20 steps versus the VPN plan for 12 steps.",
                    "label": 0
                },
                {
                    "sent": "So it's sensitive to side of how deeply you plan an picks up pics.",
                    "label": 0
                },
                {
                    "sent": "Pretty good paths.",
                    "label": 0
                },
                {
                    "sent": "This again is not a not an easy problem too.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So looks easy.",
                    "label": 0
                },
                {
                    "sent": "We compared it against many different things.",
                    "label": 0
                },
                {
                    "sent": "We compared against actually learning an observation prediction model.",
                    "label": 0
                },
                {
                    "sent": "That's opn.",
                    "label": 0
                },
                {
                    "sent": "This is a simple environment that you can actually learn decent observation prediction models and plan using observation prediction models.",
                    "label": 0
                },
                {
                    "sent": "We also did VPN by the way.",
                    "label": 0
                },
                {
                    "sent": "The number in parentheses is the amount of look at how many steps you look at 1, three or five.",
                    "label": 0
                },
                {
                    "sent": "And you can see the crucial thing to take away is VPN beats Opn which observation prediction model but also the deeper the look ahead, the better it does so.",
                    "label": 0
                },
                {
                    "sent": "Five is only slightly better than three.",
                    "label": 0
                },
                {
                    "sent": "I'd far tell you what.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Options where the options were go straight until you get to a choice point.",
                    "label": 0
                },
                {
                    "sent": "So go right until you get to Choicepoint go up.",
                    "label": 0
                },
                {
                    "sent": "When we get to Choicepoint, go left those with the options OK.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also did some Atari game work, so we tried many different games.",
                    "label": 0
                },
                {
                    "sent": "The blue curve is is VPN's performance in DQN is the DQ and performance.",
                    "label": 0
                },
                {
                    "sent": "And in all games, but two.",
                    "label": 0
                },
                {
                    "sent": "I think alien and.",
                    "label": 0
                },
                {
                    "sent": "And Pacman it led to an improvement performance over over over.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to abrupt stop that.",
                    "label": 0
                },
                {
                    "sent": "I'm going to stop, take a few minutes for questions, but you have a question right there.",
                    "label": 0
                },
                {
                    "sent": "We don't learn options accident question.",
                    "label": 0
                },
                {
                    "sent": "These were hard wired options were learning to plan given hard wired options.",
                    "label": 0
                },
                {
                    "sent": "We're not discovering options.",
                    "label": 0
                },
                {
                    "sent": "That would be really cool.",
                    "label": 0
                },
                {
                    "sent": "And we're working on it, but that's a really cool problem.",
                    "label": 0
                },
                {
                    "sent": "Yes, Diana has some work on discovering options.",
                    "label": 0
                },
                {
                    "sent": "Looking at the internal structure of the agents and made this question before.",
                    "label": 0
                },
                {
                    "sent": "Steps to do that, but we can think value iteration as two agents one, but give your readers were all there and then we are training an intrinsic.",
                    "label": 0
                },
                {
                    "sent": "Why not?",
                    "label": 0
                },
                {
                    "sent": "Are you saying why not give the optimal value as the optimal?",
                    "label": 0
                },
                {
                    "sent": "Why isn't the optimal value function the optimal internal reward even more?",
                    "label": 0
                },
                {
                    "sent": "Training of our reactive agents.",
                    "label": 0
                },
                {
                    "sent": "But can I answer the interesting question first?",
                    "label": 0
                },
                {
                    "sent": "Why isn't the optimal value function the optimal internal reward?",
                    "label": 0
                },
                {
                    "sent": "Anybody wants to help me answer that?",
                    "label": 0
                },
                {
                    "sent": "Why isn't the optimal value function of the optimal internal reward?",
                    "label": 0
                },
                {
                    "sent": "We had terrible choice of reward.",
                    "label": 0
                },
                {
                    "sent": "Hawaii.",
                    "label": 0
                },
                {
                    "sent": "You're double counting rewards no, no.",
                    "label": 0
                },
                {
                    "sent": "I'm going to cheap.",
                    "label": 0
                },
                {
                    "sent": "That's not the point.",
                    "label": 0
                },
                {
                    "sent": "Sorry, that's not the question.",
                    "label": 0
                },
                {
                    "sent": "Here's the answer.",
                    "label": 0
                },
                {
                    "sent": "Baskets good, that's that's the start of the answer right?",
                    "label": 0
                },
                {
                    "sent": "Which is that if you care about a single task, of course you can solve the task.",
                    "label": 0
                },
                {
                    "sent": "When you're done.",
                    "label": 0
                },
                {
                    "sent": "The point of a reward function is to make an agent that's good at learning good at behaving across tasks.",
                    "label": 0
                },
                {
                    "sent": "Which is what I meant when I said in the very beginning, right?",
                    "label": 0
                },
                {
                    "sent": "The real challenge is what is the right level divine?",
                    "label": 0
                },
                {
                    "sent": "The reward function?",
                    "label": 0
                },
                {
                    "sent": "You know there is the objective evolutionary reward function.",
                    "label": 0
                },
                {
                    "sent": "Procreate.",
                    "label": 0
                },
                {
                    "sent": "But sharing a Cup of coffee right now, how does that affected by that reward function?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "We have much more proxamol reward functions, but two proximal would be too constraining because very inflexible.",
                    "label": 0
                },
                {
                    "sent": "And so to get the flexibility of behavior, we have the ability to adapt to new environments requires the sort of just the right reward function.",
                    "label": 0
                },
                {
                    "sent": "And that's what evolution presumably did, or at least wanted one.",
                    "label": 0
                },
                {
                    "sent": "But that wasn't your question.",
                    "label": 0
                },
                {
                    "sent": "What is your question?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's like.",
                    "label": 0
                },
                {
                    "sent": "I assisted him by iteration, so if you used by literation and you get the finally the value, this is like training and agent where for where they can only decide what to do instead of extending a little quality.",
                    "label": 0
                },
                {
                    "sent": "What I say is that this isn't like this already happening.",
                    "label": 0
                },
                {
                    "sent": "If we allow ourselves to get into the values of internal agent.",
                    "label": 0
                },
                {
                    "sent": "So my question then is what about the communication cost?",
                    "label": 0
                },
                {
                    "sent": "Meaning this is the designer and this is the.",
                    "label": 0
                },
                {
                    "sent": "Agent there I'm participation for further robustness, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "This offer separation.",
                    "label": 0
                },
                {
                    "sent": "So there's a whole body of work I do that I haven't talked about, which is multi agent problems in which communication costs player also have a line of work.",
                    "label": 0
                },
                {
                    "sent": "If I were to give a talk on safety in AI, I would talk about work on querying.",
                    "label": 0
                },
                {
                    "sent": "How should it robot query human being and human being is distracted and as a cost to communicate all kinds of work that multi agent systems people are doing that go in that direction.",
                    "label": 0
                },
                {
                    "sent": "So yes so important questions.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Time to eat and we come back.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be safety and AI, but it's going to be safe.",
                    "label": 0
                },
                {
                    "sent": "MDP is going to send these, yes?",
                    "label": 0
                },
                {
                    "sent": "Phil Thomas, right after the coffee break, I think we're going to see.",
                    "label": 0
                },
                {
                    "sent": "Thanks, ginger.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}