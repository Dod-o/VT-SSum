{
    "id": "3mk35eyvjnhojcj4cptxvwqchdmpdjnr",
    "title": "Towards Encoding Time in Text-Based Entity Embeddings",
    "info": {
        "author": [
            "Federico Bianchi, Dipartimento Informatica Sistemistica e Comunicazione, University of Milano - Bicocca"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_bianchi_towards_encoding_time/",
    "segmentation": [
        [
            "Good afternoon everyone.",
            "My name is Federico Bianchi and today I'm going to present our work on towards encoding time into text based entity embeddings.",
            "This is a joint work with multiple Marion Deborah notes from the University of Milano Bicocca so."
        ],
        [
            "All our short recap on knowledge graphs, knowledge graph are structural way to represent knowledge and they represent entities of the real words as node in graph and relationships between the nodes are actually relationships in the knowledge graph and generally semantics.",
            "An inference are defined using RDF's axiom and semantic similarity is achieved achieved using lexical and topological similarities approach.",
            "So I did."
        ],
        [
            "Point of view for Knowledge Graph is the one coming from knowledge graph embedding.",
            "So we generate vector representation of entities and relationships and this is quite useful for computing similarity, because as soon as we have a representation of entities and relationship in a vector space, we can just use the cosine similarity or Euclidean distance to define the similarities between entities.",
            "Also generating knowledge graph embeddings is quite useful because we have feature that can be given in input to deep learning models and thus can be used to on.",
            "Entity linking a link prediction task, for example."
        ],
        [
            "So we work on text based entity embeddings that are influential influenced by what the Mannings?",
            "Just a short recap of word embeddings.",
            "They try to embed row input text by considering Co occurrence, so by taking input sentences and they put together in vector space, those words then tend to occur together.",
            "What we do is actually use word embeddings algorithm on a different kind of taxes to generate their own presentation for entities and four types.",
            "Knowledge graph in a model that we have defined type entity embeddings so."
        ],
        [
            "The embeddings are work like this.",
            "We start from Wikipedia abstract and we use link to use named entity linking tools to disambiguate the text, and we define document collection that contains only."
        ],
        [
            "The entities then, the next step is to replace each entity with its most specific type.",
            "For example, Rome has been replaced by the Type city.",
            "Next step is to generate the embeddings for entities and four types.",
            "So we are able to generate vector representation of types basing on type to type concurrence and vector representation of NPS basing on NT 22 Co occurrence final step with this."
        ],
        [
            "Is to concatenate the embedding of entity and types and in this case what we do."
        ],
        [
            "Is going to catenate the vector of each entity with director of its most specific type.",
            "So an example vector of Chrome is being converted to vector of city."
        ],
        [
            "So why time time to the best of our knowledge, our is the first approach to towards explicitly encoding time into entity embeddings.",
            "We expect that when when we evaluate the similarity between entity times is an important factor to thinking consideration, because entity that share a point in time might also share common text, and this will bring them together in the vector space.",
            "But this might not always be the way we want our model to be able, so in this paper we provide an approach to explicitly encode time.",
            "Representation and then to find a way to control the similarity with respect to time."
        ],
        [
            "So we start from textual description of time periods using events.",
            "So there are a lot of resources online that describes event and what has happened during that time period.",
            "In our approach we concentrate on years.",
            "For example we are 1941 contains information about World War Two and we can use this kind of representation to generate our representation in a vector space."
        ],
        [
            "Actually supported by cognitive science.",
            "Let's say that memory is necessary and useful for time perception, and in our work we interpret the memory as test called description of events."
        ],
        [
            "So how do we do our embedding on fears we generate?",
            "We take text, we find we named entity linking techniques.",
            "Entities inside of text and we."
        ],
        [
            "Actually put together these entities and we we aggregate them using average."
        ],
        [
            "And we represent distributed representation of higher in the example 1941 is given by the average of all the entities that have been found inside the text of 1941."
        ],
        [
            "So we also propose two measures to try to control this.",
            "What we then called the bias that I'm bias in entity embeddings.",
            "These are two simple measures to try to control this affect that I'm flattered similarity is the first one, and he actually tries to reduce the impact of time into similarity.",
            "And one example that might give the intuition of this is to make you as president, similar independently for volume temporal context.",
            "We also provide a measure that is the same.",
            "Just one term changes for time booster similarity.",
            "With the idea of boost, the similarity within entities by considering also very very temporal context and one intuition could be to make musician politician that shared temporal context more similar in the representation."
        ],
        [
            "This is just an example of how this work.",
            "We take two entities might be barkman Bill Clinton Wi-Fi."
        ],
        [
            "Their representation in the vector space and the one in the green circle are vectors."
        ],
        [
            "We find the closest years to bark Obama, Bill Clinton in the vector space, and for example, in our in this example, Barack Obama is close to the vector 2003.",
            "So the time factor similarity."
        ],
        [
            "Is defined by the cosine similarity between the vector of the two entities mine."
        ],
        [
            "The normalized cosine similarity between 0 one of the vector of the two respective years.",
            "We also add up."
        ],
        [
            "Enter Alpha to try to control the weight of the time factor.",
            "So."
        ],
        [
            "So this brings us to our research question.",
            "We would like to evaluate the quality of the representation of our here and to study our similarity in time affect the our representation.",
            "An fire show short experiment on controlling this time bias.",
            "We have found inside the representation.",
            "So."
        ],
        [
            "This is what happens when we are reducing two dimension.",
            "The we have an you can see that on the left we have the first year of the 19th century and on the right we are getting more recent year.",
            "We tried to embed this in one dimension and to compare the point relative order.",
            "Appointing one dimension with standard standard flow of time using rank correlation measure and we were actually able to find that our representation show more or less good resembles of the time flow."
        ],
        [
            "So the first example with this was to try to explain why we get the representation we we find in the figure I showed you before, and to do this we try to compare the number of average shared entities by signals of two sequences of two and three contiguous ERS with sequences of non contiguous air.",
            "For example, comparing the shared entities between 91 and 92 and 9034 and 1992 draft of this was actually showed that actually.",
            "There are a lot of shared entities between contiguous years and this is expected before you can think that in years that are related to World War Two, alot of entities might be shared that might be common between the year of World War Two."
        ],
        [
            "Next we we try that another experiment.",
            "This is one of the experiments we did in the paper of Arbor is a more stable and more robust experiment this, but this is the most intuitive one.",
            "We tried to classify bottle belonging to World War One and World War Two, and we use the the entities coming from word one.",
            "Word two representing balls an we use K means clustering to classify them and we were actually able to divide them into group.",
            "So this is an example of.",
            "While we think there is up some sort of biasing or temporal bias inside our representation, think that share temporal contexts are closer together in the vector space."
        ],
        [
            "So last thing we did was to try controlling the time bias.",
            "So we try to propose the flattener similarity.",
            "I propose you before, and the idea is that we want to reorder the neighborhood of a given entity, the neighborhood of the vectors of a given entity, and find similar entities.",
            "In the example.",
            "One possible example would be like funding, finding entities that are similar to Barack Obama independent of time, and we try to represent this by finding order you as president."
        ],
        [
            "So the idea would be to find for example, followed Coolidge or Hoover and the."
        ],
        [
            "Would be the free correct items?",
            "Find our approach and one wrong item because Kennedy was not US President.",
            "So we."
        ],
        [
            "Tried to implement this kind of experiment is a towards experiment because it's a really short experiment but shows some interesting results.",
            "We compare different model one with that uses our types and the similarity.",
            "I've just propose you and the data set is composed by 19 president of the United States in 19 British Prime Ministers and the idea is that the given input one of the most 6, the most recent six president or Prime Minister, we should be able to find in the ranking list.",
            "The order of this president.",
            "So we tried using different model.",
            "One is our time and type model."
        ],
        [
            "Actually, the result is actually bit biased on our model because it's the one that combines both a type search and a times search, but it shows that actually both time and types are useful for detecting an finding the.",
            "The entity that we want to find also show you."
        ],
        [
            "A qualitative evaluation of these.",
            "The idea is that this is the neighborhood of Barack Obama in our model, and it shows some of the recent entities that have collaborated or worked with Barack Obama and what we did was to apply on the top ten top 100 ranking list of Arc Obama.",
            "Our time flattered and measure."
        ],
        [
            "And the result with Alpha equals 2 zero point 7 is that we get some new entities.",
            "Our eyes in the ranking list.",
            "For example, for the Coolidge and Hoover when we make that I'm flattered parameter more strong."
        ],
        [
            "We get that we actually find a lot of order US President, but we also start to introduce some error because in the ranking list we found Eleanor Roosevelt."
        ],
        [
            "So conclusion we propose an approach represent time in vector space using gallon description and we show that actually there is sort of time bias in the entity embedding and we tried to propose a measure to control this time bias.",
            "Future work will be related on the study of compositionality of time inside our representation, so all the models behave in representing period from 1994 to 92,000 years and comparison with other models like Doctor Back.",
            "And also we would like to improve our time aware similarity measures so these are references and thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Federico Bianchi and today I'm going to present our work on towards encoding time into text based entity embeddings.",
                    "label": 1
                },
                {
                    "sent": "This is a joint work with multiple Marion Deborah notes from the University of Milano Bicocca so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All our short recap on knowledge graphs, knowledge graph are structural way to represent knowledge and they represent entities of the real words as node in graph and relationships between the nodes are actually relationships in the knowledge graph and generally semantics.",
                    "label": 1
                },
                {
                    "sent": "An inference are defined using RDF's axiom and semantic similarity is achieved achieved using lexical and topological similarities approach.",
                    "label": 0
                },
                {
                    "sent": "So I did.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point of view for Knowledge Graph is the one coming from knowledge graph embedding.",
                    "label": 0
                },
                {
                    "sent": "So we generate vector representation of entities and relationships and this is quite useful for computing similarity, because as soon as we have a representation of entities and relationship in a vector space, we can just use the cosine similarity or Euclidean distance to define the similarities between entities.",
                    "label": 1
                },
                {
                    "sent": "Also generating knowledge graph embeddings is quite useful because we have feature that can be given in input to deep learning models and thus can be used to on.",
                    "label": 1
                },
                {
                    "sent": "Entity linking a link prediction task, for example.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we work on text based entity embeddings that are influential influenced by what the Mannings?",
                    "label": 1
                },
                {
                    "sent": "Just a short recap of word embeddings.",
                    "label": 1
                },
                {
                    "sent": "They try to embed row input text by considering Co occurrence, so by taking input sentences and they put together in vector space, those words then tend to occur together.",
                    "label": 1
                },
                {
                    "sent": "What we do is actually use word embeddings algorithm on a different kind of taxes to generate their own presentation for entities and four types.",
                    "label": 1
                },
                {
                    "sent": "Knowledge graph in a model that we have defined type entity embeddings so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The embeddings are work like this.",
                    "label": 0
                },
                {
                    "sent": "We start from Wikipedia abstract and we use link to use named entity linking tools to disambiguate the text, and we define document collection that contains only.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The entities then, the next step is to replace each entity with its most specific type.",
                    "label": 0
                },
                {
                    "sent": "For example, Rome has been replaced by the Type city.",
                    "label": 0
                },
                {
                    "sent": "Next step is to generate the embeddings for entities and four types.",
                    "label": 0
                },
                {
                    "sent": "So we are able to generate vector representation of types basing on type to type concurrence and vector representation of NPS basing on NT 22 Co occurrence final step with this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to concatenate the embedding of entity and types and in this case what we do.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is going to catenate the vector of each entity with director of its most specific type.",
                    "label": 0
                },
                {
                    "sent": "So an example vector of Chrome is being converted to vector of city.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why time time to the best of our knowledge, our is the first approach to towards explicitly encoding time into entity embeddings.",
                    "label": 1
                },
                {
                    "sent": "We expect that when when we evaluate the similarity between entity times is an important factor to thinking consideration, because entity that share a point in time might also share common text, and this will bring them together in the vector space.",
                    "label": 1
                },
                {
                    "sent": "But this might not always be the way we want our model to be able, so in this paper we provide an approach to explicitly encode time.",
                    "label": 1
                },
                {
                    "sent": "Representation and then to find a way to control the similarity with respect to time.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we start from textual description of time periods using events.",
                    "label": 1
                },
                {
                    "sent": "So there are a lot of resources online that describes event and what has happened during that time period.",
                    "label": 0
                },
                {
                    "sent": "In our approach we concentrate on years.",
                    "label": 0
                },
                {
                    "sent": "For example we are 1941 contains information about World War Two and we can use this kind of representation to generate our representation in a vector space.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually supported by cognitive science.",
                    "label": 0
                },
                {
                    "sent": "Let's say that memory is necessary and useful for time perception, and in our work we interpret the memory as test called description of events.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we do our embedding on fears we generate?",
                    "label": 0
                },
                {
                    "sent": "We take text, we find we named entity linking techniques.",
                    "label": 0
                },
                {
                    "sent": "Entities inside of text and we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually put together these entities and we we aggregate them using average.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we represent distributed representation of higher in the example 1941 is given by the average of all the entities that have been found inside the text of 1941.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we also propose two measures to try to control this.",
                    "label": 0
                },
                {
                    "sent": "What we then called the bias that I'm bias in entity embeddings.",
                    "label": 0
                },
                {
                    "sent": "These are two simple measures to try to control this affect that I'm flattered similarity is the first one, and he actually tries to reduce the impact of time into similarity.",
                    "label": 1
                },
                {
                    "sent": "And one example that might give the intuition of this is to make you as president, similar independently for volume temporal context.",
                    "label": 0
                },
                {
                    "sent": "We also provide a measure that is the same.",
                    "label": 0
                },
                {
                    "sent": "Just one term changes for time booster similarity.",
                    "label": 1
                },
                {
                    "sent": "With the idea of boost, the similarity within entities by considering also very very temporal context and one intuition could be to make musician politician that shared temporal context more similar in the representation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just an example of how this work.",
                    "label": 0
                },
                {
                    "sent": "We take two entities might be barkman Bill Clinton Wi-Fi.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their representation in the vector space and the one in the green circle are vectors.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We find the closest years to bark Obama, Bill Clinton in the vector space, and for example, in our in this example, Barack Obama is close to the vector 2003.",
                    "label": 0
                },
                {
                    "sent": "So the time factor similarity.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is defined by the cosine similarity between the vector of the two entities mine.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The normalized cosine similarity between 0 one of the vector of the two respective years.",
                    "label": 0
                },
                {
                    "sent": "We also add up.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Enter Alpha to try to control the weight of the time factor.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this brings us to our research question.",
                    "label": 0
                },
                {
                    "sent": "We would like to evaluate the quality of the representation of our here and to study our similarity in time affect the our representation.",
                    "label": 0
                },
                {
                    "sent": "An fire show short experiment on controlling this time bias.",
                    "label": 0
                },
                {
                    "sent": "We have found inside the representation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is what happens when we are reducing two dimension.",
                    "label": 0
                },
                {
                    "sent": "The we have an you can see that on the left we have the first year of the 19th century and on the right we are getting more recent year.",
                    "label": 0
                },
                {
                    "sent": "We tried to embed this in one dimension and to compare the point relative order.",
                    "label": 0
                },
                {
                    "sent": "Appointing one dimension with standard standard flow of time using rank correlation measure and we were actually able to find that our representation show more or less good resembles of the time flow.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first example with this was to try to explain why we get the representation we we find in the figure I showed you before, and to do this we try to compare the number of average shared entities by signals of two sequences of two and three contiguous ERS with sequences of non contiguous air.",
                    "label": 1
                },
                {
                    "sent": "For example, comparing the shared entities between 91 and 92 and 9034 and 1992 draft of this was actually showed that actually.",
                    "label": 1
                },
                {
                    "sent": "There are a lot of shared entities between contiguous years and this is expected before you can think that in years that are related to World War Two, alot of entities might be shared that might be common between the year of World War Two.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next we we try that another experiment.",
                    "label": 0
                },
                {
                    "sent": "This is one of the experiments we did in the paper of Arbor is a more stable and more robust experiment this, but this is the most intuitive one.",
                    "label": 1
                },
                {
                    "sent": "We tried to classify bottle belonging to World War One and World War Two, and we use the the entities coming from word one.",
                    "label": 1
                },
                {
                    "sent": "Word two representing balls an we use K means clustering to classify them and we were actually able to divide them into group.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of.",
                    "label": 0
                },
                {
                    "sent": "While we think there is up some sort of biasing or temporal bias inside our representation, think that share temporal contexts are closer together in the vector space.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So last thing we did was to try controlling the time bias.",
                    "label": 1
                },
                {
                    "sent": "So we try to propose the flattener similarity.",
                    "label": 0
                },
                {
                    "sent": "I propose you before, and the idea is that we want to reorder the neighborhood of a given entity, the neighborhood of the vectors of a given entity, and find similar entities.",
                    "label": 1
                },
                {
                    "sent": "In the example.",
                    "label": 0
                },
                {
                    "sent": "One possible example would be like funding, finding entities that are similar to Barack Obama independent of time, and we try to represent this by finding order you as president.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea would be to find for example, followed Coolidge or Hoover and the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would be the free correct items?",
                    "label": 0
                },
                {
                    "sent": "Find our approach and one wrong item because Kennedy was not US President.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tried to implement this kind of experiment is a towards experiment because it's a really short experiment but shows some interesting results.",
                    "label": 0
                },
                {
                    "sent": "We compare different model one with that uses our types and the similarity.",
                    "label": 0
                },
                {
                    "sent": "I've just propose you and the data set is composed by 19 president of the United States in 19 British Prime Ministers and the idea is that the given input one of the most 6, the most recent six president or Prime Minister, we should be able to find in the ranking list.",
                    "label": 1
                },
                {
                    "sent": "The order of this president.",
                    "label": 0
                },
                {
                    "sent": "So we tried using different model.",
                    "label": 0
                },
                {
                    "sent": "One is our time and type model.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, the result is actually bit biased on our model because it's the one that combines both a type search and a times search, but it shows that actually both time and types are useful for detecting an finding the.",
                    "label": 0
                },
                {
                    "sent": "The entity that we want to find also show you.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A qualitative evaluation of these.",
                    "label": 0
                },
                {
                    "sent": "The idea is that this is the neighborhood of Barack Obama in our model, and it shows some of the recent entities that have collaborated or worked with Barack Obama and what we did was to apply on the top ten top 100 ranking list of Arc Obama.",
                    "label": 0
                },
                {
                    "sent": "Our time flattered and measure.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the result with Alpha equals 2 zero point 7 is that we get some new entities.",
                    "label": 0
                },
                {
                    "sent": "Our eyes in the ranking list.",
                    "label": 0
                },
                {
                    "sent": "For example, for the Coolidge and Hoover when we make that I'm flattered parameter more strong.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We get that we actually find a lot of order US President, but we also start to introduce some error because in the ranking list we found Eleanor Roosevelt.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conclusion we propose an approach represent time in vector space using gallon description and we show that actually there is sort of time bias in the entity embedding and we tried to propose a measure to control this time bias.",
                    "label": 1
                },
                {
                    "sent": "Future work will be related on the study of compositionality of time inside our representation, so all the models behave in representing period from 1994 to 92,000 years and comparison with other models like Doctor Back.",
                    "label": 0
                },
                {
                    "sent": "And also we would like to improve our time aware similarity measures so these are references and thank you.",
                    "label": 0
                }
            ]
        }
    }
}