{
    "id": "6jren54fnywwnwniwcnxkyjre5ytr4ie",
    "title": "Lean Kernels in Description Logics",
    "info": {
        "author": [
            "Rafael Pe\u00f1aloza, Faculty of Computer Science, Free University of Bozen-Bolzano"
        ],
        "published": "July 10, 2017",
        "recorded": "May 2017",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2017_penaloza_lean_kernels/",
    "segmentation": [
        [
            "Thank you all for being here in this first session.",
            "So as you can see I will speak about lean kernels in description logics and this is some work that I'm been doing with my colleagues Carlos, Alex Angelo from Spain and Portugal, but they couldn't be here unfortunately.",
            "OK, so I will start with a bit of motivation.",
            "What are our goals in general so?"
        ],
        [
            "The way I see reasoning in, at least for this paper for this presentation, is we have a big ontology and I'm using the term ontology in the in the most abstract sense that you can think.",
            "So it's just a set of restrictions or something, right?",
            "So this could be RDF triples.",
            "This could be description, logic, ontology.",
            "This can be some kinds of rules, whatever.",
            "But we have this huge set of of restrictions and then we have some kind of reasoning procedure that lets us.",
            "Derive some consequence right so we know that from this ontology we have a consequence and sometimes we are interested not only knowing that there is such a consequence that follows from it, but we want to know why this follows right?",
            "So from which of all of these bunch of restrictions we have this consequences then we start analyzing our ontology and maybe we find this five axioms that entail that consequence so that knowing those five axioms is enough to derive the consequences and we continue analyzing it.",
            "And then we find these other three and say OK, these three are also enough.",
            "We go ahead and then we find another five and so we can continue this task trying to find all the possible ways to derive this consequence.",
            "All the all, the minimal ways to do it.",
            "Right, but while we were doing this, we were focusing so all our important axioms were here in the lower left corner, right?",
            "But we have all this noise around that we were looking at, so this usually makes things more or less efficient because we have to trim out all this noise every time that we are searching for a new answer, right?",
            "So what we will actually like to do is to just focus our attention on the precise area of the ontology that will give us.",
            "The right answer is that we want to do right and ignore the rest.",
            "So what we want to do is that when we are trying to explain this consequence file, we don't want to look at the whole ontology, but we want to 1st zoom in into a part of the ontology that has some nice properties and then just focus on that part to actually solve our problem right and make things more faster.",
            "So this is a very simple idea."
        ],
        [
            "OK, So what we want to do so our desiderata are, well, we want to find a sub ontology, right?",
            "The subset of constraints are subset of axioms that contains all the minimal ways to derive the consequence that we're interested in.",
            "So this minimal ways of deriving a consequence we call the meanness.",
            "So we will say that an ontology that contains all these, all these minimum, all these meanness is called Minna preserving.",
            "That's what we want to do.",
            "The other thing, of course, is that we want this sub ontology to be.",
            "Small as possible, so it might contain some additional axioms that are superfluous, but we don't really need them to derive the consequences, but it's too many.",
            "And the last thing is that we want to compute this event aladji as fast as possible.",
            "OK, now this last two Desiderata are a bit of a conflict with each other, right?",
            "So to find.",
            "So I can give you very efficiently, 11 ontology that contains all the minimal costs for a forum for consequence.",
            "But just giving you the whole ontology, right?",
            "So in constant time there is a solution, right?",
            "But this is not small.",
            "On the other side, it is known theoretically and has been checked in practice that actually finding just the union of all the meanness.",
            "So the smallest possible minor preserving set of axioms is is very hard for, so it's intractable in theory, and it's very expensive when we want to do it in practice, right?",
            "So we have to look at the trade off, so we allow some additional axioms.",
            "As long as we can get it fast enough.",
            "So that's our goal.",
            "OK, so we started this work by looking at the different kind of ontology that maybe some of you will not call it an ontology, but I will, which is propositional logic, right?",
            "So in terms of propositional logic, when I when I say ontology well I just said I'm I'm I'm using the term in the most abstract way so it's just a set of constraints.",
            "So it's a set of clauses of propositional logic classes.",
            "For me, right?",
            "So I have this formulas or.",
            "This big formula that is just a conjunction of clauses, right?",
            "So that's a propositional formula and usually in propositional logic.",
            "People want to check whether a formula so this set of clauses is unsatisfiable, which in recent in terms means whether we can derive the empty clause from it.",
            "So whether this set of clauses and tails the empty class and what I have been calling Mina in the propositional logic has been studied a lot, and they're called music so minimal unsatisfiable preserving subsets or sub formulas, right?",
            "So I will still call it mean I just to keep in track with everything, But this has a name in propositional logic.",
            "Right?",
            "And."
        ],
        [
            "What happens so recently?",
            "Some work on trying to understand the causes for unsatisfiability, so trying to find all the mousses.",
            "Came out with a new notion that the first people that looked at it called it link kernels, which is a small soup formula, so a small subset of clauses that approximates the union of all muses or or the union of all, minus, which is exactly what we want to do right in the abstract sense.",
            "So we started looking at that notion.",
            "So where are these link kernels?",
            "So to understand link kernels, I first have to tell you a bit of propositional logic and in particular resolution.",
            "So I guess most of you know what it is, but the basic idea about resolution is that we take two clauses from our set of clauses and then we combine them together to generate a new one, right that is entailed by the previous two.",
            "So if the two that we took our true, then the third one that we generate is true.",
            "So a simple example is this."
        ],
        [
            "So we have two classes and in one classes we have X, X2 and in the other class we have not X2, so putting them together we get these new clothes that doesn't have any reference to X2 in right and it takes the disjunction of all the other things.",
            "So we took two classes and then we get a new one that is derived from.",
            "OK, and if we apply this process to decide on satisfiability, we just want to see whether we can derive an empty close by applying such a process of resolution.",
            "OK, so the way Lynn kernels are defined is just is the set of all the original clauses from our set that appear in some resolution.",
            "Proof for the empty class, right?",
            "So we look at all the possible resolution proofs for finding the empty class.",
            "We accumulate all the axioms that we used to all the original classes and this is the lean kernel.",
            "OK, this is a very simple notion.",
            "OK and well, in general we can actually define it for any class that we can derive so we can have the link kernel for any derived class.",
            "But in propositional logic."
        ],
        [
            "They're only interested in the empty one.",
            "So what is nice about this Lincoln as well?",
            "What they have is they are very fast to compute, so there are now available, some good reasoners that compute this link kernels very fast.",
            "They over approximate the union of all the music, so they are mean and preserving ontologies right?",
            "So they contain all the minors, but the approximation is is tight enough.",
            "So it might contain some superfluous classes, but not too many right?",
            "And this is.",
            "This was precisely what were the city right at the beginning, right?",
            "So the question is, can we generalize this to other kinds of ontology languages, and in particular, to description logics?",
            "Which is what we're interested in peace.",
            "In this particular paper.",
            "OK, and so the task of our paper was to generalize this idea of lean kernels to description logics and see whether this actually provides us with the Cedar.",
            "After that we're looking at."
        ],
        [
            "So now.",
            "Remember that this notion of lean kernel is based on a derivation procedure.",
            "So to define Lynn Kernel, we started by using resolution and then saying something about all the possible proofs of our class, right?",
            "So using this derivation procedure so to generalize this description, logics or to any other language, we have to also speak about the procedure with which we are reasoning in this logic.",
            "So for this paper we focused on these consequence based reasoning methods and the basic idea about these methods.",
            "Is there are algorithms that take some consequences that we have derived already and some axioms from the original ontology and put them together to derive new consequences, right?",
            "So they make explicit some of the implicit consequences of our ontology, right?",
            "So not all of them, because in description logics we will have infinitely many of them, but some relevant subset of them.",
            "OK, and in general we just procedures that are have rules of this shape, right?",
            "So they take some explicit knowledge, so this should be 0 some axioms from the original ontology and then they derive some additional explicit knowledge and then add it to our set of explicit knowledge.",
            "OK, so this is a very abstract notion of consequence based reasoning.",
            "I will so to actually put it down in description logics.",
            "I would just speak very fast about ALC and will explain you this notion of consequence based algorithms in this ALC logic, right?",
            "So ALC very very fast we have concepts that are derived from concept names and then we can have negations and then we can do conjunctions, disjunctions and this special, existential and value restrictions that we can create.",
            "And in general we can put our axioms, I will, restrictions will be axioms of this form.",
            "So this GIS."
        ],
        [
            "And I just simplifying a bit without loss of generality.",
            "But just to we only allow these GIS to be of this five different shapes, right?",
            "So we only have conjunctions of concept names are subsumed to our superclass of another concept name or a concept name is a subclass of a disjunction of concept names or we have an existential restriction either on the left or on the right, or a value restriction on the right?",
            "And this has been proven before that you can change any.",
            "ALC Tee box with arbitrary concept inclusions into this normal form in linear time.",
            "So just assume that they are in there."
        ],
        [
            "OK, and the consequences based algorithm for else just takes this 6 rules right and I don't want to go into the details of how they work, but I just want to make clear that this that there is such an algorithm.",
            "The important thing is that this so this H case are just conjunctions of literals, so concept names or negative concept names and M&N are disjunctions of literals, right?",
            "And where I have these pairs, they're just saying, so they are short or.",
            "Intuitively, they describe that this conjunction of literals here is subsumed by this disjunction of literals, given that all the actions in the original ontology are true, right?",
            "So this is an entailment and, well, this is a more complex one says that H is subsumed by North, or there exists in our successor.",
            "There is in K, and this is so, so collection, and this rule is the only thing that they do is that they take some of this assumption that we know that they are.",
            "True and take some actions potentially and then just derive some additional knowledge.",
            "So for example, the first rule over there is essentially resolution.",
            "If you think about it.",
            "So resolution for propositional logic.",
            "And then we have some others that deal with the actual ALC knowledge."
        ],
        [
            "OK, now.",
            "So our goal was to define Lynn kernels, right?",
            "So we take an arbitrary consequence based algorithm so it doesn't have to be the LC-1.",
            "Any consequence based algorithm that you know that is sound and complete for the kind of consequence that you are interested in.",
            "And you take an ontology, right?",
            "So a set of axioms in that language.",
            "A consequence from this ontology and the link kernel for that consequences for that with respect to that ontology, and that consequences based algorithm is just the set of all axioms that we will use in some derivation.",
            "Proof for that consequence.",
            "So we look at all the possible ways to prove this.",
            "Consequences we look at the actions that appear in bits and this is it.",
            "OK, so it's the exact same idea behind the link kernels in propositional logic, so just extended description logics, and in particular we have to pay attention of what algorithm reducing."
        ],
        [
            "Right, so this is the concept that consequence based algorithm.",
            "Skype so."
        ],
        [
            "Just to set a small example how this applies in this ALC setting, suppose that we have this six axioms right and we want to so from these axioms we know that a subsumed by C, right?",
            "Because a subbase subsumed by C?",
            "So if we want to derive the link kernel for this con cequence right, what can we do?",
            "OK, so let's let's look at the the consequence based algorithm.",
            "11 proof is the one that I just described.",
            "Let's look at the different proof for that subscription so we can start with.",
            "A is subsumed by a cytology.",
            "So we start with that and we have this axiom right?",
            "And so we can derive by using the first rule that a subsumed by B.",
            "So this is the first consequence now also from this technology and the second rule and this axiom we can derive that a is subsumed by the empty set or exist are a right?",
            "So just by using our notation that we have right now using this information and this extraction we can derive this further information.",
            "So this is the.",
            "The rule here, so it gets a bit complicated over here, so I don't want to go into the details, but you can play around with it and you will see.",
            "And then using the fourth rule, since we have A and not B&A and not B is in bottom, so I'm skipping 2 steps in here.",
            "But then we can derive that a subsumed by be right and so this is so here we have a resolution, not a resolution.",
            "Consequences based proof for this assumption is subsumed by B and this uses this for axioms and then of course as in B and then be subsumed by.",
            "See we have AC and this uses this for actions right?",
            "So those four accidents will be in the link kernel.",
            "And with respect to these ontology now of course, you already know that this is not minimal, because we can also derive it just from these two, but it's OK.",
            "So we have some support for success, just.",
            "Emphasize that this algorithm actually has too much at sometimes, but this."
        ],
        [
            "OK. Now the first question is OK. We have defined this nice notion of Lynn kernels.",
            "So how do we compute them, right?",
            "So how do we get?",
            "Because the definition is a bit.",
            "Too big in the sense that it just says you have to look at all the possible proofs, right?",
            "So how do we check that?",
            "We have built all the possible proofs?",
            "And how do we actually take this set of axioms right?",
            "And the first thing that we prove is that if we have a consequence based algorithm, then we can just very easily transform it into an algorithm that computes the lean kernel for all the consequences that are derived by this algorithm, and the idea is again very simple.",
            "Base for every consequence that we derive, we attach it as a label.",
            "The set of all the actions that were used to derive it right.",
            "And the other thing is that usually consequence based algorithms stop once that they derive the consequence that you're interested in.",
            "Well, since we want to find all the possible ways of it, we cannot stop.",
            "We have to continue applying rules until separation, so we will need to apply more rules and.",
            "If we find the same consequence twice, we don't generate it twice, but we just update the set of axioms that are associated to it.",
            "1.",
            "And the nice thing of this algorithm is that by running it once, we derive the link kernels of all the consequences that are derived by the original consequence based algorithm.",
            "So not just for one that we're interested in, but for all of them.",
            "And the other thing is that of course we have an overhead by doing this, we have to keep in memory all these axioms and we want to we have to apply rules until separation.",
            "So we apply more.",
            "But this overhead is just linear on the size of the of the ontology.",
            "So well just it might be big, but this is not an overkill.",
            "And well, we can prove that is mean are preserving.",
            "So we have nice properties for this, OK?",
            "So just as an example from the previous derivation, the only thing that we will do is so if you just remember the steps that I did before I have the exact same steps, just."
        ],
        [
            "I have now the set of axioms that were used for the derivation, so I used the first axiom, then for the second step we use the second action, so we will have it then for this for the third step we used some knowledge here that has action one some knowledge that has axiom two and then we use the third action.",
            "So in the derivation we will have this knowledge needs these reactions to be derived.",
            "From this we will get another step that will derive again a subsumed by B and we don't derive it again.",
            "At that point we just.",
            "Update the label of the A subsumed by B that we had created before and since we have to apply it until saturation, we will have an extra rule that doesn't have anything to do with the consequences that we were interested in, but doesn't matter.",
            "OK, so we will just run it and then we get to this kind of information between testing.",
            "OK. OK, so we have we have done this.",
            "We have defined this link kernels.",
            "We have some theoretical properties of it.",
            "Now we want to check whether our algorithm or whether this notion and our implementation that we have actually satisfies the desiderata.",
            "That I started with, right?",
            "So we want to see whether this is the notion that we are actually looking for."
        ],
        [
            "Right, and so to do this.",
            "What we did is that we tested, so we implemented it not for a subclass of AOC.",
            "So for a smaller language yield plus and then we looked at all the link kernels of all the atomics assumptions that follow from these five ontologies, right?",
            "So just a small data, small information.",
            "So this is the size of the apologies.",
            "So in particular we have this very big one and this is the number of subscriptions that we checked in the lean kernel.",
            "So overall our tests may we have to recompute at 6 million more than 6 million Lynn kernels, right?",
            "To check out these consumers.",
            "Now we need to compare.",
            "It's our first desiderata.",
            "Was it should be?",
            "Well, the first was that it's me not preserving that we already proved.",
            "But we said, it should be not too big.",
            "Should be small, right?",
            "So we have to compare it against something.",
            "So we looked at different."
        ],
        [
            "Notion of minor preserving modules.",
            "Yes.",
            "And which are the locality various modules which in practice, and so in the last years they have been proposed so way to approximate, so to have a good, meaner preserving subset.",
            "Entology because they're fast to compute, and they give a good approximation.",
            "So we want to compare against them.",
            "Are we better than that?",
            "OK, so the first thing that we did is prove formally that for ALC.",
            "So this unfortunately we couldn't do it.",
            "In the general setting which all the previous things that I said were true for arbitrary consequence based algorithms, but formally for else, we can prove that so well, we knew that is meaner preserving, but also that the link kernel is always contained in this star locality based module, which is also with a subset of another localities based model that is the bottom body.",
            "OK so we know that our approximation cannot be worse than this OK, but so is it.",
            "Better in general and what happens in practice, right?",
            "So we look at these five ontologies that we have."
        ],
        [
            "And essentially what we have is this.",
            "So this plot where it tries to show is how much bigger is the star module compared in proportion to the size of the link, kernel right?",
            "And So what I want to highlight here are two to specific things, so we have this point over there, right?",
            "That is so.",
            "See that this scale is logarithmic over there, so that's something beyond to 14,000 something.",
            "So and remember that.",
            "So this is for full gallon.",
            "Full gallon has 33,000 axioms, so these are 5000 subsumption relations that where the link kernel had size either one or two and the star module have either half of the size of the ontology or the full ontology.",
            "That's a big difference in the on the other side, which is, but for us in some sense is there is here.",
            "This is NCI in the bottom line and see I forgot ontology and see I we have exactly the same size for all the time so it returned the link kernel and the start module were always the same.",
            "So we have these two limits."
        ],
        [
            "OK, so this is maybe this is not very visible, but here we did some statistical test to see whether our sets were significantly smaller and it's actually much very significantly smaller in all cases except in NCI where the answer is always the same and The thing is also we also got for example in the gene ontology.",
            "Our sets correspond exactly to the Union of all minus, so it's a very good approximation in those cases."
        ],
        [
            "OK, now the the theater is whether it's fast, right?",
            "And it depends on what you think what you mean by being faster, right?",
            "So the problem with our algorithm is that it computes all the link kernels in one run, right?",
            "So if you see it for as Nomad, it runs in 11,000 seconds, so that's not fast.",
            "But in 11,000 seconds it computes the lean kernel for 5.3 million consequences, right?",
            "So if we are interested in finding all of them, this is very fast.",
            "Right, so this this bottom module, which is the faster of the two in the minimum takes 21 seconds.",
            "So for this amount of thing it will take 530,000 seconds, right?",
            "Of course, if you don't want to compute all of them and you're just interested in one consequence, this might be an overkill.",
            "So it depends what you want to do.",
            "OK, so just to."
        ],
        [
            "Clude link earners are a good approximation for the Union of ominous.",
            "This is I wanted to say they can be effective effectively and efficiently in some sense.",
            "Computed for all consequences with a linear overhead on the over the original consequence based algorithm.",
            "It is not called directive, but if you want a goal directed approach, so also from some additional tests that I didn't present, you can first compute the bottom module that is very fast, but it's not very good approximation.",
            "But from that bottom module you can then sorry, compute the link kernel and that's all, any questions?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you all for being here in this first session.",
                    "label": 0
                },
                {
                    "sent": "So as you can see I will speak about lean kernels in description logics and this is some work that I'm been doing with my colleagues Carlos, Alex Angelo from Spain and Portugal, but they couldn't be here unfortunately.",
                    "label": 1
                },
                {
                    "sent": "OK, so I will start with a bit of motivation.",
                    "label": 0
                },
                {
                    "sent": "What are our goals in general so?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The way I see reasoning in, at least for this paper for this presentation, is we have a big ontology and I'm using the term ontology in the in the most abstract sense that you can think.",
                    "label": 0
                },
                {
                    "sent": "So it's just a set of restrictions or something, right?",
                    "label": 0
                },
                {
                    "sent": "So this could be RDF triples.",
                    "label": 0
                },
                {
                    "sent": "This could be description, logic, ontology.",
                    "label": 0
                },
                {
                    "sent": "This can be some kinds of rules, whatever.",
                    "label": 0
                },
                {
                    "sent": "But we have this huge set of of restrictions and then we have some kind of reasoning procedure that lets us.",
                    "label": 0
                },
                {
                    "sent": "Derive some consequence right so we know that from this ontology we have a consequence and sometimes we are interested not only knowing that there is such a consequence that follows from it, but we want to know why this follows right?",
                    "label": 0
                },
                {
                    "sent": "So from which of all of these bunch of restrictions we have this consequences then we start analyzing our ontology and maybe we find this five axioms that entail that consequence so that knowing those five axioms is enough to derive the consequences and we continue analyzing it.",
                    "label": 0
                },
                {
                    "sent": "And then we find these other three and say OK, these three are also enough.",
                    "label": 0
                },
                {
                    "sent": "We go ahead and then we find another five and so we can continue this task trying to find all the possible ways to derive this consequence.",
                    "label": 0
                },
                {
                    "sent": "All the all, the minimal ways to do it.",
                    "label": 0
                },
                {
                    "sent": "Right, but while we were doing this, we were focusing so all our important axioms were here in the lower left corner, right?",
                    "label": 0
                },
                {
                    "sent": "But we have all this noise around that we were looking at, so this usually makes things more or less efficient because we have to trim out all this noise every time that we are searching for a new answer, right?",
                    "label": 0
                },
                {
                    "sent": "So what we will actually like to do is to just focus our attention on the precise area of the ontology that will give us.",
                    "label": 0
                },
                {
                    "sent": "The right answer is that we want to do right and ignore the rest.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is that when we are trying to explain this consequence file, we don't want to look at the whole ontology, but we want to 1st zoom in into a part of the ontology that has some nice properties and then just focus on that part to actually solve our problem right and make things more faster.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple idea.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what we want to do so our desiderata are, well, we want to find a sub ontology, right?",
                    "label": 0
                },
                {
                    "sent": "The subset of constraints are subset of axioms that contains all the minimal ways to derive the consequence that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So this minimal ways of deriving a consequence we call the meanness.",
                    "label": 0
                },
                {
                    "sent": "So we will say that an ontology that contains all these, all these minimum, all these meanness is called Minna preserving.",
                    "label": 0
                },
                {
                    "sent": "That's what we want to do.",
                    "label": 0
                },
                {
                    "sent": "The other thing, of course, is that we want this sub ontology to be.",
                    "label": 0
                },
                {
                    "sent": "Small as possible, so it might contain some additional axioms that are superfluous, but we don't really need them to derive the consequences, but it's too many.",
                    "label": 1
                },
                {
                    "sent": "And the last thing is that we want to compute this event aladji as fast as possible.",
                    "label": 0
                },
                {
                    "sent": "OK, now this last two Desiderata are a bit of a conflict with each other, right?",
                    "label": 0
                },
                {
                    "sent": "So to find.",
                    "label": 0
                },
                {
                    "sent": "So I can give you very efficiently, 11 ontology that contains all the minimal costs for a forum for consequence.",
                    "label": 0
                },
                {
                    "sent": "But just giving you the whole ontology, right?",
                    "label": 0
                },
                {
                    "sent": "So in constant time there is a solution, right?",
                    "label": 0
                },
                {
                    "sent": "But this is not small.",
                    "label": 0
                },
                {
                    "sent": "On the other side, it is known theoretically and has been checked in practice that actually finding just the union of all the meanness.",
                    "label": 0
                },
                {
                    "sent": "So the smallest possible minor preserving set of axioms is is very hard for, so it's intractable in theory, and it's very expensive when we want to do it in practice, right?",
                    "label": 0
                },
                {
                    "sent": "So we have to look at the trade off, so we allow some additional axioms.",
                    "label": 0
                },
                {
                    "sent": "As long as we can get it fast enough.",
                    "label": 0
                },
                {
                    "sent": "So that's our goal.",
                    "label": 0
                },
                {
                    "sent": "OK, so we started this work by looking at the different kind of ontology that maybe some of you will not call it an ontology, but I will, which is propositional logic, right?",
                    "label": 0
                },
                {
                    "sent": "So in terms of propositional logic, when I when I say ontology well I just said I'm I'm I'm using the term in the most abstract way so it's just a set of constraints.",
                    "label": 0
                },
                {
                    "sent": "So it's a set of clauses of propositional logic classes.",
                    "label": 0
                },
                {
                    "sent": "For me, right?",
                    "label": 0
                },
                {
                    "sent": "So I have this formulas or.",
                    "label": 0
                },
                {
                    "sent": "This big formula that is just a conjunction of clauses, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a propositional formula and usually in propositional logic.",
                    "label": 0
                },
                {
                    "sent": "People want to check whether a formula so this set of clauses is unsatisfiable, which in recent in terms means whether we can derive the empty clause from it.",
                    "label": 0
                },
                {
                    "sent": "So whether this set of clauses and tails the empty class and what I have been calling Mina in the propositional logic has been studied a lot, and they're called music so minimal unsatisfiable preserving subsets or sub formulas, right?",
                    "label": 0
                },
                {
                    "sent": "So I will still call it mean I just to keep in track with everything, But this has a name in propositional logic.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What happens so recently?",
                    "label": 0
                },
                {
                    "sent": "Some work on trying to understand the causes for unsatisfiability, so trying to find all the mousses.",
                    "label": 0
                },
                {
                    "sent": "Came out with a new notion that the first people that looked at it called it link kernels, which is a small soup formula, so a small subset of clauses that approximates the union of all muses or or the union of all, minus, which is exactly what we want to do right in the abstract sense.",
                    "label": 0
                },
                {
                    "sent": "So we started looking at that notion.",
                    "label": 0
                },
                {
                    "sent": "So where are these link kernels?",
                    "label": 0
                },
                {
                    "sent": "So to understand link kernels, I first have to tell you a bit of propositional logic and in particular resolution.",
                    "label": 1
                },
                {
                    "sent": "So I guess most of you know what it is, but the basic idea about resolution is that we take two clauses from our set of clauses and then we combine them together to generate a new one, right that is entailed by the previous two.",
                    "label": 1
                },
                {
                    "sent": "So if the two that we took our true, then the third one that we generate is true.",
                    "label": 0
                },
                {
                    "sent": "So a simple example is this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have two classes and in one classes we have X, X2 and in the other class we have not X2, so putting them together we get these new clothes that doesn't have any reference to X2 in right and it takes the disjunction of all the other things.",
                    "label": 0
                },
                {
                    "sent": "So we took two classes and then we get a new one that is derived from.",
                    "label": 1
                },
                {
                    "sent": "OK, and if we apply this process to decide on satisfiability, we just want to see whether we can derive an empty close by applying such a process of resolution.",
                    "label": 1
                },
                {
                    "sent": "OK, so the way Lynn kernels are defined is just is the set of all the original clauses from our set that appear in some resolution.",
                    "label": 1
                },
                {
                    "sent": "Proof for the empty class, right?",
                    "label": 0
                },
                {
                    "sent": "So we look at all the possible resolution proofs for finding the empty class.",
                    "label": 0
                },
                {
                    "sent": "We accumulate all the axioms that we used to all the original classes and this is the lean kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a very simple notion.",
                    "label": 0
                },
                {
                    "sent": "OK and well, in general we can actually define it for any class that we can derive so we can have the link kernel for any derived class.",
                    "label": 0
                },
                {
                    "sent": "But in propositional logic.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They're only interested in the empty one.",
                    "label": 0
                },
                {
                    "sent": "So what is nice about this Lincoln as well?",
                    "label": 0
                },
                {
                    "sent": "What they have is they are very fast to compute, so there are now available, some good reasoners that compute this link kernels very fast.",
                    "label": 0
                },
                {
                    "sent": "They over approximate the union of all the music, so they are mean and preserving ontologies right?",
                    "label": 1
                },
                {
                    "sent": "So they contain all the minors, but the approximation is is tight enough.",
                    "label": 0
                },
                {
                    "sent": "So it might contain some superfluous classes, but not too many right?",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "This was precisely what were the city right at the beginning, right?",
                    "label": 0
                },
                {
                    "sent": "So the question is, can we generalize this to other kinds of ontology languages, and in particular, to description logics?",
                    "label": 0
                },
                {
                    "sent": "Which is what we're interested in peace.",
                    "label": 0
                },
                {
                    "sent": "In this particular paper.",
                    "label": 0
                },
                {
                    "sent": "OK, and so the task of our paper was to generalize this idea of lean kernels to description logics and see whether this actually provides us with the Cedar.",
                    "label": 1
                },
                {
                    "sent": "After that we're looking at.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Remember that this notion of lean kernel is based on a derivation procedure.",
                    "label": 0
                },
                {
                    "sent": "So to define Lynn Kernel, we started by using resolution and then saying something about all the possible proofs of our class, right?",
                    "label": 0
                },
                {
                    "sent": "So using this derivation procedure so to generalize this description, logics or to any other language, we have to also speak about the procedure with which we are reasoning in this logic.",
                    "label": 0
                },
                {
                    "sent": "So for this paper we focused on these consequence based reasoning methods and the basic idea about these methods.",
                    "label": 0
                },
                {
                    "sent": "Is there are algorithms that take some consequences that we have derived already and some axioms from the original ontology and put them together to derive new consequences, right?",
                    "label": 0
                },
                {
                    "sent": "So they make explicit some of the implicit consequences of our ontology, right?",
                    "label": 0
                },
                {
                    "sent": "So not all of them, because in description logics we will have infinitely many of them, but some relevant subset of them.",
                    "label": 0
                },
                {
                    "sent": "OK, and in general we just procedures that are have rules of this shape, right?",
                    "label": 0
                },
                {
                    "sent": "So they take some explicit knowledge, so this should be 0 some axioms from the original ontology and then they derive some additional explicit knowledge and then add it to our set of explicit knowledge.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a very abstract notion of consequence based reasoning.",
                    "label": 0
                },
                {
                    "sent": "I will so to actually put it down in description logics.",
                    "label": 0
                },
                {
                    "sent": "I would just speak very fast about ALC and will explain you this notion of consequence based algorithms in this ALC logic, right?",
                    "label": 0
                },
                {
                    "sent": "So ALC very very fast we have concepts that are derived from concept names and then we can have negations and then we can do conjunctions, disjunctions and this special, existential and value restrictions that we can create.",
                    "label": 0
                },
                {
                    "sent": "And in general we can put our axioms, I will, restrictions will be axioms of this form.",
                    "label": 0
                },
                {
                    "sent": "So this GIS.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I just simplifying a bit without loss of generality.",
                    "label": 0
                },
                {
                    "sent": "But just to we only allow these GIS to be of this five different shapes, right?",
                    "label": 0
                },
                {
                    "sent": "So we only have conjunctions of concept names are subsumed to our superclass of another concept name or a concept name is a subclass of a disjunction of concept names or we have an existential restriction either on the left or on the right, or a value restriction on the right?",
                    "label": 0
                },
                {
                    "sent": "And this has been proven before that you can change any.",
                    "label": 0
                },
                {
                    "sent": "ALC Tee box with arbitrary concept inclusions into this normal form in linear time.",
                    "label": 0
                },
                {
                    "sent": "So just assume that they are in there.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and the consequences based algorithm for else just takes this 6 rules right and I don't want to go into the details of how they work, but I just want to make clear that this that there is such an algorithm.",
                    "label": 0
                },
                {
                    "sent": "The important thing is that this so this H case are just conjunctions of literals, so concept names or negative concept names and M&N are disjunctions of literals, right?",
                    "label": 1
                },
                {
                    "sent": "And where I have these pairs, they're just saying, so they are short or.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, they describe that this conjunction of literals here is subsumed by this disjunction of literals, given that all the actions in the original ontology are true, right?",
                    "label": 0
                },
                {
                    "sent": "So this is an entailment and, well, this is a more complex one says that H is subsumed by North, or there exists in our successor.",
                    "label": 0
                },
                {
                    "sent": "There is in K, and this is so, so collection, and this rule is the only thing that they do is that they take some of this assumption that we know that they are.",
                    "label": 0
                },
                {
                    "sent": "True and take some actions potentially and then just derive some additional knowledge.",
                    "label": 0
                },
                {
                    "sent": "So for example, the first rule over there is essentially resolution.",
                    "label": 0
                },
                {
                    "sent": "If you think about it.",
                    "label": 0
                },
                {
                    "sent": "So resolution for propositional logic.",
                    "label": 0
                },
                {
                    "sent": "And then we have some others that deal with the actual ALC knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "So our goal was to define Lynn kernels, right?",
                    "label": 0
                },
                {
                    "sent": "So we take an arbitrary consequence based algorithm so it doesn't have to be the LC-1.",
                    "label": 0
                },
                {
                    "sent": "Any consequence based algorithm that you know that is sound and complete for the kind of consequence that you are interested in.",
                    "label": 0
                },
                {
                    "sent": "And you take an ontology, right?",
                    "label": 0
                },
                {
                    "sent": "So a set of axioms in that language.",
                    "label": 0
                },
                {
                    "sent": "A consequence from this ontology and the link kernel for that consequences for that with respect to that ontology, and that consequences based algorithm is just the set of all axioms that we will use in some derivation.",
                    "label": 1
                },
                {
                    "sent": "Proof for that consequence.",
                    "label": 0
                },
                {
                    "sent": "So we look at all the possible ways to prove this.",
                    "label": 0
                },
                {
                    "sent": "Consequences we look at the actions that appear in bits and this is it.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's the exact same idea behind the link kernels in propositional logic, so just extended description logics, and in particular we have to pay attention of what algorithm reducing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so this is the concept that consequence based algorithm.",
                    "label": 0
                },
                {
                    "sent": "Skype so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to set a small example how this applies in this ALC setting, suppose that we have this six axioms right and we want to so from these axioms we know that a subsumed by C, right?",
                    "label": 0
                },
                {
                    "sent": "Because a subbase subsumed by C?",
                    "label": 0
                },
                {
                    "sent": "So if we want to derive the link kernel for this con cequence right, what can we do?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let's look at the the consequence based algorithm.",
                    "label": 0
                },
                {
                    "sent": "11 proof is the one that I just described.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the different proof for that subscription so we can start with.",
                    "label": 0
                },
                {
                    "sent": "A is subsumed by a cytology.",
                    "label": 0
                },
                {
                    "sent": "So we start with that and we have this axiom right?",
                    "label": 0
                },
                {
                    "sent": "And so we can derive by using the first rule that a subsumed by B.",
                    "label": 0
                },
                {
                    "sent": "So this is the first consequence now also from this technology and the second rule and this axiom we can derive that a is subsumed by the empty set or exist are a right?",
                    "label": 0
                },
                {
                    "sent": "So just by using our notation that we have right now using this information and this extraction we can derive this further information.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "The rule here, so it gets a bit complicated over here, so I don't want to go into the details, but you can play around with it and you will see.",
                    "label": 0
                },
                {
                    "sent": "And then using the fourth rule, since we have A and not B&A and not B is in bottom, so I'm skipping 2 steps in here.",
                    "label": 0
                },
                {
                    "sent": "But then we can derive that a subsumed by be right and so this is so here we have a resolution, not a resolution.",
                    "label": 0
                },
                {
                    "sent": "Consequences based proof for this assumption is subsumed by B and this uses this for axioms and then of course as in B and then be subsumed by.",
                    "label": 0
                },
                {
                    "sent": "See we have AC and this uses this for actions right?",
                    "label": 0
                },
                {
                    "sent": "So those four accidents will be in the link kernel.",
                    "label": 0
                },
                {
                    "sent": "And with respect to these ontology now of course, you already know that this is not minimal, because we can also derive it just from these two, but it's OK.",
                    "label": 0
                },
                {
                    "sent": "So we have some support for success, just.",
                    "label": 0
                },
                {
                    "sent": "Emphasize that this algorithm actually has too much at sometimes, but this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Now the first question is OK. We have defined this nice notion of Lynn kernels.",
                    "label": 0
                },
                {
                    "sent": "So how do we compute them, right?",
                    "label": 0
                },
                {
                    "sent": "So how do we get?",
                    "label": 0
                },
                {
                    "sent": "Because the definition is a bit.",
                    "label": 0
                },
                {
                    "sent": "Too big in the sense that it just says you have to look at all the possible proofs, right?",
                    "label": 0
                },
                {
                    "sent": "So how do we check that?",
                    "label": 0
                },
                {
                    "sent": "We have built all the possible proofs?",
                    "label": 0
                },
                {
                    "sent": "And how do we actually take this set of axioms right?",
                    "label": 1
                },
                {
                    "sent": "And the first thing that we prove is that if we have a consequence based algorithm, then we can just very easily transform it into an algorithm that computes the lean kernel for all the consequences that are derived by this algorithm, and the idea is again very simple.",
                    "label": 0
                },
                {
                    "sent": "Base for every consequence that we derive, we attach it as a label.",
                    "label": 1
                },
                {
                    "sent": "The set of all the actions that were used to derive it right.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that usually consequence based algorithms stop once that they derive the consequence that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "Well, since we want to find all the possible ways of it, we cannot stop.",
                    "label": 0
                },
                {
                    "sent": "We have to continue applying rules until separation, so we will need to apply more rules and.",
                    "label": 0
                },
                {
                    "sent": "If we find the same consequence twice, we don't generate it twice, but we just update the set of axioms that are associated to it.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 1
                },
                {
                    "sent": "And the nice thing of this algorithm is that by running it once, we derive the link kernels of all the consequences that are derived by the original consequence based algorithm.",
                    "label": 0
                },
                {
                    "sent": "So not just for one that we're interested in, but for all of them.",
                    "label": 1
                },
                {
                    "sent": "And the other thing is that of course we have an overhead by doing this, we have to keep in memory all these axioms and we want to we have to apply rules until separation.",
                    "label": 0
                },
                {
                    "sent": "So we apply more.",
                    "label": 0
                },
                {
                    "sent": "But this overhead is just linear on the size of the of the ontology.",
                    "label": 0
                },
                {
                    "sent": "So well just it might be big, but this is not an overkill.",
                    "label": 0
                },
                {
                    "sent": "And well, we can prove that is mean are preserving.",
                    "label": 0
                },
                {
                    "sent": "So we have nice properties for this, OK?",
                    "label": 0
                },
                {
                    "sent": "So just as an example from the previous derivation, the only thing that we will do is so if you just remember the steps that I did before I have the exact same steps, just.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have now the set of axioms that were used for the derivation, so I used the first axiom, then for the second step we use the second action, so we will have it then for this for the third step we used some knowledge here that has action one some knowledge that has axiom two and then we use the third action.",
                    "label": 0
                },
                {
                    "sent": "So in the derivation we will have this knowledge needs these reactions to be derived.",
                    "label": 0
                },
                {
                    "sent": "From this we will get another step that will derive again a subsumed by B and we don't derive it again.",
                    "label": 0
                },
                {
                    "sent": "At that point we just.",
                    "label": 0
                },
                {
                    "sent": "Update the label of the A subsumed by B that we had created before and since we have to apply it until saturation, we will have an extra rule that doesn't have anything to do with the consequences that we were interested in, but doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "OK, so we will just run it and then we get to this kind of information between testing.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so we have we have done this.",
                    "label": 0
                },
                {
                    "sent": "We have defined this link kernels.",
                    "label": 0
                },
                {
                    "sent": "We have some theoretical properties of it.",
                    "label": 0
                },
                {
                    "sent": "Now we want to check whether our algorithm or whether this notion and our implementation that we have actually satisfies the desiderata.",
                    "label": 0
                },
                {
                    "sent": "That I started with, right?",
                    "label": 0
                },
                {
                    "sent": "So we want to see whether this is the notion that we are actually looking for.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, and so to do this.",
                    "label": 0
                },
                {
                    "sent": "What we did is that we tested, so we implemented it not for a subclass of AOC.",
                    "label": 0
                },
                {
                    "sent": "So for a smaller language yield plus and then we looked at all the link kernels of all the atomics assumptions that follow from these five ontologies, right?",
                    "label": 0
                },
                {
                    "sent": "So just a small data, small information.",
                    "label": 0
                },
                {
                    "sent": "So this is the size of the apologies.",
                    "label": 0
                },
                {
                    "sent": "So in particular we have this very big one and this is the number of subscriptions that we checked in the lean kernel.",
                    "label": 0
                },
                {
                    "sent": "So overall our tests may we have to recompute at 6 million more than 6 million Lynn kernels, right?",
                    "label": 0
                },
                {
                    "sent": "To check out these consumers.",
                    "label": 0
                },
                {
                    "sent": "Now we need to compare.",
                    "label": 0
                },
                {
                    "sent": "It's our first desiderata.",
                    "label": 0
                },
                {
                    "sent": "Was it should be?",
                    "label": 0
                },
                {
                    "sent": "Well, the first was that it's me not preserving that we already proved.",
                    "label": 0
                },
                {
                    "sent": "But we said, it should be not too big.",
                    "label": 0
                },
                {
                    "sent": "Should be small, right?",
                    "label": 0
                },
                {
                    "sent": "So we have to compare it against something.",
                    "label": 0
                },
                {
                    "sent": "So we looked at different.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Notion of minor preserving modules.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And which are the locality various modules which in practice, and so in the last years they have been proposed so way to approximate, so to have a good, meaner preserving subset.",
                    "label": 0
                },
                {
                    "sent": "Entology because they're fast to compute, and they give a good approximation.",
                    "label": 1
                },
                {
                    "sent": "So we want to compare against them.",
                    "label": 0
                },
                {
                    "sent": "Are we better than that?",
                    "label": 0
                },
                {
                    "sent": "OK, so the first thing that we did is prove formally that for ALC.",
                    "label": 0
                },
                {
                    "sent": "So this unfortunately we couldn't do it.",
                    "label": 0
                },
                {
                    "sent": "In the general setting which all the previous things that I said were true for arbitrary consequence based algorithms, but formally for else, we can prove that so well, we knew that is meaner preserving, but also that the link kernel is always contained in this star locality based module, which is also with a subset of another localities based model that is the bottom body.",
                    "label": 0
                },
                {
                    "sent": "OK so we know that our approximation cannot be worse than this OK, but so is it.",
                    "label": 0
                },
                {
                    "sent": "Better in general and what happens in practice, right?",
                    "label": 1
                },
                {
                    "sent": "So we look at these five ontologies that we have.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And essentially what we have is this.",
                    "label": 0
                },
                {
                    "sent": "So this plot where it tries to show is how much bigger is the star module compared in proportion to the size of the link, kernel right?",
                    "label": 0
                },
                {
                    "sent": "And So what I want to highlight here are two to specific things, so we have this point over there, right?",
                    "label": 0
                },
                {
                    "sent": "That is so.",
                    "label": 0
                },
                {
                    "sent": "See that this scale is logarithmic over there, so that's something beyond to 14,000 something.",
                    "label": 0
                },
                {
                    "sent": "So and remember that.",
                    "label": 0
                },
                {
                    "sent": "So this is for full gallon.",
                    "label": 0
                },
                {
                    "sent": "Full gallon has 33,000 axioms, so these are 5000 subsumption relations that where the link kernel had size either one or two and the star module have either half of the size of the ontology or the full ontology.",
                    "label": 0
                },
                {
                    "sent": "That's a big difference in the on the other side, which is, but for us in some sense is there is here.",
                    "label": 0
                },
                {
                    "sent": "This is NCI in the bottom line and see I forgot ontology and see I we have exactly the same size for all the time so it returned the link kernel and the start module were always the same.",
                    "label": 0
                },
                {
                    "sent": "So we have these two limits.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is maybe this is not very visible, but here we did some statistical test to see whether our sets were significantly smaller and it's actually much very significantly smaller in all cases except in NCI where the answer is always the same and The thing is also we also got for example in the gene ontology.",
                    "label": 0
                },
                {
                    "sent": "Our sets correspond exactly to the Union of all minus, so it's a very good approximation in those cases.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now the the theater is whether it's fast, right?",
                    "label": 0
                },
                {
                    "sent": "And it depends on what you think what you mean by being faster, right?",
                    "label": 0
                },
                {
                    "sent": "So the problem with our algorithm is that it computes all the link kernels in one run, right?",
                    "label": 0
                },
                {
                    "sent": "So if you see it for as Nomad, it runs in 11,000 seconds, so that's not fast.",
                    "label": 0
                },
                {
                    "sent": "But in 11,000 seconds it computes the lean kernel for 5.3 million consequences, right?",
                    "label": 0
                },
                {
                    "sent": "So if we are interested in finding all of them, this is very fast.",
                    "label": 0
                },
                {
                    "sent": "Right, so this this bottom module, which is the faster of the two in the minimum takes 21 seconds.",
                    "label": 0
                },
                {
                    "sent": "So for this amount of thing it will take 530,000 seconds, right?",
                    "label": 0
                },
                {
                    "sent": "Of course, if you don't want to compute all of them and you're just interested in one consequence, this might be an overkill.",
                    "label": 0
                },
                {
                    "sent": "So it depends what you want to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clude link earners are a good approximation for the Union of ominous.",
                    "label": 0
                },
                {
                    "sent": "This is I wanted to say they can be effective effectively and efficiently in some sense.",
                    "label": 0
                },
                {
                    "sent": "Computed for all consequences with a linear overhead on the over the original consequence based algorithm.",
                    "label": 1
                },
                {
                    "sent": "It is not called directive, but if you want a goal directed approach, so also from some additional tests that I didn't present, you can first compute the bottom module that is very fast, but it's not very good approximation.",
                    "label": 0
                },
                {
                    "sent": "But from that bottom module you can then sorry, compute the link kernel and that's all, any questions?",
                    "label": 0
                }
            ]
        }
    }
}