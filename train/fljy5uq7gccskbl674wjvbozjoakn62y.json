{
    "id": "fljy5uq7gccskbl674wjvbozjoakn62y",
    "title": "On Convergence Rate of Concave-Convex Procedure",
    "info": {
        "author": [
            "Ian E.H. Yen, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_yen_procedure/",
    "segmentation": [
        [
            "When the property is differentiable, actually there have show that the convergence rate of SCPS linear and, but for some nonsmooth problem is more difficult, and it's still an open problem.",
            "So in this work, will."
        ],
        [
            "This kid.",
            "So first we'll talk about some non smooth applications of CCP in SPN literatures, and then we will review the CCP algorithm and show that it's actually a special case of materialization minimization and the magician minimization algorithm is even a special case of blackhorn decent.",
            "So our approach is actually very simple that we just convert CCP taxi Piazza.",
            "This algorithm and try to borrow some literatures from the BCD and to show that they must choose some some little show the convergence rate on the this condition.",
            "So the third part will check on which conditions we can recall this convergence result."
        ],
        [
            "First, the CCP is often often used to sell the DC program that composed of two convex function.",
            "But the difference of them is not convex, so this program cannot be solved by standard convex programming problem methods and.",
            "Toilet"
        ],
        [
            "And will give 2.",
            "Examples one very famous is the structure is being with hidden variables in this problem because the loss function is defined by the score of negative examples minus the goal of positive examples.",
            "But because we don't know the hidden variables, so to define the feature vector we have to find the hidden variable assignment as most with most agreement with the current model.",
            "And because we take this maximization so.",
            "It will become a convex piecewise linear function, and this turn is also, so this becomes a DC programs that original convex."
        ],
        [
            "And the other example is when we want to provide a tighter bound for the same problem.",
            "Because in the in the original problem the loss functions will go to Infinity if we have some outliers or something we cannot explain.",
            "And the true those we want to minimize this this Delta instead of the surrogate functions introduced by SVN.",
            "So in their work they less special case.",
            "Very famous is called.",
            "Ram blows Instagrams, the rainbows that we.",
            "They introduce a second term that offset the additional load introduced by the circuit function so that it will not penalize too much for the outliers.",
            "And because they introduced this term, so the problems becomes not convex.",
            "Actually, it's a difference of convex functions.",
            "And there's still other examples in esperion like transductive or.",
            "At least maximum margin, like clustering."
        ],
        [
            "Ann for this problem.",
            "We finally have something in common that is the the old includes a quadratic terms strictly convex quadratic term and the non smooth function is piecewise linear, so we find if it is the case then the conversion story is actually can be analyzed.",
            "So in his work we will focus on this special case."
        ],
        [
            "OK, we will review the algorithm of CCP in Mississippi.",
            "Is a convex relaxation that in each iterations we try to linearize the concave part such that the this function become you minus the linearized lesion will become a convex function and we will use whatever convex techniques to solve it.",
            "And the original proposals have shown that they have shown this is guaranteed to descend.",
            "Objective function of the program.",
            "But we know that the guarantee descending guaranteed global convergence.",
            "So in a previous talk is actually.",
            "This paper is just in this workshop.",
            "In 2009, they sure that the algorithm have global convergence fire.",
            "Then we'll theory and in the end of the talk they point out.",
            "The local convergence rate of non.",
            "Another problem with constraints is open problems.",
            "So in this work we will try to use a simple reduction that show show the CPS conversion string."
        ],
        [
            "First, we will give an intuition about why CCP will work by connecting it with the majorization Medialization algorithm in the EM algorithm.",
            "We minimize the upper bound of original function by introducing new axle airy variables Y for any value of this.",
            "Why the new function G will be upper bound F?",
            "And if we take the Y to X then they will become equals and so this is a very simple algorithm that we just fixed the Y.",
            "And sell the X and when we get the new X we just Project X into Y and sell it again.",
            "And it will work because every iteration we sell upper bound.",
            "So that would guarantee."
        ],
        [
            "Distance of the earth.",
            "An in the SCPS.",
            "What's the tree?",
            "The tree is composed by First order Taylor approximation of V. You can see this equation that if we because we is convex, so the approximation must be strict baby.",
            "Not strictly Dorland functions, so the G will be larger than the.",
            "Orginal functions for any assignment of Y and because the approximation is exact when Y equals to X, so this condition is also satisfied and."
        ],
        [
            "And.",
            "And this will actually give a very simple.",
            "Is that if we can tag this algorithm as a map from XT2XT plus one?",
            "Let's do a sign.",
            "Then we can actually buy different different differentiate the sine functions and show the different results have spectral radius less than one.",
            "Then we will provide linear convergence rate, but this is hard for problem that cannot be differentiate so.",
            "In this work we use."
        ],
        [
            "Another view.",
            "I guess we just we never let the Y the minimum of Y is occurs at X so.",
            "Actually, the EM algorithm can be seen as a block only decent between Y&X.",
            "And so, so the problem becomes why there are no others people, other people to use these simple effects to show show the convergence rate is because the G. Over is nonconvex and nonsmooth, so they will be very hard to analyze such functions, even for BCD algorithms.",
            "We will see this by because we is a piecewise linear function, right?",
            "So if we if we linearize it then the subgradients will be discontinuous, then the function is too hard to analyze."
        ],
        [
            "But we find Fortunately that for convex piecewise linear functions, we can actually express it in another way that.",
            "It's maximum over several linear function, so the gradient of each each point actually can be denoted as the coefficient A for the maximum linear function.",
            "So in this view we can transform the CCP algorithm.",
            "Interpreted as a lack of BCD over the alternative formulations.",
            "Uh.",
            "This is why this will yield the same algorithm is because.",
            "When we minimize over D and fix X, we'll actually get the D becomes one for the largest linear functions, and if there are several largest linear functions, we can just take a average of land to build the subgradient, and we know that the the eh is actually the subgradient.",
            "So when we fix the minimize respect to X, then it will be the same algorithm, CCP."
        ],
        [
            "OK."
        ],
        [
            "So why this function is easier?",
            "You can say that because the least original, this term is discontinuous function and."
        ],
        [
            "But"
        ],
        [
            "All this alternative formulation is a simple quadratic function between D&X.",
            "So it will be easier to analyze and the constraints introduced is actually polyhedral simple linear functions so.",
            "So for this type of negative formulation we can analyze it easier."
        ],
        [
            "So from this point we introduce the literatures from BCD algorithm.",
            "That under which conditions that we can recall their theorems?",
            "Actually it is a paper of Poe.",
            "Then in 2009 that they introduced there is not exactly the PCP algorithm.",
            "Is they introduced their own algorithm of coordinate gradient descent?",
            "The difference is only that in the coordinate gradient, decent algorithm.",
            "In each iteration it used second order Taylor approximation too.",
            "Strictly convex approximation to replace the original function and do coordinate descent along these approximations.",
            "Because this will be easier to analyze and.",
            "And because we find that in our problem the problem itself, the smooth part, is actually convex, strictly convex quadratic, so there's actually not not much difference between our algorithm and there's.",
            "So if we use their algorithm with the exact Haitian matrix, an example I search then actually we are the same algorithm, so we can recall their results.",
            "That they showed the algorithm have linear convergence rate if if the smooth power.",
            "Of the master problem is quadratic and the non smooth part I should first first introduce these two trends in their work.",
            "They try to decompose the.",
            "Decompose the problems into two parts, one is.",
            "Smooth but maybe not, maybe not convex.",
            "The other part is not smooth, but it should be convex, Ann.",
            "You can decompose into these two part and the second part is the constraint part is separable for your block then.",
            "They can show some results.",
            "On of convergence.",
            "OK.",
            "So.",
            "Or what the remaining things to do is just to check these conditions.",
            "Oh"
        ],
        [
            "So let's.",
            "See.",
            "The alternative formulation we proposed actually can be very easy to transform to their formulation by just.",
            "Decompose it into a smooth part in this part.",
            "In this most power because.",
            "The term we introduced is quadratic term between D&X.",
            "Let's no need to be is nonconvex, but there's no need to be convex in their formulation, so.",
            "The other part is EU functions smooth part of U function.",
            "We denoted FU.",
            "If you recall, the SVM problem is a strictly W square, so this channel will become a strictly convex quadratic functions.",
            "And further further notice most part.",
            "The actually the the constraint of D is separate separable to the constraints of X, so it is separable and we have.",
            "Now that the non smooth power of U and and Sigma is Sigma denotes the constraint space is convex and and and.",
            "So.",
            "So we satisfied all the condition and and.",
            "And we will further to check that when we minimizing over X.",
            "The function is strictly convex because there is this term is actually exactly what we minimize when we sell SVN.",
            "And when we minimize over D, although this formulation is a linear program, not a straight convex quadratic program.",
            "Because the program is very simple, so in our papers we provide another lemma show for the linear program.",
            "There is always.",
            "We can always add a small quadratic terms such that the.",
            "The solution would be the thing to the origin one, so now we face satisfies older older conditions then we request there."
        ],
        [
            "Readout of convergence rate 2."
        ],
        [
            "So that CPS actually has a linear conversion straight for the SVM problem, so that's all thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When the property is differentiable, actually there have show that the convergence rate of SCPS linear and, but for some nonsmooth problem is more difficult, and it's still an open problem.",
                    "label": 0
                },
                {
                    "sent": "So in this work, will.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kid.",
                    "label": 0
                },
                {
                    "sent": "So first we'll talk about some non smooth applications of CCP in SPN literatures, and then we will review the CCP algorithm and show that it's actually a special case of materialization minimization and the magician minimization algorithm is even a special case of blackhorn decent.",
                    "label": 0
                },
                {
                    "sent": "So our approach is actually very simple that we just convert CCP taxi Piazza.",
                    "label": 0
                },
                {
                    "sent": "This algorithm and try to borrow some literatures from the BCD and to show that they must choose some some little show the convergence rate on the this condition.",
                    "label": 0
                },
                {
                    "sent": "So the third part will check on which conditions we can recall this convergence result.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, the CCP is often often used to sell the DC program that composed of two convex function.",
                    "label": 1
                },
                {
                    "sent": "But the difference of them is not convex, so this program cannot be solved by standard convex programming problem methods and.",
                    "label": 0
                },
                {
                    "sent": "Toilet",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And will give 2.",
                    "label": 0
                },
                {
                    "sent": "Examples one very famous is the structure is being with hidden variables in this problem because the loss function is defined by the score of negative examples minus the goal of positive examples.",
                    "label": 1
                },
                {
                    "sent": "But because we don't know the hidden variables, so to define the feature vector we have to find the hidden variable assignment as most with most agreement with the current model.",
                    "label": 0
                },
                {
                    "sent": "And because we take this maximization so.",
                    "label": 0
                },
                {
                    "sent": "It will become a convex piecewise linear function, and this turn is also, so this becomes a DC programs that original convex.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the other example is when we want to provide a tighter bound for the same problem.",
                    "label": 1
                },
                {
                    "sent": "Because in the in the original problem the loss functions will go to Infinity if we have some outliers or something we cannot explain.",
                    "label": 0
                },
                {
                    "sent": "And the true those we want to minimize this this Delta instead of the surrogate functions introduced by SVN.",
                    "label": 0
                },
                {
                    "sent": "So in their work they less special case.",
                    "label": 0
                },
                {
                    "sent": "Very famous is called.",
                    "label": 0
                },
                {
                    "sent": "Ram blows Instagrams, the rainbows that we.",
                    "label": 0
                },
                {
                    "sent": "They introduce a second term that offset the additional load introduced by the circuit function so that it will not penalize too much for the outliers.",
                    "label": 0
                },
                {
                    "sent": "And because they introduced this term, so the problems becomes not convex.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's a difference of convex functions.",
                    "label": 1
                },
                {
                    "sent": "And there's still other examples in esperion like transductive or.",
                    "label": 0
                },
                {
                    "sent": "At least maximum margin, like clustering.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann for this problem.",
                    "label": 0
                },
                {
                    "sent": "We finally have something in common that is the the old includes a quadratic terms strictly convex quadratic term and the non smooth function is piecewise linear, so we find if it is the case then the conversion story is actually can be analyzed.",
                    "label": 0
                },
                {
                    "sent": "So in his work we will focus on this special case.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we will review the algorithm of CCP in Mississippi.",
                    "label": 0
                },
                {
                    "sent": "Is a convex relaxation that in each iterations we try to linearize the concave part such that the this function become you minus the linearized lesion will become a convex function and we will use whatever convex techniques to solve it.",
                    "label": 0
                },
                {
                    "sent": "And the original proposals have shown that they have shown this is guaranteed to descend.",
                    "label": 0
                },
                {
                    "sent": "Objective function of the program.",
                    "label": 0
                },
                {
                    "sent": "But we know that the guarantee descending guaranteed global convergence.",
                    "label": 0
                },
                {
                    "sent": "So in a previous talk is actually.",
                    "label": 0
                },
                {
                    "sent": "This paper is just in this workshop.",
                    "label": 0
                },
                {
                    "sent": "In 2009, they sure that the algorithm have global convergence fire.",
                    "label": 0
                },
                {
                    "sent": "Then we'll theory and in the end of the talk they point out.",
                    "label": 0
                },
                {
                    "sent": "The local convergence rate of non.",
                    "label": 1
                },
                {
                    "sent": "Another problem with constraints is open problems.",
                    "label": 0
                },
                {
                    "sent": "So in this work we will try to use a simple reduction that show show the CPS conversion string.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, we will give an intuition about why CCP will work by connecting it with the majorization Medialization algorithm in the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "We minimize the upper bound of original function by introducing new axle airy variables Y for any value of this.",
                    "label": 0
                },
                {
                    "sent": "Why the new function G will be upper bound F?",
                    "label": 0
                },
                {
                    "sent": "And if we take the Y to X then they will become equals and so this is a very simple algorithm that we just fixed the Y.",
                    "label": 0
                },
                {
                    "sent": "And sell the X and when we get the new X we just Project X into Y and sell it again.",
                    "label": 0
                },
                {
                    "sent": "And it will work because every iteration we sell upper bound.",
                    "label": 0
                },
                {
                    "sent": "So that would guarantee.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distance of the earth.",
                    "label": 0
                },
                {
                    "sent": "An in the SCPS.",
                    "label": 0
                },
                {
                    "sent": "What's the tree?",
                    "label": 0
                },
                {
                    "sent": "The tree is composed by First order Taylor approximation of V. You can see this equation that if we because we is convex, so the approximation must be strict baby.",
                    "label": 0
                },
                {
                    "sent": "Not strictly Dorland functions, so the G will be larger than the.",
                    "label": 0
                },
                {
                    "sent": "Orginal functions for any assignment of Y and because the approximation is exact when Y equals to X, so this condition is also satisfied and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And this will actually give a very simple.",
                    "label": 0
                },
                {
                    "sent": "Is that if we can tag this algorithm as a map from XT2XT plus one?",
                    "label": 0
                },
                {
                    "sent": "Let's do a sign.",
                    "label": 0
                },
                {
                    "sent": "Then we can actually buy different different differentiate the sine functions and show the different results have spectral radius less than one.",
                    "label": 0
                },
                {
                    "sent": "Then we will provide linear convergence rate, but this is hard for problem that cannot be differentiate so.",
                    "label": 0
                },
                {
                    "sent": "In this work we use.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another view.",
                    "label": 0
                },
                {
                    "sent": "I guess we just we never let the Y the minimum of Y is occurs at X so.",
                    "label": 1
                },
                {
                    "sent": "Actually, the EM algorithm can be seen as a block only decent between Y&X.",
                    "label": 0
                },
                {
                    "sent": "And so, so the problem becomes why there are no others people, other people to use these simple effects to show show the convergence rate is because the G. Over is nonconvex and nonsmooth, so they will be very hard to analyze such functions, even for BCD algorithms.",
                    "label": 0
                },
                {
                    "sent": "We will see this by because we is a piecewise linear function, right?",
                    "label": 1
                },
                {
                    "sent": "So if we if we linearize it then the subgradients will be discontinuous, then the function is too hard to analyze.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we find Fortunately that for convex piecewise linear functions, we can actually express it in another way that.",
                    "label": 0
                },
                {
                    "sent": "It's maximum over several linear function, so the gradient of each each point actually can be denoted as the coefficient A for the maximum linear function.",
                    "label": 0
                },
                {
                    "sent": "So in this view we can transform the CCP algorithm.",
                    "label": 0
                },
                {
                    "sent": "Interpreted as a lack of BCD over the alternative formulations.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "This is why this will yield the same algorithm is because.",
                    "label": 0
                },
                {
                    "sent": "When we minimize over D and fix X, we'll actually get the D becomes one for the largest linear functions, and if there are several largest linear functions, we can just take a average of land to build the subgradient, and we know that the the eh is actually the subgradient.",
                    "label": 0
                },
                {
                    "sent": "So when we fix the minimize respect to X, then it will be the same algorithm, CCP.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why this function is easier?",
                    "label": 0
                },
                {
                    "sent": "You can say that because the least original, this term is discontinuous function and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All this alternative formulation is a simple quadratic function between D&X.",
                    "label": 0
                },
                {
                    "sent": "So it will be easier to analyze and the constraints introduced is actually polyhedral simple linear functions so.",
                    "label": 0
                },
                {
                    "sent": "So for this type of negative formulation we can analyze it easier.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So from this point we introduce the literatures from BCD algorithm.",
                    "label": 0
                },
                {
                    "sent": "That under which conditions that we can recall their theorems?",
                    "label": 0
                },
                {
                    "sent": "Actually it is a paper of Poe.",
                    "label": 0
                },
                {
                    "sent": "Then in 2009 that they introduced there is not exactly the PCP algorithm.",
                    "label": 0
                },
                {
                    "sent": "Is they introduced their own algorithm of coordinate gradient descent?",
                    "label": 1
                },
                {
                    "sent": "The difference is only that in the coordinate gradient, decent algorithm.",
                    "label": 0
                },
                {
                    "sent": "In each iteration it used second order Taylor approximation too.",
                    "label": 1
                },
                {
                    "sent": "Strictly convex approximation to replace the original function and do coordinate descent along these approximations.",
                    "label": 0
                },
                {
                    "sent": "Because this will be easier to analyze and.",
                    "label": 0
                },
                {
                    "sent": "And because we find that in our problem the problem itself, the smooth part, is actually convex, strictly convex quadratic, so there's actually not not much difference between our algorithm and there's.",
                    "label": 1
                },
                {
                    "sent": "So if we use their algorithm with the exact Haitian matrix, an example I search then actually we are the same algorithm, so we can recall their results.",
                    "label": 0
                },
                {
                    "sent": "That they showed the algorithm have linear convergence rate if if the smooth power.",
                    "label": 0
                },
                {
                    "sent": "Of the master problem is quadratic and the non smooth part I should first first introduce these two trends in their work.",
                    "label": 0
                },
                {
                    "sent": "They try to decompose the.",
                    "label": 0
                },
                {
                    "sent": "Decompose the problems into two parts, one is.",
                    "label": 0
                },
                {
                    "sent": "Smooth but maybe not, maybe not convex.",
                    "label": 0
                },
                {
                    "sent": "The other part is not smooth, but it should be convex, Ann.",
                    "label": 0
                },
                {
                    "sent": "You can decompose into these two part and the second part is the constraint part is separable for your block then.",
                    "label": 0
                },
                {
                    "sent": "They can show some results.",
                    "label": 0
                },
                {
                    "sent": "On of convergence.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Or what the remaining things to do is just to check these conditions.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "The alternative formulation we proposed actually can be very easy to transform to their formulation by just.",
                    "label": 1
                },
                {
                    "sent": "Decompose it into a smooth part in this part.",
                    "label": 1
                },
                {
                    "sent": "In this most power because.",
                    "label": 1
                },
                {
                    "sent": "The term we introduced is quadratic term between D&X.",
                    "label": 0
                },
                {
                    "sent": "Let's no need to be is nonconvex, but there's no need to be convex in their formulation, so.",
                    "label": 0
                },
                {
                    "sent": "The other part is EU functions smooth part of U function.",
                    "label": 1
                },
                {
                    "sent": "We denoted FU.",
                    "label": 1
                },
                {
                    "sent": "If you recall, the SVM problem is a strictly W square, so this channel will become a strictly convex quadratic functions.",
                    "label": 0
                },
                {
                    "sent": "And further further notice most part.",
                    "label": 0
                },
                {
                    "sent": "The actually the the constraint of D is separate separable to the constraints of X, so it is separable and we have.",
                    "label": 0
                },
                {
                    "sent": "Now that the non smooth power of U and and Sigma is Sigma denotes the constraint space is convex and and and.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we satisfied all the condition and and.",
                    "label": 1
                },
                {
                    "sent": "And we will further to check that when we minimizing over X.",
                    "label": 0
                },
                {
                    "sent": "The function is strictly convex because there is this term is actually exactly what we minimize when we sell SVN.",
                    "label": 1
                },
                {
                    "sent": "And when we minimize over D, although this formulation is a linear program, not a straight convex quadratic program.",
                    "label": 0
                },
                {
                    "sent": "Because the program is very simple, so in our papers we provide another lemma show for the linear program.",
                    "label": 0
                },
                {
                    "sent": "There is always.",
                    "label": 0
                },
                {
                    "sent": "We can always add a small quadratic terms such that the.",
                    "label": 0
                },
                {
                    "sent": "The solution would be the thing to the origin one, so now we face satisfies older older conditions then we request there.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Readout of convergence rate 2.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that CPS actually has a linear conversion straight for the SVM problem, so that's all thank you.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}