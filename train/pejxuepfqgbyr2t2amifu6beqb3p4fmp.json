{
    "id": "pejxuepfqgbyr2t2amifu6beqb3p4fmp",
    "title": "Dual decomposition for inference in natural language processing",
    "info": {
        "author": [
            "Terry Koo, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output",
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_koo_ddi/",
    "segmentation": [
        [
            "This is this talk is going to be unduly composition, an LP relaxations for inference in natural language processing, and I'm of course not Michael Collins.",
            "But these are his slides.",
            "And this is going to be describing joint work with Michael Tommy Okela, Sasha Rush, and David Sontag."
        ],
        [
            "In natural language processing, there's a wide variety of structured problems.",
            "For example, speech recognition, parsing, and machine translation.",
            "And in each of these problems, there is usually a natural structured optimization problem that turns up.",
            "So for example, in speech recognition we commonly need to do some kind of sequence labeling using HMM or CRF.",
            "In parsing there are tree structured inference problems.",
            "Anna machine translation.",
            "There are alignment an phrase based in lemons and so forth."
        ],
        [
            "Common theme in all of these structured problems is that we need to usually find some kind of best possible structure for an input.",
            "So in this slide, I'm formalizing that by just saying that we have some scoring function F, and it takes a structure wise it's input, returning a score an.",
            "Usually what we want to do is find the best scoring structure according to F, and this is the decoding problem."
        ],
        [
            "Previous work in NLP has had a lot of success using standard techniques like dynamic programming or graph algorithms to solve these kinds of decoding problems and more recently LP or ILP based inference.",
            "But in this talk, I'm going to be describing work that we did in applying dual decomposition, which is a classical technique from combinatorial optimization to various inference problems in NLP.",
            "And so in broad strokes, dual decomposition is basically where you take the original optimization problem.",
            "You decompose it into easier subproblems, and you solve a dual optimization, which in turn will give you the solution to the original optimization."
        ],
        [
            "I'm not going to go through a few examples of the kinds of NLP problems you can solve using the decomposition.",
            "So to start off with, we have non projective dependency parsing and this is actually going to form the focus of most of the rest of the talk.",
            "So I'll defer a full explanation to later, but in general it's form of parsing, so you're trying to find a syntactic analysis for a sentence.",
            "And it suffices to say that this is important for many different languages.",
            "And in addition, if you want to find the best possible tree for a sentence in a non projective grammar.",
            "It's NP hard if you use any scoring method beyond kind of very simple naive types of scores."
        ],
        [
            "More direct application of decomposition can be found if you're doing a joint parsing and tagging.",
            "So hopefully most of you are familiar with keys, but in case you're not so I'll go through what's going on here.",
            "We have a sentence like redfly some large jet an.",
            "This is of course a sentence that's chosen to be very ambiguous regarding part of speech tags.",
            "Annina PCF what you do is you start at the words assigned a part of speech tag to each word and then you just group the words together into successively larger phrases and you end up with this tree structure that spans the whole sentence.",
            "And this corresponds to basically a syntactic analysis of the kinds of relationships that are going on in the sentence.",
            "Now, on the other hand, in hmm, you're just trying to find part of speech tags, and you make some simplifying assumptions.",
            "So in particular, you just want to find 1 tag for each word, and you assume that the Inter relationships between the tags are a local in sequential.",
            "Now The thing is, you might want to find a structure such that it optimizes both the score under the CFG and the score into the HMM, 'cause the CFG has some information from syntax and the HMM has some information from the sequential context.",
            "So this is represented on the slide by having one scoring function F of Y for the CFG and another scoring function G of Z for the HMM.",
            "Now, as it turns out, this is a well studied problem an it is possible to find to create a dynamic program to solve the joint tagging and parsing problem.",
            "Basically what you do is take the dynamic program of the CFG and you can intersect it with a dynamic program of the HMM.",
            "But the problem is that.",
            "The resulting dynamic program is pretty complex, has a very high dependency, it's polynomial, but with very high exponents.",
            "And beyond the computational complexity, there's a more practical matter, which is that it's very difficult to implement correctly.",
            "Now, in a dual decomposition approach.",
            "Pardon.",
            "And it's all the composition approach.",
            "What we can do is just solve each of these problems individually by using just standard decoding methods for parsing and tagging.",
            "Thank you.",
            "And.",
            "Instead of trying to put together one big dynamic program, you simply shift the weights around you.",
            "Just twiddle the weights a little bit.",
            "The PCF scores and the tagging scores until the two decoding problems agree.",
            "And so this is the basic overall approach of building composition."
        ],
        [
            "And just as a final example, this is not really related to NLP, but it might be relateable for others outside NLP.",
            "Here's another instance of dual decomposition in the case of Markov random fields.",
            "So suppose we have some binary variables with pairwise interactions.",
            "And we want to find the best scoring assignment to these variables under the pairwise interactions.",
            "Unfortunately, this is this graph is grid structured so it has a fairly high treewidth.",
            "And so you can't find the optimal assignment efficiently for large grids."
        ],
        [
            "In a dual decomposition type approach, we can take this grid and just decompose it into two smaller or two 2 simpler MFS.",
            "So in this case this is just a red tree and a blue tree.",
            "And instead of solving this original."
        ],
        [
            "Maximization.",
            "We install instead solve this decomposed maximization so we now have two summations over pairwise interactions, one for each of the trees.",
            "But the twist here is that we're acquiring that the variables of the red and blue MRF's the equivalent to each other.",
            "That's the constraint down here.",
            "So thanks to this constraint, these two optimizations are equivalent.",
            "And so actually this hasn't really gotten us anywhere yet."
        ],
        [
            "But the next step we can do is to simply relax the equality constraints.",
            "By introducing LaGrange multipliers and so this is kind of the technical background of what's going on behind the composition.",
            "And don't worry, we'll be going over this in more detail later."
        ],
        [
            "OK, so I'm going to start out by describing how we used dual decomposition to solve.",
            "On projective dependency parsing with rich scores.",
            "Then I'll go through some experimental results and time permitting, I'll try to go through some other examples of dual decomposition in NLP."
        ],
        [
            "OK, so I'll start off by describing what non projective dependency parsing is.",
            "Depending part, dependency parsing itself is a syntactic formalism where also syntactic information is represented using head modifier dependencies, and these are just interactions between pairs of words where one of the words is the head.",
            "The more important word, and the other word is the modifier, the less important.",
            "So for example, in this slide we have a sentence Johnson movie today that he liked and we also have its dependency parse down here.",
            "Each of the dependencies are represented using a directed arc.",
            "And for example, you have a dependence between John and saw where saw the head pointing to John the modifier and this represents the fact that John is a subjective saw.",
            "We also you can also note that we introduced this star this root symbol and this is more of a notational convenience than anything else, But basically a dependency parse is defined as a rooted directed tree that starts at the star and spans all the words of the sentence.",
            "OK, so given that, that's what dependency parsing is.",
            "Non productive parsing is dependency parsing where you also allow dependencies to cross so you can see in this sentence is an interaction between today and saw and also that he liked and movie and these two dependencies are crossing each other.",
            "Now, in the case of projective parsing, where you forbid crossings.",
            "It's possible to design just very straightforward dynamic programming algorithms to solve the parsing problem.",
            "But if you allow dependencies to cross in non projective parsing then it becomes difficult to parse with rich scores."
        ],
        [
            "OK, so in the remainder of this section I'm going to be focusing on two different scoring models for dependency parsing.",
            "In the first model, which we call the arc factored scoring model, we restrict we restrict the scores to be very simple, semi impoverished.",
            "But the advantage is that we can do efficient decoding under those scores.",
            "In the second approach, which I'm going to call the sibling scoring model, we have stronger, richer scores.",
            "But of course, this breaks the decoding, so we can't do efficient decoding."
        ],
        [
            "And the overall the overall goal will be to take the efficient inference from one an try to apply it to the rich scores of the US."
        ],
        [
            "OK, so I'll start by going through the scoring models first.",
            "We have the arc factored scoring."
        ],
        [
            "In this case we just Deacon."
        ],
        [
            "Close the score of a dependency tree into the."
        ],
        [
            "The scores of edges, just like it sounds.",
            "So here we have, for example, the score of the dependency between saw and today."
        ],
        [
            "Anne."
        ],
        [
            "All that."
        ],
        [
            "Dependencies and so forth."
        ],
        [
            "You know here that were just being agnostic to the source of the scores.",
            "So for example, the dependency scores could be coming from a generative model or a discriminative model, linear model, etc.",
            "But we don't care for the purposes of this talk.",
            "And as Brian McDonald pointed out in previous work, as long as your scores are restricted to the edges, then non projective parsing is the same thing as finding minimum spanning trees.",
            "So it's clear that you can just do an efficient decoding in this situation."
        ],
        [
            "Now let's take a look."
        ],
        [
            "What a sibling model is."
        ],
        [
            "So in this case I guess that's not so interesting.",
            "So in this case, instead of having scores to find an individual arcs, you have scores defined on pairs of arcs.",
            "So for example, there's you can assign a score to the triple of words saw movie in today, or solve the head and moving in today are adjacent modifyers.",
            "And it's been shown in projective parsing that if you introduce these kinds of of 2nd order scores, you can get increases in parsing performance."
        ],
        [
            "And of course, these scores can come from general models."
        ],
        [
            "Morales, what not, but the problem is that if you want to find the best tree subject to this kind of scoring, it's actually NP hard."
        ],
        [
            "OK, so given that it's hard to do tree decoding, let's take a look at some alternative methods for decoding.",
            "For example, let's see what happens if we just take one word."
        ],
        [
            "For example, saw and try to find all the modifiers that it likes to take.",
            "So what's the best set of modifiers for this single?"
        ],
        [
            "In the in the true or the correct parse, that set of modifiers would be.",
            "John is a subject movie as the object and today is just a temporal modifier."
        ],
        [
            "But there is lots of other possibilities, so you could have saw taking that role."
        ],
        [
            "Applause that he liked.",
            "Or you could have some just completely silly examples, so we're A and he or the children."
        ],
        [
            "And in general, there's an exponential number of possibilities.",
            "But Fortunately I'm not going to go into the details here, but you can find the best set of modifiers in a sibling scoring model by just using a minor variation of the Turbo decoding.",
            "Sequence labeling.",
            "So this can actually be done efficiently for one."
        ],
        [
            "So that's all.",
            "That's all very well and good for finding modifiers for one word.",
            "But what if we want to find a parse?",
            "Well, if we want to be happy about it, we can just say let's just do that for all the words.",
            "So for example, you find the best set of modifiers for the root and you."
        ],
        [
            "That dependency.",
            "Check out John and it doesn't want to have any model."
        ],
        [
            "Arts.",
            "And just continue through the sentence for."
        ],
        [
            "Sword."
        ],
        [
            "Finding the best."
        ],
        [
            "Modifyers"
        ],
        [
            "So far."
        ],
        [
            "So good, but then we get to the end, an oops.",
            "Now you see here that light actually wants to take John as its subject, which is of course improper.",
            "Now this thing is in a tree anymore.",
            "You can see that John has two heads and he has none, which is bad.",
            "So if we're lucky when we do this kind of individual decoding, we might end up with a tree.",
            "But more than more likely than not, we're going to end up violating some of the tree constraints, creating cycles, or having these words with multiple heads and so forth."
        ],
        [
            "OK, so let's take a step back.",
            "We've seen that forearc factored scoring.",
            "We can do efficient, efficient decoding using MST.",
            "For the sibling scores, we can do efficient decoding as long as we throw away tree constraints, but we want to see what happens if we have both siblings, scores, entry constraints, and in this case, it turns out that we can combine add tree constraints to the overall Deco."
        ],
        [
            "By using decomposition."
        ],
        [
            "OK, so now I'm going to go through basically how we decompose the non projective parsing problem.",
            "So first, let's start with our overall goal, which is to find the Max the best tree according to F, which is for the purposes of this slide, the sibling scoring function.",
            "An why here is the set of all valid tree."
        ],
        [
            "We're going to decompose it or rewrite this objective in this kind of silly way.",
            "I'll go through what each piece."
        ],
        [
            "Is doing.",
            "So here the set Z is the set of all possible structures.",
            "So if you throw away tree constraints and just consider what sets of edges can you take from the sentence, you end up with Z.",
            "So these things are just structures without tree."
        ],
        [
            "Strains on the other hand, why is going to be drawn from the set of valid trees?",
            "So these are just norm?"
        ],
        [
            "Dependency parses.",
            "F of Z Here is going to do sibling scoring on these unconstrained structures."
        ],
        [
            "Angie of why is going to do arc factored scoring on valid trees?",
            "And of course."
        ],
        [
            "The kicker down here is that you still have this Z = y constraint and so that means that this problem is equivalent to the upper problem.",
            "So so far we haven't done anything, but as we've seen before, what we do and do, the composition is just relax the equality constraint."
        ],
        [
            "So if we relax that constraint, I'm not going to go into the details at this moment, but we end up with an iterative algorithm for finding the optimal parse under a sibling scoring model.",
            "The first step is to the code.",
            "To find the best scoring structure under the individual model, the sibling model and also the best scoring tree using an MST model.",
            "So we do decoding using two different models and we end up with two different structures Z&Y."
        ],
        [
            "Then we check if Z is equal to Y.",
            "In the case that it is equal, we stop right there and we return that equals equivalent structure as our parse."
        ],
        [
            "On the other hand, if the two structures are not equivalent, then we're going to make some tiny changes.",
            "Small changes to the scores for the individual decoding, and the MST decoding based on the points of disagreement between the two structures CNY.",
            "And that's actually it's as simple as that.",
            "You just repeatedly do this to code there around the decoding and rounds of correction."
        ],
        [
            "OK, so I'm going to go through an example to make this more concrete.",
            "So first of all.",
            "Will state that we have penalties.",
            "These use in the upper right hand corner, so for every dependency between word I and word J we introduce a penalty.",
            "You have IJ.",
            "And these are going to be initially 0.",
            "And now on the left side we have two different decoding problems.",
            "One of them is the individual decoding and the MST.",
            "And so the objective for individual decoding is the original scoring F of Z, augmented with some extra penalties on each of the dependencies.",
            "Same thing for the MST.",
            "We have the original MST scoring G of Y augmented with penalties from the use.",
            "And of course, at the start these usaral 0 so actually it's just decoding."
        ],
        [
            "Normally."
        ],
        [
            "And so we can go ahead and to code each of the sides.",
            "And check for disagreements.",
            "So clearly these two."
        ],
        [
            "Actors are not the same.",
            "There are four dependencies that are different.",
            "And."
        ],
        [
            "For each of these.",
            "We make a change to the set of penalties, so specifically for the there are two dependencies up here that don't appear in the MST, and so for those we give a negative one wait.",
            "There are two dependencies in the MST that don't appear up top, and for those who give a positive one, wait.",
            "So now we have new penalty weights.",
            "Clearly the two structures."
        ],
        [
            "I agree, and so we did code again."
        ],
        [
            "Individual decoding MST decoding.",
            "We end up with a new structure up top, but it's still now."
        ],
        [
            "The same.",
            "So now we do another round of updating the weights so we get another minus 1 + 1 in there."
        ],
        [
            "And finally decoded."
        ],
        [
            "Another round an.",
            "In this example, we now have an agreeing."
        ],
        [
            "Set of pair of structures and so we can say we can just stop right here and output that structure.",
            "So I think this is a fairly intuitive algorithm you're just doing decoding on two sides."
        ],
        [
            "You're pushing this whole solution on.",
            "I guess 98% or greater of the attached examples."
        ],
        [
            "And even in the case that we don't find an agreeing pair of structures, we can still come up with an approximate solution at any point.",
            "I guess I won't go through the details here, but you can ask me about it."
        ],
        [
            "OK, so given that we have this framework where we have an MST versus a bunch of individual decodings, it's pretty easy to extend the scoring model of the individual decodings.",
            "So for example, instead of just using sibling factors that have 3 words, we can use these kind of grandparent and sibling factors that have 4 words.",
            "This is something that we did in the paper and we found that it improved performance.",
            "And this is extremely easy to do 'cause it's always just a single word at a time."
        ],
        [
            "OK, so now let's take a look at some of the theoretical background between behind why this tool decomposition algorithm works the way it does.",
            "So up on the left we have the original objective and on the right we have our rewritten objective that has both Z&Y.",
            "And it also has these equality constraints that force the two degree with each other.",
            "So instead of optimizing that problem, we're going to relax the equality constraints by introducing LaGrange multipliers.",
            "And we end up with the ground ring that has the two original objectives as well as this penalty term that has, for each potential disagreement, LaGrange multiplier uij.",
            "And these are exactly the usage that we were updating in that example."
        ],
        [
            "So we now can formulate a dual problem, which is exactly this pair of maximization's which we saw in the earlier example.",
            "So up top we have the sibling problem that's been augmented with penalties an MST that's been augmented with penalties.",
            "And you'll note that each of these two problems is now completely decomposed from the other, except for EU penalty weights.",
            "Of course, the dual forms an upper bound on the optimal value of the primal, so a logical thing to do is to find the least upper bound by."
        ],
        [
            "Minimizing the dual.",
            "Note that the dual as shown up here is just a pair of maximization, so this is convex, although because the objectives are linear, they're not going to be differentiable.",
            "It's going to be faceted.",
            "But an easy way to minimize this tool is to just do subgradient descent on the penalty weights you.",
            "And so I think most of us probably notice upgrades, but just in case you don't, it's basically equivalent to the grading, except when there's a corner.",
            "In that case, it's just anything that's under the corner.",
            "Since the objective the dual is a pair of maximization's, this upgrading is just a pair of Arg maximization's and so you can see at the bottom the subgradient is just the difference between the vectors or the solutions that structures for zny.",
            "So you just do so when you do the decoding on both sides and take the difference and add it to the penalties.",
            "What you're doing there is a subgradient update."
        ],
        [
            "OK."
        ],
        [
            "So I guess I'm going to switch into talking about the parsing experiments we did.",
            "We evaluated many things.",
            "Exactness, which is the amount of percentage of the time that we find an optimal solution.",
            "We also looked at parsing speed and accuracy.",
            "And we also compared our dual decomposition parser against the individual decoding model and also the MST model.",
            "And finally, we did some comparison experiments against decoding using linear program or integer linear programming.",
            "In order to train the parameters or models, use the average perceptron.",
            "Anne.",
            "And the experiments were run on a selection of languages from Connell datasets as well as English and checked remix."
        ],
        [
            "OK, so first of all will look at exactness.",
            "So at the bottom is just the set of languages we look like we looked at and you can see that for most of them were hitting 98 or above.",
            "So that means that on the test set we find the optimal solution.",
            "We find a green pair in the test set.",
            "And for the rest of them, they're above 95, so we're doing pretty well at finding exact solutions."
        ],
        [
            "Here is a comparison in terms of accuracy.",
            "The leftmost column is just the plain old MST parser using simple scores.",
            "Middle is the previous best reported result for languages for which we have a comparison.",
            "And the right is our grandparent based individual.",
            "Decoding plus MST are dual decomposition parser in this situation.",
            "So you can see that obviously the decomposition parser is, well, outperforming this arc factored parser, and we're also getting gains over previous work as well."
        ],
        [
            "Here is a comparison of performance between the decomposition parser and also the two subcomponents of that dual decomposition.",
            "So the red bar here is the individual decoding using sibling scores.",
            "The Green Bar is the MST decoding using arc factor scores and the blue bar is the two put together.",
            "And so you can see that little decomposition is doing significantly better than either of the other two alone."
        ],
        [
            "In the next few slides, I'm going to go through some comparisons between dual decomposition, inference and inference based on LP or ILP.",
            "So what we did in our experiments in this section was to reimplement previous work by Andrew Martin.",
            "And specifically we constructed.",
            "Inference algorithms based on two different types of LP relaxations which are this LP, One and LP two as well as plain integer linear programming.",
            "And we're going to make comparisons between our dual decomposition parser and these previous work based on LP LP.",
            "In terms of accuracy, exactness and speed.",
            "And one thing to note here is that because we wanted everything, it's a straight head to head comparison between the inference algorithms, since all the features and scores and so forth are identical."
        ],
        [
            "OK so here is first comparison in terms of accuracy.",
            "Not much difference here, so it seems that all the relaxed LP's are decomposition and the LP are doing roughly the same in terms of development accuracy."
        ],
        [
            "But on this slide we have comparison in terms of exactness and speed.",
            "And so this is a little more interesting, so starting from the left, the red bar is the LP one relaxation.",
            "And this is kind of the more relaxed, most relaxed relaxation.",
            "The Green Bar is LP 2, which is tighter.",
            "I mean I can go into the specifics of what why they're different but.",
            "For the moment, it's only important to know that one is more relaxed in the other, so you can see that the tighter relaxation is more exact.",
            "I LP is of course 1% exact and dual decomposition is doing pretty well.",
            "I think around 98.",
            "On the right we have this comparison in terms of speed, and so again you can see kind of reasonable results.",
            "The more relaxed relaxation LP one is, the fast faster of the two.",
            "LP is pretty slow, but dual decomposition is pretty fast and the reason for that is mainly that instead of using kind of general purpose LP solving were using special purpose parsers that are kind of.",
            "Built based on problem knowledge rather than just general.",
            "LP solvers.",
            "OK, how are you doing and.",
            "Ultimate OK."
        ],
        [
            "I guess I might as well."
        ],
        [
            "Go ahead and talk a little bit about the integrated parsing tagging.",
            "So again, in the case of an HMM.",
            "You have you're trying to assign a sequence of tags while using local sequential context.",
            "And in the case of a CFG, you're assigning this nested tree structure to a sentence which also includes."
        ],
        [
            "Part of speech tags.",
            "Ann, we've I've mentioned before that you can combine the two in the large dynamic program."
        ],
        [
            "But in this section I'll go over how we can do this by using dual decomposition.",
            "So in the case of HMM tagging, let's let Z be the set of all valid taggings and G of CBS scoring."
        ],
        [
            "It's well known that you can solve this."
        ],
        [
            "Sing for tardy.",
            "In the case of parsing, will let Y be the set of valid parse trees F of yvs scoring function, for example."
        ],
        [
            "Probabilities.",
            "And again, you can solve this using the well known CKY algorithm."
        ],
        [
            "Now suppose that we want to find the parse tree that optimizes both sequential tag scores and top down tree scores.",
            "You can do this using the intersected dynamic program, but it involves a order to six increase and runtime.",
            "I think that's for a trigram tagger, so, but in any case it's quite a bit slower and also more complex to implement."
        ],
        [
            "Now we can reformulate the integrated parsing Italian problem problem by using a similar kind of pair of scoring functions.",
            "You have FY geasey.",
            "Those are the parsing and tagging problems.",
            "And we also have these equality constraints, so in the case of the tagger all it produces are tags.",
            "In the case of the parser, it produces both structure.",
            "On top of that and also the tags beneath.",
            "And so the quality constraints are only constraining the tag part portion of the parse."
        ],
        [
            "But"
        ],
        [
            "In any case, the solution is the same.",
            "You simply parse the sentence using the."
        ],
        [
            "CECO algorithm, then you tag the sentence using the term."
        ],
        [
            "And you check out the different tags.",
            "So in this sense, in this instance we have 3."
        ],
        [
            "Tags that are different.",
            "You"
        ],
        [
            "Get your penalties an rinse."
        ],
        [
            "Repeat so again."
        ],
        [
            "Re parsing."
        ],
        [
            "At all."
        ],
        [
            "Vergence Anne.",
            "I think I'm actually going to stop here, but in any case, it suffices to say that.",
            "This works a lot faster than doing the dynamic program and so forth so.",
            "OK. Hey.",
            "Decomposition former again wasn't exactly clear, just any about sponsorship.",
            "It wasn't exactly clear to me, so I think you actually said that sometimes this is the same optimization.",
            "That's just simply optimizing offline, so I'm a bit sure, Oh yeah, so there's a.",
            "There's a bit of a that's exactly the sign.",
            "Why aren't you getting the optimal solution out right?",
            "So then why is it?",
            "How come you can solve into problems in time, right?",
            "So this these two are equivalent.",
            "But The thing is that in the dual decomposition you don't have the such that Z = y constraint.",
            "You relax that using a liberal multiplier, and so that's where the inexactness comes from.",
            "And so it's basically, so there's are there.",
            "Any guarantees they don't have close the solution when you impose that constraint is.",
            "So when the constraint is enforce the constraint because.",
            "Remind even when it's not enforced, right?",
            "So I'm just sort of wondering what is it that.",
            "Where is the source of the error here.",
            "Once again, once the approximation is being made.",
            "I will, basically you're kind of relying on the fact that either the two problems has a sensible solution.",
            "So, for example, MST parsing is something you can do and will actually let me skip ahead, so there's that slide where I had the performance of both."
        ],
        [
            "This thing so this is more or less what you're lying on, so each of the problems on their own can do reasonably well, right?",
            "Not great, but reasonably well, and so that's kind of what you're relying on that.",
            "If I chose to decompose it by just saying every other word is one problem an every other word is the other problem that would be kind of silly, right?",
            "Does that make sense?",
            "Like that's not a decomposition that captures kind of the underlying structure.",
            "So, but The thing is that if you were to do individual decoding on its own, that's like a reasonable approach, not great, reasonable MST decoding, reasonable as well.",
            "And so that's kind of what's keeping the two problems from just always recovering wildly divergent solutions.",
            "Never.",
            "Normally, in graphical models we go to the display.",
            "Start.",
            "Graphical model.",
            "And in your models are taking these equality constraints in your introducing about multipliers, right?",
            "I normally think of that as something well defined in a continuous optimization problem.",
            "YZ spaces here at the screen.",
            "Is there help relaxation underlying this?",
            "Yeah, so this is just the way that's easiest to explain all this to me so.",
            "But yeah there is a like a true LP duality and relaxation going on so.",
            "Is there a guarantee that your algorithm will always find the optimal, but you know that the ground multiplies?",
            "But yeah, so in the limit of iterations it will find.",
            "OK, well The thing is, the relaxation may not have a vertex in the appropriate place, but if well, OK."
        ],
        [
            "This isn't, maybe this might be a picture somewhere I can use.",
            "Here we go right so.",
            "So the original problem is going to be something along the lines of this green polytope constraint polytope, an R relaxation kind of is obviously an outer bound on that.",
            "And The thing is, as long as the two are tight at at the vertex of the solution, so for example, here is the point where it's tight.",
            "Here's a point where it's loose, so if the weights are pointing in the direction of the polytope, that is, yeah.",
            "So if the weights are pointing in direction polytope where they agree their tight, you're fine.",
            "But in a bad example they could be over here, and in this case.",
            "And in this case, actually that solution point.",
            "What I would believe be a fractional solution, so it would be kind of an average between two different parses an what's going to happen in the decomposition iteration is.",
            "It's just going to bounce around the neighboring vertices and basically never get there because the true solution is a fraction and your problems are parsers that can only produce integers so.",
            "But yeah, in that situation I guess what we would do is just keep track of the best tree we've seen so far, which is pretty simple.",
            "And then you just output it.",
            "OK, so let's."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is this talk is going to be unduly composition, an LP relaxations for inference in natural language processing, and I'm of course not Michael Collins.",
                    "label": 0
                },
                {
                    "sent": "But these are his slides.",
                    "label": 0
                },
                {
                    "sent": "And this is going to be describing joint work with Michael Tommy Okela, Sasha Rush, and David Sontag.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In natural language processing, there's a wide variety of structured problems.",
                    "label": 0
                },
                {
                    "sent": "For example, speech recognition, parsing, and machine translation.",
                    "label": 1
                },
                {
                    "sent": "And in each of these problems, there is usually a natural structured optimization problem that turns up.",
                    "label": 0
                },
                {
                    "sent": "So for example, in speech recognition we commonly need to do some kind of sequence labeling using HMM or CRF.",
                    "label": 0
                },
                {
                    "sent": "In parsing there are tree structured inference problems.",
                    "label": 0
                },
                {
                    "sent": "Anna machine translation.",
                    "label": 0
                },
                {
                    "sent": "There are alignment an phrase based in lemons and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Common theme in all of these structured problems is that we need to usually find some kind of best possible structure for an input.",
                    "label": 0
                },
                {
                    "sent": "So in this slide, I'm formalizing that by just saying that we have some scoring function F, and it takes a structure wise it's input, returning a score an.",
                    "label": 0
                },
                {
                    "sent": "Usually what we want to do is find the best scoring structure according to F, and this is the decoding problem.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previous work in NLP has had a lot of success using standard techniques like dynamic programming or graph algorithms to solve these kinds of decoding problems and more recently LP or ILP based inference.",
                    "label": 0
                },
                {
                    "sent": "But in this talk, I'm going to be describing work that we did in applying dual decomposition, which is a classical technique from combinatorial optimization to various inference problems in NLP.",
                    "label": 0
                },
                {
                    "sent": "And so in broad strokes, dual decomposition is basically where you take the original optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You decompose it into easier subproblems, and you solve a dual optimization, which in turn will give you the solution to the original optimization.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not going to go through a few examples of the kinds of NLP problems you can solve using the decomposition.",
                    "label": 0
                },
                {
                    "sent": "So to start off with, we have non projective dependency parsing and this is actually going to form the focus of most of the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "So I'll defer a full explanation to later, but in general it's form of parsing, so you're trying to find a syntactic analysis for a sentence.",
                    "label": 0
                },
                {
                    "sent": "And it suffices to say that this is important for many different languages.",
                    "label": 0
                },
                {
                    "sent": "And in addition, if you want to find the best possible tree for a sentence in a non projective grammar.",
                    "label": 0
                },
                {
                    "sent": "It's NP hard if you use any scoring method beyond kind of very simple naive types of scores.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More direct application of decomposition can be found if you're doing a joint parsing and tagging.",
                    "label": 0
                },
                {
                    "sent": "So hopefully most of you are familiar with keys, but in case you're not so I'll go through what's going on here.",
                    "label": 0
                },
                {
                    "sent": "We have a sentence like redfly some large jet an.",
                    "label": 0
                },
                {
                    "sent": "This is of course a sentence that's chosen to be very ambiguous regarding part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "Annina PCF what you do is you start at the words assigned a part of speech tag to each word and then you just group the words together into successively larger phrases and you end up with this tree structure that spans the whole sentence.",
                    "label": 0
                },
                {
                    "sent": "And this corresponds to basically a syntactic analysis of the kinds of relationships that are going on in the sentence.",
                    "label": 0
                },
                {
                    "sent": "Now, on the other hand, in hmm, you're just trying to find part of speech tags, and you make some simplifying assumptions.",
                    "label": 0
                },
                {
                    "sent": "So in particular, you just want to find 1 tag for each word, and you assume that the Inter relationships between the tags are a local in sequential.",
                    "label": 0
                },
                {
                    "sent": "Now The thing is, you might want to find a structure such that it optimizes both the score under the CFG and the score into the HMM, 'cause the CFG has some information from syntax and the HMM has some information from the sequential context.",
                    "label": 0
                },
                {
                    "sent": "So this is represented on the slide by having one scoring function F of Y for the CFG and another scoring function G of Z for the HMM.",
                    "label": 0
                },
                {
                    "sent": "Now, as it turns out, this is a well studied problem an it is possible to find to create a dynamic program to solve the joint tagging and parsing problem.",
                    "label": 0
                },
                {
                    "sent": "Basically what you do is take the dynamic program of the CFG and you can intersect it with a dynamic program of the HMM.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that.",
                    "label": 0
                },
                {
                    "sent": "The resulting dynamic program is pretty complex, has a very high dependency, it's polynomial, but with very high exponents.",
                    "label": 0
                },
                {
                    "sent": "And beyond the computational complexity, there's a more practical matter, which is that it's very difficult to implement correctly.",
                    "label": 0
                },
                {
                    "sent": "Now, in a dual decomposition approach.",
                    "label": 0
                },
                {
                    "sent": "Pardon.",
                    "label": 0
                },
                {
                    "sent": "And it's all the composition approach.",
                    "label": 0
                },
                {
                    "sent": "What we can do is just solve each of these problems individually by using just standard decoding methods for parsing and tagging.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Instead of trying to put together one big dynamic program, you simply shift the weights around you.",
                    "label": 0
                },
                {
                    "sent": "Just twiddle the weights a little bit.",
                    "label": 0
                },
                {
                    "sent": "The PCF scores and the tagging scores until the two decoding problems agree.",
                    "label": 0
                },
                {
                    "sent": "And so this is the basic overall approach of building composition.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And just as a final example, this is not really related to NLP, but it might be relateable for others outside NLP.",
                    "label": 0
                },
                {
                    "sent": "Here's another instance of dual decomposition in the case of Markov random fields.",
                    "label": 1
                },
                {
                    "sent": "So suppose we have some binary variables with pairwise interactions.",
                    "label": 1
                },
                {
                    "sent": "And we want to find the best scoring assignment to these variables under the pairwise interactions.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this is this graph is grid structured so it has a fairly high treewidth.",
                    "label": 0
                },
                {
                    "sent": "And so you can't find the optimal assignment efficiently for large grids.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a dual decomposition type approach, we can take this grid and just decompose it into two smaller or two 2 simpler MFS.",
                    "label": 1
                },
                {
                    "sent": "So in this case this is just a red tree and a blue tree.",
                    "label": 0
                },
                {
                    "sent": "And instead of solving this original.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maximization.",
                    "label": 0
                },
                {
                    "sent": "We install instead solve this decomposed maximization so we now have two summations over pairwise interactions, one for each of the trees.",
                    "label": 0
                },
                {
                    "sent": "But the twist here is that we're acquiring that the variables of the red and blue MRF's the equivalent to each other.",
                    "label": 0
                },
                {
                    "sent": "That's the constraint down here.",
                    "label": 0
                },
                {
                    "sent": "So thanks to this constraint, these two optimizations are equivalent.",
                    "label": 0
                },
                {
                    "sent": "And so actually this hasn't really gotten us anywhere yet.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the next step we can do is to simply relax the equality constraints.",
                    "label": 0
                },
                {
                    "sent": "By introducing LaGrange multipliers and so this is kind of the technical background of what's going on behind the composition.",
                    "label": 0
                },
                {
                    "sent": "And don't worry, we'll be going over this in more detail later.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to start out by describing how we used dual decomposition to solve.",
                    "label": 0
                },
                {
                    "sent": "On projective dependency parsing with rich scores.",
                    "label": 0
                },
                {
                    "sent": "Then I'll go through some experimental results and time permitting, I'll try to go through some other examples of dual decomposition in NLP.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'll start off by describing what non projective dependency parsing is.",
                    "label": 0
                },
                {
                    "sent": "Depending part, dependency parsing itself is a syntactic formalism where also syntactic information is represented using head modifier dependencies, and these are just interactions between pairs of words where one of the words is the head.",
                    "label": 0
                },
                {
                    "sent": "The more important word, and the other word is the modifier, the less important.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this slide we have a sentence Johnson movie today that he liked and we also have its dependency parse down here.",
                    "label": 0
                },
                {
                    "sent": "Each of the dependencies are represented using a directed arc.",
                    "label": 0
                },
                {
                    "sent": "And for example, you have a dependence between John and saw where saw the head pointing to John the modifier and this represents the fact that John is a subjective saw.",
                    "label": 0
                },
                {
                    "sent": "We also you can also note that we introduced this star this root symbol and this is more of a notational convenience than anything else, But basically a dependency parse is defined as a rooted directed tree that starts at the star and spans all the words of the sentence.",
                    "label": 1
                },
                {
                    "sent": "OK, so given that, that's what dependency parsing is.",
                    "label": 0
                },
                {
                    "sent": "Non productive parsing is dependency parsing where you also allow dependencies to cross so you can see in this sentence is an interaction between today and saw and also that he liked and movie and these two dependencies are crossing each other.",
                    "label": 0
                },
                {
                    "sent": "Now, in the case of projective parsing, where you forbid crossings.",
                    "label": 0
                },
                {
                    "sent": "It's possible to design just very straightforward dynamic programming algorithms to solve the parsing problem.",
                    "label": 0
                },
                {
                    "sent": "But if you allow dependencies to cross in non projective parsing then it becomes difficult to parse with rich scores.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in the remainder of this section I'm going to be focusing on two different scoring models for dependency parsing.",
                    "label": 0
                },
                {
                    "sent": "In the first model, which we call the arc factored scoring model, we restrict we restrict the scores to be very simple, semi impoverished.",
                    "label": 0
                },
                {
                    "sent": "But the advantage is that we can do efficient decoding under those scores.",
                    "label": 0
                },
                {
                    "sent": "In the second approach, which I'm going to call the sibling scoring model, we have stronger, richer scores.",
                    "label": 0
                },
                {
                    "sent": "But of course, this breaks the decoding, so we can't do efficient decoding.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the overall the overall goal will be to take the efficient inference from one an try to apply it to the rich scores of the US.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'll start by going through the scoring models first.",
                    "label": 0
                },
                {
                    "sent": "We have the arc factored scoring.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case we just Deacon.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Close the score of a dependency tree into the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The scores of edges, just like it sounds.",
                    "label": 0
                },
                {
                    "sent": "So here we have, for example, the score of the dependency between saw and today.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dependencies and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know here that were just being agnostic to the source of the scores.",
                    "label": 0
                },
                {
                    "sent": "So for example, the dependency scores could be coming from a generative model or a discriminative model, linear model, etc.",
                    "label": 1
                },
                {
                    "sent": "But we don't care for the purposes of this talk.",
                    "label": 0
                },
                {
                    "sent": "And as Brian McDonald pointed out in previous work, as long as your scores are restricted to the edges, then non projective parsing is the same thing as finding minimum spanning trees.",
                    "label": 0
                },
                {
                    "sent": "So it's clear that you can just do an efficient decoding in this situation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's take a look.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What a sibling model is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this case I guess that's not so interesting.",
                    "label": 0
                },
                {
                    "sent": "So in this case, instead of having scores to find an individual arcs, you have scores defined on pairs of arcs.",
                    "label": 0
                },
                {
                    "sent": "So for example, there's you can assign a score to the triple of words saw movie in today, or solve the head and moving in today are adjacent modifyers.",
                    "label": 0
                },
                {
                    "sent": "And it's been shown in projective parsing that if you introduce these kinds of of 2nd order scores, you can get increases in parsing performance.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, these scores can come from general models.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Morales, what not, but the problem is that if you want to find the best tree subject to this kind of scoring, it's actually NP hard.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so given that it's hard to do tree decoding, let's take a look at some alternative methods for decoding.",
                    "label": 0
                },
                {
                    "sent": "For example, let's see what happens if we just take one word.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, saw and try to find all the modifiers that it likes to take.",
                    "label": 0
                },
                {
                    "sent": "So what's the best set of modifiers for this single?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the in the true or the correct parse, that set of modifiers would be.",
                    "label": 0
                },
                {
                    "sent": "John is a subject movie as the object and today is just a temporal modifier.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there is lots of other possibilities, so you could have saw taking that role.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applause that he liked.",
                    "label": 0
                },
                {
                    "sent": "Or you could have some just completely silly examples, so we're A and he or the children.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in general, there's an exponential number of possibilities.",
                    "label": 0
                },
                {
                    "sent": "But Fortunately I'm not going to go into the details here, but you can find the best set of modifiers in a sibling scoring model by just using a minor variation of the Turbo decoding.",
                    "label": 0
                },
                {
                    "sent": "Sequence labeling.",
                    "label": 0
                },
                {
                    "sent": "So this can actually be done efficiently for one.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's all.",
                    "label": 0
                },
                {
                    "sent": "That's all very well and good for finding modifiers for one word.",
                    "label": 0
                },
                {
                    "sent": "But what if we want to find a parse?",
                    "label": 0
                },
                {
                    "sent": "Well, if we want to be happy about it, we can just say let's just do that for all the words.",
                    "label": 0
                },
                {
                    "sent": "So for example, you find the best set of modifiers for the root and you.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That dependency.",
                    "label": 0
                },
                {
                    "sent": "Check out John and it doesn't want to have any model.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arts.",
                    "label": 0
                },
                {
                    "sent": "And just continue through the sentence for.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sword.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finding the best.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Modifyers",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So good, but then we get to the end, an oops.",
                    "label": 0
                },
                {
                    "sent": "Now you see here that light actually wants to take John as its subject, which is of course improper.",
                    "label": 0
                },
                {
                    "sent": "Now this thing is in a tree anymore.",
                    "label": 0
                },
                {
                    "sent": "You can see that John has two heads and he has none, which is bad.",
                    "label": 0
                },
                {
                    "sent": "So if we're lucky when we do this kind of individual decoding, we might end up with a tree.",
                    "label": 1
                },
                {
                    "sent": "But more than more likely than not, we're going to end up violating some of the tree constraints, creating cycles, or having these words with multiple heads and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's take a step back.",
                    "label": 0
                },
                {
                    "sent": "We've seen that forearc factored scoring.",
                    "label": 0
                },
                {
                    "sent": "We can do efficient, efficient decoding using MST.",
                    "label": 0
                },
                {
                    "sent": "For the sibling scores, we can do efficient decoding as long as we throw away tree constraints, but we want to see what happens if we have both siblings, scores, entry constraints, and in this case, it turns out that we can combine add tree constraints to the overall Deco.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By using decomposition.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm going to go through basically how we decompose the non projective parsing problem.",
                    "label": 0
                },
                {
                    "sent": "So first, let's start with our overall goal, which is to find the Max the best tree according to F, which is for the purposes of this slide, the sibling scoring function.",
                    "label": 0
                },
                {
                    "sent": "An why here is the set of all valid tree.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to decompose it or rewrite this objective in this kind of silly way.",
                    "label": 0
                },
                {
                    "sent": "I'll go through what each piece.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is doing.",
                    "label": 0
                },
                {
                    "sent": "So here the set Z is the set of all possible structures.",
                    "label": 0
                },
                {
                    "sent": "So if you throw away tree constraints and just consider what sets of edges can you take from the sentence, you end up with Z.",
                    "label": 1
                },
                {
                    "sent": "So these things are just structures without tree.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strains on the other hand, why is going to be drawn from the set of valid trees?",
                    "label": 0
                },
                {
                    "sent": "So these are just norm?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dependency parses.",
                    "label": 0
                },
                {
                    "sent": "F of Z Here is going to do sibling scoring on these unconstrained structures.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Angie of why is going to do arc factored scoring on valid trees?",
                    "label": 0
                },
                {
                    "sent": "And of course.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The kicker down here is that you still have this Z = y constraint and so that means that this problem is equivalent to the upper problem.",
                    "label": 0
                },
                {
                    "sent": "So so far we haven't done anything, but as we've seen before, what we do and do, the composition is just relax the equality constraint.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we relax that constraint, I'm not going to go into the details at this moment, but we end up with an iterative algorithm for finding the optimal parse under a sibling scoring model.",
                    "label": 0
                },
                {
                    "sent": "The first step is to the code.",
                    "label": 0
                },
                {
                    "sent": "To find the best scoring structure under the individual model, the sibling model and also the best scoring tree using an MST model.",
                    "label": 0
                },
                {
                    "sent": "So we do decoding using two different models and we end up with two different structures Z&Y.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we check if Z is equal to Y.",
                    "label": 0
                },
                {
                    "sent": "In the case that it is equal, we stop right there and we return that equals equivalent structure as our parse.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, if the two structures are not equivalent, then we're going to make some tiny changes.",
                    "label": 0
                },
                {
                    "sent": "Small changes to the scores for the individual decoding, and the MST decoding based on the points of disagreement between the two structures CNY.",
                    "label": 0
                },
                {
                    "sent": "And that's actually it's as simple as that.",
                    "label": 0
                },
                {
                    "sent": "You just repeatedly do this to code there around the decoding and rounds of correction.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to go through an example to make this more concrete.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                },
                {
                    "sent": "Will state that we have penalties.",
                    "label": 0
                },
                {
                    "sent": "These use in the upper right hand corner, so for every dependency between word I and word J we introduce a penalty.",
                    "label": 0
                },
                {
                    "sent": "You have IJ.",
                    "label": 0
                },
                {
                    "sent": "And these are going to be initially 0.",
                    "label": 0
                },
                {
                    "sent": "And now on the left side we have two different decoding problems.",
                    "label": 0
                },
                {
                    "sent": "One of them is the individual decoding and the MST.",
                    "label": 0
                },
                {
                    "sent": "And so the objective for individual decoding is the original scoring F of Z, augmented with some extra penalties on each of the dependencies.",
                    "label": 0
                },
                {
                    "sent": "Same thing for the MST.",
                    "label": 0
                },
                {
                    "sent": "We have the original MST scoring G of Y augmented with penalties from the use.",
                    "label": 0
                },
                {
                    "sent": "And of course, at the start these usaral 0 so actually it's just decoding.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Normally.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we can go ahead and to code each of the sides.",
                    "label": 0
                },
                {
                    "sent": "And check for disagreements.",
                    "label": 0
                },
                {
                    "sent": "So clearly these two.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actors are not the same.",
                    "label": 0
                },
                {
                    "sent": "There are four dependencies that are different.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each of these.",
                    "label": 0
                },
                {
                    "sent": "We make a change to the set of penalties, so specifically for the there are two dependencies up here that don't appear in the MST, and so for those we give a negative one wait.",
                    "label": 0
                },
                {
                    "sent": "There are two dependencies in the MST that don't appear up top, and for those who give a positive one, wait.",
                    "label": 0
                },
                {
                    "sent": "So now we have new penalty weights.",
                    "label": 0
                },
                {
                    "sent": "Clearly the two structures.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I agree, and so we did code again.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Individual decoding MST decoding.",
                    "label": 0
                },
                {
                    "sent": "We end up with a new structure up top, but it's still now.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same.",
                    "label": 0
                },
                {
                    "sent": "So now we do another round of updating the weights so we get another minus 1 + 1 in there.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally decoded.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another round an.",
                    "label": 0
                },
                {
                    "sent": "In this example, we now have an agreeing.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set of pair of structures and so we can say we can just stop right here and output that structure.",
                    "label": 0
                },
                {
                    "sent": "So I think this is a fairly intuitive algorithm you're just doing decoding on two sides.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're pushing this whole solution on.",
                    "label": 0
                },
                {
                    "sent": "I guess 98% or greater of the attached examples.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And even in the case that we don't find an agreeing pair of structures, we can still come up with an approximate solution at any point.",
                    "label": 0
                },
                {
                    "sent": "I guess I won't go through the details here, but you can ask me about it.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so given that we have this framework where we have an MST versus a bunch of individual decodings, it's pretty easy to extend the scoring model of the individual decodings.",
                    "label": 0
                },
                {
                    "sent": "So for example, instead of just using sibling factors that have 3 words, we can use these kind of grandparent and sibling factors that have 4 words.",
                    "label": 0
                },
                {
                    "sent": "This is something that we did in the paper and we found that it improved performance.",
                    "label": 0
                },
                {
                    "sent": "And this is extremely easy to do 'cause it's always just a single word at a time.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now let's take a look at some of the theoretical background between behind why this tool decomposition algorithm works the way it does.",
                    "label": 0
                },
                {
                    "sent": "So up on the left we have the original objective and on the right we have our rewritten objective that has both Z&Y.",
                    "label": 0
                },
                {
                    "sent": "And it also has these equality constraints that force the two degree with each other.",
                    "label": 0
                },
                {
                    "sent": "So instead of optimizing that problem, we're going to relax the equality constraints by introducing LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "And we end up with the ground ring that has the two original objectives as well as this penalty term that has, for each potential disagreement, LaGrange multiplier uij.",
                    "label": 0
                },
                {
                    "sent": "And these are exactly the usage that we were updating in that example.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we now can formulate a dual problem, which is exactly this pair of maximization's which we saw in the earlier example.",
                    "label": 0
                },
                {
                    "sent": "So up top we have the sibling problem that's been augmented with penalties an MST that's been augmented with penalties.",
                    "label": 0
                },
                {
                    "sent": "And you'll note that each of these two problems is now completely decomposed from the other, except for EU penalty weights.",
                    "label": 0
                },
                {
                    "sent": "Of course, the dual forms an upper bound on the optimal value of the primal, so a logical thing to do is to find the least upper bound by.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Minimizing the dual.",
                    "label": 0
                },
                {
                    "sent": "Note that the dual as shown up here is just a pair of maximization, so this is convex, although because the objectives are linear, they're not going to be differentiable.",
                    "label": 0
                },
                {
                    "sent": "It's going to be faceted.",
                    "label": 0
                },
                {
                    "sent": "But an easy way to minimize this tool is to just do subgradient descent on the penalty weights you.",
                    "label": 0
                },
                {
                    "sent": "And so I think most of us probably notice upgrades, but just in case you don't, it's basically equivalent to the grading, except when there's a corner.",
                    "label": 0
                },
                {
                    "sent": "In that case, it's just anything that's under the corner.",
                    "label": 0
                },
                {
                    "sent": "Since the objective the dual is a pair of maximization's, this upgrading is just a pair of Arg maximization's and so you can see at the bottom the subgradient is just the difference between the vectors or the solutions that structures for zny.",
                    "label": 0
                },
                {
                    "sent": "So you just do so when you do the decoding on both sides and take the difference and add it to the penalties.",
                    "label": 0
                },
                {
                    "sent": "What you're doing there is a subgradient update.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I guess I'm going to switch into talking about the parsing experiments we did.",
                    "label": 1
                },
                {
                    "sent": "We evaluated many things.",
                    "label": 0
                },
                {
                    "sent": "Exactness, which is the amount of percentage of the time that we find an optimal solution.",
                    "label": 1
                },
                {
                    "sent": "We also looked at parsing speed and accuracy.",
                    "label": 0
                },
                {
                    "sent": "And we also compared our dual decomposition parser against the individual decoding model and also the MST model.",
                    "label": 0
                },
                {
                    "sent": "And finally, we did some comparison experiments against decoding using linear program or integer linear programming.",
                    "label": 0
                },
                {
                    "sent": "In order to train the parameters or models, use the average perceptron.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And the experiments were run on a selection of languages from Connell datasets as well as English and checked remix.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first of all will look at exactness.",
                    "label": 0
                },
                {
                    "sent": "So at the bottom is just the set of languages we look like we looked at and you can see that for most of them were hitting 98 or above.",
                    "label": 0
                },
                {
                    "sent": "So that means that on the test set we find the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "We find a green pair in the test set.",
                    "label": 0
                },
                {
                    "sent": "And for the rest of them, they're above 95, so we're doing pretty well at finding exact solutions.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is a comparison in terms of accuracy.",
                    "label": 0
                },
                {
                    "sent": "The leftmost column is just the plain old MST parser using simple scores.",
                    "label": 0
                },
                {
                    "sent": "Middle is the previous best reported result for languages for which we have a comparison.",
                    "label": 0
                },
                {
                    "sent": "And the right is our grandparent based individual.",
                    "label": 0
                },
                {
                    "sent": "Decoding plus MST are dual decomposition parser in this situation.",
                    "label": 0
                },
                {
                    "sent": "So you can see that obviously the decomposition parser is, well, outperforming this arc factored parser, and we're also getting gains over previous work as well.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is a comparison of performance between the decomposition parser and also the two subcomponents of that dual decomposition.",
                    "label": 0
                },
                {
                    "sent": "So the red bar here is the individual decoding using sibling scores.",
                    "label": 0
                },
                {
                    "sent": "The Green Bar is the MST decoding using arc factor scores and the blue bar is the two put together.",
                    "label": 0
                },
                {
                    "sent": "And so you can see that little decomposition is doing significantly better than either of the other two alone.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the next few slides, I'm going to go through some comparisons between dual decomposition, inference and inference based on LP or ILP.",
                    "label": 0
                },
                {
                    "sent": "So what we did in our experiments in this section was to reimplement previous work by Andrew Martin.",
                    "label": 0
                },
                {
                    "sent": "And specifically we constructed.",
                    "label": 0
                },
                {
                    "sent": "Inference algorithms based on two different types of LP relaxations which are this LP, One and LP two as well as plain integer linear programming.",
                    "label": 0
                },
                {
                    "sent": "And we're going to make comparisons between our dual decomposition parser and these previous work based on LP LP.",
                    "label": 0
                },
                {
                    "sent": "In terms of accuracy, exactness and speed.",
                    "label": 0
                },
                {
                    "sent": "And one thing to note here is that because we wanted everything, it's a straight head to head comparison between the inference algorithms, since all the features and scores and so forth are identical.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here is first comparison in terms of accuracy.",
                    "label": 0
                },
                {
                    "sent": "Not much difference here, so it seems that all the relaxed LP's are decomposition and the LP are doing roughly the same in terms of development accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But on this slide we have comparison in terms of exactness and speed.",
                    "label": 0
                },
                {
                    "sent": "And so this is a little more interesting, so starting from the left, the red bar is the LP one relaxation.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of the more relaxed, most relaxed relaxation.",
                    "label": 0
                },
                {
                    "sent": "The Green Bar is LP 2, which is tighter.",
                    "label": 0
                },
                {
                    "sent": "I mean I can go into the specifics of what why they're different but.",
                    "label": 0
                },
                {
                    "sent": "For the moment, it's only important to know that one is more relaxed in the other, so you can see that the tighter relaxation is more exact.",
                    "label": 0
                },
                {
                    "sent": "I LP is of course 1% exact and dual decomposition is doing pretty well.",
                    "label": 0
                },
                {
                    "sent": "I think around 98.",
                    "label": 0
                },
                {
                    "sent": "On the right we have this comparison in terms of speed, and so again you can see kind of reasonable results.",
                    "label": 0
                },
                {
                    "sent": "The more relaxed relaxation LP one is, the fast faster of the two.",
                    "label": 0
                },
                {
                    "sent": "LP is pretty slow, but dual decomposition is pretty fast and the reason for that is mainly that instead of using kind of general purpose LP solving were using special purpose parsers that are kind of.",
                    "label": 0
                },
                {
                    "sent": "Built based on problem knowledge rather than just general.",
                    "label": 0
                },
                {
                    "sent": "LP solvers.",
                    "label": 0
                },
                {
                    "sent": "OK, how are you doing and.",
                    "label": 0
                },
                {
                    "sent": "Ultimate OK.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess I might as well.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go ahead and talk a little bit about the integrated parsing tagging.",
                    "label": 0
                },
                {
                    "sent": "So again, in the case of an HMM.",
                    "label": 0
                },
                {
                    "sent": "You have you're trying to assign a sequence of tags while using local sequential context.",
                    "label": 0
                },
                {
                    "sent": "And in the case of a CFG, you're assigning this nested tree structure to a sentence which also includes.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "Ann, we've I've mentioned before that you can combine the two in the large dynamic program.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in this section I'll go over how we can do this by using dual decomposition.",
                    "label": 0
                },
                {
                    "sent": "So in the case of HMM tagging, let's let Z be the set of all valid taggings and G of CBS scoring.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's well known that you can solve this.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing for tardy.",
                    "label": 0
                },
                {
                    "sent": "In the case of parsing, will let Y be the set of valid parse trees F of yvs scoring function, for example.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probabilities.",
                    "label": 0
                },
                {
                    "sent": "And again, you can solve this using the well known CKY algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now suppose that we want to find the parse tree that optimizes both sequential tag scores and top down tree scores.",
                    "label": 0
                },
                {
                    "sent": "You can do this using the intersected dynamic program, but it involves a order to six increase and runtime.",
                    "label": 0
                },
                {
                    "sent": "I think that's for a trigram tagger, so, but in any case it's quite a bit slower and also more complex to implement.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can reformulate the integrated parsing Italian problem problem by using a similar kind of pair of scoring functions.",
                    "label": 1
                },
                {
                    "sent": "You have FY geasey.",
                    "label": 0
                },
                {
                    "sent": "Those are the parsing and tagging problems.",
                    "label": 1
                },
                {
                    "sent": "And we also have these equality constraints, so in the case of the tagger all it produces are tags.",
                    "label": 0
                },
                {
                    "sent": "In the case of the parser, it produces both structure.",
                    "label": 0
                },
                {
                    "sent": "On top of that and also the tags beneath.",
                    "label": 0
                },
                {
                    "sent": "And so the quality constraints are only constraining the tag part portion of the parse.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In any case, the solution is the same.",
                    "label": 0
                },
                {
                    "sent": "You simply parse the sentence using the.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "CECO algorithm, then you tag the sentence using the term.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you check out the different tags.",
                    "label": 0
                },
                {
                    "sent": "So in this sense, in this instance we have 3.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tags that are different.",
                    "label": 0
                },
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get your penalties an rinse.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Repeat so again.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Re parsing.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At all.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vergence Anne.",
                    "label": 0
                },
                {
                    "sent": "I think I'm actually going to stop here, but in any case, it suffices to say that.",
                    "label": 0
                },
                {
                    "sent": "This works a lot faster than doing the dynamic program and so forth so.",
                    "label": 0
                },
                {
                    "sent": "OK. Hey.",
                    "label": 0
                },
                {
                    "sent": "Decomposition former again wasn't exactly clear, just any about sponsorship.",
                    "label": 0
                },
                {
                    "sent": "It wasn't exactly clear to me, so I think you actually said that sometimes this is the same optimization.",
                    "label": 0
                },
                {
                    "sent": "That's just simply optimizing offline, so I'm a bit sure, Oh yeah, so there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a bit of a that's exactly the sign.",
                    "label": 0
                },
                {
                    "sent": "Why aren't you getting the optimal solution out right?",
                    "label": 0
                },
                {
                    "sent": "So then why is it?",
                    "label": 0
                },
                {
                    "sent": "How come you can solve into problems in time, right?",
                    "label": 0
                },
                {
                    "sent": "So this these two are equivalent.",
                    "label": 0
                },
                {
                    "sent": "But The thing is that in the dual decomposition you don't have the such that Z = y constraint.",
                    "label": 0
                },
                {
                    "sent": "You relax that using a liberal multiplier, and so that's where the inexactness comes from.",
                    "label": 0
                },
                {
                    "sent": "And so it's basically, so there's are there.",
                    "label": 0
                },
                {
                    "sent": "Any guarantees they don't have close the solution when you impose that constraint is.",
                    "label": 0
                },
                {
                    "sent": "So when the constraint is enforce the constraint because.",
                    "label": 0
                },
                {
                    "sent": "Remind even when it's not enforced, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm just sort of wondering what is it that.",
                    "label": 0
                },
                {
                    "sent": "Where is the source of the error here.",
                    "label": 0
                },
                {
                    "sent": "Once again, once the approximation is being made.",
                    "label": 0
                },
                {
                    "sent": "I will, basically you're kind of relying on the fact that either the two problems has a sensible solution.",
                    "label": 0
                },
                {
                    "sent": "So, for example, MST parsing is something you can do and will actually let me skip ahead, so there's that slide where I had the performance of both.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This thing so this is more or less what you're lying on, so each of the problems on their own can do reasonably well, right?",
                    "label": 0
                },
                {
                    "sent": "Not great, but reasonably well, and so that's kind of what you're relying on that.",
                    "label": 0
                },
                {
                    "sent": "If I chose to decompose it by just saying every other word is one problem an every other word is the other problem that would be kind of silly, right?",
                    "label": 0
                },
                {
                    "sent": "Does that make sense?",
                    "label": 0
                },
                {
                    "sent": "Like that's not a decomposition that captures kind of the underlying structure.",
                    "label": 0
                },
                {
                    "sent": "So, but The thing is that if you were to do individual decoding on its own, that's like a reasonable approach, not great, reasonable MST decoding, reasonable as well.",
                    "label": 0
                },
                {
                    "sent": "And so that's kind of what's keeping the two problems from just always recovering wildly divergent solutions.",
                    "label": 0
                },
                {
                    "sent": "Never.",
                    "label": 0
                },
                {
                    "sent": "Normally, in graphical models we go to the display.",
                    "label": 0
                },
                {
                    "sent": "Start.",
                    "label": 0
                },
                {
                    "sent": "Graphical model.",
                    "label": 0
                },
                {
                    "sent": "And in your models are taking these equality constraints in your introducing about multipliers, right?",
                    "label": 0
                },
                {
                    "sent": "I normally think of that as something well defined in a continuous optimization problem.",
                    "label": 0
                },
                {
                    "sent": "YZ spaces here at the screen.",
                    "label": 0
                },
                {
                    "sent": "Is there help relaxation underlying this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is just the way that's easiest to explain all this to me so.",
                    "label": 0
                },
                {
                    "sent": "But yeah there is a like a true LP duality and relaxation going on so.",
                    "label": 0
                },
                {
                    "sent": "Is there a guarantee that your algorithm will always find the optimal, but you know that the ground multiplies?",
                    "label": 0
                },
                {
                    "sent": "But yeah, so in the limit of iterations it will find.",
                    "label": 0
                },
                {
                    "sent": "OK, well The thing is, the relaxation may not have a vertex in the appropriate place, but if well, OK.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This isn't, maybe this might be a picture somewhere I can use.",
                    "label": 0
                },
                {
                    "sent": "Here we go right so.",
                    "label": 0
                },
                {
                    "sent": "So the original problem is going to be something along the lines of this green polytope constraint polytope, an R relaxation kind of is obviously an outer bound on that.",
                    "label": 0
                },
                {
                    "sent": "And The thing is, as long as the two are tight at at the vertex of the solution, so for example, here is the point where it's tight.",
                    "label": 0
                },
                {
                    "sent": "Here's a point where it's loose, so if the weights are pointing in the direction of the polytope, that is, yeah.",
                    "label": 0
                },
                {
                    "sent": "So if the weights are pointing in direction polytope where they agree their tight, you're fine.",
                    "label": 0
                },
                {
                    "sent": "But in a bad example they could be over here, and in this case.",
                    "label": 0
                },
                {
                    "sent": "And in this case, actually that solution point.",
                    "label": 0
                },
                {
                    "sent": "What I would believe be a fractional solution, so it would be kind of an average between two different parses an what's going to happen in the decomposition iteration is.",
                    "label": 0
                },
                {
                    "sent": "It's just going to bounce around the neighboring vertices and basically never get there because the true solution is a fraction and your problems are parsers that can only produce integers so.",
                    "label": 0
                },
                {
                    "sent": "But yeah, in that situation I guess what we would do is just keep track of the best tree we've seen so far, which is pretty simple.",
                    "label": 0
                },
                {
                    "sent": "And then you just output it.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's.",
                    "label": 0
                }
            ]
        }
    }
}