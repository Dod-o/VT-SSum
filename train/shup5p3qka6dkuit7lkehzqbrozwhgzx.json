{
    "id": "shup5p3qka6dkuit7lkehzqbrozwhgzx",
    "title": "Classification and Clustering via Dictionary Learning with Structured Incoherence and Shared Features",
    "info": {
        "author": [
            "Pablo Sprechmann, University of Minnesota"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_sprechmann_ccdl/",
    "segmentation": [
        [
            "OK, well thank you very much.",
            "I will be presenting the work classification and clustering via dictionary learning with structuring coherence and shared features.",
            "This is joint work with Ignacio Ramirez and Guillermo Sapiro.",
            "I want to say that I."
        ],
        [
            "Will be presenting a poster also with this work in the poster session, so any doubts you have I'll be happy to answer them there.",
            "OK. Well, the goal of this work is to present a framework based on sparse modeling to address some of the of the problems that are shown here from a modeling point of view.",
            "For example, object recognition image or texture segmentation and source separation.",
            "In this talk I will build up.",
            "I will show how we can build up a more model starting from image just for classification tasks that can be easily generalized to the unsupervised case.",
            "An hour richer model that allowed us to tackle source separation problems in a natural way."
        ],
        [
            "OK, so first I will briefly review sparse modeling.",
            "There was a very good tutorial at the beginning of the week, so maybe some of you already know a lot about this.",
            "OK, so the underlying assumption is that.",
            "A set of data can be well accurately represented as.",
            "Sparse linear combination of the columns of a dictionary.",
            "A it has been shown but with many works in the literature that learning those dictionaries performs in generally better than.",
            "Predefined ones such as PCD, Fourier, wavelets so.",
            "A success was obtained, in particular by training dictionaries on image patches.",
            "So here for example, the patches are columns of this data matrix and a dictionary that can be learned.",
            "For example, this are the columns of the dictionary.",
            "So what does this mean from geometrical point of view?",
            "OK, each path."
        ],
        [
            "Can be thought as a point in a high dimensional space and by."
        ],
        [
            "Sparse modeling what we're doing is approximating those points by."
        ],
        [
            "A union of subspaces, low dimensional subspaces, and did I mention here is not fixed.",
            "An dimension is given by a combination of atoms in the dictionary.",
            "So."
        ],
        [
            "How can we learn those dictionaries?",
            "A again here I will not go into the details, but what we do is we minimize functional like this where we have a data fitting term and a regularizer.",
            "We sparsity inducing regularizer.",
            "Here we use the L1 norm.",
            "We use as a basis to our code where we constructed the Spanish software that is freely available and very good."
        ],
        [
            "So how can we use sparse modeling for classification?",
            "So the first idea that we can have is very simple.",
            "So say we are giving C different classes with the corresponding training data.",
            "So what we can do is we can learn a dictionary per class and then just think C. Which is the class for which the best representation is obtained.",
            "This is not a is a simple idea.",
            "So what we propose."
        ],
        [
            "Here is to use us to measure how good this representation is.",
            "We propose to use this function that first is consistent with the way we were learning the dictionaries, and second it accounts for two terms, so they are reconstruction error and also what we call a complexity turn that also looks at the L1 norm of the coefficients.",
            "If we think many of these dictionaries are over complete, so achieving a small reconstruction error doesn't mean that the dictionary.",
            "Is is good but.",
            "If we look at those both things together we then we are kind of looking that.",
            "Also this representation is sparse."
        ],
        [
            "OK so I will show some results for this very basic procedure that is in the heart of the methods that I will explain later.",
            "Here we used two basic digit datasets and we're comparing with recent results that also use sparse coding with some reconstructive and discriminative ways, and this algorithm that can be written in three lines using the Spanish software performs competitively well.",
            "So now we will see how we can."
        ],
        [
            "Improve these until now we were learning the dictionaries independently for each class, so the dictionaries ignored that they were going to be compared with the other ones.",
            "So we want to learn them together.",
            "This was the functional that we were using and we propose to add an extra an additional term.",
            "That we call the crossing coherence term.",
            "And the the motivation behind this is based on recent work by drop or Elder and coauthors, for example, that show that the block sparse the success of block sparse signals.",
            "With a L1 regularizer.",
            "Increases when we have low coherence between the dictionaries.",
            "So by promoting the dictionaries we are promoting to the dictionaries, the atoms to be incoherent from different dictionaries."
        ],
        [
            "OK, so imposing that the atoms are different thus does not.",
            "Also allows to have shared features because despite the fact that we tried, addiction is to be different.",
            "There are some features or items that are important from a reconstructive POV, and those remain after the learning stage.",
            "Here I'm showing 2 examples of dictionaries of number of class of three spawn 5 and we can see that with the high coherence items capture some shared features.",
            "These shared features can be easily detected by looking at the coherence.",
            "Of the dictionary.",
            "This."
        ],
        [
            "Atoms can be if they are shared by all the classes can be ignored while computing this function.",
            "R for example.",
            "OK, so also an important fact is that the coherence term does not depend on the training data, only depends on the dictionaries.",
            "So is it can be easily plugged into an unsupervised case and clustering.",
            "So let's now move to that.",
            "How can we extend?"
        ],
        [
            "Algorithm for clustering.",
            "So in clustering we're giving a set of data and we know how many class we assume that we know how many classes are.",
            "Here, so we cannot apply our method straight directly because we don't have any initial partition.",
            "We could do it if we had so now I will review and initialization step that we're using.",
            "As I said, I will be in the poster session.",
            "I can give you all the details there.",
            "So what we do is we first learn addiction."
        ],
        [
            "I need to represent all the data points.",
            "This is 1 big global dictionary.",
            "So now we want to define a similarity between atoms.",
            "In this dictionary they way we chose is the following.",
            "We do the sparse coding of all the signals onto this dictionary and we will say the two atoms are similar if they are simultaneously used by a big number of signals.",
            "So then."
        ],
        [
            "We get is a graph that has as notes the atoms of the dictionary that can be.",
            "Cut using, for example, splits spectral clustering.",
            "This approach is closely related to the L1 graph and subspace clustering.",
            "Recent works in the literature."
        ],
        [
            "So now after we do this cut, we have some initial dictionaries and we could do whether I said for the classification we can apply this and get an initial partition.",
            "So we the dictionaries learned we partition the data.",
            "Now in two classes.",
            "OK.",
            "But this might not be accurate.",
            "So what we can do?"
        ],
        [
            "Is iterated in by learning the dictionaries and reassigning the point to the different classes until no change changes or not.",
            "Substantial changes are observed, so after we."
        ],
        [
            "Do this.",
            "This reminds a lot.",
            "The K means clustering.",
            "OK, so here instead of having class centers, what we're having is dictionaries to represent the data, which makes sense because the the data we're handling here is much.",
            "This creature so this can be seen as an energy minimization problem.",
            "Here the functional is non convex jointly in the class assignment and in the dictionaries.",
            "But if we fix the one of them, we get a convex problem.",
            "And Furthermore, if we fix the dictionaries, for example, we get the assignment based on their function R that we were talking about an if we fix the class assignments, we get the dictionary learning algorithm functional that we were.",
            "Discribe so now I."
        ],
        [
            "Will show some.",
            "Applications to vision.",
            "First, we did object recognition using the graph data set.",
            "Here we learn the dictionaries on vectors an we have two classes when background and one bike.",
            "So I'm showing some precision recall curves in the written version of the paper, we have more comparisons, but we're comparing only with algorithms that use local features.",
            "Because I mean, we have seen in this conference that adding global an spatial.",
            "Consistency gives leads to better results.",
            "This is just using the modeling.",
            "The next update."
        ],
        [
            "Patient would be texture segmentation.",
            "Here we are giving an image and we know the number of textures that we have, but we don't know anything else, so we apply our clustering algorithm and we get I'm showing here.",
            "The dictionary is the final dictionaries obtained.",
            "We can see that the structure of the texture is clearly there an also the segmentation results after with the iterations."
        ],
        [
            "So until now, what we were assuming is that each signal belongs to one single class.",
            "What happens if a signal is a mixture of classes here?",
            "So let's go to this interpretation.",
            "We can glue the dictionaries, for example for textures and get a bigger dictionary to represent them.",
            "So."
        ],
        [
            "Before we have a signal that belong to a dictionary, we test all the possible dictionaries and we select one of them.",
            "Here we have.",
            "Only non zero coefficients in the part of the coefficient matrix that corresponds to that part to that sub dictionary.",
            "So when the next."
        ],
        [
            "Signal we get another one, and so on."
        ],
        [
            "So what happens now if we get on a combination of textures here, the number is unknown.",
            "We don't know how many sources are they, so we develop an algorithm that gives a coding that allows us to find the sources and find the structure there.",
            "Here the.",
            "We call this calling Erotical because it is.",
            "It has a sparsity at a group level.",
            "An inside the groups.",
            "This is the structure to the coding is has received a lot of attention recently.",
            "An for example, group lasso is one of these cases, but there are many others."
        ],
        [
            "OK, so when the next following signal we find that we do the coding, we find which signals are present in the mixture."
        ],
        [
            "But in many image processing applications or signal or audio processing applications we have spatial or temporal correlation that tells us, like in the talk that we just saw that we can jointly code signals together."
        ],
        [
            "So we developed an algorithm for that we call a collaborative hierarchical lasso that uses all the signals to simultaneously find the active sources, but each given sample gets its own internal representation with between the dictionaries."
        ],
        [
            "So we achieve this by minimizing this following functional that in red has a group lasso penalty, an blue has a normal death penalty, and the optimization algorithm alternates between two soft thresholding algorithms.",
            "So the sparsity the reticle sparsity is obtained.",
            "That way we have an extended and extended version in an archive join work with Uni Laura apart from us and we have some theoretical guarantees for the recovery.",
            "So."
        ],
        [
            "To see just some applications here, we're doing this texture separation on the very right.",
            "We have the mixture images out of eight possible brothers textures.",
            "We are here.",
            "We are coding the patches so the collaboration is natural.",
            "All the patches have the same textures, but the algorithm without knowing how many detects the two textures."
        ],
        [
            "Also we did with missing information.",
            "Here we are adding up two different digits #3 and #5.",
            "In this case, the road picked, the red pixels are missing information and all together collaborate to find which are the sources present.",
            "I'm showing also the coefficient matrix and comparing against the law.",
            "So if we do the lawsuit with the big dictionary Black Dot mean non zero coefficients and we can see that our coding detects.",
            "The two sources using simultaneously all the signals."
        ],
        [
            "OK, to summarize the talk I I we present a framework for classification and clustering via dictionary learning.",
            "Using this very simple metric and leads to good results.",
            "Ann based only modeling properties Ann.",
            "We including coherence and shared features.",
            "We extended this to source separation and as I said I want to say it again.",
            "I will be in the poster session.",
            "I'll be happy to."
        ],
        [
            "Answer Questions, Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, well thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I will be presenting the work classification and clustering via dictionary learning with structuring coherence and shared features.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Ignacio Ramirez and Guillermo Sapiro.",
                    "label": 0
                },
                {
                    "sent": "I want to say that I.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will be presenting a poster also with this work in the poster session, so any doubts you have I'll be happy to answer them there.",
                    "label": 0
                },
                {
                    "sent": "OK. Well, the goal of this work is to present a framework based on sparse modeling to address some of the of the problems that are shown here from a modeling point of view.",
                    "label": 0
                },
                {
                    "sent": "For example, object recognition image or texture segmentation and source separation.",
                    "label": 0
                },
                {
                    "sent": "In this talk I will build up.",
                    "label": 0
                },
                {
                    "sent": "I will show how we can build up a more model starting from image just for classification tasks that can be easily generalized to the unsupervised case.",
                    "label": 0
                },
                {
                    "sent": "An hour richer model that allowed us to tackle source separation problems in a natural way.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first I will briefly review sparse modeling.",
                    "label": 0
                },
                {
                    "sent": "There was a very good tutorial at the beginning of the week, so maybe some of you already know a lot about this.",
                    "label": 0
                },
                {
                    "sent": "OK, so the underlying assumption is that.",
                    "label": 0
                },
                {
                    "sent": "A set of data can be well accurately represented as.",
                    "label": 0
                },
                {
                    "sent": "Sparse linear combination of the columns of a dictionary.",
                    "label": 0
                },
                {
                    "sent": "A it has been shown but with many works in the literature that learning those dictionaries performs in generally better than.",
                    "label": 0
                },
                {
                    "sent": "Predefined ones such as PCD, Fourier, wavelets so.",
                    "label": 0
                },
                {
                    "sent": "A success was obtained, in particular by training dictionaries on image patches.",
                    "label": 0
                },
                {
                    "sent": "So here for example, the patches are columns of this data matrix and a dictionary that can be learned.",
                    "label": 0
                },
                {
                    "sent": "For example, this are the columns of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean from geometrical point of view?",
                    "label": 0
                },
                {
                    "sent": "OK, each path.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be thought as a point in a high dimensional space and by.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sparse modeling what we're doing is approximating those points by.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A union of subspaces, low dimensional subspaces, and did I mention here is not fixed.",
                    "label": 0
                },
                {
                    "sent": "An dimension is given by a combination of atoms in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How can we learn those dictionaries?",
                    "label": 0
                },
                {
                    "sent": "A again here I will not go into the details, but what we do is we minimize functional like this where we have a data fitting term and a regularizer.",
                    "label": 0
                },
                {
                    "sent": "We sparsity inducing regularizer.",
                    "label": 0
                },
                {
                    "sent": "Here we use the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "We use as a basis to our code where we constructed the Spanish software that is freely available and very good.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how can we use sparse modeling for classification?",
                    "label": 0
                },
                {
                    "sent": "So the first idea that we can have is very simple.",
                    "label": 0
                },
                {
                    "sent": "So say we are giving C different classes with the corresponding training data.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can learn a dictionary per class and then just think C. Which is the class for which the best representation is obtained.",
                    "label": 0
                },
                {
                    "sent": "This is not a is a simple idea.",
                    "label": 0
                },
                {
                    "sent": "So what we propose.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is to use us to measure how good this representation is.",
                    "label": 0
                },
                {
                    "sent": "We propose to use this function that first is consistent with the way we were learning the dictionaries, and second it accounts for two terms, so they are reconstruction error and also what we call a complexity turn that also looks at the L1 norm of the coefficients.",
                    "label": 0
                },
                {
                    "sent": "If we think many of these dictionaries are over complete, so achieving a small reconstruction error doesn't mean that the dictionary.",
                    "label": 0
                },
                {
                    "sent": "Is is good but.",
                    "label": 0
                },
                {
                    "sent": "If we look at those both things together we then we are kind of looking that.",
                    "label": 0
                },
                {
                    "sent": "Also this representation is sparse.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I will show some results for this very basic procedure that is in the heart of the methods that I will explain later.",
                    "label": 0
                },
                {
                    "sent": "Here we used two basic digit datasets and we're comparing with recent results that also use sparse coding with some reconstructive and discriminative ways, and this algorithm that can be written in three lines using the Spanish software performs competitively well.",
                    "label": 0
                },
                {
                    "sent": "So now we will see how we can.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Improve these until now we were learning the dictionaries independently for each class, so the dictionaries ignored that they were going to be compared with the other ones.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn them together.",
                    "label": 0
                },
                {
                    "sent": "This was the functional that we were using and we propose to add an extra an additional term.",
                    "label": 0
                },
                {
                    "sent": "That we call the crossing coherence term.",
                    "label": 0
                },
                {
                    "sent": "And the the motivation behind this is based on recent work by drop or Elder and coauthors, for example, that show that the block sparse the success of block sparse signals.",
                    "label": 0
                },
                {
                    "sent": "With a L1 regularizer.",
                    "label": 0
                },
                {
                    "sent": "Increases when we have low coherence between the dictionaries.",
                    "label": 0
                },
                {
                    "sent": "So by promoting the dictionaries we are promoting to the dictionaries, the atoms to be incoherent from different dictionaries.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so imposing that the atoms are different thus does not.",
                    "label": 0
                },
                {
                    "sent": "Also allows to have shared features because despite the fact that we tried, addiction is to be different.",
                    "label": 0
                },
                {
                    "sent": "There are some features or items that are important from a reconstructive POV, and those remain after the learning stage.",
                    "label": 0
                },
                {
                    "sent": "Here I'm showing 2 examples of dictionaries of number of class of three spawn 5 and we can see that with the high coherence items capture some shared features.",
                    "label": 0
                },
                {
                    "sent": "These shared features can be easily detected by looking at the coherence.",
                    "label": 0
                },
                {
                    "sent": "Of the dictionary.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Atoms can be if they are shared by all the classes can be ignored while computing this function.",
                    "label": 0
                },
                {
                    "sent": "R for example.",
                    "label": 0
                },
                {
                    "sent": "OK, so also an important fact is that the coherence term does not depend on the training data, only depends on the dictionaries.",
                    "label": 0
                },
                {
                    "sent": "So is it can be easily plugged into an unsupervised case and clustering.",
                    "label": 0
                },
                {
                    "sent": "So let's now move to that.",
                    "label": 0
                },
                {
                    "sent": "How can we extend?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm for clustering.",
                    "label": 0
                },
                {
                    "sent": "So in clustering we're giving a set of data and we know how many class we assume that we know how many classes are.",
                    "label": 0
                },
                {
                    "sent": "Here, so we cannot apply our method straight directly because we don't have any initial partition.",
                    "label": 0
                },
                {
                    "sent": "We could do it if we had so now I will review and initialization step that we're using.",
                    "label": 0
                },
                {
                    "sent": "As I said, I will be in the poster session.",
                    "label": 0
                },
                {
                    "sent": "I can give you all the details there.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we first learn addiction.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I need to represent all the data points.",
                    "label": 0
                },
                {
                    "sent": "This is 1 big global dictionary.",
                    "label": 0
                },
                {
                    "sent": "So now we want to define a similarity between atoms.",
                    "label": 0
                },
                {
                    "sent": "In this dictionary they way we chose is the following.",
                    "label": 0
                },
                {
                    "sent": "We do the sparse coding of all the signals onto this dictionary and we will say the two atoms are similar if they are simultaneously used by a big number of signals.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We get is a graph that has as notes the atoms of the dictionary that can be.",
                    "label": 0
                },
                {
                    "sent": "Cut using, for example, splits spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "This approach is closely related to the L1 graph and subspace clustering.",
                    "label": 1
                },
                {
                    "sent": "Recent works in the literature.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now after we do this cut, we have some initial dictionaries and we could do whether I said for the classification we can apply this and get an initial partition.",
                    "label": 0
                },
                {
                    "sent": "So we the dictionaries learned we partition the data.",
                    "label": 0
                },
                {
                    "sent": "Now in two classes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But this might not be accurate.",
                    "label": 0
                },
                {
                    "sent": "So what we can do?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is iterated in by learning the dictionaries and reassigning the point to the different classes until no change changes or not.",
                    "label": 0
                },
                {
                    "sent": "Substantial changes are observed, so after we.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do this.",
                    "label": 0
                },
                {
                    "sent": "This reminds a lot.",
                    "label": 0
                },
                {
                    "sent": "The K means clustering.",
                    "label": 0
                },
                {
                    "sent": "OK, so here instead of having class centers, what we're having is dictionaries to represent the data, which makes sense because the the data we're handling here is much.",
                    "label": 0
                },
                {
                    "sent": "This creature so this can be seen as an energy minimization problem.",
                    "label": 1
                },
                {
                    "sent": "Here the functional is non convex jointly in the class assignment and in the dictionaries.",
                    "label": 0
                },
                {
                    "sent": "But if we fix the one of them, we get a convex problem.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, if we fix the dictionaries, for example, we get the assignment based on their function R that we were talking about an if we fix the class assignments, we get the dictionary learning algorithm functional that we were.",
                    "label": 0
                },
                {
                    "sent": "Discribe so now I.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will show some.",
                    "label": 0
                },
                {
                    "sent": "Applications to vision.",
                    "label": 0
                },
                {
                    "sent": "First, we did object recognition using the graph data set.",
                    "label": 0
                },
                {
                    "sent": "Here we learn the dictionaries on vectors an we have two classes when background and one bike.",
                    "label": 0
                },
                {
                    "sent": "So I'm showing some precision recall curves in the written version of the paper, we have more comparisons, but we're comparing only with algorithms that use local features.",
                    "label": 0
                },
                {
                    "sent": "Because I mean, we have seen in this conference that adding global an spatial.",
                    "label": 0
                },
                {
                    "sent": "Consistency gives leads to better results.",
                    "label": 0
                },
                {
                    "sent": "This is just using the modeling.",
                    "label": 0
                },
                {
                    "sent": "The next update.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Patient would be texture segmentation.",
                    "label": 1
                },
                {
                    "sent": "Here we are giving an image and we know the number of textures that we have, but we don't know anything else, so we apply our clustering algorithm and we get I'm showing here.",
                    "label": 0
                },
                {
                    "sent": "The dictionary is the final dictionaries obtained.",
                    "label": 0
                },
                {
                    "sent": "We can see that the structure of the texture is clearly there an also the segmentation results after with the iterations.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So until now, what we were assuming is that each signal belongs to one single class.",
                    "label": 0
                },
                {
                    "sent": "What happens if a signal is a mixture of classes here?",
                    "label": 0
                },
                {
                    "sent": "So let's go to this interpretation.",
                    "label": 0
                },
                {
                    "sent": "We can glue the dictionaries, for example for textures and get a bigger dictionary to represent them.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before we have a signal that belong to a dictionary, we test all the possible dictionaries and we select one of them.",
                    "label": 0
                },
                {
                    "sent": "Here we have.",
                    "label": 0
                },
                {
                    "sent": "Only non zero coefficients in the part of the coefficient matrix that corresponds to that part to that sub dictionary.",
                    "label": 0
                },
                {
                    "sent": "So when the next.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Signal we get another one, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what happens now if we get on a combination of textures here, the number is unknown.",
                    "label": 0
                },
                {
                    "sent": "We don't know how many sources are they, so we develop an algorithm that gives a coding that allows us to find the sources and find the structure there.",
                    "label": 0
                },
                {
                    "sent": "Here the.",
                    "label": 0
                },
                {
                    "sent": "We call this calling Erotical because it is.",
                    "label": 0
                },
                {
                    "sent": "It has a sparsity at a group level.",
                    "label": 0
                },
                {
                    "sent": "An inside the groups.",
                    "label": 0
                },
                {
                    "sent": "This is the structure to the coding is has received a lot of attention recently.",
                    "label": 0
                },
                {
                    "sent": "An for example, group lasso is one of these cases, but there are many others.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so when the next following signal we find that we do the coding, we find which signals are present in the mixture.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in many image processing applications or signal or audio processing applications we have spatial or temporal correlation that tells us, like in the talk that we just saw that we can jointly code signals together.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we developed an algorithm for that we call a collaborative hierarchical lasso that uses all the signals to simultaneously find the active sources, but each given sample gets its own internal representation with between the dictionaries.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we achieve this by minimizing this following functional that in red has a group lasso penalty, an blue has a normal death penalty, and the optimization algorithm alternates between two soft thresholding algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the sparsity the reticle sparsity is obtained.",
                    "label": 0
                },
                {
                    "sent": "That way we have an extended and extended version in an archive join work with Uni Laura apart from us and we have some theoretical guarantees for the recovery.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To see just some applications here, we're doing this texture separation on the very right.",
                    "label": 0
                },
                {
                    "sent": "We have the mixture images out of eight possible brothers textures.",
                    "label": 0
                },
                {
                    "sent": "We are here.",
                    "label": 0
                },
                {
                    "sent": "We are coding the patches so the collaboration is natural.",
                    "label": 0
                },
                {
                    "sent": "All the patches have the same textures, but the algorithm without knowing how many detects the two textures.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also we did with missing information.",
                    "label": 1
                },
                {
                    "sent": "Here we are adding up two different digits #3 and #5.",
                    "label": 0
                },
                {
                    "sent": "In this case, the road picked, the red pixels are missing information and all together collaborate to find which are the sources present.",
                    "label": 0
                },
                {
                    "sent": "I'm showing also the coefficient matrix and comparing against the law.",
                    "label": 0
                },
                {
                    "sent": "So if we do the lawsuit with the big dictionary Black Dot mean non zero coefficients and we can see that our coding detects.",
                    "label": 0
                },
                {
                    "sent": "The two sources using simultaneously all the signals.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, to summarize the talk I I we present a framework for classification and clustering via dictionary learning.",
                    "label": 1
                },
                {
                    "sent": "Using this very simple metric and leads to good results.",
                    "label": 0
                },
                {
                    "sent": "Ann based only modeling properties Ann.",
                    "label": 1
                },
                {
                    "sent": "We including coherence and shared features.",
                    "label": 0
                },
                {
                    "sent": "We extended this to source separation and as I said I want to say it again.",
                    "label": 0
                },
                {
                    "sent": "I will be in the poster session.",
                    "label": 0
                },
                {
                    "sent": "I'll be happy to.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answer Questions, Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}