{
    "id": "5tsztkp3fru3yc7x2hgcw5baq2ui7ajw",
    "title": "Optimized Cutting Plane Algorithm for Support Vector Machines",
    "info": {
        "author": [
            "Vojtech Franc, Fraunhofer Institute for Intelligent Analysis and Information Systems"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/icml08_franc_ocp/",
    "segmentation": [
        [
            "So I'm."
        ],
        [
            "Also going to talk about SVM solver for linear classification motivation nowadays more and more applications come with.",
            "Huge amount of data set and at least in some cases it's really needed to be able to process the datasets to get state of the art results.",
            "Examples of such applications can be.",
            "Could be found here.",
            "May I get this pointer?",
            "No point OK, I will just.",
            "So nevertheless.",
            "So we concentrate on linear classification, which has been proved to be powerful tool for data analysis.",
            "However.",
            "Current state of the art solvers are still not able to process only data.",
            "One particular application can be found in bioinformatics where we have so many data.",
            "That cannot be processed and from performance figures it's clear that using more and more data we are still improving performance.",
            "So motivation is to be really able to use all the data.",
            "So the motive?"
        ],
        [
            "Patient is clear.",
            "Well, the goal is to optimize this convex.",
            "Unconstrained problem.",
            "Standard way.",
            "As well, set is to express the problem as constrained quadratic program.",
            "Then one can derive the tool.",
            "The reason for optimizing the dual is at least two fold.",
            "The structure of the constraints is quite simple, so this allows for simple updates.",
            "The previous previous talk actually uses this.",
            "Property second reason is that the dual problem doesn't scale with the number of dimensions, so it's sufficient if we have to deal with high dimensional data.",
            "Our problems.",
            "Conocchia live events to solve.",
            "Problem if he wants to train from large datasets.",
            "One approach was presented.",
            "Another approach is to try to optimize directly the priorities.",
            "This origonal unconstrained formulation.",
            "So this is a."
        ],
        [
            "Exactly the way which we are going to take.",
            "Currently, the state of the art approach, which optimizes the unconstrained formulation is cutting plane algorithm.",
            "I'm going to describe this method first becauses our contribution is based on accelerating this method.",
            "So.",
            "We want to solve master problem.",
            "The objective of the master problem is given by simple quadratic term and risk term.",
            "What makes the optimization difficult?",
            "It's just this risk term because it is some of huge number of non differentiable terms.",
            "So the core idea."
        ],
        [
            "Yeah, of the cutting plane algorithm.",
            "Is to replace the risk term by some simple approximation.",
            "Namely, cutting plane algorithm uses maximum over some collection of linear functions.",
            "This leader functions are called cutting planes.",
            "And.",
            "Basically, each single cutting plane is lower bound on the risk computed at some point.",
            "To compute the cutting plane algorithm, we just need to evaluate subgradient, which for this particular case it is simple."
        ],
        [
            "So the idea can be easily illustrated.",
            "Let this be a derisk term, which is which can be complex funk."
        ],
        [
            "Soon.",
            "So we can compute cutting plan."
        ],
        [
            "At several points then."
        ],
        [
            "If we take maximum over these linear functions, we get this.",
            "Approximation, which is much simpler, can be easily.",
            "Easily."
        ],
        [
            "Optimize and we can basically get the same solution as optimizing the original problem."
        ],
        [
            "So let's look at the algorithm how it works.",
            "So we don't solve this problem directly, but instead we optimize something which is called to reduce problem.",
            "The objective of the radius program is composed of the same regularization term.",
            "Because it's a simple, simple function, but the risk term is replaced by the approximation described on the previous slide.",
            "Now the question is how we will build this approximation.",
            "Cutting plane algorithm uses a simple idea which build this approximation iteratively.",
            "At each iteration, we add a single cutting plane to the approximation, so it becomes better and better until we converge.",
            "So more precisely, the algorithm starts from no cutting plane.",
            "Anne.",
            "And then it iterates 2 steps in the first step it solves reduced problem.",
            "Then we get some approximated solution and the new cutting plane is computed exactly at the solution of the reduce problem.",
            "It's added to the approximation at this repeated nice property is that each iteration we have upper bound and lower bound on the optimum, so we have reasonable stopping condition and we have also convergence guarantees.",
            "For any non zero epsilon, this algorithm will converge in finite number of iterations."
        ],
        [
            "So it's a simple method.",
            "It works well better than other approaches, but nevertheless, if we have lots of data, it will.",
            "It still can be inefficient.",
            "And the source of inefficiency in this method.",
            "Is the following.",
            "First of all, the cutting plane algorithm basically optimizes only the reduce problem, while the master objective can fluctuate.",
            "Even though it's a guarantee that at the end we have to reach the optimum, but it can take some time.",
            "And the second reason why it can be inefficient is the simple strategy for selection of the cutting planes.",
            "In this algorithm, the new cutting plane is computed and always at the point of the reduced at the point which is a solution of the reduce problem.",
            "The problem is that this solution can be actually far from the optimum, and in fact it can be further from the optimum than some previous solutions.",
            "The result is that a lot of cutting planes are actually useless.",
            "In this sense, then, they don't contribute to the approximation of the master objective, so which makes the optimization slower?"
        ],
        [
            "What we propose.",
            "Is the modification to the cutting plane algorithm?",
            "First, we propose to introduce an additional step.",
            "A very optimized directly, the primer the master object if using a line search.",
            "And the second modification is different strategy for selecting the cutting planes.",
            "Namely, we always select the new cutting plane in a vicinity of the best so far solution.",
            "So we have higher chance that this cutting planes are approximate.",
            "Master objective close to the optimum."
        ],
        [
            "So let's look at the algorithm how it really works.",
            "What is interesting is basically the modification to the original algorithm.",
            "So first of all, we maintain the best so far solution.",
            "Then we again iterate.",
            "The first step is the same.",
            "We solve the reduced problem.",
            "We get some solution.",
            "And what we do we use this solution as a direction direction for line search.",
            "So we have some old best so far solution we have this WT which is a direction and we optimize the master objective function which usually gives improves the solution even though there is no guarantee that it is a decent direction.",
            "But in most cases it happens.",
            "And while when we have this new best so far solution, we go to the to the next step and we use.",
            "Different strategy for selecting the cutting plane.",
            "So we compute appoint.",
            "WD card which is on the line segment between the best so far solution and the.",
            "Solution of the reduce problem.",
            "How far it is from this best so far solution?",
            "It depends on this parameter.",
            "Fortunately, we found experimentally that setting this parameter to 0.1 works consistently well for all experiments.",
            "But may I ask question Vivy cannot compute the cutting plane directly at the best so far solution.",
            "The problem is that if we do this, we lose the guarantee of convergence.",
            "It can simply get stuck.",
            "So this is the modification.",
            "If we look at this step, it's clear that it doesn't introduce any expensive operation.",
            "But the problem may be the line search, because basically we optimize the original objective.",
            "So the question is if we can optimize this launch line search efficiently."
        ],
        [
            "The answer is yes, at least for this particular SVM optimization problem.",
            "So what we want to do is we have the master objective.",
            "We have these two vectors and we optimize only with respect to a single variable, so.",
            "We have again some convex function which is not differentiable.",
            "Basically, these parameters can be precomputed from W vectors and then we need to optimize this."
        ],
        [
            "Very efficient solution, basically comes directly from the optimality condition of of.",
            "For for the minimum of this problem, which says that the minimum is attained at the point at the Subdifferential contains 0.",
            "So if we write down explicit formulas for subdifferential of this function, we get something like this.",
            "She may be."
        ],
        [
            "Good, but if we draw a graph of this sub differential, we found out that it is.",
            "That is this kind of stair like function, so finding the optimum basically amounts to finding an intersection of this graph and zero XX.",
            "So this can be computed quite efficiently because we know this point.",
            "So we need only to sort them and then we go along individual line segments until we reach.",
            "This intersection.",
            "So basically, the computational effort is given by sorting the points.",
            "So if use, let's say, quicksort, then we need unlock and time to do this."
        ],
        [
            "So that's it.",
            "Now some experiments.",
            "We have compared the methods against current state of the art solvers.",
            "We found that the basically fair comparison is not that simple because currently there is no clear consensus on what is the best stopping condition.",
            "One approach is to see the learning problem as optimization, where we basically stop the optimization when we get close to the optimal point.",
            "So one can use this duality gap stopping condition or or this epsilon relaxed KKT conditions comparison to such kind of solvers is simple because basically we can use the same stopping conditions.",
            "This category, which we call a current servers, belong the SVM light which optimizes the dual.",
            "SCM Power and bundle methods.",
            "BM algorithm, which are in fact.",
            "Implementations of the cutting plane algorithm.",
            "Second, another viewpoint is.",
            "Then basically what we need is a classifier with good performance so we don't have to care about optimization at all.",
            "This is definitely argument.",
            "In place, however, still there is a question how to stop such algorithms?",
            "Because we need some constructive stopping condition.",
            "So.",
            "The methods which are actually used this approach are Pegasus and SGD, where the stopping condition is some fixed number of iterations, and in this paper.",
            "There are some suggestions how to set up this fixed number of iterations which we followed in the experiments."
        ],
        [
            "So data set.",
            "We tested the algorithms on our various kinds of data.",
            "The number of examples ranges from 70,000 to 15 millions data points.",
            "We had a low dimensional problems as well as high dimensional problems.",
            "Dense problems, sparse problems with past features."
        ],
        [
            "So first experiment.",
            "Here we measured performance versus the runtime.",
            "The performance be measured in terms of area under the receiver operator Christic characteristic curve.",
            "And the time was the total time needed for model selection because we don't know the C constant, which means that we have to train for some range of C constants and then based on validations that we pick up the best solution.",
            "So the table basically shows only the performance.",
            "The first line shows performance for the accurate servers, so we can see that basically they.",
            "Give the same same results becausw they have comparable stopping conditions.",
            "If you look at the approximative solvers like Pegasus and SGD.",
            "Indeed, in most cases the results are incomparable or slight levels up to the last data set where it gives much worse results than it.",
            "Then that could be achieved if we optimize to the full full precision."
        ],
        [
            "Now if you look at the time, so this graph presents the speedup factor computed with respect to the algorithm which we propose.",
            "So.",
            "Each single bar corresponds to one of the competing algorithms and group of bar corresponds to individual datasets.",
            "Everything which is above one is slower than our method.",
            "By this amount of.",
            "Time and everything which is below is faster.",
            "So first observation, we can see that every compared to SVM line, which is this green bar.",
            "Then we we are the speedup compared to our method is up to three orders of magnitude.",
            "If we compare against these two algorithms, which are implementations of the cutting plane method.",
            "Then the speedup is about 2 blue bars.",
            "The speedup is about factor.",
            "Of 10 some methods had problems with convergence, but the reason is probably that we used early version of the software which had some problem.",
            "If you look at the approximative solvers, then the Pegasus was at the same level as the cutting plane algorithm.",
            "Up to the last data set, but it didn't work.",
            "But if you look at the SGD algorithm, it was always faster, so.",
            "Next question was whether SGD is.",
            "So fast because it is very good optimizer or whether the stopping condition is very loose, so therefore we."
        ],
        [
            "Another experiment where we measure objective value, which is this excess a set as a function of runtime.",
            "So this experiment we did only for best three solvers from the previous experiment, which means the cutting plane algorithm which is in red.",
            "The SGD stochastic gradient.",
            "These sandwiches greed and.",
            "Our matter the Oca's.",
            "So what we can learn from the figures?",
            "The cutting plane algorithm is always slower than the remaining two methods.",
            "Second thing.",
            "So the SGD at the beginning of the optimization is usually fast.",
            "But it has problems too to achieve the accurate solution.",
            "But surprisingly, if we look at our method, it's not only faster if he wants to achieve accurate solutions, but also in most cases it is faster.",
            "Also in early stages of the optimization.",
            "So if we use the same stopping condition as GD, then we can basically get the same the same result.",
            "This dashed line basically also shows the point where the SGD algorithm stops.",
            "So.",
            "You can see that it is really.",
            "Uh.",
            "Inacurate point, but.",
            "As I said, in most cases it is sufficient to get good classification performance."
        ],
        [
            "The last experiment, nice thing about this method is that we can basically parallelize all the core parts of the method.",
            "So we did this.",
            "To show the effect of paralyzing, we trained on a 15 millions human splice.",
            "They just said it's all sorted 2 gigabytes in size.",
            "We used 16 core.",
            "Computer.",
            "What you can see on this on the figure, each bar corresponds to different number of CPUs ranging from 1 to 16.",
            "And.",
            "The height of the body is the runtime, the individual this path corresponds to the sub procedures of focus.",
            "This Brown one is the computation of.",
            "Output the green one is computation of cutting planes and the blue one is the line search which we introduced so you can see that we can achieve speedup 2 factor model."
        ],
        [
            "Four, the reason for the separation is that.",
            "The memory load created when we use four CPUs or 8 CPUs.",
            "Is huge, it is about 28 gigabytes per second."
        ],
        [
            "So conclusions we have introduced a new solver which outperforms state of the art method.",
            "Surprisingly, it is fast even in early stages of demonization which was previously domain.",
            "Of these approximative methods.",
            "It's going to be efficiently paralyzed.",
            "So.",
            "Some outlook to the future.",
            "We would like to.",
            "Extended to further problem because it's quite general method, we can use it for multiclasses VMS, logistic regression, basically whatever which is not going to be expresses convex optimization.",
            "We would also like to extend it to learning with kernels even though we don't know how we had.",
            "So we implemented everything so you can download code either from this link or from Shogun Toolbox.",
            "So last but not least, we organize a workshop tomorrow, so those who are interested in large scale learning can come.",
            "He started it's 30.",
            "We have time for some questions.",
            "Crucial part of your algorithm is line service.",
            "So how do you plan to do the line search for logistic regression?",
            "Basically, for logistic regression we will have to use some iterative method, but it shouldn't be that difficult cause if you have never ate problem then it's much easier than to solve the food problem with all variables.",
            "Yeah, of course we cannot solve it explicitly like here, but still there is a chance that optimizing with respect to a single variable will be much more efficient than.",
            "Optimizing all variables so it can lead to some speedup.",
            "Anymore questions.",
            "Great speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also going to talk about SVM solver for linear classification motivation nowadays more and more applications come with.",
                    "label": 1
                },
                {
                    "sent": "Huge amount of data set and at least in some cases it's really needed to be able to process the datasets to get state of the art results.",
                    "label": 0
                },
                {
                    "sent": "Examples of such applications can be.",
                    "label": 0
                },
                {
                    "sent": "Could be found here.",
                    "label": 0
                },
                {
                    "sent": "May I get this pointer?",
                    "label": 0
                },
                {
                    "sent": "No point OK, I will just.",
                    "label": 0
                },
                {
                    "sent": "So nevertheless.",
                    "label": 0
                },
                {
                    "sent": "So we concentrate on linear classification, which has been proved to be powerful tool for data analysis.",
                    "label": 0
                },
                {
                    "sent": "However.",
                    "label": 0
                },
                {
                    "sent": "Current state of the art solvers are still not able to process only data.",
                    "label": 0
                },
                {
                    "sent": "One particular application can be found in bioinformatics where we have so many data.",
                    "label": 0
                },
                {
                    "sent": "That cannot be processed and from performance figures it's clear that using more and more data we are still improving performance.",
                    "label": 0
                },
                {
                    "sent": "So motivation is to be really able to use all the data.",
                    "label": 0
                },
                {
                    "sent": "So the motive?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Patient is clear.",
                    "label": 0
                },
                {
                    "sent": "Well, the goal is to optimize this convex.",
                    "label": 0
                },
                {
                    "sent": "Unconstrained problem.",
                    "label": 0
                },
                {
                    "sent": "Standard way.",
                    "label": 0
                },
                {
                    "sent": "As well, set is to express the problem as constrained quadratic program.",
                    "label": 1
                },
                {
                    "sent": "Then one can derive the tool.",
                    "label": 0
                },
                {
                    "sent": "The reason for optimizing the dual is at least two fold.",
                    "label": 1
                },
                {
                    "sent": "The structure of the constraints is quite simple, so this allows for simple updates.",
                    "label": 0
                },
                {
                    "sent": "The previous previous talk actually uses this.",
                    "label": 0
                },
                {
                    "sent": "Property second reason is that the dual problem doesn't scale with the number of dimensions, so it's sufficient if we have to deal with high dimensional data.",
                    "label": 1
                },
                {
                    "sent": "Our problems.",
                    "label": 0
                },
                {
                    "sent": "Conocchia live events to solve.",
                    "label": 0
                },
                {
                    "sent": "Problem if he wants to train from large datasets.",
                    "label": 0
                },
                {
                    "sent": "One approach was presented.",
                    "label": 0
                },
                {
                    "sent": "Another approach is to try to optimize directly the priorities.",
                    "label": 0
                },
                {
                    "sent": "This origonal unconstrained formulation.",
                    "label": 0
                },
                {
                    "sent": "So this is a.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exactly the way which we are going to take.",
                    "label": 0
                },
                {
                    "sent": "Currently, the state of the art approach, which optimizes the unconstrained formulation is cutting plane algorithm.",
                    "label": 1
                },
                {
                    "sent": "I'm going to describe this method first becauses our contribution is based on accelerating this method.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We want to solve master problem.",
                    "label": 0
                },
                {
                    "sent": "The objective of the master problem is given by simple quadratic term and risk term.",
                    "label": 1
                },
                {
                    "sent": "What makes the optimization difficult?",
                    "label": 0
                },
                {
                    "sent": "It's just this risk term because it is some of huge number of non differentiable terms.",
                    "label": 0
                },
                {
                    "sent": "So the core idea.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, of the cutting plane algorithm.",
                    "label": 1
                },
                {
                    "sent": "Is to replace the risk term by some simple approximation.",
                    "label": 0
                },
                {
                    "sent": "Namely, cutting plane algorithm uses maximum over some collection of linear functions.",
                    "label": 0
                },
                {
                    "sent": "This leader functions are called cutting planes.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Basically, each single cutting plane is lower bound on the risk computed at some point.",
                    "label": 0
                },
                {
                    "sent": "To compute the cutting plane algorithm, we just need to evaluate subgradient, which for this particular case it is simple.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea can be easily illustrated.",
                    "label": 0
                },
                {
                    "sent": "Let this be a derisk term, which is which can be complex funk.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Soon.",
                    "label": 0
                },
                {
                    "sent": "So we can compute cutting plan.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At several points then.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we take maximum over these linear functions, we get this.",
                    "label": 0
                },
                {
                    "sent": "Approximation, which is much simpler, can be easily.",
                    "label": 0
                },
                {
                    "sent": "Easily.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimize and we can basically get the same solution as optimizing the original problem.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at the algorithm how it works.",
                    "label": 0
                },
                {
                    "sent": "So we don't solve this problem directly, but instead we optimize something which is called to reduce problem.",
                    "label": 0
                },
                {
                    "sent": "The objective of the radius program is composed of the same regularization term.",
                    "label": 0
                },
                {
                    "sent": "Because it's a simple, simple function, but the risk term is replaced by the approximation described on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "Now the question is how we will build this approximation.",
                    "label": 0
                },
                {
                    "sent": "Cutting plane algorithm uses a simple idea which build this approximation iteratively.",
                    "label": 1
                },
                {
                    "sent": "At each iteration, we add a single cutting plane to the approximation, so it becomes better and better until we converge.",
                    "label": 0
                },
                {
                    "sent": "So more precisely, the algorithm starts from no cutting plane.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And then it iterates 2 steps in the first step it solves reduced problem.",
                    "label": 0
                },
                {
                    "sent": "Then we get some approximated solution and the new cutting plane is computed exactly at the solution of the reduce problem.",
                    "label": 1
                },
                {
                    "sent": "It's added to the approximation at this repeated nice property is that each iteration we have upper bound and lower bound on the optimum, so we have reasonable stopping condition and we have also convergence guarantees.",
                    "label": 0
                },
                {
                    "sent": "For any non zero epsilon, this algorithm will converge in finite number of iterations.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's a simple method.",
                    "label": 0
                },
                {
                    "sent": "It works well better than other approaches, but nevertheless, if we have lots of data, it will.",
                    "label": 0
                },
                {
                    "sent": "It still can be inefficient.",
                    "label": 1
                },
                {
                    "sent": "And the source of inefficiency in this method.",
                    "label": 1
                },
                {
                    "sent": "Is the following.",
                    "label": 1
                },
                {
                    "sent": "First of all, the cutting plane algorithm basically optimizes only the reduce problem, while the master objective can fluctuate.",
                    "label": 0
                },
                {
                    "sent": "Even though it's a guarantee that at the end we have to reach the optimum, but it can take some time.",
                    "label": 0
                },
                {
                    "sent": "And the second reason why it can be inefficient is the simple strategy for selection of the cutting planes.",
                    "label": 0
                },
                {
                    "sent": "In this algorithm, the new cutting plane is computed and always at the point of the reduced at the point which is a solution of the reduce problem.",
                    "label": 1
                },
                {
                    "sent": "The problem is that this solution can be actually far from the optimum, and in fact it can be further from the optimum than some previous solutions.",
                    "label": 1
                },
                {
                    "sent": "The result is that a lot of cutting planes are actually useless.",
                    "label": 1
                },
                {
                    "sent": "In this sense, then, they don't contribute to the approximation of the master objective, so which makes the optimization slower?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we propose.",
                    "label": 0
                },
                {
                    "sent": "Is the modification to the cutting plane algorithm?",
                    "label": 1
                },
                {
                    "sent": "First, we propose to introduce an additional step.",
                    "label": 1
                },
                {
                    "sent": "A very optimized directly, the primer the master object if using a line search.",
                    "label": 0
                },
                {
                    "sent": "And the second modification is different strategy for selecting the cutting planes.",
                    "label": 0
                },
                {
                    "sent": "Namely, we always select the new cutting plane in a vicinity of the best so far solution.",
                    "label": 1
                },
                {
                    "sent": "So we have higher chance that this cutting planes are approximate.",
                    "label": 1
                },
                {
                    "sent": "Master objective close to the optimum.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at the algorithm how it really works.",
                    "label": 0
                },
                {
                    "sent": "What is interesting is basically the modification to the original algorithm.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we maintain the best so far solution.",
                    "label": 0
                },
                {
                    "sent": "Then we again iterate.",
                    "label": 0
                },
                {
                    "sent": "The first step is the same.",
                    "label": 0
                },
                {
                    "sent": "We solve the reduced problem.",
                    "label": 1
                },
                {
                    "sent": "We get some solution.",
                    "label": 0
                },
                {
                    "sent": "And what we do we use this solution as a direction direction for line search.",
                    "label": 0
                },
                {
                    "sent": "So we have some old best so far solution we have this WT which is a direction and we optimize the master objective function which usually gives improves the solution even though there is no guarantee that it is a decent direction.",
                    "label": 0
                },
                {
                    "sent": "But in most cases it happens.",
                    "label": 0
                },
                {
                    "sent": "And while when we have this new best so far solution, we go to the to the next step and we use.",
                    "label": 1
                },
                {
                    "sent": "Different strategy for selecting the cutting plane.",
                    "label": 0
                },
                {
                    "sent": "So we compute appoint.",
                    "label": 0
                },
                {
                    "sent": "WD card which is on the line segment between the best so far solution and the.",
                    "label": 0
                },
                {
                    "sent": "Solution of the reduce problem.",
                    "label": 0
                },
                {
                    "sent": "How far it is from this best so far solution?",
                    "label": 0
                },
                {
                    "sent": "It depends on this parameter.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, we found experimentally that setting this parameter to 0.1 works consistently well for all experiments.",
                    "label": 0
                },
                {
                    "sent": "But may I ask question Vivy cannot compute the cutting plane directly at the best so far solution.",
                    "label": 1
                },
                {
                    "sent": "The problem is that if we do this, we lose the guarantee of convergence.",
                    "label": 0
                },
                {
                    "sent": "It can simply get stuck.",
                    "label": 0
                },
                {
                    "sent": "So this is the modification.",
                    "label": 0
                },
                {
                    "sent": "If we look at this step, it's clear that it doesn't introduce any expensive operation.",
                    "label": 0
                },
                {
                    "sent": "But the problem may be the line search, because basically we optimize the original objective.",
                    "label": 0
                },
                {
                    "sent": "So the question is if we can optimize this launch line search efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The answer is yes, at least for this particular SVM optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is we have the master objective.",
                    "label": 0
                },
                {
                    "sent": "We have these two vectors and we optimize only with respect to a single variable, so.",
                    "label": 0
                },
                {
                    "sent": "We have again some convex function which is not differentiable.",
                    "label": 0
                },
                {
                    "sent": "Basically, these parameters can be precomputed from W vectors and then we need to optimize this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very efficient solution, basically comes directly from the optimality condition of of.",
                    "label": 0
                },
                {
                    "sent": "For for the minimum of this problem, which says that the minimum is attained at the point at the Subdifferential contains 0.",
                    "label": 0
                },
                {
                    "sent": "So if we write down explicit formulas for subdifferential of this function, we get something like this.",
                    "label": 0
                },
                {
                    "sent": "She may be.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, but if we draw a graph of this sub differential, we found out that it is.",
                    "label": 0
                },
                {
                    "sent": "That is this kind of stair like function, so finding the optimum basically amounts to finding an intersection of this graph and zero XX.",
                    "label": 0
                },
                {
                    "sent": "So this can be computed quite efficiently because we know this point.",
                    "label": 0
                },
                {
                    "sent": "So we need only to sort them and then we go along individual line segments until we reach.",
                    "label": 0
                },
                {
                    "sent": "This intersection.",
                    "label": 0
                },
                {
                    "sent": "So basically, the computational effort is given by sorting the points.",
                    "label": 0
                },
                {
                    "sent": "So if use, let's say, quicksort, then we need unlock and time to do this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's it.",
                    "label": 0
                },
                {
                    "sent": "Now some experiments.",
                    "label": 0
                },
                {
                    "sent": "We have compared the methods against current state of the art solvers.",
                    "label": 0
                },
                {
                    "sent": "We found that the basically fair comparison is not that simple because currently there is no clear consensus on what is the best stopping condition.",
                    "label": 0
                },
                {
                    "sent": "One approach is to see the learning problem as optimization, where we basically stop the optimization when we get close to the optimal point.",
                    "label": 0
                },
                {
                    "sent": "So one can use this duality gap stopping condition or or this epsilon relaxed KKT conditions comparison to such kind of solvers is simple because basically we can use the same stopping conditions.",
                    "label": 0
                },
                {
                    "sent": "This category, which we call a current servers, belong the SVM light which optimizes the dual.",
                    "label": 0
                },
                {
                    "sent": "SCM Power and bundle methods.",
                    "label": 0
                },
                {
                    "sent": "BM algorithm, which are in fact.",
                    "label": 0
                },
                {
                    "sent": "Implementations of the cutting plane algorithm.",
                    "label": 1
                },
                {
                    "sent": "Second, another viewpoint is.",
                    "label": 0
                },
                {
                    "sent": "Then basically what we need is a classifier with good performance so we don't have to care about optimization at all.",
                    "label": 0
                },
                {
                    "sent": "This is definitely argument.",
                    "label": 0
                },
                {
                    "sent": "In place, however, still there is a question how to stop such algorithms?",
                    "label": 0
                },
                {
                    "sent": "Because we need some constructive stopping condition.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The methods which are actually used this approach are Pegasus and SGD, where the stopping condition is some fixed number of iterations, and in this paper.",
                    "label": 0
                },
                {
                    "sent": "There are some suggestions how to set up this fixed number of iterations which we followed in the experiments.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So data set.",
                    "label": 0
                },
                {
                    "sent": "We tested the algorithms on our various kinds of data.",
                    "label": 0
                },
                {
                    "sent": "The number of examples ranges from 70,000 to 15 millions data points.",
                    "label": 0
                },
                {
                    "sent": "We had a low dimensional problems as well as high dimensional problems.",
                    "label": 0
                },
                {
                    "sent": "Dense problems, sparse problems with past features.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first experiment.",
                    "label": 0
                },
                {
                    "sent": "Here we measured performance versus the runtime.",
                    "label": 1
                },
                {
                    "sent": "The performance be measured in terms of area under the receiver operator Christic characteristic curve.",
                    "label": 1
                },
                {
                    "sent": "And the time was the total time needed for model selection because we don't know the C constant, which means that we have to train for some range of C constants and then based on validations that we pick up the best solution.",
                    "label": 1
                },
                {
                    "sent": "So the table basically shows only the performance.",
                    "label": 1
                },
                {
                    "sent": "The first line shows performance for the accurate servers, so we can see that basically they.",
                    "label": 0
                },
                {
                    "sent": "Give the same same results becausw they have comparable stopping conditions.",
                    "label": 0
                },
                {
                    "sent": "If you look at the approximative solvers like Pegasus and SGD.",
                    "label": 0
                },
                {
                    "sent": "Indeed, in most cases the results are incomparable or slight levels up to the last data set where it gives much worse results than it.",
                    "label": 0
                },
                {
                    "sent": "Then that could be achieved if we optimize to the full full precision.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now if you look at the time, so this graph presents the speedup factor computed with respect to the algorithm which we propose.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Each single bar corresponds to one of the competing algorithms and group of bar corresponds to individual datasets.",
                    "label": 0
                },
                {
                    "sent": "Everything which is above one is slower than our method.",
                    "label": 0
                },
                {
                    "sent": "By this amount of.",
                    "label": 0
                },
                {
                    "sent": "Time and everything which is below is faster.",
                    "label": 0
                },
                {
                    "sent": "So first observation, we can see that every compared to SVM line, which is this green bar.",
                    "label": 0
                },
                {
                    "sent": "Then we we are the speedup compared to our method is up to three orders of magnitude.",
                    "label": 0
                },
                {
                    "sent": "If we compare against these two algorithms, which are implementations of the cutting plane method.",
                    "label": 0
                },
                {
                    "sent": "Then the speedup is about 2 blue bars.",
                    "label": 0
                },
                {
                    "sent": "The speedup is about factor.",
                    "label": 0
                },
                {
                    "sent": "Of 10 some methods had problems with convergence, but the reason is probably that we used early version of the software which had some problem.",
                    "label": 0
                },
                {
                    "sent": "If you look at the approximative solvers, then the Pegasus was at the same level as the cutting plane algorithm.",
                    "label": 0
                },
                {
                    "sent": "Up to the last data set, but it didn't work.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the SGD algorithm, it was always faster, so.",
                    "label": 0
                },
                {
                    "sent": "Next question was whether SGD is.",
                    "label": 0
                },
                {
                    "sent": "So fast because it is very good optimizer or whether the stopping condition is very loose, so therefore we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another experiment where we measure objective value, which is this excess a set as a function of runtime.",
                    "label": 0
                },
                {
                    "sent": "So this experiment we did only for best three solvers from the previous experiment, which means the cutting plane algorithm which is in red.",
                    "label": 0
                },
                {
                    "sent": "The SGD stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "These sandwiches greed and.",
                    "label": 0
                },
                {
                    "sent": "Our matter the Oca's.",
                    "label": 0
                },
                {
                    "sent": "So what we can learn from the figures?",
                    "label": 0
                },
                {
                    "sent": "The cutting plane algorithm is always slower than the remaining two methods.",
                    "label": 0
                },
                {
                    "sent": "Second thing.",
                    "label": 0
                },
                {
                    "sent": "So the SGD at the beginning of the optimization is usually fast.",
                    "label": 0
                },
                {
                    "sent": "But it has problems too to achieve the accurate solution.",
                    "label": 0
                },
                {
                    "sent": "But surprisingly, if we look at our method, it's not only faster if he wants to achieve accurate solutions, but also in most cases it is faster.",
                    "label": 0
                },
                {
                    "sent": "Also in early stages of the optimization.",
                    "label": 0
                },
                {
                    "sent": "So if we use the same stopping condition as GD, then we can basically get the same the same result.",
                    "label": 0
                },
                {
                    "sent": "This dashed line basically also shows the point where the SGD algorithm stops.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You can see that it is really.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Inacurate point, but.",
                    "label": 0
                },
                {
                    "sent": "As I said, in most cases it is sufficient to get good classification performance.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last experiment, nice thing about this method is that we can basically parallelize all the core parts of the method.",
                    "label": 0
                },
                {
                    "sent": "So we did this.",
                    "label": 0
                },
                {
                    "sent": "To show the effect of paralyzing, we trained on a 15 millions human splice.",
                    "label": 0
                },
                {
                    "sent": "They just said it's all sorted 2 gigabytes in size.",
                    "label": 0
                },
                {
                    "sent": "We used 16 core.",
                    "label": 0
                },
                {
                    "sent": "Computer.",
                    "label": 0
                },
                {
                    "sent": "What you can see on this on the figure, each bar corresponds to different number of CPUs ranging from 1 to 16.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The height of the body is the runtime, the individual this path corresponds to the sub procedures of focus.",
                    "label": 0
                },
                {
                    "sent": "This Brown one is the computation of.",
                    "label": 0
                },
                {
                    "sent": "Output the green one is computation of cutting planes and the blue one is the line search which we introduced so you can see that we can achieve speedup 2 factor model.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Four, the reason for the separation is that.",
                    "label": 0
                },
                {
                    "sent": "The memory load created when we use four CPUs or 8 CPUs.",
                    "label": 1
                },
                {
                    "sent": "Is huge, it is about 28 gigabytes per second.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conclusions we have introduced a new solver which outperforms state of the art method.",
                    "label": 1
                },
                {
                    "sent": "Surprisingly, it is fast even in early stages of demonization which was previously domain.",
                    "label": 1
                },
                {
                    "sent": "Of these approximative methods.",
                    "label": 0
                },
                {
                    "sent": "It's going to be efficiently paralyzed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Some outlook to the future.",
                    "label": 0
                },
                {
                    "sent": "We would like to.",
                    "label": 1
                },
                {
                    "sent": "Extended to further problem because it's quite general method, we can use it for multiclasses VMS, logistic regression, basically whatever which is not going to be expresses convex optimization.",
                    "label": 0
                },
                {
                    "sent": "We would also like to extend it to learning with kernels even though we don't know how we had.",
                    "label": 0
                },
                {
                    "sent": "So we implemented everything so you can download code either from this link or from Shogun Toolbox.",
                    "label": 0
                },
                {
                    "sent": "So last but not least, we organize a workshop tomorrow, so those who are interested in large scale learning can come.",
                    "label": 0
                },
                {
                    "sent": "He started it's 30.",
                    "label": 0
                },
                {
                    "sent": "We have time for some questions.",
                    "label": 0
                },
                {
                    "sent": "Crucial part of your algorithm is line service.",
                    "label": 0
                },
                {
                    "sent": "So how do you plan to do the line search for logistic regression?",
                    "label": 0
                },
                {
                    "sent": "Basically, for logistic regression we will have to use some iterative method, but it shouldn't be that difficult cause if you have never ate problem then it's much easier than to solve the food problem with all variables.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course we cannot solve it explicitly like here, but still there is a chance that optimizing with respect to a single variable will be much more efficient than.",
                    "label": 0
                },
                {
                    "sent": "Optimizing all variables so it can lead to some speedup.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Great speaker again.",
                    "label": 0
                }
            ]
        }
    }
}