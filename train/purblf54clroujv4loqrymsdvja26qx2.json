{
    "id": "purblf54clroujv4loqrymsdvja26qx2",
    "title": "Online Learning",
    "info": {
        "author": [
            "Roberto Paredes, Technical University of Valencia (UPV)"
        ],
        "published": "Aug. 5, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/bootcamp2010_paredes_ol/",
    "segmentation": [
        [
            "So today I hope that we learn a lot from this topic is online learning."
        ],
        [
            "So.",
            "The index of my lecture, so we first start with some introduction and some brief notation of what we're going to suppress mathematical expression that we're going to use.",
            "We learn from, I think, one of the first online learning algorithms is the perceptron will have some basic notation some.",
            "Proofs of some bounds of error.",
            "And also we will move from perception to more elaborate techniques.",
            "After that briefly, I will explain how to extend the linear perception to kernel perception.",
            "It's quite straightforward.",
            "And after that I will focus.",
            "On very new algorithm is called passive aggressive online learning.",
            "We will see passive aggressively only for linear models.",
            "For kernel models, for regression for multiclass, so we'll see different application of the same passive aggressive mythology.",
            "And finally, we learn a little bit about or for learning on a budget becausw when we introduce the kernel methods into the perception or.",
            "Passive aggressive, etc.",
            "The problem we have is that the number of support vectors used to increase.",
            "We thought any kind of constraint and then we will learn how to put the constraint, how to put the budget on the number of support vectors that we are going to use in our online learning at the end, if we have time, we will want to show you some applications that we have.",
            "We have applied online learning, for instance for video tagging and also for relevance feedback on image retrieval."
        ],
        [
            "OK. And what this online learning?",
            "Solon learning, at least for me, is procedure algorithm for obtaining a machine learning model, where we use a unique sample at each iteration.",
            "Maybe we have more samples.",
            "Maybe we have some historic samples, but we're going to use only one sample.",
            "Moreover, maybe maybe the distribution later is unknown, so we have never seen before data from this problem and maybe also this data changed over the time, so online learning is going to help us to solve these problems.",
            "And where we have online learning problems for either, we can have we have or any problems when we want to forecast something, some financial series.",
            "And then maybe we have to the data is arrive and an as the data center in we have to decide the forecast forecast of this of this data.",
            "But sure, if we want to, for instance, to forecasts on financial series, we have some historic.",
            "So we have data, so why not to do some batch procedure of all the data that we have in set of process of process?",
            "Only one sample in each iteration.",
            "So pure PRN learning is difficult to find 'cause we always have some data that we have.",
            "We can do in a batch.",
            "So which is the motivation then?"
        ],
        [
            "Off online running, the motivation is in one part is to have a very efficient algorithm that only needs to process only one data at each iteration is not going to process all the data in a batch manner.",
            "And also, we're going to be able to deal with the problem with the shifting problem, where the data change over the time.",
            "We will see this later.",
            "And then the question that we want to answer is that could we get good models for processing an unique sample at each iteration?",
            "So doing this process only one sample at each iteration, an never cycle over the samples?",
            "We are going.",
            "Are we going to be able to get good models with a question that in part we going to solve today?"
        ],
        [
            "So is suppose that the students should know.",
            "So the best basic machine learning concepts.",
            "What linear models means.",
            "But I am going to explain here again and about kernel methods.",
            "Of course an.",
            "The notation that I went to uses the following, so the samples are vectors in additional space.",
            "The wave vector of the linear models is a vector in the semi space.",
            "For binary classification problems, we have the class label Y is minus 1 + 1 for multiclass problems, Y is between one and capital N being capital M. The number of classes.",
            "The loss function L that we're going to use.",
            "Usually we're going to use the hinge loss function loss function, and the kernels is going to be a function between two vectors.",
            "And also the support vectors are going to be represented as a set of indexes capital X, so we will see this later.",
            "Let me check the.",
            "Maybe some properties of the display will cause?"
        ],
        [
            "Well, linear models.",
            "As you know, linear model is Alisa model that is linear.",
            "So given vector X, we compute the inner product between the model.",
            "The model is just.",
            "This very compact is safe in a way back to another due an.",
            "We Samsung offset and then the class label is design of this operation.",
            "So usually we use a more compact notation combination where this person is just this W in a product with X where W is the vector.",
            "Previous vector plus the offset an X has some constant one in the first component.",
            "In order to explain this option here.",
            "And usually these the dimensional space that we're going to deal with.",
            "So this is a linear model, is a classification is performed computing the design of this inner."
        ],
        [
            "Anne, this graphical representation has useful of a linear model.",
            "This vector is W and this part in the direction of the vector is going to get positive values of this inner product, and this path is going to get negative values.",
            "This inner product and this rail line is the points where this inner product is 0.",
            "So is the boundary.",
            "The class boundaries."
        ],
        [
            "Well, the perceptron is arguing that given a set of data X1 class label X2 Plus label up to X Capital T Plus label.",
            "Is going to classify this data using Wavevector other year.",
            "The classifier is design.",
            "This is Amanda.",
            "Re is where the points are zero and we call the margin this expression the inner product multiplied by the class label an we have an error when this margin is negative.",
            "When the sign of the inner product and the sign of the class label does not match, so this negative and the perception of date, the model for the misclassified labels only for misclassify labels using this rule.",
            "So the new way vector is going to be the previous one plus this this value.",
            "So it's a vector is vector X in the sign in the direction of the with the sign of the class label.",
            "Yeah, this is what we call additive rule 'cause we add something.",
            "Ann is a conservative rule becausw we only update W when we have errors when we don't have errors.",
            "W does not change.",
            "Recent things that other rules that are not conservative.",
            "For instance, we could think about some maximum likelihood techniques where we want to maximize the joint probability an we maximize the opportunity always.",
            "But maybe this is not an error, but we also want to maximize the joint probability of the class label on the samples.",
            "So here now he only we're going to update the model if an error has happened an maybe you want to.",
            "No.",
            "What we use this simple rule.",
            "So here since the new vector is W class.",
            "Glass label on the sample.",
            "So for instance, if the new iteration in the new iteration we it appears exactly the same sample, but we're going to do is WHI is going to be the sign of this again, sorry.",
            "This is the new.",
            "And then this product is going to be the new.",
            "He's just the latest.",
            "The previous one.",
            "So.",
            "With this rule we warranty that.",
            "Going to get always higher value.",
            "Just in case that this is positive or lower value of this expression, that's in case the Y is negative.",
            "So for positive class we're going to get higher value right now, and for negative class we're going to get lower value.",
            "So this is what we want.",
            "So remember that the margin is this present.",
            "So where we always to increase the margin, he always increase the margin.",
            "So in the positive direction, the net integration depending on why so maybe?",
            "This X this sample if it appears again, we're going to improve the margin.",
            "Maybe X is still misclassify.",
            "Of course it does so in this present does not appear if this margin is going to be negative or positive, but just in case of the positive, we're going to increase the product in the product.",
            "This inner product in case of negative we're going to decrease this inner product so we go in the correct direction is what we.",
            "Use this rule."
        ],
        [
            "Well.",
            "Also.",
            "We could introduce later.",
            "We will see more general.",
            "Expression of the perceptron green.",
            "But right now I would like to introduce the margin here so we could introduce here some scalar B is positive and then we're going to classify.",
            "We're going to misclassify the samples if the margin is lower instead of 0 lower than B."
        ],
        [
            "So it means that all these samples that fall in this area are going to be misclassified despite of maybe are in the correct side of the boundary."
        ],
        [
            "Hey.",
            "Anne.",
            "At the end, given this perception, this linear model.",
            "We know that if we can find some vector wave vector you that the margin is positive for all the samples.",
            "Sorry here should be capital T. Then the problem is linearly separable.",
            "An note that in this case the norm of you does not matter.",
            "So in just to decide if this positive or negative, the norm.",
            "No matters well.",
            "Any question, no I think is this very basic."
        ],
        [
            "Please.",
            "OK, so let us take a look to the online."
        ],
        [
            "Running for with the perceptron so online learning with the personally means that the following we're going to initialize the weight vector in the first iteration to 0.",
            "Some authors put here W 0 for the initial iteration.",
            "I prefer to use W sub one.",
            "So the first reason the vector is 0 and then for all the samples that we have, we receive only the sample in the time T in the step T and we compute the sign using WT.",
            "Then for the first iteration we use 0.",
            "When we compare this sign, if the sign that we compute is different than the class label of the sample, then we have an error and we're going to update.",
            "So W sub T plus one is going to be just the perceptron update rule an.",
            "In other case W sub P + 1 is going to be the previous one with chains.",
            "At the end, after the capital T samples, we return this weight vector W Sup T + 1."
        ],
        [
            "So.",
            "Let me show you some.",
            "Song demonstration of the some bounds of the number of errors of the perception in the online learning setup.",
            "So again, let BX Capital X is the sample, the sample data that we have.",
            "So capital T samples an LED be used are the linear model with a minimum number of errors over this data.",
            "An Le."
        ],
        [
            "B as before W."
        ],
        [
            "You D + 1 The linear model that we get at the end from our online learning perception."
        ],
        [
            "Remember that this perception is not the batch perception that it turns again to repeat the samples until some number of iterations it only see the samples once it's."
        ],
        [
            "Well, so which is the relation between the number of errors of these the best linear model for this sample and the number of errors, or of the perception of this online learning perception?",
            "So I use this notation.",
            "Other authors use different stations, so the error for some particular.",
            "Linear model at step T, so the total error is this expression and the total error overall.",
            "The iteration of the perception is this.",
            "So from one to capital T Sam.",
            "If an error has happens in this particular iteration T. So this value is one or zero.",
            "Well, we hope is that this error is going to be lower than there of the best classifier plus some constant.",
            "And of course we.",
            "Hope that this constant is small coffee.",
            "This constant is.",
            "If in it is very straightforward to demonstrate this, yeah, but where we want to solve this contest is small.",
            "An is very important.",
            "Again to note that this error is an online error.",
            "But this error is there aware of some you some linear model with all the same possible label?",
            "So in hindsight, when you see all the samples and then you say OK I put here the linear model.",
            "But this error is just an online procedure."
        ],
        [
            "So."
        ],
        [
            "Anne.",
            "In order to get this relation, the main problem here is to define this error.",
            "So the minimum number of errors of some linear model over this sample, this is."
        ],
        [
            "NP hard problem and we have to relax this expression introducing a different expression.",
            "So we're going to do is to introduce what we call the hinge loss.",
            "Have you see before the hinge loss?",
            "No.",
            "OK.",
            "Yes.",
            "So the hinge loss L of the wind model W for a given pair sample and class label is.",
            "This is the maximum of zero.",
            "An 1 means the margin.",
            "And this is what we call a hinge point.",
            "This one.",
            "Usually we fix one.",
            "So this is the error loss is zero when the margin is positive is 1 when the margin is negative.",
            "So this is this expression or this expression error loss an this the hinge loss the H loss is zero when the margin is greater than one but increase.",
            "When the margin is lower than one.",
            "Here we have a convex function an we can find we can use."
        ],
        [
            "With his loss.",
            "Here is that of the."
        ],
        [
            "Of the error loss well."
        ],
        [
            "So at the end, the relation that we want to find is, the following is.",
            "Find here the relation between the number of error of errors of our online perception with respect to the sum of loss.",
            "Of the best linear model last some constant.",
            "And we get this.",
            "We get that the number of errors is going to be always lower than the loss of the best.",
            "Sorry, this is for any you not only for the best for any linear model, so the number of errors going to be here, the loss plus some the norm squared norm of this vector plus across value between.",
            "Norm an the square root of this loss?",
            "Well, think it's worth to see were seen how to get these relations so.",
            "Despite of the bar is not too big, I'm going to try to get this relation in the bar, so maybe you can learn that big hole to get this kind of things.",
            "So.",
            "Well, usually to solve these problems we have to bound some expression that is the inner product between any vector U an.",
            "And the linear model that we get from the line learning.",
            "Yeah, we're going to bound this, and we're going to get to expression this one another one.",
            "And after that we're going to relate this present, reducing their coaches sparse.",
            "Relation so.",
            "You know that.",
            "If in the last iteration we get some error.",
            "Because in the last iteration we don't get any error this.",
            "What do we get?",
            "Because W hasn't changed, so it's useless this, so we're going to consider that we get.",
            "We got some error in the last iteration, so.",
            "We can get this disc.",
            "Big cause this linear model is the result of the apply the rule of the perception.",
            "Is equal to this course.",
            "Usually I have to put here that this pause always blah blah blah, but I don't use the sports here and here is the inner product so.",
            "So I'm going to someone and to subtract 1.",
            "We get this.",
            "Anne then.",
            "This is the loss.",
            "Yeah, remember the hinge loss is.",
            "1 minutes the inner product deployed by the sample so.",
            "Well, so this is what we get.",
            "This inner product is the same.",
            "In a product but with the previous.",
            "Here should be.",
            "With the previous linear model plus one minus the loss.",
            "In the iteration, TI mean with the sample XD with a class label YT of the.",
            "Linear model that use you well so this.",
            "Is a counter of the errors, so we get an error here.",
            "So we have plus one if we.",
            "And grab this until the 1st.",
            "What we get is this.",
            "Plus so.",
            "It's one of those, yeah.",
            "Besides this.",
            "This."
        ],
        [
            "From the definition of the last function.",
            "Here.",
            "Then OK, so from this to this we someone an subtract 1.",
            "OK, we just write one is minus one plus this.",
            "OK, so we use.",
            "Brackets here.",
            "Well, so this loss and this account of the number of errors.",
            "So we get this expression.",
            "So the inner product of any.",
            "Vector you multiply by the last way better that we get from our online perception is this expression.",
            "This is sorry, sorry this is zero.",
            "Remember the first vector is 0.",
            "So at the end.",
            "The inner this inner product for any.",
            "Linear model you is just counting the number of errors that the line perception has produced means the loss of this you OK the first person that we're going to use.",
            "So I'm going to clean.",
            "I think this is the smallest bar that they have to work before.",
            "Well, second expression is the following.",
            "So we want to take a look to the square norm of the weight vector that we get from our online learning perception.",
            "Again, it has no sense if we don't consider if we consider that no error has been produced, so we're going to consider that an error has been produced, so.",
            "Is this?",
            "And here.",
            "Well, we are going to consider that the square norm of the samples is 1.",
            "So the samples are normalized.",
            "Is the square of this of the normal sample is not one, so we can bound it by some.",
            "Capital R This is the maximum norm of this, so we don't lose any kind of generality with this assumption.",
            "OK, so don't worry.",
            "So we assume that our data is normalized and the square norm data is 1.",
            "But we can use also another kind of approximation, not work this value.",
            "Is negative.",
            "Big cause.",
            "Since we have considered that has been an error, the margin.",
            "Remember, it's an error with the margin is negative, so this value is negative.",
            "And then.",
            "But we get this lower then.",
            "This.",
            "Remember, if the norm squared off XT is not one, we can say that being capital Earth, the maximum for every sample.",
            "Of the Norm square norm.",
            "We could put here Capitolare again.",
            "Becausw will be upper bound of this expression.",
            "So the one we have a capital air and at the end we will have some capital air in the bounding so.",
            "There's assume that the data is normalized.",
            "Again, if we unwrap.",
            "We get this.",
            "Again, this one is a counter of the errors.",
            "Well, so we have the previous one expression.",
            "This expression an.",
            "Now you know that using the coaches bars inequality.",
            "That sorry.",
            "We're going to use the coaches bars inequality.",
            "Well.",
            "So.",
            "So OK, let me clean this part and.",
            "We go on.",
            "Well.",
            "So we have this.",
            "From the first impression.",
            "Anne.",
            "Using the Cauchy's bars inequality this.",
            "Well, we can do now.",
            "Is too.",
            "He is square.",
            "Here.",
            "And then we get this.",
            "Square on.",
            "This part is.",
            "This.",
            "Yeah, because this value remember is lower than this from the 2nd.",
            "Expression that we get before.",
            "So what we have here is that this.",
            "Is lower than this?",
            "So if you are interested, I can show you later how to get their relation, But what we have here is we call.",
            "This is a.",
            "This is L. This is U we get something like square.",
            "Videos to time Le Plus you square, then you eat.",
            "If we solve this in equation for the possible values of EI can show you later who you want.",
            "We have not too much time.",
            "So what?"
        ],
        [
            "We get is this.",
            "Well, at least maybe.",
            "I think it's worth.",
            "Did you learn how to get one of these bounds becausw when we read a lot from the reference?",
            "Of online learning machine learning.",
            "We have a lot of bonds, proofs and sometimes it's very difficult to know how to get this stuff.",
            "So usually the way to get it is this.",
            "So to bond the inner product of on some Norman, then to use coaches bars and to relate some parts and Julie.",
            "Most of the proofs are this way.",
            "Of course there are more complicated proofs, but OK, I think that is a worth is worth to see how to get this.",
            "Well.",
            "This person is for any you know, for the best.",
            "The best in this case will be the you that gives minimum of this expression.",
            "The minimum loss plus the minimum norm plus this minimum cross reference an.",
            "To get this minimum, we can take a look to we have to."
        ],
        [
            "Take a look to the his loss.",
            "For instance.",
            "Let's assume that we have.",
            "We are in the linear separable case.",
            "Then all the samples lay in this part because it's linearly separable.",
            "I."
        ],
        [
            "BC forward to get here 0.",
            "Is to increase."
        ],
        [
            "The vector so you get.",
            "In this part get 0."
        ],
        [
            "But if you get 0 here of the loss.",
            "Then you increase the vector.",
            "This common this two factors also could be increased, so we have to find sun vector you that give the minimum possible loss but also with the minimum possible you plus this cross reference.",
            "If we remove this part here.",
            "We can take a look this only how to get the minimum norm that gives us the minimum loss is something related to the support vector machine becausw minimize the norm of a vector accepted to that there are no errors.",
            "So one of these use can be found using support vector machines.",
            "Of course, avoiding this part.",
            "The idea also of this bound is that the online learning perceptron algorithm is good.",
            "So is not here we have.",
            "We don't have any plus infinite value.",
            "So the error of the linear perception is going to have is well bound, so I'm not going to have very large values this error.",
            "Well any question."
        ],
        [
            "OK.",
            "So now let me introduce you some general algorithm or some common algorithmic structure of the.",
            "The linear perceptron.",
            "Also I would like to point you that always do we have some weight vector an here a sample that we want to compute the inner product.",
            "What we have here is this relation.",
            "OK, so considering all the past vectors, we have to multiply like when we unwrap the relation we have to multiply this sample by all the previous samples, multiply for some constant that in the perception is 1.",
            "And then here we will apply later kernel in order to extend the perception to the kernel.",
            "Quit but right now, let us focus on this.",
            "So some common algorithm structure of that online learning perception is when we receive XD we compute Y yes if this why multiply by the the true label is the margin is greater than Sun beta.",
            "Is there some margin, an note that the beta also depends on T, so the margin can change along the iterations.",
            "So then see the margin is greater than this, then we don't have any error and then Alpha sub T this value is going to be 0, so we're going to update the model.",
            "Else we are going to update the model with the previous one plus some Alpha that also could depend on T iteration.",
            "An the well known sample and the vector.",
            "Optionally this W could be a scale in order to.",
            "Put W for instance, some bound with some bound.",
            "And maybe we can multiply this this vector that we obtained with some constant that also could change for each iteration.",
            "So the perception, or, you know, perception plays with Alpha T1 always.",
            "B to the margin is zero principle and this constant is 1, so any scale is produced is not probably use.",
            "Well, but there are other algorithms like this one.",
            "Relax online margin maximum margin, the approximate maximum margin classification and the margin margin, infuse relax.",
            "Ordering that play with some of these values in order to get what."
        ],
        [
            "Note to get here better bounds of errors in order to get a better algorithms."
        ],
        [
            "Well, for instance."
        ],
        [
            "This last algorithm, the margin in future, lacks."
        ],
        [
            "Graham when we have a two class problem, this only appears in grammar and singer.",
            "So at the end of this lies not in your version, but in my version you have the reference, so maybe you can do low from the web page and then you have the new version of the slides.",
            "So these are we apply this common algorithm instructor where the Alpha sub T so the update of the model is take this form.",
            "So right now, believe me, the formula takes later will explain you with more detail what means the kind of update with for the passive aggressive algorithm.",
            "I will do some entity idea right now, believe me and this capital J Capital G function is 0 if the value is lower, lens is negative.",
            "Is this value if these values between 01 an is 1 for any value larger than one.",
            "So here we have some bound.",
            "Between one and series, the value but lower zero higher is bounded at one."
        ],
        [
            "Well, Anne for instance also.",
            "Good property of the of the online learning is the following to be able to deal with what we call the tracking ability.",
            "The tracking ability means that maybe the data changes over the time.",
            "So.",
            "Delete that changed over the time we have a problem with the.",
            "Well known algorithms an but online learning can deal with this.",
            "And we follow some very straightforward rules.",
            "The rule in order to solve this problem.",
            "Is to have a weak dependence on the past.",
            "If we can forget what we learned from the first samples, we can adapt to the new distributions.",
            "So for instance, let's assume that we have.",
            "These samples positive.",
            "So everybody can see.",
            "Online learning will learn some hyperplane like this, but over the time the samples are changing positive.",
            "Negative.",
            "So we have to adapt this to this.",
            "Is changing again positive negative?",
            "So this is what we call the system problem, because this apartment has to shift to move near to adapt to the new samples.",
            "So and now, remember that this.",
            "Is.",
            "From the first sample to the last.",
            "This is pressing, so yes.",
            "Is there some overall the history of the class label?",
            "They wait for the original percentage one and the sample so we are summing is additive model we assuming so in order to be able to change to see if this model we have to forget.",
            "The additions produced by the 1st samples and this and this and this.",
            "So we have to have some weak dependence in the past.",
            "Another another.",
            "Or whether this this goal to have this weak dependence in the past can be achieved with two different models.",
            "The first one is what we call the memory abundance.",
            "So is what we learn at the end with the online learning on a budget.",
            "We will learn later an the 2nd way to achieve this week dependence on the past is OK, this Alpha.",
            "Related to the samples.",
            "Is going to decay.",
            "As the samples are older.",
            "For instance, in this case, we can take a look to the paper of Kavalan teeth, Fabian Cajun, till there with a percent to 60% algorithm in the same paper, they present the random budget perception algorithm that we will see later in this paper.",
            "They propose this weight in decay with decay.",
            "So in case that we have some error, so the class label is not much does not match the sign of the perception rule.",
            "Then we're going to update the new weight.",
            "Just with the same as we do with the perceptron, is additive model.",
            "Alpha T is 1.",
            "Voice of the XT that we're going to multiply the previous linear model with this factor.",
            "1 minus Lambda sub KK nocti K because K is a counter of errors.",
            "So when we have a new error, we incrementing one value K An.",
            "This Lambda sub K is this expression Lambda divided by Lambda over Lambda plus K. Now we will see graphically what it means."
        ],
        [
            "It means the following.",
            "For instance, let's assume that we have K 100.",
            "So at this time we have seen 100 errors.",
            "This day last error and this is the value of Lambda for this last error.",
            "And here we see different."
        ],
        [
            "Values of Lambda."
        ],
        [
            "And here will we see is this expression.",
            "This expression is let me except I they if sample with mistake.",
            "The first sample with mistake.",
            "The simple concept with mistake, not the first, the second sample, the first sample with Mistake II.",
            "Something with mistake.",
            "So this is the weight that this sample is going to receive."
        ],
        [
            "Why try to is very straightforward unwrap this.",
            "And more, and then you get some multiplication of this value up to the first one.",
            "OK, then if you unwrap this you get."
        ],
        [
            "This relation.",
            "Well so.",
            "This means that the first sample here is going to have.",
            "No one, but almost here .4.",
            "Alpha the way, so under or POV.",
            "This WT is a sum over only.",
            "The last samples.",
            "Is there a key issue with this?",
            "Well, so in this plot you can see the different Alpha different weight.",
            "Of the samples depending on Lambda and depending on the rank of these samples.",
            "So all this for two newest.",
            "OK, yes, so it's very straightforward to see this just unwrapping this expression over all the T. Well, any question at this time.",
            "What is the axis?",
            "Yeah, sure.",
            "So the axis here.",
            "The X axis here.",
            "Is why is the sorry I is the.",
            "The sample with error so.",
            "10 means that is the 10th sample with error first I have 20 samples but only 10 errors, so this last one is the sample with the 10th 10 sample with error, why?"
        ],
        [
            "Because.",
            "Only we apply this rule when we have an error.",
            "K&K is counting the errors."
        ],
        [
            "So to this sample I.",
            "We use this weight.",
            "That depends on taking the corner, how many years we have 100 in this case means.",
            "The value of I.",
            "For instance, if I is 10, we have one minute Lambda Power 90.",
            "So it's going to be very low value, so the reason why we get this decay decay scheme.",
            "If you have further questions, maybe later in the break we can.",
            "Solve.",
            "Well."
        ],
        [
            "Go on.",
            "Up to this now, what we have learned is how to solve problems where we have a we have binary classification problems and what happens when we have to extend this is to multiclass problems.",
            "So.",
            "As I see you, I I told you before, capital M is the number of classes.",
            "And the well known Kessler Construction is applied to extend binary problems to multiclass problems.",
            "This Kessler construction works as follows.",
            "Given some sample that lays in some dimensional space, this unique sample is transformed into M -- 1 samples.",
            "And each one of these samples lays in a very high dimensional space because it's the original dimension plus the number of classes.",
            "So this is just.",
            "Literature is not use is usually under the practical point of view because for each sample you have to get more than one sample an the new samples.",
            "Also laying very high dimensional space is useless, but it's practical from the point of view of we want to obtain some convergence proof.",
            "For instance Nabokov I guess.",
            "So who use it for demonstrate the convergence of the pattern for this multiclass problems?",
            "Well, in a practical scenario we will see later maybe other order mappings, but I guess for the impractical scenario.",
            "Whether we're going to do is the following.",
            "So W is the weight vector of the model an instead of half.",
            "This way better with the same dimensionality of the samples were going to have a new way better.",
            "In this case, a matrix as you want, with emrose being and the number of classes and the columns in the original dimensions.",
            "So the idea of this approach is when we have a sample we want to multiply the sample by different roles depending on the class that we want to compute the margin we will see later worry.",
            "So then, given a pair, we're going to compute Y as the row that give me the maximum value of this inner product.",
            "So we classify to the class which row give the maximum value of this inner product.",
            "And then if Y is different than the correct class between one and capital M is an error and then I will update the model.",
            "We will select the hope of data model, but is that useful?",
            "Well."
        ],
        [
            "In this way working with multi class."
        ],
        [
            "Perceptions we have a family of additive multiclass algorithms.",
            "Also conservative algorithms or ultraconservative algorithms that is proposed in this paper.",
            "This paper, the same paper of the media algorithm soldier before so it's worth paper to read.",
            "And then what we have is again given a pair XTYTYT now is a multiclass.",
            "We compute the maximum.",
            "And then if this maximum does not match with the correct label, we're going to update the model in the following way.",
            "First, the row associated to the correct class is going to be this.",
            "We are going to add some portion of the vector as we did before in the perception.",
            "Anne.",
            "Bing R. Belongs to this set where this set is all the rows of this matrix that the inner product is larger than the inner product of the correct class.",
            "So all these rows produce.",
            "Better result than the correct class.",
            "He has not.",
            "Is not correct.",
            "OK so we have to correct this in this case for all the rows that produce larger values of the of the inner product than the correct class.",
            "So this means that when we maximize, we're not going to pick the correct class.",
            "We're going to pick one of the incorrect classes.",
            "Yeah, are you lost or no OK?",
            "So we're going to some also some constant multiply by the vector.",
            "Well, we're going to impose this constraint where the constant of the correct class has to be.",
            "Minimize the sum of the constants of the other classes.",
            "Clearly, for example, the perceptron, the constant is 1 for the correct plus minus one for another class.",
            "This binary problem.",
            "OK, then are the same.",
            "For instance, some examples when.",
            "R is the incorrect class.",
            "We're going to subtract this value.",
            "Why the value by the total number of elements in the set?",
            "For the correct class, I went to some.",
            "So are you going to some just the object like in their perception?",
            "And zero another case.",
            "So this model is conservative becausw.",
            "It applies zero just in the case that.",
            "And here, just in the case that there is no error, it does not move the weight matrix.",
            "So one example, another example is this.",
            "To use minus one for the maximum incorrect class and to use one for the correct class.",
            "Also, this sum 10 always so.",
            "Which of these models are the best?",
            "Or whatever you did, you can think about you can get different models, so this is a family of models that what you have to do is just to impose this constraint.",
            "Which of this model is the best?",
            "So maybe is class dependent?",
            "This model modifies all the incorrect class that are larger, are better than the correct.",
            "This model only modify the largest incorrect class."
        ],
        [
            "So the authors proof that the number of errors of the online perceptron for the multiclass with this for this family.",
            "Of algorithms is this.",
            "So.",
            "This R is the maximum norm of the samples.",
            "This D is related to the square of the loss.",
            "This is at the hinge loss, but for the multiclass problems.",
            "An in particular there are some weight matrix that give the minimum, so the number of errors because this is for any for any matrix with norm one for any in particular for the minimum this bound is true.",
            "So it seems a good a good bond.",
            "OK here.",
            "Thinking back on that we have also the square of this.",
            "Gamma discovers like the hinge point of this multiclass hinge loss.",
            "Hey so also this is just for your knowledge."
        ],
        [
            "I prefer that you understand this, that all this family of of.",
            "Off"
        ],
        [
            "Orient provides a good bones of the error.",
            "Well."
        ],
        [
            "And in the same paper they provide different way of update.",
            "The weight matrix.",
            "Yeah, and the diff."
        ],
        [
            "Chris, with respect to this is that here we only update the weight matrix if some error is produced.",
            "So this algorithm is conservative.",
            "It doesn't move the way matrix if no error.",
            "But he."
        ],
        [
            "Here they don't put this constraint because they say OK anyway.",
            "So find the this Tao values that optimize this problem.",
            "Minimize the the norm of this expression.",
            "An this style is such a tool is lower than this Delta.",
            "I will explain it there an the towel must sum 10.",
            "So this Delta this value is 0 for the classes that are not the correct class is the Kronecker Delta.",
            "Ann is 1 when the R is wise at the correct class, so we are going to search.",
            "Dow's lower than 0.",
            "4.",
            "When?",
            "Why is the incorrect class an DAU lower than one?",
            "When I is the correct class an we impose this constraint.",
            "And update is very straightforward for every for every class.",
            "Update this.",
            "So here the question is, if we remove this condition and also that the rule is for every class, are we going to move those classes that are lower than the correct class that are not problematic?",
            "In this case, is not ultra conservative this model, because it's going to move things that they have not to move it.",
            "And to update things so the author demonstrated yes, because this ministration problems find 0.",
            "Zero for all, thous when no error is produced.",
            "So we're going to.",
            "We're not going to move any weight.",
            "So when this is false, this memorization problem, we can demonstrate the fine all day 1000 zero.",
            "So we're not going to move any weight.",
            "When this is true, some classes get larger values that the correct class.",
            "Is going to put some value in there for the correct class.",
            "Some value for incorrect class, but also zero for those classes that are not larger than the correct class.",
            "So with this mechanism, media is still an ultraconservative algorithm.",
            "This is good news.",
            "Well."
        ],
        [
            "So I hope.",
            "Not to you.",
            "Are you tired?",
            "I can maybe is only three for a slice and we will make the break before the passive aggressive online learning.",
            "Well and how we can extend this all this stuff to kernel perception, so it's."
        ],
        [
            "A straight forward.",
            "So the current perception is just a linear perceptron in the RKHS.",
            "That's the referring kernel Hilbert space.",
            "So the perceptron model becomes a linear combination of kernels instead of a linear combination of.",
            "Samples here.",
            "An the key issue is that all past mistaken samples XD becomes super vectors.",
            "So when we have 60 has a error, so the margin is lower than zero.",
            "We add this sample as supervector.",
            "We will see later.",
            "And the number of support vectors then is not bounded in principle.",
            "So they get that done that up to the last we have from one up to capital T we are.",
            "We have this online procedure, but maybe capital T is infinite.",
            "We receive samples continuously an for example that we have an error.",
            "We had a support vector.",
            "If the problem is very difficult, is not linearly separable and has a lot of outliers, we're going to add a lot of support vectors and at the end when we have received a new sample, what we have to do is to multiply the sample by the weight model.",
            "But the way model is going to be here some kernel and for each new sample we have to compute the kernel against all the previous mistaken samples.",
            "Unequal takes a lot of time, so it will be agreed here.",
            "To bomb the number of support vectors.",
            "But what happens if we bond the number support vectors?",
            "Maybe what we get is a very bad algorithm.",
            "So we have to bond the Super vectors, but ensuring that we get a good algorithm that we're going to get lower number of arrows.",
            "Well."
        ],
        [
            "So remember the linear perception here, so why is the sign of this inner product an if we unwrap this?",
            "With model we get this from all the previous samples.",
            "Multiply the new sample excepte by the news.",
            "The past samples multiplied by the weight you select one for the perception multiplied by the sample of this past.",
            "The class label of this past samples.",
            "So here we can put the kernel.",
            "So what we are doing is simulate that.",
            "So we do.",
            "Is X principle laser not dimensional space?",
            "So went to map.",
            "This X into some feature space.",
            "But we're not going to map it.",
            "So what we're going to do is here.",
            "What do we have to have then, is this?",
            "So after the map, what we have here is the inner product between two vectors that lay in this feature space and we're going to model this inner product with the kernel.",
            "Of the vectors.",
            "In original space, so the kernel trick.",
            "So now important thing is that the.",
            "Office of either we put here can be seen as the importance of the weight of the support vector.",
            "The support vectors going to support the boundary and depending on the weight is going to support a lot or support or weak support or the class.",
            "Monday we will see later in some examples.",
            "Anne, the good thing is that all the previous salaries that we have seen can be applied.",
            "For instance, the media that we have seen before for the binary classification.",
            "Whether we have to do is this use the kernel perception in the test phase an in the learning phase?",
            "Alpha Supply is going to be this.",
            "Exactly the same that we have."
        ],
        [
            "The four year.",
            "OK, this is going to be exactly the same."
        ],
        [
            "But of course, now this expression is not so trivial.",
            "Product, but is this?",
            "All this stuff.",
            "OK, and then we have to plug this into this and we get Alpha I and all is working and all the bonds that we prove that they prove are the same because what we have is just a linear model but in another space.",
            "So the proofs are the same."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today I hope that we learn a lot from this topic is online learning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The index of my lecture, so we first start with some introduction and some brief notation of what we're going to suppress mathematical expression that we're going to use.",
                    "label": 0
                },
                {
                    "sent": "We learn from, I think, one of the first online learning algorithms is the perceptron will have some basic notation some.",
                    "label": 0
                },
                {
                    "sent": "Proofs of some bounds of error.",
                    "label": 0
                },
                {
                    "sent": "And also we will move from perception to more elaborate techniques.",
                    "label": 0
                },
                {
                    "sent": "After that briefly, I will explain how to extend the linear perception to kernel perception.",
                    "label": 0
                },
                {
                    "sent": "It's quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "And after that I will focus.",
                    "label": 0
                },
                {
                    "sent": "On very new algorithm is called passive aggressive online learning.",
                    "label": 1
                },
                {
                    "sent": "We will see passive aggressively only for linear models.",
                    "label": 0
                },
                {
                    "sent": "For kernel models, for regression for multiclass, so we'll see different application of the same passive aggressive mythology.",
                    "label": 0
                },
                {
                    "sent": "And finally, we learn a little bit about or for learning on a budget becausw when we introduce the kernel methods into the perception or.",
                    "label": 1
                },
                {
                    "sent": "Passive aggressive, etc.",
                    "label": 0
                },
                {
                    "sent": "The problem we have is that the number of support vectors used to increase.",
                    "label": 0
                },
                {
                    "sent": "We thought any kind of constraint and then we will learn how to put the constraint, how to put the budget on the number of support vectors that we are going to use in our online learning at the end, if we have time, we will want to show you some applications that we have.",
                    "label": 0
                },
                {
                    "sent": "We have applied online learning, for instance for video tagging and also for relevance feedback on image retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. And what this online learning?",
                    "label": 1
                },
                {
                    "sent": "Solon learning, at least for me, is procedure algorithm for obtaining a machine learning model, where we use a unique sample at each iteration.",
                    "label": 1
                },
                {
                    "sent": "Maybe we have more samples.",
                    "label": 0
                },
                {
                    "sent": "Maybe we have some historic samples, but we're going to use only one sample.",
                    "label": 1
                },
                {
                    "sent": "Moreover, maybe maybe the distribution later is unknown, so we have never seen before data from this problem and maybe also this data changed over the time, so online learning is going to help us to solve these problems.",
                    "label": 0
                },
                {
                    "sent": "And where we have online learning problems for either, we can have we have or any problems when we want to forecast something, some financial series.",
                    "label": 0
                },
                {
                    "sent": "And then maybe we have to the data is arrive and an as the data center in we have to decide the forecast forecast of this of this data.",
                    "label": 1
                },
                {
                    "sent": "But sure, if we want to, for instance, to forecasts on financial series, we have some historic.",
                    "label": 0
                },
                {
                    "sent": "So we have data, so why not to do some batch procedure of all the data that we have in set of process of process?",
                    "label": 0
                },
                {
                    "sent": "Only one sample in each iteration.",
                    "label": 0
                },
                {
                    "sent": "So pure PRN learning is difficult to find 'cause we always have some data that we have.",
                    "label": 1
                },
                {
                    "sent": "We can do in a batch.",
                    "label": 0
                },
                {
                    "sent": "So which is the motivation then?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Off online running, the motivation is in one part is to have a very efficient algorithm that only needs to process only one data at each iteration is not going to process all the data in a batch manner.",
                    "label": 1
                },
                {
                    "sent": "And also, we're going to be able to deal with the problem with the shifting problem, where the data change over the time.",
                    "label": 1
                },
                {
                    "sent": "We will see this later.",
                    "label": 0
                },
                {
                    "sent": "And then the question that we want to answer is that could we get good models for processing an unique sample at each iteration?",
                    "label": 1
                },
                {
                    "sent": "So doing this process only one sample at each iteration, an never cycle over the samples?",
                    "label": 0
                },
                {
                    "sent": "We are going.",
                    "label": 0
                },
                {
                    "sent": "Are we going to be able to get good models with a question that in part we going to solve today?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is suppose that the students should know.",
                    "label": 1
                },
                {
                    "sent": "So the best basic machine learning concepts.",
                    "label": 1
                },
                {
                    "sent": "What linear models means.",
                    "label": 0
                },
                {
                    "sent": "But I am going to explain here again and about kernel methods.",
                    "label": 0
                },
                {
                    "sent": "Of course an.",
                    "label": 1
                },
                {
                    "sent": "The notation that I went to uses the following, so the samples are vectors in additional space.",
                    "label": 1
                },
                {
                    "sent": "The wave vector of the linear models is a vector in the semi space.",
                    "label": 0
                },
                {
                    "sent": "For binary classification problems, we have the class label Y is minus 1 + 1 for multiclass problems, Y is between one and capital N being capital M. The number of classes.",
                    "label": 0
                },
                {
                    "sent": "The loss function L that we're going to use.",
                    "label": 1
                },
                {
                    "sent": "Usually we're going to use the hinge loss function loss function, and the kernels is going to be a function between two vectors.",
                    "label": 0
                },
                {
                    "sent": "And also the support vectors are going to be represented as a set of indexes capital X, so we will see this later.",
                    "label": 0
                },
                {
                    "sent": "Let me check the.",
                    "label": 0
                },
                {
                    "sent": "Maybe some properties of the display will cause?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, linear models.",
                    "label": 0
                },
                {
                    "sent": "As you know, linear model is Alisa model that is linear.",
                    "label": 0
                },
                {
                    "sent": "So given vector X, we compute the inner product between the model.",
                    "label": 0
                },
                {
                    "sent": "The model is just.",
                    "label": 0
                },
                {
                    "sent": "This very compact is safe in a way back to another due an.",
                    "label": 0
                },
                {
                    "sent": "We Samsung offset and then the class label is design of this operation.",
                    "label": 0
                },
                {
                    "sent": "So usually we use a more compact notation combination where this person is just this W in a product with X where W is the vector.",
                    "label": 0
                },
                {
                    "sent": "Previous vector plus the offset an X has some constant one in the first component.",
                    "label": 0
                },
                {
                    "sent": "In order to explain this option here.",
                    "label": 0
                },
                {
                    "sent": "And usually these the dimensional space that we're going to deal with.",
                    "label": 0
                },
                {
                    "sent": "So this is a linear model, is a classification is performed computing the design of this inner.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne, this graphical representation has useful of a linear model.",
                    "label": 0
                },
                {
                    "sent": "This vector is W and this part in the direction of the vector is going to get positive values of this inner product, and this path is going to get negative values.",
                    "label": 0
                },
                {
                    "sent": "This inner product and this rail line is the points where this inner product is 0.",
                    "label": 0
                },
                {
                    "sent": "So is the boundary.",
                    "label": 0
                },
                {
                    "sent": "The class boundaries.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the perceptron is arguing that given a set of data X1 class label X2 Plus label up to X Capital T Plus label.",
                    "label": 0
                },
                {
                    "sent": "Is going to classify this data using Wavevector other year.",
                    "label": 0
                },
                {
                    "sent": "The classifier is design.",
                    "label": 0
                },
                {
                    "sent": "This is Amanda.",
                    "label": 0
                },
                {
                    "sent": "Re is where the points are zero and we call the margin this expression the inner product multiplied by the class label an we have an error when this margin is negative.",
                    "label": 0
                },
                {
                    "sent": "When the sign of the inner product and the sign of the class label does not match, so this negative and the perception of date, the model for the misclassified labels only for misclassify labels using this rule.",
                    "label": 0
                },
                {
                    "sent": "So the new way vector is going to be the previous one plus this this value.",
                    "label": 0
                },
                {
                    "sent": "So it's a vector is vector X in the sign in the direction of the with the sign of the class label.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is what we call additive rule 'cause we add something.",
                    "label": 0
                },
                {
                    "sent": "Ann is a conservative rule becausw we only update W when we have errors when we don't have errors.",
                    "label": 0
                },
                {
                    "sent": "W does not change.",
                    "label": 0
                },
                {
                    "sent": "Recent things that other rules that are not conservative.",
                    "label": 0
                },
                {
                    "sent": "For instance, we could think about some maximum likelihood techniques where we want to maximize the joint probability an we maximize the opportunity always.",
                    "label": 0
                },
                {
                    "sent": "But maybe this is not an error, but we also want to maximize the joint probability of the class label on the samples.",
                    "label": 0
                },
                {
                    "sent": "So here now he only we're going to update the model if an error has happened an maybe you want to.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "What we use this simple rule.",
                    "label": 0
                },
                {
                    "sent": "So here since the new vector is W class.",
                    "label": 0
                },
                {
                    "sent": "Glass label on the sample.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if the new iteration in the new iteration we it appears exactly the same sample, but we're going to do is WHI is going to be the sign of this again, sorry.",
                    "label": 0
                },
                {
                    "sent": "This is the new.",
                    "label": 0
                },
                {
                    "sent": "And then this product is going to be the new.",
                    "label": 0
                },
                {
                    "sent": "He's just the latest.",
                    "label": 0
                },
                {
                    "sent": "The previous one.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "With this rule we warranty that.",
                    "label": 0
                },
                {
                    "sent": "Going to get always higher value.",
                    "label": 0
                },
                {
                    "sent": "Just in case that this is positive or lower value of this expression, that's in case the Y is negative.",
                    "label": 0
                },
                {
                    "sent": "So for positive class we're going to get higher value right now, and for negative class we're going to get lower value.",
                    "label": 0
                },
                {
                    "sent": "So this is what we want.",
                    "label": 0
                },
                {
                    "sent": "So remember that the margin is this present.",
                    "label": 0
                },
                {
                    "sent": "So where we always to increase the margin, he always increase the margin.",
                    "label": 0
                },
                {
                    "sent": "So in the positive direction, the net integration depending on why so maybe?",
                    "label": 0
                },
                {
                    "sent": "This X this sample if it appears again, we're going to improve the margin.",
                    "label": 0
                },
                {
                    "sent": "Maybe X is still misclassify.",
                    "label": 0
                },
                {
                    "sent": "Of course it does so in this present does not appear if this margin is going to be negative or positive, but just in case of the positive, we're going to increase the product in the product.",
                    "label": 0
                },
                {
                    "sent": "This inner product in case of negative we're going to decrease this inner product so we go in the correct direction is what we.",
                    "label": 0
                },
                {
                    "sent": "Use this rule.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "We could introduce later.",
                    "label": 0
                },
                {
                    "sent": "We will see more general.",
                    "label": 0
                },
                {
                    "sent": "Expression of the perceptron green.",
                    "label": 0
                },
                {
                    "sent": "But right now I would like to introduce the margin here so we could introduce here some scalar B is positive and then we're going to classify.",
                    "label": 0
                },
                {
                    "sent": "We're going to misclassify the samples if the margin is lower instead of 0 lower than B.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it means that all these samples that fall in this area are going to be misclassified despite of maybe are in the correct side of the boundary.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "At the end, given this perception, this linear model.",
                    "label": 0
                },
                {
                    "sent": "We know that if we can find some vector wave vector you that the margin is positive for all the samples.",
                    "label": 0
                },
                {
                    "sent": "Sorry here should be capital T. Then the problem is linearly separable.",
                    "label": 0
                },
                {
                    "sent": "An note that in this case the norm of you does not matter.",
                    "label": 0
                },
                {
                    "sent": "So in just to decide if this positive or negative, the norm.",
                    "label": 0
                },
                {
                    "sent": "No matters well.",
                    "label": 0
                },
                {
                    "sent": "Any question, no I think is this very basic.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please.",
                    "label": 0
                },
                {
                    "sent": "OK, so let us take a look to the online.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Running for with the perceptron so online learning with the personally means that the following we're going to initialize the weight vector in the first iteration to 0.",
                    "label": 1
                },
                {
                    "sent": "Some authors put here W 0 for the initial iteration.",
                    "label": 0
                },
                {
                    "sent": "I prefer to use W sub one.",
                    "label": 0
                },
                {
                    "sent": "So the first reason the vector is 0 and then for all the samples that we have, we receive only the sample in the time T in the step T and we compute the sign using WT.",
                    "label": 0
                },
                {
                    "sent": "Then for the first iteration we use 0.",
                    "label": 0
                },
                {
                    "sent": "When we compare this sign, if the sign that we compute is different than the class label of the sample, then we have an error and we're going to update.",
                    "label": 0
                },
                {
                    "sent": "So W sub T plus one is going to be just the perceptron update rule an.",
                    "label": 0
                },
                {
                    "sent": "In other case W sub P + 1 is going to be the previous one with chains.",
                    "label": 0
                },
                {
                    "sent": "At the end, after the capital T samples, we return this weight vector W Sup T + 1.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me show you some.",
                    "label": 0
                },
                {
                    "sent": "Song demonstration of the some bounds of the number of errors of the perception in the online learning setup.",
                    "label": 0
                },
                {
                    "sent": "So again, let BX Capital X is the sample, the sample data that we have.",
                    "label": 0
                },
                {
                    "sent": "So capital T samples an LED be used are the linear model with a minimum number of errors over this data.",
                    "label": 0
                },
                {
                    "sent": "An Le.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "B as before W.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You D + 1 The linear model that we get at the end from our online learning perception.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remember that this perception is not the batch perception that it turns again to repeat the samples until some number of iterations it only see the samples once it's.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, so which is the relation between the number of errors of these the best linear model for this sample and the number of errors, or of the perception of this online learning perception?",
                    "label": 0
                },
                {
                    "sent": "So I use this notation.",
                    "label": 0
                },
                {
                    "sent": "Other authors use different stations, so the error for some particular.",
                    "label": 0
                },
                {
                    "sent": "Linear model at step T, so the total error is this expression and the total error overall.",
                    "label": 0
                },
                {
                    "sent": "The iteration of the perception is this.",
                    "label": 0
                },
                {
                    "sent": "So from one to capital T Sam.",
                    "label": 0
                },
                {
                    "sent": "If an error has happens in this particular iteration T. So this value is one or zero.",
                    "label": 0
                },
                {
                    "sent": "Well, we hope is that this error is going to be lower than there of the best classifier plus some constant.",
                    "label": 0
                },
                {
                    "sent": "And of course we.",
                    "label": 0
                },
                {
                    "sent": "Hope that this constant is small coffee.",
                    "label": 0
                },
                {
                    "sent": "This constant is.",
                    "label": 0
                },
                {
                    "sent": "If in it is very straightforward to demonstrate this, yeah, but where we want to solve this contest is small.",
                    "label": 0
                },
                {
                    "sent": "An is very important.",
                    "label": 0
                },
                {
                    "sent": "Again to note that this error is an online error.",
                    "label": 0
                },
                {
                    "sent": "But this error is there aware of some you some linear model with all the same possible label?",
                    "label": 0
                },
                {
                    "sent": "So in hindsight, when you see all the samples and then you say OK I put here the linear model.",
                    "label": 0
                },
                {
                    "sent": "But this error is just an online procedure.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In order to get this relation, the main problem here is to define this error.",
                    "label": 0
                },
                {
                    "sent": "So the minimum number of errors of some linear model over this sample, this is.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "NP hard problem and we have to relax this expression introducing a different expression.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do is to introduce what we call the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Have you see before the hinge loss?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So the hinge loss L of the wind model W for a given pair sample and class label is.",
                    "label": 0
                },
                {
                    "sent": "This is the maximum of zero.",
                    "label": 0
                },
                {
                    "sent": "An 1 means the margin.",
                    "label": 0
                },
                {
                    "sent": "And this is what we call a hinge point.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Usually we fix one.",
                    "label": 0
                },
                {
                    "sent": "So this is the error loss is zero when the margin is positive is 1 when the margin is negative.",
                    "label": 0
                },
                {
                    "sent": "So this is this expression or this expression error loss an this the hinge loss the H loss is zero when the margin is greater than one but increase.",
                    "label": 0
                },
                {
                    "sent": "When the margin is lower than one.",
                    "label": 0
                },
                {
                    "sent": "Here we have a convex function an we can find we can use.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With his loss.",
                    "label": 0
                },
                {
                    "sent": "Here is that of the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the error loss well.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So at the end, the relation that we want to find is, the following is.",
                    "label": 0
                },
                {
                    "sent": "Find here the relation between the number of error of errors of our online perception with respect to the sum of loss.",
                    "label": 1
                },
                {
                    "sent": "Of the best linear model last some constant.",
                    "label": 0
                },
                {
                    "sent": "And we get this.",
                    "label": 0
                },
                {
                    "sent": "We get that the number of errors is going to be always lower than the loss of the best.",
                    "label": 1
                },
                {
                    "sent": "Sorry, this is for any you not only for the best for any linear model, so the number of errors going to be here, the loss plus some the norm squared norm of this vector plus across value between.",
                    "label": 0
                },
                {
                    "sent": "Norm an the square root of this loss?",
                    "label": 0
                },
                {
                    "sent": "Well, think it's worth to see were seen how to get these relations so.",
                    "label": 0
                },
                {
                    "sent": "Despite of the bar is not too big, I'm going to try to get this relation in the bar, so maybe you can learn that big hole to get this kind of things.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, usually to solve these problems we have to bound some expression that is the inner product between any vector U an.",
                    "label": 0
                },
                {
                    "sent": "And the linear model that we get from the line learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we're going to bound this, and we're going to get to expression this one another one.",
                    "label": 0
                },
                {
                    "sent": "And after that we're going to relate this present, reducing their coaches sparse.",
                    "label": 0
                },
                {
                    "sent": "Relation so.",
                    "label": 0
                },
                {
                    "sent": "You know that.",
                    "label": 0
                },
                {
                    "sent": "If in the last iteration we get some error.",
                    "label": 0
                },
                {
                    "sent": "Because in the last iteration we don't get any error this.",
                    "label": 0
                },
                {
                    "sent": "What do we get?",
                    "label": 0
                },
                {
                    "sent": "Because W hasn't changed, so it's useless this, so we're going to consider that we get.",
                    "label": 0
                },
                {
                    "sent": "We got some error in the last iteration, so.",
                    "label": 0
                },
                {
                    "sent": "We can get this disc.",
                    "label": 0
                },
                {
                    "sent": "Big cause this linear model is the result of the apply the rule of the perception.",
                    "label": 0
                },
                {
                    "sent": "Is equal to this course.",
                    "label": 0
                },
                {
                    "sent": "Usually I have to put here that this pause always blah blah blah, but I don't use the sports here and here is the inner product so.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to someone and to subtract 1.",
                    "label": 0
                },
                {
                    "sent": "We get this.",
                    "label": 0
                },
                {
                    "sent": "Anne then.",
                    "label": 0
                },
                {
                    "sent": "This is the loss.",
                    "label": 0
                },
                {
                    "sent": "Yeah, remember the hinge loss is.",
                    "label": 1
                },
                {
                    "sent": "1 minutes the inner product deployed by the sample so.",
                    "label": 0
                },
                {
                    "sent": "Well, so this is what we get.",
                    "label": 0
                },
                {
                    "sent": "This inner product is the same.",
                    "label": 0
                },
                {
                    "sent": "In a product but with the previous.",
                    "label": 0
                },
                {
                    "sent": "Here should be.",
                    "label": 0
                },
                {
                    "sent": "With the previous linear model plus one minus the loss.",
                    "label": 0
                },
                {
                    "sent": "In the iteration, TI mean with the sample XD with a class label YT of the.",
                    "label": 1
                },
                {
                    "sent": "Linear model that use you well so this.",
                    "label": 0
                },
                {
                    "sent": "Is a counter of the errors, so we get an error here.",
                    "label": 0
                },
                {
                    "sent": "So we have plus one if we.",
                    "label": 0
                },
                {
                    "sent": "And grab this until the 1st.",
                    "label": 0
                },
                {
                    "sent": "What we get is this.",
                    "label": 0
                },
                {
                    "sent": "Plus so.",
                    "label": 0
                },
                {
                    "sent": "It's one of those, yeah.",
                    "label": 0
                },
                {
                    "sent": "Besides this.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From the definition of the last function.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Then OK, so from this to this we someone an subtract 1.",
                    "label": 0
                },
                {
                    "sent": "OK, we just write one is minus one plus this.",
                    "label": 0
                },
                {
                    "sent": "OK, so we use.",
                    "label": 0
                },
                {
                    "sent": "Brackets here.",
                    "label": 0
                },
                {
                    "sent": "Well, so this loss and this account of the number of errors.",
                    "label": 1
                },
                {
                    "sent": "So we get this expression.",
                    "label": 0
                },
                {
                    "sent": "So the inner product of any.",
                    "label": 0
                },
                {
                    "sent": "Vector you multiply by the last way better that we get from our online perception is this expression.",
                    "label": 0
                },
                {
                    "sent": "This is sorry, sorry this is zero.",
                    "label": 0
                },
                {
                    "sent": "Remember the first vector is 0.",
                    "label": 1
                },
                {
                    "sent": "So at the end.",
                    "label": 0
                },
                {
                    "sent": "The inner this inner product for any.",
                    "label": 0
                },
                {
                    "sent": "Linear model you is just counting the number of errors that the line perception has produced means the loss of this you OK the first person that we're going to use.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to clean.",
                    "label": 0
                },
                {
                    "sent": "I think this is the smallest bar that they have to work before.",
                    "label": 0
                },
                {
                    "sent": "Well, second expression is the following.",
                    "label": 0
                },
                {
                    "sent": "So we want to take a look to the square norm of the weight vector that we get from our online learning perception.",
                    "label": 0
                },
                {
                    "sent": "Again, it has no sense if we don't consider if we consider that no error has been produced, so we're going to consider that an error has been produced, so.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "And here.",
                    "label": 0
                },
                {
                    "sent": "Well, we are going to consider that the square norm of the samples is 1.",
                    "label": 0
                },
                {
                    "sent": "So the samples are normalized.",
                    "label": 0
                },
                {
                    "sent": "Is the square of this of the normal sample is not one, so we can bound it by some.",
                    "label": 0
                },
                {
                    "sent": "Capital R This is the maximum norm of this, so we don't lose any kind of generality with this assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so don't worry.",
                    "label": 0
                },
                {
                    "sent": "So we assume that our data is normalized and the square norm data is 1.",
                    "label": 0
                },
                {
                    "sent": "But we can use also another kind of approximation, not work this value.",
                    "label": 0
                },
                {
                    "sent": "Is negative.",
                    "label": 0
                },
                {
                    "sent": "Big cause.",
                    "label": 0
                },
                {
                    "sent": "Since we have considered that has been an error, the margin.",
                    "label": 0
                },
                {
                    "sent": "Remember, it's an error with the margin is negative, so this value is negative.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "But we get this lower then.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Remember, if the norm squared off XT is not one, we can say that being capital Earth, the maximum for every sample.",
                    "label": 0
                },
                {
                    "sent": "Of the Norm square norm.",
                    "label": 0
                },
                {
                    "sent": "We could put here Capitolare again.",
                    "label": 0
                },
                {
                    "sent": "Becausw will be upper bound of this expression.",
                    "label": 0
                },
                {
                    "sent": "So the one we have a capital air and at the end we will have some capital air in the bounding so.",
                    "label": 0
                },
                {
                    "sent": "There's assume that the data is normalized.",
                    "label": 0
                },
                {
                    "sent": "Again, if we unwrap.",
                    "label": 1
                },
                {
                    "sent": "We get this.",
                    "label": 0
                },
                {
                    "sent": "Again, this one is a counter of the errors.",
                    "label": 0
                },
                {
                    "sent": "Well, so we have the previous one expression.",
                    "label": 0
                },
                {
                    "sent": "This expression an.",
                    "label": 0
                },
                {
                    "sent": "Now you know that using the coaches bars inequality.",
                    "label": 0
                },
                {
                    "sent": "That sorry.",
                    "label": 0
                },
                {
                    "sent": "We're going to use the coaches bars inequality.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So OK, let me clean this part and.",
                    "label": 0
                },
                {
                    "sent": "We go on.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "So we have this.",
                    "label": 0
                },
                {
                    "sent": "From the first impression.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Using the Cauchy's bars inequality this.",
                    "label": 0
                },
                {
                    "sent": "Well, we can do now.",
                    "label": 0
                },
                {
                    "sent": "Is too.",
                    "label": 0
                },
                {
                    "sent": "He is square.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "And then we get this.",
                    "label": 0
                },
                {
                    "sent": "Square on.",
                    "label": 0
                },
                {
                    "sent": "This part is.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because this value remember is lower than this from the 2nd.",
                    "label": 1
                },
                {
                    "sent": "Expression that we get before.",
                    "label": 0
                },
                {
                    "sent": "So what we have here is that this.",
                    "label": 0
                },
                {
                    "sent": "Is lower than this?",
                    "label": 0
                },
                {
                    "sent": "So if you are interested, I can show you later how to get their relation, But what we have here is we call.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "This is L. This is U we get something like square.",
                    "label": 0
                },
                {
                    "sent": "Videos to time Le Plus you square, then you eat.",
                    "label": 0
                },
                {
                    "sent": "If we solve this in equation for the possible values of EI can show you later who you want.",
                    "label": 0
                },
                {
                    "sent": "We have not too much time.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We get is this.",
                    "label": 0
                },
                {
                    "sent": "Well, at least maybe.",
                    "label": 0
                },
                {
                    "sent": "I think it's worth.",
                    "label": 0
                },
                {
                    "sent": "Did you learn how to get one of these bounds becausw when we read a lot from the reference?",
                    "label": 0
                },
                {
                    "sent": "Of online learning machine learning.",
                    "label": 1
                },
                {
                    "sent": "We have a lot of bonds, proofs and sometimes it's very difficult to know how to get this stuff.",
                    "label": 0
                },
                {
                    "sent": "So usually the way to get it is this.",
                    "label": 0
                },
                {
                    "sent": "So to bond the inner product of on some Norman, then to use coaches bars and to relate some parts and Julie.",
                    "label": 0
                },
                {
                    "sent": "Most of the proofs are this way.",
                    "label": 1
                },
                {
                    "sent": "Of course there are more complicated proofs, but OK, I think that is a worth is worth to see how to get this.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "This person is for any you know, for the best.",
                    "label": 0
                },
                {
                    "sent": "The best in this case will be the you that gives minimum of this expression.",
                    "label": 0
                },
                {
                    "sent": "The minimum loss plus the minimum norm plus this minimum cross reference an.",
                    "label": 0
                },
                {
                    "sent": "To get this minimum, we can take a look to we have to.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a look to the his loss.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that we have.",
                    "label": 0
                },
                {
                    "sent": "We are in the linear separable case.",
                    "label": 0
                },
                {
                    "sent": "Then all the samples lay in this part because it's linearly separable.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "BC forward to get here 0.",
                    "label": 0
                },
                {
                    "sent": "Is to increase.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The vector so you get.",
                    "label": 0
                },
                {
                    "sent": "In this part get 0.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if you get 0 here of the loss.",
                    "label": 0
                },
                {
                    "sent": "Then you increase the vector.",
                    "label": 0
                },
                {
                    "sent": "This common this two factors also could be increased, so we have to find sun vector you that give the minimum possible loss but also with the minimum possible you plus this cross reference.",
                    "label": 1
                },
                {
                    "sent": "If we remove this part here.",
                    "label": 0
                },
                {
                    "sent": "We can take a look this only how to get the minimum norm that gives us the minimum loss is something related to the support vector machine becausw minimize the norm of a vector accepted to that there are no errors.",
                    "label": 0
                },
                {
                    "sent": "So one of these use can be found using support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Of course, avoiding this part.",
                    "label": 1
                },
                {
                    "sent": "The idea also of this bound is that the online learning perceptron algorithm is good.",
                    "label": 0
                },
                {
                    "sent": "So is not here we have.",
                    "label": 0
                },
                {
                    "sent": "We don't have any plus infinite value.",
                    "label": 0
                },
                {
                    "sent": "So the error of the linear perception is going to have is well bound, so I'm not going to have very large values this error.",
                    "label": 0
                },
                {
                    "sent": "Well any question.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now let me introduce you some general algorithm or some common algorithmic structure of the.",
                    "label": 0
                },
                {
                    "sent": "The linear perceptron.",
                    "label": 0
                },
                {
                    "sent": "Also I would like to point you that always do we have some weight vector an here a sample that we want to compute the inner product.",
                    "label": 0
                },
                {
                    "sent": "What we have here is this relation.",
                    "label": 1
                },
                {
                    "sent": "OK, so considering all the past vectors, we have to multiply like when we unwrap the relation we have to multiply this sample by all the previous samples, multiply for some constant that in the perception is 1.",
                    "label": 0
                },
                {
                    "sent": "And then here we will apply later kernel in order to extend the perception to the kernel.",
                    "label": 0
                },
                {
                    "sent": "Quit but right now, let us focus on this.",
                    "label": 0
                },
                {
                    "sent": "So some common algorithm structure of that online learning perception is when we receive XD we compute Y yes if this why multiply by the the true label is the margin is greater than Sun beta.",
                    "label": 0
                },
                {
                    "sent": "Is there some margin, an note that the beta also depends on T, so the margin can change along the iterations.",
                    "label": 0
                },
                {
                    "sent": "So then see the margin is greater than this, then we don't have any error and then Alpha sub T this value is going to be 0, so we're going to update the model.",
                    "label": 1
                },
                {
                    "sent": "Else we are going to update the model with the previous one plus some Alpha that also could depend on T iteration.",
                    "label": 0
                },
                {
                    "sent": "An the well known sample and the vector.",
                    "label": 0
                },
                {
                    "sent": "Optionally this W could be a scale in order to.",
                    "label": 0
                },
                {
                    "sent": "Put W for instance, some bound with some bound.",
                    "label": 0
                },
                {
                    "sent": "And maybe we can multiply this this vector that we obtained with some constant that also could change for each iteration.",
                    "label": 0
                },
                {
                    "sent": "So the perception, or, you know, perception plays with Alpha T1 always.",
                    "label": 0
                },
                {
                    "sent": "B to the margin is zero principle and this constant is 1, so any scale is produced is not probably use.",
                    "label": 0
                },
                {
                    "sent": "Well, but there are other algorithms like this one.",
                    "label": 0
                },
                {
                    "sent": "Relax online margin maximum margin, the approximate maximum margin classification and the margin margin, infuse relax.",
                    "label": 1
                },
                {
                    "sent": "Ordering that play with some of these values in order to get what.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note to get here better bounds of errors in order to get a better algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, for instance.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This last algorithm, the margin in future, lacks.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graham when we have a two class problem, this only appears in grammar and singer.",
                    "label": 0
                },
                {
                    "sent": "So at the end of this lies not in your version, but in my version you have the reference, so maybe you can do low from the web page and then you have the new version of the slides.",
                    "label": 0
                },
                {
                    "sent": "So these are we apply this common algorithm instructor where the Alpha sub T so the update of the model is take this form.",
                    "label": 0
                },
                {
                    "sent": "So right now, believe me, the formula takes later will explain you with more detail what means the kind of update with for the passive aggressive algorithm.",
                    "label": 0
                },
                {
                    "sent": "I will do some entity idea right now, believe me and this capital J Capital G function is 0 if the value is lower, lens is negative.",
                    "label": 0
                },
                {
                    "sent": "Is this value if these values between 01 an is 1 for any value larger than one.",
                    "label": 0
                },
                {
                    "sent": "So here we have some bound.",
                    "label": 0
                },
                {
                    "sent": "Between one and series, the value but lower zero higher is bounded at one.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, Anne for instance also.",
                    "label": 0
                },
                {
                    "sent": "Good property of the of the online learning is the following to be able to deal with what we call the tracking ability.",
                    "label": 0
                },
                {
                    "sent": "The tracking ability means that maybe the data changes over the time.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Delete that changed over the time we have a problem with the.",
                    "label": 0
                },
                {
                    "sent": "Well known algorithms an but online learning can deal with this.",
                    "label": 0
                },
                {
                    "sent": "And we follow some very straightforward rules.",
                    "label": 0
                },
                {
                    "sent": "The rule in order to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Is to have a weak dependence on the past.",
                    "label": 0
                },
                {
                    "sent": "If we can forget what we learned from the first samples, we can adapt to the new distributions.",
                    "label": 0
                },
                {
                    "sent": "So for instance, let's assume that we have.",
                    "label": 0
                },
                {
                    "sent": "These samples positive.",
                    "label": 0
                },
                {
                    "sent": "So everybody can see.",
                    "label": 0
                },
                {
                    "sent": "Online learning will learn some hyperplane like this, but over the time the samples are changing positive.",
                    "label": 0
                },
                {
                    "sent": "Negative.",
                    "label": 0
                },
                {
                    "sent": "So we have to adapt this to this.",
                    "label": 0
                },
                {
                    "sent": "Is changing again positive negative?",
                    "label": 0
                },
                {
                    "sent": "So this is what we call the system problem, because this apartment has to shift to move near to adapt to the new samples.",
                    "label": 0
                },
                {
                    "sent": "So and now, remember that this.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "From the first sample to the last.",
                    "label": 0
                },
                {
                    "sent": "This is pressing, so yes.",
                    "label": 0
                },
                {
                    "sent": "Is there some overall the history of the class label?",
                    "label": 0
                },
                {
                    "sent": "They wait for the original percentage one and the sample so we are summing is additive model we assuming so in order to be able to change to see if this model we have to forget.",
                    "label": 0
                },
                {
                    "sent": "The additions produced by the 1st samples and this and this and this.",
                    "label": 0
                },
                {
                    "sent": "So we have to have some weak dependence in the past.",
                    "label": 0
                },
                {
                    "sent": "Another another.",
                    "label": 0
                },
                {
                    "sent": "Or whether this this goal to have this weak dependence in the past can be achieved with two different models.",
                    "label": 0
                },
                {
                    "sent": "The first one is what we call the memory abundance.",
                    "label": 0
                },
                {
                    "sent": "So is what we learn at the end with the online learning on a budget.",
                    "label": 0
                },
                {
                    "sent": "We will learn later an the 2nd way to achieve this week dependence on the past is OK, this Alpha.",
                    "label": 0
                },
                {
                    "sent": "Related to the samples.",
                    "label": 0
                },
                {
                    "sent": "Is going to decay.",
                    "label": 0
                },
                {
                    "sent": "As the samples are older.",
                    "label": 0
                },
                {
                    "sent": "For instance, in this case, we can take a look to the paper of Kavalan teeth, Fabian Cajun, till there with a percent to 60% algorithm in the same paper, they present the random budget perception algorithm that we will see later in this paper.",
                    "label": 0
                },
                {
                    "sent": "They propose this weight in decay with decay.",
                    "label": 0
                },
                {
                    "sent": "So in case that we have some error, so the class label is not much does not match the sign of the perception rule.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to update the new weight.",
                    "label": 0
                },
                {
                    "sent": "Just with the same as we do with the perceptron, is additive model.",
                    "label": 0
                },
                {
                    "sent": "Alpha T is 1.",
                    "label": 0
                },
                {
                    "sent": "Voice of the XT that we're going to multiply the previous linear model with this factor.",
                    "label": 0
                },
                {
                    "sent": "1 minus Lambda sub KK nocti K because K is a counter of errors.",
                    "label": 0
                },
                {
                    "sent": "So when we have a new error, we incrementing one value K An.",
                    "label": 0
                },
                {
                    "sent": "This Lambda sub K is this expression Lambda divided by Lambda over Lambda plus K. Now we will see graphically what it means.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It means the following.",
                    "label": 0
                },
                {
                    "sent": "For instance, let's assume that we have K 100.",
                    "label": 0
                },
                {
                    "sent": "So at this time we have seen 100 errors.",
                    "label": 0
                },
                {
                    "sent": "This day last error and this is the value of Lambda for this last error.",
                    "label": 0
                },
                {
                    "sent": "And here we see different.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Values of Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here will we see is this expression.",
                    "label": 0
                },
                {
                    "sent": "This expression is let me except I they if sample with mistake.",
                    "label": 0
                },
                {
                    "sent": "The first sample with mistake.",
                    "label": 0
                },
                {
                    "sent": "The simple concept with mistake, not the first, the second sample, the first sample with Mistake II.",
                    "label": 0
                },
                {
                    "sent": "Something with mistake.",
                    "label": 0
                },
                {
                    "sent": "So this is the weight that this sample is going to receive.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why try to is very straightforward unwrap this.",
                    "label": 0
                },
                {
                    "sent": "And more, and then you get some multiplication of this value up to the first one.",
                    "label": 0
                },
                {
                    "sent": "OK, then if you unwrap this you get.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This relation.",
                    "label": 0
                },
                {
                    "sent": "Well so.",
                    "label": 0
                },
                {
                    "sent": "This means that the first sample here is going to have.",
                    "label": 0
                },
                {
                    "sent": "No one, but almost here .4.",
                    "label": 0
                },
                {
                    "sent": "Alpha the way, so under or POV.",
                    "label": 0
                },
                {
                    "sent": "This WT is a sum over only.",
                    "label": 0
                },
                {
                    "sent": "The last samples.",
                    "label": 0
                },
                {
                    "sent": "Is there a key issue with this?",
                    "label": 0
                },
                {
                    "sent": "Well, so in this plot you can see the different Alpha different weight.",
                    "label": 0
                },
                {
                    "sent": "Of the samples depending on Lambda and depending on the rank of these samples.",
                    "label": 0
                },
                {
                    "sent": "So all this for two newest.",
                    "label": 0
                },
                {
                    "sent": "OK, yes, so it's very straightforward to see this just unwrapping this expression over all the T. Well, any question at this time.",
                    "label": 0
                },
                {
                    "sent": "What is the axis?",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "So the axis here.",
                    "label": 0
                },
                {
                    "sent": "The X axis here.",
                    "label": 0
                },
                {
                    "sent": "Is why is the sorry I is the.",
                    "label": 0
                },
                {
                    "sent": "The sample with error so.",
                    "label": 0
                },
                {
                    "sent": "10 means that is the 10th sample with error first I have 20 samples but only 10 errors, so this last one is the sample with the 10th 10 sample with error, why?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "Only we apply this rule when we have an error.",
                    "label": 0
                },
                {
                    "sent": "K&K is counting the errors.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to this sample I.",
                    "label": 0
                },
                {
                    "sent": "We use this weight.",
                    "label": 0
                },
                {
                    "sent": "That depends on taking the corner, how many years we have 100 in this case means.",
                    "label": 0
                },
                {
                    "sent": "The value of I.",
                    "label": 0
                },
                {
                    "sent": "For instance, if I is 10, we have one minute Lambda Power 90.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be very low value, so the reason why we get this decay decay scheme.",
                    "label": 0
                },
                {
                    "sent": "If you have further questions, maybe later in the break we can.",
                    "label": 0
                },
                {
                    "sent": "Solve.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go on.",
                    "label": 0
                },
                {
                    "sent": "Up to this now, what we have learned is how to solve problems where we have a we have binary classification problems and what happens when we have to extend this is to multiclass problems.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "As I see you, I I told you before, capital M is the number of classes.",
                    "label": 0
                },
                {
                    "sent": "And the well known Kessler Construction is applied to extend binary problems to multiclass problems.",
                    "label": 0
                },
                {
                    "sent": "This Kessler construction works as follows.",
                    "label": 0
                },
                {
                    "sent": "Given some sample that lays in some dimensional space, this unique sample is transformed into M -- 1 samples.",
                    "label": 0
                },
                {
                    "sent": "And each one of these samples lays in a very high dimensional space because it's the original dimension plus the number of classes.",
                    "label": 0
                },
                {
                    "sent": "So this is just.",
                    "label": 0
                },
                {
                    "sent": "Literature is not use is usually under the practical point of view because for each sample you have to get more than one sample an the new samples.",
                    "label": 0
                },
                {
                    "sent": "Also laying very high dimensional space is useless, but it's practical from the point of view of we want to obtain some convergence proof.",
                    "label": 0
                },
                {
                    "sent": "For instance Nabokov I guess.",
                    "label": 0
                },
                {
                    "sent": "So who use it for demonstrate the convergence of the pattern for this multiclass problems?",
                    "label": 0
                },
                {
                    "sent": "Well, in a practical scenario we will see later maybe other order mappings, but I guess for the impractical scenario.",
                    "label": 0
                },
                {
                    "sent": "Whether we're going to do is the following.",
                    "label": 0
                },
                {
                    "sent": "So W is the weight vector of the model an instead of half.",
                    "label": 0
                },
                {
                    "sent": "This way better with the same dimensionality of the samples were going to have a new way better.",
                    "label": 0
                },
                {
                    "sent": "In this case, a matrix as you want, with emrose being and the number of classes and the columns in the original dimensions.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this approach is when we have a sample we want to multiply the sample by different roles depending on the class that we want to compute the margin we will see later worry.",
                    "label": 0
                },
                {
                    "sent": "So then, given a pair, we're going to compute Y as the row that give me the maximum value of this inner product.",
                    "label": 0
                },
                {
                    "sent": "So we classify to the class which row give the maximum value of this inner product.",
                    "label": 0
                },
                {
                    "sent": "And then if Y is different than the correct class between one and capital M is an error and then I will update the model.",
                    "label": 0
                },
                {
                    "sent": "We will select the hope of data model, but is that useful?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this way working with multi class.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Perceptions we have a family of additive multiclass algorithms.",
                    "label": 0
                },
                {
                    "sent": "Also conservative algorithms or ultraconservative algorithms that is proposed in this paper.",
                    "label": 0
                },
                {
                    "sent": "This paper, the same paper of the media algorithm soldier before so it's worth paper to read.",
                    "label": 0
                },
                {
                    "sent": "And then what we have is again given a pair XTYTYT now is a multiclass.",
                    "label": 1
                },
                {
                    "sent": "We compute the maximum.",
                    "label": 0
                },
                {
                    "sent": "And then if this maximum does not match with the correct label, we're going to update the model in the following way.",
                    "label": 0
                },
                {
                    "sent": "First, the row associated to the correct class is going to be this.",
                    "label": 0
                },
                {
                    "sent": "We are going to add some portion of the vector as we did before in the perception.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Bing R. Belongs to this set where this set is all the rows of this matrix that the inner product is larger than the inner product of the correct class.",
                    "label": 0
                },
                {
                    "sent": "So all these rows produce.",
                    "label": 0
                },
                {
                    "sent": "Better result than the correct class.",
                    "label": 0
                },
                {
                    "sent": "He has not.",
                    "label": 0
                },
                {
                    "sent": "Is not correct.",
                    "label": 0
                },
                {
                    "sent": "OK so we have to correct this in this case for all the rows that produce larger values of the of the inner product than the correct class.",
                    "label": 0
                },
                {
                    "sent": "So this means that when we maximize, we're not going to pick the correct class.",
                    "label": 0
                },
                {
                    "sent": "We're going to pick one of the incorrect classes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, are you lost or no OK?",
                    "label": 0
                },
                {
                    "sent": "So we're going to some also some constant multiply by the vector.",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to impose this constraint where the constant of the correct class has to be.",
                    "label": 0
                },
                {
                    "sent": "Minimize the sum of the constants of the other classes.",
                    "label": 0
                },
                {
                    "sent": "Clearly, for example, the perceptron, the constant is 1 for the correct plus minus one for another class.",
                    "label": 0
                },
                {
                    "sent": "This binary problem.",
                    "label": 0
                },
                {
                    "sent": "OK, then are the same.",
                    "label": 0
                },
                {
                    "sent": "For instance, some examples when.",
                    "label": 0
                },
                {
                    "sent": "R is the incorrect class.",
                    "label": 0
                },
                {
                    "sent": "We're going to subtract this value.",
                    "label": 0
                },
                {
                    "sent": "Why the value by the total number of elements in the set?",
                    "label": 1
                },
                {
                    "sent": "For the correct class, I went to some.",
                    "label": 0
                },
                {
                    "sent": "So are you going to some just the object like in their perception?",
                    "label": 0
                },
                {
                    "sent": "And zero another case.",
                    "label": 0
                },
                {
                    "sent": "So this model is conservative becausw.",
                    "label": 0
                },
                {
                    "sent": "It applies zero just in the case that.",
                    "label": 0
                },
                {
                    "sent": "And here, just in the case that there is no error, it does not move the weight matrix.",
                    "label": 0
                },
                {
                    "sent": "So one example, another example is this.",
                    "label": 0
                },
                {
                    "sent": "To use minus one for the maximum incorrect class and to use one for the correct class.",
                    "label": 0
                },
                {
                    "sent": "Also, this sum 10 always so.",
                    "label": 0
                },
                {
                    "sent": "Which of these models are the best?",
                    "label": 0
                },
                {
                    "sent": "Or whatever you did, you can think about you can get different models, so this is a family of models that what you have to do is just to impose this constraint.",
                    "label": 0
                },
                {
                    "sent": "Which of this model is the best?",
                    "label": 0
                },
                {
                    "sent": "So maybe is class dependent?",
                    "label": 0
                },
                {
                    "sent": "This model modifies all the incorrect class that are larger, are better than the correct.",
                    "label": 0
                },
                {
                    "sent": "This model only modify the largest incorrect class.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the authors proof that the number of errors of the online perceptron for the multiclass with this for this family.",
                    "label": 0
                },
                {
                    "sent": "Of algorithms is this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This R is the maximum norm of the samples.",
                    "label": 0
                },
                {
                    "sent": "This D is related to the square of the loss.",
                    "label": 0
                },
                {
                    "sent": "This is at the hinge loss, but for the multiclass problems.",
                    "label": 0
                },
                {
                    "sent": "An in particular there are some weight matrix that give the minimum, so the number of errors because this is for any for any matrix with norm one for any in particular for the minimum this bound is true.",
                    "label": 0
                },
                {
                    "sent": "So it seems a good a good bond.",
                    "label": 0
                },
                {
                    "sent": "OK here.",
                    "label": 0
                },
                {
                    "sent": "Thinking back on that we have also the square of this.",
                    "label": 0
                },
                {
                    "sent": "Gamma discovers like the hinge point of this multiclass hinge loss.",
                    "label": 0
                },
                {
                    "sent": "Hey so also this is just for your knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I prefer that you understand this, that all this family of of.",
                    "label": 0
                },
                {
                    "sent": "Off",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Orient provides a good bones of the error.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the same paper they provide different way of update.",
                    "label": 0
                },
                {
                    "sent": "The weight matrix.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and the diff.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chris, with respect to this is that here we only update the weight matrix if some error is produced.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm is conservative.",
                    "label": 0
                },
                {
                    "sent": "It doesn't move the way matrix if no error.",
                    "label": 0
                },
                {
                    "sent": "But he.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here they don't put this constraint because they say OK anyway.",
                    "label": 0
                },
                {
                    "sent": "So find the this Tao values that optimize this problem.",
                    "label": 0
                },
                {
                    "sent": "Minimize the the norm of this expression.",
                    "label": 0
                },
                {
                    "sent": "An this style is such a tool is lower than this Delta.",
                    "label": 0
                },
                {
                    "sent": "I will explain it there an the towel must sum 10.",
                    "label": 0
                },
                {
                    "sent": "So this Delta this value is 0 for the classes that are not the correct class is the Kronecker Delta.",
                    "label": 0
                },
                {
                    "sent": "Ann is 1 when the R is wise at the correct class, so we are going to search.",
                    "label": 0
                },
                {
                    "sent": "Dow's lower than 0.",
                    "label": 0
                },
                {
                    "sent": "4.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                },
                {
                    "sent": "Why is the incorrect class an DAU lower than one?",
                    "label": 0
                },
                {
                    "sent": "When I is the correct class an we impose this constraint.",
                    "label": 0
                },
                {
                    "sent": "And update is very straightforward for every for every class.",
                    "label": 0
                },
                {
                    "sent": "Update this.",
                    "label": 0
                },
                {
                    "sent": "So here the question is, if we remove this condition and also that the rule is for every class, are we going to move those classes that are lower than the correct class that are not problematic?",
                    "label": 0
                },
                {
                    "sent": "In this case, is not ultra conservative this model, because it's going to move things that they have not to move it.",
                    "label": 0
                },
                {
                    "sent": "And to update things so the author demonstrated yes, because this ministration problems find 0.",
                    "label": 0
                },
                {
                    "sent": "Zero for all, thous when no error is produced.",
                    "label": 0
                },
                {
                    "sent": "So we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're not going to move any weight.",
                    "label": 0
                },
                {
                    "sent": "So when this is false, this memorization problem, we can demonstrate the fine all day 1000 zero.",
                    "label": 0
                },
                {
                    "sent": "So we're not going to move any weight.",
                    "label": 0
                },
                {
                    "sent": "When this is true, some classes get larger values that the correct class.",
                    "label": 0
                },
                {
                    "sent": "Is going to put some value in there for the correct class.",
                    "label": 0
                },
                {
                    "sent": "Some value for incorrect class, but also zero for those classes that are not larger than the correct class.",
                    "label": 0
                },
                {
                    "sent": "So with this mechanism, media is still an ultraconservative algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is good news.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I hope.",
                    "label": 0
                },
                {
                    "sent": "Not to you.",
                    "label": 0
                },
                {
                    "sent": "Are you tired?",
                    "label": 0
                },
                {
                    "sent": "I can maybe is only three for a slice and we will make the break before the passive aggressive online learning.",
                    "label": 0
                },
                {
                    "sent": "Well and how we can extend this all this stuff to kernel perception, so it's.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A straight forward.",
                    "label": 0
                },
                {
                    "sent": "So the current perception is just a linear perceptron in the RKHS.",
                    "label": 0
                },
                {
                    "sent": "That's the referring kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So the perceptron model becomes a linear combination of kernels instead of a linear combination of.",
                    "label": 0
                },
                {
                    "sent": "Samples here.",
                    "label": 0
                },
                {
                    "sent": "An the key issue is that all past mistaken samples XD becomes super vectors.",
                    "label": 0
                },
                {
                    "sent": "So when we have 60 has a error, so the margin is lower than zero.",
                    "label": 0
                },
                {
                    "sent": "We add this sample as supervector.",
                    "label": 0
                },
                {
                    "sent": "We will see later.",
                    "label": 0
                },
                {
                    "sent": "And the number of support vectors then is not bounded in principle.",
                    "label": 0
                },
                {
                    "sent": "So they get that done that up to the last we have from one up to capital T we are.",
                    "label": 0
                },
                {
                    "sent": "We have this online procedure, but maybe capital T is infinite.",
                    "label": 0
                },
                {
                    "sent": "We receive samples continuously an for example that we have an error.",
                    "label": 0
                },
                {
                    "sent": "We had a support vector.",
                    "label": 0
                },
                {
                    "sent": "If the problem is very difficult, is not linearly separable and has a lot of outliers, we're going to add a lot of support vectors and at the end when we have received a new sample, what we have to do is to multiply the sample by the weight model.",
                    "label": 0
                },
                {
                    "sent": "But the way model is going to be here some kernel and for each new sample we have to compute the kernel against all the previous mistaken samples.",
                    "label": 0
                },
                {
                    "sent": "Unequal takes a lot of time, so it will be agreed here.",
                    "label": 0
                },
                {
                    "sent": "To bomb the number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "But what happens if we bond the number support vectors?",
                    "label": 0
                },
                {
                    "sent": "Maybe what we get is a very bad algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have to bond the Super vectors, but ensuring that we get a good algorithm that we're going to get lower number of arrows.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So remember the linear perception here, so why is the sign of this inner product an if we unwrap this?",
                    "label": 0
                },
                {
                    "sent": "With model we get this from all the previous samples.",
                    "label": 0
                },
                {
                    "sent": "Multiply the new sample excepte by the news.",
                    "label": 0
                },
                {
                    "sent": "The past samples multiplied by the weight you select one for the perception multiplied by the sample of this past.",
                    "label": 0
                },
                {
                    "sent": "The class label of this past samples.",
                    "label": 0
                },
                {
                    "sent": "So here we can put the kernel.",
                    "label": 0
                },
                {
                    "sent": "So what we are doing is simulate that.",
                    "label": 0
                },
                {
                    "sent": "So we do.",
                    "label": 0
                },
                {
                    "sent": "Is X principle laser not dimensional space?",
                    "label": 0
                },
                {
                    "sent": "So went to map.",
                    "label": 0
                },
                {
                    "sent": "This X into some feature space.",
                    "label": 0
                },
                {
                    "sent": "But we're not going to map it.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is here.",
                    "label": 0
                },
                {
                    "sent": "What do we have to have then, is this?",
                    "label": 0
                },
                {
                    "sent": "So after the map, what we have here is the inner product between two vectors that lay in this feature space and we're going to model this inner product with the kernel.",
                    "label": 0
                },
                {
                    "sent": "Of the vectors.",
                    "label": 0
                },
                {
                    "sent": "In original space, so the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "So now important thing is that the.",
                    "label": 0
                },
                {
                    "sent": "Office of either we put here can be seen as the importance of the weight of the support vector.",
                    "label": 0
                },
                {
                    "sent": "The support vectors going to support the boundary and depending on the weight is going to support a lot or support or weak support or the class.",
                    "label": 0
                },
                {
                    "sent": "Monday we will see later in some examples.",
                    "label": 0
                },
                {
                    "sent": "Anne, the good thing is that all the previous salaries that we have seen can be applied.",
                    "label": 0
                },
                {
                    "sent": "For instance, the media that we have seen before for the binary classification.",
                    "label": 0
                },
                {
                    "sent": "Whether we have to do is this use the kernel perception in the test phase an in the learning phase?",
                    "label": 0
                },
                {
                    "sent": "Alpha Supply is going to be this.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same that we have.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The four year.",
                    "label": 0
                },
                {
                    "sent": "OK, this is going to be exactly the same.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But of course, now this expression is not so trivial.",
                    "label": 0
                },
                {
                    "sent": "Product, but is this?",
                    "label": 0
                },
                {
                    "sent": "All this stuff.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we have to plug this into this and we get Alpha I and all is working and all the bonds that we prove that they prove are the same because what we have is just a linear model but in another space.",
                    "label": 0
                },
                {
                    "sent": "So the proofs are the same.",
                    "label": 0
                }
            ]
        }
    }
}