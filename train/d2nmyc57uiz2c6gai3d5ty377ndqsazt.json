{
    "id": "d2nmyc57uiz2c6gai3d5ty377ndqsazt",
    "title": "The Learning Behind the Gmail Priority Inbox",
    "info": {
        "author": [
            "Douglas Aberdeen, National ICT Australia"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_aberdeen_lbg/",
    "segmentation": [
        [
            "Thanks everyone, so this is very much an applications of LCC.",
            "See paper.",
            "This is the priority inbox.",
            "And let's see this laser pointer.",
            "Let me just show you what the point is.",
            "We're trying to show you the Mail that might be important or interesting to you in the top section first.",
            "So it's basically from the users point of view.",
            "Binary classification of important stuff, not important stuff and stuff that you start in.",
            "There is a manual action obviously, so this is a copy of my inbox coming up to Nip.",
            "So this stuff is important if you hover over the important task here, we actually give you a hint about why we classified as important or not important and if it makes mistakes so you can correct it.",
            "But the idea is actually that it learns without any manual training actions at all.",
            "You can try it out today with launched about four months ago.",
            "Go to the Settings tab in Gmail for your Gmail user and turn it on there.",
            "This is joint work with myself, Andre, who's in the audience over there looking sleepy and Andrew Slater.",
            "So this other surface looks very much similar to the spam problem and I came from the spam team in Gmail and the key difference is that the protein box is highly personalized.",
            "So in fact what we're doing is learning a stochastic logistic regression model for every user of the Priority Inbox."
        ],
        [
            "The learning is this is using passive aggressive updates.",
            "We modified it slightly for logistic regression.",
            "There isn't anything too surprising here.",
            "I think that maybe the main message is what we do to determine the ground truth of importance for a message is we look at how you interact with that message over the few days after the delivery of the message.",
            "So for example, anything that you read reply to will make the message important.",
            "And if you don't, if you don't.",
            "Log into Gmail for example, and deliver messages.",
            "We won't do any updates."
        ],
        [
            "We try and guarantee that you've seen the message.",
            "So the first little trick is that we have too much data globally, but we have not enough data per user to learn a good model.",
            "So we do a very simple form of transfer learning, so we learn a global model of importance and then for each user we fix the global model and we do updates which end up representing the model ends up representing how different you are from, say, the global model of importance.",
            "So we can learn, for example, that you care more about certain you care more about the content of the message versus.",
            "The thread based features, or you care more about."
        ],
        [
            "The social features and the content based features.",
            "So the interesting part from this conference.",
            "This workshop's point of view, is how we actually scale up.",
            "So the challenges is not to learn on a lot of data for a single model.",
            "The challenges to learn for we had to prove that we could scale up to all the Gmail users, so potentially hundreds of millions of users.",
            "So the problem is how to learn hundreds of millions of very simple models, which is equivalent to one model with potentially 10s of billions of features, right?",
            "So we use big table.",
            "And we don't use MapReduce.",
            "I can explain why we don't use MapReduce offline.",
            "A lot of the problems have been talked about already, so in map reduce you end up shuffling a lot of data to disk and back again.",
            "Also, each user model represents a form of state and MapReduce doesn't work well with state.",
            "Although there are ways around that.",
            "So we try and load as many user models into memory as possible, which means fetching all these user models from big table over the network.",
            "Because we have to figure out when the last time a user was active in Gmail to prove that they could have seen a male.",
            "We actually have to pass over all the users data to figure out when their last active in another pass for learning, and that's the first pass there in the second pass for updating the models.",
            "Then we write back all of the updated models.",
            "So this is very much a hand custom built infrastructure for learning and then we can parallelize that over cores and overtime.",
            "So we end up with about 100K users per Shard.",
            "We do about 20 to 30 K features."
        ],
        [
            "Per second per core is about 200 features.",
            "Plus some personal features per per model.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks everyone, so this is very much an applications of LCC.",
                    "label": 0
                },
                {
                    "sent": "See paper.",
                    "label": 0
                },
                {
                    "sent": "This is the priority inbox.",
                    "label": 1
                },
                {
                    "sent": "And let's see this laser pointer.",
                    "label": 0
                },
                {
                    "sent": "Let me just show you what the point is.",
                    "label": 0
                },
                {
                    "sent": "We're trying to show you the Mail that might be important or interesting to you in the top section first.",
                    "label": 0
                },
                {
                    "sent": "So it's basically from the users point of view.",
                    "label": 0
                },
                {
                    "sent": "Binary classification of important stuff, not important stuff and stuff that you start in.",
                    "label": 0
                },
                {
                    "sent": "There is a manual action obviously, so this is a copy of my inbox coming up to Nip.",
                    "label": 0
                },
                {
                    "sent": "So this stuff is important if you hover over the important task here, we actually give you a hint about why we classified as important or not important and if it makes mistakes so you can correct it.",
                    "label": 0
                },
                {
                    "sent": "But the idea is actually that it learns without any manual training actions at all.",
                    "label": 1
                },
                {
                    "sent": "You can try it out today with launched about four months ago.",
                    "label": 0
                },
                {
                    "sent": "Go to the Settings tab in Gmail for your Gmail user and turn it on there.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with myself, Andre, who's in the audience over there looking sleepy and Andrew Slater.",
                    "label": 0
                },
                {
                    "sent": "So this other surface looks very much similar to the spam problem and I came from the spam team in Gmail and the key difference is that the protein box is highly personalized.",
                    "label": 0
                },
                {
                    "sent": "So in fact what we're doing is learning a stochastic logistic regression model for every user of the Priority Inbox.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The learning is this is using passive aggressive updates.",
                    "label": 0
                },
                {
                    "sent": "We modified it slightly for logistic regression.",
                    "label": 1
                },
                {
                    "sent": "There isn't anything too surprising here.",
                    "label": 0
                },
                {
                    "sent": "I think that maybe the main message is what we do to determine the ground truth of importance for a message is we look at how you interact with that message over the few days after the delivery of the message.",
                    "label": 1
                },
                {
                    "sent": "So for example, anything that you read reply to will make the message important.",
                    "label": 0
                },
                {
                    "sent": "And if you don't, if you don't.",
                    "label": 0
                },
                {
                    "sent": "Log into Gmail for example, and deliver messages.",
                    "label": 0
                },
                {
                    "sent": "We won't do any updates.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We try and guarantee that you've seen the message.",
                    "label": 0
                },
                {
                    "sent": "So the first little trick is that we have too much data globally, but we have not enough data per user to learn a good model.",
                    "label": 1
                },
                {
                    "sent": "So we do a very simple form of transfer learning, so we learn a global model of importance and then for each user we fix the global model and we do updates which end up representing the model ends up representing how different you are from, say, the global model of importance.",
                    "label": 0
                },
                {
                    "sent": "So we can learn, for example, that you care more about certain you care more about the content of the message versus.",
                    "label": 0
                },
                {
                    "sent": "The thread based features, or you care more about.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The social features and the content based features.",
                    "label": 0
                },
                {
                    "sent": "So the interesting part from this conference.",
                    "label": 0
                },
                {
                    "sent": "This workshop's point of view, is how we actually scale up.",
                    "label": 0
                },
                {
                    "sent": "So the challenges is not to learn on a lot of data for a single model.",
                    "label": 0
                },
                {
                    "sent": "The challenges to learn for we had to prove that we could scale up to all the Gmail users, so potentially hundreds of millions of users.",
                    "label": 0
                },
                {
                    "sent": "So the problem is how to learn hundreds of millions of very simple models, which is equivalent to one model with potentially 10s of billions of features, right?",
                    "label": 0
                },
                {
                    "sent": "So we use big table.",
                    "label": 0
                },
                {
                    "sent": "And we don't use MapReduce.",
                    "label": 0
                },
                {
                    "sent": "I can explain why we don't use MapReduce offline.",
                    "label": 0
                },
                {
                    "sent": "A lot of the problems have been talked about already, so in map reduce you end up shuffling a lot of data to disk and back again.",
                    "label": 0
                },
                {
                    "sent": "Also, each user model represents a form of state and MapReduce doesn't work well with state.",
                    "label": 0
                },
                {
                    "sent": "Although there are ways around that.",
                    "label": 0
                },
                {
                    "sent": "So we try and load as many user models into memory as possible, which means fetching all these user models from big table over the network.",
                    "label": 0
                },
                {
                    "sent": "Because we have to figure out when the last time a user was active in Gmail to prove that they could have seen a male.",
                    "label": 0
                },
                {
                    "sent": "We actually have to pass over all the users data to figure out when their last active in another pass for learning, and that's the first pass there in the second pass for updating the models.",
                    "label": 0
                },
                {
                    "sent": "Then we write back all of the updated models.",
                    "label": 0
                },
                {
                    "sent": "So this is very much a hand custom built infrastructure for learning and then we can parallelize that over cores and overtime.",
                    "label": 0
                },
                {
                    "sent": "So we end up with about 100K users per Shard.",
                    "label": 1
                },
                {
                    "sent": "We do about 20 to 30 K features.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Per second per core is about 200 features.",
                    "label": 0
                },
                {
                    "sent": "Plus some personal features per per model.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}