{
    "id": "kmpxxr3wawsprdmiewchhykg5ql3ddbo",
    "title": "Activity Forecasting",
    "info": {
        "author": [
            "Kris M. Kitani, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "chairman": [
            "Michael J. Black, Max Planck Institute for Intelligent Systems, Max Planck Institute",
            "Ivan Laptev, INRIA - The French National Institute for Research in Computer Science and Control"
        ],
        "published": "Nov. 12, 2012",
        "recorded": "October 2012",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Semantic Computing"
        ]
    },
    "url": "http://videolectures.net/eccv2012_kitani_forecasting/",
    "segmentation": [
        [
            "So today I'll be talking about activity forecasting and the best way to understand or explain activity forecasting is."
        ],
        [
            "Compared against activity recognition.",
            "So in activity recognition we're concerned with making reference or making inference about past."
        ],
        [
            "Observed actions.",
            "And in contrast, in activity forecasting, what we want to do?"
        ],
        [
            "As we want to make inference about future an yet UN observed actions.",
            "So let me start with an illustration."
        ],
        [
            "So let's say we're giving this novel scene.",
            "We've never seen it before, and we've."
        ],
        [
            "Just detected a person who's about to walk to the scene an just for the sake of this example, I'm going to assume that the destination is known.",
            "So what activity forecast?"
        ],
        [
            "I'm trying to do is to answer this question what path will he take so without even seeing one path through this scene, what we want to do is predict a distribution over all possible future paths, and this distribution should should reflect the natural interaction of people in the scene.",
            "So this is an exam."
        ],
        [
            "Apple of the output of our algorithm.",
            "And there are some things that we can pick up from this distribution.",
            "So first of all, the color is the distribution over the future trajectories.",
            "There are obvious interactions that are being captured here, like avoiding the car, but there's also really subtle interactions.",
            "So for example, the person is.",
            "If you look at the distribution.",
            "The shortest path would be to go straight to the white car and go straight to the goal.",
            "But what this distribution is saying is saying let's go to the sidewalk early and that's because it's learn this from the training data.",
            "It's also learning these subtle interactions like the tendency of a person to walk near curb when there's one nearby.",
            "OK, so."
        ],
        [
            "We frame this as a machine learning problem.",
            "This is what we have.",
            "We have demonstrated activities through a scene and then we have physical features that have been extracted from that scene and we want to use these two mode."
        ],
        [
            "Loads of information to learn a person's decision process and this model is going to be something that's not tide to a specific scene and at Test time."
        ],
        [
            "Will have a new scene.",
            "Will extract the physical features and then we're going to forecast all the different activities that could happen in that scene."
        ],
        [
            "OK, so the approach that we propose in our work will work generally for any kind of temporal sequence of activity, but for the as a proof of concept, we're limiting our focus to trajectory based activity analysis.",
            "So."
        ],
        [
            "The use of trajectory dynamics has been by far the most popular way of modelling human activities for the last 20 years, Ann.",
            "Usually you assume that you have persistent surveillance scenario and with a camera you'll watch the motion of the people and then based on the motion statistics, you're going to learn and recognize different kinds of activities.",
            "Well, sometimes one of the drawbacks is a model that you learn in one model or in one scene can't be immediately applied to and using."
        ],
        [
            "There are also approaches that use social dynamics and the idea here is to model the motion of the people surrounding a person and how that's going to affect a person's trajectory through a scene.",
            "So one of the good things about these methods is that since it's modeling human human interaction, you can take these models the new scenes and they generalize really well.",
            "Up"
        ],
        [
            "There's also work in robotics where they use motion planning to describe human activities, and the idea is straightforward of person has a goal that they want to get to, and they're going to think of an optimal path in such a way that they're going to avoid the obstacles in the scene.",
            "So a key point about these approaches is that they use a decision theoretic model, and later on we're going to show that these kind of models help us to build richer models of activity and also they're going to help us build models that are not seen specific."
        ],
        [
            "So one of the common kind of attributes of a lot of the previous work is that they tend to have a."
        ],
        [
            "Reduced representation of the physical world.",
            "But"
        ],
        [
            "Really, the physical world is more than just obstacles or just people.",
            "It's."
        ],
        [
            "Really filled with a lot of information and I think in computer vision we've come to a point where we're really being able to like understand the scene a lot better now so we can understand the scene.",
            "This is going to give us a lot of information about how person is going to interact with the scene.",
            "So what?"
        ],
        [
            "We propose to do is the model human activity by taking into account all of the physical features of the scene we're going to incorporate in incorporate it into a decision theoretic framework, and we're going to use it to forecast activities.",
            "So let me explain what this decision theoretic framework is."
        ],
        [
            "So again, the best way to kind of understand this is to contrast this to a dynamics based model.",
            "So in a dynamics based model, we want to learn the transition from one state to another state.",
            "And this model is often called a motion model, so emotional model what?"
        ],
        [
            "Try to do is model.",
            "Sorry how a person.",
            "Performs activities in a scene.",
            "So in contrast, the dynamics model."
        ],
        [
            "There is the decision based model and what this model tries to do is model YA.",
            "Person chooses to take certain actions.",
            "OK, so this is quantified with the value of P of a given X, so this is the probability of taking an action given some state that I'm in.",
            "And this is called policy and this is very well studied in reinforcement learning and in robotics.",
            "And in a very important point about the decision based models is that they explicitly take the future into account.",
            "How does the future effect the actions that I take now?"
        ],
        [
            "So we're going to take a decision theoretic approach, and we're going to model human activity using a Markov decision process or an MDP.",
            "So an activity sequence generated by an MDP is going to be a temporal sequence of triplets, where each triplet isn't."
        ],
        [
            "Action State and a reward.",
            "So in our context for trajectory based activity analysis X or sorry, the state is going to be the position, so XY position 2D the action is going to be the velocity of the person.",
            "At whatever velocity they choose at that time, and the reward is going to be a single numeric value that they they receive by entering into some state, and I'll talk a little bit more about this later."
        ],
        [
            "Now this sequence is completely determined by the policy when the transition dynamics of the MDP or deterministic.",
            "So in other words, if we have the policy, the policy describes a distribution over all valid activity sequences."
        ],
        [
            "To compute the policy, we use the maximum entropy framework of zebra at all from 2008 Ann.",
            "It's computed as the exponentiated some of the immediate reward.",
            "Just mark in green and the expected future payoff.",
            "How much am I going to get in the future by taking this action?",
            "OK.",
            "So in another intuitive way to understand this is a rational agent is going to choose an action that gives a greater reward.",
            "That's what this equation is saying."
        ],
        [
            "So if now the policy is also completely determined by the reward function.",
            "So if we're given a reward function, we can put this through the maximum entropy value iteration algorithm and it's going to use dynamic programming and compute these values V. This is called the soft value function.",
            "And that's how the algorithm algorithm would proceed.",
            "So to kind of summarize."
        ],
        [
            "If we have the reward function here are defined.",
            "We can compute the policy and if we can compute the policy, we can generate valid activity sequences.",
            "But the problem here is.",
            "We don't know what the reward function is.",
            "We only have the observations of the activities.",
            "So what we want to do?"
        ],
        [
            "Is infer the reward function from the observed sequences OK, so let me take a step back and just make sure we're all on the same page.",
            "This is what we want to do.",
            "In the training data, if we see a person always avoiding a car.",
            "Then what we want to do is for pixels that are occupied by a car, we want the reward function to be the reward to be really low.",
            "And if in the training data of person is always using the sidewalk, what we want is for pixels that occupy the sidewalk.",
            "We want the reward to be really high.",
            "That's what we're trying to do through this learning process."
        ],
        [
            "So this is how we parameterize the reward function.",
            "The road function is going to be a linear function of the weights Theta times the physical features F. So again, to be concrete, these."
        ],
        [
            "Physical features are going to be based off of the output of a sea of semantic scene segmentation algorithm.",
            "So we have some examples here, so we're using the method of minnows at all from 2010, and it's quite robust.",
            "So if you ever get a chance to use it, great algorithm.",
            "Here's an example of the response of a car.",
            "The car region detection of parking lot detection, grass detection.",
            "We also have features that take into account distance, so how far am I from the wall?",
            "How far am I from the curb from the grass, etc.",
            "Altogether we have about 40 features.",
            "OK, so we have these features and the Theta is the thetas.",
            "Tell us how much each feature is going to contribute to the reward.",
            "So if some feature is useless then the data will be really small.",
            "It will be ignored if it's really important, like a car, a sidewalk, the value will go up.",
            "OK."
        ],
        [
            "So again, we want to learn the weights of the reward function in order to learn the reward function, because it's parameterized by the thetas.",
            "Right now the physical features are now visualized as topological Maps, so it's just the same thing as before, and the way we're going to learn these weights is we're going to do something."
        ],
        [
            "Inverse optimal control.",
            "Now this is something that's very well studied in reinforcement learning.",
            "An in control theory.",
            "And the idea is given inputs which are the trajectories, so these green squiggly lines here.",
            "So they could be trajectories and the feature responses, which are the topological Maps here, given those two inputs, we're going to have a gradient descent algorithm.",
            "That's going to learn the status of the reward function.",
            "Right, and it's going to adjust the Theta so that explains the demonstrated activity the best.",
            "So."
        ],
        [
            "Once we've learned the thetas of the reward function, now we could.",
            "We are given you seen we could compute the reward function which looks like this.",
            "And with the reward function, remember we can now compute the policy and with the Paula."
        ],
        [
            "See, we cannot use that to propagate the problem probability through the scene of where the paths are going to go an this distribution is going to take into account all of the physical features of the scene.",
            "In the paper."
        ],
        [
            "We've also developed something called Destination Forecasting, so the examples up to now the destination is known, but in general the destination is not known.",
            "So in destination forecasting we incorporate the output of a noisy tracker into our algorithm and use that update probability distributions over the goals.",
            "So first we start out with a flat multi goal distribution that would like this, and then as the observations come in, you're going to see the.",
            "Distribution over those being updated.",
            "So what this is doing is, as the person starts walking, it's saying that this is where the person is going to go."
        ],
        [
            "OK, so as in any method an important feature to have is good performance on new data.",
            "So we performed tests on two scenes, two different scenes, 92 videos and as baselines we compare against the maximum entropy Markov model which looks at local area features and tries to find the best action and then also the Markov motion model which takes prior motion statistics and uses that to find the best action.",
            "As a metric, we use the negative log loss, which is a probabilistic measure of performance and the modified household for distance, which is more of a physical measure of performance.",
            "So the modified household distance is going to be the Euclidean distance of a demonstrated activity, so observed activity and then that distance to a many sampled trajectories from our distribution.",
            "And we're going to allow for some temporal misalignment.",
            "So if this distance is small, that's good.",
            "If it's big, it means we're very far away from the from the observed, or from the demonstrated activity.",
            "OK, so we have a test here where we train on a scene A and then we test on scene B and then we have the opposite.",
            "Over here we train on scene B and then test on scene A.",
            "So we can see that we."
        ],
        [
            "Outperform the baselines here and as a reference, I'm going to put the perform."
        ],
        [
            "Of our model, both trained and tested on the same scene, not on the same data, while on the same scene, so the test and train is different and what you can see here is that the performance is consistent, right?",
            "You could swap out the trained model and test it on the same scene and the performance is pretty much the same here."
        ],
        [
            "So what this is telling us is that we have robust performance to new scenes.",
            "We generalize well.",
            "Finally, I'm going to end with some qualitative results."
        ],
        [
            "So we trained on the parking lot sequences, seen A and we use those parameters and now we're just applying them to Internet images and other scenes from the pirate data set and you can see that the forecasted distributions are quite reasonable.",
            "Thank you very much.",
            "Can you go back a few flies before you like the one slide before you did the performance?",
            "Baby I missed it destination for this one number one how far?",
            "OK, so the way the destination fork."
        ],
        [
            "Thing works is you just assume that you have a uniform distribution over all the goals first.",
            "And then as the observations come in from the tracker, we update."
        ],
        [
            "The distribution or the posterior distribution over the goals.",
            "So the more yeah.",
            "So then in the next slide.",
            "This."
        ],
        [
            "This picture so that is the output of your optimization before you do the time sequence.",
            "Yeah, this is the output of yeah destination forecasting.",
            "My question is, are you assuming that your state is fully observable?",
            "In particular the direction of the motion.",
            "Let's see so in the pure forecasting case, there are no observations, so there's no concept of observed or unobserved.",
            "But in this state, yeah for the state.",
            "But for the destination forecasting.",
            "And correct me if I'm answering this incorrectly for the destination forecasting, we assume that the true state is hidden because there's noise in the tracker observations.",
            "Did that answer your question?",
            "So I assume that it's fully accelerated.",
            "You're not using any Model 2, two model, like the uncertainty in the state.",
            "In the tracking case.",
            "In the tracking case, we are taking into account the uncertainty in the tracker observations, so there is.",
            "It's yes it's taking into account the uncertainty there.",
            "Bro, you're still using an MDP model.",
            "Yes, we're using MDP with a hidden layer on top, thank you.",
            "I may have missed one thing, but assuming you make a forecast, you're making a forecast based on the current single state.",
            "We're using history.",
            "Yeah, So what we have is input for forecasting is the all the features of the scene plus one location where the person is.",
            "So let's just think it's actually important to look at the previous 10 frames 'cause you know that we're person walked.",
            "It makes prediction markets much more accurate as opposed to single state.",
            "Yeah, so I guess the scenario would be you have an image and a person just comes in right at the edge of the image and then your forecast.",
            "In this case he is kind of in the middle, but that's how you would use it, right?",
            "Don't you also need to forecast what everybody else is going to do in the scene like and forecast how the person is going to respond to what everybody else is going to do in the scene is that's an excellent point.",
            "So this method assumes that the world is completely static, so nothing is moving so.",
            "But I mean that would definitely be a next step to take.",
            "This work is now take into account the give and take between the environment an agent, but it gets quite complex if you try to do it correctly, because then it becomes a game theory problem.",
            "Where you have to take into account another person's policy and it gets quite complex really quickly.",
            "Alright cool, that's very exciting work.",
            "Thanks again thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today I'll be talking about activity forecasting and the best way to understand or explain activity forecasting is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compared against activity recognition.",
                    "label": 0
                },
                {
                    "sent": "So in activity recognition we're concerned with making reference or making inference about past.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Observed actions.",
                    "label": 0
                },
                {
                    "sent": "And in contrast, in activity forecasting, what we want to do?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we want to make inference about future an yet UN observed actions.",
                    "label": 0
                },
                {
                    "sent": "So let me start with an illustration.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say we're giving this novel scene.",
                    "label": 0
                },
                {
                    "sent": "We've never seen it before, and we've.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just detected a person who's about to walk to the scene an just for the sake of this example, I'm going to assume that the destination is known.",
                    "label": 0
                },
                {
                    "sent": "So what activity forecast?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm trying to do is to answer this question what path will he take so without even seeing one path through this scene, what we want to do is predict a distribution over all possible future paths, and this distribution should should reflect the natural interaction of people in the scene.",
                    "label": 0
                },
                {
                    "sent": "So this is an exam.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apple of the output of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "And there are some things that we can pick up from this distribution.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the color is the distribution over the future trajectories.",
                    "label": 0
                },
                {
                    "sent": "There are obvious interactions that are being captured here, like avoiding the car, but there's also really subtle interactions.",
                    "label": 0
                },
                {
                    "sent": "So for example, the person is.",
                    "label": 0
                },
                {
                    "sent": "If you look at the distribution.",
                    "label": 0
                },
                {
                    "sent": "The shortest path would be to go straight to the white car and go straight to the goal.",
                    "label": 0
                },
                {
                    "sent": "But what this distribution is saying is saying let's go to the sidewalk early and that's because it's learn this from the training data.",
                    "label": 0
                },
                {
                    "sent": "It's also learning these subtle interactions like the tendency of a person to walk near curb when there's one nearby.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We frame this as a machine learning problem.",
                    "label": 0
                },
                {
                    "sent": "This is what we have.",
                    "label": 0
                },
                {
                    "sent": "We have demonstrated activities through a scene and then we have physical features that have been extracted from that scene and we want to use these two mode.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loads of information to learn a person's decision process and this model is going to be something that's not tide to a specific scene and at Test time.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will have a new scene.",
                    "label": 0
                },
                {
                    "sent": "Will extract the physical features and then we're going to forecast all the different activities that could happen in that scene.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the approach that we propose in our work will work generally for any kind of temporal sequence of activity, but for the as a proof of concept, we're limiting our focus to trajectory based activity analysis.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The use of trajectory dynamics has been by far the most popular way of modelling human activities for the last 20 years, Ann.",
                    "label": 0
                },
                {
                    "sent": "Usually you assume that you have persistent surveillance scenario and with a camera you'll watch the motion of the people and then based on the motion statistics, you're going to learn and recognize different kinds of activities.",
                    "label": 0
                },
                {
                    "sent": "Well, sometimes one of the drawbacks is a model that you learn in one model or in one scene can't be immediately applied to and using.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are also approaches that use social dynamics and the idea here is to model the motion of the people surrounding a person and how that's going to affect a person's trajectory through a scene.",
                    "label": 0
                },
                {
                    "sent": "So one of the good things about these methods is that since it's modeling human human interaction, you can take these models the new scenes and they generalize really well.",
                    "label": 0
                },
                {
                    "sent": "Up",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's also work in robotics where they use motion planning to describe human activities, and the idea is straightforward of person has a goal that they want to get to, and they're going to think of an optimal path in such a way that they're going to avoid the obstacles in the scene.",
                    "label": 0
                },
                {
                    "sent": "So a key point about these approaches is that they use a decision theoretic model, and later on we're going to show that these kind of models help us to build richer models of activity and also they're going to help us build models that are not seen specific.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the common kind of attributes of a lot of the previous work is that they tend to have a.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reduced representation of the physical world.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, the physical world is more than just obstacles or just people.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really filled with a lot of information and I think in computer vision we've come to a point where we're really being able to like understand the scene a lot better now so we can understand the scene.",
                    "label": 0
                },
                {
                    "sent": "This is going to give us a lot of information about how person is going to interact with the scene.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We propose to do is the model human activity by taking into account all of the physical features of the scene we're going to incorporate in incorporate it into a decision theoretic framework, and we're going to use it to forecast activities.",
                    "label": 0
                },
                {
                    "sent": "So let me explain what this decision theoretic framework is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, the best way to kind of understand this is to contrast this to a dynamics based model.",
                    "label": 0
                },
                {
                    "sent": "So in a dynamics based model, we want to learn the transition from one state to another state.",
                    "label": 0
                },
                {
                    "sent": "And this model is often called a motion model, so emotional model what?",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try to do is model.",
                    "label": 0
                },
                {
                    "sent": "Sorry how a person.",
                    "label": 0
                },
                {
                    "sent": "Performs activities in a scene.",
                    "label": 0
                },
                {
                    "sent": "So in contrast, the dynamics model.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is the decision based model and what this model tries to do is model YA.",
                    "label": 0
                },
                {
                    "sent": "Person chooses to take certain actions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is quantified with the value of P of a given X, so this is the probability of taking an action given some state that I'm in.",
                    "label": 0
                },
                {
                    "sent": "And this is called policy and this is very well studied in reinforcement learning and in robotics.",
                    "label": 0
                },
                {
                    "sent": "And in a very important point about the decision based models is that they explicitly take the future into account.",
                    "label": 0
                },
                {
                    "sent": "How does the future effect the actions that I take now?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to take a decision theoretic approach, and we're going to model human activity using a Markov decision process or an MDP.",
                    "label": 0
                },
                {
                    "sent": "So an activity sequence generated by an MDP is going to be a temporal sequence of triplets, where each triplet isn't.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action State and a reward.",
                    "label": 0
                },
                {
                    "sent": "So in our context for trajectory based activity analysis X or sorry, the state is going to be the position, so XY position 2D the action is going to be the velocity of the person.",
                    "label": 0
                },
                {
                    "sent": "At whatever velocity they choose at that time, and the reward is going to be a single numeric value that they they receive by entering into some state, and I'll talk a little bit more about this later.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this sequence is completely determined by the policy when the transition dynamics of the MDP or deterministic.",
                    "label": 0
                },
                {
                    "sent": "So in other words, if we have the policy, the policy describes a distribution over all valid activity sequences.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To compute the policy, we use the maximum entropy framework of zebra at all from 2008 Ann.",
                    "label": 0
                },
                {
                    "sent": "It's computed as the exponentiated some of the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "Just mark in green and the expected future payoff.",
                    "label": 1
                },
                {
                    "sent": "How much am I going to get in the future by taking this action?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in another intuitive way to understand this is a rational agent is going to choose an action that gives a greater reward.",
                    "label": 0
                },
                {
                    "sent": "That's what this equation is saying.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if now the policy is also completely determined by the reward function.",
                    "label": 1
                },
                {
                    "sent": "So if we're given a reward function, we can put this through the maximum entropy value iteration algorithm and it's going to use dynamic programming and compute these values V. This is called the soft value function.",
                    "label": 0
                },
                {
                    "sent": "And that's how the algorithm algorithm would proceed.",
                    "label": 0
                },
                {
                    "sent": "So to kind of summarize.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we have the reward function here are defined.",
                    "label": 0
                },
                {
                    "sent": "We can compute the policy and if we can compute the policy, we can generate valid activity sequences.",
                    "label": 0
                },
                {
                    "sent": "But the problem here is.",
                    "label": 0
                },
                {
                    "sent": "We don't know what the reward function is.",
                    "label": 1
                },
                {
                    "sent": "We only have the observations of the activities.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is infer the reward function from the observed sequences OK, so let me take a step back and just make sure we're all on the same page.",
                    "label": 1
                },
                {
                    "sent": "This is what we want to do.",
                    "label": 0
                },
                {
                    "sent": "In the training data, if we see a person always avoiding a car.",
                    "label": 0
                },
                {
                    "sent": "Then what we want to do is for pixels that are occupied by a car, we want the reward function to be the reward to be really low.",
                    "label": 0
                },
                {
                    "sent": "And if in the training data of person is always using the sidewalk, what we want is for pixels that occupy the sidewalk.",
                    "label": 0
                },
                {
                    "sent": "We want the reward to be really high.",
                    "label": 0
                },
                {
                    "sent": "That's what we're trying to do through this learning process.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is how we parameterize the reward function.",
                    "label": 0
                },
                {
                    "sent": "The road function is going to be a linear function of the weights Theta times the physical features F. So again, to be concrete, these.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Physical features are going to be based off of the output of a sea of semantic scene segmentation algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we have some examples here, so we're using the method of minnows at all from 2010, and it's quite robust.",
                    "label": 0
                },
                {
                    "sent": "So if you ever get a chance to use it, great algorithm.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of the response of a car.",
                    "label": 0
                },
                {
                    "sent": "The car region detection of parking lot detection, grass detection.",
                    "label": 0
                },
                {
                    "sent": "We also have features that take into account distance, so how far am I from the wall?",
                    "label": 0
                },
                {
                    "sent": "How far am I from the curb from the grass, etc.",
                    "label": 0
                },
                {
                    "sent": "Altogether we have about 40 features.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have these features and the Theta is the thetas.",
                    "label": 0
                },
                {
                    "sent": "Tell us how much each feature is going to contribute to the reward.",
                    "label": 0
                },
                {
                    "sent": "So if some feature is useless then the data will be really small.",
                    "label": 0
                },
                {
                    "sent": "It will be ignored if it's really important, like a car, a sidewalk, the value will go up.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, we want to learn the weights of the reward function in order to learn the reward function, because it's parameterized by the thetas.",
                    "label": 0
                },
                {
                    "sent": "Right now the physical features are now visualized as topological Maps, so it's just the same thing as before, and the way we're going to learn these weights is we're going to do something.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inverse optimal control.",
                    "label": 0
                },
                {
                    "sent": "Now this is something that's very well studied in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "An in control theory.",
                    "label": 0
                },
                {
                    "sent": "And the idea is given inputs which are the trajectories, so these green squiggly lines here.",
                    "label": 0
                },
                {
                    "sent": "So they could be trajectories and the feature responses, which are the topological Maps here, given those two inputs, we're going to have a gradient descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's going to learn the status of the reward function.",
                    "label": 1
                },
                {
                    "sent": "Right, and it's going to adjust the Theta so that explains the demonstrated activity the best.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once we've learned the thetas of the reward function, now we could.",
                    "label": 1
                },
                {
                    "sent": "We are given you seen we could compute the reward function which looks like this.",
                    "label": 0
                },
                {
                    "sent": "And with the reward function, remember we can now compute the policy and with the Paula.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See, we cannot use that to propagate the problem probability through the scene of where the paths are going to go an this distribution is going to take into account all of the physical features of the scene.",
                    "label": 0
                },
                {
                    "sent": "In the paper.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've also developed something called Destination Forecasting, so the examples up to now the destination is known, but in general the destination is not known.",
                    "label": 0
                },
                {
                    "sent": "So in destination forecasting we incorporate the output of a noisy tracker into our algorithm and use that update probability distributions over the goals.",
                    "label": 1
                },
                {
                    "sent": "So first we start out with a flat multi goal distribution that would like this, and then as the observations come in, you're going to see the.",
                    "label": 0
                },
                {
                    "sent": "Distribution over those being updated.",
                    "label": 0
                },
                {
                    "sent": "So what this is doing is, as the person starts walking, it's saying that this is where the person is going to go.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so as in any method an important feature to have is good performance on new data.",
                    "label": 0
                },
                {
                    "sent": "So we performed tests on two scenes, two different scenes, 92 videos and as baselines we compare against the maximum entropy Markov model which looks at local area features and tries to find the best action and then also the Markov motion model which takes prior motion statistics and uses that to find the best action.",
                    "label": 1
                },
                {
                    "sent": "As a metric, we use the negative log loss, which is a probabilistic measure of performance and the modified household for distance, which is more of a physical measure of performance.",
                    "label": 0
                },
                {
                    "sent": "So the modified household distance is going to be the Euclidean distance of a demonstrated activity, so observed activity and then that distance to a many sampled trajectories from our distribution.",
                    "label": 0
                },
                {
                    "sent": "And we're going to allow for some temporal misalignment.",
                    "label": 0
                },
                {
                    "sent": "So if this distance is small, that's good.",
                    "label": 0
                },
                {
                    "sent": "If it's big, it means we're very far away from the from the observed, or from the demonstrated activity.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a test here where we train on a scene A and then we test on scene B and then we have the opposite.",
                    "label": 0
                },
                {
                    "sent": "Over here we train on scene B and then test on scene A.",
                    "label": 0
                },
                {
                    "sent": "So we can see that we.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Outperform the baselines here and as a reference, I'm going to put the perform.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of our model, both trained and tested on the same scene, not on the same data, while on the same scene, so the test and train is different and what you can see here is that the performance is consistent, right?",
                    "label": 0
                },
                {
                    "sent": "You could swap out the trained model and test it on the same scene and the performance is pretty much the same here.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what this is telling us is that we have robust performance to new scenes.",
                    "label": 0
                },
                {
                    "sent": "We generalize well.",
                    "label": 0
                },
                {
                    "sent": "Finally, I'm going to end with some qualitative results.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we trained on the parking lot sequences, seen A and we use those parameters and now we're just applying them to Internet images and other scenes from the pirate data set and you can see that the forecasted distributions are quite reasonable.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Can you go back a few flies before you like the one slide before you did the performance?",
                    "label": 0
                },
                {
                    "sent": "Baby I missed it destination for this one number one how far?",
                    "label": 0
                },
                {
                    "sent": "OK, so the way the destination fork.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing works is you just assume that you have a uniform distribution over all the goals first.",
                    "label": 0
                },
                {
                    "sent": "And then as the observations come in from the tracker, we update.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The distribution or the posterior distribution over the goals.",
                    "label": 0
                },
                {
                    "sent": "So the more yeah.",
                    "label": 0
                },
                {
                    "sent": "So then in the next slide.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This picture so that is the output of your optimization before you do the time sequence.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is the output of yeah destination forecasting.",
                    "label": 0
                },
                {
                    "sent": "My question is, are you assuming that your state is fully observable?",
                    "label": 0
                },
                {
                    "sent": "In particular the direction of the motion.",
                    "label": 0
                },
                {
                    "sent": "Let's see so in the pure forecasting case, there are no observations, so there's no concept of observed or unobserved.",
                    "label": 0
                },
                {
                    "sent": "But in this state, yeah for the state.",
                    "label": 0
                },
                {
                    "sent": "But for the destination forecasting.",
                    "label": 1
                },
                {
                    "sent": "And correct me if I'm answering this incorrectly for the destination forecasting, we assume that the true state is hidden because there's noise in the tracker observations.",
                    "label": 0
                },
                {
                    "sent": "Did that answer your question?",
                    "label": 0
                },
                {
                    "sent": "So I assume that it's fully accelerated.",
                    "label": 0
                },
                {
                    "sent": "You're not using any Model 2, two model, like the uncertainty in the state.",
                    "label": 0
                },
                {
                    "sent": "In the tracking case.",
                    "label": 0
                },
                {
                    "sent": "In the tracking case, we are taking into account the uncertainty in the tracker observations, so there is.",
                    "label": 0
                },
                {
                    "sent": "It's yes it's taking into account the uncertainty there.",
                    "label": 0
                },
                {
                    "sent": "Bro, you're still using an MDP model.",
                    "label": 1
                },
                {
                    "sent": "Yes, we're using MDP with a hidden layer on top, thank you.",
                    "label": 0
                },
                {
                    "sent": "I may have missed one thing, but assuming you make a forecast, you're making a forecast based on the current single state.",
                    "label": 0
                },
                {
                    "sent": "We're using history.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what we have is input for forecasting is the all the features of the scene plus one location where the person is.",
                    "label": 0
                },
                {
                    "sent": "So let's just think it's actually important to look at the previous 10 frames 'cause you know that we're person walked.",
                    "label": 0
                },
                {
                    "sent": "It makes prediction markets much more accurate as opposed to single state.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I guess the scenario would be you have an image and a person just comes in right at the edge of the image and then your forecast.",
                    "label": 0
                },
                {
                    "sent": "In this case he is kind of in the middle, but that's how you would use it, right?",
                    "label": 0
                },
                {
                    "sent": "Don't you also need to forecast what everybody else is going to do in the scene like and forecast how the person is going to respond to what everybody else is going to do in the scene is that's an excellent point.",
                    "label": 0
                },
                {
                    "sent": "So this method assumes that the world is completely static, so nothing is moving so.",
                    "label": 0
                },
                {
                    "sent": "But I mean that would definitely be a next step to take.",
                    "label": 0
                },
                {
                    "sent": "This work is now take into account the give and take between the environment an agent, but it gets quite complex if you try to do it correctly, because then it becomes a game theory problem.",
                    "label": 0
                },
                {
                    "sent": "Where you have to take into account another person's policy and it gets quite complex really quickly.",
                    "label": 0
                },
                {
                    "sent": "Alright cool, that's very exciting work.",
                    "label": 0
                },
                {
                    "sent": "Thanks again thank you.",
                    "label": 0
                }
            ]
        }
    }
}