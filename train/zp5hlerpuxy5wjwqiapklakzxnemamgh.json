{
    "id": "zp5hlerpuxy5wjwqiapklakzxnemamgh",
    "title": "Large-Scale Behavioral Targeting",
    "info": {
        "author": [
            "Ye Chen, Microsoft"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Web Mining->Web & Behavior Mining"
        ]
    },
    "url": "http://videolectures.net/kdd09_chen_lsbt/",
    "segmentation": [
        [
            "Good afternoon everyone.",
            "Thank you for coming to the talk.",
            "My name is Ian Chen and currently with eBay.",
            "Unfortunately, my cost of my come across or Dimitri Pavlo from Yandex Lab is also here dimmer.",
            "Would you like to?",
            "Where is dimmit?",
            "Here is a hand at least and circuses John Candy, our professor from UC Berkeley.",
            "Unfortunately who is working on some deadline in sales now?",
            "So first and foremost, I would like to thank Yahoo, Yahoo Labs for Grand Asses challenge opportunities to conduct this work and this work was conducted at Yahoo and all the three authors were at Yahoo.",
            "Open this and the great people there make this happen and I also would like to sincerely thank our former VP Power working who actually also with us today.",
            "Would you like to raise your hand power at least?",
            "Without him, this won't happen.",
            "So today we're going to."
        ],
        [
            "Talk about some great work we have done over the past two to three years at Yahoo regarding display advertising.",
            "How to target, display, advertise, advertise to two users based on his historical behavior.",
            "So the problem can be formulated in a very simple way."
        ],
        [
            "The definition of behavior targeting our so called beauty can be defined as to leverage historical user behavior to select the most relevant ads.",
            "So there is two party in this problem.",
            "If we formulate as predicted problem the target variable we want to predict the crickets rate of the ads and.",
            "Independent variable we're using is all kinds of historical behaviors which include ad click views, page views or search query and clicks.",
            "So why is probably the challenge?",
            "I bring up several perspective.",
            "First of all, it's a large scale problem.",
            "When we talking about ads.",
            "It's even larger scale than even sparser large scale.",
            "Then search, I believe at and on one month.",
            "Yahoo Log 9 terabytes of data with 500 billion entries.",
            "One month for ads only without searches, and it's extremely sparse as well.",
            "As you can imagine, the clicks rate of ads is way low lower than search organic responses search.",
            "So one example is automotive at display ads average.",
            "Our city is .05%, so it's a fraction of a percent.",
            "And it's dynamic by its nature cause user behavior changes overtime.",
            "So the whole work I'm going to talk about how to deal with scale of data and how to capture the dynamic nature of this domain.",
            "So unfortunately we have some mess slides."
        ],
        [
            "But the good news is this is only my slides I have today.",
            "So which means the method is straightforward.",
            "So we won't predict accounts, so the natural stochastic model for counts imports on.",
            "As you can see, this is the probability model underlying.",
            "The notation here why?",
            "Let by to be observed counts of some target event.",
            "So in our case is the ads, clicks or ads views.",
            "And Lambda is the mean of the person and it's only parameter for this person distribution.",
            "We assume a identity link between the linear predict, which is W transpose X.",
            "Equals Lambda, so why we can do that?",
            "So as you can notice, we assume the weights to be nonnegative.",
            "So that's why we called in an active linear person regression.",
            "So it's theoretical and empirical suitable for this domain.",
            "Two things I would like mention.",
            "First, this W is the degree of interests you can interpret as degree of interest, so it's hard to interpret and negative W 2nd, so actually it does effect that some low interests component aspect of a user.",
            "We drag them to 0 by using this nonnegative weights.",
            "The second thing I would mention is the optimization method we're using because we have this non negative assumption for this weights vectors we adopt A. Multiplicative recurrence 22 converges weights one reference with one good references.",
            "A nonnegative matrix factorization paper in from NIPS.",
            "Tucson 2 and turns out this this optimization method is extremely efficient for this domain.",
            "After 15 to 20 iterations, the weights converges for a feature space of dimensionality 200,000.",
            "And once we got the particular colleagues and views we just constructed final unbiased estimator after libration smoothing.",
            "To get the CTR prediction.",
            "And help you remember the index.",
            "Here I indexes users J index features and K indexes target variables while we index target variable, we're going to build about 900 models at a single shot, so K indexes target variables.",
            "So now I would like to go through some details we learned from our implementation.",
            "So first for this large scale implementation on the Hadoop MapReduce framework and.",
            "So we successfully can can viewed as I said above, 900 models using the audio, who's cops without any sample without any sampling within one day.",
            "So now I'm going to talk how we're going to do this.",
            "It's generally followed the KDD."
        ],
        [
            "Process from data preparation for modeling for evaluation and so and so forth.",
            "So first step data preparation.",
            "So as you can, as you can see many practical learning algorithm I IO bound our scan bond.",
            "In particular scan bond which means we don't need to random access to data.",
            "Iterative algorithm need to scan the data maybe many times.",
            "So in this case in our case we need to pre process 20 to 30 terabytes.",
            "Data feeds for ads and searches.",
            "So it's very important that we reduce the data sites at earliest opportunity.",
            "Which means by projection aggregation and merging.",
            "In this case, we merge on the compiled key of Cookie and time.",
            "We reduce the data set and that argument would like to made is this step should be made?",
            "With minimal information loss and redundancy.",
            "So for example, we built BT models.",
            "The long term model predicts the next day CTR over next day and the short term model.",
            "It's also very important, particularly 6 minutes, so when you do the time aggregation, merging do not lose the time resolution we need.",
            "So example for example, we do one minute aggregation if you do 6 minutes above you lost opportunity to build.",
            "Really short term others.",
            "Also, this step should be made loosely coupled with any downstreaming modeling logic to have a better data usability.",
            "So this is a heavy step as you can see.",
            "And after this pre processing we reduced the data size from 20 to 30 terabytes to two to three terabytes."
        ],
        [
            "Feature selection, as you can see, the person model can take all kinds of random features, individual ads, individual searches.",
            "So, but it's theoretical and practical motivated to select features.",
            "It turns out the best approach we find is a simple frequency based feature selection.",
            "We just select the most popular and most frequent features appear in our data set.",
            "However, there's several tricks when you count frequency count in terms of cookies touching cookies not in terms of occurrence.",
            "You can imagine many cookies on web iRobot, so allow them only has one vote for each cookie and there is also some theoretical reasons justifications why frequency based method is behave better in this kind of data set, better than better at least as bad as good than some information theoretical like motoring mutual information.",
            "The reason is this data set is highly sparse and in mutual information from you.",
            "If you can see the joint probability term will be dominated by the most frequent features anyway, and for sparse features, the denominator of the mutual information can be very noisy.",
            "So the outcome of this step is we have three dictionary, one for ads, 14 pages, one for queries, and collectively define our feature space and index.",
            "So after this step we deal with index of the features rather than arbitrary strings is very important for large scale as well.",
            "So once we have the pre process data and the features."
        ],
        [
            "Says we're going to generate the feature vector.",
            "Finally, will be consumed by the modeling engine in the form of X, which effect and why response variable?",
            "So traditional complexity analysis uses syntactic terms, which means OCN is linear time.",
            "We argue that in large scale computing this C constant has to be serious tick tick tick count of.",
            "For example, if N is in the order of billion, which means 01 and can take maybe one or two days if you give effect of four puts away more than one week, which make many.",
            "Which make many solution impossible for practice.",
            "So our goal is to build a feature vector generation algorithm in strictly 01 end time.",
            "So the method we're using straightforward so.",
            "First off, when we got the data, we sort the data by time.",
            "So which means we got a stream of event.",
            "So while I do sort 'cause.",
            "Relative speaking, when we do MapReduce when we block the data and when algorithms our bond is sorting pretty much can be annoyed.",
            "So once we saw that as you can see, we have we have those.",
            "Rose, it's hard to explain.",
            "Let me try my best so we will maintain three cursors.",
            "And first cursor is the beginning of the X and next cursor at the end of the X and give another cursor to the beginning of the target.",
            "So in our case target happens after the response individual variable.",
            "And then you move the cursor forward.",
            "Each time by a unit of time window it can be one day it can be 50 minutes and then you do the incremental incrementally update your shared object to maintain your feature vector.",
            "In Java case is going to be a tree map.",
            "So in that case you don't need to scan all the data you already passed, and you do you maintain your current active feature vector.",
            "Only use one shared object.",
            "So at the end of the day, since the target window usually is much shorter than the input window, you'd end up with approximately 01 an algorithm.",
            "This is important and I will show you in the experiment inside.",
            "Another point is how do you."
        ],
        [
            "Shallies awaits men ingredient based Masters.",
            "Simply initialize weights randomly are uniformly.",
            "So we argue that in practice is another is not good idea.",
            "So we gotta use the data where the data tell us to initialize weights to make the convergence fast.",
            "So we propose to approach.",
            "Here one is the first method.",
            "Feature specific normalization, so pretty much idea from TF IDF so there not normalizer is per feature.",
            "And the second will initialize method we further.",
            "On top of feature feature level normalization refers to normalize the initial weights by target traffic.",
            "This is to respect highly skewed distribution across target variables example here.",
            "So we have two categories to model for example in finance category.",
            "The other one is life staging from Yahoo's example and it's always the case that some categories way more popular than the other.",
            "So the traffic was dominated by some popular categories.",
            "So we take count of that in this case.",
            "And I consider this to be the key contribution of this paper.",
            "How do we?"
        ],
        [
            "Paralyze our optimization routine, which is multiplicative recurrence.",
            "So first I would like to make a connection to two non active wait.",
            "Matrix matrix factorization here.",
            "Given a data matrix, D can be blocked into two matrix when a target matrix.",
            "Why the other is input matrix X and we would like to find a weights matrix to maximize log likelihood of the data.",
            "So this is essentially a an an NMF problem which use a log likelihood at the quality of the factorization.",
            "That's why the multiple update applies.",
            "So the computational bottleneck of this recurrence of this update loop.",
            "Is the numerator of the multiplicative factor, which accounting the currents of the target variable and the input variable?",
            "So as many.",
            "As it turns out, many a large scale parallelize algorithms.",
            "This suffered from synchronizing.",
            "This suffer from synchronizing model parameters after each iteration."
        ],
        [
            "So this is called fine grained parallelization.",
            "I put the quote and quote here so this is not necessarily good term.",
            "And as we know fine grained parallelization is the most awkward to be paralyzed because they need to synchronize often.",
            "And another meaning I put quote unquote are fine grained parallelization.",
            "The trick here.",
            "The critical factor here is you got paralyzed.",
            "Your computation's fine grid as you can.",
            "So in our case.",
            "While input is a data matrix, we would like to get a weight matrix.",
            "So I I distributed computation by each cell of weights matrix, which is K denoting target J denoting.",
            "Features so the detail of the map reduce algorithm is there is a chart so by D distributing KJK&J, we pretty much precompute the update factor.",
            "Before they finally do step, this is another trick we got to emphasize.",
            "So in fine grained parallelization you got make the last synchronization step, which often have happened with much less machines than what you have as lightweighted as possible.",
            "So in our case the last reducer only takes 2 minutes."
        ],
        [
            "So let's talk about some experimental result.",
            "Is the general setup is we use five weeks for skill Yahoo data with about 500 million training examples and three terabytes pre processed and compressed data and the feature space is about 150K features.",
            "And evaluation use the next day.",
            "Small portion of sample we evaluate in terms of relative citylift, clickview rock and the right runtime.",
            "So our experiment is run in a 500 nodes Hadoop clusters of commodity machines.",
            "That's why you can see why.",
            "I would love to sync Yahoo Labs.",
            "So first."
        ],
        [
            "Value the value of data sites.",
            "We increase the data from 32 buckets which is 1 / 6 sample or Yahoo data.",
            "4 four scale of the update.",
            "As you can see the city I lift monotonically increase, so the model is still in date hungry stage.",
            "Even if we use hold it.",
            "That's why the large scale implementation so important.",
            "And the runtime is server you can see is increase more significantly sublinear.",
            "Thanks for the information."
        ],
        [
            "The second is the feature dimensionality.",
            "What is the best point dimensionality feature?",
            "It turns out it's what we choose.",
            "150.",
            "Give the highest CTI lifted rock area."
        ],
        [
            "And to verify the feature vector generation linear time, we conduct this experiment by adjusting the time window of the feature sites.",
            "As you can see the 3rd row which I was forced role I highlighted it's linear strictly a strict constant time."
        ],
        [
            "And this is a stratified sampling.",
            "It turns out that the takeaway is negative example, which does not have any clicks or views contribute close to 0 value to the model, but those who had views you do need to incorporate that in.",
            "Probably based on model it's view click but divided by view.",
            "And the last finding is."
        ],
        [
            "The latency between the last event you received and you make prediction.",
            "There is some production limit.",
            "You gotta have a black window latency there.",
            "You gotta move them as best as you can.",
            "So in this example of like offline evaluation, if we remove the latency between these two Windows, the city I lived almost double."
        ],
        [
            "So in summary, we propose a map reduce statistic learning algorithm in large scale.",
            "And in place, which work generation algorithm, which takes strictly om time and.",
            "In memory caching scheme, this is another trick for map reduce implementation so many times you got cash your data instead of their weights.",
            "By default map reduce Hadoop give you a 128 megabytes data.",
            "By doing that you can reduce your iOS can.",
            "Significantly we apply this tactic in various times and high efficiency data structure and sparse representation.",
            "So we use a flat version of yellow sparse representation which basically is 2.",
            "Two arrays, two arrays for for position and value for each vector.",
            "And hopefully we believe our work make significant contribution to the large scale computing.",
            "A missionary in general.",
            "Thank you very much."
        ],
        [
            "Time for a couple of questions.",
            "Any questions?",
            "Feed the precisely you started the data as a first step.",
            "In that case, it's not going to be linear anymore, right?",
            "Absolutely.",
            "It's N log N the best starting.",
            "Merging properly in this case, but as I said, all the data in memory, so comparing with the total cost, I'll cost this pretty much pretty much ignore aghbal term.",
            "So no problem.",
            "Questions.",
            "You can you give example of the ad features?",
            "Good question, so add features.",
            "Simple two type of ad features that click and view ad views and the name of the feature is just to identify of the ads.",
            "So so I guess I didn't understand that.",
            "How did you constructed 10,000 features out of Lehman?",
            "Tencent features more than 10 sound, but it's 150K sound features consists of ad clicks, ad views, which denoted by a ad identify.",
            "It's a number, right and page views, which is identify a page ID.",
            "Why is that a feature then why they are feature while their culture, their name of the feature there?",
            "Yeah, there name of the feature, the feature value.",
            "I'm sorry the feature value in the event accounts for this feature.",
            "That makes sense.",
            "For example, I have the added feature AA1 adds and value 10, which means that this user click.",
            "This adds 10 * / a certain period of time.",
            "Discuss with you afterwards about that.",
            "So second question is that town.",
            "When evaluating the effectiveness CTR, oftentimes it's a little bit.",
            "Part 2 hard to Rudy proves that the historical performance to be a good justification moving forward.",
            "Once your thought on this one, I agree, specially in the display ads business.",
            "I agree I totally agree.",
            "So City a city lift as an estimation point estimation.",
            "It can be very noisy.",
            "That's why we first relative city are.",
            "We remove the effect of population city by dividing publicity and second of all we provide the clickview rock.",
            "Which is self normalized in any way and its average sense over a large scale of X.",
            "So if rock area is higher, it's pretty robust.",
            "In our case not necessary citylift, so we provide 22.",
            "2 inch matrix that makes sense.",
            "After is.",
            "I guess that's a deeper question now.",
            "You won't change my optimization function.",
            "City is operational function.",
            "The real function to be optimization should be the conversion rate, which is advertiser really like so we can discuss more on that.",
            "We have some sites on that, but it's a large scale question.",
            "Thank you.",
            "Thank you again very much.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "Thank you for coming to the talk.",
                    "label": 0
                },
                {
                    "sent": "My name is Ian Chen and currently with eBay.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, my cost of my come across or Dimitri Pavlo from Yandex Lab is also here dimmer.",
                    "label": 0
                },
                {
                    "sent": "Would you like to?",
                    "label": 0
                },
                {
                    "sent": "Where is dimmit?",
                    "label": 0
                },
                {
                    "sent": "Here is a hand at least and circuses John Candy, our professor from UC Berkeley.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately who is working on some deadline in sales now?",
                    "label": 0
                },
                {
                    "sent": "So first and foremost, I would like to thank Yahoo, Yahoo Labs for Grand Asses challenge opportunities to conduct this work and this work was conducted at Yahoo and all the three authors were at Yahoo.",
                    "label": 1
                },
                {
                    "sent": "Open this and the great people there make this happen and I also would like to sincerely thank our former VP Power working who actually also with us today.",
                    "label": 0
                },
                {
                    "sent": "Would you like to raise your hand power at least?",
                    "label": 0
                },
                {
                    "sent": "Without him, this won't happen.",
                    "label": 0
                },
                {
                    "sent": "So today we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about some great work we have done over the past two to three years at Yahoo regarding display advertising.",
                    "label": 0
                },
                {
                    "sent": "How to target, display, advertise, advertise to two users based on his historical behavior.",
                    "label": 0
                },
                {
                    "sent": "So the problem can be formulated in a very simple way.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The definition of behavior targeting our so called beauty can be defined as to leverage historical user behavior to select the most relevant ads.",
                    "label": 1
                },
                {
                    "sent": "So there is two party in this problem.",
                    "label": 0
                },
                {
                    "sent": "If we formulate as predicted problem the target variable we want to predict the crickets rate of the ads and.",
                    "label": 1
                },
                {
                    "sent": "Independent variable we're using is all kinds of historical behaviors which include ad click views, page views or search query and clicks.",
                    "label": 0
                },
                {
                    "sent": "So why is probably the challenge?",
                    "label": 1
                },
                {
                    "sent": "I bring up several perspective.",
                    "label": 0
                },
                {
                    "sent": "First of all, it's a large scale problem.",
                    "label": 0
                },
                {
                    "sent": "When we talking about ads.",
                    "label": 0
                },
                {
                    "sent": "It's even larger scale than even sparser large scale.",
                    "label": 0
                },
                {
                    "sent": "Then search, I believe at and on one month.",
                    "label": 0
                },
                {
                    "sent": "Yahoo Log 9 terabytes of data with 500 billion entries.",
                    "label": 1
                },
                {
                    "sent": "One month for ads only without searches, and it's extremely sparse as well.",
                    "label": 0
                },
                {
                    "sent": "As you can imagine, the clicks rate of ads is way low lower than search organic responses search.",
                    "label": 0
                },
                {
                    "sent": "So one example is automotive at display ads average.",
                    "label": 0
                },
                {
                    "sent": "Our city is .05%, so it's a fraction of a percent.",
                    "label": 1
                },
                {
                    "sent": "And it's dynamic by its nature cause user behavior changes overtime.",
                    "label": 0
                },
                {
                    "sent": "So the whole work I'm going to talk about how to deal with scale of data and how to capture the dynamic nature of this domain.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately we have some mess slides.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the good news is this is only my slides I have today.",
                    "label": 0
                },
                {
                    "sent": "So which means the method is straightforward.",
                    "label": 0
                },
                {
                    "sent": "So we won't predict accounts, so the natural stochastic model for counts imports on.",
                    "label": 0
                },
                {
                    "sent": "As you can see, this is the probability model underlying.",
                    "label": 0
                },
                {
                    "sent": "The notation here why?",
                    "label": 0
                },
                {
                    "sent": "Let by to be observed counts of some target event.",
                    "label": 0
                },
                {
                    "sent": "So in our case is the ads, clicks or ads views.",
                    "label": 0
                },
                {
                    "sent": "And Lambda is the mean of the person and it's only parameter for this person distribution.",
                    "label": 0
                },
                {
                    "sent": "We assume a identity link between the linear predict, which is W transpose X.",
                    "label": 0
                },
                {
                    "sent": "Equals Lambda, so why we can do that?",
                    "label": 0
                },
                {
                    "sent": "So as you can notice, we assume the weights to be nonnegative.",
                    "label": 0
                },
                {
                    "sent": "So that's why we called in an active linear person regression.",
                    "label": 0
                },
                {
                    "sent": "So it's theoretical and empirical suitable for this domain.",
                    "label": 0
                },
                {
                    "sent": "Two things I would like mention.",
                    "label": 0
                },
                {
                    "sent": "First, this W is the degree of interests you can interpret as degree of interest, so it's hard to interpret and negative W 2nd, so actually it does effect that some low interests component aspect of a user.",
                    "label": 0
                },
                {
                    "sent": "We drag them to 0 by using this nonnegative weights.",
                    "label": 0
                },
                {
                    "sent": "The second thing I would mention is the optimization method we're using because we have this non negative assumption for this weights vectors we adopt A. Multiplicative recurrence 22 converges weights one reference with one good references.",
                    "label": 0
                },
                {
                    "sent": "A nonnegative matrix factorization paper in from NIPS.",
                    "label": 0
                },
                {
                    "sent": "Tucson 2 and turns out this this optimization method is extremely efficient for this domain.",
                    "label": 0
                },
                {
                    "sent": "After 15 to 20 iterations, the weights converges for a feature space of dimensionality 200,000.",
                    "label": 0
                },
                {
                    "sent": "And once we got the particular colleagues and views we just constructed final unbiased estimator after libration smoothing.",
                    "label": 0
                },
                {
                    "sent": "To get the CTR prediction.",
                    "label": 0
                },
                {
                    "sent": "And help you remember the index.",
                    "label": 0
                },
                {
                    "sent": "Here I indexes users J index features and K indexes target variables while we index target variable, we're going to build about 900 models at a single shot, so K indexes target variables.",
                    "label": 0
                },
                {
                    "sent": "So now I would like to go through some details we learned from our implementation.",
                    "label": 0
                },
                {
                    "sent": "So first for this large scale implementation on the Hadoop MapReduce framework and.",
                    "label": 0
                },
                {
                    "sent": "So we successfully can can viewed as I said above, 900 models using the audio, who's cops without any sample without any sampling within one day.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to talk how we're going to do this.",
                    "label": 0
                },
                {
                    "sent": "It's generally followed the KDD.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Process from data preparation for modeling for evaluation and so and so forth.",
                    "label": 0
                },
                {
                    "sent": "So first step data preparation.",
                    "label": 0
                },
                {
                    "sent": "So as you can, as you can see many practical learning algorithm I IO bound our scan bond.",
                    "label": 1
                },
                {
                    "sent": "In particular scan bond which means we don't need to random access to data.",
                    "label": 0
                },
                {
                    "sent": "Iterative algorithm need to scan the data maybe many times.",
                    "label": 0
                },
                {
                    "sent": "So in this case in our case we need to pre process 20 to 30 terabytes.",
                    "label": 0
                },
                {
                    "sent": "Data feeds for ads and searches.",
                    "label": 1
                },
                {
                    "sent": "So it's very important that we reduce the data sites at earliest opportunity.",
                    "label": 0
                },
                {
                    "sent": "Which means by projection aggregation and merging.",
                    "label": 1
                },
                {
                    "sent": "In this case, we merge on the compiled key of Cookie and time.",
                    "label": 1
                },
                {
                    "sent": "We reduce the data set and that argument would like to made is this step should be made?",
                    "label": 0
                },
                {
                    "sent": "With minimal information loss and redundancy.",
                    "label": 0
                },
                {
                    "sent": "So for example, we built BT models.",
                    "label": 0
                },
                {
                    "sent": "The long term model predicts the next day CTR over next day and the short term model.",
                    "label": 0
                },
                {
                    "sent": "It's also very important, particularly 6 minutes, so when you do the time aggregation, merging do not lose the time resolution we need.",
                    "label": 1
                },
                {
                    "sent": "So example for example, we do one minute aggregation if you do 6 minutes above you lost opportunity to build.",
                    "label": 0
                },
                {
                    "sent": "Really short term others.",
                    "label": 0
                },
                {
                    "sent": "Also, this step should be made loosely coupled with any downstreaming modeling logic to have a better data usability.",
                    "label": 0
                },
                {
                    "sent": "So this is a heavy step as you can see.",
                    "label": 0
                },
                {
                    "sent": "And after this pre processing we reduced the data size from 20 to 30 terabytes to two to three terabytes.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feature selection, as you can see, the person model can take all kinds of random features, individual ads, individual searches.",
                    "label": 1
                },
                {
                    "sent": "So, but it's theoretical and practical motivated to select features.",
                    "label": 0
                },
                {
                    "sent": "It turns out the best approach we find is a simple frequency based feature selection.",
                    "label": 1
                },
                {
                    "sent": "We just select the most popular and most frequent features appear in our data set.",
                    "label": 0
                },
                {
                    "sent": "However, there's several tricks when you count frequency count in terms of cookies touching cookies not in terms of occurrence.",
                    "label": 0
                },
                {
                    "sent": "You can imagine many cookies on web iRobot, so allow them only has one vote for each cookie and there is also some theoretical reasons justifications why frequency based method is behave better in this kind of data set, better than better at least as bad as good than some information theoretical like motoring mutual information.",
                    "label": 0
                },
                {
                    "sent": "The reason is this data set is highly sparse and in mutual information from you.",
                    "label": 0
                },
                {
                    "sent": "If you can see the joint probability term will be dominated by the most frequent features anyway, and for sparse features, the denominator of the mutual information can be very noisy.",
                    "label": 1
                },
                {
                    "sent": "So the outcome of this step is we have three dictionary, one for ads, 14 pages, one for queries, and collectively define our feature space and index.",
                    "label": 1
                },
                {
                    "sent": "So after this step we deal with index of the features rather than arbitrary strings is very important for large scale as well.",
                    "label": 1
                },
                {
                    "sent": "So once we have the pre process data and the features.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Says we're going to generate the feature vector.",
                    "label": 1
                },
                {
                    "sent": "Finally, will be consumed by the modeling engine in the form of X, which effect and why response variable?",
                    "label": 1
                },
                {
                    "sent": "So traditional complexity analysis uses syntactic terms, which means OCN is linear time.",
                    "label": 0
                },
                {
                    "sent": "We argue that in large scale computing this C constant has to be serious tick tick tick count of.",
                    "label": 0
                },
                {
                    "sent": "For example, if N is in the order of billion, which means 01 and can take maybe one or two days if you give effect of four puts away more than one week, which make many.",
                    "label": 1
                },
                {
                    "sent": "Which make many solution impossible for practice.",
                    "label": 1
                },
                {
                    "sent": "So our goal is to build a feature vector generation algorithm in strictly 01 end time.",
                    "label": 0
                },
                {
                    "sent": "So the method we're using straightforward so.",
                    "label": 0
                },
                {
                    "sent": "First off, when we got the data, we sort the data by time.",
                    "label": 0
                },
                {
                    "sent": "So which means we got a stream of event.",
                    "label": 0
                },
                {
                    "sent": "So while I do sort 'cause.",
                    "label": 0
                },
                {
                    "sent": "Relative speaking, when we do MapReduce when we block the data and when algorithms our bond is sorting pretty much can be annoyed.",
                    "label": 0
                },
                {
                    "sent": "So once we saw that as you can see, we have we have those.",
                    "label": 0
                },
                {
                    "sent": "Rose, it's hard to explain.",
                    "label": 0
                },
                {
                    "sent": "Let me try my best so we will maintain three cursors.",
                    "label": 0
                },
                {
                    "sent": "And first cursor is the beginning of the X and next cursor at the end of the X and give another cursor to the beginning of the target.",
                    "label": 0
                },
                {
                    "sent": "So in our case target happens after the response individual variable.",
                    "label": 0
                },
                {
                    "sent": "And then you move the cursor forward.",
                    "label": 0
                },
                {
                    "sent": "Each time by a unit of time window it can be one day it can be 50 minutes and then you do the incremental incrementally update your shared object to maintain your feature vector.",
                    "label": 0
                },
                {
                    "sent": "In Java case is going to be a tree map.",
                    "label": 0
                },
                {
                    "sent": "So in that case you don't need to scan all the data you already passed, and you do you maintain your current active feature vector.",
                    "label": 0
                },
                {
                    "sent": "Only use one shared object.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the day, since the target window usually is much shorter than the input window, you'd end up with approximately 01 an algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is important and I will show you in the experiment inside.",
                    "label": 0
                },
                {
                    "sent": "Another point is how do you.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shallies awaits men ingredient based Masters.",
                    "label": 0
                },
                {
                    "sent": "Simply initialize weights randomly are uniformly.",
                    "label": 0
                },
                {
                    "sent": "So we argue that in practice is another is not good idea.",
                    "label": 0
                },
                {
                    "sent": "So we gotta use the data where the data tell us to initialize weights to make the convergence fast.",
                    "label": 0
                },
                {
                    "sent": "So we propose to approach.",
                    "label": 0
                },
                {
                    "sent": "Here one is the first method.",
                    "label": 0
                },
                {
                    "sent": "Feature specific normalization, so pretty much idea from TF IDF so there not normalizer is per feature.",
                    "label": 0
                },
                {
                    "sent": "And the second will initialize method we further.",
                    "label": 0
                },
                {
                    "sent": "On top of feature feature level normalization refers to normalize the initial weights by target traffic.",
                    "label": 0
                },
                {
                    "sent": "This is to respect highly skewed distribution across target variables example here.",
                    "label": 1
                },
                {
                    "sent": "So we have two categories to model for example in finance category.",
                    "label": 0
                },
                {
                    "sent": "The other one is life staging from Yahoo's example and it's always the case that some categories way more popular than the other.",
                    "label": 0
                },
                {
                    "sent": "So the traffic was dominated by some popular categories.",
                    "label": 0
                },
                {
                    "sent": "So we take count of that in this case.",
                    "label": 0
                },
                {
                    "sent": "And I consider this to be the key contribution of this paper.",
                    "label": 0
                },
                {
                    "sent": "How do we?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Paralyze our optimization routine, which is multiplicative recurrence.",
                    "label": 0
                },
                {
                    "sent": "So first I would like to make a connection to two non active wait.",
                    "label": 0
                },
                {
                    "sent": "Matrix matrix factorization here.",
                    "label": 0
                },
                {
                    "sent": "Given a data matrix, D can be blocked into two matrix when a target matrix.",
                    "label": 0
                },
                {
                    "sent": "Why the other is input matrix X and we would like to find a weights matrix to maximize log likelihood of the data.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially a an an NMF problem which use a log likelihood at the quality of the factorization.",
                    "label": 1
                },
                {
                    "sent": "That's why the multiple update applies.",
                    "label": 0
                },
                {
                    "sent": "So the computational bottleneck of this recurrence of this update loop.",
                    "label": 0
                },
                {
                    "sent": "Is the numerator of the multiplicative factor, which accounting the currents of the target variable and the input variable?",
                    "label": 0
                },
                {
                    "sent": "So as many.",
                    "label": 0
                },
                {
                    "sent": "As it turns out, many a large scale parallelize algorithms.",
                    "label": 0
                },
                {
                    "sent": "This suffered from synchronizing.",
                    "label": 0
                },
                {
                    "sent": "This suffer from synchronizing model parameters after each iteration.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is called fine grained parallelization.",
                    "label": 0
                },
                {
                    "sent": "I put the quote and quote here so this is not necessarily good term.",
                    "label": 0
                },
                {
                    "sent": "And as we know fine grained parallelization is the most awkward to be paralyzed because they need to synchronize often.",
                    "label": 0
                },
                {
                    "sent": "And another meaning I put quote unquote are fine grained parallelization.",
                    "label": 0
                },
                {
                    "sent": "The trick here.",
                    "label": 0
                },
                {
                    "sent": "The critical factor here is you got paralyzed.",
                    "label": 0
                },
                {
                    "sent": "Your computation's fine grid as you can.",
                    "label": 0
                },
                {
                    "sent": "So in our case.",
                    "label": 0
                },
                {
                    "sent": "While input is a data matrix, we would like to get a weight matrix.",
                    "label": 0
                },
                {
                    "sent": "So I I distributed computation by each cell of weights matrix, which is K denoting target J denoting.",
                    "label": 0
                },
                {
                    "sent": "Features so the detail of the map reduce algorithm is there is a chart so by D distributing KJK&J, we pretty much precompute the update factor.",
                    "label": 0
                },
                {
                    "sent": "Before they finally do step, this is another trick we got to emphasize.",
                    "label": 0
                },
                {
                    "sent": "So in fine grained parallelization you got make the last synchronization step, which often have happened with much less machines than what you have as lightweighted as possible.",
                    "label": 0
                },
                {
                    "sent": "So in our case the last reducer only takes 2 minutes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk about some experimental result.",
                    "label": 0
                },
                {
                    "sent": "Is the general setup is we use five weeks for skill Yahoo data with about 500 million training examples and three terabytes pre processed and compressed data and the feature space is about 150K features.",
                    "label": 1
                },
                {
                    "sent": "And evaluation use the next day.",
                    "label": 1
                },
                {
                    "sent": "Small portion of sample we evaluate in terms of relative citylift, clickview rock and the right runtime.",
                    "label": 0
                },
                {
                    "sent": "So our experiment is run in a 500 nodes Hadoop clusters of commodity machines.",
                    "label": 0
                },
                {
                    "sent": "That's why you can see why.",
                    "label": 0
                },
                {
                    "sent": "I would love to sync Yahoo Labs.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Value the value of data sites.",
                    "label": 0
                },
                {
                    "sent": "We increase the data from 32 buckets which is 1 / 6 sample or Yahoo data.",
                    "label": 0
                },
                {
                    "sent": "4 four scale of the update.",
                    "label": 0
                },
                {
                    "sent": "As you can see the city I lift monotonically increase, so the model is still in date hungry stage.",
                    "label": 0
                },
                {
                    "sent": "Even if we use hold it.",
                    "label": 0
                },
                {
                    "sent": "That's why the large scale implementation so important.",
                    "label": 0
                },
                {
                    "sent": "And the runtime is server you can see is increase more significantly sublinear.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the information.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second is the feature dimensionality.",
                    "label": 0
                },
                {
                    "sent": "What is the best point dimensionality feature?",
                    "label": 0
                },
                {
                    "sent": "It turns out it's what we choose.",
                    "label": 0
                },
                {
                    "sent": "150.",
                    "label": 0
                },
                {
                    "sent": "Give the highest CTI lifted rock area.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to verify the feature vector generation linear time, we conduct this experiment by adjusting the time window of the feature sites.",
                    "label": 0
                },
                {
                    "sent": "As you can see the 3rd row which I was forced role I highlighted it's linear strictly a strict constant time.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is a stratified sampling.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the takeaway is negative example, which does not have any clicks or views contribute close to 0 value to the model, but those who had views you do need to incorporate that in.",
                    "label": 0
                },
                {
                    "sent": "Probably based on model it's view click but divided by view.",
                    "label": 0
                },
                {
                    "sent": "And the last finding is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The latency between the last event you received and you make prediction.",
                    "label": 0
                },
                {
                    "sent": "There is some production limit.",
                    "label": 0
                },
                {
                    "sent": "You gotta have a black window latency there.",
                    "label": 0
                },
                {
                    "sent": "You gotta move them as best as you can.",
                    "label": 0
                },
                {
                    "sent": "So in this example of like offline evaluation, if we remove the latency between these two Windows, the city I lived almost double.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, we propose a map reduce statistic learning algorithm in large scale.",
                    "label": 1
                },
                {
                    "sent": "And in place, which work generation algorithm, which takes strictly om time and.",
                    "label": 1
                },
                {
                    "sent": "In memory caching scheme, this is another trick for map reduce implementation so many times you got cash your data instead of their weights.",
                    "label": 0
                },
                {
                    "sent": "By default map reduce Hadoop give you a 128 megabytes data.",
                    "label": 1
                },
                {
                    "sent": "By doing that you can reduce your iOS can.",
                    "label": 0
                },
                {
                    "sent": "Significantly we apply this tactic in various times and high efficiency data structure and sparse representation.",
                    "label": 0
                },
                {
                    "sent": "So we use a flat version of yellow sparse representation which basically is 2.",
                    "label": 0
                },
                {
                    "sent": "Two arrays, two arrays for for position and value for each vector.",
                    "label": 0
                },
                {
                    "sent": "And hopefully we believe our work make significant contribution to the large scale computing.",
                    "label": 1
                },
                {
                    "sent": "A missionary in general.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time for a couple of questions.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Feed the precisely you started the data as a first step.",
                    "label": 0
                },
                {
                    "sent": "In that case, it's not going to be linear anymore, right?",
                    "label": 0
                },
                {
                    "sent": "Absolutely.",
                    "label": 0
                },
                {
                    "sent": "It's N log N the best starting.",
                    "label": 0
                },
                {
                    "sent": "Merging properly in this case, but as I said, all the data in memory, so comparing with the total cost, I'll cost this pretty much pretty much ignore aghbal term.",
                    "label": 0
                },
                {
                    "sent": "So no problem.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "You can you give example of the ad features?",
                    "label": 0
                },
                {
                    "sent": "Good question, so add features.",
                    "label": 0
                },
                {
                    "sent": "Simple two type of ad features that click and view ad views and the name of the feature is just to identify of the ads.",
                    "label": 0
                },
                {
                    "sent": "So so I guess I didn't understand that.",
                    "label": 0
                },
                {
                    "sent": "How did you constructed 10,000 features out of Lehman?",
                    "label": 0
                },
                {
                    "sent": "Tencent features more than 10 sound, but it's 150K sound features consists of ad clicks, ad views, which denoted by a ad identify.",
                    "label": 0
                },
                {
                    "sent": "It's a number, right and page views, which is identify a page ID.",
                    "label": 0
                },
                {
                    "sent": "Why is that a feature then why they are feature while their culture, their name of the feature there?",
                    "label": 0
                },
                {
                    "sent": "Yeah, there name of the feature, the feature value.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry the feature value in the event accounts for this feature.",
                    "label": 0
                },
                {
                    "sent": "That makes sense.",
                    "label": 0
                },
                {
                    "sent": "For example, I have the added feature AA1 adds and value 10, which means that this user click.",
                    "label": 0
                },
                {
                    "sent": "This adds 10 * / a certain period of time.",
                    "label": 0
                },
                {
                    "sent": "Discuss with you afterwards about that.",
                    "label": 0
                },
                {
                    "sent": "So second question is that town.",
                    "label": 0
                },
                {
                    "sent": "When evaluating the effectiveness CTR, oftentimes it's a little bit.",
                    "label": 0
                },
                {
                    "sent": "Part 2 hard to Rudy proves that the historical performance to be a good justification moving forward.",
                    "label": 0
                },
                {
                    "sent": "Once your thought on this one, I agree, specially in the display ads business.",
                    "label": 0
                },
                {
                    "sent": "I agree I totally agree.",
                    "label": 0
                },
                {
                    "sent": "So City a city lift as an estimation point estimation.",
                    "label": 0
                },
                {
                    "sent": "It can be very noisy.",
                    "label": 0
                },
                {
                    "sent": "That's why we first relative city are.",
                    "label": 0
                },
                {
                    "sent": "We remove the effect of population city by dividing publicity and second of all we provide the clickview rock.",
                    "label": 0
                },
                {
                    "sent": "Which is self normalized in any way and its average sense over a large scale of X.",
                    "label": 0
                },
                {
                    "sent": "So if rock area is higher, it's pretty robust.",
                    "label": 0
                },
                {
                    "sent": "In our case not necessary citylift, so we provide 22.",
                    "label": 0
                },
                {
                    "sent": "2 inch matrix that makes sense.",
                    "label": 0
                },
                {
                    "sent": "After is.",
                    "label": 0
                },
                {
                    "sent": "I guess that's a deeper question now.",
                    "label": 0
                },
                {
                    "sent": "You won't change my optimization function.",
                    "label": 0
                },
                {
                    "sent": "City is operational function.",
                    "label": 0
                },
                {
                    "sent": "The real function to be optimization should be the conversion rate, which is advertiser really like so we can discuss more on that.",
                    "label": 0
                },
                {
                    "sent": "We have some sites on that, but it's a large scale question.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you again very much.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}