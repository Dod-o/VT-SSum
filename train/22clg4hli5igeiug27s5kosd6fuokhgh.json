{
    "id": "22clg4hli5igeiug27s5kosd6fuokhgh",
    "title": "Unsupervised Feature Selection for Multi-Cluster Data",
    "info": {
        "author": [
            "Deng Cai, College of Computer Science, Zhejiang University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/kdd2010_cai_ufsm/",
    "segmentation": [
        [
            "Good morning everyone, so my name isn't I. I came from certain University so this is John work with three inch and shafiqul."
        ],
        [
            "So we all know that that we have always have this like high dimension data problem.",
            "So we have bunch of different like text, image, video, gene.",
            "So like the previous two activities because also talk about how to handle."
        ],
        [
            "These data so generally we have to approach to approach the first one is dimensionality reduction.",
            "Basically that will be learned.",
            "A combined feature set, so the new feature will be.",
            "Either linear or nonlinear combination of the original feature the other way."
        ],
        [
            "Is it doing feature selection?",
            "Basically we selected those like important feature with respect your tasks.",
            "So in general, feature selection method can be categorized in two categories.",
            "The first one is like a supervised method.",
            "You already have labels and label can guide you how to select those important features."
        ],
        [
            "And the second category is uncivil feature feature selection, so they know label information.",
            "So this talk maybe even harder than the first carry, since there's no label information, so they know crews to help you to decide which feature is important.",
            "So in this paper in this talk, I will focus on the second Calculator, so particularly as the first speaker also mentioned, there are like three categories for feature selection.",
            "The first one, the filter method.",
            "2nd on the wrapper message all the combined method.",
            "So in this talk I mainly focus on like the filter method or you can see somehow like the combined method since.",
            "If we do rapper method that you could do unsupervised learning, you maybe you need to follow up at like accounting method so that will be increased like the computational cost.",
            "So I mainly focus on the future method.",
            "Basically I can rank feature then you can select feature based on their ranking score.",
            "So existing method like the mix variance.",
            "So this is very simple.",
            "You simply simply compute the variance of each feature and you select the feature by by there.",
            "Various score this very much like that.",
            "The principle component analysis, but this is applying on feature selection.",
            "Let's take along the largest score.",
            "So this one essentially consider manifold structure, so it evaluates the feature based on the power to preserve the manual structure and another one that QR.",
            "So these are very similar to level scores.",
            "Also trying to preserve like the.",
            "The data structure so I will talk about our algorithm which is M CFS here called multicast or Costa feature selection."
        ],
        [
            "So here's the outline of my remaining talk.",
            "First, I will give the problem setting and describe our algorithm an, followed by experiments inconnu."
        ],
        [
            "So the problem here we're doing unsupervised.",
            "No, we do answer by few things, so there no label there, so we are.",
            "Well, considering like the multicast structure.",
            "So if there only two casters, so everything becomes easy.",
            "So like maybe like you are or fiscal can well handle it.",
            "However when we talk about if the hidden like data structure in multi cluster.",
            "So here's a very simple synthetic example here.",
            "So I show like that you're given color only show that they have given cost structure, but essentially they know label there so so you don't know.",
            "So which car like each point blonde too.",
            "So there are three three type of like the points so here.",
            "So they just building in three dimensional space.",
            "So here the first picture that feature a an feature B.",
            "So they look like this feature a an efficiency feature B and efficiency because.",
            "Essentially, if you rank feature one by one, so like the traditional, even like the Max variance or level score or QF method, they always rank like that.",
            "Maybe feature a off, maybe the first one feature and Phoebe the second one.",
            "If you see a third one.",
            "However, if you so you expect to set 2 features so you always select the feature A&B.",
            "But that is not optimal.",
            "Maybe a better way is sacrificial ANC or feature B&C?",
            "So this time essentially you need to explore the mouth problem here.",
            "Essentially like you already know that even.",
            "If these two classes are together so there one closer, so the problem become two class problem that become easy so feature ASAP is perfect.",
            "However, so you still think there three clusters, so you need to explore this.",
            "So in this talk I will actually present a very very simple approach which can well handle this handle this.",
            "So that's the basic."
        ],
        [
            "Yep.",
            "Idea is that.",
            "We using like the recently very very popular spectral analysis of data to capture the like the the intrinsic structure of the data.",
            "Then we using another very very popular method which is like the L1 regularizer like I won't regret regularize approach to select features, so essentially just combine the two very popular technique but combining nicely an can well handle this mouth cluster.",
            "Feature selection pop answer back just saying problem."
        ],
        [
            "So here, so maybe most of you may already be familiar this.",
            "So how to using spectral analysis to analyze data structure generally can construct like the canius neighbor graph.",
            "They can we do can do urgent conversation on this graph matrix.",
            "Then we select those like.",
            "Actually we need to use it like collaboration an we select those like eigenvectors quick bonding to the smallest eigenvalues.",
            "So in this way essentially.",
            "You can regarding this argument is a low dimensional embedding of original space, so essentially the.",
            "The information of our intrinsic data structure of the data user can cap can be captured in this load machine.",
            "Reading all these eigenvectors.",
            "So some very popular like the spectral clustering, essentially using to this method to embedding them to learn and do like normal claiming is counting on this low dimensional embedding.",
            "So here, so when talk about we we need to solve multi cluster structure handle.",
            "So we need to using multiple eigenvectors.",
            "General things usually like the first eigenvector, usually the trivial one, so it corresponds to.",
            "Like all imbedding it will be same.",
            "So we just regarding regard and discovered that one.",
            "So we need to using at least C -- 1 eigenvectors.",
            "See here.",
            "It denotes the number of clusters.",
            "So when we have this loan dimension.",
            "Imagine here we don't have why here, so we need to pick those features which are essentially like important to distincts.",
            "Actually, to associated with this yr distinguish with wine, so in that of."
        ],
        [
            "OK. Yeah, very popular method here.",
            "You also went well, well, studied like the lasso regression.",
            "So here Y will be.",
            "Excuse."
        ],
        [
            "So the Y will be the embedding of we learned in last step.",
            "Why came in the case Eigenvector?",
            "Yeah, and here is essentially is a loss over for me using L1 regularizer to like enforce the sparsity in English we can we learn which features are important with respect to this, why so?",
            "So if we have like multiple, why here we will learn multiple a there.",
            "So essentially things we want to do this like feature selection.",
            "We need to combine the importance with respect features for different.",
            "Like here you can regard."
        ],
        [
            "I add tasks.",
            "So.",
            "We just use their very simple way.",
            "We just pick like the Max.",
            "If so.",
            "Here we have multi cluster.",
            "We need to multiple.",
            "I connect us and we can use.",
            "We can learn multiple a there, so each A is like if you have M dimensional features each A is M dimensional vector.",
            "So the sparse vector.",
            "So we have we have D vectors, so they'll have DM dimensional either.",
            "So for each feature we just assign the MCF score as a Max, like the absolute value among these like KK vectors.",
            "So this actually essentially very.",
            "Then this is goal.",
            "We finally for once like features we simply thought thought the feature according the MCF score.",
            "Then we select the largest one."
        ],
        [
            "So here's the address summary.",
            "We can construct opinions neighbor graph an and just solve generation problem to get K eigenvectors and we solve KL one regularizer aggression to get kids bus compact and we computer MCF Scott and Sex top features.",
            "So this actually is."
        ],
        [
            "So very simple.",
            "So here's the computing complexity analysis, basically.",
            "The most like on time is in time consuming part.",
            "Maybe the graph construction.",
            "So you need do this Kenya's name search and construct Kenya's neighbor graph.",
            "Then follow step are actually very very efficient, especially for the for the last part.",
            "So if you want to select like D, become larger so it may be slow, but however their existing like many many first order method.",
            "Like the previous 2 + 1 like a speaker said like Nesterov method to efficiently solve this like lots of method problem so it can be also be used here.",
            "So."
        ],
        [
            "Experiment, we do two experiments.",
            "Since we are an unsupervised and like first one classes so that everyone will notice the signal.",
            "We name this nearest neighbor classification, but essentially it not classification.",
            "So the settings here we we do feature selection right?",
            "So in us in a reduced space we simply pick any point.",
            "We examine its nearest neighbor whether the two points have the same label share the same label.",
            "So if they share the same level.",
            "We say, oh, it's a very great.",
            "So if they were different we say so.",
            "It's not good.",
            "So we compare 4 algorithm so our algorithm and QR for an logical and maximum."
        ],
        [
            "So essentially we do experiments on four days after the first one in the USPS, the hand range is a second."
        ],
        [
            "Maybe the call tinted like it.",
            "In"
        ],
        [
            "Also image this."
        ],
        [
            "It is so my soul.",
            "It is like the voice like voice."
        ],
        [
            "Finishing so so."
        ],
        [
            "OK, extremely this turmoil is the face image."
        ],
        [
            "So here is the result, so basically.",
            "For each data set we perform like full full scenario.",
            "So the last sentence in our essential we're using all the data, so the previous three.",
            "Essentially we randomly sample three caster or 457 clusters from the entire data set an we do like this this form a subset.",
            "We do this field selection plus caching.",
            "We just ensure that whether we just introduce some line randomness to see whether the algorithm from really good.",
            "So you can see that essentially.",
            "So the black line, essentially the performance using all the features.",
            "So the another like without actually the four algorithms we can see among the like."
        ],
        [
            "All these.",
            "All these four data set our algorithm consider up form the other three competitors, so essentially.",
            "We can easily find that we got in our algorithm.",
            "We only need to select 500 features, so the original feature like the 1000 or like the 1004 two datasets and maybe two 256.",
            "And for the isolate, this had, so we only need 250 features.",
            "Essentially the caching result is almost same as using all the features.",
            "So this to essentially feature selection.",
            "Even unsupervised kids is very."
        ],
        [
            "Useful.",
            "So this is essentially the leave the nearest neighbor classification algorithm, so we simply.",
            "For each data point, we examine is nearest neighbor, so we judge whether they belong to the same class customers.",
            "So then here's a error rate, so we can also see that.",
            "By using only like 50 features, our algorithm CFS, those will select those 50 most important features for describe the whole data set.",
            "So.",
            "So here's the conclusion.",
            "So our algorithm is very, very simple, but very effective.",
            "So it can well handle multicast data can outperform their algorithm so."
        ],
        [
            "Or another, sorry I missed this slide, so here's the parameter selection.",
            "So in our algorithm there only two essential parameter.",
            "The first one is that we need to consider nearest neighbor graph so that is nearest neighbor piece there.",
            "The second one is how many.",
            "I said I need to handle multicast are right so that we use in multiple eigenvectors that that is how many arguments we need to use.",
            "So we can see that the 1st.",
            "So the performance algorithm, you very stable almost like with the range in five between maybe 15.",
            "So it's very stable.",
            "So this show their algorithm not very sensitive to parameter.",
            "So when the nearest neighbor the appeal increase, maybe algorithm will decrease.",
            "This probably due to like the very simple like even in your manifold learning algorithm will need to handle this local.",
            "So if you construct.",
            "Like the complete graph, the local structure is not well captured.",
            "So for the second parameter, so number eigenvector.",
            "Here we can see that the.",
            "The peak out like the performance.",
            "MCFS liking the the best performance around the number of custom.",
            "This again confirm our like or confirm like previous results say that we need to using the legacy K cluster and kick eigenvector too discreet distinguish like a data set with key hidden clusters.",
            "However, even like the using more aggravated performance, also very very good compared to.",
            "It's a three competitors.",
            "Let's mean that even you don't know.",
            "Essentially don't know the like the kid and the number of the data set cost a number of cost in data set.",
            "You can like it simply estimated one.",
            "It should be also also OK like it still go learn very good performance."
        ],
        [
            "So here, so the conclusion.",
            "And anyway."
        ],
        [
            "Yeah.",
            "You know the number of clusters etc."
        ],
        [
            "Yeah, in actually in this prompt.",
            "So the question is our method is unsupervised so we don't need to label.",
            "And the question is whether we our algorithm really want to have to know the number of clusters in the data set.",
            "So I said if we know the clusters we can achieve like the.",
            "Maybe the optimal performance like here so, so I'll like around the 40 and USPS around the 10 and a call 20 around the 20 an isolated around.",
            "Maybe the 26 so.",
            "So here is the X axis is the number of eigen eigen vectors we use here.",
            "So if we know the number of classes we can achieve like the optimal problems.",
            "However, if we don't know that the number of classes the performance is also not not that bad.",
            "So compared to its three competitors.",
            "Like here here you can see that the."
        ],
        [
            "The black curve is always above all the three other elements, so so here's the setting.",
            "A different yet mean three caster, 5 pastor.",
            "Stem Caster is actually I want to introduce some randomness there.",
            "So for this case, so so things together, there are 10 classes, so I randomly select three cast classes to form a subset, so all the remaining tasks algorithm perform on this subset so.",
            "Yeah, that was different setting."
        ],
        [
            "Given setting.",
            "Thought I would.",
            "So the first part about except for the 1st order or really does internationality reduction by projecting onto the.",
            "Yeah.",
            "And then do what you wanna do is 1 actually find features, capture selection and you talk about why is that important so.",
            "For like a full minute like manifold learning algorithm like the lab shag map or ISOMAP or LLE, so the basically the direct final embedding means we know that given at point X we know it's why, but we never know like it's mapping, so I don't know how to transform the original X to Y.",
            "So given lichen, you unseen data so you have to learn like a private algorithm like from the scratch to learn the embedding.",
            "We don't know that.",
            "The transformation even more important, we don't know which features are really important for, like for the capture structure of the data.",
            "So these tasks like very many some other like linear manifolding algorithm.",
            "They can learn the transformation, but they do not do speech selection.",
            "So here we simply using like special analysis to analyze the structure of data.",
            "Then we can really know that which features are important.",
            "For like for capture the structure of data without label information.",
            "Question.",
            "Hey, it's kind of fun in flight.",
            "Wild yeah, so this.",
            "So if there are always some though, the problem is actually here is that if the data another very separable, so their mixed together.",
            "So how can you select the important features?",
            "So in this case I have to say that algorithm failed, so since they all mixed together so.",
            "So technically, if then no label.",
            "So I don't think any algorithm can distinguish like that.",
            "There existed exist several cluster in the data since they're all mixed together, so.",
            "Yeah, so you made this case.",
            "I have to say maybe that Max variance can can be very good since it just select feature with Max variance.",
            "So so you will select those like maybe important feature which came captured most of the information but but so there are some I did not say that so everything can be applied for any cases.",
            "So there are some cases that you can.",
            "You can find this case that maybe other algorithm like the Max variance can outperform our approach.",
            "So yes."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning everyone, so my name isn't I. I came from certain University so this is John work with three inch and shafiqul.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we all know that that we have always have this like high dimension data problem.",
                    "label": 0
                },
                {
                    "sent": "So we have bunch of different like text, image, video, gene.",
                    "label": 1
                },
                {
                    "sent": "So like the previous two activities because also talk about how to handle.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These data so generally we have to approach to approach the first one is dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "Basically that will be learned.",
                    "label": 0
                },
                {
                    "sent": "A combined feature set, so the new feature will be.",
                    "label": 0
                },
                {
                    "sent": "Either linear or nonlinear combination of the original feature the other way.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is it doing feature selection?",
                    "label": 1
                },
                {
                    "sent": "Basically we selected those like important feature with respect your tasks.",
                    "label": 0
                },
                {
                    "sent": "So in general, feature selection method can be categorized in two categories.",
                    "label": 0
                },
                {
                    "sent": "The first one is like a supervised method.",
                    "label": 0
                },
                {
                    "sent": "You already have labels and label can guide you how to select those important features.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second category is uncivil feature feature selection, so they know label information.",
                    "label": 0
                },
                {
                    "sent": "So this talk maybe even harder than the first carry, since there's no label information, so they know crews to help you to decide which feature is important.",
                    "label": 0
                },
                {
                    "sent": "So in this paper in this talk, I will focus on the second Calculator, so particularly as the first speaker also mentioned, there are like three categories for feature selection.",
                    "label": 0
                },
                {
                    "sent": "The first one, the filter method.",
                    "label": 0
                },
                {
                    "sent": "2nd on the wrapper message all the combined method.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I mainly focus on like the filter method or you can see somehow like the combined method since.",
                    "label": 0
                },
                {
                    "sent": "If we do rapper method that you could do unsupervised learning, you maybe you need to follow up at like accounting method so that will be increased like the computational cost.",
                    "label": 0
                },
                {
                    "sent": "So I mainly focus on the future method.",
                    "label": 0
                },
                {
                    "sent": "Basically I can rank feature then you can select feature based on their ranking score.",
                    "label": 0
                },
                {
                    "sent": "So existing method like the mix variance.",
                    "label": 0
                },
                {
                    "sent": "So this is very simple.",
                    "label": 0
                },
                {
                    "sent": "You simply simply compute the variance of each feature and you select the feature by by there.",
                    "label": 0
                },
                {
                    "sent": "Various score this very much like that.",
                    "label": 0
                },
                {
                    "sent": "The principle component analysis, but this is applying on feature selection.",
                    "label": 0
                },
                {
                    "sent": "Let's take along the largest score.",
                    "label": 0
                },
                {
                    "sent": "So this one essentially consider manifold structure, so it evaluates the feature based on the power to preserve the manual structure and another one that QR.",
                    "label": 0
                },
                {
                    "sent": "So these are very similar to level scores.",
                    "label": 0
                },
                {
                    "sent": "Also trying to preserve like the.",
                    "label": 0
                },
                {
                    "sent": "The data structure so I will talk about our algorithm which is M CFS here called multicast or Costa feature selection.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the outline of my remaining talk.",
                    "label": 0
                },
                {
                    "sent": "First, I will give the problem setting and describe our algorithm an, followed by experiments inconnu.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem here we're doing unsupervised.",
                    "label": 0
                },
                {
                    "sent": "No, we do answer by few things, so there no label there, so we are.",
                    "label": 0
                },
                {
                    "sent": "Well, considering like the multicast structure.",
                    "label": 0
                },
                {
                    "sent": "So if there only two casters, so everything becomes easy.",
                    "label": 0
                },
                {
                    "sent": "So like maybe like you are or fiscal can well handle it.",
                    "label": 0
                },
                {
                    "sent": "However when we talk about if the hidden like data structure in multi cluster.",
                    "label": 0
                },
                {
                    "sent": "So here's a very simple synthetic example here.",
                    "label": 0
                },
                {
                    "sent": "So I show like that you're given color only show that they have given cost structure, but essentially they know label there so so you don't know.",
                    "label": 0
                },
                {
                    "sent": "So which car like each point blonde too.",
                    "label": 0
                },
                {
                    "sent": "So there are three three type of like the points so here.",
                    "label": 0
                },
                {
                    "sent": "So they just building in three dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So here the first picture that feature a an feature B.",
                    "label": 0
                },
                {
                    "sent": "So they look like this feature a an efficiency feature B and efficiency because.",
                    "label": 0
                },
                {
                    "sent": "Essentially, if you rank feature one by one, so like the traditional, even like the Max variance or level score or QF method, they always rank like that.",
                    "label": 0
                },
                {
                    "sent": "Maybe feature a off, maybe the first one feature and Phoebe the second one.",
                    "label": 0
                },
                {
                    "sent": "If you see a third one.",
                    "label": 0
                },
                {
                    "sent": "However, if you so you expect to set 2 features so you always select the feature A&B.",
                    "label": 0
                },
                {
                    "sent": "But that is not optimal.",
                    "label": 0
                },
                {
                    "sent": "Maybe a better way is sacrificial ANC or feature B&C?",
                    "label": 0
                },
                {
                    "sent": "So this time essentially you need to explore the mouth problem here.",
                    "label": 0
                },
                {
                    "sent": "Essentially like you already know that even.",
                    "label": 0
                },
                {
                    "sent": "If these two classes are together so there one closer, so the problem become two class problem that become easy so feature ASAP is perfect.",
                    "label": 0
                },
                {
                    "sent": "However, so you still think there three clusters, so you need to explore this.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I will actually present a very very simple approach which can well handle this handle this.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Idea is that.",
                    "label": 0
                },
                {
                    "sent": "We using like the recently very very popular spectral analysis of data to capture the like the the intrinsic structure of the data.",
                    "label": 1
                },
                {
                    "sent": "Then we using another very very popular method which is like the L1 regularizer like I won't regret regularize approach to select features, so essentially just combine the two very popular technique but combining nicely an can well handle this mouth cluster.",
                    "label": 1
                },
                {
                    "sent": "Feature selection pop answer back just saying problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here, so maybe most of you may already be familiar this.",
                    "label": 0
                },
                {
                    "sent": "So how to using spectral analysis to analyze data structure generally can construct like the canius neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "They can we do can do urgent conversation on this graph matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we select those like.",
                    "label": 0
                },
                {
                    "sent": "Actually we need to use it like collaboration an we select those like eigenvectors quick bonding to the smallest eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So in this way essentially.",
                    "label": 0
                },
                {
                    "sent": "You can regarding this argument is a low dimensional embedding of original space, so essentially the.",
                    "label": 0
                },
                {
                    "sent": "The information of our intrinsic data structure of the data user can cap can be captured in this load machine.",
                    "label": 0
                },
                {
                    "sent": "Reading all these eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "So some very popular like the spectral clustering, essentially using to this method to embedding them to learn and do like normal claiming is counting on this low dimensional embedding.",
                    "label": 0
                },
                {
                    "sent": "So here, so when talk about we we need to solve multi cluster structure handle.",
                    "label": 0
                },
                {
                    "sent": "So we need to using multiple eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "General things usually like the first eigenvector, usually the trivial one, so it corresponds to.",
                    "label": 0
                },
                {
                    "sent": "Like all imbedding it will be same.",
                    "label": 0
                },
                {
                    "sent": "So we just regarding regard and discovered that one.",
                    "label": 0
                },
                {
                    "sent": "So we need to using at least C -- 1 eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "See here.",
                    "label": 0
                },
                {
                    "sent": "It denotes the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "So when we have this loan dimension.",
                    "label": 0
                },
                {
                    "sent": "Imagine here we don't have why here, so we need to pick those features which are essentially like important to distincts.",
                    "label": 0
                },
                {
                    "sent": "Actually, to associated with this yr distinguish with wine, so in that of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Yeah, very popular method here.",
                    "label": 0
                },
                {
                    "sent": "You also went well, well, studied like the lasso regression.",
                    "label": 0
                },
                {
                    "sent": "So here Y will be.",
                    "label": 0
                },
                {
                    "sent": "Excuse.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the Y will be the embedding of we learned in last step.",
                    "label": 0
                },
                {
                    "sent": "Why came in the case Eigenvector?",
                    "label": 0
                },
                {
                    "sent": "Yeah, and here is essentially is a loss over for me using L1 regularizer to like enforce the sparsity in English we can we learn which features are important with respect to this, why so?",
                    "label": 0
                },
                {
                    "sent": "So if we have like multiple, why here we will learn multiple a there.",
                    "label": 0
                },
                {
                    "sent": "So essentially things we want to do this like feature selection.",
                    "label": 0
                },
                {
                    "sent": "We need to combine the importance with respect features for different.",
                    "label": 0
                },
                {
                    "sent": "Like here you can regard.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I add tasks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We just use their very simple way.",
                    "label": 0
                },
                {
                    "sent": "We just pick like the Max.",
                    "label": 0
                },
                {
                    "sent": "If so.",
                    "label": 0
                },
                {
                    "sent": "Here we have multi cluster.",
                    "label": 0
                },
                {
                    "sent": "We need to multiple.",
                    "label": 0
                },
                {
                    "sent": "I connect us and we can use.",
                    "label": 0
                },
                {
                    "sent": "We can learn multiple a there, so each A is like if you have M dimensional features each A is M dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "So the sparse vector.",
                    "label": 0
                },
                {
                    "sent": "So we have we have D vectors, so they'll have DM dimensional either.",
                    "label": 0
                },
                {
                    "sent": "So for each feature we just assign the MCF score as a Max, like the absolute value among these like KK vectors.",
                    "label": 0
                },
                {
                    "sent": "So this actually essentially very.",
                    "label": 0
                },
                {
                    "sent": "Then this is goal.",
                    "label": 0
                },
                {
                    "sent": "We finally for once like features we simply thought thought the feature according the MCF score.",
                    "label": 0
                },
                {
                    "sent": "Then we select the largest one.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the address summary.",
                    "label": 0
                },
                {
                    "sent": "We can construct opinions neighbor graph an and just solve generation problem to get K eigenvectors and we solve KL one regularizer aggression to get kids bus compact and we computer MCF Scott and Sex top features.",
                    "label": 1
                },
                {
                    "sent": "So this actually is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So very simple.",
                    "label": 0
                },
                {
                    "sent": "So here's the computing complexity analysis, basically.",
                    "label": 0
                },
                {
                    "sent": "The most like on time is in time consuming part.",
                    "label": 0
                },
                {
                    "sent": "Maybe the graph construction.",
                    "label": 0
                },
                {
                    "sent": "So you need do this Kenya's name search and construct Kenya's neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "Then follow step are actually very very efficient, especially for the for the last part.",
                    "label": 0
                },
                {
                    "sent": "So if you want to select like D, become larger so it may be slow, but however their existing like many many first order method.",
                    "label": 0
                },
                {
                    "sent": "Like the previous 2 + 1 like a speaker said like Nesterov method to efficiently solve this like lots of method problem so it can be also be used here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experiment, we do two experiments.",
                    "label": 0
                },
                {
                    "sent": "Since we are an unsupervised and like first one classes so that everyone will notice the signal.",
                    "label": 0
                },
                {
                    "sent": "We name this nearest neighbor classification, but essentially it not classification.",
                    "label": 1
                },
                {
                    "sent": "So the settings here we we do feature selection right?",
                    "label": 0
                },
                {
                    "sent": "So in us in a reduced space we simply pick any point.",
                    "label": 0
                },
                {
                    "sent": "We examine its nearest neighbor whether the two points have the same label share the same label.",
                    "label": 0
                },
                {
                    "sent": "So if they share the same level.",
                    "label": 0
                },
                {
                    "sent": "We say, oh, it's a very great.",
                    "label": 0
                },
                {
                    "sent": "So if they were different we say so.",
                    "label": 0
                },
                {
                    "sent": "It's not good.",
                    "label": 0
                },
                {
                    "sent": "So we compare 4 algorithm so our algorithm and QR for an logical and maximum.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially we do experiments on four days after the first one in the USPS, the hand range is a second.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe the call tinted like it.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also image this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is so my soul.",
                    "label": 0
                },
                {
                    "sent": "It is like the voice like voice.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finishing so so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, extremely this turmoil is the face image.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the result, so basically.",
                    "label": 0
                },
                {
                    "sent": "For each data set we perform like full full scenario.",
                    "label": 0
                },
                {
                    "sent": "So the last sentence in our essential we're using all the data, so the previous three.",
                    "label": 0
                },
                {
                    "sent": "Essentially we randomly sample three caster or 457 clusters from the entire data set an we do like this this form a subset.",
                    "label": 0
                },
                {
                    "sent": "We do this field selection plus caching.",
                    "label": 0
                },
                {
                    "sent": "We just ensure that whether we just introduce some line randomness to see whether the algorithm from really good.",
                    "label": 0
                },
                {
                    "sent": "So you can see that essentially.",
                    "label": 0
                },
                {
                    "sent": "So the black line, essentially the performance using all the features.",
                    "label": 0
                },
                {
                    "sent": "So the another like without actually the four algorithms we can see among the like.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these.",
                    "label": 0
                },
                {
                    "sent": "All these four data set our algorithm consider up form the other three competitors, so essentially.",
                    "label": 0
                },
                {
                    "sent": "We can easily find that we got in our algorithm.",
                    "label": 0
                },
                {
                    "sent": "We only need to select 500 features, so the original feature like the 1000 or like the 1004 two datasets and maybe two 256.",
                    "label": 0
                },
                {
                    "sent": "And for the isolate, this had, so we only need 250 features.",
                    "label": 0
                },
                {
                    "sent": "Essentially the caching result is almost same as using all the features.",
                    "label": 0
                },
                {
                    "sent": "So this to essentially feature selection.",
                    "label": 0
                },
                {
                    "sent": "Even unsupervised kids is very.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Useful.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially the leave the nearest neighbor classification algorithm, so we simply.",
                    "label": 0
                },
                {
                    "sent": "For each data point, we examine is nearest neighbor, so we judge whether they belong to the same class customers.",
                    "label": 0
                },
                {
                    "sent": "So then here's a error rate, so we can also see that.",
                    "label": 0
                },
                {
                    "sent": "By using only like 50 features, our algorithm CFS, those will select those 50 most important features for describe the whole data set.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here's the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So our algorithm is very, very simple, but very effective.",
                    "label": 0
                },
                {
                    "sent": "So it can well handle multicast data can outperform their algorithm so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or another, sorry I missed this slide, so here's the parameter selection.",
                    "label": 1
                },
                {
                    "sent": "So in our algorithm there only two essential parameter.",
                    "label": 0
                },
                {
                    "sent": "The first one is that we need to consider nearest neighbor graph so that is nearest neighbor piece there.",
                    "label": 0
                },
                {
                    "sent": "The second one is how many.",
                    "label": 0
                },
                {
                    "sent": "I said I need to handle multicast are right so that we use in multiple eigenvectors that that is how many arguments we need to use.",
                    "label": 0
                },
                {
                    "sent": "So we can see that the 1st.",
                    "label": 0
                },
                {
                    "sent": "So the performance algorithm, you very stable almost like with the range in five between maybe 15.",
                    "label": 0
                },
                {
                    "sent": "So it's very stable.",
                    "label": 0
                },
                {
                    "sent": "So this show their algorithm not very sensitive to parameter.",
                    "label": 0
                },
                {
                    "sent": "So when the nearest neighbor the appeal increase, maybe algorithm will decrease.",
                    "label": 0
                },
                {
                    "sent": "This probably due to like the very simple like even in your manifold learning algorithm will need to handle this local.",
                    "label": 0
                },
                {
                    "sent": "So if you construct.",
                    "label": 0
                },
                {
                    "sent": "Like the complete graph, the local structure is not well captured.",
                    "label": 0
                },
                {
                    "sent": "So for the second parameter, so number eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Here we can see that the.",
                    "label": 0
                },
                {
                    "sent": "The peak out like the performance.",
                    "label": 0
                },
                {
                    "sent": "MCFS liking the the best performance around the number of custom.",
                    "label": 1
                },
                {
                    "sent": "This again confirm our like or confirm like previous results say that we need to using the legacy K cluster and kick eigenvector too discreet distinguish like a data set with key hidden clusters.",
                    "label": 0
                },
                {
                    "sent": "However, even like the using more aggravated performance, also very very good compared to.",
                    "label": 0
                },
                {
                    "sent": "It's a three competitors.",
                    "label": 0
                },
                {
                    "sent": "Let's mean that even you don't know.",
                    "label": 0
                },
                {
                    "sent": "Essentially don't know the like the kid and the number of the data set cost a number of cost in data set.",
                    "label": 1
                },
                {
                    "sent": "You can like it simply estimated one.",
                    "label": 0
                },
                {
                    "sent": "It should be also also OK like it still go learn very good performance.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here, so the conclusion.",
                    "label": 0
                },
                {
                    "sent": "And anyway.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You know the number of clusters etc.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, in actually in this prompt.",
                    "label": 0
                },
                {
                    "sent": "So the question is our method is unsupervised so we don't need to label.",
                    "label": 0
                },
                {
                    "sent": "And the question is whether we our algorithm really want to have to know the number of clusters in the data set.",
                    "label": 0
                },
                {
                    "sent": "So I said if we know the clusters we can achieve like the.",
                    "label": 0
                },
                {
                    "sent": "Maybe the optimal performance like here so, so I'll like around the 40 and USPS around the 10 and a call 20 around the 20 an isolated around.",
                    "label": 0
                },
                {
                    "sent": "Maybe the 26 so.",
                    "label": 0
                },
                {
                    "sent": "So here is the X axis is the number of eigen eigen vectors we use here.",
                    "label": 0
                },
                {
                    "sent": "So if we know the number of classes we can achieve like the optimal problems.",
                    "label": 1
                },
                {
                    "sent": "However, if we don't know that the number of classes the performance is also not not that bad.",
                    "label": 0
                },
                {
                    "sent": "So compared to its three competitors.",
                    "label": 0
                },
                {
                    "sent": "Like here here you can see that the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The black curve is always above all the three other elements, so so here's the setting.",
                    "label": 0
                },
                {
                    "sent": "A different yet mean three caster, 5 pastor.",
                    "label": 0
                },
                {
                    "sent": "Stem Caster is actually I want to introduce some randomness there.",
                    "label": 0
                },
                {
                    "sent": "So for this case, so so things together, there are 10 classes, so I randomly select three cast classes to form a subset, so all the remaining tasks algorithm perform on this subset so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that was different setting.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given setting.",
                    "label": 0
                },
                {
                    "sent": "Thought I would.",
                    "label": 0
                },
                {
                    "sent": "So the first part about except for the 1st order or really does internationality reduction by projecting onto the.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And then do what you wanna do is 1 actually find features, capture selection and you talk about why is that important so.",
                    "label": 0
                },
                {
                    "sent": "For like a full minute like manifold learning algorithm like the lab shag map or ISOMAP or LLE, so the basically the direct final embedding means we know that given at point X we know it's why, but we never know like it's mapping, so I don't know how to transform the original X to Y.",
                    "label": 0
                },
                {
                    "sent": "So given lichen, you unseen data so you have to learn like a private algorithm like from the scratch to learn the embedding.",
                    "label": 0
                },
                {
                    "sent": "We don't know that.",
                    "label": 0
                },
                {
                    "sent": "The transformation even more important, we don't know which features are really important for, like for the capture structure of the data.",
                    "label": 0
                },
                {
                    "sent": "So these tasks like very many some other like linear manifolding algorithm.",
                    "label": 0
                },
                {
                    "sent": "They can learn the transformation, but they do not do speech selection.",
                    "label": 0
                },
                {
                    "sent": "So here we simply using like special analysis to analyze the structure of data.",
                    "label": 0
                },
                {
                    "sent": "Then we can really know that which features are important.",
                    "label": 0
                },
                {
                    "sent": "For like for capture the structure of data without label information.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Hey, it's kind of fun in flight.",
                    "label": 0
                },
                {
                    "sent": "Wild yeah, so this.",
                    "label": 0
                },
                {
                    "sent": "So if there are always some though, the problem is actually here is that if the data another very separable, so their mixed together.",
                    "label": 0
                },
                {
                    "sent": "So how can you select the important features?",
                    "label": 0
                },
                {
                    "sent": "So in this case I have to say that algorithm failed, so since they all mixed together so.",
                    "label": 0
                },
                {
                    "sent": "So technically, if then no label.",
                    "label": 0
                },
                {
                    "sent": "So I don't think any algorithm can distinguish like that.",
                    "label": 0
                },
                {
                    "sent": "There existed exist several cluster in the data since they're all mixed together, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you made this case.",
                    "label": 0
                },
                {
                    "sent": "I have to say maybe that Max variance can can be very good since it just select feature with Max variance.",
                    "label": 0
                },
                {
                    "sent": "So so you will select those like maybe important feature which came captured most of the information but but so there are some I did not say that so everything can be applied for any cases.",
                    "label": 0
                },
                {
                    "sent": "So there are some cases that you can.",
                    "label": 0
                },
                {
                    "sent": "You can find this case that maybe other algorithm like the Max variance can outperform our approach.",
                    "label": 0
                },
                {
                    "sent": "So yes.",
                    "label": 0
                }
            ]
        }
    }
}