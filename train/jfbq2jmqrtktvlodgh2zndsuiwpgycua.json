{
    "id": "jfbq2jmqrtktvlodgh2zndsuiwpgycua",
    "title": "GP-LVM for Data Consolidation",
    "info": {
        "author": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/lms08_lawrence_gpdc/",
    "segmentation": [
        [
            "Is GPS data.",
            "Newman OK, thanks GAIL, so apologize.",
            "I left my voice in Vancouver so it's a little bit course.",
            "Hopefully you can all hear.",
            "I should also say this work is.",
            "Shared student between myself and filter.",
            "So Carl Henric, exo.",
            "Everything I'm going to talk about today is his work unfortunately can't be here today so he asked if I could do the presentation.",
            "OK."
        ],
        [
            "So, um.",
            "This is the problem that I'm going to focus on some multiple view problem and what I want you to think about because it really highlights the part of these problems were interested in is an example of silhouettes, so this is a character and we've got data artificially generated data of this character spinning like this, but we have two views.",
            "One is silhouettes of the outline of the character and the other one is the actual pose of the character.",
            "Now what you're seeing here is 2 dimensional reductions of those two different views using GPL VM's, which is just a nonlinear dimensionality reduction method.",
            "And what you're seeing in one is that these two.",
            "Hoses have ambiguous silhouettes, so in the dimensional reduction then mapped to very close points, there are also similar ambiguities all along the rotation.",
            "Now, there's no such ambiguities in the pose either.",
            "We can represent the posed by angles, or XY zed locations, but there were no such ambiguities.",
            "So basically, when we look at the pose data, we get a nice circle representing the rotation through it.",
            "She's gone, so we can represent his rotation.",
            "His three dimensional rotation very nicely, but if we look at the silhouettes we see there's an ambiguity.",
            "OK.",
            "So.",
            "What we're interested in doing to deal with this data is to reduce."
        ],
        [
            "It's dimensionality I'm not going to sort of motivate that today, but the basic idea is that the.",
            "The true data is really low dimensional.",
            "I believe that for a lot of datasets.",
            "So one example you can think of it as a prototype data like the man being rotated.",
            "That's a nonlinear distortion that is a low dimensional data set.",
            "What we're interested in looking at is, well, how can we combine these two different modalities so as people have been talking about, you can sort of concatenate your data observations together.",
            "So I'm using Y to represent the silhouettes, and said to represent the pose in this talk, but you can easily think about things like speech signals combined with lip movements.",
            "Whatever your favorite multiview data set is, then the same ideas probably apply."
        ],
        [
            "OK, So what are our options?",
            "Well, we can think about fusing the data in one latent space like this.",
            "So we've got to have to remember.",
            "We have silhouettes Y and poses as Ed and we basically concatenate them and we look at them with a reduced dimensional space which is shared.",
            "Now this is an assumption that the data sets have an intrinsic low dimensionality.",
            "And what we're sort of doing in our modeling is resuming that each data set is a function.",
            "A possibly nonlinear function of this latent space plus some noise.",
            "So this is the relationship between the reduced 8 dimensional space and the high dimensional space.",
            "Now if you place Gaussian process priors over these Earths then this is what people have called a shared latent space variant of the Gaussian process latent variable model and that was published in 2006.",
            "And then we modified in 2007 and there's also CPR paper in this area for semi supervised learning in 2007.",
            "So that's not what we're going to do in this talk and I'll talk about in a moment why that is.",
            "OK, so why more interesting model perhaps is a CCA type model, and we've seen people talk about CCA models throughout the workshop.",
            "I happen to think that."
        ],
        [
            "Probabilistic formulation of CCA is very useful in understanding what a CCA model is doing, so in CCA with the probabilistic formation."
        ],
        [
            "Very similar to the previous one, only I've added these covariance matrices here because what we say is instead of this noise being independently sampled across."
        ],
        [
            "Each dimension, we assume that this noise is jointly sampled from a Gaussian with a specific covariance.",
            "Anne Francis Bark and Mike Jordan showed that this is probabilistic interpretation of CCA.",
            "So what's going on here is you're basically saying well independently for each data set, I'm going to model you with a Gaussian and then the information that's shared between you.",
            "I'm going to try modeling by a reduced dimensional latent space and linear mappings to your data for CCA.",
            "Now this has been extended to nonlinear mappings, so the standard cases."
        ],
        [
            "Linear where these guys are linear, that leads to CCA.",
            "But then gaylene my chair and Colin 5 have extended this to non linear CCA type models."
        ],
        [
            "Using Gaussian process latent variable models for the functions.",
            "So that's the sort of extension of this model.",
            "Basically changing the noise to be full covariance, Gaussian.",
            "What's the point in that?",
            "Well, it means that you're explaining the independent stuff in each data set separately, and you'll just care about the stuff that's shared between the datasets."
        ],
        [
            "Erratically.",
            "OK, our idea here is to say well and also this is what GAIL was showing in her poster to our idea.",
            "Here is to say what we want to do, something a little bit different in that we don't really believe that the information that's private to each of these data spaces is well represented by a Gaussian covariance.",
            "We want to think that that's well represented by a manifold itself.",
            "So what we have is now private low dimensional latent spaces associated with each of these two views of the data.",
            "So this is Ed has its own private latent space, plus it's shared latent space.",
            "And so does this.",
            "Why?",
            "And the way we model that is now.",
            "It's the same as before.",
            "But now we have this additional private latent space to help explain the data.",
            "So you're trying to explain your data through a sort of regression from some unknown values which include a shared set of values an A private set of values.",
            "OK, so we can do this with the GP LVM and we can optimize GP LVM in this case.",
            "It's just a matter of setting it up structurally correctly, that's no problem.",
            "But there are some issues and I want to skip that for the moment because I'm a bit worried about time."
        ],
        [
            "And those issues are how we initialize the GP LVM.",
            "So the Gaussian process latent variable model is a very very powerful model for probabilistic modeling of data.",
            "But there are thousands of local minima in its optimization space, and it's often relyant in success in a particular application is reliant on getting a good."
        ],
        [
            "Initialization"
        ],
        [
            "So the way the Gaussian process latent variable model works is you just integrate out these mappings and you optimize over these inputs to the mappings, and that's the nasty optimization.",
            "So what you want to do is initialize these inputs to the mappings in some."
        ],
        [
            "Fencible way."
        ],
        [
            "OK, so clearly.",
            "We could lose you something like kernel CCA to initialize that shared subspace.",
            "That's a non linear CCA variant.",
            "It's different to the GP LVM because in kernel CCA the mapping is going the opposite direction.",
            "Actually they're going from the data to the latent space, so that's an interesting thing we can do.",
            "But how do we initialize these private subspaces?",
            "Well, first of all, we can just do CC."
        ],
        [
            "OK, to get WS standards CCA more."
        ],
        [
            "So we can find the optimal of that through an eigenvalue problem.",
            "Now what we're going to extend that to do so?",
            "Hope it's on.",
            "It's on the next slide is also described the private subspaces."
        ],
        [
            "So the way we do that is we look for directions.",
            "So CCA is this eigenvalue problem.",
            "It's giving a specific directions either in a kernel space or in the original data space.",
            "But what we want to do to find the private space is look for directions of maximum data variants that are orthogonal to those Canonical correlates that we've already found.",
            "So we're looking to finds in the kernel.",
            "And then when we canalizing, we're looking to find a vector V of maximum."
        ],
        [
            "Variants where this kernel is in our one of our views of the data which is orthogonal to W where WR the CCA projections we've already found.",
            "So we constrain that to be orthogonal to W. Ann have variants one.",
            "Now it turns out you can find this V1 by another eigenvalue problem.",
            "This is straightforward to do.",
            "Now if you do multiple vis, then you have to do a new eigenvalue problem each time you have to basically be orthogonal to the original Canonical correlates and orthogonal to the directions you've already extracted.",
            "So you end up adding in these views to the thing you're being orthogonal to.",
            "So in some ways that's the major obstacle with getting this model working for this paper, getting a good initialization.",
            "Once we've got this good initialization for these private subspaces.",
            "In fact, in this case the results I'll show you we didn't even optimize away from this analysis visualization.",
            "Carl's been optimizing away from the initialization.",
            "Improved results still further, but in this case, the material I'm showing you, we didn't optimize it with just using the initialization in the Jeep."
        ],
        [
            "Elvia model.",
            "So the nice thing about the CCA is it's a purely spectral algorithm.",
            "There.",
            "This well this algorithm describing for initializing the CCA plus the additional eigenvalue problems.",
            "There purely spectral the optimization problem is convex and they lead to unique solutions.",
            "But the nasty thing about it is that they are very much less useful in inquisition of the model.",
            "So if we want to say.",
            "What pose does this particular latent space correspond to?",
            "That's very difficult to do, that's known as the pre image problem.",
            "Guido Sanquenetti using the audience, and I had a paper on that and there's a lot of optimization to doing that.",
            "What we're doing here.",
            "In effect, you can either see what we're doing in two ways.",
            "You can either see as we've got a spectral algorithm and then we're using Gaussian process mappings from this spectral algorithm today to give us our probabilistic model.",
            "Or you can see how well we've got a GP LVM algorithm that we're initializing with the spectral algorithm.",
            "I think this is a very powerful domain for this GP LVM to work in, and we've used it sort of extensively there."
        ],
        [
            "OK, so the idea is now with a model like this that we could present one, view the silhouette and we can find the corresponding position in the shared latent space for that silhouette, that's.",
            "That can be done.",
            "Once we've got that, we can look at the likelihood of different poses by scanning through the private space associated with the poses.",
            "So we've got a position in the shared space that constrains one part of our mapping to poses, and what we then do is we can scan across on a grid, for example, the private space for the poses and look at all the ad disambiguity's that are in the pose.",
            "Given the silhouette image.",
            "So we're not going to deal with this ambiguation here that can be achieved, for example through temporal information, and that's work Carl has done.",
            "I'm not presenting role results here."
        ],
        [
            "So what I can now tell you is that this model was trained in that way.",
            "So what's going on here is that the shared space was.",
            "This dimension is shared with this dimension, and then the private spaces are this dimensions.",
            "So there's no disambiguation in the private space of the pose.",
            "But there is disambiguation in the private space of the silhouette, so of the.",
            "There's no disambiguation private space of the silhouette, but there is for the pose.",
            "So, for example, if I observe this silhouette that pins me down in the shared space to this point here, which gives me this point here, and I get the ambiguity as to which way he's facing when I scan along this line to which I'm constrained in the shared subspace here."
        ],
        [
            "OK, so we can look at multi modalities by doing that.",
            "So this is a more complicated model still based on silhouettes.",
            "But basically I put a silhouette in and then I'm visualizing the private subspace, which in this case has two modes.",
            "The modes are the guys, one leg forward, white, sit.",
            "There are two modes, but this sort of continuous mode between them.",
            "So actually because he's facing forward, it's very difficult for you to tell where his legs are, so the mode is a sort of.",
            "There are actually two distinct modes, but there's a continuous area of high probability which is associated with all the different leg positions between.",
            "On the right here where the side on view, there's a very clear ambiguity, which is what leg is forward.",
            "Is it the right leg or left leg?",
            "So what you're seeing are the poses a silhouette associated with these two modes.",
            "So there's left leg forward, there's right leg forward, and the same thing here.",
            "So that's sort of just a toy."
        ],
        [
            "Sample to show you the sort of things OK.",
            "So what I'm going to show you here are results from this model.",
            "Now I should warn you.",
            "There is an optical illusion in this model which I can also explain given I've got 15 minutes.",
            "What you're going to see is an input silhouette here, and we've trained on a load of similar data.",
            "This is artificial data 'cause it's real poses, but we're generating the silhouettes using this software called Poser.",
            "And what you're going to see is the ground truth walk in these two views here and our in Ferd walk from just putting the silhouette in.",
            "And getting these guys out.",
            "Actually, we are disambiguating here so this is a result with the disambiguation.",
            "So we're using a temporal model as well which I haven't talked about now.",
            "The things, the mistakes you'll see in the model.",
            "Oh, another point is.",
            "This isn't standard motion capture.",
            "This isn't angle based motion capture.",
            "This is XY zed based motion capture, so the model is predicting the actual position.",
            "The points you're seeing making up the human are a point cloud.",
            "There's no constraints imposed by the length of the limbs or anything else.",
            "The only constraints are coming from what the model has learned.",
            "So if it's doing a bad job, you should see this guy's legs and arms all extending and really unrealistic things coming out and you would see that with linear models and so on and so forth.",
            "You will see where I think you'll pick up that it does about job.",
            "It's sometimes when the silhouette turns a corner, that turn wasn't in the model, and then to try and catch that turn it sort of pauses for a moment and carries on.",
            "What's the optical illusion?",
            "Well, it turns out to be a very bad idea to use dash lines to disambiguate the right from the left, because your eye is very good.",
            "If the dash line goes behind the solid line in a particular way, your eye forces you to think the dashed line is behind.",
            "So what you might see is that the guy appears to turn and then turn back, which is an optical.",
            "Well, I don't know if it's an illusion, but it's it's not what actually happens.",
            "So, and the reason will show you, that's not what actually happens.",
            "If you look at that guy on the right, so.",
            "Just be warned.",
            "It's also in the ground truth that that happens too.",
            "So bear in mind that the guy you're seeing walking here, OK, so notice how he paused there to try and catch up on the corner 'cause it wasn't in the data.",
            "So that's where there's the optical illusion coming in.",
            "What's the guy on the right?",
            "If it freaks you out?",
            "Yeah, it again freaks me out.",
            "So, but it took me awhile to realize what the optical illusion was.",
            "So what you should be saying is that basically the model is doing a really good job of giving a pose that's very similar to the original training data.",
            "We're incredibly accurate in this domain, but this isn't a real.",
            "It's not like we've solved this problem completely.",
            "This is very artificial data, 'cause it's silhouette data that is artificially generated this.",
            "Is actually the silhouette.",
            "The features we're using are very predictive of what the actual poses, but bear in mind.",
            "We aren't using any constraints of our on.",
            "What the limb lengths are, or anything else.",
            "There are various other techniques I could show you videos of where you do regression and various things like that and the limbs just extend in and out and the guy is freaking around all the time, so that's very strong result.",
            "OK.",
            "So the other thing I want to show you is some results on real data that."
        ],
        [
            "Has been working on.",
            "And this is a result from the so called human, either database.",
            "So this is a database of a real video of people walking around where underlying motion capture has also been done.",
            "So you've got the ground truth, motion capture and a video of the walk.",
            "So what we're doing is taking one view of that walk, and we're using so called Hog features, and we build an envy you that's maximum variance unfolding kernel on top of those hog features as Killian Taxi.",
            "It's nice when everyone in the audience, so that's Killian's work, so we basically use one of these kernels.",
            "In our kernel CCA we found that was the best.",
            "We tried various things, Isomap, linear kernels, RBF kernels and that really did a much nicer job than the other stuff.",
            "We represent the images using these 100 dimensional integral hog descriptors.",
            "Um?",
            "I just"
        ],
        [
            "Give you the computation.",
            "The computation is basically the computation of this envy you Colonel how long it takes to run.",
            "I say only frames.",
            "I think it's sort of the order of 1000 frames.",
            "I don't seem to have said it there, so the computation is about 10 minutes on an Intel Core duo and the inference procedure that we use to recover the.",
            "Testator took about few seconds to compute, so it's it's pretty fast.",
            "Now what I'm going to show comparisons with is the so called shared GP LVM.",
            "It doesn't have these separate prior private spaces, and it will also see that that's not being initialized very well, so you get very nasty multimodal solution."
        ],
        [
            "On the share GP LVM.",
            "So here are the images here.",
            "And what you're seeing is the private space associated with the pose.",
            "And you should hopefully be able to pick out a white dot just there, here, and that's the mode we're showing below.",
            "So this is the mode that's closest to the ground truth.",
            "But notice of course that there's ambiguity here.",
            "It's equally happy to think that this lady is walking in the opposite direction.",
            "As she turns the nature of the ambiguity sort of changes, so it's less.",
            "It's not silhouettes, so it's hog features so it's less clear how the ambiguity should exist, but it seems that when she's sort of in this direction, you get this four point ambiguity rather than these two elongated ambiguities, and again the actual.",
            "Pose associated with the white dot, which is the nearest one to the truth, is shown below, so it's a really nice way of visualizing the confusions that are going on.",
            "We haven't well cars, latest work is basically disambiguating in this space and getting actual results on this human, either data."
        ],
        [
            "To look at one of those sort of modes or two, one of the sets of modes.",
            "Here we've got two sides and I'm showing you four points on each of these modes.",
            "And then what you're seeing is what those points look like.",
            "So actually these ambiguities are there whether the front leg is forward or backwards, so the front on post doesn't look very different, But these other poses to the side show that the legs are going in directions, and the other ambiguity is which way they're facing, so those are all the ceramic duties you might expect."
        ],
        [
            "Using the features.",
            "OK, so this is the shared GP LVM and the big problem here is also poor initialization.",
            "Not a large enough dimensionality space to represent this data, so when you look at the likelihoods of pose it well, we can find mode.",
            "So here we are finding some modes and they are quite good there.",
            "Good representations of the data, so despite the ugly latent space it has captured, what's going on in the data, but it's not well ordered together, and the likelihoods of these solutions will be lower than the stuff we've just seen.",
            "So.",
            "It's a distinct improvement on just having one shared subspace switches."
        ],
        [
            "Modeling everything.",
            "Here's another example with those four modes.",
            "Are they going in sort of pointing in different directions but with similar poses too?"
        ],
        [
            "The one we see.",
            "And again, said GP LVM."
        ],
        [
            "This ugly sort of latent space.",
            "OK.",
            "So I think the argument that we're trying to make is that careful Fusion of this multimodal data, or this multiview data multi, whatever you want to call it data at the training stage, allows for sort of elegant disambiguation when only part of the data is available to test time.",
            "What we're doing is effectively missing missing data problem, but we're being quite careful about how we initialize the algorithm.",
            "Further work that causes already done and this improves things a lot is doing a bit of optimization with the GP LVM algorithm.",
            "After you've got the initialization and disambiguation, he's also done this as well, but I don't have the results in the shape to show you with temporal information, so you can sort of understand which mode you're in by tracking which mode.",
            "If you don't have it right, the mode, so it will suddenly jump across from one space to the other.",
            "So you basically understand where you are by having a dynamical model in that private latent space.",
            "OK and then."
        ],
        [
            "The set of some references of.",
            "Ready to work OK?"
        ],
        [
            "Parameters that you have to fix in your code.",
            "Yeah, that's a good question.",
            "So that question is sort of answered a little bit by this paper here.",
            "So there are parameters if we're going to do the kernel.",
            "In fact there aren't for Mvu."
        ],
        [
            "It is just the neighborhood size for envy you, but there are generally sort of parameters that we could fix.",
            "Now.",
            "We tend to do that in the base case by just looking at what the kernel looks like once we've computed it, and seeing if we think it makes sense.",
            "But actually we can be much more principled.",
            "What Stefan Harmeling suggests in this paper is that you do your dimensionality reduction by your spectral method if you like, and then you evaluate its quality by training a Gaussian process on it.",
            "So yes, we would have to choose those parameters, but we can always look at the log likelihood of the different initializations, and Steven shows, so that's.",
            "Really good way of deciding how well your spectral method has unravelled the data so they do really well work well together.",
            "I mean we didn't do it exhaustively, but we did it across because actually started using funny funky combinations of kernels.",
            "But you can certainly deal with that problem that way, but that's a good point.",
            "Extended cause what positive that?",
            "People.",
            "Yeah, it's interesting.",
            "The angle based model is, it's true, but I mean your machine learning audience, so I don't want you to.",
            "If I showed you regression results with the angle based model, then you wouldn't see the guy doing weird stuff.",
            "I didn't show you the regression results, but what I'm really showing you is you've just watched a very high dimensional data set with these point cloud based data is it's probably about 50 or 60 dimensions and you've seen that wasn't completely natural.",
            "Think you're seeing it directly now?",
            "If I show you the angles, it's true we do.",
            "We can do the angles and we've tried it with the angles.",
            "It works with the angles, but you're not visualizing directly a high dimensional data set that you can judge about.",
            "I mean, that's why I love motion capture because you know exactly how humans move and if the model gets it wrong, you will see it so you can do it with angle based.",
            "And we do do it.",
            "But for machine learning papers we prefer not to because it hides the weaknesses of the algorithms.",
            "So.",
            "And I think this paper on extreme component analysis.",
            "Two subspaces.",
            "So was wondering whether in New York City construction again some interesting interpretation.",
            "So I think it's kind of in the extreme components.",
            "Is that their projections handles they cover the whole space?",
            "While I don't think that is the case of your husband.",
            "I'd have to think about that.",
            "I can't immediately see relation.",
            "What they do is they?",
            "They basically have large compared they take look at data with the Gaussian covariance as a structure of large components.",
            "Then they have a sort of noise structure tunnel in the middle, then the other structure of small components.",
            "But I'd have to think about it.",
            "There's a relation.",
            "What we do?",
            "Well, it's not clear because you extract the Canonical correlates first.",
            "So actually the private space could be larger variance than the shared space.",
            "Depending on the data, it's the question that logic was asking before.",
            "If you've got very weak correlation.",
            "I mean, to be fair, I thought the answer to that question was in that case it's not extracting very much interested in the data, it's just a random projection.",
            "If there's very weak correlation between your datasets, you're just going to get rubbish in the shared space, and then the private space will dominate and you won't get a good communication between them.",
            "And we said it's kind of odd that people have worked a lot on CCA and not worried about these private spaces.",
            "You know, they worry only about the shared stuff when actually all most of the information could be not shared.",
            "Justification for the paper orthogonal dimensions in the private space.",
            "Yes, you can do it through an eigenvalue problem, yeah only.",
            "Yeah, it's true.",
            "I mean, it's a very good point why orthogonal directions that it might not be?",
            "I mean the right thing to do is to actually find the probabilistically correct directions, and it's not necessarily clear that those are the orthogonal directions, but it's what people do in PCA, and you know, I think it seems a reasonable thing to do when it's convex.",
            "Yeah, but I think it's a very good question.",
            "I mean, it's one of those things we thought to do it.",
            "We did it, and it worked.",
            "We stop thinking.",
            "Great.",
            "So my name is Lisa model using some kind of nonlinear.",
            "Well, you might use isomer.",
            "What we do in effect we do so we do what I showed you was linear type.",
            "So what Carl did is he looked at kernel CCA, the nonlinear variance and he found the what they tended to do was find a lot of correlation.",
            "The most correlation was often in the really small eigenvalues like the noise and he read a paper by multi person Torah grapple.",
            "That said, it's often better to just project into the new kernel PCA.",
            "Retain a large number of components and then do CCA in the subspace linear CCA in those subspaces, and you lose that problem.",
            "So we sort of have a mix of linear formulations, but it is in effect nonlinear.",
            "But it's not, um.",
            "It's not direct nonlinear CCA because he found that didn't actually work that well.",
            "So yes, linear space linear projection in the non linear subspace."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is GPS data.",
                    "label": 0
                },
                {
                    "sent": "Newman OK, thanks GAIL, so apologize.",
                    "label": 0
                },
                {
                    "sent": "I left my voice in Vancouver so it's a little bit course.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you can all hear.",
                    "label": 0
                },
                {
                    "sent": "I should also say this work is.",
                    "label": 0
                },
                {
                    "sent": "Shared student between myself and filter.",
                    "label": 0
                },
                {
                    "sent": "So Carl Henric, exo.",
                    "label": 0
                },
                {
                    "sent": "Everything I'm going to talk about today is his work unfortunately can't be here today so he asked if I could do the presentation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "This is the problem that I'm going to focus on some multiple view problem and what I want you to think about because it really highlights the part of these problems were interested in is an example of silhouettes, so this is a character and we've got data artificially generated data of this character spinning like this, but we have two views.",
                    "label": 0
                },
                {
                    "sent": "One is silhouettes of the outline of the character and the other one is the actual pose of the character.",
                    "label": 0
                },
                {
                    "sent": "Now what you're seeing here is 2 dimensional reductions of those two different views using GPL VM's, which is just a nonlinear dimensionality reduction method.",
                    "label": 0
                },
                {
                    "sent": "And what you're seeing in one is that these two.",
                    "label": 0
                },
                {
                    "sent": "Hoses have ambiguous silhouettes, so in the dimensional reduction then mapped to very close points, there are also similar ambiguities all along the rotation.",
                    "label": 0
                },
                {
                    "sent": "Now, there's no such ambiguities in the pose either.",
                    "label": 0
                },
                {
                    "sent": "We can represent the posed by angles, or XY zed locations, but there were no such ambiguities.",
                    "label": 0
                },
                {
                    "sent": "So basically, when we look at the pose data, we get a nice circle representing the rotation through it.",
                    "label": 0
                },
                {
                    "sent": "She's gone, so we can represent his rotation.",
                    "label": 0
                },
                {
                    "sent": "His three dimensional rotation very nicely, but if we look at the silhouettes we see there's an ambiguity.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we're interested in doing to deal with this data is to reduce.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's dimensionality I'm not going to sort of motivate that today, but the basic idea is that the.",
                    "label": 0
                },
                {
                    "sent": "The true data is really low dimensional.",
                    "label": 1
                },
                {
                    "sent": "I believe that for a lot of datasets.",
                    "label": 0
                },
                {
                    "sent": "So one example you can think of it as a prototype data like the man being rotated.",
                    "label": 0
                },
                {
                    "sent": "That's a nonlinear distortion that is a low dimensional data set.",
                    "label": 0
                },
                {
                    "sent": "What we're interested in looking at is, well, how can we combine these two different modalities so as people have been talking about, you can sort of concatenate your data observations together.",
                    "label": 0
                },
                {
                    "sent": "So I'm using Y to represent the silhouettes, and said to represent the pose in this talk, but you can easily think about things like speech signals combined with lip movements.",
                    "label": 0
                },
                {
                    "sent": "Whatever your favorite multiview data set is, then the same ideas probably apply.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what are our options?",
                    "label": 0
                },
                {
                    "sent": "Well, we can think about fusing the data in one latent space like this.",
                    "label": 0
                },
                {
                    "sent": "So we've got to have to remember.",
                    "label": 0
                },
                {
                    "sent": "We have silhouettes Y and poses as Ed and we basically concatenate them and we look at them with a reduced dimensional space which is shared.",
                    "label": 0
                },
                {
                    "sent": "Now this is an assumption that the data sets have an intrinsic low dimensionality.",
                    "label": 1
                },
                {
                    "sent": "And what we're sort of doing in our modeling is resuming that each data set is a function.",
                    "label": 0
                },
                {
                    "sent": "A possibly nonlinear function of this latent space plus some noise.",
                    "label": 0
                },
                {
                    "sent": "So this is the relationship between the reduced 8 dimensional space and the high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Now if you place Gaussian process priors over these Earths then this is what people have called a shared latent space variant of the Gaussian process latent variable model and that was published in 2006.",
                    "label": 1
                },
                {
                    "sent": "And then we modified in 2007 and there's also CPR paper in this area for semi supervised learning in 2007.",
                    "label": 0
                },
                {
                    "sent": "So that's not what we're going to do in this talk and I'll talk about in a moment why that is.",
                    "label": 0
                },
                {
                    "sent": "OK, so why more interesting model perhaps is a CCA type model, and we've seen people talk about CCA models throughout the workshop.",
                    "label": 0
                },
                {
                    "sent": "I happen to think that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probabilistic formulation of CCA is very useful in understanding what a CCA model is doing, so in CCA with the probabilistic formation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very similar to the previous one, only I've added these covariance matrices here because what we say is instead of this noise being independently sampled across.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each dimension, we assume that this noise is jointly sampled from a Gaussian with a specific covariance.",
                    "label": 0
                },
                {
                    "sent": "Anne Francis Bark and Mike Jordan showed that this is probabilistic interpretation of CCA.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here is you're basically saying well independently for each data set, I'm going to model you with a Gaussian and then the information that's shared between you.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try modeling by a reduced dimensional latent space and linear mappings to your data for CCA.",
                    "label": 0
                },
                {
                    "sent": "Now this has been extended to nonlinear mappings, so the standard cases.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Linear where these guys are linear, that leads to CCA.",
                    "label": 0
                },
                {
                    "sent": "But then gaylene my chair and Colin 5 have extended this to non linear CCA type models.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using Gaussian process latent variable models for the functions.",
                    "label": 0
                },
                {
                    "sent": "So that's the sort of extension of this model.",
                    "label": 0
                },
                {
                    "sent": "Basically changing the noise to be full covariance, Gaussian.",
                    "label": 0
                },
                {
                    "sent": "What's the point in that?",
                    "label": 0
                },
                {
                    "sent": "Well, it means that you're explaining the independent stuff in each data set separately, and you'll just care about the stuff that's shared between the datasets.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Erratically.",
                    "label": 0
                },
                {
                    "sent": "OK, our idea here is to say well and also this is what GAIL was showing in her poster to our idea.",
                    "label": 0
                },
                {
                    "sent": "Here is to say what we want to do, something a little bit different in that we don't really believe that the information that's private to each of these data spaces is well represented by a Gaussian covariance.",
                    "label": 0
                },
                {
                    "sent": "We want to think that that's well represented by a manifold itself.",
                    "label": 0
                },
                {
                    "sent": "So what we have is now private low dimensional latent spaces associated with each of these two views of the data.",
                    "label": 0
                },
                {
                    "sent": "So this is Ed has its own private latent space, plus it's shared latent space.",
                    "label": 0
                },
                {
                    "sent": "And so does this.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "And the way we model that is now.",
                    "label": 0
                },
                {
                    "sent": "It's the same as before.",
                    "label": 0
                },
                {
                    "sent": "But now we have this additional private latent space to help explain the data.",
                    "label": 0
                },
                {
                    "sent": "So you're trying to explain your data through a sort of regression from some unknown values which include a shared set of values an A private set of values.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can do this with the GP LVM and we can optimize GP LVM in this case.",
                    "label": 0
                },
                {
                    "sent": "It's just a matter of setting it up structurally correctly, that's no problem.",
                    "label": 0
                },
                {
                    "sent": "But there are some issues and I want to skip that for the moment because I'm a bit worried about time.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And those issues are how we initialize the GP LVM.",
                    "label": 0
                },
                {
                    "sent": "So the Gaussian process latent variable model is a very very powerful model for probabilistic modeling of data.",
                    "label": 0
                },
                {
                    "sent": "But there are thousands of local minima in its optimization space, and it's often relyant in success in a particular application is reliant on getting a good.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Initialization",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way the Gaussian process latent variable model works is you just integrate out these mappings and you optimize over these inputs to the mappings, and that's the nasty optimization.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is initialize these inputs to the mappings in some.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fencible way.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so clearly.",
                    "label": 0
                },
                {
                    "sent": "We could lose you something like kernel CCA to initialize that shared subspace.",
                    "label": 1
                },
                {
                    "sent": "That's a non linear CCA variant.",
                    "label": 0
                },
                {
                    "sent": "It's different to the GP LVM because in kernel CCA the mapping is going the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "Actually they're going from the data to the latent space, so that's an interesting thing we can do.",
                    "label": 0
                },
                {
                    "sent": "But how do we initialize these private subspaces?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, we can just do CC.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, to get WS standards CCA more.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can find the optimal of that through an eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "Now what we're going to extend that to do so?",
                    "label": 0
                },
                {
                    "sent": "Hope it's on.",
                    "label": 0
                },
                {
                    "sent": "It's on the next slide is also described the private subspaces.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way we do that is we look for directions.",
                    "label": 0
                },
                {
                    "sent": "So CCA is this eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "It's giving a specific directions either in a kernel space or in the original data space.",
                    "label": 0
                },
                {
                    "sent": "But what we want to do to find the private space is look for directions of maximum data variants that are orthogonal to those Canonical correlates that we've already found.",
                    "label": 1
                },
                {
                    "sent": "So we're looking to finds in the kernel.",
                    "label": 0
                },
                {
                    "sent": "And then when we canalizing, we're looking to find a vector V of maximum.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variants where this kernel is in our one of our views of the data which is orthogonal to W where WR the CCA projections we've already found.",
                    "label": 0
                },
                {
                    "sent": "So we constrain that to be orthogonal to W. Ann have variants one.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out you can find this V1 by another eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "This is straightforward to do.",
                    "label": 0
                },
                {
                    "sent": "Now if you do multiple vis, then you have to do a new eigenvalue problem each time you have to basically be orthogonal to the original Canonical correlates and orthogonal to the directions you've already extracted.",
                    "label": 0
                },
                {
                    "sent": "So you end up adding in these views to the thing you're being orthogonal to.",
                    "label": 0
                },
                {
                    "sent": "So in some ways that's the major obstacle with getting this model working for this paper, getting a good initialization.",
                    "label": 0
                },
                {
                    "sent": "Once we've got this good initialization for these private subspaces.",
                    "label": 0
                },
                {
                    "sent": "In fact, in this case the results I'll show you we didn't even optimize away from this analysis visualization.",
                    "label": 0
                },
                {
                    "sent": "Carl's been optimizing away from the initialization.",
                    "label": 0
                },
                {
                    "sent": "Improved results still further, but in this case, the material I'm showing you, we didn't optimize it with just using the initialization in the Jeep.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Elvia model.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing about the CCA is it's a purely spectral algorithm.",
                    "label": 0
                },
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "This well this algorithm describing for initializing the CCA plus the additional eigenvalue problems.",
                    "label": 0
                },
                {
                    "sent": "There purely spectral the optimization problem is convex and they lead to unique solutions.",
                    "label": 1
                },
                {
                    "sent": "But the nasty thing about it is that they are very much less useful in inquisition of the model.",
                    "label": 0
                },
                {
                    "sent": "So if we want to say.",
                    "label": 0
                },
                {
                    "sent": "What pose does this particular latent space correspond to?",
                    "label": 0
                },
                {
                    "sent": "That's very difficult to do, that's known as the pre image problem.",
                    "label": 0
                },
                {
                    "sent": "Guido Sanquenetti using the audience, and I had a paper on that and there's a lot of optimization to doing that.",
                    "label": 0
                },
                {
                    "sent": "What we're doing here.",
                    "label": 0
                },
                {
                    "sent": "In effect, you can either see what we're doing in two ways.",
                    "label": 1
                },
                {
                    "sent": "You can either see as we've got a spectral algorithm and then we're using Gaussian process mappings from this spectral algorithm today to give us our probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Or you can see how well we've got a GP LVM algorithm that we're initializing with the spectral algorithm.",
                    "label": 0
                },
                {
                    "sent": "I think this is a very powerful domain for this GP LVM to work in, and we've used it sort of extensively there.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the idea is now with a model like this that we could present one, view the silhouette and we can find the corresponding position in the shared latent space for that silhouette, that's.",
                    "label": 1
                },
                {
                    "sent": "That can be done.",
                    "label": 0
                },
                {
                    "sent": "Once we've got that, we can look at the likelihood of different poses by scanning through the private space associated with the poses.",
                    "label": 1
                },
                {
                    "sent": "So we've got a position in the shared space that constrains one part of our mapping to poses, and what we then do is we can scan across on a grid, for example, the private space for the poses and look at all the ad disambiguity's that are in the pose.",
                    "label": 0
                },
                {
                    "sent": "Given the silhouette image.",
                    "label": 0
                },
                {
                    "sent": "So we're not going to deal with this ambiguation here that can be achieved, for example through temporal information, and that's work Carl has done.",
                    "label": 0
                },
                {
                    "sent": "I'm not presenting role results here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I can now tell you is that this model was trained in that way.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here is that the shared space was.",
                    "label": 1
                },
                {
                    "sent": "This dimension is shared with this dimension, and then the private spaces are this dimensions.",
                    "label": 0
                },
                {
                    "sent": "So there's no disambiguation in the private space of the pose.",
                    "label": 1
                },
                {
                    "sent": "But there is disambiguation in the private space of the silhouette, so of the.",
                    "label": 0
                },
                {
                    "sent": "There's no disambiguation private space of the silhouette, but there is for the pose.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if I observe this silhouette that pins me down in the shared space to this point here, which gives me this point here, and I get the ambiguity as to which way he's facing when I scan along this line to which I'm constrained in the shared subspace here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we can look at multi modalities by doing that.",
                    "label": 0
                },
                {
                    "sent": "So this is a more complicated model still based on silhouettes.",
                    "label": 0
                },
                {
                    "sent": "But basically I put a silhouette in and then I'm visualizing the private subspace, which in this case has two modes.",
                    "label": 0
                },
                {
                    "sent": "The modes are the guys, one leg forward, white, sit.",
                    "label": 0
                },
                {
                    "sent": "There are two modes, but this sort of continuous mode between them.",
                    "label": 0
                },
                {
                    "sent": "So actually because he's facing forward, it's very difficult for you to tell where his legs are, so the mode is a sort of.",
                    "label": 0
                },
                {
                    "sent": "There are actually two distinct modes, but there's a continuous area of high probability which is associated with all the different leg positions between.",
                    "label": 0
                },
                {
                    "sent": "On the right here where the side on view, there's a very clear ambiguity, which is what leg is forward.",
                    "label": 0
                },
                {
                    "sent": "Is it the right leg or left leg?",
                    "label": 0
                },
                {
                    "sent": "So what you're seeing are the poses a silhouette associated with these two modes.",
                    "label": 0
                },
                {
                    "sent": "So there's left leg forward, there's right leg forward, and the same thing here.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of just a toy.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample to show you the sort of things OK.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to show you here are results from this model.",
                    "label": 0
                },
                {
                    "sent": "Now I should warn you.",
                    "label": 0
                },
                {
                    "sent": "There is an optical illusion in this model which I can also explain given I've got 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "What you're going to see is an input silhouette here, and we've trained on a load of similar data.",
                    "label": 0
                },
                {
                    "sent": "This is artificial data 'cause it's real poses, but we're generating the silhouettes using this software called Poser.",
                    "label": 0
                },
                {
                    "sent": "And what you're going to see is the ground truth walk in these two views here and our in Ferd walk from just putting the silhouette in.",
                    "label": 0
                },
                {
                    "sent": "And getting these guys out.",
                    "label": 0
                },
                {
                    "sent": "Actually, we are disambiguating here so this is a result with the disambiguation.",
                    "label": 0
                },
                {
                    "sent": "So we're using a temporal model as well which I haven't talked about now.",
                    "label": 0
                },
                {
                    "sent": "The things, the mistakes you'll see in the model.",
                    "label": 0
                },
                {
                    "sent": "Oh, another point is.",
                    "label": 0
                },
                {
                    "sent": "This isn't standard motion capture.",
                    "label": 0
                },
                {
                    "sent": "This isn't angle based motion capture.",
                    "label": 0
                },
                {
                    "sent": "This is XY zed based motion capture, so the model is predicting the actual position.",
                    "label": 0
                },
                {
                    "sent": "The points you're seeing making up the human are a point cloud.",
                    "label": 0
                },
                {
                    "sent": "There's no constraints imposed by the length of the limbs or anything else.",
                    "label": 0
                },
                {
                    "sent": "The only constraints are coming from what the model has learned.",
                    "label": 0
                },
                {
                    "sent": "So if it's doing a bad job, you should see this guy's legs and arms all extending and really unrealistic things coming out and you would see that with linear models and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "You will see where I think you'll pick up that it does about job.",
                    "label": 0
                },
                {
                    "sent": "It's sometimes when the silhouette turns a corner, that turn wasn't in the model, and then to try and catch that turn it sort of pauses for a moment and carries on.",
                    "label": 0
                },
                {
                    "sent": "What's the optical illusion?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out to be a very bad idea to use dash lines to disambiguate the right from the left, because your eye is very good.",
                    "label": 0
                },
                {
                    "sent": "If the dash line goes behind the solid line in a particular way, your eye forces you to think the dashed line is behind.",
                    "label": 0
                },
                {
                    "sent": "So what you might see is that the guy appears to turn and then turn back, which is an optical.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know if it's an illusion, but it's it's not what actually happens.",
                    "label": 0
                },
                {
                    "sent": "So, and the reason will show you, that's not what actually happens.",
                    "label": 0
                },
                {
                    "sent": "If you look at that guy on the right, so.",
                    "label": 0
                },
                {
                    "sent": "Just be warned.",
                    "label": 0
                },
                {
                    "sent": "It's also in the ground truth that that happens too.",
                    "label": 0
                },
                {
                    "sent": "So bear in mind that the guy you're seeing walking here, OK, so notice how he paused there to try and catch up on the corner 'cause it wasn't in the data.",
                    "label": 0
                },
                {
                    "sent": "So that's where there's the optical illusion coming in.",
                    "label": 0
                },
                {
                    "sent": "What's the guy on the right?",
                    "label": 0
                },
                {
                    "sent": "If it freaks you out?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it again freaks me out.",
                    "label": 0
                },
                {
                    "sent": "So, but it took me awhile to realize what the optical illusion was.",
                    "label": 0
                },
                {
                    "sent": "So what you should be saying is that basically the model is doing a really good job of giving a pose that's very similar to the original training data.",
                    "label": 0
                },
                {
                    "sent": "We're incredibly accurate in this domain, but this isn't a real.",
                    "label": 0
                },
                {
                    "sent": "It's not like we've solved this problem completely.",
                    "label": 0
                },
                {
                    "sent": "This is very artificial data, 'cause it's silhouette data that is artificially generated this.",
                    "label": 0
                },
                {
                    "sent": "Is actually the silhouette.",
                    "label": 0
                },
                {
                    "sent": "The features we're using are very predictive of what the actual poses, but bear in mind.",
                    "label": 0
                },
                {
                    "sent": "We aren't using any constraints of our on.",
                    "label": 0
                },
                {
                    "sent": "What the limb lengths are, or anything else.",
                    "label": 0
                },
                {
                    "sent": "There are various other techniques I could show you videos of where you do regression and various things like that and the limbs just extend in and out and the guy is freaking around all the time, so that's very strong result.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the other thing I want to show you is some results on real data that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has been working on.",
                    "label": 0
                },
                {
                    "sent": "And this is a result from the so called human, either database.",
                    "label": 1
                },
                {
                    "sent": "So this is a database of a real video of people walking around where underlying motion capture has also been done.",
                    "label": 1
                },
                {
                    "sent": "So you've got the ground truth, motion capture and a video of the walk.",
                    "label": 1
                },
                {
                    "sent": "So what we're doing is taking one view of that walk, and we're using so called Hog features, and we build an envy you that's maximum variance unfolding kernel on top of those hog features as Killian Taxi.",
                    "label": 0
                },
                {
                    "sent": "It's nice when everyone in the audience, so that's Killian's work, so we basically use one of these kernels.",
                    "label": 0
                },
                {
                    "sent": "In our kernel CCA we found that was the best.",
                    "label": 0
                },
                {
                    "sent": "We tried various things, Isomap, linear kernels, RBF kernels and that really did a much nicer job than the other stuff.",
                    "label": 0
                },
                {
                    "sent": "We represent the images using these 100 dimensional integral hog descriptors.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I just",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give you the computation.",
                    "label": 0
                },
                {
                    "sent": "The computation is basically the computation of this envy you Colonel how long it takes to run.",
                    "label": 0
                },
                {
                    "sent": "I say only frames.",
                    "label": 0
                },
                {
                    "sent": "I think it's sort of the order of 1000 frames.",
                    "label": 0
                },
                {
                    "sent": "I don't seem to have said it there, so the computation is about 10 minutes on an Intel Core duo and the inference procedure that we use to recover the.",
                    "label": 1
                },
                {
                    "sent": "Testator took about few seconds to compute, so it's it's pretty fast.",
                    "label": 0
                },
                {
                    "sent": "Now what I'm going to show comparisons with is the so called shared GP LVM.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have these separate prior private spaces, and it will also see that that's not being initialized very well, so you get very nasty multimodal solution.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the share GP LVM.",
                    "label": 0
                },
                {
                    "sent": "So here are the images here.",
                    "label": 0
                },
                {
                    "sent": "And what you're seeing is the private space associated with the pose.",
                    "label": 0
                },
                {
                    "sent": "And you should hopefully be able to pick out a white dot just there, here, and that's the mode we're showing below.",
                    "label": 0
                },
                {
                    "sent": "So this is the mode that's closest to the ground truth.",
                    "label": 1
                },
                {
                    "sent": "But notice of course that there's ambiguity here.",
                    "label": 0
                },
                {
                    "sent": "It's equally happy to think that this lady is walking in the opposite direction.",
                    "label": 0
                },
                {
                    "sent": "As she turns the nature of the ambiguity sort of changes, so it's less.",
                    "label": 0
                },
                {
                    "sent": "It's not silhouettes, so it's hog features so it's less clear how the ambiguity should exist, but it seems that when she's sort of in this direction, you get this four point ambiguity rather than these two elongated ambiguities, and again the actual.",
                    "label": 0
                },
                {
                    "sent": "Pose associated with the white dot, which is the nearest one to the truth, is shown below, so it's a really nice way of visualizing the confusions that are going on.",
                    "label": 0
                },
                {
                    "sent": "We haven't well cars, latest work is basically disambiguating in this space and getting actual results on this human, either data.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To look at one of those sort of modes or two, one of the sets of modes.",
                    "label": 0
                },
                {
                    "sent": "Here we've got two sides and I'm showing you four points on each of these modes.",
                    "label": 0
                },
                {
                    "sent": "And then what you're seeing is what those points look like.",
                    "label": 0
                },
                {
                    "sent": "So actually these ambiguities are there whether the front leg is forward or backwards, so the front on post doesn't look very different, But these other poses to the side show that the legs are going in directions, and the other ambiguity is which way they're facing, so those are all the ceramic duties you might expect.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using the features.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the shared GP LVM and the big problem here is also poor initialization.",
                    "label": 0
                },
                {
                    "sent": "Not a large enough dimensionality space to represent this data, so when you look at the likelihoods of pose it well, we can find mode.",
                    "label": 0
                },
                {
                    "sent": "So here we are finding some modes and they are quite good there.",
                    "label": 0
                },
                {
                    "sent": "Good representations of the data, so despite the ugly latent space it has captured, what's going on in the data, but it's not well ordered together, and the likelihoods of these solutions will be lower than the stuff we've just seen.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's a distinct improvement on just having one shared subspace switches.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Modeling everything.",
                    "label": 0
                },
                {
                    "sent": "Here's another example with those four modes.",
                    "label": 0
                },
                {
                    "sent": "Are they going in sort of pointing in different directions but with similar poses too?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The one we see.",
                    "label": 0
                },
                {
                    "sent": "And again, said GP LVM.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This ugly sort of latent space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I think the argument that we're trying to make is that careful Fusion of this multimodal data, or this multiview data multi, whatever you want to call it data at the training stage, allows for sort of elegant disambiguation when only part of the data is available to test time.",
                    "label": 1
                },
                {
                    "sent": "What we're doing is effectively missing missing data problem, but we're being quite careful about how we initialize the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Further work that causes already done and this improves things a lot is doing a bit of optimization with the GP LVM algorithm.",
                    "label": 0
                },
                {
                    "sent": "After you've got the initialization and disambiguation, he's also done this as well, but I don't have the results in the shape to show you with temporal information, so you can sort of understand which mode you're in by tracking which mode.",
                    "label": 0
                },
                {
                    "sent": "If you don't have it right, the mode, so it will suddenly jump across from one space to the other.",
                    "label": 0
                },
                {
                    "sent": "So you basically understand where you are by having a dynamical model in that private latent space.",
                    "label": 0
                },
                {
                    "sent": "OK and then.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The set of some references of.",
                    "label": 0
                },
                {
                    "sent": "Ready to work OK?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parameters that you have to fix in your code.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "So that question is sort of answered a little bit by this paper here.",
                    "label": 0
                },
                {
                    "sent": "So there are parameters if we're going to do the kernel.",
                    "label": 0
                },
                {
                    "sent": "In fact there aren't for Mvu.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is just the neighborhood size for envy you, but there are generally sort of parameters that we could fix.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "We tend to do that in the base case by just looking at what the kernel looks like once we've computed it, and seeing if we think it makes sense.",
                    "label": 0
                },
                {
                    "sent": "But actually we can be much more principled.",
                    "label": 0
                },
                {
                    "sent": "What Stefan Harmeling suggests in this paper is that you do your dimensionality reduction by your spectral method if you like, and then you evaluate its quality by training a Gaussian process on it.",
                    "label": 0
                },
                {
                    "sent": "So yes, we would have to choose those parameters, but we can always look at the log likelihood of the different initializations, and Steven shows, so that's.",
                    "label": 0
                },
                {
                    "sent": "Really good way of deciding how well your spectral method has unravelled the data so they do really well work well together.",
                    "label": 0
                },
                {
                    "sent": "I mean we didn't do it exhaustively, but we did it across because actually started using funny funky combinations of kernels.",
                    "label": 0
                },
                {
                    "sent": "But you can certainly deal with that problem that way, but that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Extended cause what positive that?",
                    "label": 0
                },
                {
                    "sent": "People.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's interesting.",
                    "label": 0
                },
                {
                    "sent": "The angle based model is, it's true, but I mean your machine learning audience, so I don't want you to.",
                    "label": 0
                },
                {
                    "sent": "If I showed you regression results with the angle based model, then you wouldn't see the guy doing weird stuff.",
                    "label": 0
                },
                {
                    "sent": "I didn't show you the regression results, but what I'm really showing you is you've just watched a very high dimensional data set with these point cloud based data is it's probably about 50 or 60 dimensions and you've seen that wasn't completely natural.",
                    "label": 0
                },
                {
                    "sent": "Think you're seeing it directly now?",
                    "label": 0
                },
                {
                    "sent": "If I show you the angles, it's true we do.",
                    "label": 0
                },
                {
                    "sent": "We can do the angles and we've tried it with the angles.",
                    "label": 0
                },
                {
                    "sent": "It works with the angles, but you're not visualizing directly a high dimensional data set that you can judge about.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's why I love motion capture because you know exactly how humans move and if the model gets it wrong, you will see it so you can do it with angle based.",
                    "label": 0
                },
                {
                    "sent": "And we do do it.",
                    "label": 0
                },
                {
                    "sent": "But for machine learning papers we prefer not to because it hides the weaknesses of the algorithms.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And I think this paper on extreme component analysis.",
                    "label": 0
                },
                {
                    "sent": "Two subspaces.",
                    "label": 0
                },
                {
                    "sent": "So was wondering whether in New York City construction again some interesting interpretation.",
                    "label": 0
                },
                {
                    "sent": "So I think it's kind of in the extreme components.",
                    "label": 0
                },
                {
                    "sent": "Is that their projections handles they cover the whole space?",
                    "label": 0
                },
                {
                    "sent": "While I don't think that is the case of your husband.",
                    "label": 0
                },
                {
                    "sent": "I'd have to think about that.",
                    "label": 0
                },
                {
                    "sent": "I can't immediately see relation.",
                    "label": 0
                },
                {
                    "sent": "What they do is they?",
                    "label": 0
                },
                {
                    "sent": "They basically have large compared they take look at data with the Gaussian covariance as a structure of large components.",
                    "label": 0
                },
                {
                    "sent": "Then they have a sort of noise structure tunnel in the middle, then the other structure of small components.",
                    "label": 0
                },
                {
                    "sent": "But I'd have to think about it.",
                    "label": 0
                },
                {
                    "sent": "There's a relation.",
                    "label": 0
                },
                {
                    "sent": "What we do?",
                    "label": 0
                },
                {
                    "sent": "Well, it's not clear because you extract the Canonical correlates first.",
                    "label": 0
                },
                {
                    "sent": "So actually the private space could be larger variance than the shared space.",
                    "label": 0
                },
                {
                    "sent": "Depending on the data, it's the question that logic was asking before.",
                    "label": 0
                },
                {
                    "sent": "If you've got very weak correlation.",
                    "label": 0
                },
                {
                    "sent": "I mean, to be fair, I thought the answer to that question was in that case it's not extracting very much interested in the data, it's just a random projection.",
                    "label": 0
                },
                {
                    "sent": "If there's very weak correlation between your datasets, you're just going to get rubbish in the shared space, and then the private space will dominate and you won't get a good communication between them.",
                    "label": 0
                },
                {
                    "sent": "And we said it's kind of odd that people have worked a lot on CCA and not worried about these private spaces.",
                    "label": 0
                },
                {
                    "sent": "You know, they worry only about the shared stuff when actually all most of the information could be not shared.",
                    "label": 0
                },
                {
                    "sent": "Justification for the paper orthogonal dimensions in the private space.",
                    "label": 0
                },
                {
                    "sent": "Yes, you can do it through an eigenvalue problem, yeah only.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's true.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a very good point why orthogonal directions that it might not be?",
                    "label": 0
                },
                {
                    "sent": "I mean the right thing to do is to actually find the probabilistically correct directions, and it's not necessarily clear that those are the orthogonal directions, but it's what people do in PCA, and you know, I think it seems a reasonable thing to do when it's convex.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I think it's a very good question.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's one of those things we thought to do it.",
                    "label": 0
                },
                {
                    "sent": "We did it, and it worked.",
                    "label": 0
                },
                {
                    "sent": "We stop thinking.",
                    "label": 0
                },
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "So my name is Lisa model using some kind of nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Well, you might use isomer.",
                    "label": 0
                },
                {
                    "sent": "What we do in effect we do so we do what I showed you was linear type.",
                    "label": 0
                },
                {
                    "sent": "So what Carl did is he looked at kernel CCA, the nonlinear variance and he found the what they tended to do was find a lot of correlation.",
                    "label": 0
                },
                {
                    "sent": "The most correlation was often in the really small eigenvalues like the noise and he read a paper by multi person Torah grapple.",
                    "label": 0
                },
                {
                    "sent": "That said, it's often better to just project into the new kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "Retain a large number of components and then do CCA in the subspace linear CCA in those subspaces, and you lose that problem.",
                    "label": 0
                },
                {
                    "sent": "So we sort of have a mix of linear formulations, but it is in effect nonlinear.",
                    "label": 0
                },
                {
                    "sent": "But it's not, um.",
                    "label": 0
                },
                {
                    "sent": "It's not direct nonlinear CCA because he found that didn't actually work that well.",
                    "label": 0
                },
                {
                    "sent": "So yes, linear space linear projection in the non linear subspace.",
                    "label": 0
                }
            ]
        }
    }
}