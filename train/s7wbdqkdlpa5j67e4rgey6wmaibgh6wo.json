{
    "id": "s7wbdqkdlpa5j67e4rgey6wmaibgh6wo",
    "title": "Efficient Minimization of Decomposable Submodular Functions",
    "info": {
        "author": [
            "Peter G. Stobbe, California Institute of Technology (Caltech)"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Algorithmic Information Theory"
        ]
    },
    "url": "http://videolectures.net/nips2010_stobbe_emd/",
    "segmentation": [
        [
            "Optimization is ubiquitous in machine learning.",
            "And for continuous optimization, convexity is a key property that makes many problems tractable in much the same way for discrete optimization.",
            "There's a similarly useful property called submodularity now by discrete optimization I mean optimizing a function of subsets.",
            "So if you have some application where you're trying to find the optimal subset of pixels or random variables, or any object, oftentimes the objective function will be submodular.",
            "Now some modularity is defined formally by the inequality shown here on this slide.",
            "But intuitively, you can think of it as a sort of diminishing returns type property.",
            "Now general submodular minimization when you assume nothing about the function other than submodularity, that's polynomial time solvable.",
            "However, the best known complexity is not very practical like order into the 5th.",
            "Of course, if the function resides in certain special subclasses, then there are much faster algorithms you can take advantage of.",
            "However, these subclasses can be somewhat restrictive, so our work is to introduce a new class of submodular functions which is quite rich and expr."
        ],
        [
            "But can also be minimized efficiently with an algorithm that we introduce.",
            "So we call our class of functions decomposable, which means simply that they can be decomposed as a sum of similar functions, each of which is the composition of a concave function with a non negative modular function.",
            "Now these decompositions aren't unique, but we assume we know at least one now the key thing to note is that each term this sum can involve any number of elements, so one example where this might be useful is for map inference.",
            "Of a Markov random field with higher order potentials, so that instead of an easing model where each potential only depends on two elements at a time, now we can take some regions from some over segmentation and then develop concave potentials based on those regions."
        ],
        [
            "So briefly, to go over how the algorithm works, we start off with the standard way to reformulate submodular minimization as a convex minimization problem, where the.",
            "The objective function is called the Bosch extension and then the last extensions nonsmooth.",
            "So we apply modern techniques from Nesterov for smooth minimization of nonsmooth functions.",
            "So in order to do this we affectively have to somehow smooth the function and so you can see here of the two plots up top is a 2 dimensional example of a lavash extension which is convex and non smooth and below is the corresponding smoothed lavash extension now.",
            "In general, for a high dimensional function, it's actually quite difficult to compute the smooth lavosh extension, and this is where it's critical we have a decomposition.",
            "If we have a decomposition, then we actually have an efficient algorithm for computing this smooth lavash extension, and then we can apply our algorithm.",
            "And Lastly, we've got this neat trick where we can actually stop the algorithm little bit early with the optimal answer for the discrete problem without actually having to wait for the convex or the continuous reformulation to converge."
        ],
        [
            "Away.",
            "OK, and so, as mentioned before we tested our algorithm on some image segmentation problems.",
            "And now we can take advantage of higher order potentials.",
            "So using these higher order potentials with this function is more expressive.",
            "Now you can sometimes get better segmentation results and then we also tested.",
            "Synthetic examples and compared the running time to general purpose algorithms, and it was comparable or better than these general purpose algorithms but but the key thing emphasizes that this image segmentation example with higher order potentials.",
            "With State 10s of thousands of variables, we can solve it in about a minute, whereas all the other techniques would take much, much longer, like over an hour.",
            "And so there are a lot of details that I had to leave out, but I think they're really interesting, and if you'd like to learn more about them, please come by poster 25 tonight.",
            "I'd be happy to explain, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimization is ubiquitous in machine learning.",
                    "label": 0
                },
                {
                    "sent": "And for continuous optimization, convexity is a key property that makes many problems tractable in much the same way for discrete optimization.",
                    "label": 1
                },
                {
                    "sent": "There's a similarly useful property called submodularity now by discrete optimization I mean optimizing a function of subsets.",
                    "label": 0
                },
                {
                    "sent": "So if you have some application where you're trying to find the optimal subset of pixels or random variables, or any object, oftentimes the objective function will be submodular.",
                    "label": 0
                },
                {
                    "sent": "Now some modularity is defined formally by the inequality shown here on this slide.",
                    "label": 0
                },
                {
                    "sent": "But intuitively, you can think of it as a sort of diminishing returns type property.",
                    "label": 1
                },
                {
                    "sent": "Now general submodular minimization when you assume nothing about the function other than submodularity, that's polynomial time solvable.",
                    "label": 0
                },
                {
                    "sent": "However, the best known complexity is not very practical like order into the 5th.",
                    "label": 0
                },
                {
                    "sent": "Of course, if the function resides in certain special subclasses, then there are much faster algorithms you can take advantage of.",
                    "label": 0
                },
                {
                    "sent": "However, these subclasses can be somewhat restrictive, so our work is to introduce a new class of submodular functions which is quite rich and expr.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But can also be minimized efficiently with an algorithm that we introduce.",
                    "label": 0
                },
                {
                    "sent": "So we call our class of functions decomposable, which means simply that they can be decomposed as a sum of similar functions, each of which is the composition of a concave function with a non negative modular function.",
                    "label": 1
                },
                {
                    "sent": "Now these decompositions aren't unique, but we assume we know at least one now the key thing to note is that each term this sum can involve any number of elements, so one example where this might be useful is for map inference.",
                    "label": 1
                },
                {
                    "sent": "Of a Markov random field with higher order potentials, so that instead of an easing model where each potential only depends on two elements at a time, now we can take some regions from some over segmentation and then develop concave potentials based on those regions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So briefly, to go over how the algorithm works, we start off with the standard way to reformulate submodular minimization as a convex minimization problem, where the.",
                    "label": 1
                },
                {
                    "sent": "The objective function is called the Bosch extension and then the last extensions nonsmooth.",
                    "label": 0
                },
                {
                    "sent": "So we apply modern techniques from Nesterov for smooth minimization of nonsmooth functions.",
                    "label": 1
                },
                {
                    "sent": "So in order to do this we affectively have to somehow smooth the function and so you can see here of the two plots up top is a 2 dimensional example of a lavash extension which is convex and non smooth and below is the corresponding smoothed lavash extension now.",
                    "label": 0
                },
                {
                    "sent": "In general, for a high dimensional function, it's actually quite difficult to compute the smooth lavosh extension, and this is where it's critical we have a decomposition.",
                    "label": 0
                },
                {
                    "sent": "If we have a decomposition, then we actually have an efficient algorithm for computing this smooth lavash extension, and then we can apply our algorithm.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, we've got this neat trick where we can actually stop the algorithm little bit early with the optimal answer for the discrete problem without actually having to wait for the convex or the continuous reformulation to converge.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Away.",
                    "label": 0
                },
                {
                    "sent": "OK, and so, as mentioned before we tested our algorithm on some image segmentation problems.",
                    "label": 0
                },
                {
                    "sent": "And now we can take advantage of higher order potentials.",
                    "label": 1
                },
                {
                    "sent": "So using these higher order potentials with this function is more expressive.",
                    "label": 0
                },
                {
                    "sent": "Now you can sometimes get better segmentation results and then we also tested.",
                    "label": 0
                },
                {
                    "sent": "Synthetic examples and compared the running time to general purpose algorithms, and it was comparable or better than these general purpose algorithms but but the key thing emphasizes that this image segmentation example with higher order potentials.",
                    "label": 1
                },
                {
                    "sent": "With State 10s of thousands of variables, we can solve it in about a minute, whereas all the other techniques would take much, much longer, like over an hour.",
                    "label": 0
                },
                {
                    "sent": "And so there are a lot of details that I had to leave out, but I think they're really interesting, and if you'd like to learn more about them, please come by poster 25 tonight.",
                    "label": 0
                },
                {
                    "sent": "I'd be happy to explain, thanks.",
                    "label": 0
                }
            ]
        }
    }
}