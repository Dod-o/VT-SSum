{
    "id": "m36kixngpfmfsbgyfbh4lvcsqx2iyi4m",
    "title": "Minimax algorithm for learning rotations",
    "info": {
        "author": [
            "Wojciech Kotlowski, Institute of Computing Science, Poznan University of Technology"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Mathematics->Game Theory",
            "Top->Mathematics->Set Theory"
        ]
    },
    "url": "http://videolectures.net/colt2011_kotlowski_rotations/",
    "segmentation": [
        [
            "I would like to pose no problem about solving the game of learning rotation in a min Max sense.",
            "So as you may guess, I'm the first author and this is.",
            "Joint, the problem is jointly with and mostly by Manfred Warmuth."
        ],
        [
            "So here is the online protocol for the game.",
            "So first the input is input is given to the learner.",
            "This is the unit vector on a."
        ],
        [
            "Here, then, the learner predicts with a rotation.",
            "And it applies to rotation to the input, obtaining some kind of output."
        ],
        [
            "Then the real output is revealed.",
            "And."
        ],
        [
            "The loss which is suffered by the learner is the squared difference between the input between the output vector and the predicted prediction of the learner, and we assume that both the input and Apple vectors are unit vectors on this field."
        ],
        [
            "And then the game continues, so the next out."
        ],
        [
            "As revealed down, the learner predicts."
        ],
        [
            "Rotation next output."
        ],
        [
            "Is is revealed and then the."
        ],
        [
            "Again, computed and so on, and the goal is to minimize the cumulative loss of the learner minus the loss of the best rotation.",
            "We see all the data in hindsight, so this problem might be important because there are various applications in various fields like vision, robotics and so on."
        ],
        [
            "So let me let me give you a bit more details about the problem.",
            "So first of all, the set of rotation matrices is.",
            "This is called SO of N and this is a non covid set, but it is also a Li group equipped with early algebra so it has a nice algebraic structure.",
            "The other thing is that the loss function that we're using, although it's A squared error, is actually Nara in terms of the rotation matrix.",
            "So if you decompose this loss and you notice that some of the terms are have unit norm that you end up with a linear expression with respect to the rotation when the expectation on the on the slide is with the internal randomization of the algorithm, because algorithm potentially can play with randomized rotations.",
            "So the batch problem.",
            "So the problem when you just need to find the single rotation which minimizes this loss on the sequence of data is is known as the bug problem, and it's solvable using the singular value composition."
        ],
        [
            "So previous work on this problem is at NIPS 2009 the matrix exponentiated gradient algorithm is proposed, which in a very nice and elegant way exploits this Lee Group only algebra structure of the set of rotations.",
            "But the problem is that this algorithm is deterministic.",
            "I will tell in a moment, I'll tell you know what actually, it's already on the slide, why it's a problem, and there was also the lack of motivation in terms of regret minimization.",
            "So then there was.",
            "Paper it called 2010.",
            "Buyer has done it all and they showed that any deterministic algorithm can be forced to have linear regret in the adversarial setting."
        ],
        [
            "And they also proposed a randomized gradient decent algorithm, which is like based on Frobenius norm regularization with projections and achieve Sir twice the square root of NT regret bound when N is the dimension of the space and T is the number of iterations.",
            "So the idea of this algorithm is that it uses the gradient descent and it works on the convex Hull of SN.",
            "So in some situations the move towards the gradient we may end up outside this set.",
            "And then we just project back using the Frobenius projecting, which is a special case of background projection."
        ],
        [
            "So the question is, is this problem already solved?",
            "Well, there is an algorithm which achieves this square root of T bound, but is Frobenius norm is a proper way to regularize rotations?",
            "It seems to not exploit this special structure of the rotations and maybe there is some kind of a notion of entropy over so and then it would allow us to come up with some exponentiated gradient algorithm which would.",
            "We should get some reasonable regret bounds.",
            "And so, as I mentioned, this elegantly group and the algebra structure of the problem is not actually exploited and also for gradient descent, the constant that will get in front of the square root of T is not optimal, so it's like for an equal to its twice square root of T. While we know that minimax regret for this problem is just square root of T, so we hope to shed some light on these issues by finding the.",
            "Min Max algorithm for ranking rotations.",
            "So maybe I will."
        ],
        [
            "Tell about our partial results.",
            "So basically we were able to solve a similar problem when we just fix the input vector to be always the same vector.",
            "For example, one like one of the Canonical vectors in R to the N. So if we fix this input vector, then we don't really need to predict with rotations because this input vector makes the.",
            "I mean it will.",
            "It will most of the columns of the rotation matrix are not important, only the first column for each one.",
            "Matters so we can.",
            "As well, just predict with a vector Y head of tea with norm at most one smaller and smaller, the one corresponds to randomization in this case, and we know that the min Max prediction is in this case along the vector S, T -- 1, which is like a sufficient statistic is just the sum over all previous outputs and this is this.",
            "Why had tea is also divided by some shrinking factor, which ranked this vector towards 0?",
            "Which depends on the whole time horizon T and the current iteration.",
            "And we are also able to prove we are also able to prove that the minimax regret for this game is just square root of."
        ],
        [
            "So this game for an equal to so on a plane appears to be the same game as a learning rotations and the simple proof is as follows.",
            "So in case we have arbitrary input vector, what we do is that we rotate this input vector by using the rotation matrix RX, which just takes this input vector and rotate it to the vector one.",
            "Then we also rotate the output in the same way, and then we use the algorithm to solve the simpler problem.",
            "The algorithm will produce rotation RT which we will use and then we can show that the loss of the algorithm.",
            "The simpler problem is the same as the loss of the algorithm in the in the original problem with learning rotations, but it only holds for N equal to because at some point we need to interchange the rotation matrices which commute only for only two dimensions in more than two dimensions it doesn't work anymore."
        ],
        [
            "So that's basically the problem.",
            "To find to solve this game for general Case, an creator imposing too.",
            "Thank you.",
            "Yes.",
            "How big is the constant with current target?",
            "Like?",
            "How big is the gap that you're trying to close so we only know what's the minmax regret for an equal 2.",
            "So the constant is."
        ],
        [
            "So this is the manager at sqrt 2 sqrt T while the currently.",
            "Obtain the rate is.",
            "2 * sqrt 2 * sqrt T OK thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would like to pose no problem about solving the game of learning rotation in a min Max sense.",
                    "label": 0
                },
                {
                    "sent": "So as you may guess, I'm the first author and this is.",
                    "label": 0
                },
                {
                    "sent": "Joint, the problem is jointly with and mostly by Manfred Warmuth.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the online protocol for the game.",
                    "label": 1
                },
                {
                    "sent": "So first the input is input is given to the learner.",
                    "label": 0
                },
                {
                    "sent": "This is the unit vector on a.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, then, the learner predicts with a rotation.",
                    "label": 0
                },
                {
                    "sent": "And it applies to rotation to the input, obtaining some kind of output.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the real output is revealed.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The loss which is suffered by the learner is the squared difference between the input between the output vector and the predicted prediction of the learner, and we assume that both the input and Apple vectors are unit vectors on this field.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the game continues, so the next out.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As revealed down, the learner predicts.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rotation next output.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is is revealed and then the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, computed and so on, and the goal is to minimize the cumulative loss of the learner minus the loss of the best rotation.",
                    "label": 0
                },
                {
                    "sent": "We see all the data in hindsight, so this problem might be important because there are various applications in various fields like vision, robotics and so on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me let me give you a bit more details about the problem.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the set of rotation matrices is.",
                    "label": 1
                },
                {
                    "sent": "This is called SO of N and this is a non covid set, but it is also a Li group equipped with early algebra so it has a nice algebraic structure.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that the loss function that we're using, although it's A squared error, is actually Nara in terms of the rotation matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you decompose this loss and you notice that some of the terms are have unit norm that you end up with a linear expression with respect to the rotation when the expectation on the on the slide is with the internal randomization of the algorithm, because algorithm potentially can play with randomized rotations.",
                    "label": 0
                },
                {
                    "sent": "So the batch problem.",
                    "label": 0
                },
                {
                    "sent": "So the problem when you just need to find the single rotation which minimizes this loss on the sequence of data is is known as the bug problem, and it's solvable using the singular value composition.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So previous work on this problem is at NIPS 2009 the matrix exponentiated gradient algorithm is proposed, which in a very nice and elegant way exploits this Lee Group only algebra structure of the set of rotations.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that this algorithm is deterministic.",
                    "label": 0
                },
                {
                    "sent": "I will tell in a moment, I'll tell you know what actually, it's already on the slide, why it's a problem, and there was also the lack of motivation in terms of regret minimization.",
                    "label": 0
                },
                {
                    "sent": "So then there was.",
                    "label": 0
                },
                {
                    "sent": "Paper it called 2010.",
                    "label": 0
                },
                {
                    "sent": "Buyer has done it all and they showed that any deterministic algorithm can be forced to have linear regret in the adversarial setting.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And they also proposed a randomized gradient decent algorithm, which is like based on Frobenius norm regularization with projections and achieve Sir twice the square root of NT regret bound when N is the dimension of the space and T is the number of iterations.",
                    "label": 1
                },
                {
                    "sent": "So the idea of this algorithm is that it uses the gradient descent and it works on the convex Hull of SN.",
                    "label": 0
                },
                {
                    "sent": "So in some situations the move towards the gradient we may end up outside this set.",
                    "label": 0
                },
                {
                    "sent": "And then we just project back using the Frobenius projecting, which is a special case of background projection.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is, is this problem already solved?",
                    "label": 0
                },
                {
                    "sent": "Well, there is an algorithm which achieves this square root of T bound, but is Frobenius norm is a proper way to regularize rotations?",
                    "label": 0
                },
                {
                    "sent": "It seems to not exploit this special structure of the rotations and maybe there is some kind of a notion of entropy over so and then it would allow us to come up with some exponentiated gradient algorithm which would.",
                    "label": 0
                },
                {
                    "sent": "We should get some reasonable regret bounds.",
                    "label": 0
                },
                {
                    "sent": "And so, as I mentioned, this elegantly group and the algebra structure of the problem is not actually exploited and also for gradient descent, the constant that will get in front of the square root of T is not optimal, so it's like for an equal to its twice square root of T. While we know that minimax regret for this problem is just square root of T, so we hope to shed some light on these issues by finding the.",
                    "label": 1
                },
                {
                    "sent": "Min Max algorithm for ranking rotations.",
                    "label": 0
                },
                {
                    "sent": "So maybe I will.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tell about our partial results.",
                    "label": 1
                },
                {
                    "sent": "So basically we were able to solve a similar problem when we just fix the input vector to be always the same vector.",
                    "label": 0
                },
                {
                    "sent": "For example, one like one of the Canonical vectors in R to the N. So if we fix this input vector, then we don't really need to predict with rotations because this input vector makes the.",
                    "label": 1
                },
                {
                    "sent": "I mean it will.",
                    "label": 0
                },
                {
                    "sent": "It will most of the columns of the rotation matrix are not important, only the first column for each one.",
                    "label": 0
                },
                {
                    "sent": "Matters so we can.",
                    "label": 0
                },
                {
                    "sent": "As well, just predict with a vector Y head of tea with norm at most one smaller and smaller, the one corresponds to randomization in this case, and we know that the min Max prediction is in this case along the vector S, T -- 1, which is like a sufficient statistic is just the sum over all previous outputs and this is this.",
                    "label": 0
                },
                {
                    "sent": "Why had tea is also divided by some shrinking factor, which ranked this vector towards 0?",
                    "label": 0
                },
                {
                    "sent": "Which depends on the whole time horizon T and the current iteration.",
                    "label": 1
                },
                {
                    "sent": "And we are also able to prove we are also able to prove that the minimax regret for this game is just square root of.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this game for an equal to so on a plane appears to be the same game as a learning rotations and the simple proof is as follows.",
                    "label": 0
                },
                {
                    "sent": "So in case we have arbitrary input vector, what we do is that we rotate this input vector by using the rotation matrix RX, which just takes this input vector and rotate it to the vector one.",
                    "label": 0
                },
                {
                    "sent": "Then we also rotate the output in the same way, and then we use the algorithm to solve the simpler problem.",
                    "label": 0
                },
                {
                    "sent": "The algorithm will produce rotation RT which we will use and then we can show that the loss of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The simpler problem is the same as the loss of the algorithm in the in the original problem with learning rotations, but it only holds for N equal to because at some point we need to interchange the rotation matrices which commute only for only two dimensions in more than two dimensions it doesn't work anymore.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's basically the problem.",
                    "label": 0
                },
                {
                    "sent": "To find to solve this game for general Case, an creator imposing too.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "How big is the constant with current target?",
                    "label": 0
                },
                {
                    "sent": "Like?",
                    "label": 0
                },
                {
                    "sent": "How big is the gap that you're trying to close so we only know what's the minmax regret for an equal 2.",
                    "label": 0
                },
                {
                    "sent": "So the constant is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the manager at sqrt 2 sqrt T while the currently.",
                    "label": 0
                },
                {
                    "sent": "Obtain the rate is.",
                    "label": 0
                },
                {
                    "sent": "2 * sqrt 2 * sqrt T OK thank you.",
                    "label": 0
                }
            ]
        }
    }
}