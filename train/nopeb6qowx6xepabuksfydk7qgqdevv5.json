{
    "id": "nopeb6qowx6xepabuksfydk7qgqdevv5",
    "title": "Learning Nonlinear Dynamic Models",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Machine Learning Department, Carnegie Mellon University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_salakhutdinov_lndm/",
    "segmentation": [
        [
            "Buy some."
        ],
        [
            "Um?",
            "Mapping B, that's going to take our observation as well as the previous sufficient statistic parameter, and that's going to be used.",
            "And these deterministic function that's going to be used to update the next vector right?",
            "And so in that particular diagram, you can see that B is a mapping that takes your observation.",
            "It takes the previous state and it's used to predict.",
            "The next state, and it's a deterministic function, and we can also give an arbitrary value to our initial state, and we're just going to have a mapping A.",
            "Right, it's just for initial conditions."
        ],
        [
            "And the second thing we need to do is we need to make predictions.",
            "So we're going to have some again deterministic map C. That's going to take our vector of sufficient statistics and predict the distribution over the next time step.",
            "OK, now the key observation here is that all of these mappings are deterministic, and this is what will allow us to learn efficiently nonlinear systems.",
            "Or have good predictions for models that have whose underlying dynamics is nonlinear?",
            "Um?"
        ],
        [
            "So let me just specify what the sufficient Misty representation for dynamical model is.",
            "It's basically given by we have observed sequence.",
            "We have an observed hidden state and here you should remember that the hidden state is really the sufficient statistic for your belief, right?",
            "It's a really sufficient statistics for your posterior distribution over your hidden states.",
            "We're going to have a state initialization map.",
            "State update map or state evolution map and the prediction map.",
            "OK, once we specify.",
            "What kind of forms we're going to have for our Maps?",
            "We can proceed with learning.",
            "Yes, there is a question.",
            "Observation variance means given that.",
            "Your X sub T second last line is good influence yours or state.",
            "Usually the observation independent from the."
        ],
        [
            "Date.",
            "I mean, it's a function of state, but it's not your your your, your observation as well as your previous state.",
            "Just like in the traditional view, influence so that makes it a state sort of.",
            "Really, if you was really the.",
            "Subsequent state.",
            "Yep.",
            "It sufficient statistics.",
            "That's for the belief state.",
            "You can think of it that way, but at the end of the end of the presentation will give a very precise form of what these mappings are and go through through specific example.",
            "So now."
        ],
        [
            "Now the question is how can we learn the parameters of these Maps and we're just going to do bunch of simple prediction."
        ],
        [
            "Right, so our first prediction problem is we're going to try to predict the distribution over X2 given X one through Maps, A&C, and in that representation you can think of the state as the information that summarizes or helps the first observation to predict the next observation.",
            "OK, whatever that may be.",
            "Typically in our experiments the dimensionality of this state is much smaller than the dimensionality of the observation case, but it doesn't necessarily have to be that way.",
            "If people are familiar with autoencoders, type of learning that people using deep belief Nets community, then you can think of this as being a sort of an auto encoder module.",
            "But instead of reconstructing the observation itself, you're trying to reconstruct next time step, and this is particularly useful when you have very high dimensional.",
            "Observations.",
            "And the other point, I would like to make is that the internal state can come from any learning algorithm, so you can use any supervised learning algorithms for these prediction problems."
        ],
        [
            "And then we're going to solve the second prediction problem and the second prediction problem is again just takes your observation.",
            "It takes your previous state, and then it makes prediction over over the next.",
            "It looks at the probability distribution over the next time step.",
            "And once you put all of these things together, then effectively."
        ],
        [
            "What you're doing is you're solving bunch of supervised learning problems.",
            "OK, so initially you solve the rat problem.",
            "Then you use its internal representation as an input for training the black problem.",
            "Right, so an which is then you use the representation for solving the blue problem.",
            "So this is very similar to the idea of pre training that people use in the belief Nets community and we sort of training one module we take its internal representation and then we start training the next module using the internal representation that we get from the previous problem right?",
            "And then we do backpropagation through time and this seems to be quite an important thing because if you just start doing backpropagation through time.",
            "It's very hard to train these models.",
            "Yeah.",
            "So if you have a bunch of sequences, you presumably in the training set, you have multiple sequences, so for those multiple sequences.",
            "You just train the first part.",
            "Right and.",
            "There are various ways of training these things and I can.",
            "Look at it after after the talk.",
            "So now one question you may ask is if if we have this procedure, will it be consistent?",
            "Will we be able to recover these Maps such that they could be good at predicting future events?",
            "And the answer is yes, we can show the consistency, although in a very limited setting, but we also need the notion of invertibility."
        ],
        [
            "And the notion of invertibility is basically saying that.",
            "The dynamical model is invertible if there exists a function R which you can think of it as an inverse function, such that for all T, if we apply this function R2R mapping C, we can recover back our state use of T and effectively.",
            "Intuitively, what it means is the following.",
            "Suppose I have a mapping C that's that's used to predict the next case steps instead of predicting just a single step in the future.",
            "Then in that case, invariability basically means that if two states you tnu septi.",
            "Induce the same short range behavior.",
            "Over the next K timesteps, then they are identical in the sense that they induce the same long range behavior for all time steps, right?",
            "And it's a very natural assumption to have, and generally speaking.",
            "Non verbal models are models that we cannot efficiently train anyways.",
            "Just because we don't have sufficient information to be able to differentiate or recover two states that have different long range behavior but behave identically in short ranges.",
            "And again, by considering different Maps by considering predicting further into the future, you can broaden the class of convertible models.",
            "This is more of a technical thing that we actually needed in order to show the consistency, and if you're interested to see the consistency, you could probably look at the paper or come to our poster.",
            "Now let's look at the experimental setup."
        ],
        [
            "Um?",
            "In in our simple experiments we basically trained bunch of neural networks OK, so if you look at the read problem, the red problem could be filled as a simple neural network.",
            "With one hidden layer.",
            "OK. Once we train this simple neural net, we're going to take the representation from that simple neural net and use it as an input for training the next neural network and so forth.",
            "So again, it's very similar to the flavor of pretraining.",
            "Deep belief networks where you train one module and use its hidden representation for as an input for training the next module and.",
            "At that's a simple.",
            "It's a simple algorithm and it's a simple way.",
            "It's very also very easy to do backpropagation.",
            "In this model as well backpropagation through time.",
            "So we've tried on those two experiments on 2."
        ],
        [
            "Two datasets, one is the motion capture.",
            "Data is basically sequences of 3D joint angles and there were various working styles like people were working normally or people were simulating working as if they were drunk or graceful chicken walk and such.",
            "There were 30 training and a test sequences each of length 50 and each time step was represented by 58 dimensional.",
            "Real valued numbers.",
            "OK, so the dimensionality of the input space was 58, so it wasn't a very big problem.",
            "And if we look at the performance."
        ],
        [
            "So only Y axis, I'm showing the task squared error and on the X axis I'm showing the length of prediction horizon.",
            "So you can see that if we compare it to simple hidden Markov models then we can do much better if you actually.",
            "If you also look at the predictions of a simple linear dynamical system, it's making the curve is somewhere in between those two lines.",
            "Um, now we also compare it to simple linear prediction problems and simple linear prediction problems would just look at the two previous time step and make predictions into the future at XT plus 10 or X D + 16.",
            "So we were training a multitude of simple linear prediction models and what happens in this graph is quite interesting.",
            "Is basically saying that we're losing in terms of predicting the next time step.",
            "And this is probably due to the fact that in the motion capture data nearby time steps have been very linear.",
            "Type of behavior can predict really well, but if you look at predicting it time steps from now or predicting it X D + 10, then we are doing much better.",
            "Then simple linear models that conditioning on the previous five steps on the previous two steps and this this yellow line sort of green line is the average prediction.",
            "Basically it means that instead of looking, you just always predict the mean.",
            "Right, so you can see that we can never predict after 25 steps is basically random.",
            "You're not predicting anything.",
            "So this was a sort of like a smallish data set.",
            "But we were a little disappointed to see that you know, linear models do just as well as our model for the motion capture data.",
            "So it just suggests that this is not a very nonlinear model, so we've tried."
        ],
        [
            "Modeling video data so these way video sequences of nine human subjects they were doing various actions such as waving one hand or two hands or jumping or bending or sitting down.",
            "There were 36 training and test and test sequences and again each time step was represented by 464 dimensional real valued numbers.",
            "So it was I think 29 by 16 also quite small.",
            "Images of video now in it.",
            "In that case, you'd expect the relationship between pixels.",
            "Annual underlying dynamics to be very nonlinear, right?",
            "So here the results again.",
            "We can show that we can do much better than him."
        ],
        [
            "Of models and then in that that data set, we actually did considerably better.",
            "Too simple.",
            "Simple linear regression.",
            "Models, and in particular we do consider it better into predicting further into the future.",
            "So this gives us the advantage of if you really care about recovering the dynamics and making good predictions far into the future.",
            "This approach seems to be working fairly well.",
            "One interesting aspect about that model that I wanted to point out is that.",
            "Conditioning on the five previous steps was actually working worse than conditioning on the two previous steps, and this seems like a very weird behavior.",
            "But what's happening here is that if you conditional five previous steps, then you effectively have a very high dimensional input, and you're trying to make a linear prediction to the very high dimensional output, and here for these models, regularization becomes an important thing.",
            "You have to be able to regularize, and it's much harder to regularize for.",
            "Therefore, building simple linear regression.",
            "And in input spaces is probably not a very good approach.",
            "Um?"
        ],
        [
            "And I think I'm done, thank you."
        ],
        [
            "Yeah.",
            "Stack.",
            "Different layers.",
            "So in this case, it's not really sticking North.",
            "Encoders like the intuition is very similar because if you take this model, Anja twisted upside down, you will have the same basic idea of pretraining stacks, stacked autoencoders into multiple levels.",
            "But in this case these stackings we're done depending on the time series.",
            "So if there were 50 dimensional sequences then you would have I guess 49.",
            "Stacks so you sticking it in time instead of stacking it into layers.",
            "You can also.",
            "Alright, so I haven't tried that.",
            "Yep.",
            "Because you could take.",
            "Best technique?",
            "Using function.",
            "Something the state and then it maximizing.",
            "This model.",
            "Method.",
            "Let's be protested models for real data.",
            "Or and doesn't work that well.",
            "But I can pay for this year.",
            "Not, but it's generally true.",
            "Space.",
            "And beyond that.",
            "Basic methods for doing a parameter estimation formalities distress calls me the whole field of volatility.",
            "Algorithms require that you have a substantial amount of prior knowledge, but the dynamic system because you you specify that the distribution of the transformation distribution can come up to a few parameters.",
            "Men in between those parameters.",
            "I guess what I'm finishing about this is that.",
            "You don't.",
            "Possible chance we can just apply supervised learning algorithms and develop a good non linear predictor?",
            "The sampling process.",
            "The little bit less efficient computationally then.",
            "But it's definitely the case that we need to do more comparison to the existing work.",
            "That's absolutely true.",
            "Yeah.",
            "To say something.",
            "This looks to me like surprise.",
            "And so.",
            "MEMM this is maximum entropy mark my cough.",
            "So we have like some kind of like.",
            "None unseen label bias.",
            "You're just going in One Direction.",
            "Right, so when you are training, you can go in One Direction, but you the parameters are shared through the entire thing, right?",
            "And you can sort of do multiple passes through training these things and then finally you also using backpropagation to refine the entire parameters in your model so you know.",
            "There was, yeah.",
            "Presenting the speaker.",
            "Delta.",
            "Because you use the new X ND, you calculate your next view.",
            "Next generation.",
            "And in fact, you end up with first.",
            "Position.",
            "Build your model, calculate your future position.",
            "I'm not sure I have to think about it because the way I think about these things, these representations, I think of them as some distributed representations of of that somehow summarized the entire past history and allow me to make good predictions into the future.",
            "Maybe there are alternative use of using of viewing these what these are but.",
            "There is.",
            "And the reason that for the motion test.",
            "Get about the same result is because you probably there's probably no real acceleration and the linear system as he is using the speed only when you get the same result.",
            "Well, for linear for linear predictions I did condition on two previous steps or five previous steps.",
            "So in that respect, linear model could potentially see the acceleration right?",
            "Because you need three steps to figure out roughly what the acceleration is.",
            "So it's.",
            "It depends on what right?",
            "I. Dragon.",
            "Why?",
            "This distinction.",
            "The same thing, but will you state can sense that it is a sufficient statistic for subsequent sequence.",
            "Is this like it application between the idea that you know when people talk about the state in general information systems?",
            "They kind of this idea?",
            "Actual statements out there, so why is like the environmental real state that's right?",
            "That's exactly right, yeah.",
            "Reason why we need to make it.",
            "Why is that?",
            "Yeah, so in particular in that particular representation we never instantiate.",
            "For example wise we never specified what the dynamics is of the underlying dynamical system.",
            "Whatever state, what we call state, we get as a byproduct of the learning problem.",
            "Right?",
            "Yeah, I presume so.",
            "If you want to look at it this way, yes.",
            "Ask question.",
            "So one question is.",
            "So yeah, so we've thought about doing robustness analysis to figure out, you know, if your prediction problems are not correct, how the errors would get propagated forward, and we haven't done that yet.",
            "But that's something to do on our list.",
            "So there's a paper coming.",
            "And.",
            "If you don't want to have achievements.",
            "We need this.",
            "Couldn't eigenvalues which produced the information that you're getting these around there.",
            "Yeah, but it's a little bit tricky to do it in only in case.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Buy some.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Mapping B, that's going to take our observation as well as the previous sufficient statistic parameter, and that's going to be used.",
                    "label": 0
                },
                {
                    "sent": "And these deterministic function that's going to be used to update the next vector right?",
                    "label": 0
                },
                {
                    "sent": "And so in that particular diagram, you can see that B is a mapping that takes your observation.",
                    "label": 0
                },
                {
                    "sent": "It takes the previous state and it's used to predict.",
                    "label": 0
                },
                {
                    "sent": "The next state, and it's a deterministic function, and we can also give an arbitrary value to our initial state, and we're just going to have a mapping A.",
                    "label": 1
                },
                {
                    "sent": "Right, it's just for initial conditions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the second thing we need to do is we need to make predictions.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have some again deterministic map C. That's going to take our vector of sufficient statistics and predict the distribution over the next time step.",
                    "label": 0
                },
                {
                    "sent": "OK, now the key observation here is that all of these mappings are deterministic, and this is what will allow us to learn efficiently nonlinear systems.",
                    "label": 1
                },
                {
                    "sent": "Or have good predictions for models that have whose underlying dynamics is nonlinear?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just specify what the sufficient Misty representation for dynamical model is.",
                    "label": 0
                },
                {
                    "sent": "It's basically given by we have observed sequence.",
                    "label": 1
                },
                {
                    "sent": "We have an observed hidden state and here you should remember that the hidden state is really the sufficient statistic for your belief, right?",
                    "label": 0
                },
                {
                    "sent": "It's a really sufficient statistics for your posterior distribution over your hidden states.",
                    "label": 1
                },
                {
                    "sent": "We're going to have a state initialization map.",
                    "label": 0
                },
                {
                    "sent": "State update map or state evolution map and the prediction map.",
                    "label": 1
                },
                {
                    "sent": "OK, once we specify.",
                    "label": 0
                },
                {
                    "sent": "What kind of forms we're going to have for our Maps?",
                    "label": 0
                },
                {
                    "sent": "We can proceed with learning.",
                    "label": 0
                },
                {
                    "sent": "Yes, there is a question.",
                    "label": 0
                },
                {
                    "sent": "Observation variance means given that.",
                    "label": 0
                },
                {
                    "sent": "Your X sub T second last line is good influence yours or state.",
                    "label": 0
                },
                {
                    "sent": "Usually the observation independent from the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Date.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a function of state, but it's not your your your, your observation as well as your previous state.",
                    "label": 0
                },
                {
                    "sent": "Just like in the traditional view, influence so that makes it a state sort of.",
                    "label": 0
                },
                {
                    "sent": "Really, if you was really the.",
                    "label": 0
                },
                {
                    "sent": "Subsequent state.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "It sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "That's for the belief state.",
                    "label": 0
                },
                {
                    "sent": "You can think of it that way, but at the end of the end of the presentation will give a very precise form of what these mappings are and go through through specific example.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the question is how can we learn the parameters of these Maps and we're just going to do bunch of simple prediction.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so our first prediction problem is we're going to try to predict the distribution over X2 given X one through Maps, A&C, and in that representation you can think of the state as the information that summarizes or helps the first observation to predict the next observation.",
                    "label": 0
                },
                {
                    "sent": "OK, whatever that may be.",
                    "label": 0
                },
                {
                    "sent": "Typically in our experiments the dimensionality of this state is much smaller than the dimensionality of the observation case, but it doesn't necessarily have to be that way.",
                    "label": 0
                },
                {
                    "sent": "If people are familiar with autoencoders, type of learning that people using deep belief Nets community, then you can think of this as being a sort of an auto encoder module.",
                    "label": 0
                },
                {
                    "sent": "But instead of reconstructing the observation itself, you're trying to reconstruct next time step, and this is particularly useful when you have very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Observations.",
                    "label": 0
                },
                {
                    "sent": "And the other point, I would like to make is that the internal state can come from any learning algorithm, so you can use any supervised learning algorithms for these prediction problems.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we're going to solve the second prediction problem and the second prediction problem is again just takes your observation.",
                    "label": 1
                },
                {
                    "sent": "It takes your previous state, and then it makes prediction over over the next.",
                    "label": 1
                },
                {
                    "sent": "It looks at the probability distribution over the next time step.",
                    "label": 0
                },
                {
                    "sent": "And once you put all of these things together, then effectively.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you're doing is you're solving bunch of supervised learning problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so initially you solve the rat problem.",
                    "label": 0
                },
                {
                    "sent": "Then you use its internal representation as an input for training the black problem.",
                    "label": 0
                },
                {
                    "sent": "Right, so an which is then you use the representation for solving the blue problem.",
                    "label": 0
                },
                {
                    "sent": "So this is very similar to the idea of pre training that people use in the belief Nets community and we sort of training one module we take its internal representation and then we start training the next module using the internal representation that we get from the previous problem right?",
                    "label": 0
                },
                {
                    "sent": "And then we do backpropagation through time and this seems to be quite an important thing because if you just start doing backpropagation through time.",
                    "label": 1
                },
                {
                    "sent": "It's very hard to train these models.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So if you have a bunch of sequences, you presumably in the training set, you have multiple sequences, so for those multiple sequences.",
                    "label": 0
                },
                {
                    "sent": "You just train the first part.",
                    "label": 0
                },
                {
                    "sent": "Right and.",
                    "label": 0
                },
                {
                    "sent": "There are various ways of training these things and I can.",
                    "label": 0
                },
                {
                    "sent": "Look at it after after the talk.",
                    "label": 0
                },
                {
                    "sent": "So now one question you may ask is if if we have this procedure, will it be consistent?",
                    "label": 0
                },
                {
                    "sent": "Will we be able to recover these Maps such that they could be good at predicting future events?",
                    "label": 0
                },
                {
                    "sent": "And the answer is yes, we can show the consistency, although in a very limited setting, but we also need the notion of invertibility.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the notion of invertibility is basically saying that.",
                    "label": 0
                },
                {
                    "sent": "The dynamical model is invertible if there exists a function R which you can think of it as an inverse function, such that for all T, if we apply this function R2R mapping C, we can recover back our state use of T and effectively.",
                    "label": 1
                },
                {
                    "sent": "Intuitively, what it means is the following.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have a mapping C that's that's used to predict the next case steps instead of predicting just a single step in the future.",
                    "label": 0
                },
                {
                    "sent": "Then in that case, invariability basically means that if two states you tnu septi.",
                    "label": 0
                },
                {
                    "sent": "Induce the same short range behavior.",
                    "label": 1
                },
                {
                    "sent": "Over the next K timesteps, then they are identical in the sense that they induce the same long range behavior for all time steps, right?",
                    "label": 1
                },
                {
                    "sent": "And it's a very natural assumption to have, and generally speaking.",
                    "label": 0
                },
                {
                    "sent": "Non verbal models are models that we cannot efficiently train anyways.",
                    "label": 0
                },
                {
                    "sent": "Just because we don't have sufficient information to be able to differentiate or recover two states that have different long range behavior but behave identically in short ranges.",
                    "label": 0
                },
                {
                    "sent": "And again, by considering different Maps by considering predicting further into the future, you can broaden the class of convertible models.",
                    "label": 0
                },
                {
                    "sent": "This is more of a technical thing that we actually needed in order to show the consistency, and if you're interested to see the consistency, you could probably look at the paper or come to our poster.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at the experimental setup.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In in our simple experiments we basically trained bunch of neural networks OK, so if you look at the read problem, the red problem could be filled as a simple neural network.",
                    "label": 0
                },
                {
                    "sent": "With one hidden layer.",
                    "label": 0
                },
                {
                    "sent": "OK. Once we train this simple neural net, we're going to take the representation from that simple neural net and use it as an input for training the next neural network and so forth.",
                    "label": 0
                },
                {
                    "sent": "So again, it's very similar to the flavor of pretraining.",
                    "label": 0
                },
                {
                    "sent": "Deep belief networks where you train one module and use its hidden representation for as an input for training the next module and.",
                    "label": 0
                },
                {
                    "sent": "At that's a simple.",
                    "label": 0
                },
                {
                    "sent": "It's a simple algorithm and it's a simple way.",
                    "label": 0
                },
                {
                    "sent": "It's very also very easy to do backpropagation.",
                    "label": 0
                },
                {
                    "sent": "In this model as well backpropagation through time.",
                    "label": 0
                },
                {
                    "sent": "So we've tried on those two experiments on 2.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two datasets, one is the motion capture.",
                    "label": 0
                },
                {
                    "sent": "Data is basically sequences of 3D joint angles and there were various working styles like people were working normally or people were simulating working as if they were drunk or graceful chicken walk and such.",
                    "label": 0
                },
                {
                    "sent": "There were 30 training and a test sequences each of length 50 and each time step was represented by 58 dimensional.",
                    "label": 1
                },
                {
                    "sent": "Real valued numbers.",
                    "label": 0
                },
                {
                    "sent": "OK, so the dimensionality of the input space was 58, so it wasn't a very big problem.",
                    "label": 0
                },
                {
                    "sent": "And if we look at the performance.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So only Y axis, I'm showing the task squared error and on the X axis I'm showing the length of prediction horizon.",
                    "label": 0
                },
                {
                    "sent": "So you can see that if we compare it to simple hidden Markov models then we can do much better if you actually.",
                    "label": 0
                },
                {
                    "sent": "If you also look at the predictions of a simple linear dynamical system, it's making the curve is somewhere in between those two lines.",
                    "label": 0
                },
                {
                    "sent": "Um, now we also compare it to simple linear prediction problems and simple linear prediction problems would just look at the two previous time step and make predictions into the future at XT plus 10 or X D + 16.",
                    "label": 1
                },
                {
                    "sent": "So we were training a multitude of simple linear prediction models and what happens in this graph is quite interesting.",
                    "label": 0
                },
                {
                    "sent": "Is basically saying that we're losing in terms of predicting the next time step.",
                    "label": 0
                },
                {
                    "sent": "And this is probably due to the fact that in the motion capture data nearby time steps have been very linear.",
                    "label": 1
                },
                {
                    "sent": "Type of behavior can predict really well, but if you look at predicting it time steps from now or predicting it X D + 10, then we are doing much better.",
                    "label": 0
                },
                {
                    "sent": "Then simple linear models that conditioning on the previous five steps on the previous two steps and this this yellow line sort of green line is the average prediction.",
                    "label": 0
                },
                {
                    "sent": "Basically it means that instead of looking, you just always predict the mean.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can see that we can never predict after 25 steps is basically random.",
                    "label": 0
                },
                {
                    "sent": "You're not predicting anything.",
                    "label": 0
                },
                {
                    "sent": "So this was a sort of like a smallish data set.",
                    "label": 0
                },
                {
                    "sent": "But we were a little disappointed to see that you know, linear models do just as well as our model for the motion capture data.",
                    "label": 0
                },
                {
                    "sent": "So it just suggests that this is not a very nonlinear model, so we've tried.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Modeling video data so these way video sequences of nine human subjects they were doing various actions such as waving one hand or two hands or jumping or bending or sitting down.",
                    "label": 1
                },
                {
                    "sent": "There were 36 training and test and test sequences and again each time step was represented by 464 dimensional real valued numbers.",
                    "label": 0
                },
                {
                    "sent": "So it was I think 29 by 16 also quite small.",
                    "label": 0
                },
                {
                    "sent": "Images of video now in it.",
                    "label": 0
                },
                {
                    "sent": "In that case, you'd expect the relationship between pixels.",
                    "label": 0
                },
                {
                    "sent": "Annual underlying dynamics to be very nonlinear, right?",
                    "label": 0
                },
                {
                    "sent": "So here the results again.",
                    "label": 0
                },
                {
                    "sent": "We can show that we can do much better than him.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of models and then in that that data set, we actually did considerably better.",
                    "label": 0
                },
                {
                    "sent": "Too simple.",
                    "label": 0
                },
                {
                    "sent": "Simple linear regression.",
                    "label": 0
                },
                {
                    "sent": "Models, and in particular we do consider it better into predicting further into the future.",
                    "label": 0
                },
                {
                    "sent": "So this gives us the advantage of if you really care about recovering the dynamics and making good predictions far into the future.",
                    "label": 0
                },
                {
                    "sent": "This approach seems to be working fairly well.",
                    "label": 0
                },
                {
                    "sent": "One interesting aspect about that model that I wanted to point out is that.",
                    "label": 0
                },
                {
                    "sent": "Conditioning on the five previous steps was actually working worse than conditioning on the two previous steps, and this seems like a very weird behavior.",
                    "label": 0
                },
                {
                    "sent": "But what's happening here is that if you conditional five previous steps, then you effectively have a very high dimensional input, and you're trying to make a linear prediction to the very high dimensional output, and here for these models, regularization becomes an important thing.",
                    "label": 0
                },
                {
                    "sent": "You have to be able to regularize, and it's much harder to regularize for.",
                    "label": 0
                },
                {
                    "sent": "Therefore, building simple linear regression.",
                    "label": 0
                },
                {
                    "sent": "And in input spaces is probably not a very good approach.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think I'm done, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Stack.",
                    "label": 0
                },
                {
                    "sent": "Different layers.",
                    "label": 0
                },
                {
                    "sent": "So in this case, it's not really sticking North.",
                    "label": 0
                },
                {
                    "sent": "Encoders like the intuition is very similar because if you take this model, Anja twisted upside down, you will have the same basic idea of pretraining stacks, stacked autoencoders into multiple levels.",
                    "label": 0
                },
                {
                    "sent": "But in this case these stackings we're done depending on the time series.",
                    "label": 0
                },
                {
                    "sent": "So if there were 50 dimensional sequences then you would have I guess 49.",
                    "label": 0
                },
                {
                    "sent": "Stacks so you sticking it in time instead of stacking it into layers.",
                    "label": 0
                },
                {
                    "sent": "You can also.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I haven't tried that.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Because you could take.",
                    "label": 0
                },
                {
                    "sent": "Best technique?",
                    "label": 0
                },
                {
                    "sent": "Using function.",
                    "label": 0
                },
                {
                    "sent": "Something the state and then it maximizing.",
                    "label": 0
                },
                {
                    "sent": "This model.",
                    "label": 0
                },
                {
                    "sent": "Method.",
                    "label": 0
                },
                {
                    "sent": "Let's be protested models for real data.",
                    "label": 0
                },
                {
                    "sent": "Or and doesn't work that well.",
                    "label": 0
                },
                {
                    "sent": "But I can pay for this year.",
                    "label": 0
                },
                {
                    "sent": "Not, but it's generally true.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "And beyond that.",
                    "label": 0
                },
                {
                    "sent": "Basic methods for doing a parameter estimation formalities distress calls me the whole field of volatility.",
                    "label": 0
                },
                {
                    "sent": "Algorithms require that you have a substantial amount of prior knowledge, but the dynamic system because you you specify that the distribution of the transformation distribution can come up to a few parameters.",
                    "label": 0
                },
                {
                    "sent": "Men in between those parameters.",
                    "label": 0
                },
                {
                    "sent": "I guess what I'm finishing about this is that.",
                    "label": 0
                },
                {
                    "sent": "You don't.",
                    "label": 0
                },
                {
                    "sent": "Possible chance we can just apply supervised learning algorithms and develop a good non linear predictor?",
                    "label": 0
                },
                {
                    "sent": "The sampling process.",
                    "label": 0
                },
                {
                    "sent": "The little bit less efficient computationally then.",
                    "label": 0
                },
                {
                    "sent": "But it's definitely the case that we need to do more comparison to the existing work.",
                    "label": 0
                },
                {
                    "sent": "That's absolutely true.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "To say something.",
                    "label": 0
                },
                {
                    "sent": "This looks to me like surprise.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "MEMM this is maximum entropy mark my cough.",
                    "label": 0
                },
                {
                    "sent": "So we have like some kind of like.",
                    "label": 0
                },
                {
                    "sent": "None unseen label bias.",
                    "label": 0
                },
                {
                    "sent": "You're just going in One Direction.",
                    "label": 0
                },
                {
                    "sent": "Right, so when you are training, you can go in One Direction, but you the parameters are shared through the entire thing, right?",
                    "label": 0
                },
                {
                    "sent": "And you can sort of do multiple passes through training these things and then finally you also using backpropagation to refine the entire parameters in your model so you know.",
                    "label": 0
                },
                {
                    "sent": "There was, yeah.",
                    "label": 0
                },
                {
                    "sent": "Presenting the speaker.",
                    "label": 0
                },
                {
                    "sent": "Delta.",
                    "label": 0
                },
                {
                    "sent": "Because you use the new X ND, you calculate your next view.",
                    "label": 0
                },
                {
                    "sent": "Next generation.",
                    "label": 0
                },
                {
                    "sent": "And in fact, you end up with first.",
                    "label": 0
                },
                {
                    "sent": "Position.",
                    "label": 0
                },
                {
                    "sent": "Build your model, calculate your future position.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure I have to think about it because the way I think about these things, these representations, I think of them as some distributed representations of of that somehow summarized the entire past history and allow me to make good predictions into the future.",
                    "label": 0
                },
                {
                    "sent": "Maybe there are alternative use of using of viewing these what these are but.",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 0
                },
                {
                    "sent": "And the reason that for the motion test.",
                    "label": 0
                },
                {
                    "sent": "Get about the same result is because you probably there's probably no real acceleration and the linear system as he is using the speed only when you get the same result.",
                    "label": 0
                },
                {
                    "sent": "Well, for linear for linear predictions I did condition on two previous steps or five previous steps.",
                    "label": 0
                },
                {
                    "sent": "So in that respect, linear model could potentially see the acceleration right?",
                    "label": 0
                },
                {
                    "sent": "Because you need three steps to figure out roughly what the acceleration is.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "It depends on what right?",
                    "label": 0
                },
                {
                    "sent": "I. Dragon.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "This distinction.",
                    "label": 0
                },
                {
                    "sent": "The same thing, but will you state can sense that it is a sufficient statistic for subsequent sequence.",
                    "label": 0
                },
                {
                    "sent": "Is this like it application between the idea that you know when people talk about the state in general information systems?",
                    "label": 0
                },
                {
                    "sent": "They kind of this idea?",
                    "label": 0
                },
                {
                    "sent": "Actual statements out there, so why is like the environmental real state that's right?",
                    "label": 0
                },
                {
                    "sent": "That's exactly right, yeah.",
                    "label": 0
                },
                {
                    "sent": "Reason why we need to make it.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in particular in that particular representation we never instantiate.",
                    "label": 0
                },
                {
                    "sent": "For example wise we never specified what the dynamics is of the underlying dynamical system.",
                    "label": 0
                },
                {
                    "sent": "Whatever state, what we call state, we get as a byproduct of the learning problem.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I presume so.",
                    "label": 0
                },
                {
                    "sent": "If you want to look at it this way, yes.",
                    "label": 0
                },
                {
                    "sent": "Ask question.",
                    "label": 0
                },
                {
                    "sent": "So one question is.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so we've thought about doing robustness analysis to figure out, you know, if your prediction problems are not correct, how the errors would get propagated forward, and we haven't done that yet.",
                    "label": 0
                },
                {
                    "sent": "But that's something to do on our list.",
                    "label": 0
                },
                {
                    "sent": "So there's a paper coming.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If you don't want to have achievements.",
                    "label": 0
                },
                {
                    "sent": "We need this.",
                    "label": 0
                },
                {
                    "sent": "Couldn't eigenvalues which produced the information that you're getting these around there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but it's a little bit tricky to do it in only in case.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}