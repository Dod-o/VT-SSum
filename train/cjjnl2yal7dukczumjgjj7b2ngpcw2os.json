{
    "id": "cjjnl2yal7dukczumjgjj7b2ngpcw2os",
    "title": "A Polynomial-time Nash Equilibrium Algorithm for Repeated Stochastic Games",
    "info": {
        "author": [
            "Enrique Munoz de Cote, Politecnico di Milano"
        ],
        "published": "July 30, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Mathematics->Game Theory",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Computer Science->Machine Learning->Markov Processes"
        ]
    },
    "url": "http://videolectures.net/uai08_munoz_aptnea/",
    "segmentation": [
        [
            "Hi so this is joint work with Michael Eatmon.",
            "While I was at his lab at RL 3 at doctors.",
            "This is a polynomial time Nash equilibrium algorithm for repeated stochastic games."
        ],
        [
            "So, concretely, what what we're dealing with.",
            "The problem is that we've given a repeated stochastic game.",
            "We want an algorithm that can return.",
            "Pair of strategy.",
            "That is, in Nash equilibrium in the repeated stochastic game.",
            "We know that the repeated games there are many, even infinitely number of equilibrium.",
            "So we want to choose one which specifically is the what we call inegalitarian equilibrium.",
            "And we want to do that in in Poly."
        ],
        [
            "Final time.",
            "So the framework here is a stochastic games that's super set of MDP's and matrix games, which means that there's multiple States and multiple agents."
        ],
        [
            "More formally, we want a stochastic game is defined as a tuple of number of players and number of set of states.",
            "A set of actions.",
            "Available for each agent in each state.",
            "Add transition function and reward function for each player.",
            "For any given state.",
            "And any given action joint action actually so.",
            "When we look at these, for example, it's a bimatrix game.",
            "In each state we have, we're going to play a metric game.",
            "And the transition function given a joint strategy in a given state, we have a transition function for for the next given state, the next given matrix game would be."
        ],
        [
            "Instantiating this just to give up a grasp of what's going on.",
            "We have two agents.",
            "For example, we have this set of factions going up right, left, or right and or to stay put.",
            "Instantiation players when they when they collide when they try to go to the same cell they can't go go to the same cell at the same time, so that will be resolved by a coin flip on weighted coin flip.",
            "There are for example in this case there are semi walls which with some probability they can go through the next state.",
            "So for example, if each player want to go up, they can go up with some probability.",
            "If they both want to try get to the same sale they will.",
            "There will be a cost to it, which is minus 5.",
            "There is a step cost of minus one.",
            "Both agents can get to the goal that if they both get to the goal at the same time and it has a value plus 100, there's a discount factor.",
            "And as I said, both agents can get to the."
        ],
        [
            "Goal at the same time.",
            "Up so this is stochastic representation of the of famous game Chicken and we can see the strategic.",
            "Conflict there because it's.",
            "Both of the edges would like to get to the center cell 'cause they can guarantee to get to the goal in three steps.",
            "But if they do that at the same time, they they will collide and only one will get to to that sensor sale.",
            "We have many strategies, many clearing strategies.",
            "For example, we have one Nash strategy profile, one getting the center and one going through the wall.",
            "And that would have, for example, that perova.",
            "Average pay off.",
            "The other way around, for example, we can see that these Nash equilibrium, for example, are very is very imbalanced and kind of inefficient in some way.",
            "There's other, for example, mixed strategy mixing between going.",
            "To the center sale going up.",
            "There is a correlated equilibrium that has a continuum of solutions.",
            "For example, we have a pair of strategy that is a minimax strategy.",
            "For example, both not trying to get to the center still and guaranteeing themselves some payoff.",
            "Or, for example, this is not equivalent for friends strategy to wear one agent things, the other one is a is going to be active for.",
            "The benefit of the other one in that case, both of them would hit to the center cell.",
            "And has low expected average work, for example.",
            "In the general case, we know that the solving Nash equilibria it's computationally difficult.",
            "But we know that in repeated games these might change and we will try to Prof."
        ],
        [
            "From that we base the solution on the.",
            "So this is a repeated game setting and we we profit from a representation of the folk theorems, which can be computationally easy to abstract.",
            "These folk theorems there's a concept or conceptual drawback, although there are infinitely number of solutions in these repeated game setting.",
            "We call this the.",
            "The convex Hull of solutions.",
            "This is the average pay of of the whole.",
            "Set of solutions that we can see in on a hypothetical stochastic repeated stochastic game.",
            "They both accesses the.",
            "Safe strategy the minimax strategy.",
            "What the each agent could guarantee themselves playing that the safe strategy.",
            "So in the repeated game, and nobody would want to get below that average expected reward.",
            "So everything above and to the right of that of that limit is what bounds what we call the advantage space.",
            "We define the gala Terrian life, which is the line where the payoff of both agents is are equally above this minimax pair of strategies, which we call the.",
            "At the point B, for example.",
            "And the gala Terrian point would be the one that maximizes the minimum advantage of the players rewards.",
            "For example, in that case it would be that that point because it would be the exact intersection of the legality in line with the frontier.",
            "The Parade of Frontier in that case.",
            "In general, we know it would be a good thing in repeated games.",
            "Two to always find a solution in the frontier of this advantage space, but we know there's there's also an infinite set of parido efficient solutions, meaning parade efficient when we can to optimize above that without hurting the other player.",
            "So we can one player is a can't optimize above that without hurting the other one.",
            "So we define this what we call a gala terrian point, and that's a.",
            "It's actually the name that we borrowed from.",
            "From Miami Green wealth."
        ],
        [
            "So let's say for example, the short story of how these goes.",
            "It's let's suppose we have.",
            "McAuliffe some game.",
            "Of course, the easy way to to solve this is if we buy exhaustive search, get all these computer all these convex.",
            "Hope we can easily know where they got it.",
            "I'm point is.",
            "Of course he has to be in the advantage space.",
            "'cause that's the only way that we can guarantee the that those are the Nash equilibrium, for example.",
            "But we don't want to compute all these set of course.",
            "So what we do is we compute the attack and defense strategies.",
            "The Defense strategy is what will tell us the.",
            "They did this line of.",
            "Disagreement points.",
            "And the attack strategies would be our threat against deviations.",
            "For that.",
            "That's how we construct the national solutions, and I'll talk a little bit about that.",
            "To do that, any kind of game, we can convert it to 0 sum game.",
            "By lady one being the attacker and the other one being the defender, and we know that Minimax game.",
            "We can solve it using linear programming, so that's polynomial.",
            "And mathematically, where we look for it's for a point which is the point P. Which is.",
            "For example, we tried to do that.",
            "That kind of search we go through that line until we hit what we call the gala Terrian point, which in this case would for example would be that.",
            "Um?",
            "We can see that L shape like the it's a track where all of the points.",
            "On that L shaped track would have the same value value in that case.",
            "For example, I gave a value of two."
        ],
        [
            "So we know the fault theorems can be interpreted computationally an that was shown by Bedlinen stone paper for Matrix Games.",
            "Then how how they represented that it's using a matrix game form first stochastic games we don't want to represent it as a matrix in matrix form, 'cause we would have infinitely number of solutions depending on the strategies.",
            "There are infinitely many strategies, so we don't want to represent it as a matrix game, and we represented a stochastic game explicitly using MVP's.",
            "So for example, let's define a weighted combination of value P1 and P2 would be the.",
            "Join to pay off for the players.",
            "Expected average payoff for the players, and we we we have a weighted value of these two utilities.",
            "Of course, that's on the advantage case.",
            "And using M DPS modeling and NDP we can find a pair of strategies that achieve this weighted combination value."
        ],
        [
            "So we use we model MVP for both of the players.",
            "We call that a major player which would be trying to find the joint strategy profile that maximizes awaited combination of of players payoff.",
            "So for example, we can have the friend solution if we define a function MDP, one meaning we want to optimize for first player.",
            "We want to give the whole weight for the first player.",
            "That would be a friend solution for the first player and that would find me the best strategy.",
            "Joint strategy profile for the first player.",
            "In that case it's R0 for example.",
            "And the joint strategy that would achieve that.",
            "We can define that for the.",
            "For the other player, giving away 0 to the first one.",
            "And we have a pair of strategies, so we solve an MDP that could give you give us the joint strategy that would maximize for the other player, which would give us a that point LO for example.",
            "Well, we can have awaited solution solving another MVP with some.",
            "Other linear combination of that weight.",
            "And that would give us another joint strategy."
        ],
        [
            "So the algorithm what we do is first we compute the attack and defense strategies by solving this with linear programming we get two points.",
            "The what we call the friends strategies which would give us R&L for example.",
            "And we're trying to find it.",
            "The gala terrian point.",
            "Given these two points are a nail.",
            "If our for example in this case it's left from the Gala Terrian line are would be maximizing for Player 1.",
            "With if we find that point that it's on the left side, that's even more advantages to the second player, so we have already found the legality and point.",
            "Example here.",
            "If we maximize for the for the first player, and we know that that point which we called P, it's it's on the left side of the Gala Terrian line.",
            "Then that's the solution.",
            "Otherwise, if it's the other way around and deal point, for example, it's below the egalitarian line.",
            "We've already found the glittering point.",
            "And the not so easy cases when both of the none of the of the points are.",
            "If R is not on the left side or L is not on the right side.",
            "Because we what we do that there is, we try to do like a binary search in that case.",
            "And that's the essence of."
        ],
        [
            "The algorithm.",
            "So that's super routine.",
            "It's given two points to pair of strategies.",
            "An their utility points.",
            "And the number of iterations.",
            "Then we want to do the number of iterations T it's a bound, so we can solve it below that bound and that the number T would be related to the error that we want to.",
            "That we are willing to make here for the for the algorithm, and we want to find the intersection between the whole convex Hull set and the Gala terrian line 'cause we know we there is no there.",
            "There was nothing in between.",
            "So this part is close to binary search in some way.",
            "Input today to the subroutine would be the point L and R&D on this number of iterations that.",
            "And we we return is the gala terrian point with some accuracy epsilon.",
            "On each iteration where we do it, we solve an MDP.",
            "With some special weight.",
            "And we try to find a solution.",
            "The intersection where we are in different from from these two points.",
            "So we're trying to find a way to where this point LR.",
            "It's a where.",
            "Different and if we try to solve the next iteration for that value.",
            "We know that an MDP with the with maximize would find the joint strategy that would maximize, so we will find another point that would be an optimization for that.",
            "From these two."
        ],
        [
            "Alan R. Now.",
            "Summer.",
            "Experiments for example.",
            "This is the prisoners dilemma converting in stochastic inversion.",
            "For example, if these three.",
            "This is our three algorithms.",
            "One is the security which would give me the minimax strategies.",
            "For example one, another one is a friend.",
            "Algorithm which.",
            "Things one of the agencies, the other one, is trying to optimize for me and the other one and the other way around.",
            "Correlated equilibrium for example and ours.",
            "The first three would find the mutual defection in the Impressionist dilemma.",
            "We know that correlated also finds the.",
            "These, which would be defect affecting the Matrix game version.",
            "And.",
            "In that case would be trying to get the sensor sale and try to get to the first goal that three of the goals in this case are weighted the same, so it's 100 for the three of them.",
            "But because of the cost step and the discount factor, it's all the agents want to go to the center goal instead of going to the other ones.",
            "Um?",
            "Instead, what this one finds.",
            "It's kind of trivial.",
            "They both go to the to their goals without trying to get the center ago agree stick goal."
        ],
        [
            "OK, that's fair.",
            "Kind of not that interesting, but for example this one.",
            "The security strategy would be the attacker, they both because.",
            "They both get to the center cell.",
            "The Friends strategy because the other one is thinking that the other one is trying to maximize for me.",
            "They both tried to get to the center cell.",
            "Instead, the correlated equilibrium.",
            "We found that it they find some interesting strategy, for example, Agent A goes to Agent B goal.",
            "Agent B goes all the way up and in this situation Agent A.",
            "Will only go to eat go now because it knows that it's it's not being blocked here.",
            "So he tries to get to its goal, and that's perfectly.",
            "Agent P is not needs to go up in order to be an equilibrium 'cause the agent A knows that it won't move from the other goal unless you know it knows it's safe for it.",
            "And they both get to to its goal, so it's kind of like a really interesting strategy in this that correlated.",
            "Find sing that situation instead of this faulty galeton equilibrium where it finds it.",
            "It goes to the center cell, B goes.",
            "We disappeared.",
            "Be.",
            "Disappeared.",
            "Well, what do you guys do?",
            "It's a goes to the center sale be approaches its goal and they both get get to its goals in five steps.",
            "Anne.",
            "There's some saving their first.",
            "First, it's a Switch 1 first, first day goes to the center cell and B goes directly, which would lead to some advantage for Agent P and then the Switch strategies agent goes to the center and then the other one goes."
        ],
        [
            "It's cold.",
            "And even more interesting, for example, it's a symmetric game that we designed where.",
            "Anne.",
            "The first strategy, for example, the security strategy.",
            "They would just block themselves.",
            "The French strategy OK, the security strategy they would just stay put because nobody would move it then.",
            "That's that's the best they can guarantee themselves, friends, strategies.",
            "They just keep on bumping into each other correlated.",
            "They find some interesting strategy where they compromise and they get to their goals.",
            "And the.",
            "App.",
            "And the focal term they find the same exact strategy, but they switch back and forth between these pair of strategies to make it symmetric and they both can get some weighted combination of that.",
            "And of course correlated.",
            "For example, they don't use, they don't need to use threads in order to construct this equilibrium.",
            "Faulty galectin they mix from one strategy to the other and they construct the threads in order to do that.",
            "And."
        ],
        [
            "That's basically it.",
            "Couple questions.",
            "Well, I I've been thinking about that like making it in some setting like a like colors paper for example or something like that.",
            "I. I don't think first of all, guaranteeing anything in that in that particular setting.",
            "It would be quite hard, and that's basically what it stopped me from doing that I didn't talk anything about complexity here, but it is polynomial and that was kind of a cheap thing that we could guarantee, and in that setting it would be just very impractical to try to do some theory there."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi so this is joint work with Michael Eatmon.",
                    "label": 0
                },
                {
                    "sent": "While I was at his lab at RL 3 at doctors.",
                    "label": 0
                },
                {
                    "sent": "This is a polynomial time Nash equilibrium algorithm for repeated stochastic games.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, concretely, what what we're dealing with.",
                    "label": 0
                },
                {
                    "sent": "The problem is that we've given a repeated stochastic game.",
                    "label": 1
                },
                {
                    "sent": "We want an algorithm that can return.",
                    "label": 0
                },
                {
                    "sent": "Pair of strategy.",
                    "label": 1
                },
                {
                    "sent": "That is, in Nash equilibrium in the repeated stochastic game.",
                    "label": 0
                },
                {
                    "sent": "We know that the repeated games there are many, even infinitely number of equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So we want to choose one which specifically is the what we call inegalitarian equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And we want to do that in in Poly.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Final time.",
                    "label": 0
                },
                {
                    "sent": "So the framework here is a stochastic games that's super set of MDP's and matrix games, which means that there's multiple States and multiple agents.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More formally, we want a stochastic game is defined as a tuple of number of players and number of set of states.",
                    "label": 1
                },
                {
                    "sent": "A set of actions.",
                    "label": 0
                },
                {
                    "sent": "Available for each agent in each state.",
                    "label": 1
                },
                {
                    "sent": "Add transition function and reward function for each player.",
                    "label": 0
                },
                {
                    "sent": "For any given state.",
                    "label": 0
                },
                {
                    "sent": "And any given action joint action actually so.",
                    "label": 0
                },
                {
                    "sent": "When we look at these, for example, it's a bimatrix game.",
                    "label": 0
                },
                {
                    "sent": "In each state we have, we're going to play a metric game.",
                    "label": 0
                },
                {
                    "sent": "And the transition function given a joint strategy in a given state, we have a transition function for for the next given state, the next given matrix game would be.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instantiating this just to give up a grasp of what's going on.",
                    "label": 0
                },
                {
                    "sent": "We have two agents.",
                    "label": 0
                },
                {
                    "sent": "For example, we have this set of factions going up right, left, or right and or to stay put.",
                    "label": 0
                },
                {
                    "sent": "Instantiation players when they when they collide when they try to go to the same cell they can't go go to the same cell at the same time, so that will be resolved by a coin flip on weighted coin flip.",
                    "label": 1
                },
                {
                    "sent": "There are for example in this case there are semi walls which with some probability they can go through the next state.",
                    "label": 0
                },
                {
                    "sent": "So for example, if each player want to go up, they can go up with some probability.",
                    "label": 0
                },
                {
                    "sent": "If they both want to try get to the same sale they will.",
                    "label": 0
                },
                {
                    "sent": "There will be a cost to it, which is minus 5.",
                    "label": 0
                },
                {
                    "sent": "There is a step cost of minus one.",
                    "label": 1
                },
                {
                    "sent": "Both agents can get to the goal that if they both get to the goal at the same time and it has a value plus 100, there's a discount factor.",
                    "label": 1
                },
                {
                    "sent": "And as I said, both agents can get to the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Goal at the same time.",
                    "label": 0
                },
                {
                    "sent": "Up so this is stochastic representation of the of famous game Chicken and we can see the strategic.",
                    "label": 0
                },
                {
                    "sent": "Conflict there because it's.",
                    "label": 0
                },
                {
                    "sent": "Both of the edges would like to get to the center cell 'cause they can guarantee to get to the goal in three steps.",
                    "label": 0
                },
                {
                    "sent": "But if they do that at the same time, they they will collide and only one will get to to that sensor sale.",
                    "label": 0
                },
                {
                    "sent": "We have many strategies, many clearing strategies.",
                    "label": 0
                },
                {
                    "sent": "For example, we have one Nash strategy profile, one getting the center and one going through the wall.",
                    "label": 0
                },
                {
                    "sent": "And that would have, for example, that perova.",
                    "label": 0
                },
                {
                    "sent": "Average pay off.",
                    "label": 0
                },
                {
                    "sent": "The other way around, for example, we can see that these Nash equilibrium, for example, are very is very imbalanced and kind of inefficient in some way.",
                    "label": 0
                },
                {
                    "sent": "There's other, for example, mixed strategy mixing between going.",
                    "label": 0
                },
                {
                    "sent": "To the center sale going up.",
                    "label": 1
                },
                {
                    "sent": "There is a correlated equilibrium that has a continuum of solutions.",
                    "label": 0
                },
                {
                    "sent": "For example, we have a pair of strategy that is a minimax strategy.",
                    "label": 0
                },
                {
                    "sent": "For example, both not trying to get to the center still and guaranteeing themselves some payoff.",
                    "label": 0
                },
                {
                    "sent": "Or, for example, this is not equivalent for friends strategy to wear one agent things, the other one is a is going to be active for.",
                    "label": 0
                },
                {
                    "sent": "The benefit of the other one in that case, both of them would hit to the center cell.",
                    "label": 0
                },
                {
                    "sent": "And has low expected average work, for example.",
                    "label": 0
                },
                {
                    "sent": "In the general case, we know that the solving Nash equilibria it's computationally difficult.",
                    "label": 1
                },
                {
                    "sent": "But we know that in repeated games these might change and we will try to Prof.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From that we base the solution on the.",
                    "label": 0
                },
                {
                    "sent": "So this is a repeated game setting and we we profit from a representation of the folk theorems, which can be computationally easy to abstract.",
                    "label": 0
                },
                {
                    "sent": "These folk theorems there's a concept or conceptual drawback, although there are infinitely number of solutions in these repeated game setting.",
                    "label": 0
                },
                {
                    "sent": "We call this the.",
                    "label": 0
                },
                {
                    "sent": "The convex Hull of solutions.",
                    "label": 1
                },
                {
                    "sent": "This is the average pay of of the whole.",
                    "label": 0
                },
                {
                    "sent": "Set of solutions that we can see in on a hypothetical stochastic repeated stochastic game.",
                    "label": 0
                },
                {
                    "sent": "They both accesses the.",
                    "label": 0
                },
                {
                    "sent": "Safe strategy the minimax strategy.",
                    "label": 0
                },
                {
                    "sent": "What the each agent could guarantee themselves playing that the safe strategy.",
                    "label": 0
                },
                {
                    "sent": "So in the repeated game, and nobody would want to get below that average expected reward.",
                    "label": 0
                },
                {
                    "sent": "So everything above and to the right of that of that limit is what bounds what we call the advantage space.",
                    "label": 0
                },
                {
                    "sent": "We define the gala Terrian life, which is the line where the payoff of both agents is are equally above this minimax pair of strategies, which we call the.",
                    "label": 0
                },
                {
                    "sent": "At the point B, for example.",
                    "label": 0
                },
                {
                    "sent": "And the gala Terrian point would be the one that maximizes the minimum advantage of the players rewards.",
                    "label": 1
                },
                {
                    "sent": "For example, in that case it would be that that point because it would be the exact intersection of the legality in line with the frontier.",
                    "label": 0
                },
                {
                    "sent": "The Parade of Frontier in that case.",
                    "label": 0
                },
                {
                    "sent": "In general, we know it would be a good thing in repeated games.",
                    "label": 0
                },
                {
                    "sent": "Two to always find a solution in the frontier of this advantage space, but we know there's there's also an infinite set of parido efficient solutions, meaning parade efficient when we can to optimize above that without hurting the other player.",
                    "label": 0
                },
                {
                    "sent": "So we can one player is a can't optimize above that without hurting the other one.",
                    "label": 0
                },
                {
                    "sent": "So we define this what we call a gala terrian point, and that's a.",
                    "label": 0
                },
                {
                    "sent": "It's actually the name that we borrowed from.",
                    "label": 0
                },
                {
                    "sent": "From Miami Green wealth.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's say for example, the short story of how these goes.",
                    "label": 1
                },
                {
                    "sent": "It's let's suppose we have.",
                    "label": 0
                },
                {
                    "sent": "McAuliffe some game.",
                    "label": 0
                },
                {
                    "sent": "Of course, the easy way to to solve this is if we buy exhaustive search, get all these computer all these convex.",
                    "label": 0
                },
                {
                    "sent": "Hope we can easily know where they got it.",
                    "label": 0
                },
                {
                    "sent": "I'm point is.",
                    "label": 0
                },
                {
                    "sent": "Of course he has to be in the advantage space.",
                    "label": 0
                },
                {
                    "sent": "'cause that's the only way that we can guarantee the that those are the Nash equilibrium, for example.",
                    "label": 0
                },
                {
                    "sent": "But we don't want to compute all these set of course.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we compute the attack and defense strategies.",
                    "label": 1
                },
                {
                    "sent": "The Defense strategy is what will tell us the.",
                    "label": 0
                },
                {
                    "sent": "They did this line of.",
                    "label": 0
                },
                {
                    "sent": "Disagreement points.",
                    "label": 0
                },
                {
                    "sent": "And the attack strategies would be our threat against deviations.",
                    "label": 0
                },
                {
                    "sent": "For that.",
                    "label": 0
                },
                {
                    "sent": "That's how we construct the national solutions, and I'll talk a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "To do that, any kind of game, we can convert it to 0 sum game.",
                    "label": 0
                },
                {
                    "sent": "By lady one being the attacker and the other one being the defender, and we know that Minimax game.",
                    "label": 0
                },
                {
                    "sent": "We can solve it using linear programming, so that's polynomial.",
                    "label": 0
                },
                {
                    "sent": "And mathematically, where we look for it's for a point which is the point P. Which is.",
                    "label": 1
                },
                {
                    "sent": "For example, we tried to do that.",
                    "label": 0
                },
                {
                    "sent": "That kind of search we go through that line until we hit what we call the gala Terrian point, which in this case would for example would be that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We can see that L shape like the it's a track where all of the points.",
                    "label": 0
                },
                {
                    "sent": "On that L shaped track would have the same value value in that case.",
                    "label": 0
                },
                {
                    "sent": "For example, I gave a value of two.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we know the fault theorems can be interpreted computationally an that was shown by Bedlinen stone paper for Matrix Games.",
                    "label": 1
                },
                {
                    "sent": "Then how how they represented that it's using a matrix game form first stochastic games we don't want to represent it as a matrix in matrix form, 'cause we would have infinitely number of solutions depending on the strategies.",
                    "label": 0
                },
                {
                    "sent": "There are infinitely many strategies, so we don't want to represent it as a matrix game, and we represented a stochastic game explicitly using MVP's.",
                    "label": 1
                },
                {
                    "sent": "So for example, let's define a weighted combination of value P1 and P2 would be the.",
                    "label": 0
                },
                {
                    "sent": "Join to pay off for the players.",
                    "label": 0
                },
                {
                    "sent": "Expected average payoff for the players, and we we we have a weighted value of these two utilities.",
                    "label": 0
                },
                {
                    "sent": "Of course, that's on the advantage case.",
                    "label": 0
                },
                {
                    "sent": "And using M DPS modeling and NDP we can find a pair of strategies that achieve this weighted combination value.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we use we model MVP for both of the players.",
                    "label": 1
                },
                {
                    "sent": "We call that a major player which would be trying to find the joint strategy profile that maximizes awaited combination of of players payoff.",
                    "label": 1
                },
                {
                    "sent": "So for example, we can have the friend solution if we define a function MDP, one meaning we want to optimize for first player.",
                    "label": 0
                },
                {
                    "sent": "We want to give the whole weight for the first player.",
                    "label": 0
                },
                {
                    "sent": "That would be a friend solution for the first player and that would find me the best strategy.",
                    "label": 0
                },
                {
                    "sent": "Joint strategy profile for the first player.",
                    "label": 0
                },
                {
                    "sent": "In that case it's R0 for example.",
                    "label": 0
                },
                {
                    "sent": "And the joint strategy that would achieve that.",
                    "label": 0
                },
                {
                    "sent": "We can define that for the.",
                    "label": 0
                },
                {
                    "sent": "For the other player, giving away 0 to the first one.",
                    "label": 0
                },
                {
                    "sent": "And we have a pair of strategies, so we solve an MDP that could give you give us the joint strategy that would maximize for the other player, which would give us a that point LO for example.",
                    "label": 0
                },
                {
                    "sent": "Well, we can have awaited solution solving another MVP with some.",
                    "label": 0
                },
                {
                    "sent": "Other linear combination of that weight.",
                    "label": 0
                },
                {
                    "sent": "And that would give us another joint strategy.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the algorithm what we do is first we compute the attack and defense strategies by solving this with linear programming we get two points.",
                    "label": 0
                },
                {
                    "sent": "The what we call the friends strategies which would give us R&L for example.",
                    "label": 0
                },
                {
                    "sent": "And we're trying to find it.",
                    "label": 0
                },
                {
                    "sent": "The gala terrian point.",
                    "label": 0
                },
                {
                    "sent": "Given these two points are a nail.",
                    "label": 0
                },
                {
                    "sent": "If our for example in this case it's left from the Gala Terrian line are would be maximizing for Player 1.",
                    "label": 0
                },
                {
                    "sent": "With if we find that point that it's on the left side, that's even more advantages to the second player, so we have already found the legality and point.",
                    "label": 0
                },
                {
                    "sent": "Example here.",
                    "label": 0
                },
                {
                    "sent": "If we maximize for the for the first player, and we know that that point which we called P, it's it's on the left side of the Gala Terrian line.",
                    "label": 0
                },
                {
                    "sent": "Then that's the solution.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if it's the other way around and deal point, for example, it's below the egalitarian line.",
                    "label": 0
                },
                {
                    "sent": "We've already found the glittering point.",
                    "label": 0
                },
                {
                    "sent": "And the not so easy cases when both of the none of the of the points are.",
                    "label": 0
                },
                {
                    "sent": "If R is not on the left side or L is not on the right side.",
                    "label": 1
                },
                {
                    "sent": "Because we what we do that there is, we try to do like a binary search in that case.",
                    "label": 0
                },
                {
                    "sent": "And that's the essence of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm.",
                    "label": 0
                },
                {
                    "sent": "So that's super routine.",
                    "label": 0
                },
                {
                    "sent": "It's given two points to pair of strategies.",
                    "label": 0
                },
                {
                    "sent": "An their utility points.",
                    "label": 0
                },
                {
                    "sent": "And the number of iterations.",
                    "label": 0
                },
                {
                    "sent": "Then we want to do the number of iterations T it's a bound, so we can solve it below that bound and that the number T would be related to the error that we want to.",
                    "label": 1
                },
                {
                    "sent": "That we are willing to make here for the for the algorithm, and we want to find the intersection between the whole convex Hull set and the Gala terrian line 'cause we know we there is no there.",
                    "label": 0
                },
                {
                    "sent": "There was nothing in between.",
                    "label": 1
                },
                {
                    "sent": "So this part is close to binary search in some way.",
                    "label": 0
                },
                {
                    "sent": "Input today to the subroutine would be the point L and R&D on this number of iterations that.",
                    "label": 0
                },
                {
                    "sent": "And we we return is the gala terrian point with some accuracy epsilon.",
                    "label": 0
                },
                {
                    "sent": "On each iteration where we do it, we solve an MDP.",
                    "label": 0
                },
                {
                    "sent": "With some special weight.",
                    "label": 0
                },
                {
                    "sent": "And we try to find a solution.",
                    "label": 0
                },
                {
                    "sent": "The intersection where we are in different from from these two points.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to find a way to where this point LR.",
                    "label": 0
                },
                {
                    "sent": "It's a where.",
                    "label": 0
                },
                {
                    "sent": "Different and if we try to solve the next iteration for that value.",
                    "label": 0
                },
                {
                    "sent": "We know that an MDP with the with maximize would find the joint strategy that would maximize, so we will find another point that would be an optimization for that.",
                    "label": 0
                },
                {
                    "sent": "From these two.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alan R. Now.",
                    "label": 0
                },
                {
                    "sent": "Summer.",
                    "label": 0
                },
                {
                    "sent": "Experiments for example.",
                    "label": 0
                },
                {
                    "sent": "This is the prisoners dilemma converting in stochastic inversion.",
                    "label": 0
                },
                {
                    "sent": "For example, if these three.",
                    "label": 0
                },
                {
                    "sent": "This is our three algorithms.",
                    "label": 0
                },
                {
                    "sent": "One is the security which would give me the minimax strategies.",
                    "label": 0
                },
                {
                    "sent": "For example one, another one is a friend.",
                    "label": 0
                },
                {
                    "sent": "Algorithm which.",
                    "label": 0
                },
                {
                    "sent": "Things one of the agencies, the other one, is trying to optimize for me and the other one and the other way around.",
                    "label": 0
                },
                {
                    "sent": "Correlated equilibrium for example and ours.",
                    "label": 0
                },
                {
                    "sent": "The first three would find the mutual defection in the Impressionist dilemma.",
                    "label": 0
                },
                {
                    "sent": "We know that correlated also finds the.",
                    "label": 0
                },
                {
                    "sent": "These, which would be defect affecting the Matrix game version.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In that case would be trying to get the sensor sale and try to get to the first goal that three of the goals in this case are weighted the same, so it's 100 for the three of them.",
                    "label": 0
                },
                {
                    "sent": "But because of the cost step and the discount factor, it's all the agents want to go to the center goal instead of going to the other ones.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Instead, what this one finds.",
                    "label": 0
                },
                {
                    "sent": "It's kind of trivial.",
                    "label": 0
                },
                {
                    "sent": "They both go to the to their goals without trying to get the center ago agree stick goal.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, that's fair.",
                    "label": 0
                },
                {
                    "sent": "Kind of not that interesting, but for example this one.",
                    "label": 0
                },
                {
                    "sent": "The security strategy would be the attacker, they both because.",
                    "label": 0
                },
                {
                    "sent": "They both get to the center cell.",
                    "label": 0
                },
                {
                    "sent": "The Friends strategy because the other one is thinking that the other one is trying to maximize for me.",
                    "label": 0
                },
                {
                    "sent": "They both tried to get to the center cell.",
                    "label": 0
                },
                {
                    "sent": "Instead, the correlated equilibrium.",
                    "label": 0
                },
                {
                    "sent": "We found that it they find some interesting strategy, for example, Agent A goes to Agent B goal.",
                    "label": 0
                },
                {
                    "sent": "Agent B goes all the way up and in this situation Agent A.",
                    "label": 1
                },
                {
                    "sent": "Will only go to eat go now because it knows that it's it's not being blocked here.",
                    "label": 0
                },
                {
                    "sent": "So he tries to get to its goal, and that's perfectly.",
                    "label": 0
                },
                {
                    "sent": "Agent P is not needs to go up in order to be an equilibrium 'cause the agent A knows that it won't move from the other goal unless you know it knows it's safe for it.",
                    "label": 0
                },
                {
                    "sent": "And they both get to to its goal, so it's kind of like a really interesting strategy in this that correlated.",
                    "label": 0
                },
                {
                    "sent": "Find sing that situation instead of this faulty galeton equilibrium where it finds it.",
                    "label": 0
                },
                {
                    "sent": "It goes to the center cell, B goes.",
                    "label": 0
                },
                {
                    "sent": "We disappeared.",
                    "label": 0
                },
                {
                    "sent": "Be.",
                    "label": 0
                },
                {
                    "sent": "Disappeared.",
                    "label": 0
                },
                {
                    "sent": "Well, what do you guys do?",
                    "label": 0
                },
                {
                    "sent": "It's a goes to the center sale be approaches its goal and they both get get to its goals in five steps.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "There's some saving their first.",
                    "label": 0
                },
                {
                    "sent": "First, it's a Switch 1 first, first day goes to the center cell and B goes directly, which would lead to some advantage for Agent P and then the Switch strategies agent goes to the center and then the other one goes.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's cold.",
                    "label": 0
                },
                {
                    "sent": "And even more interesting, for example, it's a symmetric game that we designed where.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The first strategy, for example, the security strategy.",
                    "label": 0
                },
                {
                    "sent": "They would just block themselves.",
                    "label": 0
                },
                {
                    "sent": "The French strategy OK, the security strategy they would just stay put because nobody would move it then.",
                    "label": 0
                },
                {
                    "sent": "That's that's the best they can guarantee themselves, friends, strategies.",
                    "label": 0
                },
                {
                    "sent": "They just keep on bumping into each other correlated.",
                    "label": 0
                },
                {
                    "sent": "They find some interesting strategy where they compromise and they get to their goals.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "And the focal term they find the same exact strategy, but they switch back and forth between these pair of strategies to make it symmetric and they both can get some weighted combination of that.",
                    "label": 0
                },
                {
                    "sent": "And of course correlated.",
                    "label": 0
                },
                {
                    "sent": "For example, they don't use, they don't need to use threads in order to construct this equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Faulty galectin they mix from one strategy to the other and they construct the threads in order to do that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's basically it.",
                    "label": 0
                },
                {
                    "sent": "Couple questions.",
                    "label": 0
                },
                {
                    "sent": "Well, I I've been thinking about that like making it in some setting like a like colors paper for example or something like that.",
                    "label": 0
                },
                {
                    "sent": "I. I don't think first of all, guaranteeing anything in that in that particular setting.",
                    "label": 0
                },
                {
                    "sent": "It would be quite hard, and that's basically what it stopped me from doing that I didn't talk anything about complexity here, but it is polynomial and that was kind of a cheap thing that we could guarantee, and in that setting it would be just very impractical to try to do some theory there.",
                    "label": 0
                }
            ]
        }
    }
}