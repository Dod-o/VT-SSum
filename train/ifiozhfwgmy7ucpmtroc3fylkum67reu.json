{
    "id": "ifiozhfwgmy7ucpmtroc3fylkum67reu",
    "title": "Bootstrapping Skills",
    "info": {
        "author": [
            "Daniel Mankowitz, Technion - Israel Institute of Technology"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Psychology",
            "Top->Social Sciences->Economics",
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering"
        ]
    },
    "url": "http://videolectures.net/rldm2015_mankowitz_bootstrapping_skills/",
    "segmentation": [
        [
            "Hey everybody, I she said my name is Daniel and I'm I'm talking about our work bootstrapping skills that was Co authored by myself, Timothy Man and Shimon or from the Technion and Tim is now just moved onto Google Deep Mind."
        ],
        [
            "So the structure of the talk I'm going to motivate our idea of bootstrapping skills and then talk about a key structure in our work, which is skills and then go into detail about our algorithm.",
            "Learning skills via bootstrapping as well as the theoretical results, which is the main contribution of our work.",
            "I'm going to then discuss and then sort of show how this works practically and sort of confirming some of our theoretical results."
        ],
        [
            "So what is the idea behind a monolithic policy?",
            "So imagine we're given an S shaped MDP and the task areas for an agent to move from the bottom left of the MDP to the goal region.",
            "Now, in the monolithic approach, you're learning a single policy, one policy that solves the entire MDP.",
            "Now this single policy, if you're imagine scaling up to really high dimensional M DPS can become really big and complex and require a huge number of parameters, and the question is is how can we somehow decompose this and break this down so that we're able to learn with less complex policy?"
        ],
        [
            "So just to give you an idea of what this means in a high dimensional domain, imagine we have now are robots, and this robot's task is to leave the room.",
            "So here the skill that the robot needs to learn or its policy.",
            "I'm going to use these words interchangeably is to walk to the door, open the door, or grasp the door, open it and walk through the door.",
            "As a single skill, this is a very complex structure that you need to learn.",
            "So the question is, how can we decompose this an?",
            "Learn this with more simple skills."
        ],
        [
            "Or policies.",
            "So one way of doing this is to decompose the task to partition the state space into a set of partitions and within each partition define a skill.",
            "And this skills task is to accomplish a subgoal, and the idea is that by learning a set of skills in each of these partitions, we can then combine these skills together in order to complete the task in a near optimal fashion, so a good way to think of skill is essentially a special form of an option, with their idea is that you want to be able to reuse these skills throughout different parts of the state space."
        ],
        [
            "So to give you an idea of where this how this can help.",
            "For example, let's consider that same task where the robot needs to leave the room.",
            "Now here we can take that really big skill or policy and we can break it down into more decompose it at least into a similar set of skills.",
            "So here for example, the new skills are to walk, grasp the door and open the door nob.",
            "Now the idea here is that we partition the state space and in each region of that state space learn a skill that is relevant to solving that part of the state space.",
            "So for example, when the robot is in the corridor, he learns a skill to walk to the door.",
            "When he's at the door, he learns a skill to grasp the doorknob, and once he crossed the doorknob, he learns a skill to open the door.",
            "Once the door is open, he can then reuse a skill that is learned in a previous partition in order to walk through the door."
        ],
        [
            "So how does it?",
            "How do we do this?",
            "How do we learn skills?",
            "So the idea here is that in our work in our algorithm we require a partitioning of the state space, so we assume that this partitioning is provided by a domain expert, and we then define a skill in each partition in the state space.",
            "So here there are five partitions of the state space and therefore 5 skills have been arbitrarily defined in the state space.",
            "The goal then is to learn the best local policy for each skill.",
            "So in other words, to learn the best skill.",
            "That when combined together with all the other skills, is able to solve the task.",
            "This work has been inspired by skill chaining a work by Colonel Diaries and Barter.",
            "But here what we do is we provide the first theoretical convergence guarantees for iteratively learning skills in a continuous MDP."
        ],
        [
            "So how does this algorithm work?",
            "How do we learn these skills so we recognize two important things when when learning these skills in order to get convergence and one is that the skills need to work together, a skill in isolation.",
            "For example, Sigma 3.",
            "The third skill doesn't know necessarily whether it needs to move to Sigma four or down to Sigma 2.",
            "The skills need to feed off of 1 another in order to work together and be able to compose themselves such that a task can be solved.",
            "So the way this would work is we start in the first iteration by having a set of arbitrarily defined skills, one per partition.",
            "The idea then is that the skill that contains the goal region, the skill in the goal partition then eventually reaches the goal and then bootstraps reward signal.",
            "Through its partition.",
            "That skill is then able to learn what it's supposed to do in order, in other words, to reach the goal location and in the subsequent iteration the adjacent skill then is able to take back this meaningful reward that has been bootstrapped by the original skill and is then able to learn it skills such that it can then move towards the goal.",
            "This is then repeated over a number of iterations until every partition or every skill has been learned in a partition, and these skills are then composed together in order to be able to.",
            "Reached the goal near optimally."
        ],
        [
            "So when another sort of inherent thing that is happening here is something that we've turned model iteration.",
            "So if you take a set of skills and you take an MDP, you get a SMD model and the idea is that when we start off in this algorithm, we have a misspecified model.",
            "We have a set of skills that when placed into this MDP cannot solve the task with their current state and the idea is then to do this model improvement procedure to improve the model.",
            "IE continually improve learning the skill set and therefore improve the SMD model such that overtime you're able to get this optimal model or this optimal solution that allows you to solve the task."
        ],
        [
            "So as I mentioned, we one of our main results was to have convergence guarantees for learning for getting near optimal convergence for iteratively learning skills in a continuous state MVP.",
            "But another result that has come out of this is the ability to look at the quality of the policy that is returned by this LSP algorithm and what we see is the quality of this policy depends on two important factors.",
            "These are the variables M, Anita P. M Here represents the number of sub partitions.",
            "So in the previous S shaped world when it was partitioned into five partitions and would be 5.",
            "What you can see with M is that increasing the number of partitions seems to decrease the quality of the policy.",
            "But what we also see is that there is the skill learning error E to P and what this skill learning error etape states is how close in a single partition the skill that you've learned is to the optimal skill that can be used in that partition.",
            "So what this means then is that the more partitions that you create in the state space, the more refined the skills are that you're able to learn and therefore the lower the skill learning error becomes.",
            "And what we find in practice, since there is this tradeoff between increasing the.",
            "Number of partitions and wanting to decrease the skill learning error, we see that in practice the skill learning error actually tends to dominate this term, so therefore increasing the number of partitions tends in practice to decrease the auto.",
            "Sorry to increase the quality of the policy."
        ],
        [
            "So what we did is we wanted to test this on a number of different domains to sort of see this.",
            "In practice an one we chose is the well known paddle world domain.",
            "The goal in Puddle World is for an agent to circumnavigate the puddles represented by the L region and two because in the paddles that receives negative reward signal and to try and reach the goal location.",
            "What we did here is we created very simple policy representations.",
            "The policies that we used was a single parameter.",
            "Allowing the agents to move in a straight line in a given direction.",
            "Now, the reason that we chose a very simple policy representation is firstly we want to learn data and sample efficient policies or skills and in addition to that, if you can imagine sort of branching up to a very high dimensional domain.",
            "There you you don't because you are limited with your computational resources.",
            "It means that you're not.",
            "You are not sort of unlimited with the complexity with which you can represent your policy.",
            "So what?",
            "So what you need to do is to take the complexity that you are given and use that to somehow solve the task.",
            "And this is what we're trying to do here where the given complexity that we're taking is a single parameter.",
            "And what we do is we run that over different grid partitioning's where the one by one grid partitioning.",
            "Is this monolithic policy solving the single task in with one policy?",
            "Sorry, solving the whole task with one policy and what we see when solving the task with a single parameter and monolithic policy is that we get a very bad average cost and.",
            "But if we take this single parameter policy representation and we then assign that as the policy representation for each of the skills in partitions, for example the two by two or three by three partitions, we see that we are able to solve the task and to get far better performance than you would with a monolithic approach.",
            "So yeah, So what I was saying is that if you look for example at the three by three domain, what you're going to see, you can see that the average cost is slightly better, and what this means is that there are certain partitionings.",
            "That are better suited to domains which actually reduces the skill learning error and yeah, so."
        ],
        [
            "So basically another interesting property from this is, it converges really quickly.",
            "This is an example on the figure.",
            "On the left is running LS be on a 2 by two grid partitioning and what you can see is that it converges in two iterations of LSP.",
            "And obviously has better performance than running it with the monolithic policy.",
            "Now another interesting element is we want skills to be reusable and what we did is we ran Powder World on a four by four domain and what has been plotted here is a quiver plot of the direction of the skills that have been chosen or that have been learned.",
            "And what we see is that many of the skills here are repeated and what this tells us is that there is this potential that when you move to really high dimensional domains, alot of the skills can essentially be reused in different parts of the state space which can essentially then speed up your planning procedure."
        ],
        [
            "We then decided to test this on something.",
            "With more complicated dynamics, namely pinball, the idea with pinball is the dynamics are more complicated because the obstacles.",
            "The Gray figures that you see in the picture are very nonlinear.",
            "The collisions are very nonlinear dynamics in this domain or nonlinear.",
            "And the goal is for the agents.",
            "The blue ball to reach the red region.",
            "Now what we did here is again, we restricted the policy representation to a very simple policy that allows the agent to curve in a certain direction in a single direction.",
            "And what we saw is when applying this to trying to solve this with a monolithic policy, a single policy, it was unable to get the desired performance.",
            "It was unable to solve the task.",
            "However, when partitioning this pinball domain into four different part into four different skills, or Inter petitioning of four skills.",
            "We were able to solve the task and achieve near optimal performance."
        ],
        [
            "We then applied this film more challenging pinball domain and this this we partitioned again, this time into 12 skills.",
            "All these partitionings are arbitrary.",
            "I mean, you can partition it however you want.",
            "The skills again, will arbitrarily defined and what we found is that LSP, even though it was slightly less optimal than in the previous case, still significantly outperformed the monolithic policy which was unable to solve the task at all."
        ],
        [
            "So what we see is that the monolithic approach, solving it with a single policy is not necessarily.",
            "It is not necessarily feasible for high dimensional domains, and therefore one way to deal with this is to decompose the task such that an iteratively learn skills such that we are able to scale to these high dimensional domains.",
            "Something that we provide is the first theoretical convergence guarantee for iteratively learning these skills in continuous MDP's, and we do this by recognizing important properties that the skills need to work together.",
            "They need to feed off of 1 another in order to solve the task.",
            "And that skill learning requires iterative improvements.",
            "You learn one skill and then you need to.",
            "In the second iteration, use what you've learned from that one skill to propagate it back to learn the next skill.",
            "And using this framework there is this potential to then hopefully scale up to even more high dimensional problems.",
            "Thanks.",
            "Decompose the space or you showed that the results were good in the.",
            "In that last example with the separation.",
            "So what's the ratio of performance of the best decomposition versus the worst decomposition?",
            "For instance?",
            "Sorry, the best partitions this position, so yes, OK.",
            "So I mean, in this work we didn't necessarily look at.",
            "Partitions in depth.",
            "It was more to show that by taking the state space and partitioning it, we are we able to sort of decompose the task and solve the problem.",
            "The partitioning the state space.",
            "We don't have too much like really that much results on it other than the partitions that we did create were arbitrarily chosen.",
            "Were they all equivalent in their performance?",
            "Excuse me.",
            "Where they all equivalent in their performance?",
            "Oh, you mean the partitions?",
            "So as I said here?"
        ],
        [
            "You can see that like certain partition designs are better suited to the problem than other partitions, and that very much depends on the skill learning error, etape here and it's essentially a matter of you know.",
            "Choosing the right chyper partitioning in order to figure out exactly.",
            "What is better suited but the biggest point that we were trying to put across was that this at least outperforms the monolithic approach of solving it in a single shot.",
            "I think I had a very similar question, so I thought this was really cool work, but I guess I wanted to see a quantification of how epsilon related to M, because it seems like in general that might scale exponentially with the dimension.",
            "Yeah, I mean again, so I yeah, that's a good question and I think that this is something that is very problem specific.",
            "You know, I think I think it almost could be like another sort of analysis to kind of go into that and figure out how it relates to it.",
            "So yeah, intuitively yeah, I would just say problem specific, but it would definitely be something to look into, yeah?",
            "OK, thank you for the talk.",
            "I didn't.",
            "I feel I didn't really understand the algorithm and I just wonder if I could ask a couple of questions.",
            "I understand you pick a partition and then we'll look at the problem.",
            "Then you learn a skill within a partition.",
            "But to do that you have to have a goal for the partition and the goal an you have to have a way of handling if you're within the partition you executing your policy in that partition, that skill you may leave the partition where that would happen all the time and you have to say how good it is to leave it in.",
            "Different ways and.",
            "Yeah, how does it work at all, you know?",
            "Yeah, OK.",
            "So yeah.",
            "So these were yeah, more technical details.",
            "But yeah, definitely the way that it works is essentially as follows.",
            "We're essentially we were given a partition of say 9 skills and the idea is that in one of the partitions is this call region.",
            "So what we do the way the algorithm works is you learn a skill.",
            "So the skill we call that like a skill MDP, and this then bootstraps reward back through the domain.",
            "We then perform a policy evaluation step over the entire MDP.",
            "Using this newly learned skill that we've created, how can you learn a skill when there isn't a goal?",
            "No, so so the way it works is that we learn we do this one by one.",
            "So initially, if the skill is not in the goal and it doesn't reach the goal, nothing meaningful will be learned.",
            "So the idea is that as soon as one of the skills, the skill that is in the partition containing the goal reaches the goal region, then meaningful reward is propagated back through the MDP.",
            "And then when we perform policy evaluation over the entire MDP, having learned this skill, that is propagated back meaningful reward, the value function starts to shape.",
            "In a sense that allows you to start to solve the problem on the next iteration.",
            "The skills that are adjacent to that skill will then be able to get that value that has been propagated back by the skill that was in the goal region.",
            "You then perform policy evaluation over the MDP again.",
            "And you repeat this process until you've learned all the skills from other partitions.",
            "Did that.",
            "Answer.",
            "OK cool thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey everybody, I she said my name is Daniel and I'm I'm talking about our work bootstrapping skills that was Co authored by myself, Timothy Man and Shimon or from the Technion and Tim is now just moved onto Google Deep Mind.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the structure of the talk I'm going to motivate our idea of bootstrapping skills and then talk about a key structure in our work, which is skills and then go into detail about our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Learning skills via bootstrapping as well as the theoretical results, which is the main contribution of our work.",
                    "label": 1
                },
                {
                    "sent": "I'm going to then discuss and then sort of show how this works practically and sort of confirming some of our theoretical results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the idea behind a monolithic policy?",
                    "label": 1
                },
                {
                    "sent": "So imagine we're given an S shaped MDP and the task areas for an agent to move from the bottom left of the MDP to the goal region.",
                    "label": 0
                },
                {
                    "sent": "Now, in the monolithic approach, you're learning a single policy, one policy that solves the entire MDP.",
                    "label": 1
                },
                {
                    "sent": "Now this single policy, if you're imagine scaling up to really high dimensional M DPS can become really big and complex and require a huge number of parameters, and the question is is how can we somehow decompose this and break this down so that we're able to learn with less complex policy?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to give you an idea of what this means in a high dimensional domain, imagine we have now are robots, and this robot's task is to leave the room.",
                    "label": 0
                },
                {
                    "sent": "So here the skill that the robot needs to learn or its policy.",
                    "label": 1
                },
                {
                    "sent": "I'm going to use these words interchangeably is to walk to the door, open the door, or grasp the door, open it and walk through the door.",
                    "label": 1
                },
                {
                    "sent": "As a single skill, this is a very complex structure that you need to learn.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how can we decompose this an?",
                    "label": 0
                },
                {
                    "sent": "Learn this with more simple skills.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or policies.",
                    "label": 0
                },
                {
                    "sent": "So one way of doing this is to decompose the task to partition the state space into a set of partitions and within each partition define a skill.",
                    "label": 0
                },
                {
                    "sent": "And this skills task is to accomplish a subgoal, and the idea is that by learning a set of skills in each of these partitions, we can then combine these skills together in order to complete the task in a near optimal fashion, so a good way to think of skill is essentially a special form of an option, with their idea is that you want to be able to reuse these skills throughout different parts of the state space.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to give you an idea of where this how this can help.",
                    "label": 0
                },
                {
                    "sent": "For example, let's consider that same task where the robot needs to leave the room.",
                    "label": 1
                },
                {
                    "sent": "Now here we can take that really big skill or policy and we can break it down into more decompose it at least into a similar set of skills.",
                    "label": 0
                },
                {
                    "sent": "So here for example, the new skills are to walk, grasp the door and open the door nob.",
                    "label": 1
                },
                {
                    "sent": "Now the idea here is that we partition the state space and in each region of that state space learn a skill that is relevant to solving that part of the state space.",
                    "label": 0
                },
                {
                    "sent": "So for example, when the robot is in the corridor, he learns a skill to walk to the door.",
                    "label": 0
                },
                {
                    "sent": "When he's at the door, he learns a skill to grasp the doorknob, and once he crossed the doorknob, he learns a skill to open the door.",
                    "label": 0
                },
                {
                    "sent": "Once the door is open, he can then reuse a skill that is learned in a previous partition in order to walk through the door.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does it?",
                    "label": 0
                },
                {
                    "sent": "How do we do this?",
                    "label": 0
                },
                {
                    "sent": "How do we learn skills?",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that in our work in our algorithm we require a partitioning of the state space, so we assume that this partitioning is provided by a domain expert, and we then define a skill in each partition in the state space.",
                    "label": 0
                },
                {
                    "sent": "So here there are five partitions of the state space and therefore 5 skills have been arbitrarily defined in the state space.",
                    "label": 0
                },
                {
                    "sent": "The goal then is to learn the best local policy for each skill.",
                    "label": 1
                },
                {
                    "sent": "So in other words, to learn the best skill.",
                    "label": 0
                },
                {
                    "sent": "That when combined together with all the other skills, is able to solve the task.",
                    "label": 1
                },
                {
                    "sent": "This work has been inspired by skill chaining a work by Colonel Diaries and Barter.",
                    "label": 0
                },
                {
                    "sent": "But here what we do is we provide the first theoretical convergence guarantees for iteratively learning skills in a continuous MDP.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does this algorithm work?",
                    "label": 0
                },
                {
                    "sent": "How do we learn these skills so we recognize two important things when when learning these skills in order to get convergence and one is that the skills need to work together, a skill in isolation.",
                    "label": 0
                },
                {
                    "sent": "For example, Sigma 3.",
                    "label": 0
                },
                {
                    "sent": "The third skill doesn't know necessarily whether it needs to move to Sigma four or down to Sigma 2.",
                    "label": 0
                },
                {
                    "sent": "The skills need to feed off of 1 another in order to work together and be able to compose themselves such that a task can be solved.",
                    "label": 0
                },
                {
                    "sent": "So the way this would work is we start in the first iteration by having a set of arbitrarily defined skills, one per partition.",
                    "label": 0
                },
                {
                    "sent": "The idea then is that the skill that contains the goal region, the skill in the goal partition then eventually reaches the goal and then bootstraps reward signal.",
                    "label": 0
                },
                {
                    "sent": "Through its partition.",
                    "label": 0
                },
                {
                    "sent": "That skill is then able to learn what it's supposed to do in order, in other words, to reach the goal location and in the subsequent iteration the adjacent skill then is able to take back this meaningful reward that has been bootstrapped by the original skill and is then able to learn it skills such that it can then move towards the goal.",
                    "label": 0
                },
                {
                    "sent": "This is then repeated over a number of iterations until every partition or every skill has been learned in a partition, and these skills are then composed together in order to be able to.",
                    "label": 0
                },
                {
                    "sent": "Reached the goal near optimally.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when another sort of inherent thing that is happening here is something that we've turned model iteration.",
                    "label": 0
                },
                {
                    "sent": "So if you take a set of skills and you take an MDP, you get a SMD model and the idea is that when we start off in this algorithm, we have a misspecified model.",
                    "label": 0
                },
                {
                    "sent": "We have a set of skills that when placed into this MDP cannot solve the task with their current state and the idea is then to do this model improvement procedure to improve the model.",
                    "label": 0
                },
                {
                    "sent": "IE continually improve learning the skill set and therefore improve the SMD model such that overtime you're able to get this optimal model or this optimal solution that allows you to solve the task.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I mentioned, we one of our main results was to have convergence guarantees for learning for getting near optimal convergence for iteratively learning skills in a continuous state MVP.",
                    "label": 0
                },
                {
                    "sent": "But another result that has come out of this is the ability to look at the quality of the policy that is returned by this LSP algorithm and what we see is the quality of this policy depends on two important factors.",
                    "label": 0
                },
                {
                    "sent": "These are the variables M, Anita P. M Here represents the number of sub partitions.",
                    "label": 0
                },
                {
                    "sent": "So in the previous S shaped world when it was partitioned into five partitions and would be 5.",
                    "label": 0
                },
                {
                    "sent": "What you can see with M is that increasing the number of partitions seems to decrease the quality of the policy.",
                    "label": 1
                },
                {
                    "sent": "But what we also see is that there is the skill learning error E to P and what this skill learning error etape states is how close in a single partition the skill that you've learned is to the optimal skill that can be used in that partition.",
                    "label": 0
                },
                {
                    "sent": "So what this means then is that the more partitions that you create in the state space, the more refined the skills are that you're able to learn and therefore the lower the skill learning error becomes.",
                    "label": 0
                },
                {
                    "sent": "And what we find in practice, since there is this tradeoff between increasing the.",
                    "label": 0
                },
                {
                    "sent": "Number of partitions and wanting to decrease the skill learning error, we see that in practice the skill learning error actually tends to dominate this term, so therefore increasing the number of partitions tends in practice to decrease the auto.",
                    "label": 0
                },
                {
                    "sent": "Sorry to increase the quality of the policy.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we did is we wanted to test this on a number of different domains to sort of see this.",
                    "label": 0
                },
                {
                    "sent": "In practice an one we chose is the well known paddle world domain.",
                    "label": 0
                },
                {
                    "sent": "The goal in Puddle World is for an agent to circumnavigate the puddles represented by the L region and two because in the paddles that receives negative reward signal and to try and reach the goal location.",
                    "label": 0
                },
                {
                    "sent": "What we did here is we created very simple policy representations.",
                    "label": 0
                },
                {
                    "sent": "The policies that we used was a single parameter.",
                    "label": 0
                },
                {
                    "sent": "Allowing the agents to move in a straight line in a given direction.",
                    "label": 0
                },
                {
                    "sent": "Now, the reason that we chose a very simple policy representation is firstly we want to learn data and sample efficient policies or skills and in addition to that, if you can imagine sort of branching up to a very high dimensional domain.",
                    "label": 0
                },
                {
                    "sent": "There you you don't because you are limited with your computational resources.",
                    "label": 0
                },
                {
                    "sent": "It means that you're not.",
                    "label": 0
                },
                {
                    "sent": "You are not sort of unlimited with the complexity with which you can represent your policy.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "So what you need to do is to take the complexity that you are given and use that to somehow solve the task.",
                    "label": 0
                },
                {
                    "sent": "And this is what we're trying to do here where the given complexity that we're taking is a single parameter.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we run that over different grid partitioning's where the one by one grid partitioning.",
                    "label": 0
                },
                {
                    "sent": "Is this monolithic policy solving the single task in with one policy?",
                    "label": 0
                },
                {
                    "sent": "Sorry, solving the whole task with one policy and what we see when solving the task with a single parameter and monolithic policy is that we get a very bad average cost and.",
                    "label": 0
                },
                {
                    "sent": "But if we take this single parameter policy representation and we then assign that as the policy representation for each of the skills in partitions, for example the two by two or three by three partitions, we see that we are able to solve the task and to get far better performance than you would with a monolithic approach.",
                    "label": 0
                },
                {
                    "sent": "So yeah, So what I was saying is that if you look for example at the three by three domain, what you're going to see, you can see that the average cost is slightly better, and what this means is that there are certain partitionings.",
                    "label": 0
                },
                {
                    "sent": "That are better suited to domains which actually reduces the skill learning error and yeah, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically another interesting property from this is, it converges really quickly.",
                    "label": 0
                },
                {
                    "sent": "This is an example on the figure.",
                    "label": 0
                },
                {
                    "sent": "On the left is running LS be on a 2 by two grid partitioning and what you can see is that it converges in two iterations of LSP.",
                    "label": 0
                },
                {
                    "sent": "And obviously has better performance than running it with the monolithic policy.",
                    "label": 0
                },
                {
                    "sent": "Now another interesting element is we want skills to be reusable and what we did is we ran Powder World on a four by four domain and what has been plotted here is a quiver plot of the direction of the skills that have been chosen or that have been learned.",
                    "label": 0
                },
                {
                    "sent": "And what we see is that many of the skills here are repeated and what this tells us is that there is this potential that when you move to really high dimensional domains, alot of the skills can essentially be reused in different parts of the state space which can essentially then speed up your planning procedure.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We then decided to test this on something.",
                    "label": 0
                },
                {
                    "sent": "With more complicated dynamics, namely pinball, the idea with pinball is the dynamics are more complicated because the obstacles.",
                    "label": 0
                },
                {
                    "sent": "The Gray figures that you see in the picture are very nonlinear.",
                    "label": 0
                },
                {
                    "sent": "The collisions are very nonlinear dynamics in this domain or nonlinear.",
                    "label": 0
                },
                {
                    "sent": "And the goal is for the agents.",
                    "label": 0
                },
                {
                    "sent": "The blue ball to reach the red region.",
                    "label": 0
                },
                {
                    "sent": "Now what we did here is again, we restricted the policy representation to a very simple policy that allows the agent to curve in a certain direction in a single direction.",
                    "label": 0
                },
                {
                    "sent": "And what we saw is when applying this to trying to solve this with a monolithic policy, a single policy, it was unable to get the desired performance.",
                    "label": 0
                },
                {
                    "sent": "It was unable to solve the task.",
                    "label": 0
                },
                {
                    "sent": "However, when partitioning this pinball domain into four different part into four different skills, or Inter petitioning of four skills.",
                    "label": 0
                },
                {
                    "sent": "We were able to solve the task and achieve near optimal performance.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We then applied this film more challenging pinball domain and this this we partitioned again, this time into 12 skills.",
                    "label": 0
                },
                {
                    "sent": "All these partitionings are arbitrary.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can partition it however you want.",
                    "label": 0
                },
                {
                    "sent": "The skills again, will arbitrarily defined and what we found is that LSP, even though it was slightly less optimal than in the previous case, still significantly outperformed the monolithic policy which was unable to solve the task at all.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we see is that the monolithic approach, solving it with a single policy is not necessarily.",
                    "label": 1
                },
                {
                    "sent": "It is not necessarily feasible for high dimensional domains, and therefore one way to deal with this is to decompose the task such that an iteratively learn skills such that we are able to scale to these high dimensional domains.",
                    "label": 0
                },
                {
                    "sent": "Something that we provide is the first theoretical convergence guarantee for iteratively learning these skills in continuous MDP's, and we do this by recognizing important properties that the skills need to work together.",
                    "label": 1
                },
                {
                    "sent": "They need to feed off of 1 another in order to solve the task.",
                    "label": 1
                },
                {
                    "sent": "And that skill learning requires iterative improvements.",
                    "label": 0
                },
                {
                    "sent": "You learn one skill and then you need to.",
                    "label": 0
                },
                {
                    "sent": "In the second iteration, use what you've learned from that one skill to propagate it back to learn the next skill.",
                    "label": 0
                },
                {
                    "sent": "And using this framework there is this potential to then hopefully scale up to even more high dimensional problems.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Decompose the space or you showed that the results were good in the.",
                    "label": 0
                },
                {
                    "sent": "In that last example with the separation.",
                    "label": 0
                },
                {
                    "sent": "So what's the ratio of performance of the best decomposition versus the worst decomposition?",
                    "label": 0
                },
                {
                    "sent": "For instance?",
                    "label": 0
                },
                {
                    "sent": "Sorry, the best partitions this position, so yes, OK.",
                    "label": 0
                },
                {
                    "sent": "So I mean, in this work we didn't necessarily look at.",
                    "label": 0
                },
                {
                    "sent": "Partitions in depth.",
                    "label": 0
                },
                {
                    "sent": "It was more to show that by taking the state space and partitioning it, we are we able to sort of decompose the task and solve the problem.",
                    "label": 0
                },
                {
                    "sent": "The partitioning the state space.",
                    "label": 0
                },
                {
                    "sent": "We don't have too much like really that much results on it other than the partitions that we did create were arbitrarily chosen.",
                    "label": 0
                },
                {
                    "sent": "Were they all equivalent in their performance?",
                    "label": 0
                },
                {
                    "sent": "Excuse me.",
                    "label": 0
                },
                {
                    "sent": "Where they all equivalent in their performance?",
                    "label": 0
                },
                {
                    "sent": "Oh, you mean the partitions?",
                    "label": 0
                },
                {
                    "sent": "So as I said here?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see that like certain partition designs are better suited to the problem than other partitions, and that very much depends on the skill learning error, etape here and it's essentially a matter of you know.",
                    "label": 0
                },
                {
                    "sent": "Choosing the right chyper partitioning in order to figure out exactly.",
                    "label": 0
                },
                {
                    "sent": "What is better suited but the biggest point that we were trying to put across was that this at least outperforms the monolithic approach of solving it in a single shot.",
                    "label": 0
                },
                {
                    "sent": "I think I had a very similar question, so I thought this was really cool work, but I guess I wanted to see a quantification of how epsilon related to M, because it seems like in general that might scale exponentially with the dimension.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean again, so I yeah, that's a good question and I think that this is something that is very problem specific.",
                    "label": 0
                },
                {
                    "sent": "You know, I think I think it almost could be like another sort of analysis to kind of go into that and figure out how it relates to it.",
                    "label": 0
                },
                {
                    "sent": "So yeah, intuitively yeah, I would just say problem specific, but it would definitely be something to look into, yeah?",
                    "label": 0
                },
                {
                    "sent": "OK, thank you for the talk.",
                    "label": 0
                },
                {
                    "sent": "I didn't.",
                    "label": 0
                },
                {
                    "sent": "I feel I didn't really understand the algorithm and I just wonder if I could ask a couple of questions.",
                    "label": 0
                },
                {
                    "sent": "I understand you pick a partition and then we'll look at the problem.",
                    "label": 0
                },
                {
                    "sent": "Then you learn a skill within a partition.",
                    "label": 0
                },
                {
                    "sent": "But to do that you have to have a goal for the partition and the goal an you have to have a way of handling if you're within the partition you executing your policy in that partition, that skill you may leave the partition where that would happen all the time and you have to say how good it is to leave it in.",
                    "label": 0
                },
                {
                    "sent": "Different ways and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, how does it work at all, you know?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "So these were yeah, more technical details.",
                    "label": 0
                },
                {
                    "sent": "But yeah, definitely the way that it works is essentially as follows.",
                    "label": 0
                },
                {
                    "sent": "We're essentially we were given a partition of say 9 skills and the idea is that in one of the partitions is this call region.",
                    "label": 0
                },
                {
                    "sent": "So what we do the way the algorithm works is you learn a skill.",
                    "label": 0
                },
                {
                    "sent": "So the skill we call that like a skill MDP, and this then bootstraps reward back through the domain.",
                    "label": 0
                },
                {
                    "sent": "We then perform a policy evaluation step over the entire MDP.",
                    "label": 0
                },
                {
                    "sent": "Using this newly learned skill that we've created, how can you learn a skill when there isn't a goal?",
                    "label": 0
                },
                {
                    "sent": "No, so so the way it works is that we learn we do this one by one.",
                    "label": 0
                },
                {
                    "sent": "So initially, if the skill is not in the goal and it doesn't reach the goal, nothing meaningful will be learned.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that as soon as one of the skills, the skill that is in the partition containing the goal reaches the goal region, then meaningful reward is propagated back through the MDP.",
                    "label": 0
                },
                {
                    "sent": "And then when we perform policy evaluation over the entire MDP, having learned this skill, that is propagated back meaningful reward, the value function starts to shape.",
                    "label": 0
                },
                {
                    "sent": "In a sense that allows you to start to solve the problem on the next iteration.",
                    "label": 0
                },
                {
                    "sent": "The skills that are adjacent to that skill will then be able to get that value that has been propagated back by the skill that was in the goal region.",
                    "label": 0
                },
                {
                    "sent": "You then perform policy evaluation over the MDP again.",
                    "label": 0
                },
                {
                    "sent": "And you repeat this process until you've learned all the skills from other partitions.",
                    "label": 0
                },
                {
                    "sent": "Did that.",
                    "label": 0
                },
                {
                    "sent": "Answer.",
                    "label": 0
                },
                {
                    "sent": "OK cool thanks.",
                    "label": 0
                }
            ]
        }
    }
}