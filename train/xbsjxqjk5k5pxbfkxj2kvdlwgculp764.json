{
    "id": "xbsjxqjk5k5pxbfkxj2kvdlwgculp764",
    "title": "Partially Supervised Feature Selection with Regularized Linear Models",
    "info": {
        "author": [
            "Thibault Helleputte, Computing Science Engineering Department, Universit\u00e9 catholique de Louvain"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection",
            "Top->Computer Science->Machine Learning->Linear Models"
        ]
    },
    "url": "http://videolectures.net/icml09_helleputte_psfs/",
    "segmentation": [
        [
            "OK, so I'm going to present you a technique for feature selection embedded within linear regularised models, and it is a partially supervised feature selection technique.",
            "And here I have to clarify some things because as far as I understand in this week supervision session.",
            "Every methods are about partial or semi supervised in the sense of the classification, but here the supervision will be on the feature selection and not on the classification.",
            "And we've been designing this technique in a general context, but we have been applying this to the particular case of microarray data, which are a good example of high dimensional data with few sample."
        ],
        [
            "So for those who don't know what our microarray data, well, it's not really important to know in details, but roughly micheri measure measure gene gene expression, and if you don't know what is gene expression is a kind of activity of the jeans and those data come in such large matrix where each column stands for a gene and each rose stands for a sample.",
            "And usually we have a class label.",
            "Which come from external annotation, it can be the patient status or the biological type of."
        ],
        [
            "Chu and what is particular to that kind of data is that the number of dimension is huge, usually several 10s of thousands, and the number of sample is at best if you're lucky, kind of 300."
        ],
        [
            "What are my credit I useful for?",
            "Well, for diagnosis or prognosis issues and it is applyable in clinical settings or in pharmaceutical applications to predict.",
            "For example if treatment under development will react on a patient or not.",
            "And in that context."
        ],
        [
            "Selection is often applied for explanatory concern to get an idea of what dimensions are useful to predict the status of the patient and also to make cheaper diagnosis or prognosis.",
            "Kits and in some case more general.",
            "If you do selection has been shown to improve the classification performances.",
            "All of you know."
        ],
        [
            "That the support vector machines in early show good performances in the classification setting and also they have been extended in many ways to perform feature selection and one of."
        ],
        [
            "Those extension is name recursive, feature elimination and in fact it is an iterative algorithm which trains linear SVM and based on the magnitude of the the weight components of the model will discard the less important features, less important meaning that will less decrease the margin and it is an embedded technique which then use the classifier structure and that is elegant and it's interesting.",
            "And another way to."
        ],
        [
            "Formulate the feature selection problem for classification.",
            "Is the zero normalization.",
            "So you want to minimize the number of non new dimensions in your model, subject to a margin constraint to guarantee a good qualification.",
            "Classification performance is.",
            "Unfortunately, this problem is computationally difficult and so relaxations have been proposed.",
            "For example, the arrow."
        ],
        [
            "Methods from Western colleagues.",
            "Instead of solving the directly the zero normalization problem, they solve an approximation to it, which is based on the logarithm of the absolute value of the weight components and to get an intuition of why it is a good approximation to the 0 known problem, you have that figure way you see the objective function is nearly flat everywhere except along the axis, meaning where.",
            "Some weights become zero.",
            "There you have a huge gain in your minimization problem, and again it is subject to the same margin constraints."
        ],
        [
            "And this has been the related to give a convenient algorithm with his name L2 around, which is also like arefi, an iterative algorithm based on linear SVM, and at each iteration you will train an SVM, but with the inputs of the margin constraint re scaled proportionally to the weight vector you find at the previous iteration.",
            "And that leads to.",
            "Need to retrieve algorithm which will decrease the importance of feature not important for the classification program.",
            "So some ways will decrease below the machine precision.",
            "An will finally be assimilated as zero components."
        ],
        [
            "I'm so OK, but the problem with high dimensional data is that in that case you have much more dimension than samples and so even with simple models like linear models you have an indetermined system.",
            "And of course we can use."
        ],
        [
            "And we do use regularization.",
            "For example the maximal margin.",
            "But even with that kind of regularization, we witness overfitting and lack of robustness in the selected dimensions also.",
            "And so we need a strong."
        ],
        [
            "For regularization or a stronger stronger inductive bias or another inductive bias, the problem is to find extra information to give to or model.",
            "But we can ask the field experts."
        ],
        [
            "If they have explained formation and it happens that most of the time that they have, and for example in the case of microarray biologist may know or guess that some of the genes.",
            "Are likely to be more relevant to be related to the pathology in their study."
        ],
        [
            "And even if that knowledge maybe not sufficient in itself to build a robust model, and even if it is imprecise, they may be wrong about that prior knowledge where it is extra information.",
            "And if we can include it in our models, we should."
        ],
        [
            "So that's what we do, and we've been designing a technical, partially supervised feature selection where we use the prior knowledge on feature relevance to bias the selection of the feature.",
            "So here again, I I precisely we have a full supervision on the samples labels, so this technique is different from feature selection."
        ],
        [
            "For some supervised classification, that's what that's not what we do.",
            "We have a semi supervised feature selection, partially supervised feature selection on, well, the the semi supervision is the on the features and that's on the samples."
        ],
        [
            "So, mathematically we define a vector of relevance which is in beta, and if you have no information about the relevance of a given feature, you just put a weight equal to 1.",
            "But if you have an idea of the prior relevance of feature JU encoded in the better J component with a higher value than one.",
            "And so, in fact, this technique extends naturally the wrong method.",
            "And so you have nearly the same objective function as before, except that you have a rescaling factor here, and meaning that the dimensions apriori more relevant will be less penalizing the minimization problem.",
            "And again, you have the margin constraint to guarantee good classification."
        ],
        [
            "And this leads to the same kind of algorithm as L to run, except that here it is partially supervised, and so at each iteration we train a linear SVM.",
            "Subject also to rescaled margin constraints, and here the inputs are rescaled in by multiplying component wise with the weight found at the previous iteration but also by the prior relevance.",
            "And."
        ],
        [
            "In our experiments we've been using for micro data set, and you see that each of them has few samples with respect to the number of features.",
            "And those are all binary classification problems and they are all unbalanced data set.",
            "And then we're going to money, money, talk to."
        ],
        [
            "Measures the first one is the stability.",
            "What is the stability of feature selection technique?",
            "Well, suppose you build several sets of selected features on different sub samplings of your patients.",
            "For example, well, you expect that the genes selected will remain the same because the metabolism of the patients is roughly the same normally, so we expect a high stability even when we.",
            "Make take different sub samplings of the data and that is what the Queen Sheba Stability Index measures.",
            "And although that's not the only thing we we decided to measure because the stability in itself is not a sufficient criterion to evaluate the feature selection technique.",
            "Because suppose you you arbitrarily choose.",
            "I don't know 20 features among all your possible features, and whatever the supersampling you take, you always select those 20 jeans.",
            "You probably will have a poor classification.",
            "Performance is but a high stability just by construction, so stability in itself is not a sufficient criterion.",
            "So we use also the justification performance measured by the balance classification rate.",
            "We don't use the accuracy because of the imbalance.",
            "Minton of the datasets.",
            "And in fact, the balance classification rate is also the average between the specificity and sensitivity, which are two common measure for biological applications.",
            "And."
        ],
        [
            "In in our first experimental setting we we had prior knowledge on two or three genes for two datasets.",
            "Two or three jeans that were used as clinical markers.",
            "And so we we set orbital vector 21 everywhere except for the component corresponding to those two or three jeans where we set a higher value to 10.",
            "And.",
            "Then we repeat 200 times the following.",
            "We split our data into 90% train and 10% test on the training part.",
            "We normalize.",
            "We select our feature with partially supervised feature selection.",
            "And then we build linear SVM on the selected features and then we test on the test part.",
            "We evaluate the PCR we averaged in on the 200 loops.",
            "And after all, we have 200 sets of selected dimension and we compute the stability between those selected feature sets.",
            "In"
        ],
        [
            "Here are the results for one of our data set where we started from more than 7000 feature.",
            "But here we show results only for the interesting part which are between 256 and four dimension which is which are common size for practical application and so on.",
            "The left on the right you have the classification performances and on the left you have the stability.",
            "And first, let's have a look at the classification performances and compare the blue curve, which is the standard L2 aroma technique and the red curve, which is the partially supervised version of that technique, and we see that when the number of feature really decrease below 16 dimensions, the traditional tuorum crashes.",
            "However, our technique remains at the high level of performance.",
            "And we also compared to the arefi technique in purple, and their results looks comparable to our method and we also tested the God of index.",
            "If you don't know what Gottlieb index is, it's a technique for feature selection that is closely related to at Test is not exactly the same, but it's the same idea.",
            "And there, whether it seems to perform a bit better and we also have another curve which is interesting.",
            "It's we called it random.",
            "It's partial supervision for feature selection where we favoured two genes at random.",
            "And what's interesting is that the results are the nearly the same.",
            "Then for the non supervised technique.",
            "And that means that our technique is able to depart from probably wrong prior knowledge.",
            "I know if you if we have a look at the stability.",
            "There, let's begin by the comparison between L2 Roman partially supervised L2 room.",
            "So the blue and the red curve again.",
            "And there we see that.",
            "We not only gain here on classification performances, but we mainly gain instability and there is a striking difference between L2 romanets partially supervised version.",
            "And here we we also beat the arefi.",
            "And even at the end goal of index.",
            "And here the random curve in in Black is a bit higher than the non supervised and unsupervised L2 room which can be explained by the fact that by by construction, even if the two favorite jeans are random jeans.",
            "Yet they are favored, and so the stability is a bit higher, but it is really lower than when you favor to jeans according to relevant prior knowledge.",
            "And once again, that means that our technique is able to depart from wrong knowledge, and this is very, very important result.",
            "And so we had also prior knowledge for another that I set on three jeans and."
        ],
        [
            "It's Arkham Parable and we wanted to generalize or conclusion to other datasets, but it was difficult to find in the literature.",
            "Other data set for which we had prior knowledge, so we simulated it.",
            "By a bit more complicated protocol where we use a part of the data set to compute feature selection and to favor those gene as if they were prior knowledge.",
            "But anyway, the conclusion are the same.",
            "Here we don't see really significant difference in classification performances.",
            "Between any technique?",
            "But once again, the stability is really different.",
            "If you have a look at the blue curve, which is the once again the L2 around technique so non supervised feature selection and or partially supervised feature selection in red there the difference is really large.",
            "And so when you take both the classification performances in the stability globally, we really improved the quality of the selection."
        ],
        [
            "And so here are the conclusions.",
            "First, we think that the stability is a metric that should be more used when assessing the quality of feature selection techniques.",
            "But not alone.",
            "As I said, it would be done with another classification.",
            "Performance is metric.",
            "Or partially supervised feature selection technique allows the incorporation of prior knowledge which is in itself an important result because it allows the interaction with the field experts.",
            "And it also it is also able to depart from potentially wrong prior knowledge or technique, naturally extends other techniques in the field of feature selection.",
            "In this case, it's around the arrow methods and it increases the stability of the selected features with respect to sampling variations.",
            "And in some cases it also improves classification performances.",
            "And since it is a multivariate method, the supervision of few dimensions influence also the selection of the other component of the final selected feature set you found.",
            "And I didn't show that.",
            "But it is also an interesting reason.",
            "So thank you for your."
        ],
        [
            "And if you have question.",
            "So I have time for some questions.",
            "What happens?"
        ],
        [
            "There are correlations.",
            "How?",
            "Is there a correlation between what?",
            "What is the relevance?",
            "We did understood it that but.",
            "Well, I I don't know.",
            "Google.",
            "So is it?",
            "When you have prior knowledge of that means something like, well, you should consider including these features that the biologists found useful, is it?",
            "Is is it something that it's something that increases stability?",
            "Is there some other effects that increase stability or?",
            "So have you studied why disability increases?",
            "The first I think that partly the stability increased because of course the gene order dimensions you favor are more likely to be finally selected, although it is not always the case.",
            "You may favor some dimension that can be not selected finally, but I think.",
            "I think that if you truly if your prior knowledge is really relevant, it will favor.",
            "In fact the ground truth that you are looking for, and so that's another reason for why the stability is increased.",
            "Do you have insight on how to select the paper size and play on the the?",
            "Yeah, nothing in that paper, but in another paper we studied at the sensibility to the magnitude of the beaten, and in fact it has an influence.",
            "Of course, if you said it set it close to one, it will nearly have no influence.",
            "But there is a large area where it has not a major influence.",
            "So I said between 5 and 100 in or experience in our experience, it's it doesn't matter.",
            "Thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to present you a technique for feature selection embedded within linear regularised models, and it is a partially supervised feature selection technique.",
                    "label": 1
                },
                {
                    "sent": "And here I have to clarify some things because as far as I understand in this week supervision session.",
                    "label": 0
                },
                {
                    "sent": "Every methods are about partial or semi supervised in the sense of the classification, but here the supervision will be on the feature selection and not on the classification.",
                    "label": 0
                },
                {
                    "sent": "And we've been designing this technique in a general context, but we have been applying this to the particular case of microarray data, which are a good example of high dimensional data with few sample.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for those who don't know what our microarray data, well, it's not really important to know in details, but roughly micheri measure measure gene gene expression, and if you don't know what is gene expression is a kind of activity of the jeans and those data come in such large matrix where each column stands for a gene and each rose stands for a sample.",
                    "label": 0
                },
                {
                    "sent": "And usually we have a class label.",
                    "label": 1
                },
                {
                    "sent": "Which come from external annotation, it can be the patient status or the biological type of.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chu and what is particular to that kind of data is that the number of dimension is huge, usually several 10s of thousands, and the number of sample is at best if you're lucky, kind of 300.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What are my credit I useful for?",
                    "label": 0
                },
                {
                    "sent": "Well, for diagnosis or prognosis issues and it is applyable in clinical settings or in pharmaceutical applications to predict.",
                    "label": 0
                },
                {
                    "sent": "For example if treatment under development will react on a patient or not.",
                    "label": 0
                },
                {
                    "sent": "And in that context.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Selection is often applied for explanatory concern to get an idea of what dimensions are useful to predict the status of the patient and also to make cheaper diagnosis or prognosis.",
                    "label": 0
                },
                {
                    "sent": "Kits and in some case more general.",
                    "label": 0
                },
                {
                    "sent": "If you do selection has been shown to improve the classification performances.",
                    "label": 0
                },
                {
                    "sent": "All of you know.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That the support vector machines in early show good performances in the classification setting and also they have been extended in many ways to perform feature selection and one of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those extension is name recursive, feature elimination and in fact it is an iterative algorithm which trains linear SVM and based on the magnitude of the the weight components of the model will discard the less important features, less important meaning that will less decrease the margin and it is an embedded technique which then use the classifier structure and that is elegant and it's interesting.",
                    "label": 0
                },
                {
                    "sent": "And another way to.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formulate the feature selection problem for classification.",
                    "label": 1
                },
                {
                    "sent": "Is the zero normalization.",
                    "label": 0
                },
                {
                    "sent": "So you want to minimize the number of non new dimensions in your model, subject to a margin constraint to guarantee a good qualification.",
                    "label": 0
                },
                {
                    "sent": "Classification performance is.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this problem is computationally difficult and so relaxations have been proposed.",
                    "label": 1
                },
                {
                    "sent": "For example, the arrow.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Methods from Western colleagues.",
                    "label": 0
                },
                {
                    "sent": "Instead of solving the directly the zero normalization problem, they solve an approximation to it, which is based on the logarithm of the absolute value of the weight components and to get an intuition of why it is a good approximation to the 0 known problem, you have that figure way you see the objective function is nearly flat everywhere except along the axis, meaning where.",
                    "label": 0
                },
                {
                    "sent": "Some weights become zero.",
                    "label": 0
                },
                {
                    "sent": "There you have a huge gain in your minimization problem, and again it is subject to the same margin constraints.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this has been the related to give a convenient algorithm with his name L2 around, which is also like arefi, an iterative algorithm based on linear SVM, and at each iteration you will train an SVM, but with the inputs of the margin constraint re scaled proportionally to the weight vector you find at the previous iteration.",
                    "label": 0
                },
                {
                    "sent": "And that leads to.",
                    "label": 0
                },
                {
                    "sent": "Need to retrieve algorithm which will decrease the importance of feature not important for the classification program.",
                    "label": 0
                },
                {
                    "sent": "So some ways will decrease below the machine precision.",
                    "label": 0
                },
                {
                    "sent": "An will finally be assimilated as zero components.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm so OK, but the problem with high dimensional data is that in that case you have much more dimension than samples and so even with simple models like linear models you have an indetermined system.",
                    "label": 0
                },
                {
                    "sent": "And of course we can use.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we do use regularization.",
                    "label": 0
                },
                {
                    "sent": "For example the maximal margin.",
                    "label": 0
                },
                {
                    "sent": "But even with that kind of regularization, we witness overfitting and lack of robustness in the selected dimensions also.",
                    "label": 1
                },
                {
                    "sent": "And so we need a strong.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For regularization or a stronger stronger inductive bias or another inductive bias, the problem is to find extra information to give to or model.",
                    "label": 0
                },
                {
                    "sent": "But we can ask the field experts.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If they have explained formation and it happens that most of the time that they have, and for example in the case of microarray biologist may know or guess that some of the genes.",
                    "label": 0
                },
                {
                    "sent": "Are likely to be more relevant to be related to the pathology in their study.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And even if that knowledge maybe not sufficient in itself to build a robust model, and even if it is imprecise, they may be wrong about that prior knowledge where it is extra information.",
                    "label": 0
                },
                {
                    "sent": "And if we can include it in our models, we should.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's what we do, and we've been designing a technical, partially supervised feature selection where we use the prior knowledge on feature relevance to bias the selection of the feature.",
                    "label": 0
                },
                {
                    "sent": "So here again, I I precisely we have a full supervision on the samples labels, so this technique is different from feature selection.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For some supervised classification, that's what that's not what we do.",
                    "label": 0
                },
                {
                    "sent": "We have a semi supervised feature selection, partially supervised feature selection on, well, the the semi supervision is the on the features and that's on the samples.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, mathematically we define a vector of relevance which is in beta, and if you have no information about the relevance of a given feature, you just put a weight equal to 1.",
                    "label": 0
                },
                {
                    "sent": "But if you have an idea of the prior relevance of feature JU encoded in the better J component with a higher value than one.",
                    "label": 1
                },
                {
                    "sent": "And so, in fact, this technique extends naturally the wrong method.",
                    "label": 0
                },
                {
                    "sent": "And so you have nearly the same objective function as before, except that you have a rescaling factor here, and meaning that the dimensions apriori more relevant will be less penalizing the minimization problem.",
                    "label": 0
                },
                {
                    "sent": "And again, you have the margin constraint to guarantee good classification.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this leads to the same kind of algorithm as L to run, except that here it is partially supervised, and so at each iteration we train a linear SVM.",
                    "label": 0
                },
                {
                    "sent": "Subject also to rescaled margin constraints, and here the inputs are rescaled in by multiplying component wise with the weight found at the previous iteration but also by the prior relevance.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our experiments we've been using for micro data set, and you see that each of them has few samples with respect to the number of features.",
                    "label": 0
                },
                {
                    "sent": "And those are all binary classification problems and they are all unbalanced data set.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to money, money, talk to.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Measures the first one is the stability.",
                    "label": 0
                },
                {
                    "sent": "What is the stability of feature selection technique?",
                    "label": 1
                },
                {
                    "sent": "Well, suppose you build several sets of selected features on different sub samplings of your patients.",
                    "label": 0
                },
                {
                    "sent": "For example, well, you expect that the genes selected will remain the same because the metabolism of the patients is roughly the same normally, so we expect a high stability even when we.",
                    "label": 0
                },
                {
                    "sent": "Make take different sub samplings of the data and that is what the Queen Sheba Stability Index measures.",
                    "label": 0
                },
                {
                    "sent": "And although that's not the only thing we we decided to measure because the stability in itself is not a sufficient criterion to evaluate the feature selection technique.",
                    "label": 0
                },
                {
                    "sent": "Because suppose you you arbitrarily choose.",
                    "label": 1
                },
                {
                    "sent": "I don't know 20 features among all your possible features, and whatever the supersampling you take, you always select those 20 jeans.",
                    "label": 0
                },
                {
                    "sent": "You probably will have a poor classification.",
                    "label": 0
                },
                {
                    "sent": "Performance is but a high stability just by construction, so stability in itself is not a sufficient criterion.",
                    "label": 0
                },
                {
                    "sent": "So we use also the justification performance measured by the balance classification rate.",
                    "label": 0
                },
                {
                    "sent": "We don't use the accuracy because of the imbalance.",
                    "label": 0
                },
                {
                    "sent": "Minton of the datasets.",
                    "label": 0
                },
                {
                    "sent": "And in fact, the balance classification rate is also the average between the specificity and sensitivity, which are two common measure for biological applications.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In in our first experimental setting we we had prior knowledge on two or three genes for two datasets.",
                    "label": 0
                },
                {
                    "sent": "Two or three jeans that were used as clinical markers.",
                    "label": 1
                },
                {
                    "sent": "And so we we set orbital vector 21 everywhere except for the component corresponding to those two or three jeans where we set a higher value to 10.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "Then we repeat 200 times the following.",
                    "label": 0
                },
                {
                    "sent": "We split our data into 90% train and 10% test on the training part.",
                    "label": 1
                },
                {
                    "sent": "We normalize.",
                    "label": 1
                },
                {
                    "sent": "We select our feature with partially supervised feature selection.",
                    "label": 0
                },
                {
                    "sent": "And then we build linear SVM on the selected features and then we test on the test part.",
                    "label": 0
                },
                {
                    "sent": "We evaluate the PCR we averaged in on the 200 loops.",
                    "label": 0
                },
                {
                    "sent": "And after all, we have 200 sets of selected dimension and we compute the stability between those selected feature sets.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are the results for one of our data set where we started from more than 7000 feature.",
                    "label": 0
                },
                {
                    "sent": "But here we show results only for the interesting part which are between 256 and four dimension which is which are common size for practical application and so on.",
                    "label": 0
                },
                {
                    "sent": "The left on the right you have the classification performances and on the left you have the stability.",
                    "label": 0
                },
                {
                    "sent": "And first, let's have a look at the classification performances and compare the blue curve, which is the standard L2 aroma technique and the red curve, which is the partially supervised version of that technique, and we see that when the number of feature really decrease below 16 dimensions, the traditional tuorum crashes.",
                    "label": 1
                },
                {
                    "sent": "However, our technique remains at the high level of performance.",
                    "label": 0
                },
                {
                    "sent": "And we also compared to the arefi technique in purple, and their results looks comparable to our method and we also tested the God of index.",
                    "label": 0
                },
                {
                    "sent": "If you don't know what Gottlieb index is, it's a technique for feature selection that is closely related to at Test is not exactly the same, but it's the same idea.",
                    "label": 0
                },
                {
                    "sent": "And there, whether it seems to perform a bit better and we also have another curve which is interesting.",
                    "label": 0
                },
                {
                    "sent": "It's we called it random.",
                    "label": 1
                },
                {
                    "sent": "It's partial supervision for feature selection where we favoured two genes at random.",
                    "label": 0
                },
                {
                    "sent": "And what's interesting is that the results are the nearly the same.",
                    "label": 0
                },
                {
                    "sent": "Then for the non supervised technique.",
                    "label": 0
                },
                {
                    "sent": "And that means that our technique is able to depart from probably wrong prior knowledge.",
                    "label": 1
                },
                {
                    "sent": "I know if you if we have a look at the stability.",
                    "label": 0
                },
                {
                    "sent": "There, let's begin by the comparison between L2 Roman partially supervised L2 room.",
                    "label": 0
                },
                {
                    "sent": "So the blue and the red curve again.",
                    "label": 0
                },
                {
                    "sent": "And there we see that.",
                    "label": 0
                },
                {
                    "sent": "We not only gain here on classification performances, but we mainly gain instability and there is a striking difference between L2 romanets partially supervised version.",
                    "label": 0
                },
                {
                    "sent": "And here we we also beat the arefi.",
                    "label": 0
                },
                {
                    "sent": "And even at the end goal of index.",
                    "label": 0
                },
                {
                    "sent": "And here the random curve in in Black is a bit higher than the non supervised and unsupervised L2 room which can be explained by the fact that by by construction, even if the two favorite jeans are random jeans.",
                    "label": 0
                },
                {
                    "sent": "Yet they are favored, and so the stability is a bit higher, but it is really lower than when you favor to jeans according to relevant prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "And once again, that means that our technique is able to depart from wrong knowledge, and this is very, very important result.",
                    "label": 0
                },
                {
                    "sent": "And so we had also prior knowledge for another that I set on three jeans and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's Arkham Parable and we wanted to generalize or conclusion to other datasets, but it was difficult to find in the literature.",
                    "label": 0
                },
                {
                    "sent": "Other data set for which we had prior knowledge, so we simulated it.",
                    "label": 0
                },
                {
                    "sent": "By a bit more complicated protocol where we use a part of the data set to compute feature selection and to favor those gene as if they were prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "But anyway, the conclusion are the same.",
                    "label": 0
                },
                {
                    "sent": "Here we don't see really significant difference in classification performances.",
                    "label": 1
                },
                {
                    "sent": "Between any technique?",
                    "label": 0
                },
                {
                    "sent": "But once again, the stability is really different.",
                    "label": 0
                },
                {
                    "sent": "If you have a look at the blue curve, which is the once again the L2 around technique so non supervised feature selection and or partially supervised feature selection in red there the difference is really large.",
                    "label": 1
                },
                {
                    "sent": "And so when you take both the classification performances in the stability globally, we really improved the quality of the selection.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so here are the conclusions.",
                    "label": 0
                },
                {
                    "sent": "First, we think that the stability is a metric that should be more used when assessing the quality of feature selection techniques.",
                    "label": 0
                },
                {
                    "sent": "But not alone.",
                    "label": 0
                },
                {
                    "sent": "As I said, it would be done with another classification.",
                    "label": 0
                },
                {
                    "sent": "Performance is metric.",
                    "label": 0
                },
                {
                    "sent": "Or partially supervised feature selection technique allows the incorporation of prior knowledge which is in itself an important result because it allows the interaction with the field experts.",
                    "label": 0
                },
                {
                    "sent": "And it also it is also able to depart from potentially wrong prior knowledge or technique, naturally extends other techniques in the field of feature selection.",
                    "label": 1
                },
                {
                    "sent": "In this case, it's around the arrow methods and it increases the stability of the selected features with respect to sampling variations.",
                    "label": 1
                },
                {
                    "sent": "And in some cases it also improves classification performances.",
                    "label": 1
                },
                {
                    "sent": "And since it is a multivariate method, the supervision of few dimensions influence also the selection of the other component of the final selected feature set you found.",
                    "label": 0
                },
                {
                    "sent": "And I didn't show that.",
                    "label": 0
                },
                {
                    "sent": "But it is also an interesting reason.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you have question.",
                    "label": 0
                },
                {
                    "sent": "So I have time for some questions.",
                    "label": 0
                },
                {
                    "sent": "What happens?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are correlations.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Is there a correlation between what?",
                    "label": 0
                },
                {
                    "sent": "What is the relevance?",
                    "label": 0
                },
                {
                    "sent": "We did understood it that but.",
                    "label": 0
                },
                {
                    "sent": "Well, I I don't know.",
                    "label": 0
                },
                {
                    "sent": "Google.",
                    "label": 0
                },
                {
                    "sent": "So is it?",
                    "label": 0
                },
                {
                    "sent": "When you have prior knowledge of that means something like, well, you should consider including these features that the biologists found useful, is it?",
                    "label": 0
                },
                {
                    "sent": "Is is it something that it's something that increases stability?",
                    "label": 0
                },
                {
                    "sent": "Is there some other effects that increase stability or?",
                    "label": 0
                },
                {
                    "sent": "So have you studied why disability increases?",
                    "label": 0
                },
                {
                    "sent": "The first I think that partly the stability increased because of course the gene order dimensions you favor are more likely to be finally selected, although it is not always the case.",
                    "label": 0
                },
                {
                    "sent": "You may favor some dimension that can be not selected finally, but I think.",
                    "label": 0
                },
                {
                    "sent": "I think that if you truly if your prior knowledge is really relevant, it will favor.",
                    "label": 0
                },
                {
                    "sent": "In fact the ground truth that you are looking for, and so that's another reason for why the stability is increased.",
                    "label": 0
                },
                {
                    "sent": "Do you have insight on how to select the paper size and play on the the?",
                    "label": 0
                },
                {
                    "sent": "Yeah, nothing in that paper, but in another paper we studied at the sensibility to the magnitude of the beaten, and in fact it has an influence.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you said it set it close to one, it will nearly have no influence.",
                    "label": 0
                },
                {
                    "sent": "But there is a large area where it has not a major influence.",
                    "label": 0
                },
                {
                    "sent": "So I said between 5 and 100 in or experience in our experience, it's it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you.",
                    "label": 0
                }
            ]
        }
    }
}