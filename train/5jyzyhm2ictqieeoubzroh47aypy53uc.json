{
    "id": "5jyzyhm2ictqieeoubzroh47aypy53uc",
    "title": "Towards Universal Paraphrastic Sentence Embeddings",
    "info": {
        "author": [
            "John Wieting, Toyota Technological Institute at Chicago"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_wieting_universal_paraphrastic/",
    "segmentation": [
        [
            "Hi, my name is John Whiting and today I'm going to discuss our work on creating Universal Paraphrastic sentence embeddings."
        ],
        [
            "So you know, we study the compositionality of national natural language.",
            "So what that means is we try to figure out ways to encode sequences of words into a representation that encodes the meaning of those words.",
            "This is very popular in pretty much all natural language processing tasks these days from neural machine translation.",
            "The question answering to dialoging.",
            "This diagram just a quick example of what we mean.",
            "This is a neural empty example, so the sequence of words is generated representation, and then from that representation the translation is generated, so the representation encodes the meaning of this sentence, which can be used for the generation part."
        ],
        [
            "So the space that we actually focus on in this work is semantic similarity, so this task is pretty simple, but it is still a good measure of our ability to model the composition of language.",
            "So just to be clear on what we mean here.",
            "Essentially some similar, just you know it gets high scores to senses that have similar meaning.",
            "Anne low scores to those that are on different topics or contradictory in some sense, so these examples show manual annotations on two sentence pairs.",
            "The scoring is from zero to five or zero means not related, and five means like pretty equivalent.",
            "So the blue is equivalent 'cause they actually mean the same thing essentially, but the red, despite having a lot of lexical overlap, they are contradictory.",
            "So it gives a low score.",
            "So."
        ],
        [
            "Begin to do this.",
            "How can we create these representations that are agnostic to the domain and not overfit to any particular data set?",
            "So.",
            "It's unlikely that we could, you know, develop such a representation from monolingual corpora.",
            "So getting some kind of data that can help us some kind of natural supervision is very desirable, and So what we do is we try this pivoting technique, which is actually pretty general.",
            "Essentially you take you have a target language, so in this example it's English.",
            "And you have pivoting language in this case, is German.",
            "So essentially you look for two English phrases that match the same the same phrase in your in your pivoting language, and then you can surmise that these two are paraphrases of each other.",
            "So in this example, under control and in check are.",
            "This might be paraphrases of each other."
        ],
        [
            "So there's this.",
            "Actually, this really cool resource that has done all this work for us in the sense they actually did this over many many bits of bilingual text, and it's called the paraphrase database, and it looks like he's kind of like what it looks like a little bit, so you have these.",
            "These are short, very short snippets of text.",
            "These are a little bit noisy, but they're essentially paraphrases of each other in general, but they're only like 2 to three to four words long, so we're going to try and use this resource and see if we can create a model just from this.",
            "And so you're kind of missing, like the high level sentence view, but you do get some kind of supervision, and there's 10s of millions of these.",
            "It's very large."
        ],
        [
            "So just to be clear here, we want to learn an encoder, a function that Maps a text sentence to a fixed length.",
            "Things fixed length vector we spent with encoders and I'll talk a little bit more about them when I go to the results, but they're all pretty standard."
        ],
        [
            "Um?",
            "So just a quick word about modeling.",
            "So here's our objective function.",
            "It's actually very straightforward, it's just a hingeless hinge loss based objective function with negative sampling, so so G again is our encoder X one X2 are just paraphrase pairs from PDB.",
            "And then T1 and T2 are negative examples which can be obtained in, you know, numerous ways.",
            "One way that seems to work well just taking the argmax over the mini batch, they said the nearest neighbor for the for one of the paraphrase pairs.",
            "And then we also have some regularization terms which are not included here."
        ],
        [
            "So again, we somewhere there's so we want.",
            "Essentially, we want this this, you know, the score of the actual paraphrase pairs to be greater than the margin plus that of the negative examples."
        ],
        [
            "So here's our evaluation and I think in this way we differentiate differentiate ourselves a little bit from other works.",
            "So we essentially went and found pretty much all the sentence summary tasks we could find.",
            "These come from these similar competitions that are held most years.",
            "She took it was just a huge collection from all the Sunnyvale tests we could find and there's 22 data sets in total here and they cover tons of domains from web forums, the empty output to blogs to glasses.",
            "The news and so that's you know, our evaluation set, and we also include some in domain data sets in domain.",
            "For us is just PDB, so annotated phrases and we use these for both for model selection.",
            "So we don't want to bias ourselves at all to any outside data sources."
        ],
        [
            "So just a clarification on what we mean by these in domain data sets, there's a.",
            "There's two main ones, one is from one of our prior work and one is another public at all, which is a bit larger than ours.",
            "So that's when we actually used for model selection.",
            "And again you have this one to five rating, and these are actual phrase pairs you can find in PDB.",
            "We can see that how there is some noise in here then."
        ],
        [
            "So here are some baseline, so we actually use the same email competitions as baselines.",
            "So we took the 75th percentile as a really strong baseline that we hope to beat.",
            "So these these numbers that are obtained from these competitions, they have training data and they have these lots of resources and we don't touch any of that stuff.",
            "So these are just didn't know if you could get these.",
            "Well, we thought we would try so.",
            "We started to get some just an initial baseline.",
            "So like word averaging actually tends to work pretty well of fixed embeddings from other sources, so we tried glove embeddings.",
            "We tried our own Paragon buildings will better, which we include here.",
            "We also tried to use Skip thought vectors, but that also wasn't quite as good, so so our best baseline was just this.",
            "This word averaging baseline and then we thought, well, let's just train an LCM encoder.",
            "And that surprising didn't do so great.",
            "So we tried two different variations of it, one without output gate, one with output gate without the advocate actually works a little better, but still only like slightly better than the fixed word averaging.",
            "So then we just thought, well, why not just tune the update they word embeddings themselves, but that's it.",
            "Just keep it really simple and just for clarification, the LCM here the embeddings were updated during training as well.",
            "And that actually worked out surprisingly well, so that actually beats pretty much everything and is around equal to the 75th percentile."
        ],
        [
            "So you know with that discovery, then we tried a few more in quarters, so we tried adding a little bit more complexity by adding a linear layer after averaging or everything, or even having a deep averaging network.",
            "We have deep layers with nonlinear activations after the averaging, and those were pretty close to the word.",
            "Every model or a little bit better.",
            "We also have recurrent network.",
            "Just for sanity sake, which is where something else TM.",
            "And then we tried this special identity RNN were.",
            "Essentially we basically word averaging an initial state, 'cause we initialized the weight matrices dignity and then zero of the biases.",
            "And then we.",
            "We heavily regularize back to this initial state, so it's essentially doing averaging with some kind of, but hopefully we can learn some some extra composition and it actually works a little bit better."
        ],
        [
            "So these those numbers that we got were actually we only tune down a little bit of PDB and then took cartoon model to about 3 million phrase pairs.",
            "And if we scale everything up and tune in like 2 million phrase pairs and train on on well, 9 million performance goes up even more considerably showing that this model can really scale as more data is used, which isn't too surprising, but it's actually a really significant jump here."
        ],
        [
            "So then we had to reflect a little bit on like why the LCM didn't perform so well and there are a few like obvious things to check out here.",
            "So one was was it only working on short sentences?",
            "Could it work on?",
            "Was it just kind of learning just how to do very short things and like kind of like overfitting to the domain a little bit, which is actually a danger here because our model selection data is very similar is basically from the same distribution as our training data and we're asking it to go further than that.",
            "Investigated these things as well as insufficient."
        ],
        [
            "Camera tuning so length experiment.",
            "We just we chopped up our test data into little bins based on like the Max number of tokens in the in the sentence pairs.",
            "And we just compare here the STM in the word averaging and it seems pretty well seem pretty robust to length.",
            "Doesn't seem to be a huge determining factor."
        ],
        [
            "Overfitting in domain data is similar, where actually all the models basically do pretty much the same.",
            "They all seems a little bit better, but it's not enough to really be too concerned about it.",
            "You know, memorizing the training data?"
        ],
        [
            "And then parameter tuning.",
            "So we we really kind of pushed our resources to the limit to try and see if we were like in some horrible hyperparameter space or there's some optimization optimization issues.",
            "So we tune."
        ],
        [
            "Pretty much all these things, and the story is still was the same and nothing really changed.",
            "So."
        ],
        [
            "So that's that.",
            "But do we have any other use cases of this?",
            "Is it was just the sentence similarity model.",
            "So we tried using it as a kind of a prior for more conventional supervised tasks.",
            "Where we initialize, you know, using our pre trained model and then we kind of try to regularize back towards it so we penalized deviations from our initial model.",
            "We also tried his features like the Skip thought paper, which is a very cool paper and we found that actually our model is pretty similar to that in terms of performance.",
            "As long as you project the embeddings to a higher dimensional space before averaging them.",
            "So if you both in script using 2400 dimensions that we projected 24 mentions, an average performance is very compareable to them.",
            "And let's discuss more in the paper."
        ],
        [
            "So these are those initialization regularization experiments so.",
            "The green is like the state of the art at the time, and the blue was our normal training and we actually validated overall encoders here an actual word.",
            "Everything was still the best in this domain for this similarity in attainment.",
            "And by the way, in 10 minutes, just similarity with the classification output.",
            "So basically instead of scoring regression kind of test is actually does it.",
            "You know it's essentially equivalent or not equivalent.",
            "So then using these using our PDB trained models is prior performance improves considerably in both cases."
        ],
        [
            "Now we also tried this for sentiment, so the binary, the Stanford bent sentiment task.",
            "Here our normal model actually accidentally beat the state of the art by a lot, so LCMS are actually really good at sentiment, but using the prior was a little bit of a failure.",
            "It, which isn't too surprising because the sentence models we got from the LCM weren't so good.",
            "So using that prior is.",
            "Probably not the greatest idea, and it didn't work too well."
        ],
        [
            "So we could briefly some qualitative quantitative analysis.",
            "So we found a significant part of the word.",
            "Every model Ann is also holds true for the other models where the embeddings are updated during training.",
            "Was that the L2 norms of the word embeddings were related ascentia Lee giving more importance to very discriminative words such as like numbers like 18 and then much less performances upwards.",
            "So normalizing these vectors is a terrible idea because it will just destroy performance as these.",
            "As L2 norms are actually very important part of the process here.",
            "We also could learn a few things from the nearest neighbors, so these paragraphs, embeddings on the right were from prior work and they're actually like extremely strong performance on just word similarity paraphrasing tasks.",
            "So we actually get a little bit of a different kind of nearest neighbors here in some improvement over the paragraph phrase for the first 2 rows, the third row is actually very interesting because it shows that while their friends are able to kind of handles antonym or synonym issue for disagree.",
            "Embeddings these program phrase ones trained on just the word averaging thing.",
            "I've been talking about.",
            "She reverts back to the antonym of the word.",
            "And why this is essentially simply because of all the there's some knots, so the training data include things like, not disagree, and then it'll be agree in the same fragment of text.",
            "So then it starts to like push those together, which is very undesirable.",
            "And that's because essentially using a bigger word model here we're not using context at all.",
            "And the other thing was not surprisingly, were very limited in the vocabulary of the work of our urban model, because the only word embeddings that are being updated are those that we've seen in the training data, which is only like 30,000 or so.",
            "So the correlation between performance in our vocabulary rate is about negative 45%.",
            "And so these two are huge sources of error for us, just from a quick, you know, quick analysis of what's going on."
        ],
        [
            "So that kind of this fun new little like divergent thing into characters, so characters are actually kind of fun.",
            "Models play around and this is when we tried that.",
            "It worked really well.",
            "It's essentially it's motivated from Microsoft.",
            "Research is deep, structured semantic model and essentially like given like a word or phrase like.",
            "I Clr you chop it up into little pieces.",
            "Bigrams, trigrams, quad grams of characters, and then you create the feature count vector.",
            "Essentially, counting the number of each grams that appear in your phrase or sentence, and then we just use a single layer to project it into a into a smaller representation.",
            "And as a side effect of using a single layer, you get some nice character embeddings.",
            "Each column of the matrix can be ngram character ngram embedding, which has its own fund properties."
        ],
        [
            "So you just hear something like fun examples that show that it kind of handles the issues that we just talked about it, so it's able to model context.",
            "Kind of, surprisingly so we did.",
            "Here is we took not bigrams and then we just found nearest neighbors after encoding all the words in our vocabulary.",
            "So the program phase model fails with this because all the neighbors nearest neighbors are just not.",
            "That's all that focuses on.",
            "Whereas the other one it's actually able to learn that things like not capable.",
            "Nearest neighbor incapable or not possible is the nearest neighbor of impossible, which is kind of cool and similar here.",
            "For the vocabulary words, so it's able to capture the slang such as like the baby with you know, six wise after it, or misspelling a vehicle, mostly come from Twitter, but so.",
            "And you might think, well, it's just characters, so it's just going to like look for character overlap, which isn't really true.",
            "Like as this huge example shows, so the nearest neighbors huge are enormous and tremendous, so there's very little character overlap between huge and those words.",
            "So it's able to kind of transcend beyond just.",
            "You know simple character level matching here."
        ],
        [
            "Just a quick experiment just to show that this model is actually very superior to the word average model.",
            "Not surprisingly so.",
            "This is kind of a cool direction to go into using characters, and this is pretty hot right now.",
            "I think in the community.",
            "So."
        ],
        [
            "Inclusion we have shown how essentially using just you know, bilingual text, it is possible to create a strong model of composition that is agnostic to domain to the data set, which we think is very important direction of work.",
            "Secondly, I think we raised a few questions about Ellis teams.",
            "You know why do they not work in this scenario?",
            "Is a little bit unclear, but I think you know answering this question and working on you know this kind of direction of research could lead to much greater models of composition.",
            "Hopefully in the coming years.",
            "And then Lastly, I just want the release everything to replicate experiments, the code, the resources.",
            "The train models and they're available on my website so you guys can check it out if you're interested.",
            "That's all."
        ],
        [
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, my name is John Whiting and today I'm going to discuss our work on creating Universal Paraphrastic sentence embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know, we study the compositionality of national natural language.",
                    "label": 1
                },
                {
                    "sent": "So what that means is we try to figure out ways to encode sequences of words into a representation that encodes the meaning of those words.",
                    "label": 1
                },
                {
                    "sent": "This is very popular in pretty much all natural language processing tasks these days from neural machine translation.",
                    "label": 0
                },
                {
                    "sent": "The question answering to dialoging.",
                    "label": 0
                },
                {
                    "sent": "This diagram just a quick example of what we mean.",
                    "label": 0
                },
                {
                    "sent": "This is a neural empty example, so the sequence of words is generated representation, and then from that representation the translation is generated, so the representation encodes the meaning of this sentence, which can be used for the generation part.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the space that we actually focus on in this work is semantic similarity, so this task is pretty simple, but it is still a good measure of our ability to model the composition of language.",
                    "label": 0
                },
                {
                    "sent": "So just to be clear on what we mean here.",
                    "label": 0
                },
                {
                    "sent": "Essentially some similar, just you know it gets high scores to senses that have similar meaning.",
                    "label": 0
                },
                {
                    "sent": "Anne low scores to those that are on different topics or contradictory in some sense, so these examples show manual annotations on two sentence pairs.",
                    "label": 0
                },
                {
                    "sent": "The scoring is from zero to five or zero means not related, and five means like pretty equivalent.",
                    "label": 0
                },
                {
                    "sent": "So the blue is equivalent 'cause they actually mean the same thing essentially, but the red, despite having a lot of lexical overlap, they are contradictory.",
                    "label": 0
                },
                {
                    "sent": "So it gives a low score.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Begin to do this.",
                    "label": 0
                },
                {
                    "sent": "How can we create these representations that are agnostic to the domain and not overfit to any particular data set?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's unlikely that we could, you know, develop such a representation from monolingual corpora.",
                    "label": 0
                },
                {
                    "sent": "So getting some kind of data that can help us some kind of natural supervision is very desirable, and So what we do is we try this pivoting technique, which is actually pretty general.",
                    "label": 0
                },
                {
                    "sent": "Essentially you take you have a target language, so in this example it's English.",
                    "label": 0
                },
                {
                    "sent": "And you have pivoting language in this case, is German.",
                    "label": 0
                },
                {
                    "sent": "So essentially you look for two English phrases that match the same the same phrase in your in your pivoting language, and then you can surmise that these two are paraphrases of each other.",
                    "label": 0
                },
                {
                    "sent": "So in this example, under control and in check are.",
                    "label": 0
                },
                {
                    "sent": "This might be paraphrases of each other.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's this.",
                    "label": 0
                },
                {
                    "sent": "Actually, this really cool resource that has done all this work for us in the sense they actually did this over many many bits of bilingual text, and it's called the paraphrase database, and it looks like he's kind of like what it looks like a little bit, so you have these.",
                    "label": 0
                },
                {
                    "sent": "These are short, very short snippets of text.",
                    "label": 0
                },
                {
                    "sent": "These are a little bit noisy, but they're essentially paraphrases of each other in general, but they're only like 2 to three to four words long, so we're going to try and use this resource and see if we can create a model just from this.",
                    "label": 0
                },
                {
                    "sent": "And so you're kind of missing, like the high level sentence view, but you do get some kind of supervision, and there's 10s of millions of these.",
                    "label": 0
                },
                {
                    "sent": "It's very large.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to be clear here, we want to learn an encoder, a function that Maps a text sentence to a fixed length.",
                    "label": 0
                },
                {
                    "sent": "Things fixed length vector we spent with encoders and I'll talk a little bit more about them when I go to the results, but they're all pretty standard.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So just a quick word about modeling.",
                    "label": 0
                },
                {
                    "sent": "So here's our objective function.",
                    "label": 0
                },
                {
                    "sent": "It's actually very straightforward, it's just a hingeless hinge loss based objective function with negative sampling, so so G again is our encoder X one X2 are just paraphrase pairs from PDB.",
                    "label": 0
                },
                {
                    "sent": "And then T1 and T2 are negative examples which can be obtained in, you know, numerous ways.",
                    "label": 0
                },
                {
                    "sent": "One way that seems to work well just taking the argmax over the mini batch, they said the nearest neighbor for the for one of the paraphrase pairs.",
                    "label": 0
                },
                {
                    "sent": "And then we also have some regularization terms which are not included here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, we somewhere there's so we want.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we want this this, you know, the score of the actual paraphrase pairs to be greater than the margin plus that of the negative examples.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's our evaluation and I think in this way we differentiate differentiate ourselves a little bit from other works.",
                    "label": 0
                },
                {
                    "sent": "So we essentially went and found pretty much all the sentence summary tasks we could find.",
                    "label": 0
                },
                {
                    "sent": "These come from these similar competitions that are held most years.",
                    "label": 0
                },
                {
                    "sent": "She took it was just a huge collection from all the Sunnyvale tests we could find and there's 22 data sets in total here and they cover tons of domains from web forums, the empty output to blogs to glasses.",
                    "label": 0
                },
                {
                    "sent": "The news and so that's you know, our evaluation set, and we also include some in domain data sets in domain.",
                    "label": 0
                },
                {
                    "sent": "For us is just PDB, so annotated phrases and we use these for both for model selection.",
                    "label": 1
                },
                {
                    "sent": "So we don't want to bias ourselves at all to any outside data sources.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just a clarification on what we mean by these in domain data sets, there's a.",
                    "label": 0
                },
                {
                    "sent": "There's two main ones, one is from one of our prior work and one is another public at all, which is a bit larger than ours.",
                    "label": 0
                },
                {
                    "sent": "So that's when we actually used for model selection.",
                    "label": 0
                },
                {
                    "sent": "And again you have this one to five rating, and these are actual phrase pairs you can find in PDB.",
                    "label": 0
                },
                {
                    "sent": "We can see that how there is some noise in here then.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are some baseline, so we actually use the same email competitions as baselines.",
                    "label": 0
                },
                {
                    "sent": "So we took the 75th percentile as a really strong baseline that we hope to beat.",
                    "label": 0
                },
                {
                    "sent": "So these these numbers that are obtained from these competitions, they have training data and they have these lots of resources and we don't touch any of that stuff.",
                    "label": 0
                },
                {
                    "sent": "So these are just didn't know if you could get these.",
                    "label": 0
                },
                {
                    "sent": "Well, we thought we would try so.",
                    "label": 0
                },
                {
                    "sent": "We started to get some just an initial baseline.",
                    "label": 0
                },
                {
                    "sent": "So like word averaging actually tends to work pretty well of fixed embeddings from other sources, so we tried glove embeddings.",
                    "label": 0
                },
                {
                    "sent": "We tried our own Paragon buildings will better, which we include here.",
                    "label": 0
                },
                {
                    "sent": "We also tried to use Skip thought vectors, but that also wasn't quite as good, so so our best baseline was just this.",
                    "label": 0
                },
                {
                    "sent": "This word averaging baseline and then we thought, well, let's just train an LCM encoder.",
                    "label": 0
                },
                {
                    "sent": "And that surprising didn't do so great.",
                    "label": 0
                },
                {
                    "sent": "So we tried two different variations of it, one without output gate, one with output gate without the advocate actually works a little better, but still only like slightly better than the fixed word averaging.",
                    "label": 0
                },
                {
                    "sent": "So then we just thought, well, why not just tune the update they word embeddings themselves, but that's it.",
                    "label": 0
                },
                {
                    "sent": "Just keep it really simple and just for clarification, the LCM here the embeddings were updated during training as well.",
                    "label": 0
                },
                {
                    "sent": "And that actually worked out surprisingly well, so that actually beats pretty much everything and is around equal to the 75th percentile.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know with that discovery, then we tried a few more in quarters, so we tried adding a little bit more complexity by adding a linear layer after averaging or everything, or even having a deep averaging network.",
                    "label": 0
                },
                {
                    "sent": "We have deep layers with nonlinear activations after the averaging, and those were pretty close to the word.",
                    "label": 0
                },
                {
                    "sent": "Every model or a little bit better.",
                    "label": 0
                },
                {
                    "sent": "We also have recurrent network.",
                    "label": 0
                },
                {
                    "sent": "Just for sanity sake, which is where something else TM.",
                    "label": 0
                },
                {
                    "sent": "And then we tried this special identity RNN were.",
                    "label": 0
                },
                {
                    "sent": "Essentially we basically word averaging an initial state, 'cause we initialized the weight matrices dignity and then zero of the biases.",
                    "label": 0
                },
                {
                    "sent": "And then we.",
                    "label": 0
                },
                {
                    "sent": "We heavily regularize back to this initial state, so it's essentially doing averaging with some kind of, but hopefully we can learn some some extra composition and it actually works a little bit better.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these those numbers that we got were actually we only tune down a little bit of PDB and then took cartoon model to about 3 million phrase pairs.",
                    "label": 0
                },
                {
                    "sent": "And if we scale everything up and tune in like 2 million phrase pairs and train on on well, 9 million performance goes up even more considerably showing that this model can really scale as more data is used, which isn't too surprising, but it's actually a really significant jump here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we had to reflect a little bit on like why the LCM didn't perform so well and there are a few like obvious things to check out here.",
                    "label": 0
                },
                {
                    "sent": "So one was was it only working on short sentences?",
                    "label": 1
                },
                {
                    "sent": "Could it work on?",
                    "label": 0
                },
                {
                    "sent": "Was it just kind of learning just how to do very short things and like kind of like overfitting to the domain a little bit, which is actually a danger here because our model selection data is very similar is basically from the same distribution as our training data and we're asking it to go further than that.",
                    "label": 0
                },
                {
                    "sent": "Investigated these things as well as insufficient.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Camera tuning so length experiment.",
                    "label": 0
                },
                {
                    "sent": "We just we chopped up our test data into little bins based on like the Max number of tokens in the in the sentence pairs.",
                    "label": 0
                },
                {
                    "sent": "And we just compare here the STM in the word averaging and it seems pretty well seem pretty robust to length.",
                    "label": 0
                },
                {
                    "sent": "Doesn't seem to be a huge determining factor.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overfitting in domain data is similar, where actually all the models basically do pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "They all seems a little bit better, but it's not enough to really be too concerned about it.",
                    "label": 0
                },
                {
                    "sent": "You know, memorizing the training data?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then parameter tuning.",
                    "label": 0
                },
                {
                    "sent": "So we we really kind of pushed our resources to the limit to try and see if we were like in some horrible hyperparameter space or there's some optimization optimization issues.",
                    "label": 1
                },
                {
                    "sent": "So we tune.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty much all these things, and the story is still was the same and nothing really changed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's that.",
                    "label": 0
                },
                {
                    "sent": "But do we have any other use cases of this?",
                    "label": 1
                },
                {
                    "sent": "Is it was just the sentence similarity model.",
                    "label": 0
                },
                {
                    "sent": "So we tried using it as a kind of a prior for more conventional supervised tasks.",
                    "label": 0
                },
                {
                    "sent": "Where we initialize, you know, using our pre trained model and then we kind of try to regularize back towards it so we penalized deviations from our initial model.",
                    "label": 0
                },
                {
                    "sent": "We also tried his features like the Skip thought paper, which is a very cool paper and we found that actually our model is pretty similar to that in terms of performance.",
                    "label": 0
                },
                {
                    "sent": "As long as you project the embeddings to a higher dimensional space before averaging them.",
                    "label": 0
                },
                {
                    "sent": "So if you both in script using 2400 dimensions that we projected 24 mentions, an average performance is very compareable to them.",
                    "label": 0
                },
                {
                    "sent": "And let's discuss more in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are those initialization regularization experiments so.",
                    "label": 0
                },
                {
                    "sent": "The green is like the state of the art at the time, and the blue was our normal training and we actually validated overall encoders here an actual word.",
                    "label": 0
                },
                {
                    "sent": "Everything was still the best in this domain for this similarity in attainment.",
                    "label": 0
                },
                {
                    "sent": "And by the way, in 10 minutes, just similarity with the classification output.",
                    "label": 0
                },
                {
                    "sent": "So basically instead of scoring regression kind of test is actually does it.",
                    "label": 0
                },
                {
                    "sent": "You know it's essentially equivalent or not equivalent.",
                    "label": 0
                },
                {
                    "sent": "So then using these using our PDB trained models is prior performance improves considerably in both cases.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we also tried this for sentiment, so the binary, the Stanford bent sentiment task.",
                    "label": 0
                },
                {
                    "sent": "Here our normal model actually accidentally beat the state of the art by a lot, so LCMS are actually really good at sentiment, but using the prior was a little bit of a failure.",
                    "label": 0
                },
                {
                    "sent": "It, which isn't too surprising because the sentence models we got from the LCM weren't so good.",
                    "label": 1
                },
                {
                    "sent": "So using that prior is.",
                    "label": 0
                },
                {
                    "sent": "Probably not the greatest idea, and it didn't work too well.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we could briefly some qualitative quantitative analysis.",
                    "label": 0
                },
                {
                    "sent": "So we found a significant part of the word.",
                    "label": 1
                },
                {
                    "sent": "Every model Ann is also holds true for the other models where the embeddings are updated during training.",
                    "label": 0
                },
                {
                    "sent": "Was that the L2 norms of the word embeddings were related ascentia Lee giving more importance to very discriminative words such as like numbers like 18 and then much less performances upwards.",
                    "label": 0
                },
                {
                    "sent": "So normalizing these vectors is a terrible idea because it will just destroy performance as these.",
                    "label": 0
                },
                {
                    "sent": "As L2 norms are actually very important part of the process here.",
                    "label": 0
                },
                {
                    "sent": "We also could learn a few things from the nearest neighbors, so these paragraphs, embeddings on the right were from prior work and they're actually like extremely strong performance on just word similarity paraphrasing tasks.",
                    "label": 0
                },
                {
                    "sent": "So we actually get a little bit of a different kind of nearest neighbors here in some improvement over the paragraph phrase for the first 2 rows, the third row is actually very interesting because it shows that while their friends are able to kind of handles antonym or synonym issue for disagree.",
                    "label": 0
                },
                {
                    "sent": "Embeddings these program phrase ones trained on just the word averaging thing.",
                    "label": 0
                },
                {
                    "sent": "I've been talking about.",
                    "label": 0
                },
                {
                    "sent": "She reverts back to the antonym of the word.",
                    "label": 0
                },
                {
                    "sent": "And why this is essentially simply because of all the there's some knots, so the training data include things like, not disagree, and then it'll be agree in the same fragment of text.",
                    "label": 0
                },
                {
                    "sent": "So then it starts to like push those together, which is very undesirable.",
                    "label": 0
                },
                {
                    "sent": "And that's because essentially using a bigger word model here we're not using context at all.",
                    "label": 0
                },
                {
                    "sent": "And the other thing was not surprisingly, were very limited in the vocabulary of the work of our urban model, because the only word embeddings that are being updated are those that we've seen in the training data, which is only like 30,000 or so.",
                    "label": 1
                },
                {
                    "sent": "So the correlation between performance in our vocabulary rate is about negative 45%.",
                    "label": 0
                },
                {
                    "sent": "And so these two are huge sources of error for us, just from a quick, you know, quick analysis of what's going on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that kind of this fun new little like divergent thing into characters, so characters are actually kind of fun.",
                    "label": 0
                },
                {
                    "sent": "Models play around and this is when we tried that.",
                    "label": 0
                },
                {
                    "sent": "It worked really well.",
                    "label": 0
                },
                {
                    "sent": "It's essentially it's motivated from Microsoft.",
                    "label": 0
                },
                {
                    "sent": "Research is deep, structured semantic model and essentially like given like a word or phrase like.",
                    "label": 1
                },
                {
                    "sent": "I Clr you chop it up into little pieces.",
                    "label": 0
                },
                {
                    "sent": "Bigrams, trigrams, quad grams of characters, and then you create the feature count vector.",
                    "label": 0
                },
                {
                    "sent": "Essentially, counting the number of each grams that appear in your phrase or sentence, and then we just use a single layer to project it into a into a smaller representation.",
                    "label": 0
                },
                {
                    "sent": "And as a side effect of using a single layer, you get some nice character embeddings.",
                    "label": 0
                },
                {
                    "sent": "Each column of the matrix can be ngram character ngram embedding, which has its own fund properties.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you just hear something like fun examples that show that it kind of handles the issues that we just talked about it, so it's able to model context.",
                    "label": 1
                },
                {
                    "sent": "Kind of, surprisingly so we did.",
                    "label": 0
                },
                {
                    "sent": "Here is we took not bigrams and then we just found nearest neighbors after encoding all the words in our vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So the program phase model fails with this because all the neighbors nearest neighbors are just not.",
                    "label": 0
                },
                {
                    "sent": "That's all that focuses on.",
                    "label": 0
                },
                {
                    "sent": "Whereas the other one it's actually able to learn that things like not capable.",
                    "label": 1
                },
                {
                    "sent": "Nearest neighbor incapable or not possible is the nearest neighbor of impossible, which is kind of cool and similar here.",
                    "label": 0
                },
                {
                    "sent": "For the vocabulary words, so it's able to capture the slang such as like the baby with you know, six wise after it, or misspelling a vehicle, mostly come from Twitter, but so.",
                    "label": 0
                },
                {
                    "sent": "And you might think, well, it's just characters, so it's just going to like look for character overlap, which isn't really true.",
                    "label": 0
                },
                {
                    "sent": "Like as this huge example shows, so the nearest neighbors huge are enormous and tremendous, so there's very little character overlap between huge and those words.",
                    "label": 0
                },
                {
                    "sent": "So it's able to kind of transcend beyond just.",
                    "label": 0
                },
                {
                    "sent": "You know simple character level matching here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a quick experiment just to show that this model is actually very superior to the word average model.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly so.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a cool direction to go into using characters, and this is pretty hot right now.",
                    "label": 0
                },
                {
                    "sent": "I think in the community.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inclusion we have shown how essentially using just you know, bilingual text, it is possible to create a strong model of composition that is agnostic to domain to the data set, which we think is very important direction of work.",
                    "label": 1
                },
                {
                    "sent": "Secondly, I think we raised a few questions about Ellis teams.",
                    "label": 1
                },
                {
                    "sent": "You know why do they not work in this scenario?",
                    "label": 0
                },
                {
                    "sent": "Is a little bit unclear, but I think you know answering this question and working on you know this kind of direction of research could lead to much greater models of composition.",
                    "label": 0
                },
                {
                    "sent": "Hopefully in the coming years.",
                    "label": 0
                },
                {
                    "sent": "And then Lastly, I just want the release everything to replicate experiments, the code, the resources.",
                    "label": 0
                },
                {
                    "sent": "The train models and they're available on my website so you guys can check it out if you're interested.",
                    "label": 0
                },
                {
                    "sent": "That's all.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}