{
    "id": "nodocysugqodlr762c6pvyqwq2fo7rzw",
    "title": "Words in puddles of sound",
    "info": {
        "author": [
            "Padraic Monaghan, Department of Psychology, University of York"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Speech Analysis"
        ]
    },
    "url": "http://videolectures.net/mlcs07_monaghan_wip/",
    "segmentation": [
        [
            "So this this title is intended to convey is a topic of speech segmentation.",
            "So so how children who are exposed to lots of continuous noisy speech in the environment can learn to individuate that into into words."
        ],
        [
            "Now the reason why I chose this title puddles of sound, because this is the standard orthodoxy that this this language environment is extremely noisy and continuous.",
            "It's like a sea of sounding Jenny Saffran's words.",
            "And so John have to face with this task of discovering words from this continuous speech.",
            "So there is no physical boundaries towards in speech.",
            "The child has to learn where those boundaries should come within the speech and worse, and there being no boundaries at all, there are certain pauses that occur in speech and they are not always in the right place.",
            "OK, so if you depending on on these breaks in the speech, then that may be extremely misleading.",
            "Some old studies about this.",
            "Now another problem in the speech is extremely noisy, so there's a great deal of variability in the pronunciations.",
            "OK, so the same word may be realized rather differently from one utterance to another, one speaker to another, so that's the idea.",
            "It's an extremely difficult task facing children figure out figuring out where words are."
        ],
        [
            "So this talk is really about what sort of cues are available within the language environment for the child.",
            "To begin to individuate words.",
            "Now this is not a terribly clever computational treatment of this, so I'd like you to treat this as just a starting point for the potential cues that might be there, and then how we can develop some more sophisticated models to realize those.",
            "So it's really looking at potential constraints, potential information available in the language itself for this task to proceed.",
            "So because then there's all this problem about variability of pronunciations and this this unreliability of pauses within the speech, a lot of the work recently is being focused on sub lexical cues to word boundaries.",
            "So so there are various prosodic sources of information in speech which help to define where words may begin and end.",
            "So one example is that.",
            "The final syllables of words tend to be longer than medial or initial syllables within words, and so there was a study on hamster versus ham.",
            "And if you look at the properties of those syllables, then the ham, and when it's monosyllabic, is much, much longer than the ham in hamster.",
            "So the potential cues there about where words may begin and end from this sub lexical information in terms of duration of utterance.",
            "And there are other prosodic cues as well.",
            "So in English the first syllable words tend to be stressed about 60% of the time OK. And this has been shown in various artificial language learning studies to be quite a powerful cue to indicate where words may begin and end as well.",
            "And there's also some work on particular diphones particular pairs of phonemes which are more likely to occur within a word than across word boundaries.",
            "And Morten Christiansen is going to say a bit more about that source of information tomorrow."
        ],
        [
            "So here's an an influential model of the sorts of information that may be combined in order to segment words.",
            "This is friend Mathis model, A hierarchical model of the different sources of information that enabled enable us to divide continuous speech into its word constituents.",
            "So at the bottom we have this word stress.",
            "The prosodic information.",
            "Then we have some sort of fun, a tactic acoustic, phonetic, and allophonic variation information.",
            "And then above that we have lexical knowledge in sentential context and so spent Mathis idea is that if you don't have information at one of these tears, then you just have to concentrate on information at another level, and in particular for the child beginning to learn their language.",
            "They are not given the lexicon, So what they have to do is rely on some segmental information, some subsegmental information and some potential regularity's in the prosodic structure."
        ],
        [
            "OK, so that's the that's the orthodoxy.",
            "So here's a little example of child directed speech at the top.",
            "So, so who's lovely baby?",
            "Yes you are.",
            "You're a lovely baby, aren't you?",
            "Yes, you are.",
            "This comes from the child is corpus and there are no.",
            "There are no gaps, no pauses, physical pauses between those words the child has to learn where words begin, and from that sort of thing.",
            "This is rather puzzling task.",
            "So put it in context for the child.",
            "Now the orthodoxy is you have this sea of sound, but if you actually look at.",
            "At the length of utterances from child directed speech, what you find is that a large proportion of them are just single word utterances.",
            "So from about 5 1/2 million words from the English child is corpus.",
            "That's the latest count we made of it.",
            "About quarter of them are just monosyllabic mono.",
            "Just single word utterances.",
            "That's a quarter of what the child hears.",
            "A single word utterance.",
            "So.",
            "So presumably.",
            "That source of information may be potentially useful to the child that, so that's where we're going with this.",
            "So these are the puddles of sound that I refer to in the title.",
            "OK, so it's not so much a see 'cause 1/4 of the time.",
            "What you hear is a single word individu atede, either by turn taking in a conversation or by pauses before and after it.",
            "So the question really is whether a computational model which just relies on that information could accurately."
        ],
        [
            "Divide up speech overtime so that was the starting point here.",
            "So can we go from from this noisy environment just to the lexical level?",
            "And once you've got the words the adults have the words, the task is just easy.",
            "And there's some very nice models by Dennis Norris, his latest version of shortlist, which is a Bayesian approach.",
            "And I think there's a talk tomorrow about Bayesian approaches to segmentation as well, so I wish that talk was before mine, but these are some of the ways once you have a lexical approach, you can produce some very nice sophisticated models to really perform well on segmentation.",
            "So that's the ultimate aim.",
            "But how do we generate the lexicon in the 1st place?",
            "That's a key question here.",
            "So the model I'm going to present now is that we're going to assume that each utterance is a word in the language until we know differently.",
            "OK. And one source of information for knowing whether it's a word on its own or not is whether it's repeated or not OK, so if it is a word on its own, if it's a single word utterance, you're more likely to hear it repeated as another single word utterance elsewhere, or if we have a very long utterance made up of several different words, then the chances are that being repeated much lower, so the idea is, if we repeat of an utterance, we keep it, we increase its activation, we maintain it within our lexicon.",
            "And if we're not hearing repeats of an utterance and it sort of slowly drops out of the lexicon as a candidate, that's the idea."
        ],
        [
            "So things are modeling to look at whether these utterances can give us enough information to really get started in producing the lexicon.",
            "Brinson Cartwright in their paper on segmentation, which is an MGL approach in 1996, question the use of utterances because you can't tell whether or not it is a single word or a multi word.",
            "How do you make that distinction?",
            "So I'm going to try to make the case now showing you the results of this model, that the utterance boundaries alone is sufficient information to get started in producing the lexicon.",
            "And once we've isolated, identified some of those single word utterances, they're going to be extremely useful to act as anchors or divide us within the rest of the speech.",
            "OK. And I'm going to show that it is possible to distinguish most of the single word utterances from most of the multiple word utterances in child directed speech.",
            "And I'm also going to hopefully show that proper nouns have a special role in this in this regard.",
            "I'll show you some data on that in just a moment.",
            "OK, but there's also another attractive property of this sort of approach is that we're not making any decisions in advance about what is a lexical item.",
            "That's just going to emerge from the model itself, and there's some work by Michael Tomasello on multiple word sequences that occur frequently together, maybe lexicalized as well, and our model has potential power to reflect that too."
        ],
        [
            "So the special role of proper nouns.",
            "There's a paper by Bartfeld and colleagues, and this was an experiment done on an infant called Maggie.",
            "She was pretty young.",
            "And Maggie was familiarized with lots of sentences which contained her name.",
            "So Maggie's bike had big black wheels and so on and so on.",
            "And she was familiarized on these words.",
            "And bike always occurred after Maggie's.",
            "The idea is does this very familiar word to Maggie help her to lexicalized the word that follows it?",
            "In this case bike.",
            "And that was contrasted to use of another proper noun that Maggie hadn't heard before with Hannah and Cup always followed the word Hannah.",
            "And when Maggie was familiarized to these, she became familiarized to the bike much more quickly than to Cup.",
            "So it's a suggestion here that that the word bike was entered into her lexicon more easily, became more familiar sooner than a word that followed an unfamiliar word to.",
            "So there's a special case of proper nouns, and if we look at single word utterance occurrence within child directed speech, what you see is about 3.3% of the utterances in one of the corporate.",
            "I'm going to show you in a moment was just a single use of of this child, Naomi.",
            "In this case, just single use of her words as a single word utterance.",
            "And equally proper nouns occur on their own quite a lot.",
            "They're very frequent for the child, especially their own name, but also other very high frequent words are going to be useful for categorizing the content words and what I'm going to show is that these same words which are useful for generating as markers of grammatical categories seem to be the words that pop out.",
            "First of all, as anchors for segmenting the speech as well.",
            "So I think there's some nice interplay between higher levels of linguistic structure.",
            "Which can be observed and may be extremely useful for the very early stages of language acquisition in terms."
        ],
        [
            "Segmenting words.",
            "So here's a corporate used for the model, so these are 6 corporate of children who are less than 2 1/2 years old.",
            "OK, and then the different properties of them.",
            "So in center range of about 10 to 30,000 utterances in each language and the mean length of utterance is about 3 or 4 words for each of these languages when they come from various sources.",
            "Now the child is has an orthographic transcription, so to try and mimic the input to the child we created Fanta logical transcription just by running the orthography through Festival speech synthesizer synthesizer."
        ],
        [
            "So here's what the model does in outline.",
            "So we have a lexicon of candidates, list of candidate words here.",
            "And over on the left is the actual corpora separated by utterance that the child observes.",
            "So Kitty is the first utterance that's right, Kitty is an ex utterance, and so on."
        ],
        [
            "So we advance through the corpus utterance by utterance."
        ],
        [
            "And we're looking for matches from the lexicon within the corpus.",
            "When we start off, and there are no matches at all, so we're going to enter this single word utterance, sufferance as a candidate lexical item, and we associate it with some sort of activation.",
            "And you can.",
            "Plug in your own more sophisticated model.",
            "At that point I think."
        ],
        [
            "So let me go to the next utterance and what we're going to do now is search for single word utterances that occur within the speech.",
            "Within the next utterance."
        ],
        [
            "And in this case we do get a match here for an item that's already in our lexicon."
        ],
        [
            "So what we're going to do there is increase the activation of our lexical item Kitty.",
            "And we're also going to strip off what leads up to the word that's already in our lexicon.",
            "In this case, that's right.",
            "So now we have two candidates."
        ],
        [
            "For lexical items.",
            "And we've advanced through looking for matches.",
            "There's another match."
        ],
        [
            "We increase the activation of Kitty."
        ],
        [
            "This is what happens when we go through this mini corpus, so here are candidate lexical items with their respective activations.",
            "Now model like this won't work.",
            "We need some additional cost."
        ],
        [
            "Trains.",
            "Now the reason why it won't work is if we have a corpus.",
            "A bit like this in the top left.",
            "Then we start with this word, oh.",
            "We enter it as a lexical item.",
            "But then we have a word like OK and we have a match in terms of technology for oh in.",
            "OK, so that's going to give us a candidate lexical item.",
            "K. We've got an error already, but it gets worse because in no way we've all we have a match to oh as well.",
            "So in this case we're going to have enter into our potential lexicon the single phoneme, and that gets you into real trouble.",
            "Once you have single phoning's 'cause we're just going to get an over segmentation.",
            "Where we treat each phoneme as a candidate word eventually.",
            "So this is what happens eventually if we just use this.",
            "If we don't add anymore constraints.",
            "So what we need is is to look at the information potentially available within those single word utterances.",
            "And think about this in terms of what the legal beginnings and endings of words are given those single word utterance constraints.",
            "So just on its own, we don't have enough information from the utterances, we need some additional constraints here.",
            "And so here are two potential constraints we may add in now what we want to do is only look for a match of words when there's a known ending that ends at the point that one of our lexical items begins.",
            "And there's a known word beginning.",
            "That commences after the point of our lexical item.",
            "I'll show you that in just a moment, but we candidate words can only are only the ones with recognized beginnings and endings.",
            "That's one constraint we could potentially use, and this is information just given from from the single treating each utterance as a single word.",
            "So it comes from the same source of information and the other which Morton is going to concentrate more on tomorrow.",
            "Is this idea about glue so certain diphones are more likely to occur within words.",
            "Then across word boundaries and we can impose that constraint to within our model.",
            "But that's not the one we're going to impose within this model.",
            "'cause there's some work that needs to go in terms of developing what candidates for for final logical blue arm, which ones aren't.",
            "So I'm just going to stick with."
        ],
        [
            "A slightly simpler constraint, but there are these other potential sources of information that may be useful to us.",
            "So, so here's how it works with this additional glue constraint, Whoops.",
            "Oh dear, OK my.",
            "My animation's gone haywire, OK, so so we we start with the corpus candidate.",
            "Is oh, we enter that into our lexicon and we also encode it as a big inning potential beginning of an utterance and a potential ending of an utterance.",
            "And it's also a potential glue as well, so these diphones for oh.",
            "In this case, a dipthong are legal beginnings, legal endings, and they're also diphones."
        ],
        [
            "Within a word OK. Soaps.",
            "So then when we go to Canada, OK?",
            "We have the potential match of oh in there.",
            "Oh dear."
        ],
        [
            "So we have potential match of oh now.",
            "But the beginning of the sequence following a potential lexical candidate begins K. And that's not in our list of potential beginnings for legal words, so we reject that breaking up of OK into O&K in this time.",
            "OK, so in this case, the beginning constraint prevents us from entering OK as a lexical item."
        ],
        [
            "And so we we increase our list of potential beginnings and endings and the glue that."
        ],
        [
            "That binds the words together.",
            "And in this case two we reject the match of oh in no.",
            "And just with these extra constraints we get.",
            "Inaccurate.",
            "Anne."
        ],
        [
            "Lexicalization of the corpus.",
            "OK, so various decisions we have to make in terms of testing the model, so I'm going to have to make a decision about whether to use a diphone glue constraint and how to implement that.",
            "Also, about whether we're going to legal beginnings and endings constraint.",
            "And also need to make a decision about the decay rate of potential lexical candidates in the lexicon as well.",
            "And also we need to make a decision about how to order the lexicon.",
            "So should we be ordering it in terms of activation levels or in terms of word length or or what?",
            "This is an error.",
            "Internal diphone glue constraint wasn't included.",
            "So, for included, read excluded there.",
            "Sorry about that, but we did include this legal binding beginnings endings constraint.",
            "For the first simulations, I'll show you use to decay rate of 0, and we see that it still does a reasonably good job.",
            "And we can order the lexicon by length or by activation levels, and it doesn't seem to have a lot of difference.",
            "I'll just show you the data for by length ordering.",
            "So in this case we're just looking for the biggest match possible within each novel utterance.",
            "We encoded it as is standard in the literature for segmentation in terms of accuracy.",
            "So this is a proportion of words segmented that are actually words.",
            "And completeness, which is the proportion of the words that we know words in the lexicon that are segmented by the model.",
            "OK. Now for the baseline segmentation, we use the same as Brendan Cartwright where we gave the model the correct number of words in each utterance, and then we randomly located.",
            "The lexical boundaries within the utterance.",
            "So we're giving the base."
        ],
        [
            "Quite a lot of information here and here are the data for accuracy for the six corpora, that's here.",
            "The baseline is shown as the bottom part of the bar, the Gray bar, and the red part is improvement over this chance.",
            "Baseline for comparison, there are three models from Brenton Cartwright where they have this model of the idealized learner.",
            "Using MDL approaches, with additional constraints about words having to contain vowels, and they used a similar constraint here in terms of legal boundaries between words too, and we're in more or less the same ballpark of accuracy for our corpora as Brenton Cartwrights idealized learner.",
            "But we're giving the model much less information to go by for performing its segmentation."
        ],
        [
            "And it's highly significant improvement over chance.",
            "And here's the completeness, and again, we're in more or less the same bull parkers Brenton Cartwright's idealized learner.",
            "Again, significantly better than chance for all these corpora."
        ],
        [
            "So if we have a look at some of the qualitative results for for the types of words that are activated within the corpus, here are the top 10 most highly activated words for this nomico corpus and right at the top is her own name Naomi.",
            "This is really important and useful cue for for identifying where other words are going to be bounded and the other words are more or less the ones words.",
            "But we found another worker useful for marking the grammatical categories, so there's real interplay here.",
            "The types of distribution information that useful for grammar, or also those are rather useful for giving us information about lexical boundaries too.",
            "And we see that some of the Tomasello sticking together of frequently Co occurring words there, what's this?",
            "And after."
        ],
        [
            "10,000 sorry 8000 utterances so towards the end of this corpus, for Naomi, it's more or less unchanged.",
            "But now we've broken down those multiword utterances into."
        ],
        [
            "Legal candidate worth.",
            "So another nice property of this is is that these are tractable models, so here's Naomi's lexicon.",
            "When we have a decade .05.",
            "So this means every 20 utterances that we don't get some revival of each lexical item.",
            "It's going to be dropped out of the lexicon, and in this case we have a rather small lexicon, giving us reasonably accurate segmentation of the whole language.",
            "And so that's in yellow.",
            "The size of the lexicon for number of 1000 utterances processed from 1 to 8000.",
            "And in red we have the mean word length, and we see that the dotted bar is the mean word length in the.",
            "In the target corpus and the red line is more or less the same, so we are discovering discovering information about the mean word length.",
            "Anne."
        ],
        [
            "With this algorithm, I'm certainly for .01 decay.",
            "Anne.",
            "We haven't yet reduced to the target mean word length, but we're reasonably close right from the outset.",
            "Whoops, so this is superimposed.",
            "This is again 1 to 8.",
            "And the size of the lexicon here is again tractable, and it's showing some explosion of.",
            "Of the lexicon towards the end as well."
        ],
        [
            "OK, so in summary.",
            "Puddles are sounded, giving the child an awful lot of information and just have presented an extremely simple model which can exploit exploit that information just by treating every utterance as a single word utterance and going from there.",
            "So it's accurate and it's complete, so we're segmenting the right things at the right point, and we're not over segmenting.",
            "It shows them the same reliance on proper nouns that have been demonstrated in infants studies.",
            "It shows the frequent words just pop out within this sort of analysis, and those are the ones marking phrasal boundaries are useful for.",
            "Learning about the grammar and also extremely useful in this case in learning about word boundaries.",
            "Anne.",
            "This speech I don't know what I mean by that.",
            "I'm very sorry.",
            "No mechanism.",
            "Oh yes, yes, so this is what we need to do next with this.",
            "So currently we don't have any way to compare alternative pauses of speech.",
            "So the classic example recognize speech which has two readings, recognize speech and recognize speech.",
            "I think this is about the time of the Exxon Valdez disaster, so there is a lot of contextual information at the time where this tanker landed on the beach in Alaska and spilled a lot of oil there.",
            "But one of those readings is now almost dropped out of our.",
            "I potential models are passing, but the Bayesian approaches like Dennis Norris is approach and and these idealized learner approaches of Brenton Cartwright.",
            "Example, for example, gives us a mechanism for comparing alternatives.",
            "Once we have potential candidates for the lexical items.",
            "So this is really the pre stage before we actually begin to use those lexical items in combination.",
            "How do we actually generate those lexical items at all?",
            "So, So what I?",
            "Like to have shown you is that this is a first cognitively plausible.",
            "These are tractable approaches to the problem for how the lexicon may be generated, and there's all sorts of ways in which we can augment this by using a rather cleverer.",
            "Approach to to the activation of the lexical units and how those may fit together and also the potential Fanta, logical and prosodic information that may augment it as well.",
            "OK thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this this title is intended to convey is a topic of speech segmentation.",
                    "label": 0
                },
                {
                    "sent": "So so how children who are exposed to lots of continuous noisy speech in the environment can learn to individuate that into into words.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the reason why I chose this title puddles of sound, because this is the standard orthodoxy that this this language environment is extremely noisy and continuous.",
                    "label": 0
                },
                {
                    "sent": "It's like a sea of sounding Jenny Saffran's words.",
                    "label": 1
                },
                {
                    "sent": "And so John have to face with this task of discovering words from this continuous speech.",
                    "label": 1
                },
                {
                    "sent": "So there is no physical boundaries towards in speech.",
                    "label": 0
                },
                {
                    "sent": "The child has to learn where those boundaries should come within the speech and worse, and there being no boundaries at all, there are certain pauses that occur in speech and they are not always in the right place.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you depending on on these breaks in the speech, then that may be extremely misleading.",
                    "label": 0
                },
                {
                    "sent": "Some old studies about this.",
                    "label": 0
                },
                {
                    "sent": "Now another problem in the speech is extremely noisy, so there's a great deal of variability in the pronunciations.",
                    "label": 1
                },
                {
                    "sent": "OK, so the same word may be realized rather differently from one utterance to another, one speaker to another, so that's the idea.",
                    "label": 0
                },
                {
                    "sent": "It's an extremely difficult task facing children figure out figuring out where words are.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this talk is really about what sort of cues are available within the language environment for the child.",
                    "label": 0
                },
                {
                    "sent": "To begin to individuate words.",
                    "label": 0
                },
                {
                    "sent": "Now this is not a terribly clever computational treatment of this, so I'd like you to treat this as just a starting point for the potential cues that might be there, and then how we can develop some more sophisticated models to realize those.",
                    "label": 0
                },
                {
                    "sent": "So it's really looking at potential constraints, potential information available in the language itself for this task to proceed.",
                    "label": 0
                },
                {
                    "sent": "So because then there's all this problem about variability of pronunciations and this this unreliability of pauses within the speech, a lot of the work recently is being focused on sub lexical cues to word boundaries.",
                    "label": 0
                },
                {
                    "sent": "So so there are various prosodic sources of information in speech which help to define where words may begin and end.",
                    "label": 0
                },
                {
                    "sent": "So one example is that.",
                    "label": 0
                },
                {
                    "sent": "The final syllables of words tend to be longer than medial or initial syllables within words, and so there was a study on hamster versus ham.",
                    "label": 1
                },
                {
                    "sent": "And if you look at the properties of those syllables, then the ham, and when it's monosyllabic, is much, much longer than the ham in hamster.",
                    "label": 0
                },
                {
                    "sent": "So the potential cues there about where words may begin and end from this sub lexical information in terms of duration of utterance.",
                    "label": 0
                },
                {
                    "sent": "And there are other prosodic cues as well.",
                    "label": 1
                },
                {
                    "sent": "So in English the first syllable words tend to be stressed about 60% of the time OK. And this has been shown in various artificial language learning studies to be quite a powerful cue to indicate where words may begin and end as well.",
                    "label": 0
                },
                {
                    "sent": "And there's also some work on particular diphones particular pairs of phonemes which are more likely to occur within a word than across word boundaries.",
                    "label": 1
                },
                {
                    "sent": "And Morten Christiansen is going to say a bit more about that source of information tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's an an influential model of the sorts of information that may be combined in order to segment words.",
                    "label": 0
                },
                {
                    "sent": "This is friend Mathis model, A hierarchical model of the different sources of information that enabled enable us to divide continuous speech into its word constituents.",
                    "label": 1
                },
                {
                    "sent": "So at the bottom we have this word stress.",
                    "label": 0
                },
                {
                    "sent": "The prosodic information.",
                    "label": 0
                },
                {
                    "sent": "Then we have some sort of fun, a tactic acoustic, phonetic, and allophonic variation information.",
                    "label": 0
                },
                {
                    "sent": "And then above that we have lexical knowledge in sentential context and so spent Mathis idea is that if you don't have information at one of these tears, then you just have to concentrate on information at another level, and in particular for the child beginning to learn their language.",
                    "label": 0
                },
                {
                    "sent": "They are not given the lexicon, So what they have to do is rely on some segmental information, some subsegmental information and some potential regularity's in the prosodic structure.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the that's the orthodoxy.",
                    "label": 0
                },
                {
                    "sent": "So here's a little example of child directed speech at the top.",
                    "label": 0
                },
                {
                    "sent": "So, so who's lovely baby?",
                    "label": 0
                },
                {
                    "sent": "Yes you are.",
                    "label": 0
                },
                {
                    "sent": "You're a lovely baby, aren't you?",
                    "label": 0
                },
                {
                    "sent": "Yes, you are.",
                    "label": 0
                },
                {
                    "sent": "This comes from the child is corpus and there are no.",
                    "label": 0
                },
                {
                    "sent": "There are no gaps, no pauses, physical pauses between those words the child has to learn where words begin, and from that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "This is rather puzzling task.",
                    "label": 0
                },
                {
                    "sent": "So put it in context for the child.",
                    "label": 0
                },
                {
                    "sent": "Now the orthodoxy is you have this sea of sound, but if you actually look at.",
                    "label": 0
                },
                {
                    "sent": "At the length of utterances from child directed speech, what you find is that a large proportion of them are just single word utterances.",
                    "label": 0
                },
                {
                    "sent": "So from about 5 1/2 million words from the English child is corpus.",
                    "label": 0
                },
                {
                    "sent": "That's the latest count we made of it.",
                    "label": 0
                },
                {
                    "sent": "About quarter of them are just monosyllabic mono.",
                    "label": 0
                },
                {
                    "sent": "Just single word utterances.",
                    "label": 0
                },
                {
                    "sent": "That's a quarter of what the child hears.",
                    "label": 0
                },
                {
                    "sent": "A single word utterance.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So presumably.",
                    "label": 0
                },
                {
                    "sent": "That source of information may be potentially useful to the child that, so that's where we're going with this.",
                    "label": 0
                },
                {
                    "sent": "So these are the puddles of sound that I refer to in the title.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not so much a see 'cause 1/4 of the time.",
                    "label": 0
                },
                {
                    "sent": "What you hear is a single word individu atede, either by turn taking in a conversation or by pauses before and after it.",
                    "label": 0
                },
                {
                    "sent": "So the question really is whether a computational model which just relies on that information could accurately.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Divide up speech overtime so that was the starting point here.",
                    "label": 0
                },
                {
                    "sent": "So can we go from from this noisy environment just to the lexical level?",
                    "label": 0
                },
                {
                    "sent": "And once you've got the words the adults have the words, the task is just easy.",
                    "label": 0
                },
                {
                    "sent": "And there's some very nice models by Dennis Norris, his latest version of shortlist, which is a Bayesian approach.",
                    "label": 0
                },
                {
                    "sent": "And I think there's a talk tomorrow about Bayesian approaches to segmentation as well, so I wish that talk was before mine, but these are some of the ways once you have a lexical approach, you can produce some very nice sophisticated models to really perform well on segmentation.",
                    "label": 0
                },
                {
                    "sent": "So that's the ultimate aim.",
                    "label": 0
                },
                {
                    "sent": "But how do we generate the lexicon in the 1st place?",
                    "label": 0
                },
                {
                    "sent": "That's a key question here.",
                    "label": 0
                },
                {
                    "sent": "So the model I'm going to present now is that we're going to assume that each utterance is a word in the language until we know differently.",
                    "label": 0
                },
                {
                    "sent": "OK. And one source of information for knowing whether it's a word on its own or not is whether it's repeated or not OK, so if it is a word on its own, if it's a single word utterance, you're more likely to hear it repeated as another single word utterance elsewhere, or if we have a very long utterance made up of several different words, then the chances are that being repeated much lower, so the idea is, if we repeat of an utterance, we keep it, we increase its activation, we maintain it within our lexicon.",
                    "label": 0
                },
                {
                    "sent": "And if we're not hearing repeats of an utterance and it sort of slowly drops out of the lexicon as a candidate, that's the idea.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So things are modeling to look at whether these utterances can give us enough information to really get started in producing the lexicon.",
                    "label": 0
                },
                {
                    "sent": "Brinson Cartwright in their paper on segmentation, which is an MGL approach in 1996, question the use of utterances because you can't tell whether or not it is a single word or a multi word.",
                    "label": 0
                },
                {
                    "sent": "How do you make that distinction?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to try to make the case now showing you the results of this model, that the utterance boundaries alone is sufficient information to get started in producing the lexicon.",
                    "label": 0
                },
                {
                    "sent": "And once we've isolated, identified some of those single word utterances, they're going to be extremely useful to act as anchors or divide us within the rest of the speech.",
                    "label": 0
                },
                {
                    "sent": "OK. And I'm going to show that it is possible to distinguish most of the single word utterances from most of the multiple word utterances in child directed speech.",
                    "label": 1
                },
                {
                    "sent": "And I'm also going to hopefully show that proper nouns have a special role in this in this regard.",
                    "label": 0
                },
                {
                    "sent": "I'll show you some data on that in just a moment.",
                    "label": 0
                },
                {
                    "sent": "OK, but there's also another attractive property of this sort of approach is that we're not making any decisions in advance about what is a lexical item.",
                    "label": 0
                },
                {
                    "sent": "That's just going to emerge from the model itself, and there's some work by Michael Tomasello on multiple word sequences that occur frequently together, maybe lexicalized as well, and our model has potential power to reflect that too.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the special role of proper nouns.",
                    "label": 1
                },
                {
                    "sent": "There's a paper by Bartfeld and colleagues, and this was an experiment done on an infant called Maggie.",
                    "label": 0
                },
                {
                    "sent": "She was pretty young.",
                    "label": 0
                },
                {
                    "sent": "And Maggie was familiarized with lots of sentences which contained her name.",
                    "label": 0
                },
                {
                    "sent": "So Maggie's bike had big black wheels and so on and so on.",
                    "label": 1
                },
                {
                    "sent": "And she was familiarized on these words.",
                    "label": 0
                },
                {
                    "sent": "And bike always occurred after Maggie's.",
                    "label": 0
                },
                {
                    "sent": "The idea is does this very familiar word to Maggie help her to lexicalized the word that follows it?",
                    "label": 0
                },
                {
                    "sent": "In this case bike.",
                    "label": 1
                },
                {
                    "sent": "And that was contrasted to use of another proper noun that Maggie hadn't heard before with Hannah and Cup always followed the word Hannah.",
                    "label": 0
                },
                {
                    "sent": "And when Maggie was familiarized to these, she became familiarized to the bike much more quickly than to Cup.",
                    "label": 0
                },
                {
                    "sent": "So it's a suggestion here that that the word bike was entered into her lexicon more easily, became more familiar sooner than a word that followed an unfamiliar word to.",
                    "label": 0
                },
                {
                    "sent": "So there's a special case of proper nouns, and if we look at single word utterance occurrence within child directed speech, what you see is about 3.3% of the utterances in one of the corporate.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you in a moment was just a single use of of this child, Naomi.",
                    "label": 0
                },
                {
                    "sent": "In this case, just single use of her words as a single word utterance.",
                    "label": 0
                },
                {
                    "sent": "And equally proper nouns occur on their own quite a lot.",
                    "label": 1
                },
                {
                    "sent": "They're very frequent for the child, especially their own name, but also other very high frequent words are going to be useful for categorizing the content words and what I'm going to show is that these same words which are useful for generating as markers of grammatical categories seem to be the words that pop out.",
                    "label": 0
                },
                {
                    "sent": "First of all, as anchors for segmenting the speech as well.",
                    "label": 0
                },
                {
                    "sent": "So I think there's some nice interplay between higher levels of linguistic structure.",
                    "label": 0
                },
                {
                    "sent": "Which can be observed and may be extremely useful for the very early stages of language acquisition in terms.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Segmenting words.",
                    "label": 0
                },
                {
                    "sent": "So here's a corporate used for the model, so these are 6 corporate of children who are less than 2 1/2 years old.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the different properties of them.",
                    "label": 0
                },
                {
                    "sent": "So in center range of about 10 to 30,000 utterances in each language and the mean length of utterance is about 3 or 4 words for each of these languages when they come from various sources.",
                    "label": 0
                },
                {
                    "sent": "Now the child is has an orthographic transcription, so to try and mimic the input to the child we created Fanta logical transcription just by running the orthography through Festival speech synthesizer synthesizer.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's what the model does in outline.",
                    "label": 1
                },
                {
                    "sent": "So we have a lexicon of candidates, list of candidate words here.",
                    "label": 0
                },
                {
                    "sent": "And over on the left is the actual corpora separated by utterance that the child observes.",
                    "label": 0
                },
                {
                    "sent": "So Kitty is the first utterance that's right, Kitty is an ex utterance, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we advance through the corpus utterance by utterance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're looking for matches from the lexicon within the corpus.",
                    "label": 0
                },
                {
                    "sent": "When we start off, and there are no matches at all, so we're going to enter this single word utterance, sufferance as a candidate lexical item, and we associate it with some sort of activation.",
                    "label": 0
                },
                {
                    "sent": "And you can.",
                    "label": 0
                },
                {
                    "sent": "Plug in your own more sophisticated model.",
                    "label": 0
                },
                {
                    "sent": "At that point I think.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me go to the next utterance and what we're going to do now is search for single word utterances that occur within the speech.",
                    "label": 0
                },
                {
                    "sent": "Within the next utterance.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in this case we do get a match here for an item that's already in our lexicon.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're going to do there is increase the activation of our lexical item Kitty.",
                    "label": 0
                },
                {
                    "sent": "And we're also going to strip off what leads up to the word that's already in our lexicon.",
                    "label": 0
                },
                {
                    "sent": "In this case, that's right.",
                    "label": 0
                },
                {
                    "sent": "So now we have two candidates.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For lexical items.",
                    "label": 0
                },
                {
                    "sent": "And we've advanced through looking for matches.",
                    "label": 0
                },
                {
                    "sent": "There's another match.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We increase the activation of Kitty.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is what happens when we go through this mini corpus, so here are candidate lexical items with their respective activations.",
                    "label": 0
                },
                {
                    "sent": "Now model like this won't work.",
                    "label": 0
                },
                {
                    "sent": "We need some additional cost.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trains.",
                    "label": 0
                },
                {
                    "sent": "Now the reason why it won't work is if we have a corpus.",
                    "label": 0
                },
                {
                    "sent": "A bit like this in the top left.",
                    "label": 1
                },
                {
                    "sent": "Then we start with this word, oh.",
                    "label": 0
                },
                {
                    "sent": "We enter it as a lexical item.",
                    "label": 0
                },
                {
                    "sent": "But then we have a word like OK and we have a match in terms of technology for oh in.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's going to give us a candidate lexical item.",
                    "label": 0
                },
                {
                    "sent": "K. We've got an error already, but it gets worse because in no way we've all we have a match to oh as well.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're going to have enter into our potential lexicon the single phoneme, and that gets you into real trouble.",
                    "label": 0
                },
                {
                    "sent": "Once you have single phoning's 'cause we're just going to get an over segmentation.",
                    "label": 0
                },
                {
                    "sent": "Where we treat each phoneme as a candidate word eventually.",
                    "label": 0
                },
                {
                    "sent": "So this is what happens eventually if we just use this.",
                    "label": 0
                },
                {
                    "sent": "If we don't add anymore constraints.",
                    "label": 0
                },
                {
                    "sent": "So what we need is is to look at the information potentially available within those single word utterances.",
                    "label": 0
                },
                {
                    "sent": "And think about this in terms of what the legal beginnings and endings of words are given those single word utterance constraints.",
                    "label": 0
                },
                {
                    "sent": "So just on its own, we don't have enough information from the utterances, we need some additional constraints here.",
                    "label": 0
                },
                {
                    "sent": "And so here are two potential constraints we may add in now what we want to do is only look for a match of words when there's a known ending that ends at the point that one of our lexical items begins.",
                    "label": 0
                },
                {
                    "sent": "And there's a known word beginning.",
                    "label": 0
                },
                {
                    "sent": "That commences after the point of our lexical item.",
                    "label": 0
                },
                {
                    "sent": "I'll show you that in just a moment, but we candidate words can only are only the ones with recognized beginnings and endings.",
                    "label": 1
                },
                {
                    "sent": "That's one constraint we could potentially use, and this is information just given from from the single treating each utterance as a single word.",
                    "label": 0
                },
                {
                    "sent": "So it comes from the same source of information and the other which Morton is going to concentrate more on tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Is this idea about glue so certain diphones are more likely to occur within words.",
                    "label": 0
                },
                {
                    "sent": "Then across word boundaries and we can impose that constraint to within our model.",
                    "label": 0
                },
                {
                    "sent": "But that's not the one we're going to impose within this model.",
                    "label": 0
                },
                {
                    "sent": "'cause there's some work that needs to go in terms of developing what candidates for for final logical blue arm, which ones aren't.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to stick with.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A slightly simpler constraint, but there are these other potential sources of information that may be useful to us.",
                    "label": 0
                },
                {
                    "sent": "So, so here's how it works with this additional glue constraint, Whoops.",
                    "label": 0
                },
                {
                    "sent": "Oh dear, OK my.",
                    "label": 0
                },
                {
                    "sent": "My animation's gone haywire, OK, so so we we start with the corpus candidate.",
                    "label": 0
                },
                {
                    "sent": "Is oh, we enter that into our lexicon and we also encode it as a big inning potential beginning of an utterance and a potential ending of an utterance.",
                    "label": 0
                },
                {
                    "sent": "And it's also a potential glue as well, so these diphones for oh.",
                    "label": 0
                },
                {
                    "sent": "In this case, a dipthong are legal beginnings, legal endings, and they're also diphones.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Within a word OK. Soaps.",
                    "label": 0
                },
                {
                    "sent": "So then when we go to Canada, OK?",
                    "label": 0
                },
                {
                    "sent": "We have the potential match of oh in there.",
                    "label": 0
                },
                {
                    "sent": "Oh dear.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have potential match of oh now.",
                    "label": 0
                },
                {
                    "sent": "But the beginning of the sequence following a potential lexical candidate begins K. And that's not in our list of potential beginnings for legal words, so we reject that breaking up of OK into O&K in this time.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case, the beginning constraint prevents us from entering OK as a lexical item.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we we increase our list of potential beginnings and endings and the glue that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That binds the words together.",
                    "label": 0
                },
                {
                    "sent": "And in this case two we reject the match of oh in no.",
                    "label": 0
                },
                {
                    "sent": "And just with these extra constraints we get.",
                    "label": 0
                },
                {
                    "sent": "Inaccurate.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lexicalization of the corpus.",
                    "label": 0
                },
                {
                    "sent": "OK, so various decisions we have to make in terms of testing the model, so I'm going to have to make a decision about whether to use a diphone glue constraint and how to implement that.",
                    "label": 0
                },
                {
                    "sent": "Also, about whether we're going to legal beginnings and endings constraint.",
                    "label": 0
                },
                {
                    "sent": "And also need to make a decision about the decay rate of potential lexical candidates in the lexicon as well.",
                    "label": 0
                },
                {
                    "sent": "And also we need to make a decision about how to order the lexicon.",
                    "label": 0
                },
                {
                    "sent": "So should we be ordering it in terms of activation levels or in terms of word length or or what?",
                    "label": 0
                },
                {
                    "sent": "This is an error.",
                    "label": 0
                },
                {
                    "sent": "Internal diphone glue constraint wasn't included.",
                    "label": 1
                },
                {
                    "sent": "So, for included, read excluded there.",
                    "label": 0
                },
                {
                    "sent": "Sorry about that, but we did include this legal binding beginnings endings constraint.",
                    "label": 0
                },
                {
                    "sent": "For the first simulations, I'll show you use to decay rate of 0, and we see that it still does a reasonably good job.",
                    "label": 0
                },
                {
                    "sent": "And we can order the lexicon by length or by activation levels, and it doesn't seem to have a lot of difference.",
                    "label": 0
                },
                {
                    "sent": "I'll just show you the data for by length ordering.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're just looking for the biggest match possible within each novel utterance.",
                    "label": 0
                },
                {
                    "sent": "We encoded it as is standard in the literature for segmentation in terms of accuracy.",
                    "label": 0
                },
                {
                    "sent": "So this is a proportion of words segmented that are actually words.",
                    "label": 1
                },
                {
                    "sent": "And completeness, which is the proportion of the words that we know words in the lexicon that are segmented by the model.",
                    "label": 1
                },
                {
                    "sent": "OK. Now for the baseline segmentation, we use the same as Brendan Cartwright where we gave the model the correct number of words in each utterance, and then we randomly located.",
                    "label": 0
                },
                {
                    "sent": "The lexical boundaries within the utterance.",
                    "label": 0
                },
                {
                    "sent": "So we're giving the base.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quite a lot of information here and here are the data for accuracy for the six corpora, that's here.",
                    "label": 0
                },
                {
                    "sent": "The baseline is shown as the bottom part of the bar, the Gray bar, and the red part is improvement over this chance.",
                    "label": 0
                },
                {
                    "sent": "Baseline for comparison, there are three models from Brenton Cartwright where they have this model of the idealized learner.",
                    "label": 0
                },
                {
                    "sent": "Using MDL approaches, with additional constraints about words having to contain vowels, and they used a similar constraint here in terms of legal boundaries between words too, and we're in more or less the same ballpark of accuracy for our corpora as Brenton Cartwrights idealized learner.",
                    "label": 0
                },
                {
                    "sent": "But we're giving the model much less information to go by for performing its segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's highly significant improvement over chance.",
                    "label": 0
                },
                {
                    "sent": "And here's the completeness, and again, we're in more or less the same bull parkers Brenton Cartwright's idealized learner.",
                    "label": 0
                },
                {
                    "sent": "Again, significantly better than chance for all these corpora.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we have a look at some of the qualitative results for for the types of words that are activated within the corpus, here are the top 10 most highly activated words for this nomico corpus and right at the top is her own name Naomi.",
                    "label": 0
                },
                {
                    "sent": "This is really important and useful cue for for identifying where other words are going to be bounded and the other words are more or less the ones words.",
                    "label": 0
                },
                {
                    "sent": "But we found another worker useful for marking the grammatical categories, so there's real interplay here.",
                    "label": 0
                },
                {
                    "sent": "The types of distribution information that useful for grammar, or also those are rather useful for giving us information about lexical boundaries too.",
                    "label": 0
                },
                {
                    "sent": "And we see that some of the Tomasello sticking together of frequently Co occurring words there, what's this?",
                    "label": 0
                },
                {
                    "sent": "And after.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "10,000 sorry 8000 utterances so towards the end of this corpus, for Naomi, it's more or less unchanged.",
                    "label": 0
                },
                {
                    "sent": "But now we've broken down those multiword utterances into.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Legal candidate worth.",
                    "label": 0
                },
                {
                    "sent": "So another nice property of this is is that these are tractable models, so here's Naomi's lexicon.",
                    "label": 1
                },
                {
                    "sent": "When we have a decade .05.",
                    "label": 0
                },
                {
                    "sent": "So this means every 20 utterances that we don't get some revival of each lexical item.",
                    "label": 0
                },
                {
                    "sent": "It's going to be dropped out of the lexicon, and in this case we have a rather small lexicon, giving us reasonably accurate segmentation of the whole language.",
                    "label": 0
                },
                {
                    "sent": "And so that's in yellow.",
                    "label": 0
                },
                {
                    "sent": "The size of the lexicon for number of 1000 utterances processed from 1 to 8000.",
                    "label": 1
                },
                {
                    "sent": "And in red we have the mean word length, and we see that the dotted bar is the mean word length in the.",
                    "label": 0
                },
                {
                    "sent": "In the target corpus and the red line is more or less the same, so we are discovering discovering information about the mean word length.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this algorithm, I'm certainly for .01 decay.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We haven't yet reduced to the target mean word length, but we're reasonably close right from the outset.",
                    "label": 1
                },
                {
                    "sent": "Whoops, so this is superimposed.",
                    "label": 0
                },
                {
                    "sent": "This is again 1 to 8.",
                    "label": 1
                },
                {
                    "sent": "And the size of the lexicon here is again tractable, and it's showing some explosion of.",
                    "label": 0
                },
                {
                    "sent": "Of the lexicon towards the end as well.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in summary.",
                    "label": 0
                },
                {
                    "sent": "Puddles are sounded, giving the child an awful lot of information and just have presented an extremely simple model which can exploit exploit that information just by treating every utterance as a single word utterance and going from there.",
                    "label": 0
                },
                {
                    "sent": "So it's accurate and it's complete, so we're segmenting the right things at the right point, and we're not over segmenting.",
                    "label": 0
                },
                {
                    "sent": "It shows them the same reliance on proper nouns that have been demonstrated in infants studies.",
                    "label": 1
                },
                {
                    "sent": "It shows the frequent words just pop out within this sort of analysis, and those are the ones marking phrasal boundaries are useful for.",
                    "label": 1
                },
                {
                    "sent": "Learning about the grammar and also extremely useful in this case in learning about word boundaries.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This speech I don't know what I mean by that.",
                    "label": 0
                },
                {
                    "sent": "I'm very sorry.",
                    "label": 0
                },
                {
                    "sent": "No mechanism.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, yes, so this is what we need to do next with this.",
                    "label": 0
                },
                {
                    "sent": "So currently we don't have any way to compare alternative pauses of speech.",
                    "label": 0
                },
                {
                    "sent": "So the classic example recognize speech which has two readings, recognize speech and recognize speech.",
                    "label": 0
                },
                {
                    "sent": "I think this is about the time of the Exxon Valdez disaster, so there is a lot of contextual information at the time where this tanker landed on the beach in Alaska and spilled a lot of oil there.",
                    "label": 0
                },
                {
                    "sent": "But one of those readings is now almost dropped out of our.",
                    "label": 1
                },
                {
                    "sent": "I potential models are passing, but the Bayesian approaches like Dennis Norris is approach and and these idealized learner approaches of Brenton Cartwright.",
                    "label": 0
                },
                {
                    "sent": "Example, for example, gives us a mechanism for comparing alternatives.",
                    "label": 0
                },
                {
                    "sent": "Once we have potential candidates for the lexical items.",
                    "label": 0
                },
                {
                    "sent": "So this is really the pre stage before we actually begin to use those lexical items in combination.",
                    "label": 0
                },
                {
                    "sent": "How do we actually generate those lexical items at all?",
                    "label": 0
                },
                {
                    "sent": "So, So what I?",
                    "label": 1
                },
                {
                    "sent": "Like to have shown you is that this is a first cognitively plausible.",
                    "label": 0
                },
                {
                    "sent": "These are tractable approaches to the problem for how the lexicon may be generated, and there's all sorts of ways in which we can augment this by using a rather cleverer.",
                    "label": 1
                },
                {
                    "sent": "Approach to to the activation of the lexical units and how those may fit together and also the potential Fanta, logical and prosodic information that may augment it as well.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                }
            ]
        }
    }
}