{
    "id": "3cuevtwxamx2a5fxcwikjxshr37so57q",
    "title": "Learning issues in image segmentation",
    "info": {
        "author": [
            "Joachim M. Buhmann, Institute of Computational Science, ETH Zurich"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2004",
        "category": [
            "Top->Computer Science->Image Analysis"
        ]
    },
    "url": "http://videolectures.net/lmcv04_buhmann_liis/",
    "segmentation": [
        [
            "It sounds separately, but welcome to the workshop.",
            "The idea is to discuss a little bit the application of learning and computer vision.",
            "We have a set of tutorials to start with on vision issues related to learning, so I think I'll just start by welcoming Jochum and thanking him for coming and he's going to talk to us about image segmentation.",
            "OK, ladies and gentlemen, what I would like to do is to.",
            "To summarize up some ideas on image segmentation, and in particular to relate it to one of the long term interests of my group on data clustering so."
        ],
        [
            "So this is based on a tutorial which I gave at CPR and I rearrange some of the stuff and hope that now the learning issues are becoming clear and I would also like to point out what other challenges from my point of view, the challenges for consultum like Pascal which brings together expertise now in particular in that in that setting on computer vision on one hand and learning classification regression on the other hand.",
            "So the problem of data class ring is 1 formalization.",
            "You could you could use for image segmentation and to sort of layout.",
            "The mathematical framework is you have a set of objects and these objects are local descriptors, usually of an image and you might represent them as point in Euclidean space, feature space and then you have the clustering problem of putting those objects together which are similar in some respect.",
            "So the segmentation paradigm would then be to have pixels or small image blocks or something like that put together to a group when you believe that they have something in common.",
            "And obviously that is an inference step from the world of of, you know.",
            "Identifying objects in an image.",
            "So the number of clusters, how many of these groups you have is usually predefined.",
            "That's one of the challenges actually to get rid of this assumption that you have to know apriori how many of these classes you have and what is also given in many contexts is the similarity measure.",
            "So what?",
            "What is the function which you have to use to compare these these different objects which are denoted by X1 to XN?",
            "So that is the clustering problem and I will go through a number of those clustering problems unrelated to on one hand to what are the learning issues and on the other hand, what how that can be used for for image segmentation."
        ],
        [
            "So the image segmentation and please feel free to interrupt whenever you have the feeling that that you are lost or something is not clear.",
            "We can skip parts of the of the tutorial without too much loss for the overall presentation.",
            "The image segmentation problem is is defined like this.",
            "You have an image and according to some criteria you put together objects which you believe that they belong to each other.",
            "So these are regions containing the similar pixels that is color segmentation approach to partition that object.",
            "And you might actually define then the segments on the basis of the area characteristics.",
            "Or you could also say, well, segmentation is basically to give you a list of boundaries and these boundaries then have to have certain constraints in the constraints that they form closed areas.",
            "In the image I should also say that most people think that image segmentation as well as data clustering is not a very well defined mathematical problem.",
            "It.",
            "There is some truth to it.",
            "You will also see it with a wide variety of different approaches to clustering and image segmentation.",
            "On the other hand, I think it's still valid to have the partitioning of an image like this at an intermediate level into into different groups.",
            "It's a valid approach to sort of condense information in the image and to bridge the gap a little bit from.",
            "The low level type of information up to the high level semantic problem where you really ask what kind of objects are in the image here.",
            "So the semantics problem.",
            "That's what I have put down here.",
            "How should we infer objects from segments that's out of the key, and I guess for the Pascal Group One of the big challenges and where I think the group could make a big contribution to their research field is too.",
            "To get some more insights, how much of image segmentation we have to solve in order to have robust object recognition on one hand or maybe robust action planning for some applications like robotics or other control applications.",
            "It's not clear if you have to go through image segmentation.",
            "Some people feel that image segmentation might be a too hard of a problem because it's not very well defined and I remind you.",
            "That vapnik was always pushing the point that you should not solve problem, which is actually more common intermediate problem, which is more complicated than your final goal.",
            "So it could very well be that image segmentation because it's so well so so so wake and not very well defined is a much harder problem then ultimately to classify for a particular domain.",
            "The types of objects which are depicted in an image and I guess in for example in face recognition.",
            "The observation was that it's much better not to segment the image, at least at the point of state of the art which we have right now.",
            "Then just start classifying the image right away.",
            "OK, the class."
        ],
        [
            "During approach to image segmentation, you have some some objects pixels you want to classify them according to color, grayscale values, local descriptors like histograms and the method is to apply an appropriate grouping algorithm to the image data and the result then is I just said these segments connected regions assigned to the same cluster and obviously we then make the inference that these regions have something to do.",
            "With an object out there in the in the world, and either it's a part of that object or is the object itself and we want to actually label them the segment as some."
        ],
        [
            "In the category which belongs to that object OK, data set here is XX, one to XN and groups are denoted by C1 to CK.",
            "These are the clusters which contain all the objects belonging to one of these clusters and two sets of the mathematical framework.",
            "We have to find a mapping function.",
            "All these assignment vectors C which basically tells you.",
            "To which class to see Alpha a particular data point XI actually belongs to and then you code that information in CI in the variable CI equals 2A.",
            "That is a discrete search problem.",
            "We have an exponentially large search space.",
            "Set 1 two K to the power of N denotes this discrete space, or you can view these segmentation problem or the clustering problem is a combinatorial optimization problem and we know how to solve that in some instances and I will come round later to that.",
            "OK, so we have to find these assignment vectors."
        ],
        [
            "A point which I stressed for quite some time and many people did the same is to formulate these these grouping problems as optimization problems.",
            "Basically to layout a cost function which tells you how good a particular assignment choice of image blocks two groups actually is and that is sort of the conceptual framework.",
            "I would like to pursue.",
            "In that tutorial I should also say that not everybody is actually adhering to that framework.",
            "There are some people who invent algorithms just from scratch, and then they start analyzing these algorithms and later on you will realize that these algorithms, not minimizing a particular cost function.",
            "And sometimes you can even prove that there does not exist the cost function which is actually minimized by these algorithms.",
            "Now how well it's such an approach?",
            "Is it completely different question?",
            "What I believe is that in analogy to classification and regression, you have a much cleaner framework when you actually write down a cost function which is optimized by a particular approach and from a practical point of view, I strongly recommend in doing that.",
            "And I actually also believe that this is sort of the quote, unquote best way to attack a pattern recognition problem because in the end of the day.",
            "You have some solutions and you have to compare them and you better have a function which allows you to do that, although many of the search touristics might not actually follow such a stringent criterion, so the cost function is denoted by age.",
            "That is sort of.",
            "A relic from from my future education in physics and it's called cost function or risk in statistics or Hamiltonian in the more engineering or physics type literature and it basically Maps configuration.",
            "A segmentation solution to the positive reels and these.",
            "The number then tells you how good that solution is.",
            "And obviously then you have you have sort of a connotation coming with it.",
            "Assignment of dissimilar objects, things which really should not belong to the same segment will produce high cost.",
            "This is an unfavorable solution for it.",
            "In the algorithmic, that is sort of the ladies sort of the modeling framework which you have to design and coming up with a particular cost function allows you to really specify from an application's point of view what is important in a particular application.",
            "What do you care often?",
            "What are sort of almost arbitrary selections which should not enter the evaluation of particular clustering?",
            "Solutions all that can be can be sort of encoded in that function age.",
            "The."
        ],
        [
            "Then after you have sort of decided what cost function is appropriate for your application, you find a good solution to these cost functions and here here comes the first sort of.",
            "The first, I think conceptual challenge for the pattern recognition community for the learning community as well.",
            "Usually what we did in vision is we we.",
            "We took that function age depending on the assignment C conditioned on your data X and we found then the best possible partitioning.",
            "See star.",
            "Now if you bring that in contact with let's say classification, then the analogy would be simply the following age is your empirical risk.",
            "You have an image and then you minimize the empirical risk.",
            "For that particular image and see style, then is the best possible.",
            "What you can do on your training data.",
            "However, we should also be warned that in classification that is not the correct strategy because in the end in classification what you are paying for is the expected risk.",
            "So I strongly believe that we as a community have to extend these notions which are very clear in classification.",
            "And regression to more complicated problems like the bands coming up in computer vision and there it's by no means so clear.",
            "What is the expected risk for segmentation?",
            "So when I right now and see star is the minimum, the argument of the minimum of age, I'm sort of shortcutting the full problem of inference which might be behind an image segmentation task because I'm now saying by writing that down I want to get the minimum of the empirical risk, but maybe that's not what I really want.",
            "Maybe I want to have the minimum of some expected risk and I don't really know what that is.",
            "OK, let's just summarize we have two basic problems.",
            "The choice of the cost function, the algorithmic optimization, and the first one is a strongly application driven.",
            "The Chinese which we have to make here, and I think you can find a lot for the first point and you know different approaches, sort of tend to emphasize different properties of an application area, and that's why people don't get a unique answer to the first one.",
            "But then after you have settled foreign cost function H, It's a clear combinatorial it's.",
            "It's an informatics problem, you just find a solution and.",
            "You ask your favorite guy from from from applied mathematics to give you an optimization algorithm.",
            "By strictly separating these two design steps, I think you can.",
            "You can sort of separate the model design phase from the algorithmic issues, which is very valuable when you when you look for for new."
        ],
        [
            "Allusions to to your data analysis problems.",
            "There's a tradeoff.",
            "You know most of you have experienced that simple cost functions are easy to optimize, but they are not capturing all the all the details of your complex problem at hand.",
            "If you have a complex cost function, usually it's hard to optimize.",
            "It might have many local minimize you get recipes from all over the world, how to improve that, and in the end you are not really sure.",
            "What what you should pursue and we all live in this in the spectrum of complicated and simple models and introspective problems with with with the image interpretation.",
            "One particular simple example is that you take a cost function and it's just the sum of individual contributions from object I and this contribution.",
            "This cost contribution is.",
            "Captured here by the functional described by the function CI conditioned on XI that is actually sort of a cost function which which calculates the clustering solution or the segmentation solution separately for every object and then adds up.",
            "Two to get the cost for the complete configuration."
        ],
        [
            "So.",
            "Extensions incorporate neighborhood information in cluster assignments.",
            "It's fairly clear that when you have a situation like this, you know these spirals.",
            "They already color coded for you so that you know what I mean by groups here.",
            "Then you want to have some some more refined.",
            "Properties which are sort of more sophisticated, more sophisticated criterion to really define a group, and it's no longer closeness in that embedding space in the 2D space.",
            "It's sort of being being in a high density area, and if that high density area is sort of.",
            "Part of a spiral.",
            "Then you want to have one of these arms of the spiral denoted at one of the is one of the segments.",
            "Here this this example is chosen to make clear that an object wise evaluation of costs is not sufficient.",
            "You have to have a more sophisticated cost function to help handle a situation like this, and actually we have.",
            "We have proposed a model for this spiral example couple of years ago and you see there are still some errors in here, so so one of these green dots here.",
            "Actually, do I have a laser pointer here?",
            "So one of the of the green dots belongs to that spiral over there an it's.",
            "So.",
            "OK, OK, I mean this.",
            "I mean this data point here.",
            "You would probably say this should be blue.",
            "It's a particularity of that model that it's green.",
            "But don't worry about it."
        ],
        [
            "So to to sort of set the landscape of Avoyelles where we position these segmentation problems in the classroom.",
            "Problems, let me distinguish between two types of sort of extreme two types of problems, learning problems.",
            "Supervised and unsupervised problems.",
            "In supervised problems you have methods which are trained on sampled off label examples, so at least on some special cases you know what should come out.",
            "You train, then you have the problem of noise in your data and then you have to make a trade of how much you believe the examples in your in your training set and how much you want to generalize to new cases.",
            "Plastering belongs to a different setting in clustering.",
            "Nobody tells you what the correct segments are, it depends on your application and you have to specify sort of a cost function which gives you a guide installed solution and these cost function age is the input which distinguishes between good clustering solutions and bad ones.",
            "In a sense you have a different cost function for the supervised methods in the unsupervised methods in at that level of abstraction.",
            "I think one can compare.",
            "Here and see what is really necessary in order to learn something."
        ],
        [
            "So problem specification what is given in data clustering.",
            "I said it already.",
            "In classification, your partition, your object space, and.",
            "You have these training objects from the supervisor and the key problem is how to generalize the partition to new objects so it's fairly clear what you would like to achieve in.",
            "In classification you would want to make correct classification of future and future examples.",
            "So in class string it's not so clear how you actually.",
            "How you actually pursue these training?",
            "These testing step here.",
            "You have unsupervised partitions of objects of the object space and equality criterium.",
            "And how do you optimize the cluster criterion?",
            "That is sort of the challenge here.",
            "But there is also."
        ],
        [
            "And generalization issue in unsupervised learning problems and I will come later to that.",
            "Now you have these notions which are clear in classification.",
            "There is underfitting sort of step in between and there is overfitting.",
            "And when I, when I alluded to these notions of.",
            "Of classification then what I basically meant is that on the training data, the blue and the red dots here this complicated boundary looks fairly OK. You know none of the Blues on the wrong side, and another red on the wrong side.",
            "However, you would not put your personal money on this classifier.",
            "I'm sure otherwise let me know afterwards, I will bet against you and.",
            "So this is the overfitting situation.",
            "I make too much.",
            "I extract too much information from my data and I will be penalized on future tests of that of that classifier.",
            "And I guess it's fair to say that this is the key problem in machine learning and it sort of relates what I believe two of the most deep issues in computer science there's the computational complexity involved, which has been put on the on the sort of the research landscape by people from computer science, in particular Valiant and the called communities.",
            "So that is computational learning theory on one hand.",
            "And you have these statistical issues.",
            "The statistical learning theory problems.",
            "How much can you say on the basis of noisy data and how fast can your how efficiently can you say that?",
            "And the relation here is not very well understood.",
            "So far, and I think this key problem in machine learning classification will come up also in applications like object recognition like segmentation, but it comes up in a more complex way and we learn a lot if we start identifying what is the generalization problem in vision in division setting.",
            "OK."
        ],
        [
            "Now let me see I have to rush a little bit, otherwise these 75 will not be reached.",
            "The structure is.",
            "I talked a little bit about these basic concepts and then I will go through a number of clustering approaches or segmentation approaches and then I will start to say a little bit about optimization, much less than what I presented at the CPR tutorial.",
            "And I would like to spend a little bit more time on validation of clustering solutions, because I think the validation issue is absolutely critical.",
            "First of all, because our customers really want to have algorithmic solutions which work well on future data.",
            "You know in all of the industrial projects I pursued so far, the robustness of the methods was a key ingredient the engineers asked for, and that actually cuts down there.",
            "The.",
            "Development time, so if you have a better understanding what it means, I have a robust solution for an image interpretation task.",
            "Then I think we learn a lot about the generalization in their context.",
            "There are a couple of approaches for clustering in particular, and I will focus on the last one because it comes from my group.",
            "It's stability analysis of."
        ],
        [
            "Solutions and who doesn't want to have stable solutions?",
            "OK.",
            "So data types you will find clustering approaches for quite different data types.",
            "Vector data probably used in more than 90% of the applications.",
            "If you are sure that you know in which space you have to represent your objects, then just build a vector space for these objects and then start doing your grouping there.",
            "Another approach, very successful and pursued by by Talley Tishby and his group in Israel and also by.",
            "My group is to use histogram data to find good groups and.",
            "A third approach which I want to go into is the proximity data approach, where you no longer represent individual objects, either by vectors or by histograms, but you characterize objects by their pairwise relation proximity data or dissimilarity.",
            "Doesn't really matter if you keep track of the sign.",
            "Obviously the last representation, the proximity data representation here.",
            "Is a harder problem because the structure now is hidden in pairwise relations between your objects.",
            "You have a square matrix which might actually have missing values and so on."
        ],
        [
            "No, there's a lot of sort of.",
            "Classification of the different clustering principles in the literature.",
            "I would like on a meta level, sort of two to point you to some criteria which you could actually use to see if sort of several methods basically do the same thing, despite the fact that they are written up in completely different ways.",
            "So compactness is one of the criteria which allows you to find groups of objects and depending on how you represent your objects in which underlying space you represent them.",
            "You might actually use a method called K means I will come to that about probably most of you know that already if you represent your objects by histograms, then you could use a method called histogram clustering an it does roughly the same as K means, except in a different space.",
            "It's the space of probability distributions here and here.",
            "It's the space of vectors, paralyzed data clustering for for relational data for proximity data achieves also.",
            "A compactness type of of grouping of your objects.",
            "Average Association is a different name for pairwise data clustering.",
            "Focal road and and people in my group has looked at at connection between pairwise data clustering and K means and it's called the constant shift embedding paradigm which really makes explicit how you go from relational data to vectorial data.",
            "And here is yet another method which is which is.",
            "Fairly similar to histogram clustering, so I will basically show you examples of this method.",
            "Little bit of that one, and K means now you can, rather than stressing compactness in some underlying space, but you could also do is you could say, well here is a group and here is a group and let's find a partitioning such that if I if I now specify this As for Group One and this is Group Two, let the separation between these groups is particularly large.",
            "And methods which go in that direction.",
            "And now the laser pointer seems to.",
            "To become weak methods which are discussed in the in the vision community on the average cut, I think that was a proposal couple of years ago from the Hebrew Group and Malik's proposal, normalized cut.",
            "They fall into that class.",
            "They basically stress the separation of groups rather than the compactness.",
            "However, these are fairly similar concepts, although they look different and you will see that when you analyze the mathematically.",
            "In a different class, I think belong these methods which are connectedness methods.",
            "They are methods which allow you to handle situations like like the spiral case.",
            "There is in particular an algorithm going back to Peter Meyers Group mean Shift clustering which which is sort of, you know, has features of compactness and has features of connectedness.",
            "Single linkage is the old method.",
            "Which does the connected connectedness analysis and we have an approach which falls into the same class but is more robust than single linkage.",
            "I will not go to into that that."
        ],
        [
            "Something which I sort of pruned away when I put together the tutorial vectorial data.",
            "You know that raw data here you have the labels color coded, and that's what you want to."
        ],
        [
            "To find out, you do that by K means.",
            "You have your D dimensional vectors X one to XN.",
            "You want to specify the assignment vector and the prototypes sort of the prototype in that context is the information you can learn about the clustering.",
            "The nature of your class string and that's something you could actually then take from one instance of the clustering problem to the second instance of the clustering problem, and then try to execute your clustering solution or new data in K means.",
            "Let's fairly straightforward how that would go.",
            "What you then do is you use the nearest neighbor rule when you transfer your prototypes from the training instance to the test instance.",
            "And the seas are in the nearest neighbor assignments.",
            "You put you put that together in a cost function and the cost function is given down here you some overall data and whenever data point XI belongs to cluster C Alpha then these assignment function CI is equals 2A and you have to pay the distortion between your vector and the prototype in terms of in terms of distortion costs.",
            "This is a mix combinatorial and continuous optimization problem.",
            "The continuous parameters are the centroids.",
            "The discrete variables are the seas."
        ],
        [
            "OK, you know that you iterate.",
            "You have the following iterative update scheme.",
            "You fix your prototypes first, then you find the assignments according to the nearest neighbor rule.",
            "Then you fix the assignments and you re estimate the position of the prototypes.",
            "By using these the mean value function, and then you go through that until you find a local minimum of the K means cost function and there are various ways how you."
        ],
        [
            "How you can actually, you can actually refine the algorithm so that it gets less sensitive to local minima which.",
            "Abundant for that type of cost function.",
            "How would you apply K means actually to image segmentation?",
            "We have done that with Landsat images.",
            "Here you have a Landsat image.",
            "Every pixel here is a vector in a 6 dimensional space.",
            "You project it down to two dimensions and you see already some structure what you essentially do is you find these partitioning functions C in the six dimensional space and then you color code the image according to the the.",
            "Classes which we have found here and basically we have selected now 13 different groups in here color coded and then you go to your favorite remote sensing person.",
            "Ask him or her whether this is a good solution and in many application that works nicely.",
            "We have implicitly made the assumption that all the pixels line independent because we don't have any constraints locali so that neighboring pixels.",
            "Actually Hood should have should belong with a higher probability to the same class and that's why you get this special type noise in here.",
            "Now in most image processing tasks you would like to have these local coupling.",
            "But the local coupling then would raise these serious problem.",
            "What it actually means to do generalization.",
            "So if I have this solution here and I get from the satellite and you image what I basically would do is I would take my 13 different centroids in that representation.",
            "An I would label all the pixels according to the closest centroid and that would generalize would produce a new solution.",
            "An if I have, I have good centroids then probably the labeling of the new image is then satisfactory for the application domain.",
            "Now if I have a smoothness constraint in here, it's not so clear how I would actually transfer my my learn solution on my training image to the test image and we had a lot of discussions in the group.",
            "How you can actually formulate in such a context the generalization problem and I think this is one of the questions we have to address in the past car meeting.",
            "So if I sort of mix all these remarks.",
            "Uh, in the tutorial, put it in the on the side.",
            "I haven't separated what I mean by research issues and what I mean by tutorial material.",
            "I just want to present it as it as it comes and you should interrupt me whenever you have the feeling."
        ],
        [
            "It's getting too sketchy.",
            "There's a.",
            "There's a face ification of probabilistic version of K means it comes under the label of mixture models where you can also change the metric for your individual groups.",
            "And here you have a solution to such a mixture problem.",
            "Conceptually it's very similar to K means, and in particular for the learning issues I think it falls in the same class."
        ],
        [
            "OK, the second model I would like to discuss is sort of the K means version.",
            "But not in a vectorial space in the space of probability distributions you can formulate the same question with a cost function which has very similar properties.",
            "In that context an for image segmentation actually gives you quite good results.",
            "So what we have here is."
        ],
        [
            "And then we have a grid and we want to lay."
        ],
        [
            "Label the grid points so at every grid point you take a local statistics, maybe a local histogram of the Gray values, or the color values that something we have pursued, or you have the local statistics or Fourier coefficients.",
            "When you filter your image with Gabor filters of different orientation different sizes, then you have texture descriptors locally and you can actually characterize gridpoint by these local statistics of of your image contact."
        ],
        [
            "And then you classify it on the basis of that information which is extracted.",
            "And here you see that that you know the lighter point sign in labeled with rat here.",
            "Here you have high frequency content.",
            "This is a synthetic aperture radar image.",
            "That's why you have so much spectral noise in here and here you have a forest area and that's why you get the different texture characteristics here.",
            "So these are all nodes.",
            "the Blues are nodes in the in the forest area."
        ],
        [
            "The idea is now the following probabilistic model and those from you in the learning community will recognize it.",
            "It's a mixture model.",
            "Here you have a segment knew and your data X and the probability distribution of observing data given your segment is depicted like, here is a gamma distribution that's particularly appropriate for remote sensing.",
            "Then you have an image and you take your statistics in one of these blocks.",
            "And what you observe is a local histogram.",
            "And obviously this histogram has to compare it with the prototypical distribution for a particular segment here.",
            "So that's the idea behind these these parametric distribution and clustering.",
            "That's actually what PDC stands for."
        ],
        [
            "Here you have.",
            "A little bit of an animation down here you see already.",
            "We want to model these P of X given you as a mixture model with Alpha going through a set of different modes here and this.",
            "G Alpha is the basis function of Gaussian or.",
            "Truncated Gaussian or some other some other basis function which is appropriate for your data, so here."
        ],
        [
            "You have a different weighting of these basis functions and that gives you the first mode, the film."
        ],
        [
            "Histogram, so to speak, and that's something we."
        ],
        [
            "Have learned here now."
        ],
        [
            "Now this is the distribution for the first cluster in green you have the weighting of the base."
        ],
        [
            "These functions for the second cluster, you sum it up."
        ],
        [
            "And that's"
        ],
        [
            "What you get as the prototypical distribution for the second cluster?"
        ],
        [
            "The same for the third."
        ],
        [
            "Let me just click through the whole."
        ],
        [
            "Animation and that that are then the prototypical distributions sort of your prototype."
        ],
        [
            "But these are now prototypes in the space of probability distribution and this method at least this segmentation techniques works much better than than K means because in K means you have these implicit assumption that Euclidean distances actually mean something for your, for your application, and in many cases that's not the case.",
            "That is not a true statement."
        ],
        [
            "The generative model is given here from your objects, labeled by I.",
            "You go to your segments labeled by new by.",
            "By choosing these assignments here and then, you go to your local descriptors.",
            "And I J which are the joint observation of some features J with your object I that could be for example different colors, different feature, different texture descriptors in a particular window, and that's what you want to model with these prototypical distribution of the data X.",
            "The feature vectors X.",
            "Given this segment new."
        ],
        [
            "Here is the mathematics coming with it.",
            "Here you have a prior.",
            "Here you have the likelihood of X given the assignment C and some other variable fee to these Theta.",
            "For example denote the centroid of your of your modes.",
            "Here of these G and it might also give you the mixture coefficients P, Alpha given segment CI.",
            "And then you have these mixture of Gaussians.",
            "Let's let's use tree for a Gaussian.",
            "These mixtures of Gaussians.",
            "Here you make your observations how often a particular value has been observed with a with a different window.",
            "I image Patch I then you have a product over all your different features J.",
            "So you have to quantize your distribution.",
            "Otherwise you would have an integral here.",
            "Infinite product here.",
            "And you some you you, you take all your windows in product here over all your objects and if you take it together."
        ],
        [
            "And you define as cost function the negative log likelihood.",
            "Then what you get is a prior term.",
            "Here you can forget about this one, but you have the joint.",
            "The histogram of observing object I together with feature J you some overall features and you have these log of the mixture contribution here and this is again mixed continuous discrete optimization problem.",
            "The discrete part is given here.",
            "By these assignments CI like in K means and what you have is here.",
            "The mixing weights which have to be learned and.",
            "Preferably also some of the characteristics of these of these Gaussians and you can interpret this as a two part coding scheme.",
            "You want to minimize the expected code length when encoding the cluster membership and based on that information you encode the individual color values.",
            "So from a mathematical point of view, this cost function has the same characteristics.",
            "It's a mixed continuous discrete optimization problems and the assignment variables for object I are not coupled to the assignment variables for object J given the characteristics of your mixture coefficients and the Gaussians.",
            "So if you do that conditioning on your your statistics, the modes and the mixing variables then.",
            "The assignments of different objects are independent and that is expressed by basing.",
            "Basically the linearity.",
            "Here the linearity of CI not being coupled to seed CI prime."
        ],
        [
            "So tiny Tishby has interpreted this scheme as a bottleneck scheme, as finding an efficient code for your feature vectors, you have to find prototypic codebook, vectors X~ and the cost function, which I just have have presented on the previous.",
            "Transparency can be written as a difference between two mutual information, and that's sort of the NYS interpretation in terms of.",
            "In terms of information theoretic approach, our image blocks are the axis.",
            "Here they are supposed to be compressed into segment descriptors.",
            "These are the X still is here and what you want to preserve is with these these segment descriptors you want to preserve as much knowledge as possible about your features and the features might be the colors or the local frequencies.",
            "So the wise encode your colors and local frequencies or other local descriptors.",
            "And you sort of compress in a context, and that's the way Tally Tishby sells these ideas.",
            "You compress your image patches in a context sensitive way so that the resulting description of your image patches is maximally informative with respect to the color content and.",
            "The frequency content if you have selected Y as colors and frequencies.",
            "Heal you."
        ],
        [
            "Have the same thing.",
            "A pictorial describe this is the feature space.",
            "This is the object space and you compress it, sort of in a reduced representation.",
            "By fixing this constraint on the minimal amount of informativeness of your clusters, whereas here you want to minimize the mutual information to get generic descriptions of your objects, basically of your of your image patches.",
            "I think this is a very nice concept from the interpretation point of view and it works successfully.",
            "Surprisingly successful in in in many different areas like document retrieval.",
            "My first PhD student, Thomas Hoffman, has applied that in document retrieval with very good results.",
            "the Hebrew group is using their concept for analyzing spike trains as well as Galaxy information, and they have some other applications in bioinformatics and we have used it in in vision and also in some areas of my info method."
        ],
        [
            "Now the segmentation with PDC looks like this.",
            "Here you have these image."
        ],
        [
            "And basically on the on the basis of local color descriptors.",
            "These are the types of segments you get and so for many application this is a good enough segmentation.",
            "We are currently participating in a competition set up by the Mally Group.",
            "They have a large number of hand labeled images and what you can do is you can basically use your favorite segmentation algorithm, run it against the database, and then do a precision recall analysis of your results compared to the ground truth, which is defined by by Berkeley undergraduate students.",
            "So if you believe that is a good source for ground truth, then you can compare yourself against.",
            "This database."
        ],
        [
            "Since it's a fully probabilistic model, you can do resampling, so this is the original image where we use which we used to train our segmentation model and this is sort of a sampling from that from that probabilistic model and you see it's a color segmentation approach.",
            "And obviously we screwed up the texture and actually we could do that for texture as well, but the reconstruction is much more complicated when you have couple features to go from feature space back to the original space."
        ],
        [
            "It works sufficiently well, and that's actually what we were paid for in in remote sensing imagery.",
            "So this was the original and this is a reasonable image, and for many people.",
            "In that area, this was surprisingly good.",
            "So they they bought into that model."
        ],
        [
            "Let me now come to.",
            "Are there any questions?",
            "In these pictures you just showed us, we just sort of partitioning by color, really not using when I see pixels are placed together or something.",
            "No no, no that was not used.",
            "Now since the local windows are overlapping you get you get an implicit coupling between different between different sites by feeding in the same statistics.",
            "But what you really would like to know is if you, for example, have a malkoff random field model in the back of your head and you add you add to your your cost function for clustering local coupling of the CI value to a neighboring CI plus one or CI minus one object.",
            "Then you would also have to have a smooth mapping.",
            "But then what I'm saying is we don't really know how to.",
            "You know we can do sort of optimizing the cost function.",
            "That's not a big problem.",
            "You go to the people in combinatorial optimization.",
            "They give you a cutting plane algorithm and then you just get a solution.",
            "But it's not so clear how you would actually transfer such a solution to a new new image.",
            "So the factorial nature of these cost functions makes it particularly simple to talk about generalization problem.",
            "We just take the statistics and we use then the nearest neighbor rule with the new cost function and then we get the labeling.",
            "That one.",
            "Coding session I don't know, not sure I want to be recorded OK. OK, so let me just take you up on that so but why can't you say that you are happy to learn the individual site statistics that basically distributions from a bunch of?",
            "Well even for one image.",
            "But then if you learn some Markov random field interactions from a say a few images, then can't you just run something like the boy cough, golf?",
            "You know this is efficient ways of finding MFP solutions.",
            "Democracy random fields, conditional on the data so couldn't can you then make that work?",
            "I mean I don't see a problem there well.",
            "I mean what no?",
            "No, no, no, I think I think on on our way here to Grenoble.",
            "We had a discussion in the train and let me let me sort of tell you a little bit about what the problem I think is and it makes it clear when you use K means or these distributional clustering cost function and you have trained you are training.",
            "You have explored, exploited the information in your training image.",
            "And you have now your centroids.",
            "Basically what this, then the scenario actually is, you get step by step pixel by pixel.",
            "The new information from the test image and you have to make a decision after I give you a particular local statistics.",
            "Now what do you really want to do?",
            "If I tell you I give you in some order the pixels of the new image, but you have your coupled model?",
            "You know well?",
            "I mean there isn't the game I was playing, I just you give me the whole image, no, but if you yeah, but but the true, the true challenge is to define the generalization problem and you have no generalization problem.",
            "If I give you the full image.",
            "An if you are only tested if you are judged on the on the on the empirical error on the test case, then nobody can prevent you from basically doing the best.",
            "What your combinatorial optimization gives you.",
            "I still think I can make that work now.",
            "I'm not saying that there is no solution pieces from the image and give it to me later sure sure.",
            "Sure you have, but you have to think about a little bit later how how you would actually use that smoothness constraint, which is in your statistical model build into which led to some changes.",
            "So the game that I had in mind is that you have an ensemble of images, maybe I have a lot of things that I want to you know.",
            "Landsat images and I keep some back.",
            "So just like we play, you know, in in classification hold the game is now not on the individual image but on a whole bunch of images.",
            "I sort of used some to learn about things and then use the rest.",
            "So yeah, that is sort of that is the well then you are in the game that you say, well, you have asymptotically many images.",
            "The images are taken in dependently, you know, and you want to build a probability probability model for these images.",
            "This is what people in optimization called the sample average approach.",
            "However, the real world situation in many combinatorial optimization problems is that the definition of your problem instance is noisy.",
            "An you at most get one or two cases or maybe 10.",
            "But there is much more to be learned about local smoothness and segment statistics from a very small number of of, let's say, images, so that you are forced to use.",
            "You know statistically, relations between neighboring feature descriptors in order to build up your model.",
            "And I think that that is one of the challenges.",
            "At least I perceived so far.",
            "If you have a setting where your objects are independent, then the whole bag of statistical learning theories will work.",
            "You know you have you have large deviation results, you have IID contributions of your empirical risks to build up your costs, and then you can easily define what you mean by expected costs and so on.",
            "When the random valves are coupled, it's no longer so clear.",
            "And it's probably one out we can discuss it later this week.",
            "OK, so so maybe this example also makes it clear what at least one possible route is.",
            "I'm not saying this is the only one, but one possible route is to to learning in a more complicated setting.",
            "Here you have you have protein data.",
            "They are pairwise compared.",
            "You get these.",
            "These sort of these little bit structure, but otherwise randomly looking matrix here it's symmetric, so some of the structure is coming from that symmetry here.",
            "If you permute your columns and your rose then you clearly see that you have big groups in here 3 and you want to find these these groups for example."
        ],
        [
            "Data abandoned here is an example from biology, molecular biology we have."
        ],
        [
            "First, used that following up the research by Guimond Gaming kafenion down.",
            "We have used it for these Landsat imagery.",
            "Basically what you do is you have a local image Patch.",
            "Here you have a local image Patch.",
            "Here you get some empirical distribution of your features in that area in that area.",
            "Then you apply a statistical test.",
            "Whether these two areas below come from the same source are empirical realizations of these.",
            "Source and if if high squared statistics for example tells you these are very similar, then you should have a small devalue, otherwise you get a large one and then you have a pairwise clustering problem and this is the ultimate labeling of three classes here.",
            "Now it's not so clear how you would generalize from this solution where I basically have a bunch of of assignment variables of all my different local image patches with the local statistics extracted there.",
            "How I would?",
            "I would use how I would use a solution to my training image and then sort of just copy it to the test image without actually optimizing on the test image from scratch.",
            "And I should also say.",
            "In that setting, you know you compare all possible image pairs here.",
            "So you also take a one up here and compare it to that one down here.",
            "So all these all these sites are compared to each other."
        ],
        [
            "Now the cost function is sort of rebuild according to K means for that for that new data type.",
            "Basically what you do is you sum up the dissimilarity's between object I&J whenever both objects belong to the same cluster.",
            "So that's the compactness criterion, which you also have in K means when they belong to the same class are you compare them to the mean?",
            "Here you have to compare them pairwise to each other and in order to get the proper scaling of these quadratic.",
            "Turns here you know you sum over I entry, so you get quadratically many comparison contributions.",
            "Here you normalize it with the size of these of these.",
            "Of these segments, so seeing you is the set of the segment and you take the cardinality here and what we want to find is, is this assignment vectors, see note here.",
            "We don't have sort of a generic descriptor for cluster mu.",
            "So these vinnu is not not available here.",
            "Now what you can you will find out to Diane how the third is 73 addition.",
            "I think page 200 something you will find out that if these DJs squared Euclidean distances that this cost function, the pairwise clustering cost function is mathematically equivalent to the K means cost function.",
            "When you choose the wise as the means.",
            "So now we."
        ],
        [
            "You also have to realize and that was known already before this work has been found as been being sort of put together.",
            "Mainly bifocal was that the pairwise clustering cost function is has certain invariances which are quite important for the application.",
            "The first one is a trivial and you know you sort of can only.",
            "By summing over all pairs, if you have an asymmetric dissimilarity value, which is probably not what you what you want to do live with.",
            "But sometimes your application.",
            "If the application people design it for you, you can just symmetrize it without changing the cost function.",
            "So H tildes equals 2H.",
            "The more important, the more important.",
            "A similarity invariants is these additive shift invariant.",
            "So if I add a constant D02 all off diagonal values, then the cost function H~ which I guess after the transformation is identical to the cost function before the transformation plus a constant.",
            "So we just transform the cost function by a constant.",
            "But obviously since I had only a cost constant to it.",
            "The solutions the minimum assignments.",
            "For agent for H~ all the same, and that was the key insight too.",
            "The two basic."
        ],
        [
            "Ugly establish a general relation between pairwise clustering and in K means an.",
            "I think you will learn something about the generalization issue in their context, so here is some mathematics I don't want to go to through that, but just tell you what these constant shift impending is.",
            "These transformed by JD similarities can now be into interpreted after a proper shift of the dissimilarity's by Lambda 0 times these.",
            "Matrix, it can be interpreted as squared Euclidean distances between vectors.",
            "Basically, now you can use the kernel trick and then you see that the optimal assignment for K means in that constructed Euclidean vector spaces.",
            "Are they identical to the optimal assignments in their paralyze problem?",
            "And you can find these coordinates which are not explicitly given, because all what you have is this dissimilarity matrix.",
            "You can find these coordinates XI by agan value decomposition an you can now optimally approximate things by using kernel PCA.",
            "And what is the lesson which you can draw from that.",
            "You basically go from a problem which looks inherently coupled.",
            "Every object is compared to every other object and by using these invariants of the cost function we find a transformation such that the.",
            "Existed Euclidean space.",
            "It might be N -- 1 dimensional and in that Euclidean space we do K means and K means has the property.",
            "If you give me the centroids, I know what I have to do on new data, I just take these fringe centroids and I compare every object of the new data set with the centroids and I find the cluster label which gives me the nearest neighbor assignment.",
            "So implicitly what you do is by these transformation you decouples the objects object.",
            "I is now decoupled from object J if I know the statistics in that embedding space.",
            "And I think that's what you have to learn.",
            "You have to learn what kind of statistics allows you to describe a new object, independent of all the other objects which might actually come with the same instance.",
            "And in a situation like this, it's easy to talk about generalization in situations where you don't know what these decoupling statistics is, it's becoming fairly complicated to talk about, talk about generalization and what you mean by generalizing from one instance to the next one.",
            "So a couple of years ago I was working on on noisy TZP problems, and we can discuss offline how that might fit into that into that framework.",
            "But here it's sort of clear."
        ],
        [
            "Here's some some examples that it actually works.",
            "These bioinformatics applications five different classes.",
            "That is the ideal solution that is, the original dissimilarity matrix, and after denoising you get rid of basically dimensionality reduction in that embedding space you get rid of a lot of noise entries in this dissimilarity matrix, and what you find is you get a much better distinction between the different blocks than what you could do on.",
            "On the basis here and you can do a lot of things more things.",
            "For example, now you have a vectorial representation in that embedding space, it's a generative model there, and you know, denoising all these issues are now fairly clear."
        ],
        [
            "Here is another example which tells you that that this is actually working."
        ],
        [
            "OK, by the way, how much time do I actually have?",
            "What?",
            "I'm over.",
            "15 minutes.",
            "Now 15 minutes left.",
            "OK. Well.",
            "I spend too much time on these models.",
            "Another very popular segmentation model is normalized, cut, normalized cut basically has the same problem.",
            "I would say as is pairwise clustering when you apply it to segmentation and basically what you do is you have you have a graph, a set of vertices V an you have a subset A of where this is a subset B of vertices and you have weights on the on the on the edges and when you cut.",
            "A from B when you when you calculate the cat between A&B.",
            "This gives you a criterion how good use your segmentation actually is and that's why this method is sort of called separation method and you normalize it so that it's getting fairly robust with the Association between that set A and the full set of vertices V. So and it's formulated tier in such a way that you always have a clustering between two groups.",
            "Full vault in the foreground and background.",
            "And again, I give you one image.",
            "You execute and cut and then you have a solution for front end cut and then I gave you the second image and there's again a Tiger as in the first image and you tell me how you generalize your Tiger solution from the first image to the second one, and it's absolutely unclear how you would transfer here the solution.",
            "So here it's differently formulated with these assignment values, CII.",
            "Don't want to go into detail here."
        ],
        [
            "You can find the relaxation and that's why N cut is one of the examples for for spectral methods in computer vision, spectral methods are fairly popular in the learning Community these days, and even there I think people haven't appropriately discussed what they mean as sort of the the generalization issue when you use some of these methods to interpret your data.",
            "But at least here you you can.",
            "You can relax from from the Boolean assignment C. You can relax everything and find here you have these.",
            "These axes which are defined on the interval between minus one and one.",
            "And here the seas have to have the.",
            "Use minus one and one, so the relaxation to end cut can be solved efficiently and cut by itself is NP hard.",
            "That was proven by by a colleague of of.",
            "Jitendra Malik and he probably the the type of of images you have seen before from Alex Sheehan Malik's papers.",
            "OK um.",
            "So that concludes base."
        ],
        [
            "Really, the modeling part in the tutorial.",
            "I still had some other methods, but I think from a learning point of view this sort of covers the full spectrum of what you what you would like to discuss on one hand, conceptually, methods which are fairly well.",
            "Located in the in the in the ballpark of learning, problems like K means all these parametric distribution clustering.",
            "Some other methods which are fairly popular which achieves the same thing, partitioning local image characteristics feature values in K different groups, but which which look not be addressable or analyzable.",
            "In the usual way, when it comes to generalization, let's end cut and one relation is the one between pairwise data clustering.",
            "K means where you see through this mathematically equivalent how you can go from.",
            "From what I would call a generative Model K means in that embedding space to a model where it's not so clear how you how you actually would generate new data, and that's something we have to learn in the future."
        ],
        [
            "In the Section 2 I was focusing on on.",
            "Noise insensitive ways, robust ways, and I think the notion of robustness in in application domains is mostly related to be sort of invariant to slide show very, very little variant to slide changes in your data, and it basically is kind of kind of.",
            "Yeah, generalization ability of the methods from one instance in a slightly different instance.",
            "So maybe an instance where the signal quote unquote is the same but the noise is different and sometimes it's not so clear what is actually the signal.",
            "If I give you 2 Tiger images in one, the Tiger is jumping in the lower left corner, and in the other is sleeping in the upper right corner, the signal I would say is being a Tiger being present in these two images, but it's not clear how you distinguish.",
            "On on on the level of individual image blocks.",
            "How you would distinguish signal from noise.",
            "And the challenge for in particular the Paski project, I think, is is clarifying these issues because they they are very deeply related to what we mean by learning.",
            "So candidate solutions should be typical and maybe they have to be averages of partitioning and."
        ],
        [
            "Method which I was advocating for for for for a number of years now is in maximum entropy principle and you have a strategy to search through the solution space and that is the space of possible partitions and you accepte partitions, which with costs expected costs here of age, smaller of being smaller than a constant in this constant is parameterized by the temperature T. So for those of you coming from statistical physics, they know these relations and the others just should imagine that T is a parameter which allows you to tune the quality of your expected quality of your of your costs.",
            "Here you can do that by Markov chain Monte Carlo or Mean Field approximation to methods in that context."
        ],
        [
            "And here you have one of these algorithms and MCMC algorithm, and I think the essential the essential.",
            "What you should.",
            "Basically what you should basically take from these transparencies that the algorithm explores the universe of partitions which have a certain characteristic and the characteristic here is that in terms of these empirical costs they are below a threshold.",
            "And if you if you believe that noise sort of adds to the variability of your partitions and the signal votes always for the same type of partition, then averaging over different partitions might actually give you something like a prototypical partition.",
            "And that's exactly what happened in these pairwise clustering, because in pairwise clustering we also wanted to find the partition and going in that embedding space where you can do K means by mathematical equivalence you find these generative.",
            "Partition by the centroid in that space.",
            "That is basically the model which was underlying these equivalence between K means in pairwise clustering.",
            "Here you can do that trust algorithmically by searching through many of these partitions, and whenever they fulfill the criterion, you take them and you extract the statistics of it."
        ],
        [
            "So usually what you do is you maximize the entropy with additional free parameters.",
            "That sort of one of the approximation schemes, and then you have an EM like iteration where you iterate between where you are.",
            "You maximize the entropy and basically what you do is you.",
            "You fix some some statistics which you have for measuring how costly it is for one of your objects being assigned to a particular cluster.",
            "And the assignment function and that is very similar to what we have discussed before in terms of K means."
        ],
        [
            "What you see is sometimes surprising.",
            "You get these quote unquote phase transitions.",
            "Here you have a data density and when you have very crude approximations down here, this is the inverse temperature better, which is also often used.",
            "Then you will estimate six different clusters and they are all located in the same position and that's here around.",
            "Oh point probably oh .30 point 4 and then all of a sudden when you reduce.",
            "Your approximation quality, you lower the temperature and your constraint on the acceptable quality of a solution is low.",
            "Blood, then you you force the the statistics to split up and four of the centroids are on this branch and one centroid is located in here and if you lower again these threshold in this approximation scheme you get further breakups until you have six different six different clusters for these six different modes.",
            "OK, let me skip the EM power."
        ],
        [
            "For parametric distribution and clustering, and just show you on that image which you have already seen before, how how you will find solutions at different temperatures at very high temperature.",
            "You basically get these sort of these salt and pepper type image.",
            "You have six different colors in here and you see it's basically a random choice.",
            "Which of these colors you select you lower."
        ],
        [
            "The temperature a little bit and the most dominant structure according to your cost function start to appear.",
            "So here you get sort of a condensation of Gray values in that area and in that area, and these are if you remember, these are the black parts in the in the Landsat image and the black parts are coming from from the not in the lens at in the synthetic aperture radar image and it comes from the radar shadow.",
            "You go."
        ],
        [
            "Down and all this."
        ],
        [
            "Button more structures appear."
        ],
        [
            "You still have a mixture between yellow and blue.",
            "He ran between 2 Blues here and you go."
        ],
        [
            "Further down."
        ],
        [
            "And at the end."
        ],
        [
            "And.",
            "At the end you basically get this type of of picture.",
            "We have now much better solutions than that, but I only have the video.",
            "In this in this presentation format, here from from that image."
        ],
        [
            "Here you have the same for Mondrian and what is plotted here for different temperatures.",
            "It's the kullback Leibler divergent to the mean histogram.",
            "So if I do the same thing in histogram space, I have to somehow characterize my different solutions and it's I do that by plotting by plotting the.",
            "The prototypical histogram for one of the segments, by basically plotting the distance of the prototypical histogram to the mean histogram.",
            "And you get these abrupt changes in solution space, which basically gives you these.",
            "These these these splits.",
            "Let me not go into that because I will lose too much time.",
            "You can use a multi scale approach for it."
        ],
        [
            "And.",
            "I guess I have 5 more minutes.",
            "Yeah, because we started a little bit late, but I don't want to go in the negotiation phase after 1 1/2 hours.",
            "It's usually we have already exhausted.",
            "The attention potential.",
            "OK, the third part I think is inherently.",
            "Connected to the generalization issue and it basically means how many classes do I have up to now.",
            "I used this magic number K as an input.",
            "Somebody told me how many classes I have to use.",
            "Ideally I would say, well, maybe I know an upper bound on the number of classes, but please if it happened to be an image where two of my envisioned clusters cannot be separated on the basis of the data.",
            "You know the algorithm should figure out by itself that rather than 10, which is the maximum number of clusters in that application, I should only use seven and discard in some way or another the other the other segments.",
            "So that's the cluster validation problem, and there are a couple of approaches to it.",
            "The most prominent ones by Reason and Schwartz.",
            "This is the minimum description length approach or stochastic complexity approach.",
            "And here you have the Bayesian information criterion.",
            "Porex Miss couple of years ago used to cause validated likelihood.",
            "Works also fairly well.",
            "More recently, in particular in the context of bioinformatics, the Stanford Group came up with the.",
            "A method called gap statistics and we have proposed a stability based validation criterion and."
        ],
        [
            "I will, I will say a little bit about.",
            "The cluster validity problem and then come to the stability issue.",
            "So clustering algorithms always impose.",
            "Hello, always impose structure on the data and here you see it looks like an inappropriate model order.",
            "You know here you you tear up the this this group of data points in basically four different clusters and maybe the fluctuations have decided which of these clustering solutions have been found.",
            "Here you have something like an inappropriate model type.",
            "We have used the compactness kind time.",
            "Criterion and we have these ring structure and obviously we expect that we find the rings and not not these these cake type slices of these rings.",
            "And actually both problems come up in cluster validation."
        ],
        [
            "So validation methods are procedures or concepts for quantitative and objective assessment of clustering solutions, and it's sort of a definition, and we want to evaluate specific clustering measures and they should be treated at the same footing.",
            "I would say so we have to concentrate on on what we really get as an answer from a clustering method, and these are the partitions and then you have external validation criteria.",
            "In particular, when you can compare with ground truth.",
            "So the segmentation on the Berkeley database would give you ground truth full segmentations.",
            "If you accept that, and that would be external or you have internal criteria and they can you be used for model selection.",
            "And the important question is what is the appropriate number of clusters for my data?",
            "So in general, what you would like to do is you measure the quality for different criteria and you need a function which distinguishes between the.",
            "You need a method which distinguishes between the different answers for these cluster number."
        ],
        [
            "Complexity based validation maybe go very quickly.",
            "You add a complexity term to your log likelihood and you penalize for two complex solutions.",
            "And in that context it's it's modeling all comes razor, favoring smaller solutions.",
            "If they don't give simple solutions.",
            "If they don't give you an appropriate amount of model fitting.",
            "And the two methods which I just mentioned fall into that class.",
            "So."
        ],
        [
            "The underlying principle is here.",
            "You have your independent model parameters.",
            "You have your negative log likelihood.",
            "The model complexity penalty is going up.",
            "The fitting term is going down and somewhere in between you get a nice operating point for these methods and that's why people often look when they when they plot the local likelihood as is a number of the parameters they look for, such for such a kink in that curve, and then they say, well, this is the.",
            "The right model complexity where you should stop.",
            "I'm very sceptical because you know that might be another another kinkier in that curve.",
            "Should I stop here or here so it's not so clear?"
        ],
        [
            "Here you have another solution for BIC and you see here.",
            "It's slightly going up here with the penalty term you start penalising these more complex class does more than they help you and actually lowering your your negative log likelihood and that's why you get a minimum here and that that's the number you would choose."
        ],
        [
            "Now.",
            "These methods are all fine, but they require that you have a generative model, which you can actually use for now.",
            "If I just use end cut in the way I have described it, all these pairwise clustering cost function all what I have is I have some some partitions and I want to have a model validation criterion which tells me on the basis of these partitions whether two solutions are whether a solution is appropriate for my data or not and.",
            "If you use the methods which I just briefly described, then you incorporate some structural biases.",
            "Sometimes people believe that they want to do that, and sometimes it's not even clear how you how you would formulate your problem in another way.",
            "OK, so the main idea of that line of work and I still consider it to some degree preliminary, but we can discuss that later is that solutions should be stable and they should be stable when you have two datasets from the same data source and how you can realize that is a question which we have to discuss, But that's sort of these training testing paradigm.",
            "You train on one instance of your data and then you transfer the solution from that instance.",
            "To the test instance, and then you measure how well you do on the test instance and if you have roughly the same quality, then you believe your solution and hopefully when you get bad solutions they show up as being very unstable.",
            "Otherwise this concept will not work."
        ],
        [
            "And here it's sort of depicted graphically.",
            "That is obviously a stable solution when you have two different instances, and you overlay the two solutions and here you take 2 instances of the data, and once you get these blue dashed cluster boundaries and in the other case you get the black cluster boundaries.",
            "And obviously you see already that 4343 different three bumps, an 4K equals 4.",
            "You might get this inherent cluster instability which is driven by fluctuations, and that's something the algorithm should be able to detect and discard.",
            "The solution K = 4 in favor of K = 3."
        ],
        [
            "So the general procedure then would be let's this cluster stability based validation technique.",
            "You draw two datasets from the same source cluster.",
            "Both datasets compute the agreement and then the stability is the expected agreement of these solutions and you can interpret that in the context of classification by basically treating the classifier the clustering algorithm on one data set as the teacher for the second data set.",
            "OK, so we have.",
            "So this is the thought of the theory part.",
            "In practice you have only one data set available, so somehow you have to live with that, but you still can S estimate the expected agreement by resampling and you have to pay attention when you do.",
            "By resampling you get the same objects in both sets and they might actually sort of generate artificial stability.",
            "Which should not be used as an argument for for particular model or So what we did is we split the data set in half and we basically found solutions on the first half and on the second half, and then we calculated the agreement and how we do that will now be wrapped up in way too short time."
        ],
        [
            "And on the next couple of slides, measuring the disagreement, 3 problems clustering solutions of labelings, labelings of disjoint sets.",
            "So we really have to know how we transfer the solution from this first problem instance to the second one.",
            "The labeling itself is not unique, it's only unique up to a permutation.",
            "So you have to really permute at least one solution so that it best fits to the first one in the resulting.",
            "Mismatch then is used as a criterion that these solutions are unstable and you have to take into account that when you have only two clusters then 50% is totally random random result.",
            "However, when you have K = 10 classes in many application of 50% agreement between two clustering solutions is is acceptable in these applications.",
            "So somehow you have to you have to discount your error when you go to larger number of clusters.",
            "Because you also have more possibilities to make errors, and this entropic term has to be taken into account."
        ],
        [
            "So the the the biggest problem so far and I think that's a problem for the learning community in large is we have to find away how we extend solutions from 1 Somerset 8 to a set B and that as we started to discuss is not so easy in these contexts.",
            "So what we do is we train a predictor on A and then we predict the labels on B.",
            "That is sort of my second half of the data.",
            "And then you compare the clustering solution.",
            "You compare these predicted labels with the clustering solution on B.",
            "So you do your empirical minimizations on the test cases and you compare it with the predicted labels from the from the predictor trained on the training case and then extend it to the test case and that sort of allows you to measure how much in the solution is actually.",
            "Captured by you by your by your training procedure.",
            "So the the predictor, how to choose the predictors is sort of yeah's problem which we have not where we have not found a unique solution.",
            "You have to use a meta principle and you will see if you use a connected type connectedness type of of extension principle then it will be appropriate for this.",
            "For this data set.",
            "Compactness type extension principle is inappropriate for it, so you might get a poor predictor as well."
        ],
        [
            "So the labeling is unique up to a permutation, that is, that is denoted here and the solution is then the stability index with which which is defined as the expected minimal disagreement over all permutations here.",
            "So to put it together, there's a method called the Hungarian method which allows you to find these best rearrangements of your clusters."
        ],
        [
            "So the stability costs are scale sensitive to K. That's what I wanted to make clear with this, comparing K = 2 and 50% error in K = 10 and 50% error.",
            "And you can normalize for that by by this comparison to the random predict which gives you sort of a scaling argument.",
            "How you should judge these errors and then you normalize these expected agreements here.",
            "With the error random predicted would make."
        ],
        [
            "And then you put everything together and the final solution is you concentrate on the disagreement rate and that is what your what your labeling algorithm gives you on the data of the second set.",
            "And this is.",
            "This is the predictor G which takes an object from from the second set and the data from the first set.",
            "These are the prime, the training data, and the partitioning solution from the first data set, an if that if what you would do, knowing all the information from the second instance compared to the prediction of a solution from the second instance if that.",
            "Is in disagreement.",
            "You counted as an error.",
            "You have to break the permutation symmetry so you permute over all possible relabeling Zanu, find the minimum the best free labeling of it you have to take the expectation over, or training data in all test data X&X prime.",
            "Ann, you have to normalize that with respect to these two these.",
            "K scaling of the random predictor and then you have a problem.",
            "We cannot calculate these expectation values, so you have to after estimate it and what we do is we estimated by resampling.",
            "So that allows you."
        ],
        [
            "So then to look at at here toy datasets here these these five different.",
            "Gaussian distributed data sets or groups have a clear minimum in that instability index, so the instability is the lowest around 5.",
            "Here you have an inappropriate model chosen.",
            "I think that is K mean and it gives you a minimum around 7:00.",
            "But if you use if you use a connectedness type criterion and we used a path based clustering procedure which basically looks at.",
            "It passed in terms of measuring how well 22 objects belong together.",
            "Then you get a very clear minimum with zero variance on these empirical data.",
            "For these three different drinks.",
            "So these stability criterion measures together the fluctuation coming from all the instability coming from the fluctuation in the data as well as the instability from a model mismatch.",
            "Chanpur models.",
            "Seem not to capture the properties of the data, and if they don't capture the properties of the data, they might be sort of describing the groups in awake way, which which favors instability.",
            "Now this proves, and here you have the."
        ],
        [
            "Predicted labels and it's not so clear.",
            "Doctors might even fight over having two different classes of cancer or three, so this uncertainty is captured by that validation methods, basically giving you large error bars on many different resampling's and slightly better instability value here 4K equal 3, so we would not get too much money on three rather than two, but.",
            "Give you the mixture back here."
        ],
        [
            "For these data you have the situation that you have a minimum here at.",
            "I think this is 3 and a minimum at 9 and that roughly corresponds to what people know about this data in the 1st place.",
            "So they are biologically possible."
        ],
        [
            "OK, let me summarize this part and with not too much.",
            "Running over the time, I would say in in segmentation the big learning issue is to validate the solutions.",
            "While you have vastly different criteria to come up with the partitioning, some of these criteria are fairly close to what we are now.",
            "What we know from the statistical learning theory.",
            "Area in particular K means has the structure that, given the statistics, the centroids, we have a very simple optimization procedure to produce a new class string.",
            "We basically use the nearest neighbor rule and that gives me kiss me on the next test set of objects and new labeling for many of these.",
            "Also widely used methods like normalized cut or average cut and another graph based.",
            "Approaches of partitioning.",
            "It's not at all clear how I take a solution from the training set and produce a new partitioning on the test set on the test image for Markov random field type approaches you have these local coupling.",
            "It's also not clear how you can use use.",
            "A segmentation learned on one image and transfer it to a second image and the ultimate challenge, I think in that context is to use shape information so we know a Tiger.",
            "I give you 5 Tiger images.",
            "By that time you should have learned that Tigers have four legs, a tail end and the head and that sort of spans a possible universe of shapes, and these shapes should be learned, and that's something, at least where I believe that humans are very well.",
            "Able to transfer that image that that type of shape information to a second image.",
            "But we don't really know first how to describe shape and 2nd if we would have a shape.",
            "Description How we actually can take that.",
            "Learn it on one set of data and then use that for finding efficient solutions on the second data set.",
            "So to summarize here, no additional assumptions are made by our approach.",
            "The weakness of that approach is it does measure instability, but it does not make the trade of explicit between instability.",
            "On one hand of the solution and informativeness of a solution.",
            "So maybe I'm I'm willing to accept a little bit more instability when I have one more cluster, because one more class is also more informative, and ultimately I believe that this has to be traded off one against the other one.",
            "And that is certainly something which is opening that in that cluster validation business.",
            "And by that I would like to end here.",
            "Thank you very much.",
            "Running a bit overtime, but there any quick questions.",
            "No OK.",
            "I will break for let's say 20 minutes and come back.",
            "At.",
            "Changing meeting schedule.",
            "Is it sorry?",
            "Oh looking you have to set up your stuff now.",
            "Who?",
            "Well, why don't we break here but break for 15 minutes only.",
            "Will come back."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It sounds separately, but welcome to the workshop.",
                    "label": 0
                },
                {
                    "sent": "The idea is to discuss a little bit the application of learning and computer vision.",
                    "label": 0
                },
                {
                    "sent": "We have a set of tutorials to start with on vision issues related to learning, so I think I'll just start by welcoming Jochum and thanking him for coming and he's going to talk to us about image segmentation.",
                    "label": 0
                },
                {
                    "sent": "OK, ladies and gentlemen, what I would like to do is to.",
                    "label": 0
                },
                {
                    "sent": "To summarize up some ideas on image segmentation, and in particular to relate it to one of the long term interests of my group on data clustering so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is based on a tutorial which I gave at CPR and I rearrange some of the stuff and hope that now the learning issues are becoming clear and I would also like to point out what other challenges from my point of view, the challenges for consultum like Pascal which brings together expertise now in particular in that in that setting on computer vision on one hand and learning classification regression on the other hand.",
                    "label": 0
                },
                {
                    "sent": "So the problem of data class ring is 1 formalization.",
                    "label": 1
                },
                {
                    "sent": "You could you could use for image segmentation and to sort of layout.",
                    "label": 0
                },
                {
                    "sent": "The mathematical framework is you have a set of objects and these objects are local descriptors, usually of an image and you might represent them as point in Euclidean space, feature space and then you have the clustering problem of putting those objects together which are similar in some respect.",
                    "label": 0
                },
                {
                    "sent": "So the segmentation paradigm would then be to have pixels or small image blocks or something like that put together to a group when you believe that they have something in common.",
                    "label": 0
                },
                {
                    "sent": "And obviously that is an inference step from the world of of, you know.",
                    "label": 0
                },
                {
                    "sent": "Identifying objects in an image.",
                    "label": 1
                },
                {
                    "sent": "So the number of clusters, how many of these groups you have is usually predefined.",
                    "label": 0
                },
                {
                    "sent": "That's one of the challenges actually to get rid of this assumption that you have to know apriori how many of these classes you have and what is also given in many contexts is the similarity measure.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "What is the function which you have to use to compare these these different objects which are denoted by X1 to XN?",
                    "label": 1
                },
                {
                    "sent": "So that is the clustering problem and I will go through a number of those clustering problems unrelated to on one hand to what are the learning issues and on the other hand, what how that can be used for for image segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the image segmentation and please feel free to interrupt whenever you have the feeling that that you are lost or something is not clear.",
                    "label": 0
                },
                {
                    "sent": "We can skip parts of the of the tutorial without too much loss for the overall presentation.",
                    "label": 0
                },
                {
                    "sent": "The image segmentation problem is is defined like this.",
                    "label": 1
                },
                {
                    "sent": "You have an image and according to some criteria you put together objects which you believe that they belong to each other.",
                    "label": 0
                },
                {
                    "sent": "So these are regions containing the similar pixels that is color segmentation approach to partition that object.",
                    "label": 0
                },
                {
                    "sent": "And you might actually define then the segments on the basis of the area characteristics.",
                    "label": 0
                },
                {
                    "sent": "Or you could also say, well, segmentation is basically to give you a list of boundaries and these boundaries then have to have certain constraints in the constraints that they form closed areas.",
                    "label": 0
                },
                {
                    "sent": "In the image I should also say that most people think that image segmentation as well as data clustering is not a very well defined mathematical problem.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "There is some truth to it.",
                    "label": 0
                },
                {
                    "sent": "You will also see it with a wide variety of different approaches to clustering and image segmentation.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I think it's still valid to have the partitioning of an image like this at an intermediate level into into different groups.",
                    "label": 0
                },
                {
                    "sent": "It's a valid approach to sort of condense information in the image and to bridge the gap a little bit from.",
                    "label": 0
                },
                {
                    "sent": "The low level type of information up to the high level semantic problem where you really ask what kind of objects are in the image here.",
                    "label": 0
                },
                {
                    "sent": "So the semantics problem.",
                    "label": 0
                },
                {
                    "sent": "That's what I have put down here.",
                    "label": 0
                },
                {
                    "sent": "How should we infer objects from segments that's out of the key, and I guess for the Pascal Group One of the big challenges and where I think the group could make a big contribution to their research field is too.",
                    "label": 1
                },
                {
                    "sent": "To get some more insights, how much of image segmentation we have to solve in order to have robust object recognition on one hand or maybe robust action planning for some applications like robotics or other control applications.",
                    "label": 0
                },
                {
                    "sent": "It's not clear if you have to go through image segmentation.",
                    "label": 0
                },
                {
                    "sent": "Some people feel that image segmentation might be a too hard of a problem because it's not very well defined and I remind you.",
                    "label": 0
                },
                {
                    "sent": "That vapnik was always pushing the point that you should not solve problem, which is actually more common intermediate problem, which is more complicated than your final goal.",
                    "label": 0
                },
                {
                    "sent": "So it could very well be that image segmentation because it's so well so so so wake and not very well defined is a much harder problem then ultimately to classify for a particular domain.",
                    "label": 0
                },
                {
                    "sent": "The types of objects which are depicted in an image and I guess in for example in face recognition.",
                    "label": 0
                },
                {
                    "sent": "The observation was that it's much better not to segment the image, at least at the point of state of the art which we have right now.",
                    "label": 0
                },
                {
                    "sent": "Then just start classifying the image right away.",
                    "label": 0
                },
                {
                    "sent": "OK, the class.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "During approach to image segmentation, you have some some objects pixels you want to classify them according to color, grayscale values, local descriptors like histograms and the method is to apply an appropriate grouping algorithm to the image data and the result then is I just said these segments connected regions assigned to the same cluster and obviously we then make the inference that these regions have something to do.",
                    "label": 0
                },
                {
                    "sent": "With an object out there in the in the world, and either it's a part of that object or is the object itself and we want to actually label them the segment as some.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the category which belongs to that object OK, data set here is XX, one to XN and groups are denoted by C1 to CK.",
                    "label": 0
                },
                {
                    "sent": "These are the clusters which contain all the objects belonging to one of these clusters and two sets of the mathematical framework.",
                    "label": 0
                },
                {
                    "sent": "We have to find a mapping function.",
                    "label": 0
                },
                {
                    "sent": "All these assignment vectors C which basically tells you.",
                    "label": 0
                },
                {
                    "sent": "To which class to see Alpha a particular data point XI actually belongs to and then you code that information in CI in the variable CI equals 2A.",
                    "label": 0
                },
                {
                    "sent": "That is a discrete search problem.",
                    "label": 0
                },
                {
                    "sent": "We have an exponentially large search space.",
                    "label": 0
                },
                {
                    "sent": "Set 1 two K to the power of N denotes this discrete space, or you can view these segmentation problem or the clustering problem is a combinatorial optimization problem and we know how to solve that in some instances and I will come round later to that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have to find these assignment vectors.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A point which I stressed for quite some time and many people did the same is to formulate these these grouping problems as optimization problems.",
                    "label": 0
                },
                {
                    "sent": "Basically to layout a cost function which tells you how good a particular assignment choice of image blocks two groups actually is and that is sort of the conceptual framework.",
                    "label": 0
                },
                {
                    "sent": "I would like to pursue.",
                    "label": 0
                },
                {
                    "sent": "In that tutorial I should also say that not everybody is actually adhering to that framework.",
                    "label": 0
                },
                {
                    "sent": "There are some people who invent algorithms just from scratch, and then they start analyzing these algorithms and later on you will realize that these algorithms, not minimizing a particular cost function.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you can even prove that there does not exist the cost function which is actually minimized by these algorithms.",
                    "label": 0
                },
                {
                    "sent": "Now how well it's such an approach?",
                    "label": 0
                },
                {
                    "sent": "Is it completely different question?",
                    "label": 0
                },
                {
                    "sent": "What I believe is that in analogy to classification and regression, you have a much cleaner framework when you actually write down a cost function which is optimized by a particular approach and from a practical point of view, I strongly recommend in doing that.",
                    "label": 0
                },
                {
                    "sent": "And I actually also believe that this is sort of the quote, unquote best way to attack a pattern recognition problem because in the end of the day.",
                    "label": 0
                },
                {
                    "sent": "You have some solutions and you have to compare them and you better have a function which allows you to do that, although many of the search touristics might not actually follow such a stringent criterion, so the cost function is denoted by age.",
                    "label": 0
                },
                {
                    "sent": "That is sort of.",
                    "label": 0
                },
                {
                    "sent": "A relic from from my future education in physics and it's called cost function or risk in statistics or Hamiltonian in the more engineering or physics type literature and it basically Maps configuration.",
                    "label": 0
                },
                {
                    "sent": "A segmentation solution to the positive reels and these.",
                    "label": 0
                },
                {
                    "sent": "The number then tells you how good that solution is.",
                    "label": 0
                },
                {
                    "sent": "And obviously then you have you have sort of a connotation coming with it.",
                    "label": 0
                },
                {
                    "sent": "Assignment of dissimilar objects, things which really should not belong to the same segment will produce high cost.",
                    "label": 1
                },
                {
                    "sent": "This is an unfavorable solution for it.",
                    "label": 0
                },
                {
                    "sent": "In the algorithmic, that is sort of the ladies sort of the modeling framework which you have to design and coming up with a particular cost function allows you to really specify from an application's point of view what is important in a particular application.",
                    "label": 0
                },
                {
                    "sent": "What do you care often?",
                    "label": 0
                },
                {
                    "sent": "What are sort of almost arbitrary selections which should not enter the evaluation of particular clustering?",
                    "label": 0
                },
                {
                    "sent": "Solutions all that can be can be sort of encoded in that function age.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then after you have sort of decided what cost function is appropriate for your application, you find a good solution to these cost functions and here here comes the first sort of.",
                    "label": 0
                },
                {
                    "sent": "The first, I think conceptual challenge for the pattern recognition community for the learning community as well.",
                    "label": 0
                },
                {
                    "sent": "Usually what we did in vision is we we.",
                    "label": 0
                },
                {
                    "sent": "We took that function age depending on the assignment C conditioned on your data X and we found then the best possible partitioning.",
                    "label": 0
                },
                {
                    "sent": "See star.",
                    "label": 0
                },
                {
                    "sent": "Now if you bring that in contact with let's say classification, then the analogy would be simply the following age is your empirical risk.",
                    "label": 0
                },
                {
                    "sent": "You have an image and then you minimize the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "For that particular image and see style, then is the best possible.",
                    "label": 0
                },
                {
                    "sent": "What you can do on your training data.",
                    "label": 0
                },
                {
                    "sent": "However, we should also be warned that in classification that is not the correct strategy because in the end in classification what you are paying for is the expected risk.",
                    "label": 0
                },
                {
                    "sent": "So I strongly believe that we as a community have to extend these notions which are very clear in classification.",
                    "label": 0
                },
                {
                    "sent": "And regression to more complicated problems like the bands coming up in computer vision and there it's by no means so clear.",
                    "label": 0
                },
                {
                    "sent": "What is the expected risk for segmentation?",
                    "label": 0
                },
                {
                    "sent": "So when I right now and see star is the minimum, the argument of the minimum of age, I'm sort of shortcutting the full problem of inference which might be behind an image segmentation task because I'm now saying by writing that down I want to get the minimum of the empirical risk, but maybe that's not what I really want.",
                    "label": 0
                },
                {
                    "sent": "Maybe I want to have the minimum of some expected risk and I don't really know what that is.",
                    "label": 0
                },
                {
                    "sent": "OK, let's just summarize we have two basic problems.",
                    "label": 1
                },
                {
                    "sent": "The choice of the cost function, the algorithmic optimization, and the first one is a strongly application driven.",
                    "label": 1
                },
                {
                    "sent": "The Chinese which we have to make here, and I think you can find a lot for the first point and you know different approaches, sort of tend to emphasize different properties of an application area, and that's why people don't get a unique answer to the first one.",
                    "label": 0
                },
                {
                    "sent": "But then after you have settled foreign cost function H, It's a clear combinatorial it's.",
                    "label": 0
                },
                {
                    "sent": "It's an informatics problem, you just find a solution and.",
                    "label": 0
                },
                {
                    "sent": "You ask your favorite guy from from from applied mathematics to give you an optimization algorithm.",
                    "label": 1
                },
                {
                    "sent": "By strictly separating these two design steps, I think you can.",
                    "label": 0
                },
                {
                    "sent": "You can sort of separate the model design phase from the algorithmic issues, which is very valuable when you when you look for for new.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Allusions to to your data analysis problems.",
                    "label": 0
                },
                {
                    "sent": "There's a tradeoff.",
                    "label": 0
                },
                {
                    "sent": "You know most of you have experienced that simple cost functions are easy to optimize, but they are not capturing all the all the details of your complex problem at hand.",
                    "label": 1
                },
                {
                    "sent": "If you have a complex cost function, usually it's hard to optimize.",
                    "label": 0
                },
                {
                    "sent": "It might have many local minimize you get recipes from all over the world, how to improve that, and in the end you are not really sure.",
                    "label": 0
                },
                {
                    "sent": "What what you should pursue and we all live in this in the spectrum of complicated and simple models and introspective problems with with with the image interpretation.",
                    "label": 0
                },
                {
                    "sent": "One particular simple example is that you take a cost function and it's just the sum of individual contributions from object I and this contribution.",
                    "label": 0
                },
                {
                    "sent": "This cost contribution is.",
                    "label": 0
                },
                {
                    "sent": "Captured here by the functional described by the function CI conditioned on XI that is actually sort of a cost function which which calculates the clustering solution or the segmentation solution separately for every object and then adds up.",
                    "label": 0
                },
                {
                    "sent": "Two to get the cost for the complete configuration.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Extensions incorporate neighborhood information in cluster assignments.",
                    "label": 1
                },
                {
                    "sent": "It's fairly clear that when you have a situation like this, you know these spirals.",
                    "label": 0
                },
                {
                    "sent": "They already color coded for you so that you know what I mean by groups here.",
                    "label": 0
                },
                {
                    "sent": "Then you want to have some some more refined.",
                    "label": 0
                },
                {
                    "sent": "Properties which are sort of more sophisticated, more sophisticated criterion to really define a group, and it's no longer closeness in that embedding space in the 2D space.",
                    "label": 0
                },
                {
                    "sent": "It's sort of being being in a high density area, and if that high density area is sort of.",
                    "label": 0
                },
                {
                    "sent": "Part of a spiral.",
                    "label": 0
                },
                {
                    "sent": "Then you want to have one of these arms of the spiral denoted at one of the is one of the segments.",
                    "label": 0
                },
                {
                    "sent": "Here this this example is chosen to make clear that an object wise evaluation of costs is not sufficient.",
                    "label": 1
                },
                {
                    "sent": "You have to have a more sophisticated cost function to help handle a situation like this, and actually we have.",
                    "label": 0
                },
                {
                    "sent": "We have proposed a model for this spiral example couple of years ago and you see there are still some errors in here, so so one of these green dots here.",
                    "label": 0
                },
                {
                    "sent": "Actually, do I have a laser pointer here?",
                    "label": 0
                },
                {
                    "sent": "So one of the of the green dots belongs to that spiral over there an it's.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, I mean this.",
                    "label": 0
                },
                {
                    "sent": "I mean this data point here.",
                    "label": 0
                },
                {
                    "sent": "You would probably say this should be blue.",
                    "label": 0
                },
                {
                    "sent": "It's a particularity of that model that it's green.",
                    "label": 0
                },
                {
                    "sent": "But don't worry about it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to to sort of set the landscape of Avoyelles where we position these segmentation problems in the classroom.",
                    "label": 0
                },
                {
                    "sent": "Problems, let me distinguish between two types of sort of extreme two types of problems, learning problems.",
                    "label": 1
                },
                {
                    "sent": "Supervised and unsupervised problems.",
                    "label": 0
                },
                {
                    "sent": "In supervised problems you have methods which are trained on sampled off label examples, so at least on some special cases you know what should come out.",
                    "label": 1
                },
                {
                    "sent": "You train, then you have the problem of noise in your data and then you have to make a trade of how much you believe the examples in your in your training set and how much you want to generalize to new cases.",
                    "label": 0
                },
                {
                    "sent": "Plastering belongs to a different setting in clustering.",
                    "label": 1
                },
                {
                    "sent": "Nobody tells you what the correct segments are, it depends on your application and you have to specify sort of a cost function which gives you a guide installed solution and these cost function age is the input which distinguishes between good clustering solutions and bad ones.",
                    "label": 0
                },
                {
                    "sent": "In a sense you have a different cost function for the supervised methods in the unsupervised methods in at that level of abstraction.",
                    "label": 0
                },
                {
                    "sent": "I think one can compare.",
                    "label": 0
                },
                {
                    "sent": "Here and see what is really necessary in order to learn something.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So problem specification what is given in data clustering.",
                    "label": 1
                },
                {
                    "sent": "I said it already.",
                    "label": 0
                },
                {
                    "sent": "In classification, your partition, your object space, and.",
                    "label": 0
                },
                {
                    "sent": "You have these training objects from the supervisor and the key problem is how to generalize the partition to new objects so it's fairly clear what you would like to achieve in.",
                    "label": 1
                },
                {
                    "sent": "In classification you would want to make correct classification of future and future examples.",
                    "label": 0
                },
                {
                    "sent": "So in class string it's not so clear how you actually.",
                    "label": 0
                },
                {
                    "sent": "How you actually pursue these training?",
                    "label": 1
                },
                {
                    "sent": "These testing step here.",
                    "label": 0
                },
                {
                    "sent": "You have unsupervised partitions of objects of the object space and equality criterium.",
                    "label": 0
                },
                {
                    "sent": "And how do you optimize the cluster criterion?",
                    "label": 0
                },
                {
                    "sent": "That is sort of the challenge here.",
                    "label": 0
                },
                {
                    "sent": "But there is also.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And generalization issue in unsupervised learning problems and I will come later to that.",
                    "label": 0
                },
                {
                    "sent": "Now you have these notions which are clear in classification.",
                    "label": 1
                },
                {
                    "sent": "There is underfitting sort of step in between and there is overfitting.",
                    "label": 0
                },
                {
                    "sent": "And when I, when I alluded to these notions of.",
                    "label": 0
                },
                {
                    "sent": "Of classification then what I basically meant is that on the training data, the blue and the red dots here this complicated boundary looks fairly OK. You know none of the Blues on the wrong side, and another red on the wrong side.",
                    "label": 0
                },
                {
                    "sent": "However, you would not put your personal money on this classifier.",
                    "label": 0
                },
                {
                    "sent": "I'm sure otherwise let me know afterwards, I will bet against you and.",
                    "label": 0
                },
                {
                    "sent": "So this is the overfitting situation.",
                    "label": 0
                },
                {
                    "sent": "I make too much.",
                    "label": 0
                },
                {
                    "sent": "I extract too much information from my data and I will be penalized on future tests of that of that classifier.",
                    "label": 0
                },
                {
                    "sent": "And I guess it's fair to say that this is the key problem in machine learning and it sort of relates what I believe two of the most deep issues in computer science there's the computational complexity involved, which has been put on the on the sort of the research landscape by people from computer science, in particular Valiant and the called communities.",
                    "label": 0
                },
                {
                    "sent": "So that is computational learning theory on one hand.",
                    "label": 1
                },
                {
                    "sent": "And you have these statistical issues.",
                    "label": 0
                },
                {
                    "sent": "The statistical learning theory problems.",
                    "label": 0
                },
                {
                    "sent": "How much can you say on the basis of noisy data and how fast can your how efficiently can you say that?",
                    "label": 0
                },
                {
                    "sent": "And the relation here is not very well understood.",
                    "label": 0
                },
                {
                    "sent": "So far, and I think this key problem in machine learning classification will come up also in applications like object recognition like segmentation, but it comes up in a more complex way and we learn a lot if we start identifying what is the generalization problem in vision in division setting.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let me see I have to rush a little bit, otherwise these 75 will not be reached.",
                    "label": 0
                },
                {
                    "sent": "The structure is.",
                    "label": 0
                },
                {
                    "sent": "I talked a little bit about these basic concepts and then I will go through a number of clustering approaches or segmentation approaches and then I will start to say a little bit about optimization, much less than what I presented at the CPR tutorial.",
                    "label": 0
                },
                {
                    "sent": "And I would like to spend a little bit more time on validation of clustering solutions, because I think the validation issue is absolutely critical.",
                    "label": 0
                },
                {
                    "sent": "First of all, because our customers really want to have algorithmic solutions which work well on future data.",
                    "label": 0
                },
                {
                    "sent": "You know in all of the industrial projects I pursued so far, the robustness of the methods was a key ingredient the engineers asked for, and that actually cuts down there.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Development time, so if you have a better understanding what it means, I have a robust solution for an image interpretation task.",
                    "label": 0
                },
                {
                    "sent": "Then I think we learn a lot about the generalization in their context.",
                    "label": 0
                },
                {
                    "sent": "There are a couple of approaches for clustering in particular, and I will focus on the last one because it comes from my group.",
                    "label": 0
                },
                {
                    "sent": "It's stability analysis of.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solutions and who doesn't want to have stable solutions?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So data types you will find clustering approaches for quite different data types.",
                    "label": 0
                },
                {
                    "sent": "Vector data probably used in more than 90% of the applications.",
                    "label": 1
                },
                {
                    "sent": "If you are sure that you know in which space you have to represent your objects, then just build a vector space for these objects and then start doing your grouping there.",
                    "label": 0
                },
                {
                    "sent": "Another approach, very successful and pursued by by Talley Tishby and his group in Israel and also by.",
                    "label": 0
                },
                {
                    "sent": "My group is to use histogram data to find good groups and.",
                    "label": 0
                },
                {
                    "sent": "A third approach which I want to go into is the proximity data approach, where you no longer represent individual objects, either by vectors or by histograms, but you characterize objects by their pairwise relation proximity data or dissimilarity.",
                    "label": 0
                },
                {
                    "sent": "Doesn't really matter if you keep track of the sign.",
                    "label": 0
                },
                {
                    "sent": "Obviously the last representation, the proximity data representation here.",
                    "label": 1
                },
                {
                    "sent": "Is a harder problem because the structure now is hidden in pairwise relations between your objects.",
                    "label": 1
                },
                {
                    "sent": "You have a square matrix which might actually have missing values and so on.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, there's a lot of sort of.",
                    "label": 0
                },
                {
                    "sent": "Classification of the different clustering principles in the literature.",
                    "label": 0
                },
                {
                    "sent": "I would like on a meta level, sort of two to point you to some criteria which you could actually use to see if sort of several methods basically do the same thing, despite the fact that they are written up in completely different ways.",
                    "label": 0
                },
                {
                    "sent": "So compactness is one of the criteria which allows you to find groups of objects and depending on how you represent your objects in which underlying space you represent them.",
                    "label": 0
                },
                {
                    "sent": "You might actually use a method called K means I will come to that about probably most of you know that already if you represent your objects by histograms, then you could use a method called histogram clustering an it does roughly the same as K means, except in a different space.",
                    "label": 0
                },
                {
                    "sent": "It's the space of probability distributions here and here.",
                    "label": 0
                },
                {
                    "sent": "It's the space of vectors, paralyzed data clustering for for relational data for proximity data achieves also.",
                    "label": 0
                },
                {
                    "sent": "A compactness type of of grouping of your objects.",
                    "label": 0
                },
                {
                    "sent": "Average Association is a different name for pairwise data clustering.",
                    "label": 1
                },
                {
                    "sent": "Focal road and and people in my group has looked at at connection between pairwise data clustering and K means and it's called the constant shift embedding paradigm which really makes explicit how you go from relational data to vectorial data.",
                    "label": 0
                },
                {
                    "sent": "And here is yet another method which is which is.",
                    "label": 0
                },
                {
                    "sent": "Fairly similar to histogram clustering, so I will basically show you examples of this method.",
                    "label": 0
                },
                {
                    "sent": "Little bit of that one, and K means now you can, rather than stressing compactness in some underlying space, but you could also do is you could say, well here is a group and here is a group and let's find a partitioning such that if I if I now specify this As for Group One and this is Group Two, let the separation between these groups is particularly large.",
                    "label": 0
                },
                {
                    "sent": "And methods which go in that direction.",
                    "label": 0
                },
                {
                    "sent": "And now the laser pointer seems to.",
                    "label": 0
                },
                {
                    "sent": "To become weak methods which are discussed in the in the vision community on the average cut, I think that was a proposal couple of years ago from the Hebrew Group and Malik's proposal, normalized cut.",
                    "label": 0
                },
                {
                    "sent": "They fall into that class.",
                    "label": 0
                },
                {
                    "sent": "They basically stress the separation of groups rather than the compactness.",
                    "label": 0
                },
                {
                    "sent": "However, these are fairly similar concepts, although they look different and you will see that when you analyze the mathematically.",
                    "label": 0
                },
                {
                    "sent": "In a different class, I think belong these methods which are connectedness methods.",
                    "label": 0
                },
                {
                    "sent": "They are methods which allow you to handle situations like like the spiral case.",
                    "label": 0
                },
                {
                    "sent": "There is in particular an algorithm going back to Peter Meyers Group mean Shift clustering which which is sort of, you know, has features of compactness and has features of connectedness.",
                    "label": 0
                },
                {
                    "sent": "Single linkage is the old method.",
                    "label": 0
                },
                {
                    "sent": "Which does the connected connectedness analysis and we have an approach which falls into the same class but is more robust than single linkage.",
                    "label": 0
                },
                {
                    "sent": "I will not go to into that that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something which I sort of pruned away when I put together the tutorial vectorial data.",
                    "label": 0
                },
                {
                    "sent": "You know that raw data here you have the labels color coded, and that's what you want to.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To find out, you do that by K means.",
                    "label": 0
                },
                {
                    "sent": "You have your D dimensional vectors X one to XN.",
                    "label": 0
                },
                {
                    "sent": "You want to specify the assignment vector and the prototypes sort of the prototype in that context is the information you can learn about the clustering.",
                    "label": 0
                },
                {
                    "sent": "The nature of your class string and that's something you could actually then take from one instance of the clustering problem to the second instance of the clustering problem, and then try to execute your clustering solution or new data in K means.",
                    "label": 0
                },
                {
                    "sent": "Let's fairly straightforward how that would go.",
                    "label": 0
                },
                {
                    "sent": "What you then do is you use the nearest neighbor rule when you transfer your prototypes from the training instance to the test instance.",
                    "label": 0
                },
                {
                    "sent": "And the seas are in the nearest neighbor assignments.",
                    "label": 0
                },
                {
                    "sent": "You put you put that together in a cost function and the cost function is given down here you some overall data and whenever data point XI belongs to cluster C Alpha then these assignment function CI is equals 2A and you have to pay the distortion between your vector and the prototype in terms of in terms of distortion costs.",
                    "label": 0
                },
                {
                    "sent": "This is a mix combinatorial and continuous optimization problem.",
                    "label": 1
                },
                {
                    "sent": "The continuous parameters are the centroids.",
                    "label": 0
                },
                {
                    "sent": "The discrete variables are the seas.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, you know that you iterate.",
                    "label": 0
                },
                {
                    "sent": "You have the following iterative update scheme.",
                    "label": 0
                },
                {
                    "sent": "You fix your prototypes first, then you find the assignments according to the nearest neighbor rule.",
                    "label": 0
                },
                {
                    "sent": "Then you fix the assignments and you re estimate the position of the prototypes.",
                    "label": 0
                },
                {
                    "sent": "By using these the mean value function, and then you go through that until you find a local minimum of the K means cost function and there are various ways how you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How you can actually, you can actually refine the algorithm so that it gets less sensitive to local minima which.",
                    "label": 0
                },
                {
                    "sent": "Abundant for that type of cost function.",
                    "label": 0
                },
                {
                    "sent": "How would you apply K means actually to image segmentation?",
                    "label": 0
                },
                {
                    "sent": "We have done that with Landsat images.",
                    "label": 0
                },
                {
                    "sent": "Here you have a Landsat image.",
                    "label": 0
                },
                {
                    "sent": "Every pixel here is a vector in a 6 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "You project it down to two dimensions and you see already some structure what you essentially do is you find these partitioning functions C in the six dimensional space and then you color code the image according to the the.",
                    "label": 0
                },
                {
                    "sent": "Classes which we have found here and basically we have selected now 13 different groups in here color coded and then you go to your favorite remote sensing person.",
                    "label": 0
                },
                {
                    "sent": "Ask him or her whether this is a good solution and in many application that works nicely.",
                    "label": 0
                },
                {
                    "sent": "We have implicitly made the assumption that all the pixels line independent because we don't have any constraints locali so that neighboring pixels.",
                    "label": 0
                },
                {
                    "sent": "Actually Hood should have should belong with a higher probability to the same class and that's why you get this special type noise in here.",
                    "label": 0
                },
                {
                    "sent": "Now in most image processing tasks you would like to have these local coupling.",
                    "label": 0
                },
                {
                    "sent": "But the local coupling then would raise these serious problem.",
                    "label": 0
                },
                {
                    "sent": "What it actually means to do generalization.",
                    "label": 0
                },
                {
                    "sent": "So if I have this solution here and I get from the satellite and you image what I basically would do is I would take my 13 different centroids in that representation.",
                    "label": 0
                },
                {
                    "sent": "An I would label all the pixels according to the closest centroid and that would generalize would produce a new solution.",
                    "label": 0
                },
                {
                    "sent": "An if I have, I have good centroids then probably the labeling of the new image is then satisfactory for the application domain.",
                    "label": 0
                },
                {
                    "sent": "Now if I have a smoothness constraint in here, it's not so clear how I would actually transfer my my learn solution on my training image to the test image and we had a lot of discussions in the group.",
                    "label": 0
                },
                {
                    "sent": "How you can actually formulate in such a context the generalization problem and I think this is one of the questions we have to address in the past car meeting.",
                    "label": 0
                },
                {
                    "sent": "So if I sort of mix all these remarks.",
                    "label": 0
                },
                {
                    "sent": "Uh, in the tutorial, put it in the on the side.",
                    "label": 0
                },
                {
                    "sent": "I haven't separated what I mean by research issues and what I mean by tutorial material.",
                    "label": 0
                },
                {
                    "sent": "I just want to present it as it as it comes and you should interrupt me whenever you have the feeling.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's getting too sketchy.",
                    "label": 0
                },
                {
                    "sent": "There's a.",
                    "label": 0
                },
                {
                    "sent": "There's a face ification of probabilistic version of K means it comes under the label of mixture models where you can also change the metric for your individual groups.",
                    "label": 0
                },
                {
                    "sent": "And here you have a solution to such a mixture problem.",
                    "label": 0
                },
                {
                    "sent": "Conceptually it's very similar to K means, and in particular for the learning issues I think it falls in the same class.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the second model I would like to discuss is sort of the K means version.",
                    "label": 0
                },
                {
                    "sent": "But not in a vectorial space in the space of probability distributions you can formulate the same question with a cost function which has very similar properties.",
                    "label": 0
                },
                {
                    "sent": "In that context an for image segmentation actually gives you quite good results.",
                    "label": 0
                },
                {
                    "sent": "So what we have here is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have a grid and we want to lay.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Label the grid points so at every grid point you take a local statistics, maybe a local histogram of the Gray values, or the color values that something we have pursued, or you have the local statistics or Fourier coefficients.",
                    "label": 0
                },
                {
                    "sent": "When you filter your image with Gabor filters of different orientation different sizes, then you have texture descriptors locally and you can actually characterize gridpoint by these local statistics of of your image contact.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you classify it on the basis of that information which is extracted.",
                    "label": 0
                },
                {
                    "sent": "And here you see that that you know the lighter point sign in labeled with rat here.",
                    "label": 0
                },
                {
                    "sent": "Here you have high frequency content.",
                    "label": 0
                },
                {
                    "sent": "This is a synthetic aperture radar image.",
                    "label": 0
                },
                {
                    "sent": "That's why you have so much spectral noise in here and here you have a forest area and that's why you get the different texture characteristics here.",
                    "label": 0
                },
                {
                    "sent": "So these are all nodes.",
                    "label": 0
                },
                {
                    "sent": "the Blues are nodes in the in the forest area.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea is now the following probabilistic model and those from you in the learning community will recognize it.",
                    "label": 0
                },
                {
                    "sent": "It's a mixture model.",
                    "label": 0
                },
                {
                    "sent": "Here you have a segment knew and your data X and the probability distribution of observing data given your segment is depicted like, here is a gamma distribution that's particularly appropriate for remote sensing.",
                    "label": 0
                },
                {
                    "sent": "Then you have an image and you take your statistics in one of these blocks.",
                    "label": 0
                },
                {
                    "sent": "And what you observe is a local histogram.",
                    "label": 0
                },
                {
                    "sent": "And obviously this histogram has to compare it with the prototypical distribution for a particular segment here.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea behind these these parametric distribution and clustering.",
                    "label": 0
                },
                {
                    "sent": "That's actually what PDC stands for.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you have.",
                    "label": 0
                },
                {
                    "sent": "A little bit of an animation down here you see already.",
                    "label": 0
                },
                {
                    "sent": "We want to model these P of X given you as a mixture model with Alpha going through a set of different modes here and this.",
                    "label": 0
                },
                {
                    "sent": "G Alpha is the basis function of Gaussian or.",
                    "label": 0
                },
                {
                    "sent": "Truncated Gaussian or some other some other basis function which is appropriate for your data, so here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have a different weighting of these basis functions and that gives you the first mode, the film.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Histogram, so to speak, and that's something we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have learned here now.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this is the distribution for the first cluster in green you have the weighting of the base.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These functions for the second cluster, you sum it up.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you get as the prototypical distribution for the second cluster?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same for the third.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just click through the whole.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Animation and that that are then the prototypical distributions sort of your prototype.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But these are now prototypes in the space of probability distribution and this method at least this segmentation techniques works much better than than K means because in K means you have these implicit assumption that Euclidean distances actually mean something for your, for your application, and in many cases that's not the case.",
                    "label": 0
                },
                {
                    "sent": "That is not a true statement.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The generative model is given here from your objects, labeled by I.",
                    "label": 1
                },
                {
                    "sent": "You go to your segments labeled by new by.",
                    "label": 0
                },
                {
                    "sent": "By choosing these assignments here and then, you go to your local descriptors.",
                    "label": 0
                },
                {
                    "sent": "And I J which are the joint observation of some features J with your object I that could be for example different colors, different feature, different texture descriptors in a particular window, and that's what you want to model with these prototypical distribution of the data X.",
                    "label": 0
                },
                {
                    "sent": "The feature vectors X.",
                    "label": 0
                },
                {
                    "sent": "Given this segment new.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the mathematics coming with it.",
                    "label": 0
                },
                {
                    "sent": "Here you have a prior.",
                    "label": 0
                },
                {
                    "sent": "Here you have the likelihood of X given the assignment C and some other variable fee to these Theta.",
                    "label": 0
                },
                {
                    "sent": "For example denote the centroid of your of your modes.",
                    "label": 0
                },
                {
                    "sent": "Here of these G and it might also give you the mixture coefficients P, Alpha given segment CI.",
                    "label": 0
                },
                {
                    "sent": "And then you have these mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Let's let's use tree for a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "These mixtures of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Here you make your observations how often a particular value has been observed with a with a different window.",
                    "label": 0
                },
                {
                    "sent": "I image Patch I then you have a product over all your different features J.",
                    "label": 0
                },
                {
                    "sent": "So you have to quantize your distribution.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you would have an integral here.",
                    "label": 0
                },
                {
                    "sent": "Infinite product here.",
                    "label": 0
                },
                {
                    "sent": "And you some you you, you take all your windows in product here over all your objects and if you take it together.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you define as cost function the negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Then what you get is a prior term.",
                    "label": 0
                },
                {
                    "sent": "Here you can forget about this one, but you have the joint.",
                    "label": 0
                },
                {
                    "sent": "The histogram of observing object I together with feature J you some overall features and you have these log of the mixture contribution here and this is again mixed continuous discrete optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The discrete part is given here.",
                    "label": 0
                },
                {
                    "sent": "By these assignments CI like in K means and what you have is here.",
                    "label": 0
                },
                {
                    "sent": "The mixing weights which have to be learned and.",
                    "label": 0
                },
                {
                    "sent": "Preferably also some of the characteristics of these of these Gaussians and you can interpret this as a two part coding scheme.",
                    "label": 0
                },
                {
                    "sent": "You want to minimize the expected code length when encoding the cluster membership and based on that information you encode the individual color values.",
                    "label": 1
                },
                {
                    "sent": "So from a mathematical point of view, this cost function has the same characteristics.",
                    "label": 0
                },
                {
                    "sent": "It's a mixed continuous discrete optimization problems and the assignment variables for object I are not coupled to the assignment variables for object J given the characteristics of your mixture coefficients and the Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So if you do that conditioning on your your statistics, the modes and the mixing variables then.",
                    "label": 0
                },
                {
                    "sent": "The assignments of different objects are independent and that is expressed by basing.",
                    "label": 0
                },
                {
                    "sent": "Basically the linearity.",
                    "label": 0
                },
                {
                    "sent": "Here the linearity of CI not being coupled to seed CI prime.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So tiny Tishby has interpreted this scheme as a bottleneck scheme, as finding an efficient code for your feature vectors, you have to find prototypic codebook, vectors X~ and the cost function, which I just have have presented on the previous.",
                    "label": 0
                },
                {
                    "sent": "Transparency can be written as a difference between two mutual information, and that's sort of the NYS interpretation in terms of.",
                    "label": 0
                },
                {
                    "sent": "In terms of information theoretic approach, our image blocks are the axis.",
                    "label": 0
                },
                {
                    "sent": "Here they are supposed to be compressed into segment descriptors.",
                    "label": 0
                },
                {
                    "sent": "These are the X still is here and what you want to preserve is with these these segment descriptors you want to preserve as much knowledge as possible about your features and the features might be the colors or the local frequencies.",
                    "label": 0
                },
                {
                    "sent": "So the wise encode your colors and local frequencies or other local descriptors.",
                    "label": 0
                },
                {
                    "sent": "And you sort of compress in a context, and that's the way Tally Tishby sells these ideas.",
                    "label": 0
                },
                {
                    "sent": "You compress your image patches in a context sensitive way so that the resulting description of your image patches is maximally informative with respect to the color content and.",
                    "label": 0
                },
                {
                    "sent": "The frequency content if you have selected Y as colors and frequencies.",
                    "label": 0
                },
                {
                    "sent": "Heal you.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have the same thing.",
                    "label": 0
                },
                {
                    "sent": "A pictorial describe this is the feature space.",
                    "label": 1
                },
                {
                    "sent": "This is the object space and you compress it, sort of in a reduced representation.",
                    "label": 0
                },
                {
                    "sent": "By fixing this constraint on the minimal amount of informativeness of your clusters, whereas here you want to minimize the mutual information to get generic descriptions of your objects, basically of your of your image patches.",
                    "label": 0
                },
                {
                    "sent": "I think this is a very nice concept from the interpretation point of view and it works successfully.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly successful in in in many different areas like document retrieval.",
                    "label": 0
                },
                {
                    "sent": "My first PhD student, Thomas Hoffman, has applied that in document retrieval with very good results.",
                    "label": 0
                },
                {
                    "sent": "the Hebrew group is using their concept for analyzing spike trains as well as Galaxy information, and they have some other applications in bioinformatics and we have used it in in vision and also in some areas of my info method.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the segmentation with PDC looks like this.",
                    "label": 0
                },
                {
                    "sent": "Here you have these image.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically on the on the basis of local color descriptors.",
                    "label": 0
                },
                {
                    "sent": "These are the types of segments you get and so for many application this is a good enough segmentation.",
                    "label": 0
                },
                {
                    "sent": "We are currently participating in a competition set up by the Mally Group.",
                    "label": 0
                },
                {
                    "sent": "They have a large number of hand labeled images and what you can do is you can basically use your favorite segmentation algorithm, run it against the database, and then do a precision recall analysis of your results compared to the ground truth, which is defined by by Berkeley undergraduate students.",
                    "label": 0
                },
                {
                    "sent": "So if you believe that is a good source for ground truth, then you can compare yourself against.",
                    "label": 0
                },
                {
                    "sent": "This database.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since it's a fully probabilistic model, you can do resampling, so this is the original image where we use which we used to train our segmentation model and this is sort of a sampling from that from that probabilistic model and you see it's a color segmentation approach.",
                    "label": 0
                },
                {
                    "sent": "And obviously we screwed up the texture and actually we could do that for texture as well, but the reconstruction is much more complicated when you have couple features to go from feature space back to the original space.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It works sufficiently well, and that's actually what we were paid for in in remote sensing imagery.",
                    "label": 0
                },
                {
                    "sent": "So this was the original and this is a reasonable image, and for many people.",
                    "label": 0
                },
                {
                    "sent": "In that area, this was surprisingly good.",
                    "label": 0
                },
                {
                    "sent": "So they they bought into that model.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me now come to.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "In these pictures you just showed us, we just sort of partitioning by color, really not using when I see pixels are placed together or something.",
                    "label": 0
                },
                {
                    "sent": "No no, no that was not used.",
                    "label": 0
                },
                {
                    "sent": "Now since the local windows are overlapping you get you get an implicit coupling between different between different sites by feeding in the same statistics.",
                    "label": 0
                },
                {
                    "sent": "But what you really would like to know is if you, for example, have a malkoff random field model in the back of your head and you add you add to your your cost function for clustering local coupling of the CI value to a neighboring CI plus one or CI minus one object.",
                    "label": 0
                },
                {
                    "sent": "Then you would also have to have a smooth mapping.",
                    "label": 0
                },
                {
                    "sent": "But then what I'm saying is we don't really know how to.",
                    "label": 0
                },
                {
                    "sent": "You know we can do sort of optimizing the cost function.",
                    "label": 0
                },
                {
                    "sent": "That's not a big problem.",
                    "label": 0
                },
                {
                    "sent": "You go to the people in combinatorial optimization.",
                    "label": 0
                },
                {
                    "sent": "They give you a cutting plane algorithm and then you just get a solution.",
                    "label": 0
                },
                {
                    "sent": "But it's not so clear how you would actually transfer such a solution to a new new image.",
                    "label": 0
                },
                {
                    "sent": "So the factorial nature of these cost functions makes it particularly simple to talk about generalization problem.",
                    "label": 0
                },
                {
                    "sent": "We just take the statistics and we use then the nearest neighbor rule with the new cost function and then we get the labeling.",
                    "label": 0
                },
                {
                    "sent": "That one.",
                    "label": 0
                },
                {
                    "sent": "Coding session I don't know, not sure I want to be recorded OK. OK, so let me just take you up on that so but why can't you say that you are happy to learn the individual site statistics that basically distributions from a bunch of?",
                    "label": 0
                },
                {
                    "sent": "Well even for one image.",
                    "label": 0
                },
                {
                    "sent": "But then if you learn some Markov random field interactions from a say a few images, then can't you just run something like the boy cough, golf?",
                    "label": 0
                },
                {
                    "sent": "You know this is efficient ways of finding MFP solutions.",
                    "label": 0
                },
                {
                    "sent": "Democracy random fields, conditional on the data so couldn't can you then make that work?",
                    "label": 0
                },
                {
                    "sent": "I mean I don't see a problem there well.",
                    "label": 0
                },
                {
                    "sent": "I mean what no?",
                    "label": 0
                },
                {
                    "sent": "No, no, no, I think I think on on our way here to Grenoble.",
                    "label": 0
                },
                {
                    "sent": "We had a discussion in the train and let me let me sort of tell you a little bit about what the problem I think is and it makes it clear when you use K means or these distributional clustering cost function and you have trained you are training.",
                    "label": 0
                },
                {
                    "sent": "You have explored, exploited the information in your training image.",
                    "label": 0
                },
                {
                    "sent": "And you have now your centroids.",
                    "label": 0
                },
                {
                    "sent": "Basically what this, then the scenario actually is, you get step by step pixel by pixel.",
                    "label": 0
                },
                {
                    "sent": "The new information from the test image and you have to make a decision after I give you a particular local statistics.",
                    "label": 0
                },
                {
                    "sent": "Now what do you really want to do?",
                    "label": 0
                },
                {
                    "sent": "If I tell you I give you in some order the pixels of the new image, but you have your coupled model?",
                    "label": 0
                },
                {
                    "sent": "You know well?",
                    "label": 0
                },
                {
                    "sent": "I mean there isn't the game I was playing, I just you give me the whole image, no, but if you yeah, but but the true, the true challenge is to define the generalization problem and you have no generalization problem.",
                    "label": 0
                },
                {
                    "sent": "If I give you the full image.",
                    "label": 0
                },
                {
                    "sent": "An if you are only tested if you are judged on the on the on the empirical error on the test case, then nobody can prevent you from basically doing the best.",
                    "label": 0
                },
                {
                    "sent": "What your combinatorial optimization gives you.",
                    "label": 0
                },
                {
                    "sent": "I still think I can make that work now.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying that there is no solution pieces from the image and give it to me later sure sure.",
                    "label": 0
                },
                {
                    "sent": "Sure you have, but you have to think about a little bit later how how you would actually use that smoothness constraint, which is in your statistical model build into which led to some changes.",
                    "label": 0
                },
                {
                    "sent": "So the game that I had in mind is that you have an ensemble of images, maybe I have a lot of things that I want to you know.",
                    "label": 0
                },
                {
                    "sent": "Landsat images and I keep some back.",
                    "label": 0
                },
                {
                    "sent": "So just like we play, you know, in in classification hold the game is now not on the individual image but on a whole bunch of images.",
                    "label": 0
                },
                {
                    "sent": "I sort of used some to learn about things and then use the rest.",
                    "label": 0
                },
                {
                    "sent": "So yeah, that is sort of that is the well then you are in the game that you say, well, you have asymptotically many images.",
                    "label": 0
                },
                {
                    "sent": "The images are taken in dependently, you know, and you want to build a probability probability model for these images.",
                    "label": 0
                },
                {
                    "sent": "This is what people in optimization called the sample average approach.",
                    "label": 0
                },
                {
                    "sent": "However, the real world situation in many combinatorial optimization problems is that the definition of your problem instance is noisy.",
                    "label": 0
                },
                {
                    "sent": "An you at most get one or two cases or maybe 10.",
                    "label": 0
                },
                {
                    "sent": "But there is much more to be learned about local smoothness and segment statistics from a very small number of of, let's say, images, so that you are forced to use.",
                    "label": 0
                },
                {
                    "sent": "You know statistically, relations between neighboring feature descriptors in order to build up your model.",
                    "label": 0
                },
                {
                    "sent": "And I think that that is one of the challenges.",
                    "label": 0
                },
                {
                    "sent": "At least I perceived so far.",
                    "label": 0
                },
                {
                    "sent": "If you have a setting where your objects are independent, then the whole bag of statistical learning theories will work.",
                    "label": 0
                },
                {
                    "sent": "You know you have you have large deviation results, you have IID contributions of your empirical risks to build up your costs, and then you can easily define what you mean by expected costs and so on.",
                    "label": 0
                },
                {
                    "sent": "When the random valves are coupled, it's no longer so clear.",
                    "label": 0
                },
                {
                    "sent": "And it's probably one out we can discuss it later this week.",
                    "label": 0
                },
                {
                    "sent": "OK, so so maybe this example also makes it clear what at least one possible route is.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying this is the only one, but one possible route is to to learning in a more complicated setting.",
                    "label": 0
                },
                {
                    "sent": "Here you have you have protein data.",
                    "label": 0
                },
                {
                    "sent": "They are pairwise compared.",
                    "label": 0
                },
                {
                    "sent": "You get these.",
                    "label": 0
                },
                {
                    "sent": "These sort of these little bit structure, but otherwise randomly looking matrix here it's symmetric, so some of the structure is coming from that symmetry here.",
                    "label": 0
                },
                {
                    "sent": "If you permute your columns and your rose then you clearly see that you have big groups in here 3 and you want to find these these groups for example.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data abandoned here is an example from biology, molecular biology we have.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, used that following up the research by Guimond Gaming kafenion down.",
                    "label": 0
                },
                {
                    "sent": "We have used it for these Landsat imagery.",
                    "label": 0
                },
                {
                    "sent": "Basically what you do is you have a local image Patch.",
                    "label": 0
                },
                {
                    "sent": "Here you have a local image Patch.",
                    "label": 0
                },
                {
                    "sent": "Here you get some empirical distribution of your features in that area in that area.",
                    "label": 0
                },
                {
                    "sent": "Then you apply a statistical test.",
                    "label": 0
                },
                {
                    "sent": "Whether these two areas below come from the same source are empirical realizations of these.",
                    "label": 0
                },
                {
                    "sent": "Source and if if high squared statistics for example tells you these are very similar, then you should have a small devalue, otherwise you get a large one and then you have a pairwise clustering problem and this is the ultimate labeling of three classes here.",
                    "label": 0
                },
                {
                    "sent": "Now it's not so clear how you would generalize from this solution where I basically have a bunch of of assignment variables of all my different local image patches with the local statistics extracted there.",
                    "label": 0
                },
                {
                    "sent": "How I would?",
                    "label": 0
                },
                {
                    "sent": "I would use how I would use a solution to my training image and then sort of just copy it to the test image without actually optimizing on the test image from scratch.",
                    "label": 0
                },
                {
                    "sent": "And I should also say.",
                    "label": 0
                },
                {
                    "sent": "In that setting, you know you compare all possible image pairs here.",
                    "label": 0
                },
                {
                    "sent": "So you also take a one up here and compare it to that one down here.",
                    "label": 0
                },
                {
                    "sent": "So all these all these sites are compared to each other.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the cost function is sort of rebuild according to K means for that for that new data type.",
                    "label": 0
                },
                {
                    "sent": "Basically what you do is you sum up the dissimilarity's between object I&J whenever both objects belong to the same cluster.",
                    "label": 0
                },
                {
                    "sent": "So that's the compactness criterion, which you also have in K means when they belong to the same class are you compare them to the mean?",
                    "label": 0
                },
                {
                    "sent": "Here you have to compare them pairwise to each other and in order to get the proper scaling of these quadratic.",
                    "label": 0
                },
                {
                    "sent": "Turns here you know you sum over I entry, so you get quadratically many comparison contributions.",
                    "label": 0
                },
                {
                    "sent": "Here you normalize it with the size of these of these.",
                    "label": 0
                },
                {
                    "sent": "Of these segments, so seeing you is the set of the segment and you take the cardinality here and what we want to find is, is this assignment vectors, see note here.",
                    "label": 0
                },
                {
                    "sent": "We don't have sort of a generic descriptor for cluster mu.",
                    "label": 0
                },
                {
                    "sent": "So these vinnu is not not available here.",
                    "label": 0
                },
                {
                    "sent": "Now what you can you will find out to Diane how the third is 73 addition.",
                    "label": 0
                },
                {
                    "sent": "I think page 200 something you will find out that if these DJs squared Euclidean distances that this cost function, the pairwise clustering cost function is mathematically equivalent to the K means cost function.",
                    "label": 1
                },
                {
                    "sent": "When you choose the wise as the means.",
                    "label": 0
                },
                {
                    "sent": "So now we.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You also have to realize and that was known already before this work has been found as been being sort of put together.",
                    "label": 0
                },
                {
                    "sent": "Mainly bifocal was that the pairwise clustering cost function is has certain invariances which are quite important for the application.",
                    "label": 0
                },
                {
                    "sent": "The first one is a trivial and you know you sort of can only.",
                    "label": 0
                },
                {
                    "sent": "By summing over all pairs, if you have an asymmetric dissimilarity value, which is probably not what you what you want to do live with.",
                    "label": 0
                },
                {
                    "sent": "But sometimes your application.",
                    "label": 0
                },
                {
                    "sent": "If the application people design it for you, you can just symmetrize it without changing the cost function.",
                    "label": 0
                },
                {
                    "sent": "So H tildes equals 2H.",
                    "label": 0
                },
                {
                    "sent": "The more important, the more important.",
                    "label": 0
                },
                {
                    "sent": "A similarity invariants is these additive shift invariant.",
                    "label": 0
                },
                {
                    "sent": "So if I add a constant D02 all off diagonal values, then the cost function H~ which I guess after the transformation is identical to the cost function before the transformation plus a constant.",
                    "label": 0
                },
                {
                    "sent": "So we just transform the cost function by a constant.",
                    "label": 0
                },
                {
                    "sent": "But obviously since I had only a cost constant to it.",
                    "label": 0
                },
                {
                    "sent": "The solutions the minimum assignments.",
                    "label": 0
                },
                {
                    "sent": "For agent for H~ all the same, and that was the key insight too.",
                    "label": 0
                },
                {
                    "sent": "The two basic.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ugly establish a general relation between pairwise clustering and in K means an.",
                    "label": 0
                },
                {
                    "sent": "I think you will learn something about the generalization issue in their context, so here is some mathematics I don't want to go to through that, but just tell you what these constant shift impending is.",
                    "label": 0
                },
                {
                    "sent": "These transformed by JD similarities can now be into interpreted after a proper shift of the dissimilarity's by Lambda 0 times these.",
                    "label": 0
                },
                {
                    "sent": "Matrix, it can be interpreted as squared Euclidean distances between vectors.",
                    "label": 1
                },
                {
                    "sent": "Basically, now you can use the kernel trick and then you see that the optimal assignment for K means in that constructed Euclidean vector spaces.",
                    "label": 0
                },
                {
                    "sent": "Are they identical to the optimal assignments in their paralyze problem?",
                    "label": 0
                },
                {
                    "sent": "And you can find these coordinates which are not explicitly given, because all what you have is this dissimilarity matrix.",
                    "label": 0
                },
                {
                    "sent": "You can find these coordinates XI by agan value decomposition an you can now optimally approximate things by using kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "And what is the lesson which you can draw from that.",
                    "label": 0
                },
                {
                    "sent": "You basically go from a problem which looks inherently coupled.",
                    "label": 0
                },
                {
                    "sent": "Every object is compared to every other object and by using these invariants of the cost function we find a transformation such that the.",
                    "label": 0
                },
                {
                    "sent": "Existed Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "It might be N -- 1 dimensional and in that Euclidean space we do K means and K means has the property.",
                    "label": 0
                },
                {
                    "sent": "If you give me the centroids, I know what I have to do on new data, I just take these fringe centroids and I compare every object of the new data set with the centroids and I find the cluster label which gives me the nearest neighbor assignment.",
                    "label": 0
                },
                {
                    "sent": "So implicitly what you do is by these transformation you decouples the objects object.",
                    "label": 0
                },
                {
                    "sent": "I is now decoupled from object J if I know the statistics in that embedding space.",
                    "label": 0
                },
                {
                    "sent": "And I think that's what you have to learn.",
                    "label": 0
                },
                {
                    "sent": "You have to learn what kind of statistics allows you to describe a new object, independent of all the other objects which might actually come with the same instance.",
                    "label": 0
                },
                {
                    "sent": "And in a situation like this, it's easy to talk about generalization in situations where you don't know what these decoupling statistics is, it's becoming fairly complicated to talk about, talk about generalization and what you mean by generalizing from one instance to the next one.",
                    "label": 0
                },
                {
                    "sent": "So a couple of years ago I was working on on noisy TZP problems, and we can discuss offline how that might fit into that into that framework.",
                    "label": 0
                },
                {
                    "sent": "But here it's sort of clear.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's some some examples that it actually works.",
                    "label": 0
                },
                {
                    "sent": "These bioinformatics applications five different classes.",
                    "label": 0
                },
                {
                    "sent": "That is the ideal solution that is, the original dissimilarity matrix, and after denoising you get rid of basically dimensionality reduction in that embedding space you get rid of a lot of noise entries in this dissimilarity matrix, and what you find is you get a much better distinction between the different blocks than what you could do on.",
                    "label": 0
                },
                {
                    "sent": "On the basis here and you can do a lot of things more things.",
                    "label": 0
                },
                {
                    "sent": "For example, now you have a vectorial representation in that embedding space, it's a generative model there, and you know, denoising all these issues are now fairly clear.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is another example which tells you that that this is actually working.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, by the way, how much time do I actually have?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "I'm over.",
                    "label": 0
                },
                {
                    "sent": "15 minutes.",
                    "label": 0
                },
                {
                    "sent": "Now 15 minutes left.",
                    "label": 0
                },
                {
                    "sent": "OK. Well.",
                    "label": 0
                },
                {
                    "sent": "I spend too much time on these models.",
                    "label": 0
                },
                {
                    "sent": "Another very popular segmentation model is normalized, cut, normalized cut basically has the same problem.",
                    "label": 0
                },
                {
                    "sent": "I would say as is pairwise clustering when you apply it to segmentation and basically what you do is you have you have a graph, a set of vertices V an you have a subset A of where this is a subset B of vertices and you have weights on the on the on the edges and when you cut.",
                    "label": 0
                },
                {
                    "sent": "A from B when you when you calculate the cat between A&B.",
                    "label": 0
                },
                {
                    "sent": "This gives you a criterion how good use your segmentation actually is and that's why this method is sort of called separation method and you normalize it so that it's getting fairly robust with the Association between that set A and the full set of vertices V. So and it's formulated tier in such a way that you always have a clustering between two groups.",
                    "label": 0
                },
                {
                    "sent": "Full vault in the foreground and background.",
                    "label": 0
                },
                {
                    "sent": "And again, I give you one image.",
                    "label": 0
                },
                {
                    "sent": "You execute and cut and then you have a solution for front end cut and then I gave you the second image and there's again a Tiger as in the first image and you tell me how you generalize your Tiger solution from the first image to the second one, and it's absolutely unclear how you would transfer here the solution.",
                    "label": 0
                },
                {
                    "sent": "So here it's differently formulated with these assignment values, CII.",
                    "label": 0
                },
                {
                    "sent": "Don't want to go into detail here.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can find the relaxation and that's why N cut is one of the examples for for spectral methods in computer vision, spectral methods are fairly popular in the learning Community these days, and even there I think people haven't appropriately discussed what they mean as sort of the the generalization issue when you use some of these methods to interpret your data.",
                    "label": 0
                },
                {
                    "sent": "But at least here you you can.",
                    "label": 0
                },
                {
                    "sent": "You can relax from from the Boolean assignment C. You can relax everything and find here you have these.",
                    "label": 0
                },
                {
                    "sent": "These axes which are defined on the interval between minus one and one.",
                    "label": 0
                },
                {
                    "sent": "And here the seas have to have the.",
                    "label": 0
                },
                {
                    "sent": "Use minus one and one, so the relaxation to end cut can be solved efficiently and cut by itself is NP hard.",
                    "label": 0
                },
                {
                    "sent": "That was proven by by a colleague of of.",
                    "label": 0
                },
                {
                    "sent": "Jitendra Malik and he probably the the type of of images you have seen before from Alex Sheehan Malik's papers.",
                    "label": 0
                },
                {
                    "sent": "OK um.",
                    "label": 0
                },
                {
                    "sent": "So that concludes base.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, the modeling part in the tutorial.",
                    "label": 0
                },
                {
                    "sent": "I still had some other methods, but I think from a learning point of view this sort of covers the full spectrum of what you what you would like to discuss on one hand, conceptually, methods which are fairly well.",
                    "label": 0
                },
                {
                    "sent": "Located in the in the in the ballpark of learning, problems like K means all these parametric distribution clustering.",
                    "label": 0
                },
                {
                    "sent": "Some other methods which are fairly popular which achieves the same thing, partitioning local image characteristics feature values in K different groups, but which which look not be addressable or analyzable.",
                    "label": 0
                },
                {
                    "sent": "In the usual way, when it comes to generalization, let's end cut and one relation is the one between pairwise data clustering.",
                    "label": 0
                },
                {
                    "sent": "K means where you see through this mathematically equivalent how you can go from.",
                    "label": 0
                },
                {
                    "sent": "From what I would call a generative Model K means in that embedding space to a model where it's not so clear how you how you actually would generate new data, and that's something we have to learn in the future.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the Section 2 I was focusing on on.",
                    "label": 0
                },
                {
                    "sent": "Noise insensitive ways, robust ways, and I think the notion of robustness in in application domains is mostly related to be sort of invariant to slide show very, very little variant to slide changes in your data, and it basically is kind of kind of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, generalization ability of the methods from one instance in a slightly different instance.",
                    "label": 0
                },
                {
                    "sent": "So maybe an instance where the signal quote unquote is the same but the noise is different and sometimes it's not so clear what is actually the signal.",
                    "label": 0
                },
                {
                    "sent": "If I give you 2 Tiger images in one, the Tiger is jumping in the lower left corner, and in the other is sleeping in the upper right corner, the signal I would say is being a Tiger being present in these two images, but it's not clear how you distinguish.",
                    "label": 0
                },
                {
                    "sent": "On on on the level of individual image blocks.",
                    "label": 0
                },
                {
                    "sent": "How you would distinguish signal from noise.",
                    "label": 0
                },
                {
                    "sent": "And the challenge for in particular the Paski project, I think, is is clarifying these issues because they they are very deeply related to what we mean by learning.",
                    "label": 0
                },
                {
                    "sent": "So candidate solutions should be typical and maybe they have to be averages of partitioning and.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Method which I was advocating for for for for a number of years now is in maximum entropy principle and you have a strategy to search through the solution space and that is the space of possible partitions and you accepte partitions, which with costs expected costs here of age, smaller of being smaller than a constant in this constant is parameterized by the temperature T. So for those of you coming from statistical physics, they know these relations and the others just should imagine that T is a parameter which allows you to tune the quality of your expected quality of your of your costs.",
                    "label": 0
                },
                {
                    "sent": "Here you can do that by Markov chain Monte Carlo or Mean Field approximation to methods in that context.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here you have one of these algorithms and MCMC algorithm, and I think the essential the essential.",
                    "label": 0
                },
                {
                    "sent": "What you should.",
                    "label": 0
                },
                {
                    "sent": "Basically what you should basically take from these transparencies that the algorithm explores the universe of partitions which have a certain characteristic and the characteristic here is that in terms of these empirical costs they are below a threshold.",
                    "label": 0
                },
                {
                    "sent": "And if you if you believe that noise sort of adds to the variability of your partitions and the signal votes always for the same type of partition, then averaging over different partitions might actually give you something like a prototypical partition.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what happened in these pairwise clustering, because in pairwise clustering we also wanted to find the partition and going in that embedding space where you can do K means by mathematical equivalence you find these generative.",
                    "label": 0
                },
                {
                    "sent": "Partition by the centroid in that space.",
                    "label": 0
                },
                {
                    "sent": "That is basically the model which was underlying these equivalence between K means in pairwise clustering.",
                    "label": 0
                },
                {
                    "sent": "Here you can do that trust algorithmically by searching through many of these partitions, and whenever they fulfill the criterion, you take them and you extract the statistics of it.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So usually what you do is you maximize the entropy with additional free parameters.",
                    "label": 1
                },
                {
                    "sent": "That sort of one of the approximation schemes, and then you have an EM like iteration where you iterate between where you are.",
                    "label": 0
                },
                {
                    "sent": "You maximize the entropy and basically what you do is you.",
                    "label": 0
                },
                {
                    "sent": "You fix some some statistics which you have for measuring how costly it is for one of your objects being assigned to a particular cluster.",
                    "label": 0
                },
                {
                    "sent": "And the assignment function and that is very similar to what we have discussed before in terms of K means.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you see is sometimes surprising.",
                    "label": 0
                },
                {
                    "sent": "You get these quote unquote phase transitions.",
                    "label": 1
                },
                {
                    "sent": "Here you have a data density and when you have very crude approximations down here, this is the inverse temperature better, which is also often used.",
                    "label": 0
                },
                {
                    "sent": "Then you will estimate six different clusters and they are all located in the same position and that's here around.",
                    "label": 0
                },
                {
                    "sent": "Oh point probably oh .30 point 4 and then all of a sudden when you reduce.",
                    "label": 0
                },
                {
                    "sent": "Your approximation quality, you lower the temperature and your constraint on the acceptable quality of a solution is low.",
                    "label": 0
                },
                {
                    "sent": "Blood, then you you force the the statistics to split up and four of the centroids are on this branch and one centroid is located in here and if you lower again these threshold in this approximation scheme you get further breakups until you have six different six different clusters for these six different modes.",
                    "label": 0
                },
                {
                    "sent": "OK, let me skip the EM power.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For parametric distribution and clustering, and just show you on that image which you have already seen before, how how you will find solutions at different temperatures at very high temperature.",
                    "label": 0
                },
                {
                    "sent": "You basically get these sort of these salt and pepper type image.",
                    "label": 0
                },
                {
                    "sent": "You have six different colors in here and you see it's basically a random choice.",
                    "label": 0
                },
                {
                    "sent": "Which of these colors you select you lower.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The temperature a little bit and the most dominant structure according to your cost function start to appear.",
                    "label": 0
                },
                {
                    "sent": "So here you get sort of a condensation of Gray values in that area and in that area, and these are if you remember, these are the black parts in the in the Landsat image and the black parts are coming from from the not in the lens at in the synthetic aperture radar image and it comes from the radar shadow.",
                    "label": 0
                },
                {
                    "sent": "You go.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down and all this.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Button more structures appear.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You still have a mixture between yellow and blue.",
                    "label": 0
                },
                {
                    "sent": "He ran between 2 Blues here and you go.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Further down.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And at the end.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "At the end you basically get this type of of picture.",
                    "label": 0
                },
                {
                    "sent": "We have now much better solutions than that, but I only have the video.",
                    "label": 0
                },
                {
                    "sent": "In this in this presentation format, here from from that image.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here you have the same for Mondrian and what is plotted here for different temperatures.",
                    "label": 0
                },
                {
                    "sent": "It's the kullback Leibler divergent to the mean histogram.",
                    "label": 1
                },
                {
                    "sent": "So if I do the same thing in histogram space, I have to somehow characterize my different solutions and it's I do that by plotting by plotting the.",
                    "label": 1
                },
                {
                    "sent": "The prototypical histogram for one of the segments, by basically plotting the distance of the prototypical histogram to the mean histogram.",
                    "label": 0
                },
                {
                    "sent": "And you get these abrupt changes in solution space, which basically gives you these.",
                    "label": 0
                },
                {
                    "sent": "These these these splits.",
                    "label": 0
                },
                {
                    "sent": "Let me not go into that because I will lose too much time.",
                    "label": 0
                },
                {
                    "sent": "You can use a multi scale approach for it.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I guess I have 5 more minutes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because we started a little bit late, but I don't want to go in the negotiation phase after 1 1/2 hours.",
                    "label": 0
                },
                {
                    "sent": "It's usually we have already exhausted.",
                    "label": 0
                },
                {
                    "sent": "The attention potential.",
                    "label": 0
                },
                {
                    "sent": "OK, the third part I think is inherently.",
                    "label": 0
                },
                {
                    "sent": "Connected to the generalization issue and it basically means how many classes do I have up to now.",
                    "label": 0
                },
                {
                    "sent": "I used this magic number K as an input.",
                    "label": 0
                },
                {
                    "sent": "Somebody told me how many classes I have to use.",
                    "label": 0
                },
                {
                    "sent": "Ideally I would say, well, maybe I know an upper bound on the number of classes, but please if it happened to be an image where two of my envisioned clusters cannot be separated on the basis of the data.",
                    "label": 0
                },
                {
                    "sent": "You know the algorithm should figure out by itself that rather than 10, which is the maximum number of clusters in that application, I should only use seven and discard in some way or another the other the other segments.",
                    "label": 0
                },
                {
                    "sent": "So that's the cluster validation problem, and there are a couple of approaches to it.",
                    "label": 0
                },
                {
                    "sent": "The most prominent ones by Reason and Schwartz.",
                    "label": 0
                },
                {
                    "sent": "This is the minimum description length approach or stochastic complexity approach.",
                    "label": 0
                },
                {
                    "sent": "And here you have the Bayesian information criterion.",
                    "label": 0
                },
                {
                    "sent": "Porex Miss couple of years ago used to cause validated likelihood.",
                    "label": 0
                },
                {
                    "sent": "Works also fairly well.",
                    "label": 0
                },
                {
                    "sent": "More recently, in particular in the context of bioinformatics, the Stanford Group came up with the.",
                    "label": 0
                },
                {
                    "sent": "A method called gap statistics and we have proposed a stability based validation criterion and.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I will, I will say a little bit about.",
                    "label": 0
                },
                {
                    "sent": "The cluster validity problem and then come to the stability issue.",
                    "label": 0
                },
                {
                    "sent": "So clustering algorithms always impose.",
                    "label": 1
                },
                {
                    "sent": "Hello, always impose structure on the data and here you see it looks like an inappropriate model order.",
                    "label": 1
                },
                {
                    "sent": "You know here you you tear up the this this group of data points in basically four different clusters and maybe the fluctuations have decided which of these clustering solutions have been found.",
                    "label": 0
                },
                {
                    "sent": "Here you have something like an inappropriate model type.",
                    "label": 0
                },
                {
                    "sent": "We have used the compactness kind time.",
                    "label": 0
                },
                {
                    "sent": "Criterion and we have these ring structure and obviously we expect that we find the rings and not not these these cake type slices of these rings.",
                    "label": 0
                },
                {
                    "sent": "And actually both problems come up in cluster validation.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So validation methods are procedures or concepts for quantitative and objective assessment of clustering solutions, and it's sort of a definition, and we want to evaluate specific clustering measures and they should be treated at the same footing.",
                    "label": 1
                },
                {
                    "sent": "I would say so we have to concentrate on on what we really get as an answer from a clustering method, and these are the partitions and then you have external validation criteria.",
                    "label": 0
                },
                {
                    "sent": "In particular, when you can compare with ground truth.",
                    "label": 0
                },
                {
                    "sent": "So the segmentation on the Berkeley database would give you ground truth full segmentations.",
                    "label": 1
                },
                {
                    "sent": "If you accept that, and that would be external or you have internal criteria and they can you be used for model selection.",
                    "label": 0
                },
                {
                    "sent": "And the important question is what is the appropriate number of clusters for my data?",
                    "label": 1
                },
                {
                    "sent": "So in general, what you would like to do is you measure the quality for different criteria and you need a function which distinguishes between the.",
                    "label": 0
                },
                {
                    "sent": "You need a method which distinguishes between the different answers for these cluster number.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Complexity based validation maybe go very quickly.",
                    "label": 0
                },
                {
                    "sent": "You add a complexity term to your log likelihood and you penalize for two complex solutions.",
                    "label": 1
                },
                {
                    "sent": "And in that context it's it's modeling all comes razor, favoring smaller solutions.",
                    "label": 0
                },
                {
                    "sent": "If they don't give simple solutions.",
                    "label": 0
                },
                {
                    "sent": "If they don't give you an appropriate amount of model fitting.",
                    "label": 0
                },
                {
                    "sent": "And the two methods which I just mentioned fall into that class.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The underlying principle is here.",
                    "label": 1
                },
                {
                    "sent": "You have your independent model parameters.",
                    "label": 0
                },
                {
                    "sent": "You have your negative log likelihood.",
                    "label": 0
                },
                {
                    "sent": "The model complexity penalty is going up.",
                    "label": 1
                },
                {
                    "sent": "The fitting term is going down and somewhere in between you get a nice operating point for these methods and that's why people often look when they when they plot the local likelihood as is a number of the parameters they look for, such for such a kink in that curve, and then they say, well, this is the.",
                    "label": 0
                },
                {
                    "sent": "The right model complexity where you should stop.",
                    "label": 0
                },
                {
                    "sent": "I'm very sceptical because you know that might be another another kinkier in that curve.",
                    "label": 0
                },
                {
                    "sent": "Should I stop here or here so it's not so clear?",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you have another solution for BIC and you see here.",
                    "label": 0
                },
                {
                    "sent": "It's slightly going up here with the penalty term you start penalising these more complex class does more than they help you and actually lowering your your negative log likelihood and that's why you get a minimum here and that that's the number you would choose.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "These methods are all fine, but they require that you have a generative model, which you can actually use for now.",
                    "label": 0
                },
                {
                    "sent": "If I just use end cut in the way I have described it, all these pairwise clustering cost function all what I have is I have some some partitions and I want to have a model validation criterion which tells me on the basis of these partitions whether two solutions are whether a solution is appropriate for my data or not and.",
                    "label": 0
                },
                {
                    "sent": "If you use the methods which I just briefly described, then you incorporate some structural biases.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people believe that they want to do that, and sometimes it's not even clear how you how you would formulate your problem in another way.",
                    "label": 1
                },
                {
                    "sent": "OK, so the main idea of that line of work and I still consider it to some degree preliminary, but we can discuss that later is that solutions should be stable and they should be stable when you have two datasets from the same data source and how you can realize that is a question which we have to discuss, But that's sort of these training testing paradigm.",
                    "label": 1
                },
                {
                    "sent": "You train on one instance of your data and then you transfer the solution from that instance.",
                    "label": 0
                },
                {
                    "sent": "To the test instance, and then you measure how well you do on the test instance and if you have roughly the same quality, then you believe your solution and hopefully when you get bad solutions they show up as being very unstable.",
                    "label": 0
                },
                {
                    "sent": "Otherwise this concept will not work.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here it's sort of depicted graphically.",
                    "label": 0
                },
                {
                    "sent": "That is obviously a stable solution when you have two different instances, and you overlay the two solutions and here you take 2 instances of the data, and once you get these blue dashed cluster boundaries and in the other case you get the black cluster boundaries.",
                    "label": 0
                },
                {
                    "sent": "And obviously you see already that 4343 different three bumps, an 4K equals 4.",
                    "label": 0
                },
                {
                    "sent": "You might get this inherent cluster instability which is driven by fluctuations, and that's something the algorithm should be able to detect and discard.",
                    "label": 0
                },
                {
                    "sent": "The solution K = 4 in favor of K = 3.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the general procedure then would be let's this cluster stability based validation technique.",
                    "label": 0
                },
                {
                    "sent": "You draw two datasets from the same source cluster.",
                    "label": 1
                },
                {
                    "sent": "Both datasets compute the agreement and then the stability is the expected agreement of these solutions and you can interpret that in the context of classification by basically treating the classifier the clustering algorithm on one data set as the teacher for the second data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have.",
                    "label": 1
                },
                {
                    "sent": "So this is the thought of the theory part.",
                    "label": 0
                },
                {
                    "sent": "In practice you have only one data set available, so somehow you have to live with that, but you still can S estimate the expected agreement by resampling and you have to pay attention when you do.",
                    "label": 1
                },
                {
                    "sent": "By resampling you get the same objects in both sets and they might actually sort of generate artificial stability.",
                    "label": 0
                },
                {
                    "sent": "Which should not be used as an argument for for particular model or So what we did is we split the data set in half and we basically found solutions on the first half and on the second half, and then we calculated the agreement and how we do that will now be wrapped up in way too short time.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And on the next couple of slides, measuring the disagreement, 3 problems clustering solutions of labelings, labelings of disjoint sets.",
                    "label": 1
                },
                {
                    "sent": "So we really have to know how we transfer the solution from this first problem instance to the second one.",
                    "label": 1
                },
                {
                    "sent": "The labeling itself is not unique, it's only unique up to a permutation.",
                    "label": 0
                },
                {
                    "sent": "So you have to really permute at least one solution so that it best fits to the first one in the resulting.",
                    "label": 0
                },
                {
                    "sent": "Mismatch then is used as a criterion that these solutions are unstable and you have to take into account that when you have only two clusters then 50% is totally random random result.",
                    "label": 0
                },
                {
                    "sent": "However, when you have K = 10 classes in many application of 50% agreement between two clustering solutions is is acceptable in these applications.",
                    "label": 0
                },
                {
                    "sent": "So somehow you have to you have to discount your error when you go to larger number of clusters.",
                    "label": 0
                },
                {
                    "sent": "Because you also have more possibilities to make errors, and this entropic term has to be taken into account.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the the the biggest problem so far and I think that's a problem for the learning community in large is we have to find away how we extend solutions from 1 Somerset 8 to a set B and that as we started to discuss is not so easy in these contexts.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we train a predictor on A and then we predict the labels on B.",
                    "label": 1
                },
                {
                    "sent": "That is sort of my second half of the data.",
                    "label": 0
                },
                {
                    "sent": "And then you compare the clustering solution.",
                    "label": 0
                },
                {
                    "sent": "You compare these predicted labels with the clustering solution on B.",
                    "label": 0
                },
                {
                    "sent": "So you do your empirical minimizations on the test cases and you compare it with the predicted labels from the from the predictor trained on the training case and then extend it to the test case and that sort of allows you to measure how much in the solution is actually.",
                    "label": 0
                },
                {
                    "sent": "Captured by you by your by your training procedure.",
                    "label": 0
                },
                {
                    "sent": "So the the predictor, how to choose the predictors is sort of yeah's problem which we have not where we have not found a unique solution.",
                    "label": 0
                },
                {
                    "sent": "You have to use a meta principle and you will see if you use a connected type connectedness type of of extension principle then it will be appropriate for this.",
                    "label": 0
                },
                {
                    "sent": "For this data set.",
                    "label": 0
                },
                {
                    "sent": "Compactness type extension principle is inappropriate for it, so you might get a poor predictor as well.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the labeling is unique up to a permutation, that is, that is denoted here and the solution is then the stability index with which which is defined as the expected minimal disagreement over all permutations here.",
                    "label": 0
                },
                {
                    "sent": "So to put it together, there's a method called the Hungarian method which allows you to find these best rearrangements of your clusters.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the stability costs are scale sensitive to K. That's what I wanted to make clear with this, comparing K = 2 and 50% error in K = 10 and 50% error.",
                    "label": 1
                },
                {
                    "sent": "And you can normalize for that by by this comparison to the random predict which gives you sort of a scaling argument.",
                    "label": 0
                },
                {
                    "sent": "How you should judge these errors and then you normalize these expected agreements here.",
                    "label": 0
                },
                {
                    "sent": "With the error random predicted would make.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then you put everything together and the final solution is you concentrate on the disagreement rate and that is what your what your labeling algorithm gives you on the data of the second set.",
                    "label": 1
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "This is the predictor G which takes an object from from the second set and the data from the first set.",
                    "label": 0
                },
                {
                    "sent": "These are the prime, the training data, and the partitioning solution from the first data set, an if that if what you would do, knowing all the information from the second instance compared to the prediction of a solution from the second instance if that.",
                    "label": 0
                },
                {
                    "sent": "Is in disagreement.",
                    "label": 0
                },
                {
                    "sent": "You counted as an error.",
                    "label": 0
                },
                {
                    "sent": "You have to break the permutation symmetry so you permute over all possible relabeling Zanu, find the minimum the best free labeling of it you have to take the expectation over, or training data in all test data X&X prime.",
                    "label": 0
                },
                {
                    "sent": "Ann, you have to normalize that with respect to these two these.",
                    "label": 0
                },
                {
                    "sent": "K scaling of the random predictor and then you have a problem.",
                    "label": 1
                },
                {
                    "sent": "We cannot calculate these expectation values, so you have to after estimate it and what we do is we estimated by resampling.",
                    "label": 0
                },
                {
                    "sent": "So that allows you.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then to look at at here toy datasets here these these five different.",
                    "label": 0
                },
                {
                    "sent": "Gaussian distributed data sets or groups have a clear minimum in that instability index, so the instability is the lowest around 5.",
                    "label": 0
                },
                {
                    "sent": "Here you have an inappropriate model chosen.",
                    "label": 0
                },
                {
                    "sent": "I think that is K mean and it gives you a minimum around 7:00.",
                    "label": 0
                },
                {
                    "sent": "But if you use if you use a connectedness type criterion and we used a path based clustering procedure which basically looks at.",
                    "label": 0
                },
                {
                    "sent": "It passed in terms of measuring how well 22 objects belong together.",
                    "label": 0
                },
                {
                    "sent": "Then you get a very clear minimum with zero variance on these empirical data.",
                    "label": 0
                },
                {
                    "sent": "For these three different drinks.",
                    "label": 0
                },
                {
                    "sent": "So these stability criterion measures together the fluctuation coming from all the instability coming from the fluctuation in the data as well as the instability from a model mismatch.",
                    "label": 0
                },
                {
                    "sent": "Chanpur models.",
                    "label": 0
                },
                {
                    "sent": "Seem not to capture the properties of the data, and if they don't capture the properties of the data, they might be sort of describing the groups in awake way, which which favors instability.",
                    "label": 0
                },
                {
                    "sent": "Now this proves, and here you have the.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Predicted labels and it's not so clear.",
                    "label": 0
                },
                {
                    "sent": "Doctors might even fight over having two different classes of cancer or three, so this uncertainty is captured by that validation methods, basically giving you large error bars on many different resampling's and slightly better instability value here 4K equal 3, so we would not get too much money on three rather than two, but.",
                    "label": 0
                },
                {
                    "sent": "Give you the mixture back here.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For these data you have the situation that you have a minimum here at.",
                    "label": 0
                },
                {
                    "sent": "I think this is 3 and a minimum at 9 and that roughly corresponds to what people know about this data in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So they are biologically possible.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me summarize this part and with not too much.",
                    "label": 0
                },
                {
                    "sent": "Running over the time, I would say in in segmentation the big learning issue is to validate the solutions.",
                    "label": 0
                },
                {
                    "sent": "While you have vastly different criteria to come up with the partitioning, some of these criteria are fairly close to what we are now.",
                    "label": 0
                },
                {
                    "sent": "What we know from the statistical learning theory.",
                    "label": 1
                },
                {
                    "sent": "Area in particular K means has the structure that, given the statistics, the centroids, we have a very simple optimization procedure to produce a new class string.",
                    "label": 0
                },
                {
                    "sent": "We basically use the nearest neighbor rule and that gives me kiss me on the next test set of objects and new labeling for many of these.",
                    "label": 0
                },
                {
                    "sent": "Also widely used methods like normalized cut or average cut and another graph based.",
                    "label": 0
                },
                {
                    "sent": "Approaches of partitioning.",
                    "label": 0
                },
                {
                    "sent": "It's not at all clear how I take a solution from the training set and produce a new partitioning on the test set on the test image for Markov random field type approaches you have these local coupling.",
                    "label": 0
                },
                {
                    "sent": "It's also not clear how you can use use.",
                    "label": 0
                },
                {
                    "sent": "A segmentation learned on one image and transfer it to a second image and the ultimate challenge, I think in that context is to use shape information so we know a Tiger.",
                    "label": 0
                },
                {
                    "sent": "I give you 5 Tiger images.",
                    "label": 0
                },
                {
                    "sent": "By that time you should have learned that Tigers have four legs, a tail end and the head and that sort of spans a possible universe of shapes, and these shapes should be learned, and that's something, at least where I believe that humans are very well.",
                    "label": 0
                },
                {
                    "sent": "Able to transfer that image that that type of shape information to a second image.",
                    "label": 0
                },
                {
                    "sent": "But we don't really know first how to describe shape and 2nd if we would have a shape.",
                    "label": 0
                },
                {
                    "sent": "Description How we actually can take that.",
                    "label": 0
                },
                {
                    "sent": "Learn it on one set of data and then use that for finding efficient solutions on the second data set.",
                    "label": 0
                },
                {
                    "sent": "So to summarize here, no additional assumptions are made by our approach.",
                    "label": 1
                },
                {
                    "sent": "The weakness of that approach is it does measure instability, but it does not make the trade of explicit between instability.",
                    "label": 0
                },
                {
                    "sent": "On one hand of the solution and informativeness of a solution.",
                    "label": 0
                },
                {
                    "sent": "So maybe I'm I'm willing to accept a little bit more instability when I have one more cluster, because one more class is also more informative, and ultimately I believe that this has to be traded off one against the other one.",
                    "label": 0
                },
                {
                    "sent": "And that is certainly something which is opening that in that cluster validation business.",
                    "label": 0
                },
                {
                    "sent": "And by that I would like to end here.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Running a bit overtime, but there any quick questions.",
                    "label": 0
                },
                {
                    "sent": "No OK.",
                    "label": 0
                },
                {
                    "sent": "I will break for let's say 20 minutes and come back.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "Changing meeting schedule.",
                    "label": 0
                },
                {
                    "sent": "Is it sorry?",
                    "label": 0
                },
                {
                    "sent": "Oh looking you have to set up your stuff now.",
                    "label": 0
                },
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "Well, why don't we break here but break for 15 minutes only.",
                    "label": 0
                },
                {
                    "sent": "Will come back.",
                    "label": 0
                }
            ]
        }
    }
}