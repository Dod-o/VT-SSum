{
    "id": "funa5j67wbuorwbg4qo3lm6r23lfhqcl",
    "title": "Querying Large Knowledge Graphs over Triple Pattern Fragments: An Empirical Study",
    "info": {
        "author": [
            "Lars Heling, Karlsruhe Institute of Technology (KIT)"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_heling_querying_fragments_study/",
    "segmentation": [
        [
            "Alright, thank you.",
            "I will be presenting our work on querying large knowledge graphs over triple pattern fragments where we conducted in empirical study.",
            "So first let's."
        ],
        [
            "Look into the motivation of this work."
        ],
        [
            "First of all, as all of you know who attended the conference knowledge, graphs are everywhere and querying knowledge graphs is in a very important factor.",
            "And currently we basically have, let's say, two very popular web interfaces to query knowledge graphs and RDF on the right hand side we have sparkle endpoints which are very expressive.",
            "This means that it's harder on their servers.",
            "More expensive on the servers, but it reduces the cost for the clients.",
            "And the other side of the spectrum.",
            "We have triple pattern fragments which are more lightweight on the servers.",
            "However, they increased the cost on the client side and our work.",
            "We wanted to look into more detail on how the performance of triple pattern fragments is when we query different types of triple patterns and also how different configurations of the server setup effect the response time of triple pattern FRAGMENTE servers."
        ],
        [
            "So how did we approach this?",
            "In our work we present our."
        ],
        [
            "Research with which is the TPF profiler.",
            "It's basically a software tool where you can plug in a TPF server URL.",
            "You specify a number of triples that you want to be sampled from the triple pattern FRAGMENTE servers, and then you receive the study results, which are fine grained results on which on top of which you can do your analysis.",
            "So how does this work?",
            "Let's take a look into."
        ],
        [
            "To the sampling component first in order to assess the performance of the servers, we generate a random sample.",
            "So for example, let's say we set the parameter M2 one.",
            "This means we sample one triple from the Knowledge graph knowledge graph, which is accessible via the TPF server, and we basically request a triple pattern with only variables.",
            "This means via paginating, we can access any triple.",
            "Of the graph, and then we basically randomly."
        ],
        [
            "Select pages in this case, since M is one, we just select one page and a page consists of a number of triple pattern triples and we randomly select one of those triples.",
            "In this case we select Monterrey is in EU."
        ],
        [
            "Knighted states having this triple.",
            "We can generate triple patterns based on this by simp."
        ],
        [
            "We replacing the RDF terms of the triple with variables, so for each triple we can generate eight different triple patterns.",
            "And this is how we generate the sample.",
            "And then in the second step."
        ],
        [
            "We basically evaluate the sample over the server.",
            "This is done by requesting requesting each triple pattern from the sample, and then we record the response time and also we record other meta data such as the answer cardinality or the page size which is available via the meta data of the TPF servers.",
            "And also we persist these samples so that we can reuse it for future evaluations.",
            "OK, so this is how we assess the performance.",
            "What kind of setup that we use in our study we used two different environment."
        ],
        [
            "First we used a controlled environment.",
            "This means we set up the TPF profiler and the TPF server on the same server.",
            "This allows us to have no network delays, however.",
            "Of course, the profiler and the server share memory and CPU.",
            "But it also allowed us to configure the TPF server so we can test different configurations and see how this effects the performance.",
            "The second environment that."
        ],
        [
            "We looked into was a real world environment where we basically looked for publicly available servers and we run that EPF profiler on our local server and requested the triple patterns from the publicly available server.",
            "This means, of course, that the server that we are requesting will have server workload from other requests.",
            "Potentially there will be network delays and also the TPF server will not be configurable.",
            "Because we have to use what's out there, basically.",
            "So."
        ],
        [
            "What kind of knowledge graphs that we take into account?",
            "We looked into visionary geonames DB Pedia and blpi as they are quite large.",
            "Knowledge graphs and also we could find TPF servers online with these knowledge graphs for the back end types we looked into HTTP and sparkle however we could only set up a sparkletts server locally because all of the publicly available TPF servers use HTTP backends.",
            "The third type of back end that you could use RDF documents we didn't include because we wanted to look into large knowledge graphs and they're just not feasible for that for the page sizes.",
            "We looked into four different page size configurations.",
            "105 hundred 1010 thousand an for the.",
            "Real world environment.",
            "We only had TF service available with the page size of 100.",
            "Alright, let's take a look in."
        ],
        [
            "To the results of our experimental study.",
            "So first of."
        ],
        [
            "How does the triple pattern type impact the response time?",
            "And here we can see the response time with respect to the different types of triple patterns and the different colors indicate the different knowledge graphs.",
            "Overall, we can see that on the rather left hand side where we have more or fewer variables, the response times are a bit lower and on the right hand side as we have more variables, the response time seemed to increase.",
            "Which stands out is that.",
            "The triple pattern types where we have only the predicate instantiate.",
            "It has the highest response time and where we have only variables has the second highest response time.",
            "In our controlled setup, and this let us think.",
            "OK, maybe these triple pattern types result or have many answers, so that's why the response time is higher than for the other triple pattern types.",
            "So we took a look.",
            "In two, how does the answer cardinality impasse?"
        ],
        [
            "At the response time.",
            "And here you can see the response time with respect to the number of results, the answer cardinality the black line indicates the page size used in this case for the blpi knowledge graph in the controlled environment and you can see there appears to be a linear increase in response time up to the page size, and then thereafter it appears to be steady.",
            "So we took a deeper look into that by computing the correlation for this again."
        ],
        [
            "Looking at the DB LP knowledge Graph here, indicated in green, are values above 0.5, meaning there's a positive linear correlation between the answer cardinality and the response time, and we've actually found that we find a linear correlation mostly for the virtual also back end type for HTT we didn't see that strong correlations and interesting Lee.",
            "In the real world environment, there were no correlation at all, so it appears that network delays and other effects impact on the response time such that the differences induced by the answer cat analogies are somehow illuminated.",
            "OK, next we also looked into the different page sizes.",
            "So can we improve the response time by increasing the page size for instance?"
        ],
        [
            "Here we looked into the controlled environment only because as I said before, publicly we only had publicly available service.",
            "Only had a page size of 100, but in our controlled environment we set up page size of 105 hundred 1010 thousand and we can clearly see that there's an increase in the throughput when we increase the page size for both HTTP and Virtuoso back end from 100 to 500.",
            "Answers per page and then thereafter the increase is not clear for all knowledge graphs.",
            "It even decreases.",
            "But this indicates that there's potential for improving configuration of these TPF servers to improve on the throughput.",
            "Next we looked into the."
        ],
        [
            "Different back end types and first of all the main results are we looked into HTN virtual sets as backends and we found that of course HTT is more performant than virtuozzo with an average response time, which is half as high as the one for.",
            "Also, and this kind of makes sense because when you use virtual also as a back end type, you basically use the TPF server to build another web interface on top of an existing interface.",
            "And so it makes sense that the response times are higher.",
            "Furthermore, we looked into how to, how does paginating through the results impact the response time?",
            "And so we basically requested the chip pattern with only variables to get access to the entire knowledge graph, and then we just paginated through and blue.",
            "You can see the response time for HTTP back end, which are rather stable and don't seem to increase for the increasing number of pages, however for.",
            "The Verge also back end.",
            "And Please note this is a log scale.",
            "We have an exponential almost exponential increase in the response time as we paginate through the results and this actually makes it impossible to access all triples from the knowledge graph we ever to.",
            "Also in this way.",
            "And Lastly, another look into the."
        ],
        [
            "Controlled and the real world environment.",
            "How do they differ?",
            "What did we find there?",
            "So first of all, the main difference is, as you can see on the top, you have way more outliers for the real world environment.",
            "This is definitely due to network delays and server workload that influences the response time and what was very interesting to us is that in the real world environment the triple pattern only requesting.",
            "With only variables, is the second fastest in the real world environment, while it's the second slowest in the controlled environment, and we think this is due to the fact that this results for this triple pattern is cashed on the real world server because it's frequently requested, for instance, every time you go onto the web interface of a TPF server it will request this triple pattern in order to display the first triples of the.",
            "Knowledge graph.",
            "All right, summing up."
        ],
        [
            "We found that the triple pattern type impacts on the response time and also that the triple pattern types that are more expensive typically also yield a higher answer cat analogy.",
            "Furthermore, we saw that the answer cardinality impacting response time doesn't really draw a clear picture because there is somehow a linear increase increase, but it's not visible for all back end types and not for all.",
            "Datasets.",
            "Furthermore, the page size impacts on the throughput.",
            "We saw that we can increase the throughput by increasing the page size and this means that you could actually use our tool to improve on the configuration of your TPF server by testing different page sizes and determining how it impacts on the throughput.",
            "The back end type we saw a clear picture there.",
            "HTT is the most suitable back end type for large knowledge graphs.",
            "And for the environments we saw that you need to take different server characteristics into account.",
            "When using TPF on the wild or in the wild in the real world.",
            "Alright, last."
        ],
        [
            "The concluding, how can you use our resource that EPF profiler?",
            "You can use it to improve query engines and deploy new optimization techniques when using query engines that leverage TPF servers.",
            "And on the other side you can use our resource to monitor TPF servers on the web.",
            "If you have your own.",
            "Server setup you can check and monitor the behavior of your service.",
            "You can access our resource on GitHub.",
            "We have a Jupiter notebook set up with all the statistical analysis that we did and all the visualizations and we have a demo this evening demo #32 so come check us out if you're more interested.",
            "Thank you.",
            "Healers good stuff, maybe just want fairness comment regarding Virtuoso.",
            "I think we should mention Virtuosos is reader writes, obviously so, so this also explains different behavior.",
            "I would say like agency is easy because it just read only.",
            "Guarding the different patterns of follow up question would be because yes, some patterns are slower than this is obviously a trade in design of any file format, so I think the right question here is like.",
            "Are the patterns that are slow like because if he would look at at all sparkle queries or these are patterns, we need least 'cause?",
            "If not, it's really easy done just in the back end instead of storing SPO you switch them around.",
            "So do you think that the right patterns are slow and the right ones are faster?",
            "Would you have done it differently?",
            "So if I understand correctly, is either slower patterns, those that are also used more frequently, slower patterns, the ones that we don't need offer like VRV for instance is slow.",
            "Is that a problem because it might not be a problem because it may be just never occurs in sparkle queries we haven't looked into the number of incurrent occurrences of the different pattern types in actual queries, but that's a good good pointer to look into it, 'cause otherwise you just like flip the back ends right and then yeah.",
            "And yeah, maybe improve the indexing module if you see that the if you VRV is used very often, then you could try to improve indexing for Viavi triples.",
            "And also I mean, kudos to like releasing this as a tool as a resource that we can retry these things that we can optimize.",
            "This is something I think we should all do way more so thanks.",
            "Anymore questions.",
            "If not, then.",
            "Hi I had a question on the.",
            "When you use the experiment we have back end.",
            "Did you try to to install index secondary index for for easing the pagination virtuoso in this case?",
            "Or you just use the standard installation of because you can declare over indexing videos of course yeah we just use the standard setup.",
            "But also as it is, you might tweak virtual also a little bit to improve the response times, but I think as we saw it's twice as slow as HTTP, so it doesn't really make sense to use it as a back end I think.",
            "And that's actually, I mean one of the.",
            "The motivations of PF services that you don't want to decrease the load on the servers by having similar requests and so building using Sparkle endpoints as a back end for a similar endpoint is kind of contradictive to me.",
            "I don't think so, but OK, it's fine, Sir.",
            "Questions.",
            "If not, then.",
            "Hi very nice tool.",
            "Maybe one small comment regarding the whole virtuoso thing.",
            "If I would change anything, I would maybe add more nuanced to the data sources, because I mean for private company like opening, this is very important, that everything that is communicated about their software is fair and honest, and everybody knows that because you know, like you said, you go through the sparkle interface, but in fact you could also go straight into the native code or future and implemented in that way so.",
            "Maybe that's more performant.",
            "And so on.",
            "I was I had two small questions so one if I would build a query engine that would use your tool to do some query optimization.",
            "How does that work?",
            "Is there a machine understandable explanation of the results?",
            "And second question is do you have any recommendations regarding 2 caches?",
            "Like should we change the caches or the caching strategy?",
            "Can we use this information for that as well OK?",
            "For the first question, we do not do not have yet a generic description of the results of like a summary of the performance, but this would be a next step to have, like a summary of performance over server and then similar to sparkless you could have like a online monitoring tool where you can retrieve the information about different servers to use in your query engine and what was the second question.",
            "Again, whether you could.",
            "Or how would you use this information for your caching strategy OK?",
            "I think it would make sense to not only use HTTP cache, but have a caching mechanism that leverages the fact that you have pages in TPF servers.",
            "And then you could re or pre load the the next pages and then you could parallelize retreiving pages.",
            "From a client side.",
            "So if you have a result with 100 pages, then you could cache them already and you could go through them faster in a parallel fashion instead of requesting one by one in a linear sequential way.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, thank you.",
                    "label": 0
                },
                {
                    "sent": "I will be presenting our work on querying large knowledge graphs over triple pattern fragments where we conducted in empirical study.",
                    "label": 1
                },
                {
                    "sent": "So first let's.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look into the motivation of this work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, as all of you know who attended the conference knowledge, graphs are everywhere and querying knowledge graphs is in a very important factor.",
                    "label": 0
                },
                {
                    "sent": "And currently we basically have, let's say, two very popular web interfaces to query knowledge graphs and RDF on the right hand side we have sparkle endpoints which are very expressive.",
                    "label": 1
                },
                {
                    "sent": "This means that it's harder on their servers.",
                    "label": 1
                },
                {
                    "sent": "More expensive on the servers, but it reduces the cost for the clients.",
                    "label": 0
                },
                {
                    "sent": "And the other side of the spectrum.",
                    "label": 1
                },
                {
                    "sent": "We have triple pattern fragments which are more lightweight on the servers.",
                    "label": 1
                },
                {
                    "sent": "However, they increased the cost on the client side and our work.",
                    "label": 0
                },
                {
                    "sent": "We wanted to look into more detail on how the performance of triple pattern fragments is when we query different types of triple patterns and also how different configurations of the server setup effect the response time of triple pattern FRAGMENTE servers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how did we approach this?",
                    "label": 0
                },
                {
                    "sent": "In our work we present our.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Research with which is the TPF profiler.",
                    "label": 0
                },
                {
                    "sent": "It's basically a software tool where you can plug in a TPF server URL.",
                    "label": 0
                },
                {
                    "sent": "You specify a number of triples that you want to be sampled from the triple pattern FRAGMENTE servers, and then you receive the study results, which are fine grained results on which on top of which you can do your analysis.",
                    "label": 0
                },
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "Let's take a look into.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the sampling component first in order to assess the performance of the servers, we generate a random sample.",
                    "label": 0
                },
                {
                    "sent": "So for example, let's say we set the parameter M2 one.",
                    "label": 0
                },
                {
                    "sent": "This means we sample one triple from the Knowledge graph knowledge graph, which is accessible via the TPF server, and we basically request a triple pattern with only variables.",
                    "label": 1
                },
                {
                    "sent": "This means via paginating, we can access any triple.",
                    "label": 0
                },
                {
                    "sent": "Of the graph, and then we basically randomly.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Select pages in this case, since M is one, we just select one page and a page consists of a number of triple pattern triples and we randomly select one of those triples.",
                    "label": 0
                },
                {
                    "sent": "In this case we select Monterrey is in EU.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Knighted states having this triple.",
                    "label": 0
                },
                {
                    "sent": "We can generate triple patterns based on this by simp.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We replacing the RDF terms of the triple with variables, so for each triple we can generate eight different triple patterns.",
                    "label": 0
                },
                {
                    "sent": "And this is how we generate the sample.",
                    "label": 0
                },
                {
                    "sent": "And then in the second step.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We basically evaluate the sample over the server.",
                    "label": 0
                },
                {
                    "sent": "This is done by requesting requesting each triple pattern from the sample, and then we record the response time and also we record other meta data such as the answer cardinality or the page size which is available via the meta data of the TPF servers.",
                    "label": 0
                },
                {
                    "sent": "And also we persist these samples so that we can reuse it for future evaluations.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is how we assess the performance.",
                    "label": 0
                },
                {
                    "sent": "What kind of setup that we use in our study we used two different environment.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First we used a controlled environment.",
                    "label": 0
                },
                {
                    "sent": "This means we set up the TPF profiler and the TPF server on the same server.",
                    "label": 0
                },
                {
                    "sent": "This allows us to have no network delays, however.",
                    "label": 0
                },
                {
                    "sent": "Of course, the profiler and the server share memory and CPU.",
                    "label": 0
                },
                {
                    "sent": "But it also allowed us to configure the TPF server so we can test different configurations and see how this effects the performance.",
                    "label": 0
                },
                {
                    "sent": "The second environment that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We looked into was a real world environment where we basically looked for publicly available servers and we run that EPF profiler on our local server and requested the triple patterns from the publicly available server.",
                    "label": 1
                },
                {
                    "sent": "This means, of course, that the server that we are requesting will have server workload from other requests.",
                    "label": 1
                },
                {
                    "sent": "Potentially there will be network delays and also the TPF server will not be configurable.",
                    "label": 0
                },
                {
                    "sent": "Because we have to use what's out there, basically.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What kind of knowledge graphs that we take into account?",
                    "label": 0
                },
                {
                    "sent": "We looked into visionary geonames DB Pedia and blpi as they are quite large.",
                    "label": 0
                },
                {
                    "sent": "Knowledge graphs and also we could find TPF servers online with these knowledge graphs for the back end types we looked into HTTP and sparkle however we could only set up a sparkletts server locally because all of the publicly available TPF servers use HTTP backends.",
                    "label": 0
                },
                {
                    "sent": "The third type of back end that you could use RDF documents we didn't include because we wanted to look into large knowledge graphs and they're just not feasible for that for the page sizes.",
                    "label": 0
                },
                {
                    "sent": "We looked into four different page size configurations.",
                    "label": 0
                },
                {
                    "sent": "105 hundred 1010 thousand an for the.",
                    "label": 0
                },
                {
                    "sent": "Real world environment.",
                    "label": 0
                },
                {
                    "sent": "We only had TF service available with the page size of 100.",
                    "label": 0
                },
                {
                    "sent": "Alright, let's take a look in.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the results of our experimental study.",
                    "label": 0
                },
                {
                    "sent": "So first of.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How does the triple pattern type impact the response time?",
                    "label": 1
                },
                {
                    "sent": "And here we can see the response time with respect to the different types of triple patterns and the different colors indicate the different knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "Overall, we can see that on the rather left hand side where we have more or fewer variables, the response times are a bit lower and on the right hand side as we have more variables, the response time seemed to increase.",
                    "label": 0
                },
                {
                    "sent": "Which stands out is that.",
                    "label": 0
                },
                {
                    "sent": "The triple pattern types where we have only the predicate instantiate.",
                    "label": 0
                },
                {
                    "sent": "It has the highest response time and where we have only variables has the second highest response time.",
                    "label": 0
                },
                {
                    "sent": "In our controlled setup, and this let us think.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe these triple pattern types result or have many answers, so that's why the response time is higher than for the other triple pattern types.",
                    "label": 0
                },
                {
                    "sent": "So we took a look.",
                    "label": 0
                },
                {
                    "sent": "In two, how does the answer cardinality impasse?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the response time.",
                    "label": 0
                },
                {
                    "sent": "And here you can see the response time with respect to the number of results, the answer cardinality the black line indicates the page size used in this case for the blpi knowledge graph in the controlled environment and you can see there appears to be a linear increase in response time up to the page size, and then thereafter it appears to be steady.",
                    "label": 1
                },
                {
                    "sent": "So we took a deeper look into that by computing the correlation for this again.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looking at the DB LP knowledge Graph here, indicated in green, are values above 0.5, meaning there's a positive linear correlation between the answer cardinality and the response time, and we've actually found that we find a linear correlation mostly for the virtual also back end type for HTT we didn't see that strong correlations and interesting Lee.",
                    "label": 1
                },
                {
                    "sent": "In the real world environment, there were no correlation at all, so it appears that network delays and other effects impact on the response time such that the differences induced by the answer cat analogies are somehow illuminated.",
                    "label": 0
                },
                {
                    "sent": "OK, next we also looked into the different page sizes.",
                    "label": 0
                },
                {
                    "sent": "So can we improve the response time by increasing the page size for instance?",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we looked into the controlled environment only because as I said before, publicly we only had publicly available service.",
                    "label": 0
                },
                {
                    "sent": "Only had a page size of 100, but in our controlled environment we set up page size of 105 hundred 1010 thousand and we can clearly see that there's an increase in the throughput when we increase the page size for both HTTP and Virtuoso back end from 100 to 500.",
                    "label": 0
                },
                {
                    "sent": "Answers per page and then thereafter the increase is not clear for all knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "It even decreases.",
                    "label": 0
                },
                {
                    "sent": "But this indicates that there's potential for improving configuration of these TPF servers to improve on the throughput.",
                    "label": 0
                },
                {
                    "sent": "Next we looked into the.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different back end types and first of all the main results are we looked into HTN virtual sets as backends and we found that of course HTT is more performant than virtuozzo with an average response time, which is half as high as the one for.",
                    "label": 0
                },
                {
                    "sent": "Also, and this kind of makes sense because when you use virtual also as a back end type, you basically use the TPF server to build another web interface on top of an existing interface.",
                    "label": 0
                },
                {
                    "sent": "And so it makes sense that the response times are higher.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, we looked into how to, how does paginating through the results impact the response time?",
                    "label": 0
                },
                {
                    "sent": "And so we basically requested the chip pattern with only variables to get access to the entire knowledge graph, and then we just paginated through and blue.",
                    "label": 0
                },
                {
                    "sent": "You can see the response time for HTTP back end, which are rather stable and don't seem to increase for the increasing number of pages, however for.",
                    "label": 0
                },
                {
                    "sent": "The Verge also back end.",
                    "label": 0
                },
                {
                    "sent": "And Please note this is a log scale.",
                    "label": 0
                },
                {
                    "sent": "We have an exponential almost exponential increase in the response time as we paginate through the results and this actually makes it impossible to access all triples from the knowledge graph we ever to.",
                    "label": 0
                },
                {
                    "sent": "Also in this way.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, another look into the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Controlled and the real world environment.",
                    "label": 0
                },
                {
                    "sent": "How do they differ?",
                    "label": 0
                },
                {
                    "sent": "What did we find there?",
                    "label": 0
                },
                {
                    "sent": "So first of all, the main difference is, as you can see on the top, you have way more outliers for the real world environment.",
                    "label": 0
                },
                {
                    "sent": "This is definitely due to network delays and server workload that influences the response time and what was very interesting to us is that in the real world environment the triple pattern only requesting.",
                    "label": 1
                },
                {
                    "sent": "With only variables, is the second fastest in the real world environment, while it's the second slowest in the controlled environment, and we think this is due to the fact that this results for this triple pattern is cashed on the real world server because it's frequently requested, for instance, every time you go onto the web interface of a TPF server it will request this triple pattern in order to display the first triples of the.",
                    "label": 0
                },
                {
                    "sent": "Knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "All right, summing up.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We found that the triple pattern type impacts on the response time and also that the triple pattern types that are more expensive typically also yield a higher answer cat analogy.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, we saw that the answer cardinality impacting response time doesn't really draw a clear picture because there is somehow a linear increase increase, but it's not visible for all back end types and not for all.",
                    "label": 0
                },
                {
                    "sent": "Datasets.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, the page size impacts on the throughput.",
                    "label": 0
                },
                {
                    "sent": "We saw that we can increase the throughput by increasing the page size and this means that you could actually use our tool to improve on the configuration of your TPF server by testing different page sizes and determining how it impacts on the throughput.",
                    "label": 0
                },
                {
                    "sent": "The back end type we saw a clear picture there.",
                    "label": 0
                },
                {
                    "sent": "HTT is the most suitable back end type for large knowledge graphs.",
                    "label": 1
                },
                {
                    "sent": "And for the environments we saw that you need to take different server characteristics into account.",
                    "label": 0
                },
                {
                    "sent": "When using TPF on the wild or in the wild in the real world.",
                    "label": 0
                },
                {
                    "sent": "Alright, last.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The concluding, how can you use our resource that EPF profiler?",
                    "label": 0
                },
                {
                    "sent": "You can use it to improve query engines and deploy new optimization techniques when using query engines that leverage TPF servers.",
                    "label": 0
                },
                {
                    "sent": "And on the other side you can use our resource to monitor TPF servers on the web.",
                    "label": 1
                },
                {
                    "sent": "If you have your own.",
                    "label": 0
                },
                {
                    "sent": "Server setup you can check and monitor the behavior of your service.",
                    "label": 0
                },
                {
                    "sent": "You can access our resource on GitHub.",
                    "label": 0
                },
                {
                    "sent": "We have a Jupiter notebook set up with all the statistical analysis that we did and all the visualizations and we have a demo this evening demo #32 so come check us out if you're more interested.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Healers good stuff, maybe just want fairness comment regarding Virtuoso.",
                    "label": 0
                },
                {
                    "sent": "I think we should mention Virtuosos is reader writes, obviously so, so this also explains different behavior.",
                    "label": 0
                },
                {
                    "sent": "I would say like agency is easy because it just read only.",
                    "label": 0
                },
                {
                    "sent": "Guarding the different patterns of follow up question would be because yes, some patterns are slower than this is obviously a trade in design of any file format, so I think the right question here is like.",
                    "label": 0
                },
                {
                    "sent": "Are the patterns that are slow like because if he would look at at all sparkle queries or these are patterns, we need least 'cause?",
                    "label": 0
                },
                {
                    "sent": "If not, it's really easy done just in the back end instead of storing SPO you switch them around.",
                    "label": 1
                },
                {
                    "sent": "So do you think that the right patterns are slow and the right ones are faster?",
                    "label": 0
                },
                {
                    "sent": "Would you have done it differently?",
                    "label": 1
                },
                {
                    "sent": "So if I understand correctly, is either slower patterns, those that are also used more frequently, slower patterns, the ones that we don't need offer like VRV for instance is slow.",
                    "label": 0
                },
                {
                    "sent": "Is that a problem because it might not be a problem because it may be just never occurs in sparkle queries we haven't looked into the number of incurrent occurrences of the different pattern types in actual queries, but that's a good good pointer to look into it, 'cause otherwise you just like flip the back ends right and then yeah.",
                    "label": 0
                },
                {
                    "sent": "And yeah, maybe improve the indexing module if you see that the if you VRV is used very often, then you could try to improve indexing for Viavi triples.",
                    "label": 0
                },
                {
                    "sent": "And also I mean, kudos to like releasing this as a tool as a resource that we can retry these things that we can optimize.",
                    "label": 0
                },
                {
                    "sent": "This is something I think we should all do way more so thanks.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "If not, then.",
                    "label": 0
                },
                {
                    "sent": "Hi I had a question on the.",
                    "label": 1
                },
                {
                    "sent": "When you use the experiment we have back end.",
                    "label": 0
                },
                {
                    "sent": "Did you try to to install index secondary index for for easing the pagination virtuoso in this case?",
                    "label": 0
                },
                {
                    "sent": "Or you just use the standard installation of because you can declare over indexing videos of course yeah we just use the standard setup.",
                    "label": 1
                },
                {
                    "sent": "But also as it is, you might tweak virtual also a little bit to improve the response times, but I think as we saw it's twice as slow as HTTP, so it doesn't really make sense to use it as a back end I think.",
                    "label": 0
                },
                {
                    "sent": "And that's actually, I mean one of the.",
                    "label": 0
                },
                {
                    "sent": "The motivations of PF services that you don't want to decrease the load on the servers by having similar requests and so building using Sparkle endpoints as a back end for a similar endpoint is kind of contradictive to me.",
                    "label": 0
                },
                {
                    "sent": "I don't think so, but OK, it's fine, Sir.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "If not, then.",
                    "label": 0
                },
                {
                    "sent": "Hi very nice tool.",
                    "label": 0
                },
                {
                    "sent": "Maybe one small comment regarding the whole virtuoso thing.",
                    "label": 0
                },
                {
                    "sent": "If I would change anything, I would maybe add more nuanced to the data sources, because I mean for private company like opening, this is very important, that everything that is communicated about their software is fair and honest, and everybody knows that because you know, like you said, you go through the sparkle interface, but in fact you could also go straight into the native code or future and implemented in that way so.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's more performant.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "I was I had two small questions so one if I would build a query engine that would use your tool to do some query optimization.",
                    "label": 0
                },
                {
                    "sent": "How does that work?",
                    "label": 0
                },
                {
                    "sent": "Is there a machine understandable explanation of the results?",
                    "label": 0
                },
                {
                    "sent": "And second question is do you have any recommendations regarding 2 caches?",
                    "label": 0
                },
                {
                    "sent": "Like should we change the caches or the caching strategy?",
                    "label": 0
                },
                {
                    "sent": "Can we use this information for that as well OK?",
                    "label": 0
                },
                {
                    "sent": "For the first question, we do not do not have yet a generic description of the results of like a summary of the performance, but this would be a next step to have, like a summary of performance over server and then similar to sparkless you could have like a online monitoring tool where you can retrieve the information about different servers to use in your query engine and what was the second question.",
                    "label": 0
                },
                {
                    "sent": "Again, whether you could.",
                    "label": 0
                },
                {
                    "sent": "Or how would you use this information for your caching strategy OK?",
                    "label": 0
                },
                {
                    "sent": "I think it would make sense to not only use HTTP cache, but have a caching mechanism that leverages the fact that you have pages in TPF servers.",
                    "label": 0
                },
                {
                    "sent": "And then you could re or pre load the the next pages and then you could parallelize retreiving pages.",
                    "label": 0
                },
                {
                    "sent": "From a client side.",
                    "label": 0
                },
                {
                    "sent": "So if you have a result with 100 pages, then you could cache them already and you could go through them faster in a parallel fashion instead of requesting one by one in a linear sequential way.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}