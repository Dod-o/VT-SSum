{
    "id": "4as6pbvhhulk7icifbem2paqitedalmr",
    "title": "Efficient Learning of Linear Separators under Bounded Noise",
    "info": {
        "author": [
            "Ruth Urner, Computer Science Department, Carnegie Mellon University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_urner_linear_separators/",
    "segmentation": [
        [
            "OK so I should say this is a joint work with parental OSD, Nina, Balcan and Nicaragua."
        ],
        [
            "And yeah, so since I have very little time here, so let me start with stating our main result.",
            "So we looked at a very classic problem in learning.",
            "It's actually the one that I'm it also just talked about.",
            "We looked looked at learning linear linear classifiers efficiently, and we showed that there is computationally efficient algorithm that learns half spaces on the uniform distribution in the unit ball.",
            "Another massage noise condition and I'll explain in a moment what I mean by this noise condition.",
            "But let me start with."
        ],
        [
            "Putting this result a little bit into context, so there's long, lat, long lines of work both on the statistical learning and the computational learning community, and what's interesting is that these communities have looked at kind of different types of assumptions and sometimes also different objectives.",
            "So for example, computationally efficient algorithms are very often shown to converge to some constant times the optimal where's where's from a statistical point of view, really?",
            "What we would like is that as we see more and more data.",
            "Our method really should converge to the to the.",
            "Best possible and similarly.",
            "In the computational community, people have looked at noise models like uniform classification noise or what they call the malicious or adversarial noise models, whereas fast statistical rates have been thrown under anti backofen massage type of noise assumptions and let me now quickly say what I mean by by my son noise I mean that the base classifier is in our class.",
            "When our case it will be a via linear classifier and there's a noise wait.",
            "So the noise for every power point is bounded away from 1/2, but it's not necessarily uniformly the same noise noise wait for every point.",
            "So the question that we ask this is learning computationally easing in this setting where it is statistically easy and so now you already know from my previous site that the answer is yes, but let me tell you that."
        ],
        [
            "Actually not that easy, so we looked at a couple of like obvious candidates and we also.",
            "In particular, we show that formally in this work that averaging algorithm and hingeless minimization do not succeed."
        ],
        [
            "At this task, so in particular for hingeless minimization, we show that even under what seems maybe like very benign conditions, so we have a bounded domain uniform distribution over the unit ball for arbitrary small bound on WhatsApp, type of noise hingeless minimization were not necessarily converge to the base, which is a linear classifier.",
            "So."
        ],
        [
            "Hingeless minimizations by itself doesn't work, but what does work is we can actually save it.",
            "What does work is we can do in just memorization in rounds with a margin based technique.",
            "So we we optimize their dangers and smaller and smaller bands around our current classifier, and we can thereby force force the output of our algorithm to convert to to the Bayes optimal.",
            "So this actually achieves arbitrarily small excess error in polynomial time.",
            "And if you would like to hear more."
        ],
        [
            "I'd be happy to talk to you at the post."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I should say this is a joint work with parental OSD, Nina, Balcan and Nicaragua.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And yeah, so since I have very little time here, so let me start with stating our main result.",
                    "label": 0
                },
                {
                    "sent": "So we looked at a very classic problem in learning.",
                    "label": 0
                },
                {
                    "sent": "It's actually the one that I'm it also just talked about.",
                    "label": 0
                },
                {
                    "sent": "We looked looked at learning linear linear classifiers efficiently, and we showed that there is computationally efficient algorithm that learns half spaces on the uniform distribution in the unit ball.",
                    "label": 1
                },
                {
                    "sent": "Another massage noise condition and I'll explain in a moment what I mean by this noise condition.",
                    "label": 0
                },
                {
                    "sent": "But let me start with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Putting this result a little bit into context, so there's long, lat, long lines of work both on the statistical learning and the computational learning community, and what's interesting is that these communities have looked at kind of different types of assumptions and sometimes also different objectives.",
                    "label": 1
                },
                {
                    "sent": "So for example, computationally efficient algorithms are very often shown to converge to some constant times the optimal where's where's from a statistical point of view, really?",
                    "label": 0
                },
                {
                    "sent": "What we would like is that as we see more and more data.",
                    "label": 0
                },
                {
                    "sent": "Our method really should converge to the to the.",
                    "label": 0
                },
                {
                    "sent": "Best possible and similarly.",
                    "label": 0
                },
                {
                    "sent": "In the computational community, people have looked at noise models like uniform classification noise or what they call the malicious or adversarial noise models, whereas fast statistical rates have been thrown under anti backofen massage type of noise assumptions and let me now quickly say what I mean by by my son noise I mean that the base classifier is in our class.",
                    "label": 1
                },
                {
                    "sent": "When our case it will be a via linear classifier and there's a noise wait.",
                    "label": 0
                },
                {
                    "sent": "So the noise for every power point is bounded away from 1/2, but it's not necessarily uniformly the same noise noise wait for every point.",
                    "label": 0
                },
                {
                    "sent": "So the question that we ask this is learning computationally easing in this setting where it is statistically easy and so now you already know from my previous site that the answer is yes, but let me tell you that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually not that easy, so we looked at a couple of like obvious candidates and we also.",
                    "label": 0
                },
                {
                    "sent": "In particular, we show that formally in this work that averaging algorithm and hingeless minimization do not succeed.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At this task, so in particular for hingeless minimization, we show that even under what seems maybe like very benign conditions, so we have a bounded domain uniform distribution over the unit ball for arbitrary small bound on WhatsApp, type of noise hingeless minimization were not necessarily converge to the base, which is a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hingeless minimizations by itself doesn't work, but what does work is we can actually save it.",
                    "label": 0
                },
                {
                    "sent": "What does work is we can do in just memorization in rounds with a margin based technique.",
                    "label": 0
                },
                {
                    "sent": "So we we optimize their dangers and smaller and smaller bands around our current classifier, and we can thereby force force the output of our algorithm to convert to to the Bayes optimal.",
                    "label": 0
                },
                {
                    "sent": "So this actually achieves arbitrarily small excess error in polynomial time.",
                    "label": 1
                },
                {
                    "sent": "And if you would like to hear more.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd be happy to talk to you at the post.",
                    "label": 0
                }
            ]
        }
    }
}