{
    "id": "vyvauufbgrnpofq76hhql3sn4lqapi2k",
    "title": "Near-Optimal MAP Inference for Determinantal Point Processes",
    "info": {
        "author": [
            "Jennifer A. Gillenwater, University of Pennsylvania"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nips2012_gillenwater_processes/",
    "segmentation": [
        [
            "Jennifer Gillenwater and this is joint work with Alex Kulesa and Ben Tasker on approximating the mode of Determinantal point processes or DP."
        ],
        [
            "Peace for short.",
            "So if someone doesn't image search for Jaguar and gets back these results, they might be disappointed.",
            "If they're instead interested and Jaguar, the animal or Jaguars the football team.",
            "But the second set of results where the image is sort of span the diverse meanings of Jaguar is probably better than in general when we're selecting a subset from the ground."
        ],
        [
            "Of items, for example, all images related to Jaguars.",
            "They're often two goals to balance.",
            "So first of all, we want the subset to contain high quality items.",
            "And Secondly, we want the subset to be diverse.",
            "So let's go through.",
            "In a second example."
        ],
        [
            "Yes.",
            "Consider the following task were given some set of documents and we want to select a set of document pairs such that the pairs are high quality, meaning the documents within a pair of similar and Secondly such that the overall set of pairs is diverse.",
            "Illustrate this."
        ],
        [
            "Suppose we have a collection of statements that a political candidate made during 1 timeframe and statements he made during another.",
            "If the seller selecting from is old, new statement pairs will encounter pairs of varying quality.",
            "For example, this pair would be high quality because the statements are topically similar both about financial bailouts, whereas this pair would be low quality because the second quote focuses on a different topic.",
            "If we select."
        ],
        [
            "Asset based only on pair quality.",
            "We might get the three pairs shown here.",
            "The first ones about an instance of a financial bailout, the SEC, about another instance of a financial bailout, and the thirds about another.",
            "Yet another instance.",
            "So these pairs are all about money and bailouts, which only tells us that it used to be against bailouts.",
            "And now he's for them.",
            "But if instead we balance quality"
        ],
        [
            "Diversity the resulting set might look more like this one.",
            "We see opinions on multiple issues, not just bailouts, but we also get information about abortion and gun control.",
            "So now that we've established some basic motivation for solving a subset selection problem."
        ],
        [
            "Our end goal is a high quality but diverse set.",
            "Let's look at a couple of natural measures for quality and diversity and these will lead us eventually to the definition of the determinantal point process.",
            "So if an item I for example, an image or sentence is represented by this feature vector GI, we could define its quality to be the vectors magnitude and its similarity to another item to be the dot product of feature vectors."
        ],
        [
            "So as a measure of how good the set consisting of these two vectors is, we want some expression that rewards high quality and penalizes similarities such as this formula.",
            "The first term here is the product of the items qualities and the second term is the square of their similarity.",
            "The square root of the."
        ],
        [
            "Expression also happens to be equal to the area of the parallelogram that the vectors define, which is going to allow us to reason geometrically about the goodness of a set, and it will make for some nice visuals later."
        ],
        [
            "On in addition to being related to area, the set goodness formula can be written as the following determinant.",
            "Further, if we define a matrix G where each row in this matrix is a feature vector of one of the items in our ground set, then the goodness of us at IJ."
        ],
        [
            "It's just the determinant of Rose I&J of G times their transpose."
        ],
        [
            "For two items, we've now established that the goodness of a set can be written as area, which can be written as a determinant, and it turns out that the geometric intuitions extend to sets of all sizes.",
            "If we replace this area with some K dimensional volume, so for a set of size 1, volume is length for a set of size 2, its area and 1st set of size 3, it's 3 dimensional volume, and so on.",
            "And in general, if we have a set Y that contains D items, the goodness of this set can be written as the Square D dimensional volume of the set.",
            "Or is the determinant of the submatrix of this GG transpose with rows and columns indexed by the set Y?",
            "So we're going to find L is shorthand for GG transpose, and if we normalize this measure over all possible sets, then the result is a probability distribution.",
            "This P of Y here and this probably."
        ],
        [
            "The distribution is exactly the definition of a determinantal point process."
        ],
        [
            "Which will refer to as a DP for short."
        ],
        [
            "DPS have a lot of nice properties, so when we do inference in DPS, there's several operations that we're concerned with that can be done exactly in relatively efficiently.",
            "In particular here a couple of examples that can be done in time CUBICIN N, where N is here.",
            "The total size of the ground set of items were selecting from, for instance, the total number of sentences in a set of documents for summarizing.",
            "Normalization can be done with a single determinant computation due to the identity shown here, and marginalization, conditioning and sampling can also be done exactly in cubic time.",
            "But I'm more concerned with a somewhat different problem, nice."
        ],
        [
            "Namely, selecting the single best set, which is the map of the DPP, let's give a little more intuition about DP map."
        ],
        [
            "Suppose a ground set of items.",
            "We have to choose a subset from is shown here, and the feature vectors are defined such that L is a Gaussian kernel.",
            "Now."
        ],
        [
            "And sampling points independently would look like this.",
            "And sampling from a DP distribution which recalls the determinant L sub?",
            "Why normalized by the expression from the previous slide?"
        ],
        [
            "Sampling from a DP would look like this, where the points are a little bit more diverse than the independent sample case and finally the map would look something like this where the selected points are very well separated.",
            "Unfortunately exactly solving DP map is NP hard."
        ],
        [
            "However, we can approximate the map mostly due to the fact that determinant is log submodular.",
            "So this means that obeys the following diminishing returns property.",
            "We're adding a new element ketua set X has more of an impact on the determinant than adding ketua super set of X, which is why here, so we'll take a look at this with a small example to see why it's true for determinant."
        ],
        [
            "Since we saw earlier determine is proportional to volume squared, will just look at volumes here.",
            "Suppose this horizontal vector is the only item in set X and this other vector represents an item we're considering adding to the set X.",
            "If the length of the first vector is B1 and the area spanned by both vectors, this parallelogram is given by the base times height formula.",
            "We can see that adding item K to the set X changes the volume by a factor that's equal to the height.",
            "OK, so if instead of our initial set was called this new set Y we had a super set of X that included the third vector shown here, then the three vectors would span this blue parallelogram."
        ],
        [
            "And the two vectors in our initial set would spend this base whose area will call B2 the three vector volume can be computed using the base times height formula again and taking the ratio of the three vector volume to the two vector volume.",
            "We see that again adding item K changes the volume by a factor that's equal to the height.",
            "But note that the height will be smaller in this case."
        ],
        [
            "Since height is defined as the shortest distance to the base, so we can find a shorter distance with B2, then we could with only be one.",
            "Thus we have that height one is greater than height two, which gives us exactly the diminishing returns property.",
            "That implies determinant is log submodular."
        ],
        [
            "Many algorithms for approximating the map of a submodular function rely on the function also being monotone, though, and this means that the value of a set X can only grow with the addition of items.",
            "However, determinant is non monotone.",
            "For example, consider an instance where this act where are set, X is just this one vector with length 10.",
            "Then if we added another item, this could result in a lesser area if the New Times magnitudes too small or it's too similar to the initial item."
        ],
        [
            "Let's look at some prior work with submodular functions.",
            "There was a greedy algorithm published in 1978 and it gives a guarantee for monotone functions.",
            "For non monotone functions, there's more recent work that proposes another greedy algorithm, and it gives a 1/2 approximation, but in practice we found that the second algorithm performs very poorly even compared to the 1978 algorithm.",
            "So in this work, we instead build off of 2011 paper that gives a 1/4 approximation for non monotone and this work by chicory further has the advantage of being able to elegantly incorporate constraints.",
            "This means that instead of just solving 'em."
        ],
        [
            "Ax over all subsets.",
            "Why we consider we can consider restricting Y to the interior of some solvable polytope which we'll denote by S. Let's briefly look at an example of why these kind of constraints might be useful."
        ],
        [
            "Imagine that you have two images and you run a detector on each to identify key points, and we're visualizing that here with these two point sets.",
            "Now, if you want to compare the two images, one reasonable thing to do is to match the key points in one image to those in the other."
        ],
        [
            "This third image here superimposes the key points from the first 2 and indicates with green lines and pairs that are fairly close good quality matches.",
            "To improve the accuracy of our image comparison, we might want to impose the one to one matching constraint that says no key point in image one can map to more than one key point in Image 2 and vice versa."
        ],
        [
            "If just the highest quality pairs obeying this constraint were selected, it would look like this.",
            "But if we use our algorithm to compute the approximate DP map that obeys the constraints, the result is much more diverse with only slightly longer edit edges and the advantage of the DP map here is that it takes into account a wider variety of regions in the two images.",
            "So whereas this highest quality method may indicate 2 images are very similar, even if there are large regions of disparity, the DP map map method probably won't make this map."
        ],
        [
            "Jake have you moderated the constraint problem a little bit?",
            "Let's look at the previously mentioned prior work that handles constraints well and has a 1/4 approximation guarantee for non monotone.",
            "The first step in their method is to represent each set by its characteristic vector X.",
            "So for example for this at 2 four we get this vector and then they relax X such that XI ranges from zero to one representing the probability of including an item I."
        ],
        [
            "The second step is to extend the objective with an expectation says commonly called the multilinear extension, and what we mean by this expectation is that each item I from the ground set is included in Y with some probability XI.",
            "In other words, we flip independent coins for each item, giving the following expression the center."
        ],
        [
            "Right here is just the probability of a set Y."
        ],
        [
            "Some here is over all possible two to the N subsets of our ground set.",
            "So."
        ],
        [
            "Three in this method is to optimize this objective using gradient based methods and the final step is rounding a short proof here shows this final step is really only necessary if the problem is constrained, so this overall procedure has some nice approximation guarantees, but it's somewhat."
        ],
        [
            "Practical due to this some which requires us to use multi Monte Carlo estimation.",
            "Our work proposes a fix for that inefficiency."
        ],
        [
            "Specifically, we apply all the same steps as in the trickery prior work, but we change the objective slightly.",
            "Instead of this multilinear extension we optimize will call the softmax extension and it's very similar to the multilinear with just this log term pulled outside.",
            "Let's take a quick."
        ],
        [
            "Look at what these objectives look like for this.",
            "These two vectors do you want in G2 here where G2 has a slightly larger magnitude than G1?",
            "So the domain of these functions is this plane."
        ],
        [
            "And if we look at the multilinear extension, it looks like this.",
            "And here."
        ],
        [
            "The softmax, which upper bounds it?",
            "If we look at."
        ],
        [
            "Integral points then the.",
            "These are the points where XIR zero or one.",
            "We recover exactly the determinant of LY, which you can see a slightly larger at this front corner than at this back corner, because the front corner score corresponds to the set y = 2, where is the back corner corresponds to the set, y = 1, and the magnitude of the two is slightly greater than that of G1."
        ],
        [
            "So softmax and multilinear closely related, related, but the slight difference with the log placement is very important, because it means that for the submodular function determinant were able to prove the softmax objective can be simplified to the computation of a single determinant shown here.",
            "So now we have an objective, and in fact also a derivative that sufficient to compute.",
            "So natural next question to ask is, does softmax also come with approximation guarantees?"
        ],
        [
            "And the main property that Multilinear relies on for its guarantee, is that it's concave in any all positive or all negative direction.",
            "So for any direction where X1 and X2 are both increasing or both decreasing as illustrated here, this turns out to be true for softmax as well, so."
        ],
        [
            "So while neither objective is concave in general, the restricted concavity property from the previous slide is enough to give us a 1/4 approximation guarantee."
        ],
        [
            "To summarize, we prove softmax is concave in all positive directions, and by combining this with submodularity of log determinant, we're further able to show that the local optimum reached by following the gradient is at least a 1/4 approximation to those softmax optimum.",
            "Additionally, in the unconstrained case, we prove that the local optimum is integer, meaning that no roundings necessary, and we can directly infer 1/4 approximation to the true DP math objective.",
            "So for the constraint setting, we don't have any guarantees due to the need to round, but we found in practice that there are various simple rounding techniques that perform well.",
            "On that note, let's talk about some experiments.",
            "The baseline we compare 2."
        ],
        [
            "Previously mentioned greedy algorithm and the basic idea of this algorithm is that we start from the empty set.",
            "We find the single item to add that most increases that determine it, and then we."
        ],
        [
            "Great.",
            "We have results for an unconstrained setting Anna constraint setting where both are similar to the previous illustrations of the settings."
        ],
        [
            "So here the results on the X axis.",
            "We have the size of the ground set N and on the Y axis the log ratio of the softmax score to the greedy score.",
            "Each data point in this graph summarizes the result of running our optimization.",
            "On 100 randomly generated ground sets, so the red line shows the median ratio and the blue lines are the 1st and 3rd quartiles, red line above 0 means softmax wins over greedy as the size of the ground set grows.",
            "You can see that softmax advantage over greedy tends to increase, and this is even more pronounced into the one in the one to one constraint case where you can see that the ratios indicated on the Y axis are much larger."
        ],
        [
            "In addition to valuing their effectiveness of softmax, we check it sufficiency.",
            "So a naive implementation of the greedy algorithm we compare two would be orderin to the Fifth Ann.",
            "You may have heard about less naive implementations, sometimes called lazy, greedy.",
            "But in this case we can actually make it even faster than that just by exploiting the fact that DPS are closed under conditioning.",
            "This gives us a.",
            "Order end of the fourth version and it's this optimized version that we compare to here.",
            "So taking a look at the unconstrained."
        ],
        [
            "Case for small ground sets.",
            "Greedy is a bit faster, but as the size of the ground set grows, softmax advantage becomes apparent.",
            "In the constraint case, the trend is not as clear, but we do see that the ground set as its eyes gets above maybe 150 softmax starts trending towards greater efficiency.",
            "So to summarize, softmax is better at optimizing determinant and for larger ground sets it could even be faster than greedy.",
            "Finally, let's consider some real world."
        ],
        [
            "Data and a match summarization task is sort of similar to the one to one constrained synthetic setting and is also similar to the second motivational example from the beginning of the talk we took the 20 Republican primary debates from the previous US presidential election, and we extracted all statements made by the 8 main candidates, which gave us an average 179 quotes per candidate.",
            "And then for each pair of candidates for."
        ],
        [
            "Sample Romney and Santorum.",
            "We let the ground set of items consist of pairs of Romney Santorum."
        ],
        [
            "Payments, for instance.",
            "Here's one pair of statements.",
            "The words here are all stem nouns from one of the 332 statements by Romney and one of the 208 statements by Santorum and Assize Award.",
            "Here indicates its frequency within the quote.",
            "This would be a high quality pair, since the two statements have similar word distributions, so the overall DP map task we set for ourselves was to select a set of such pairs such that the statements within a pair of similar but the overall set of pairs is diverse.",
            "Basically a match summarization comparing the viewpoints of the two candidates."
        ],
        [
            "Let's take a look at part of a match somewhere that softmax creates.",
            "I'm going to go through a few examples snippets taken from within Romney statements, and here the color of a statement indicates it's general topic.",
            "So we have a statement about taxes, no tax on interest, dividends, or capital gains, a statement about law statement about healthcare, statement about foreign aid, and another statement about healthcare, and similarly for Santora.",
            "We have a statement about taxes, another statement about taxes, statement about foreign aid.",
            "A statement about ethanol in a statement about health care.",
            "Finally, if we run our softmax optimization."
        ],
        [
            "To select a set of statement pairs, here are a few of the pairs that are selected to include in the map set.",
            "The quotes are clearly diverse in topic, but within a pair we have similar topics and using this match summary we can do deduce what topics are important to both candidates, how they differ on these topics, for example, from this summer it's easy to see that Romney and Santorum disagree about capital gains tax and foreign aid, but they have similar views on Obamacare."
        ],
        [
            "We also compared softmax degree on this task and along the X axis.",
            "Here is an auxiliary parameter, basically a scaling factor on the quality scores of the pairs and the Y axis is again the log probability ratio which is always above 0 in this case, indicating the softmax consistently beats greedy for this data."
        ],
        [
            "To conclude, we have an efficient effective approximate DP map algorithm.",
            "It can be applied to all manner of subset selection problems.",
            "The code and the political data can be found on this site which is linked off my web page and we believe that there are many other subset selection problems besides those illustrated in this talk that softmax can be applied to a few such problems are listed here, and there's also possibly other submodular functions for which softmax is efficiently computable."
        ],
        [
            "So for any questions there on time for now, please come by the poster.",
            "Question hi, so your algorithms is N to the power four and you have ideas for extending this to the structure output output case where N might itself be exponentially large.",
            "So you can extend these kind of algorithms to a structure case if you work in the space of the features, it might be possible, so we haven't worked this out yet, but there's something called structure DPS, which sort of instead of computing GG transpose.",
            "I'm working with that computes G transpose G and works in the feature space, so maybe there might be an extension that way.",
            "Question, yeah, this maximizing the determinant.",
            "Overall sub matrices is a beautiful problem.",
            "You mentioned that it's known to be NP hard.",
            "Is anything known about NP hardness of approximating to within?",
            "You know 90% or some ratio like that?",
            "I'm not sure about that.",
            "I have to take that offline OK. Next so you work, your baseline was using the greedy algorithm from Nemhauser from 1978, and that was.",
            "That's an algorithm that doesn't have any guarantees for the non monotone so much lower Max and I was wondering you said also that the book been doing Warren Feldman Schwartz algorithm that gives you the 1/2 approximation for the non monotone case didn't work for you and so the question is.",
            "Did you run at one time or did you actually run it multiple times and take the cumulative best over multiple runs?",
            "Multiple linear sweeps?",
            "That is, so I think we implemented the deterministic version of it so you only read approximation.",
            "You only run it once, right once, so why not?",
            "Why did you do that?",
            "Why not run it multiple times?",
            "Well, the deterministic version is already.",
            "I thought it supposed to be a 1/3 approximation and so we give that a try an it really did not do well.",
            "I have the experimental results from it up on this slide.",
            "Um, so certainly it might have done better if we'd given it a bunch of of different restarts.",
            "That's something we could try.",
            "Last quick question here.",
            "So it's very nicely.",
            "You can compute the advantage of the pieces that you can efficiently compute the partition function, but in the end, if you're going to do map inference, that sort of takes away that advantage, right?",
            "You're not using that aspect of it, so in retrospect, if you're going to use any inference, would you be better off?",
            "Using some other learning objective that was just a little more like discriminative.",
            "So there have been a bunch of recent papers about DPS that have sort of compared to other learning methods, and they've successfully shown the DPS tend to give better performance like they've done it for some.",
            "Some image selections pose tracking and things like that, and.",
            "I feel like we didn't bother to compare to other methods in this paper just because there are already several papers where DPS have been proven to be better than other other methods like that.",
            "OK, time is up.",
            "Thank you Jennifer."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jennifer Gillenwater and this is joint work with Alex Kulesa and Ben Tasker on approximating the mode of Determinantal point processes or DP.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Peace for short.",
                    "label": 0
                },
                {
                    "sent": "So if someone doesn't image search for Jaguar and gets back these results, they might be disappointed.",
                    "label": 1
                },
                {
                    "sent": "If they're instead interested and Jaguar, the animal or Jaguars the football team.",
                    "label": 0
                },
                {
                    "sent": "But the second set of results where the image is sort of span the diverse meanings of Jaguar is probably better than in general when we're selecting a subset from the ground.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of items, for example, all images related to Jaguars.",
                    "label": 0
                },
                {
                    "sent": "They're often two goals to balance.",
                    "label": 1
                },
                {
                    "sent": "So first of all, we want the subset to contain high quality items.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, we want the subset to be diverse.",
                    "label": 0
                },
                {
                    "sent": "So let's go through.",
                    "label": 0
                },
                {
                    "sent": "In a second example.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Consider the following task were given some set of documents and we want to select a set of document pairs such that the pairs are high quality, meaning the documents within a pair of similar and Secondly such that the overall set of pairs is diverse.",
                    "label": 0
                },
                {
                    "sent": "Illustrate this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose we have a collection of statements that a political candidate made during 1 timeframe and statements he made during another.",
                    "label": 0
                },
                {
                    "sent": "If the seller selecting from is old, new statement pairs will encounter pairs of varying quality.",
                    "label": 0
                },
                {
                    "sent": "For example, this pair would be high quality because the statements are topically similar both about financial bailouts, whereas this pair would be low quality because the second quote focuses on a different topic.",
                    "label": 0
                },
                {
                    "sent": "If we select.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asset based only on pair quality.",
                    "label": 0
                },
                {
                    "sent": "We might get the three pairs shown here.",
                    "label": 0
                },
                {
                    "sent": "The first ones about an instance of a financial bailout, the SEC, about another instance of a financial bailout, and the thirds about another.",
                    "label": 0
                },
                {
                    "sent": "Yet another instance.",
                    "label": 0
                },
                {
                    "sent": "So these pairs are all about money and bailouts, which only tells us that it used to be against bailouts.",
                    "label": 0
                },
                {
                    "sent": "And now he's for them.",
                    "label": 0
                },
                {
                    "sent": "But if instead we balance quality",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diversity the resulting set might look more like this one.",
                    "label": 0
                },
                {
                    "sent": "We see opinions on multiple issues, not just bailouts, but we also get information about abortion and gun control.",
                    "label": 0
                },
                {
                    "sent": "So now that we've established some basic motivation for solving a subset selection problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our end goal is a high quality but diverse set.",
                    "label": 0
                },
                {
                    "sent": "Let's look at a couple of natural measures for quality and diversity and these will lead us eventually to the definition of the determinantal point process.",
                    "label": 0
                },
                {
                    "sent": "So if an item I for example, an image or sentence is represented by this feature vector GI, we could define its quality to be the vectors magnitude and its similarity to another item to be the dot product of feature vectors.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a measure of how good the set consisting of these two vectors is, we want some expression that rewards high quality and penalizes similarities such as this formula.",
                    "label": 0
                },
                {
                    "sent": "The first term here is the product of the items qualities and the second term is the square of their similarity.",
                    "label": 0
                },
                {
                    "sent": "The square root of the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expression also happens to be equal to the area of the parallelogram that the vectors define, which is going to allow us to reason geometrically about the goodness of a set, and it will make for some nice visuals later.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On in addition to being related to area, the set goodness formula can be written as the following determinant.",
                    "label": 0
                },
                {
                    "sent": "Further, if we define a matrix G where each row in this matrix is a feature vector of one of the items in our ground set, then the goodness of us at IJ.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just the determinant of Rose I&J of G times their transpose.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For two items, we've now established that the goodness of a set can be written as area, which can be written as a determinant, and it turns out that the geometric intuitions extend to sets of all sizes.",
                    "label": 1
                },
                {
                    "sent": "If we replace this area with some K dimensional volume, so for a set of size 1, volume is length for a set of size 2, its area and 1st set of size 3, it's 3 dimensional volume, and so on.",
                    "label": 0
                },
                {
                    "sent": "And in general, if we have a set Y that contains D items, the goodness of this set can be written as the Square D dimensional volume of the set.",
                    "label": 0
                },
                {
                    "sent": "Or is the determinant of the submatrix of this GG transpose with rows and columns indexed by the set Y?",
                    "label": 0
                },
                {
                    "sent": "So we're going to find L is shorthand for GG transpose, and if we normalize this measure over all possible sets, then the result is a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "This P of Y here and this probably.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The distribution is exactly the definition of a determinantal point process.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which will refer to as a DP for short.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "DPS have a lot of nice properties, so when we do inference in DPS, there's several operations that we're concerned with that can be done exactly in relatively efficiently.",
                    "label": 0
                },
                {
                    "sent": "In particular here a couple of examples that can be done in time CUBICIN N, where N is here.",
                    "label": 0
                },
                {
                    "sent": "The total size of the ground set of items were selecting from, for instance, the total number of sentences in a set of documents for summarizing.",
                    "label": 0
                },
                {
                    "sent": "Normalization can be done with a single determinant computation due to the identity shown here, and marginalization, conditioning and sampling can also be done exactly in cubic time.",
                    "label": 0
                },
                {
                    "sent": "But I'm more concerned with a somewhat different problem, nice.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Namely, selecting the single best set, which is the map of the DPP, let's give a little more intuition about DP map.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose a ground set of items.",
                    "label": 0
                },
                {
                    "sent": "We have to choose a subset from is shown here, and the feature vectors are defined such that L is a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And sampling points independently would look like this.",
                    "label": 0
                },
                {
                    "sent": "And sampling from a DP distribution which recalls the determinant L sub?",
                    "label": 0
                },
                {
                    "sent": "Why normalized by the expression from the previous slide?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sampling from a DP would look like this, where the points are a little bit more diverse than the independent sample case and finally the map would look something like this where the selected points are very well separated.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately exactly solving DP map is NP hard.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, we can approximate the map mostly due to the fact that determinant is log submodular.",
                    "label": 1
                },
                {
                    "sent": "So this means that obeys the following diminishing returns property.",
                    "label": 1
                },
                {
                    "sent": "We're adding a new element ketua set X has more of an impact on the determinant than adding ketua super set of X, which is why here, so we'll take a look at this with a small example to see why it's true for determinant.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since we saw earlier determine is proportional to volume squared, will just look at volumes here.",
                    "label": 0
                },
                {
                    "sent": "Suppose this horizontal vector is the only item in set X and this other vector represents an item we're considering adding to the set X.",
                    "label": 0
                },
                {
                    "sent": "If the length of the first vector is B1 and the area spanned by both vectors, this parallelogram is given by the base times height formula.",
                    "label": 0
                },
                {
                    "sent": "We can see that adding item K to the set X changes the volume by a factor that's equal to the height.",
                    "label": 0
                },
                {
                    "sent": "OK, so if instead of our initial set was called this new set Y we had a super set of X that included the third vector shown here, then the three vectors would span this blue parallelogram.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the two vectors in our initial set would spend this base whose area will call B2 the three vector volume can be computed using the base times height formula again and taking the ratio of the three vector volume to the two vector volume.",
                    "label": 0
                },
                {
                    "sent": "We see that again adding item K changes the volume by a factor that's equal to the height.",
                    "label": 0
                },
                {
                    "sent": "But note that the height will be smaller in this case.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since height is defined as the shortest distance to the base, so we can find a shorter distance with B2, then we could with only be one.",
                    "label": 0
                },
                {
                    "sent": "Thus we have that height one is greater than height two, which gives us exactly the diminishing returns property.",
                    "label": 0
                },
                {
                    "sent": "That implies determinant is log submodular.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many algorithms for approximating the map of a submodular function rely on the function also being monotone, though, and this means that the value of a set X can only grow with the addition of items.",
                    "label": 0
                },
                {
                    "sent": "However, determinant is non monotone.",
                    "label": 0
                },
                {
                    "sent": "For example, consider an instance where this act where are set, X is just this one vector with length 10.",
                    "label": 1
                },
                {
                    "sent": "Then if we added another item, this could result in a lesser area if the New Times magnitudes too small or it's too similar to the initial item.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at some prior work with submodular functions.",
                    "label": 0
                },
                {
                    "sent": "There was a greedy algorithm published in 1978 and it gives a guarantee for monotone functions.",
                    "label": 0
                },
                {
                    "sent": "For non monotone functions, there's more recent work that proposes another greedy algorithm, and it gives a 1/2 approximation, but in practice we found that the second algorithm performs very poorly even compared to the 1978 algorithm.",
                    "label": 0
                },
                {
                    "sent": "So in this work, we instead build off of 2011 paper that gives a 1/4 approximation for non monotone and this work by chicory further has the advantage of being able to elegantly incorporate constraints.",
                    "label": 0
                },
                {
                    "sent": "This means that instead of just solving 'em.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ax over all subsets.",
                    "label": 0
                },
                {
                    "sent": "Why we consider we can consider restricting Y to the interior of some solvable polytope which we'll denote by S. Let's briefly look at an example of why these kind of constraints might be useful.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imagine that you have two images and you run a detector on each to identify key points, and we're visualizing that here with these two point sets.",
                    "label": 0
                },
                {
                    "sent": "Now, if you want to compare the two images, one reasonable thing to do is to match the key points in one image to those in the other.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This third image here superimposes the key points from the first 2 and indicates with green lines and pairs that are fairly close good quality matches.",
                    "label": 0
                },
                {
                    "sent": "To improve the accuracy of our image comparison, we might want to impose the one to one matching constraint that says no key point in image one can map to more than one key point in Image 2 and vice versa.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If just the highest quality pairs obeying this constraint were selected, it would look like this.",
                    "label": 1
                },
                {
                    "sent": "But if we use our algorithm to compute the approximate DP map that obeys the constraints, the result is much more diverse with only slightly longer edit edges and the advantage of the DP map here is that it takes into account a wider variety of regions in the two images.",
                    "label": 0
                },
                {
                    "sent": "So whereas this highest quality method may indicate 2 images are very similar, even if there are large regions of disparity, the DP map map method probably won't make this map.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jake have you moderated the constraint problem a little bit?",
                    "label": 0
                },
                {
                    "sent": "Let's look at the previously mentioned prior work that handles constraints well and has a 1/4 approximation guarantee for non monotone.",
                    "label": 0
                },
                {
                    "sent": "The first step in their method is to represent each set by its characteristic vector X.",
                    "label": 0
                },
                {
                    "sent": "So for example for this at 2 four we get this vector and then they relax X such that XI ranges from zero to one representing the probability of including an item I.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second step is to extend the objective with an expectation says commonly called the multilinear extension, and what we mean by this expectation is that each item I from the ground set is included in Y with some probability XI.",
                    "label": 0
                },
                {
                    "sent": "In other words, we flip independent coins for each item, giving the following expression the center.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right here is just the probability of a set Y.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some here is over all possible two to the N subsets of our ground set.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three in this method is to optimize this objective using gradient based methods and the final step is rounding a short proof here shows this final step is really only necessary if the problem is constrained, so this overall procedure has some nice approximation guarantees, but it's somewhat.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Practical due to this some which requires us to use multi Monte Carlo estimation.",
                    "label": 0
                },
                {
                    "sent": "Our work proposes a fix for that inefficiency.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Specifically, we apply all the same steps as in the trickery prior work, but we change the objective slightly.",
                    "label": 0
                },
                {
                    "sent": "Instead of this multilinear extension we optimize will call the softmax extension and it's very similar to the multilinear with just this log term pulled outside.",
                    "label": 0
                },
                {
                    "sent": "Let's take a quick.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at what these objectives look like for this.",
                    "label": 0
                },
                {
                    "sent": "These two vectors do you want in G2 here where G2 has a slightly larger magnitude than G1?",
                    "label": 0
                },
                {
                    "sent": "So the domain of these functions is this plane.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we look at the multilinear extension, it looks like this.",
                    "label": 0
                },
                {
                    "sent": "And here.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The softmax, which upper bounds it?",
                    "label": 0
                },
                {
                    "sent": "If we look at.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Integral points then the.",
                    "label": 0
                },
                {
                    "sent": "These are the points where XIR zero or one.",
                    "label": 0
                },
                {
                    "sent": "We recover exactly the determinant of LY, which you can see a slightly larger at this front corner than at this back corner, because the front corner score corresponds to the set y = 2, where is the back corner corresponds to the set, y = 1, and the magnitude of the two is slightly greater than that of G1.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So softmax and multilinear closely related, related, but the slight difference with the log placement is very important, because it means that for the submodular function determinant were able to prove the softmax objective can be simplified to the computation of a single determinant shown here.",
                    "label": 0
                },
                {
                    "sent": "So now we have an objective, and in fact also a derivative that sufficient to compute.",
                    "label": 0
                },
                {
                    "sent": "So natural next question to ask is, does softmax also come with approximation guarantees?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the main property that Multilinear relies on for its guarantee, is that it's concave in any all positive or all negative direction.",
                    "label": 0
                },
                {
                    "sent": "So for any direction where X1 and X2 are both increasing or both decreasing as illustrated here, this turns out to be true for softmax as well, so.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So while neither objective is concave in general, the restricted concavity property from the previous slide is enough to give us a 1/4 approximation guarantee.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To summarize, we prove softmax is concave in all positive directions, and by combining this with submodularity of log determinant, we're further able to show that the local optimum reached by following the gradient is at least a 1/4 approximation to those softmax optimum.",
                    "label": 0
                },
                {
                    "sent": "Additionally, in the unconstrained case, we prove that the local optimum is integer, meaning that no roundings necessary, and we can directly infer 1/4 approximation to the true DP math objective.",
                    "label": 1
                },
                {
                    "sent": "So for the constraint setting, we don't have any guarantees due to the need to round, but we found in practice that there are various simple rounding techniques that perform well.",
                    "label": 0
                },
                {
                    "sent": "On that note, let's talk about some experiments.",
                    "label": 0
                },
                {
                    "sent": "The baseline we compare 2.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previously mentioned greedy algorithm and the basic idea of this algorithm is that we start from the empty set.",
                    "label": 0
                },
                {
                    "sent": "We find the single item to add that most increases that determine it, and then we.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Great.",
                    "label": 0
                },
                {
                    "sent": "We have results for an unconstrained setting Anna constraint setting where both are similar to the previous illustrations of the settings.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here the results on the X axis.",
                    "label": 0
                },
                {
                    "sent": "We have the size of the ground set N and on the Y axis the log ratio of the softmax score to the greedy score.",
                    "label": 0
                },
                {
                    "sent": "Each data point in this graph summarizes the result of running our optimization.",
                    "label": 0
                },
                {
                    "sent": "On 100 randomly generated ground sets, so the red line shows the median ratio and the blue lines are the 1st and 3rd quartiles, red line above 0 means softmax wins over greedy as the size of the ground set grows.",
                    "label": 0
                },
                {
                    "sent": "You can see that softmax advantage over greedy tends to increase, and this is even more pronounced into the one in the one to one constraint case where you can see that the ratios indicated on the Y axis are much larger.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In addition to valuing their effectiveness of softmax, we check it sufficiency.",
                    "label": 0
                },
                {
                    "sent": "So a naive implementation of the greedy algorithm we compare two would be orderin to the Fifth Ann.",
                    "label": 1
                },
                {
                    "sent": "You may have heard about less naive implementations, sometimes called lazy, greedy.",
                    "label": 0
                },
                {
                    "sent": "But in this case we can actually make it even faster than that just by exploiting the fact that DPS are closed under conditioning.",
                    "label": 0
                },
                {
                    "sent": "This gives us a.",
                    "label": 0
                },
                {
                    "sent": "Order end of the fourth version and it's this optimized version that we compare to here.",
                    "label": 1
                },
                {
                    "sent": "So taking a look at the unconstrained.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case for small ground sets.",
                    "label": 0
                },
                {
                    "sent": "Greedy is a bit faster, but as the size of the ground set grows, softmax advantage becomes apparent.",
                    "label": 0
                },
                {
                    "sent": "In the constraint case, the trend is not as clear, but we do see that the ground set as its eyes gets above maybe 150 softmax starts trending towards greater efficiency.",
                    "label": 0
                },
                {
                    "sent": "So to summarize, softmax is better at optimizing determinant and for larger ground sets it could even be faster than greedy.",
                    "label": 0
                },
                {
                    "sent": "Finally, let's consider some real world.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data and a match summarization task is sort of similar to the one to one constrained synthetic setting and is also similar to the second motivational example from the beginning of the talk we took the 20 Republican primary debates from the previous US presidential election, and we extracted all statements made by the 8 main candidates, which gave us an average 179 quotes per candidate.",
                    "label": 0
                },
                {
                    "sent": "And then for each pair of candidates for.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample Romney and Santorum.",
                    "label": 0
                },
                {
                    "sent": "We let the ground set of items consist of pairs of Romney Santorum.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Payments, for instance.",
                    "label": 0
                },
                {
                    "sent": "Here's one pair of statements.",
                    "label": 0
                },
                {
                    "sent": "The words here are all stem nouns from one of the 332 statements by Romney and one of the 208 statements by Santorum and Assize Award.",
                    "label": 0
                },
                {
                    "sent": "Here indicates its frequency within the quote.",
                    "label": 0
                },
                {
                    "sent": "This would be a high quality pair, since the two statements have similar word distributions, so the overall DP map task we set for ourselves was to select a set of such pairs such that the statements within a pair of similar but the overall set of pairs is diverse.",
                    "label": 0
                },
                {
                    "sent": "Basically a match summarization comparing the viewpoints of the two candidates.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's take a look at part of a match somewhere that softmax creates.",
                    "label": 1
                },
                {
                    "sent": "I'm going to go through a few examples snippets taken from within Romney statements, and here the color of a statement indicates it's general topic.",
                    "label": 1
                },
                {
                    "sent": "So we have a statement about taxes, no tax on interest, dividends, or capital gains, a statement about law statement about healthcare, statement about foreign aid, and another statement about healthcare, and similarly for Santora.",
                    "label": 1
                },
                {
                    "sent": "We have a statement about taxes, another statement about taxes, statement about foreign aid.",
                    "label": 1
                },
                {
                    "sent": "A statement about ethanol in a statement about health care.",
                    "label": 0
                },
                {
                    "sent": "Finally, if we run our softmax optimization.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To select a set of statement pairs, here are a few of the pairs that are selected to include in the map set.",
                    "label": 0
                },
                {
                    "sent": "The quotes are clearly diverse in topic, but within a pair we have similar topics and using this match summary we can do deduce what topics are important to both candidates, how they differ on these topics, for example, from this summer it's easy to see that Romney and Santorum disagree about capital gains tax and foreign aid, but they have similar views on Obamacare.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also compared softmax degree on this task and along the X axis.",
                    "label": 0
                },
                {
                    "sent": "Here is an auxiliary parameter, basically a scaling factor on the quality scores of the pairs and the Y axis is again the log probability ratio which is always above 0 in this case, indicating the softmax consistently beats greedy for this data.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude, we have an efficient effective approximate DP map algorithm.",
                    "label": 0
                },
                {
                    "sent": "It can be applied to all manner of subset selection problems.",
                    "label": 0
                },
                {
                    "sent": "The code and the political data can be found on this site which is linked off my web page and we believe that there are many other subset selection problems besides those illustrated in this talk that softmax can be applied to a few such problems are listed here, and there's also possibly other submodular functions for which softmax is efficiently computable.",
                    "label": 1
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for any questions there on time for now, please come by the poster.",
                    "label": 0
                },
                {
                    "sent": "Question hi, so your algorithms is N to the power four and you have ideas for extending this to the structure output output case where N might itself be exponentially large.",
                    "label": 0
                },
                {
                    "sent": "So you can extend these kind of algorithms to a structure case if you work in the space of the features, it might be possible, so we haven't worked this out yet, but there's something called structure DPS, which sort of instead of computing GG transpose.",
                    "label": 0
                },
                {
                    "sent": "I'm working with that computes G transpose G and works in the feature space, so maybe there might be an extension that way.",
                    "label": 0
                },
                {
                    "sent": "Question, yeah, this maximizing the determinant.",
                    "label": 0
                },
                {
                    "sent": "Overall sub matrices is a beautiful problem.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that it's known to be NP hard.",
                    "label": 0
                },
                {
                    "sent": "Is anything known about NP hardness of approximating to within?",
                    "label": 0
                },
                {
                    "sent": "You know 90% or some ratio like that?",
                    "label": 0
                },
                {
                    "sent": "I'm not sure about that.",
                    "label": 0
                },
                {
                    "sent": "I have to take that offline OK. Next so you work, your baseline was using the greedy algorithm from Nemhauser from 1978, and that was.",
                    "label": 0
                },
                {
                    "sent": "That's an algorithm that doesn't have any guarantees for the non monotone so much lower Max and I was wondering you said also that the book been doing Warren Feldman Schwartz algorithm that gives you the 1/2 approximation for the non monotone case didn't work for you and so the question is.",
                    "label": 0
                },
                {
                    "sent": "Did you run at one time or did you actually run it multiple times and take the cumulative best over multiple runs?",
                    "label": 0
                },
                {
                    "sent": "Multiple linear sweeps?",
                    "label": 0
                },
                {
                    "sent": "That is, so I think we implemented the deterministic version of it so you only read approximation.",
                    "label": 0
                },
                {
                    "sent": "You only run it once, right once, so why not?",
                    "label": 0
                },
                {
                    "sent": "Why did you do that?",
                    "label": 0
                },
                {
                    "sent": "Why not run it multiple times?",
                    "label": 0
                },
                {
                    "sent": "Well, the deterministic version is already.",
                    "label": 0
                },
                {
                    "sent": "I thought it supposed to be a 1/3 approximation and so we give that a try an it really did not do well.",
                    "label": 0
                },
                {
                    "sent": "I have the experimental results from it up on this slide.",
                    "label": 0
                },
                {
                    "sent": "Um, so certainly it might have done better if we'd given it a bunch of of different restarts.",
                    "label": 0
                },
                {
                    "sent": "That's something we could try.",
                    "label": 0
                },
                {
                    "sent": "Last quick question here.",
                    "label": 0
                },
                {
                    "sent": "So it's very nicely.",
                    "label": 0
                },
                {
                    "sent": "You can compute the advantage of the pieces that you can efficiently compute the partition function, but in the end, if you're going to do map inference, that sort of takes away that advantage, right?",
                    "label": 0
                },
                {
                    "sent": "You're not using that aspect of it, so in retrospect, if you're going to use any inference, would you be better off?",
                    "label": 0
                },
                {
                    "sent": "Using some other learning objective that was just a little more like discriminative.",
                    "label": 0
                },
                {
                    "sent": "So there have been a bunch of recent papers about DPS that have sort of compared to other learning methods, and they've successfully shown the DPS tend to give better performance like they've done it for some.",
                    "label": 0
                },
                {
                    "sent": "Some image selections pose tracking and things like that, and.",
                    "label": 0
                },
                {
                    "sent": "I feel like we didn't bother to compare to other methods in this paper just because there are already several papers where DPS have been proven to be better than other other methods like that.",
                    "label": 0
                },
                {
                    "sent": "OK, time is up.",
                    "label": 0
                },
                {
                    "sent": "Thank you Jennifer.",
                    "label": 0
                }
            ]
        }
    }
}