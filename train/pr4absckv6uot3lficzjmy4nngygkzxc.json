{
    "id": "pr4absckv6uot3lficzjmy4nngygkzxc",
    "title": "Guaranteed Rank Minimization via Singular Value Projection",
    "info": {
        "author": [
            "Prateek Jain, Nuance Communications, Inc."
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_jain_grm/",
    "segmentation": [
        [
            "Hi so this is a joint work with humankind and the Dillon from Utah, Austin so."
        ],
        [
            "This work we consider this problem of affine constraints rank minimization, where we want to estimate a matrix X subject to bunch of affine constraints, and so here this affine transformation a transforms a matrix X2AD dimensional vector.",
            "And typically we want D to scale as linear in the size of the matrix.",
            "So there are a number of applications for this arnp problem, most famous being matrix completion or loading matrix completion for collaborative filtering.",
            "However, this problem is in general NP, hard to solve, and in fact it is NP hard even to approximate within a logarithm factor.",
            "But Fortunately recently there has been a lot of work in this area and it has been shown that under certain assumptions on this linear transformation matrix, we have one can actually solve this problem exactly in polytime using something called Trace norm.",
            "Convex relaxation of this rank objective function.",
            "But the problem with that approach is that the obtained problem is typically hard to solve, especially when there's a lot of noise in the data or when the underlying assumption is not satisfied.",
            "Completely."
        ],
        [
            "So in this work we take a contrasting approach where we do not relax the objective function.",
            "We use rank objective function only.",
            "So to motivate this our approach, we formulate this robust arnp problem where we want to minimize this error or L2 norm of the error X -- B and we want our matrix X to be restricted to a set of low rank or rank matrices only.",
            "So now our."
        ],
        [
            "Singular value projection algorithm, which we call SCP, is just a projected gradient method.",
            "That is, our next rate is obtained by doing a gradient descent owner current rate and then projecting it onto the nonconvex set of ranking matrices, and this projection can be done efficiently using singular value decomposition.",
            "So next we show that our method converges to the optimal solution assuming restricted isometry properties.",
            "These properties are the same property which.",
            "Trace numbers relaxation also assumes.",
            "And so, under this assumption, we show that our method converges to the optimal solution and analysis is very, very simple.",
            "Furthermore, since matrix completion is a special case of affine constraints, rank, mission problem, so we can directly apply our method for matrix completion.",
            "However, matrix completion doesn't satisfy this ripi property, so we cannot directly apply our analysts there.",
            "But we have made some partial progress by showing a more restricted I. Rapu property and we formulate precise conjecture, which we empirically verify.",
            "And if we can prove that conjecture, then it implies optimality of OCP algorithm for matrix completion as well.",
            "And then we extend our first order projected gradient method to include some fashion information as well.",
            "So that gives us this method that we call SVP Newton and is Newton D. So we apply all three of our methods on a bunch of synthetic datasets, an movielens data set, and on each of the data set we observed that as people method is significantly faster and more stable than most of the existing methods.",
            "So I'll be happy to go in detail on this work at our poster number W. 56.",
            "Thanks a lot."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi so this is a joint work with humankind and the Dillon from Utah, Austin so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work we consider this problem of affine constraints rank minimization, where we want to estimate a matrix X subject to bunch of affine constraints, and so here this affine transformation a transforms a matrix X2AD dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "And typically we want D to scale as linear in the size of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So there are a number of applications for this arnp problem, most famous being matrix completion or loading matrix completion for collaborative filtering.",
                    "label": 1
                },
                {
                    "sent": "However, this problem is in general NP, hard to solve, and in fact it is NP hard even to approximate within a logarithm factor.",
                    "label": 1
                },
                {
                    "sent": "But Fortunately recently there has been a lot of work in this area and it has been shown that under certain assumptions on this linear transformation matrix, we have one can actually solve this problem exactly in polytime using something called Trace norm.",
                    "label": 0
                },
                {
                    "sent": "Convex relaxation of this rank objective function.",
                    "label": 0
                },
                {
                    "sent": "But the problem with that approach is that the obtained problem is typically hard to solve, especially when there's a lot of noise in the data or when the underlying assumption is not satisfied.",
                    "label": 0
                },
                {
                    "sent": "Completely.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this work we take a contrasting approach where we do not relax the objective function.",
                    "label": 0
                },
                {
                    "sent": "We use rank objective function only.",
                    "label": 0
                },
                {
                    "sent": "So to motivate this our approach, we formulate this robust arnp problem where we want to minimize this error or L2 norm of the error X -- B and we want our matrix X to be restricted to a set of low rank or rank matrices only.",
                    "label": 0
                },
                {
                    "sent": "So now our.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Singular value projection algorithm, which we call SCP, is just a projected gradient method.",
                    "label": 1
                },
                {
                    "sent": "That is, our next rate is obtained by doing a gradient descent owner current rate and then projecting it onto the nonconvex set of ranking matrices, and this projection can be done efficiently using singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "So next we show that our method converges to the optimal solution assuming restricted isometry properties.",
                    "label": 0
                },
                {
                    "sent": "These properties are the same property which.",
                    "label": 0
                },
                {
                    "sent": "Trace numbers relaxation also assumes.",
                    "label": 0
                },
                {
                    "sent": "And so, under this assumption, we show that our method converges to the optimal solution and analysis is very, very simple.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, since matrix completion is a special case of affine constraints, rank, mission problem, so we can directly apply our method for matrix completion.",
                    "label": 0
                },
                {
                    "sent": "However, matrix completion doesn't satisfy this ripi property, so we cannot directly apply our analysts there.",
                    "label": 0
                },
                {
                    "sent": "But we have made some partial progress by showing a more restricted I. Rapu property and we formulate precise conjecture, which we empirically verify.",
                    "label": 0
                },
                {
                    "sent": "And if we can prove that conjecture, then it implies optimality of OCP algorithm for matrix completion as well.",
                    "label": 0
                },
                {
                    "sent": "And then we extend our first order projected gradient method to include some fashion information as well.",
                    "label": 0
                },
                {
                    "sent": "So that gives us this method that we call SVP Newton and is Newton D. So we apply all three of our methods on a bunch of synthetic datasets, an movielens data set, and on each of the data set we observed that as people method is significantly faster and more stable than most of the existing methods.",
                    "label": 1
                },
                {
                    "sent": "So I'll be happy to go in detail on this work at our poster number W. 56.",
                    "label": 0
                },
                {
                    "sent": "Thanks a lot.",
                    "label": 0
                }
            ]
        }
    }
}