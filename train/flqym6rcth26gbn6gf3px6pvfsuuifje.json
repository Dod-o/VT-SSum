{
    "id": "flqym6rcth26gbn6gf3px6pvfsuuifje",
    "title": "The VC-Dimension of SQL Queries and Selectivity Estimation Through Sampling",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Matteo Riondato, Two Sigma Investments, LP"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Information Retrieval",
            "Top->Computer Science->Databases"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_riondato_sampling/",
    "segmentation": [
        [
            "Dimensional queries and selectivity estimation sampling.",
            "Quite complex, but basically do you hear me in the background?",
            "Now it's OK. Now it works.",
            "OK, so yeah, basically what we do is we solve a very important problem in databases using a very theoretical tool from the statistical learning theory.",
            "So let's start by giving you the setting and the definition."
        ],
        [
            "The problem so you have a database or large database and the idea is that we want to run queries on the database and this query can be very complex, but in general we can see them as a composition of elementary operation like selection and joints.",
            "Now we can organize this simple operation in many different ways and a lot of these different ways lead to the same final output and we can see the ways there is operational, organized and these are called the execution plans.",
            "As binary three is where the nodes are the elementary operations.",
            "And that will be system must choose must pick one of these execution plan and hopefully should shoot the pick their execution plan with the shortest execution time.",
            "Now this is not an easy task.",
            "Is actually an NPR problem to find their execution plan with the shortest execution down.",
            "Therefore we must rely on heuristics."
        ],
        [
            "And the most user is logistic is to execute the small selectivity operation first, where they selectivity is defined as the ratio between the input and the output sizes of the operation.",
            "Now obviously we don't know these exact quantities for to compute the selectivity of an operation before executing the operation, and therefore we have to rely when we have to.",
            "We want to choose an execution plan.",
            "We have to rely on estimation and good execution plan.",
            "We need good estimation of the selectivity of an operation."
        ],
        [
            "So this is traditionally done in a database systems using histograms and basically and Instagrams is an approximation of the value frequency distribution on a single table on a single column.",
            "So we have an Instagram's for each column on each table of the database.",
            "Now, Instagrams are very.",
            "It's very easy to build these programs, and it's very fast to query histograms.",
            "The problem is that the prediction you can get for the selectivity out of these programs only is only good as long as.",
            "The frequency distribution are independent and uniform.",
            "That is, the distribution in different columns are independent and the distribution of inner column in a single column is beautiful and therefore because of this we can actually leave that we can actually get bad estimates because in real life where your friend these two assumptions basically don't don't hold.",
            "So the Instagrams don't give any guarantee on the accuracy of the of the prediction of the selectivity."
        ],
        [
            "Our goal, therefore, is to develop a method that allows us to obtain good estimates for all queries in a class that is specified in a class of query that is specified by the user and in particular.",
            "Our estimate will be an epsilon Delta estimator for the selectivities, that is, that for a parameter epsilon specified by the user.",
            "Our estimates will be not will not be more than epsilon far from the real one, and we want we want this to hold with probability 1 minus Delta across all day.",
            "All the queries in the class, that is, the probability that there is querying the class for which our estimate is more than epsilon, far from the real one, is less than that."
        ],
        [
            "So how do we get this where the idea is very simple, we create a sample of the database.",
            "We run the query under sample to obtain its real selectivity on the sample, and we use this as an estimator for the selectivity on the large database.",
            "So the idea is not new IT was used before.",
            "But what is new is that we give guarantees on the quality of the estimation and we have very interesting sample properties that allows us to reach this guarantees in an efficient way efficient way.",
            "And we also use new techniques to prove the results.",
            "Prove what we what we get."
        ],
        [
            "So what we what do we expect?",
            "So remember that.",
            "For the guarantees we want.",
            "So how does this work?",
            "OK, so remember that they guarantee that we want is that the the probability that there is a query for which our selectivity in the sample is more than national far from the real one should be less than that.",
            "Now we want the sample to be small enough to fit into my memory, so that when we execute the query on the sample, the execution is very fast because we don't have to access the disk.",
            "We also want our sample to be static.",
            "That is, we want to create our sample once.",
            "And use it for multiple queries as long as the database underneath doesn't change, we want to be able to use the same sample.",
            "This is absolutely needed because except for large databases, creating a new sample is very expensive.",
            "And we also want our sample to only depend on characteristic of the class of queries we want to run on the database.",
            "That is, we don't want our sample size to depend on the data that are stored in the in the database, so we want to be able to compute the sample size before even seeing the database.",
            "So how long should a sample that guarantees the power should it be?"
        ],
        [
            "Now a simple approach that one may think of.",
            "Buy stuff, think about this problem is that one can use term of bound on the deviations between this selectivity on the sample, which is basically the proportion of a binomial distribution and reactivity.",
            "But this only works for a single query.",
            "And to extend this to multiple queries, we would have to either create a new sample for each query we see, which is absolutely too expensive for practical purposes.",
            "Or we can use the Union bound.",
            "The problem with the Union bound is that it's very loose as long as the basically the sequence of the queries we want we want to run gets longer and longer.",
            "So it's impractical."
        ],
        [
            "So what we do instead that we will avoid the Union bound and instead we will use results from the VC dimension theory to compute a sample size that guarantees that the sample assembles death size as the property we look for so that.",
            "Even in the sample is not more than epsilon away from the real and the result we use is that if we can bound if you can give a bound to the VC dimension of the class of queries we want to run, then we can get a bound to the sample size needed to achieve such results.",
            "So that's what we need to do.",
            "We need to give a bound on the VC dimension of the class of queries."
        ],
        [
            "Now what is now this dimension is a tool from statistical learning theories.",
            "So that's like basically the mathematical theory behind machine learning and then.",
            "The main use of it is that it gives a bound to the sample size needed to approximately learn a function from a given class, and it finds application alot of application in machine learning.",
            "Computational geometry in a graph algorithms and now thanks to our results also in databases."
        ],
        [
            "So let me give you some definition so we have a set of points and we have a family of subsets of these points that have called ranges and if there is a dimension of the pair X&F that is called out in space is basically the Captain ality of the largest subset of X such that when we compute the family of subsets of a that we get by intersecting the ranges with.",
            "A if this is equal to this to the power set of a, that is, if we are able to identify all subset of a, then we say that FA is shattered by F&VC.",
            "Dimension is basically the cardinality of the larger of the largest chapter set.",
            "Now to make you an example, if we have the set of points, is the plane and the family in the ranges are.",
            "Diaper players on on the plane.",
            "Sorry, yeah, the aspect of planes basically.",
            "We can shut the three points, but we cannot shut at four points, so this dimension of this range space is actually free.",
            "Now computing visit dimension is not easy because we actually need to show that it doesn't exist at a set that is larger than some.",
            "Some D that can be shattered.",
            "So it's basically to show that something is impossible."
        ],
        [
            "And the main tool we use is called action approximation.",
            "So if we fix the parameters epsilon and Delta and we draw if they arrange space as we see dimension D, then we can create and we draw a random sample of effects that is sufficiently larger.",
            "This sample with probability at least one minus data is such that for all for all ranges in the family.",
            "So if we do not that with a little FYI measure, the ratio of point of X that belongs to the range minus the.",
            "The ratio of points in the sample that actually belong to the range is less than epsilon is at most epsilon.",
            "And this must all for all for all ranges.",
            "Now you can see that the sample size to achieve the result only depends on the VC dimension of of the range space on epsilon on data.",
            "And it doesn't depend at all on the size of the on the other side of the set of points or on the size of the number of ranges we are working with.",
            "And this is very interesting because it basically allows us to achieve something more than the Union bound for, which is that the sample size will depend on the size of X and the quality on this item goes on the size of the range space."
        ],
        [
            "Changes, so how does it help us?",
            "In our case, the set of coins will actually be the database and the family and the family of ranges would actually be the family of the output of the queries in our in the in the class of queries the specified by the user, and if we can get an epsilon approximation of the rain space, that is, if you can create a sample of the database that is an actual approximation, then what we have is exactly the property we are looking for.",
            "Because that error on the left is exactly selectivity of the query on the large database and the term on the right is exactly the selectivity of the query in the sample.",
            "So if we can create an approximation, we get exactly the probability the property that we wanted and remember that we can create the probabilistically create an approximation by rambles by using random sampling."
        ],
        [
            "So what we do is that we can domain tool.",
            "The main thing we need is actually a bound to the VC dimension of queries.",
            "So we characterize the queries by the SQL expression.",
            "So the more complex is the expression, the higher will be the VC dimension and therefore the larger will be the sample we need to achieve this guarantees.",
            "So the VC dimension is completely dependent on the complexity of the expert expression.",
            "And when I say the complexity of the SQL expression, I mean that the number of selection predicates that are in the in the query, the number of joint operations that are involved in the query, the number of columns in the in the largest table in the data set, and so on in the database and so on.",
            "And because of this, given that is completed as kind of a squad expression of the queries, it only depends on the query and not at all on the database, which is one of the things that we wanted to achieve."
        ],
        [
            "So we provide a result.",
            "We start from very simple selection queries that only have one selection predicate in their expression, and we've gone from.",
            "We build up from there extending to multiple selection predicates, then to join queries, then to generate queries to prove our bound to visit dimension.",
            "We sometimes use a result from computational geometry, and other times we actually actually develop.",
            "The new bound some.",
            "There are some technical details, especially for the direct proof, so I refer you to the full paper on the on the archive or in the proceedings.",
            "Um?"
        ],
        [
            "But let me just give you a.",
            "Result of the result.",
            "We develop on the most generic query possible.",
            "Now if we can specify the following parameters, that is the maximum number of joint operation that are involved in the query.",
            "The maximum number of conditions that are in a selection predicate in their squared expression and the maximum number of columns in a table in the database.",
            "Then define the class of queries that includes all discreet and remember these queries only this class only depends on the SQL expression of the query and not on the data stored in the database.",
            "And we can then bound give a bound to the DC dimension of the rain space where I access the database in Q.",
            "Is this class of queries and you can see that the dimension doesn't grow too fast, and given that the sample size is actually linear in the is actually linear individual dimension, we can see that even for complex queries our sample size are very reasonable and grow slowly as we increase the complexity of our queries.",
            "And from here we can directly compute the size of the apps approximation that allows us to achieve the guarantees we were looking."
        ],
        [
            "Now there is a little sampling issue that I'd like to describe.",
            "The theorem requires a sample of the Cartesian product of the table, but computing the Cartesian product, installing the Cartesian product of a large database is completely not practical.",
            "Absolutely not practical, is expensive, and if we store the sample as a Cartesian product, we would need to have a system that rewrites queries so that they can be executed on the Cartesian product.",
            "So basically on a single table instead of a.",
            "So we want to keep our goal is to keep the same table layout of the original database in the sample."
        ],
        [
            "And so our solution is to sample independently for each table and basically assign a unique index to each sample tables.",
            "So Switch Temple top assistant which sampled rows in the in the.",
            "In the database.",
            "So in the sample and only combination of tackle that have the same index will be actually member of the sample and we only count this when we compute the Cartesian.",
            "The sample size to compute the selectivity in the sample.",
            "And this is where the Fusion between because we only need to do a linear scan of the of the database of the output of the query to identify such a query to compute the selectivity on the sample."
        ],
        [
            "We run a huge number of experiments using artificial data and our sample can always fit into my memory and this was one of the goals we had to achieve high speed.",
            "When we execute the queries on the sample, our estimation of that of the selectivity are always within epsilon from the real value, which is another of the goal we were looking for.",
            "We were looking to achieve and most importantly we actually outperform the two main.",
            "Available.",
            "Raise database system by orders of magnitude, so the figure shows the relative error of the prediction when the data in the table doesn't.",
            "Don't don't respect the condition that so these two systems both use histograms, and here we see how bad they perform.",
            "If the data in the table are correlated, that is when they are not independent anymore as the histograms request.",
            "And how well instead the hour.",
            "The metal platform said so.",
            "The vertical scale the Y axis is actually a logarithmic scale, so both the year.",
            "Sorry, this is a log log image and we see that we basically do much better orders of magnitude better than Postgres and those of SQL Server, so it's kind of surprising somehow that people still in commercial systems theory lie on on histograms to compute the selectivity, and even if they have to have to incur in this kind of.",
            "Of huge error basically.",
            "And given that the memory size is growing and growing, we believe that there is absolutely space to move to a sample based approach."
        ],
        [
            "So to sum up, what we did is basically we did.",
            "We use are very tactical tool from statistical learning theory.",
            "That is that baby see dimension and to solve efficiently a very practical problem and a very important one in databases that is selectivity estimation that is at the core of having.",
            "Efficient query execution in databases."
        ],
        [
            "This concludes my talk, so if you have any question and answer, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dimensional queries and selectivity estimation sampling.",
                    "label": 1
                },
                {
                    "sent": "Quite complex, but basically do you hear me in the background?",
                    "label": 0
                },
                {
                    "sent": "Now it's OK. Now it works.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, basically what we do is we solve a very important problem in databases using a very theoretical tool from the statistical learning theory.",
                    "label": 0
                },
                {
                    "sent": "So let's start by giving you the setting and the definition.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem so you have a database or large database and the idea is that we want to run queries on the database and this query can be very complex, but in general we can see them as a composition of elementary operation like selection and joints.",
                    "label": 0
                },
                {
                    "sent": "Now we can organize this simple operation in many different ways and a lot of these different ways lead to the same final output and we can see the ways there is operational, organized and these are called the execution plans.",
                    "label": 1
                },
                {
                    "sent": "As binary three is where the nodes are the elementary operations.",
                    "label": 0
                },
                {
                    "sent": "And that will be system must choose must pick one of these execution plan and hopefully should shoot the pick their execution plan with the shortest execution time.",
                    "label": 1
                },
                {
                    "sent": "Now this is not an easy task.",
                    "label": 0
                },
                {
                    "sent": "Is actually an NPR problem to find their execution plan with the shortest execution down.",
                    "label": 0
                },
                {
                    "sent": "Therefore we must rely on heuristics.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the most user is logistic is to execute the small selectivity operation first, where they selectivity is defined as the ratio between the input and the output sizes of the operation.",
                    "label": 1
                },
                {
                    "sent": "Now obviously we don't know these exact quantities for to compute the selectivity of an operation before executing the operation, and therefore we have to rely when we have to.",
                    "label": 1
                },
                {
                    "sent": "We want to choose an execution plan.",
                    "label": 0
                },
                {
                    "sent": "We have to rely on estimation and good execution plan.",
                    "label": 0
                },
                {
                    "sent": "We need good estimation of the selectivity of an operation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is traditionally done in a database systems using histograms and basically and Instagrams is an approximation of the value frequency distribution on a single table on a single column.",
                    "label": 1
                },
                {
                    "sent": "So we have an Instagram's for each column on each table of the database.",
                    "label": 0
                },
                {
                    "sent": "Now, Instagrams are very.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to build these programs, and it's very fast to query histograms.",
                    "label": 1
                },
                {
                    "sent": "The problem is that the prediction you can get for the selectivity out of these programs only is only good as long as.",
                    "label": 0
                },
                {
                    "sent": "The frequency distribution are independent and uniform.",
                    "label": 1
                },
                {
                    "sent": "That is, the distribution in different columns are independent and the distribution of inner column in a single column is beautiful and therefore because of this we can actually leave that we can actually get bad estimates because in real life where your friend these two assumptions basically don't don't hold.",
                    "label": 0
                },
                {
                    "sent": "So the Instagrams don't give any guarantee on the accuracy of the of the prediction of the selectivity.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our goal, therefore, is to develop a method that allows us to obtain good estimates for all queries in a class that is specified in a class of query that is specified by the user and in particular.",
                    "label": 1
                },
                {
                    "sent": "Our estimate will be an epsilon Delta estimator for the selectivities, that is, that for a parameter epsilon specified by the user.",
                    "label": 0
                },
                {
                    "sent": "Our estimates will be not will not be more than epsilon far from the real one, and we want we want this to hold with probability 1 minus Delta across all day.",
                    "label": 0
                },
                {
                    "sent": "All the queries in the class, that is, the probability that there is querying the class for which our estimate is more than epsilon, far from the real one, is less than that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we get this where the idea is very simple, we create a sample of the database.",
                    "label": 0
                },
                {
                    "sent": "We run the query under sample to obtain its real selectivity on the sample, and we use this as an estimator for the selectivity on the large database.",
                    "label": 1
                },
                {
                    "sent": "So the idea is not new IT was used before.",
                    "label": 1
                },
                {
                    "sent": "But what is new is that we give guarantees on the quality of the estimation and we have very interesting sample properties that allows us to reach this guarantees in an efficient way efficient way.",
                    "label": 1
                },
                {
                    "sent": "And we also use new techniques to prove the results.",
                    "label": 0
                },
                {
                    "sent": "Prove what we what we get.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we what do we expect?",
                    "label": 0
                },
                {
                    "sent": "So remember that.",
                    "label": 0
                },
                {
                    "sent": "For the guarantees we want.",
                    "label": 0
                },
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "OK, so remember that they guarantee that we want is that the the probability that there is a query for which our selectivity in the sample is more than national far from the real one should be less than that.",
                    "label": 0
                },
                {
                    "sent": "Now we want the sample to be small enough to fit into my memory, so that when we execute the query on the sample, the execution is very fast because we don't have to access the disk.",
                    "label": 1
                },
                {
                    "sent": "We also want our sample to be static.",
                    "label": 0
                },
                {
                    "sent": "That is, we want to create our sample once.",
                    "label": 0
                },
                {
                    "sent": "And use it for multiple queries as long as the database underneath doesn't change, we want to be able to use the same sample.",
                    "label": 0
                },
                {
                    "sent": "This is absolutely needed because except for large databases, creating a new sample is very expensive.",
                    "label": 0
                },
                {
                    "sent": "And we also want our sample to only depend on characteristic of the class of queries we want to run on the database.",
                    "label": 0
                },
                {
                    "sent": "That is, we don't want our sample size to depend on the data that are stored in the in the database, so we want to be able to compute the sample size before even seeing the database.",
                    "label": 0
                },
                {
                    "sent": "So how long should a sample that guarantees the power should it be?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now a simple approach that one may think of.",
                    "label": 0
                },
                {
                    "sent": "Buy stuff, think about this problem is that one can use term of bound on the deviations between this selectivity on the sample, which is basically the proportion of a binomial distribution and reactivity.",
                    "label": 0
                },
                {
                    "sent": "But this only works for a single query.",
                    "label": 1
                },
                {
                    "sent": "And to extend this to multiple queries, we would have to either create a new sample for each query we see, which is absolutely too expensive for practical purposes.",
                    "label": 0
                },
                {
                    "sent": "Or we can use the Union bound.",
                    "label": 0
                },
                {
                    "sent": "The problem with the Union bound is that it's very loose as long as the basically the sequence of the queries we want we want to run gets longer and longer.",
                    "label": 0
                },
                {
                    "sent": "So it's impractical.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do instead that we will avoid the Union bound and instead we will use results from the VC dimension theory to compute a sample size that guarantees that the sample assembles death size as the property we look for so that.",
                    "label": 0
                },
                {
                    "sent": "Even in the sample is not more than epsilon away from the real and the result we use is that if we can bound if you can give a bound to the VC dimension of the class of queries we want to run, then we can get a bound to the sample size needed to achieve such results.",
                    "label": 0
                },
                {
                    "sent": "So that's what we need to do.",
                    "label": 0
                },
                {
                    "sent": "We need to give a bound on the VC dimension of the class of queries.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what is now this dimension is a tool from statistical learning theories.",
                    "label": 0
                },
                {
                    "sent": "So that's like basically the mathematical theory behind machine learning and then.",
                    "label": 0
                },
                {
                    "sent": "The main use of it is that it gives a bound to the sample size needed to approximately learn a function from a given class, and it finds application alot of application in machine learning.",
                    "label": 1
                },
                {
                    "sent": "Computational geometry in a graph algorithms and now thanks to our results also in databases.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me give you some definition so we have a set of points and we have a family of subsets of these points that have called ranges and if there is a dimension of the pair X&F that is called out in space is basically the Captain ality of the largest subset of X such that when we compute the family of subsets of a that we get by intersecting the ranges with.",
                    "label": 0
                },
                {
                    "sent": "A if this is equal to this to the power set of a, that is, if we are able to identify all subset of a, then we say that FA is shattered by F&VC.",
                    "label": 0
                },
                {
                    "sent": "Dimension is basically the cardinality of the larger of the largest chapter set.",
                    "label": 1
                },
                {
                    "sent": "Now to make you an example, if we have the set of points, is the plane and the family in the ranges are.",
                    "label": 0
                },
                {
                    "sent": "Diaper players on on the plane.",
                    "label": 0
                },
                {
                    "sent": "Sorry, yeah, the aspect of planes basically.",
                    "label": 0
                },
                {
                    "sent": "We can shut the three points, but we cannot shut at four points, so this dimension of this range space is actually free.",
                    "label": 0
                },
                {
                    "sent": "Now computing visit dimension is not easy because we actually need to show that it doesn't exist at a set that is larger than some.",
                    "label": 1
                },
                {
                    "sent": "Some D that can be shattered.",
                    "label": 0
                },
                {
                    "sent": "So it's basically to show that something is impossible.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the main tool we use is called action approximation.",
                    "label": 0
                },
                {
                    "sent": "So if we fix the parameters epsilon and Delta and we draw if they arrange space as we see dimension D, then we can create and we draw a random sample of effects that is sufficiently larger.",
                    "label": 0
                },
                {
                    "sent": "This sample with probability at least one minus data is such that for all for all ranges in the family.",
                    "label": 0
                },
                {
                    "sent": "So if we do not that with a little FYI measure, the ratio of point of X that belongs to the range minus the.",
                    "label": 0
                },
                {
                    "sent": "The ratio of points in the sample that actually belong to the range is less than epsilon is at most epsilon.",
                    "label": 0
                },
                {
                    "sent": "And this must all for all for all ranges.",
                    "label": 0
                },
                {
                    "sent": "Now you can see that the sample size to achieve the result only depends on the VC dimension of of the range space on epsilon on data.",
                    "label": 0
                },
                {
                    "sent": "And it doesn't depend at all on the size of the on the other side of the set of points or on the size of the number of ranges we are working with.",
                    "label": 0
                },
                {
                    "sent": "And this is very interesting because it basically allows us to achieve something more than the Union bound for, which is that the sample size will depend on the size of X and the quality on this item goes on the size of the range space.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Changes, so how does it help us?",
                    "label": 1
                },
                {
                    "sent": "In our case, the set of coins will actually be the database and the family and the family of ranges would actually be the family of the output of the queries in our in the in the class of queries the specified by the user, and if we can get an epsilon approximation of the rain space, that is, if you can create a sample of the database that is an actual approximation, then what we have is exactly the property we are looking for.",
                    "label": 0
                },
                {
                    "sent": "Because that error on the left is exactly selectivity of the query on the large database and the term on the right is exactly the selectivity of the query in the sample.",
                    "label": 0
                },
                {
                    "sent": "So if we can create an approximation, we get exactly the probability the property that we wanted and remember that we can create the probabilistically create an approximation by rambles by using random sampling.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we do is that we can domain tool.",
                    "label": 0
                },
                {
                    "sent": "The main thing we need is actually a bound to the VC dimension of queries.",
                    "label": 1
                },
                {
                    "sent": "So we characterize the queries by the SQL expression.",
                    "label": 1
                },
                {
                    "sent": "So the more complex is the expression, the higher will be the VC dimension and therefore the larger will be the sample we need to achieve this guarantees.",
                    "label": 0
                },
                {
                    "sent": "So the VC dimension is completely dependent on the complexity of the expert expression.",
                    "label": 0
                },
                {
                    "sent": "And when I say the complexity of the SQL expression, I mean that the number of selection predicates that are in the in the query, the number of joint operations that are involved in the query, the number of columns in the in the largest table in the data set, and so on in the database and so on.",
                    "label": 0
                },
                {
                    "sent": "And because of this, given that is completed as kind of a squad expression of the queries, it only depends on the query and not at all on the database, which is one of the things that we wanted to achieve.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we provide a result.",
                    "label": 0
                },
                {
                    "sent": "We start from very simple selection queries that only have one selection predicate in their expression, and we've gone from.",
                    "label": 1
                },
                {
                    "sent": "We build up from there extending to multiple selection predicates, then to join queries, then to generate queries to prove our bound to visit dimension.",
                    "label": 1
                },
                {
                    "sent": "We sometimes use a result from computational geometry, and other times we actually actually develop.",
                    "label": 0
                },
                {
                    "sent": "The new bound some.",
                    "label": 0
                },
                {
                    "sent": "There are some technical details, especially for the direct proof, so I refer you to the full paper on the on the archive or in the proceedings.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But let me just give you a.",
                    "label": 0
                },
                {
                    "sent": "Result of the result.",
                    "label": 0
                },
                {
                    "sent": "We develop on the most generic query possible.",
                    "label": 0
                },
                {
                    "sent": "Now if we can specify the following parameters, that is the maximum number of joint operation that are involved in the query.",
                    "label": 0
                },
                {
                    "sent": "The maximum number of conditions that are in a selection predicate in their squared expression and the maximum number of columns in a table in the database.",
                    "label": 1
                },
                {
                    "sent": "Then define the class of queries that includes all discreet and remember these queries only this class only depends on the SQL expression of the query and not on the data stored in the database.",
                    "label": 0
                },
                {
                    "sent": "And we can then bound give a bound to the DC dimension of the rain space where I access the database in Q.",
                    "label": 0
                },
                {
                    "sent": "Is this class of queries and you can see that the dimension doesn't grow too fast, and given that the sample size is actually linear in the is actually linear individual dimension, we can see that even for complex queries our sample size are very reasonable and grow slowly as we increase the complexity of our queries.",
                    "label": 0
                },
                {
                    "sent": "And from here we can directly compute the size of the apps approximation that allows us to achieve the guarantees we were looking.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there is a little sampling issue that I'd like to describe.",
                    "label": 0
                },
                {
                    "sent": "The theorem requires a sample of the Cartesian product of the table, but computing the Cartesian product, installing the Cartesian product of a large database is completely not practical.",
                    "label": 1
                },
                {
                    "sent": "Absolutely not practical, is expensive, and if we store the sample as a Cartesian product, we would need to have a system that rewrites queries so that they can be executed on the Cartesian product.",
                    "label": 0
                },
                {
                    "sent": "So basically on a single table instead of a.",
                    "label": 0
                },
                {
                    "sent": "So we want to keep our goal is to keep the same table layout of the original database in the sample.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so our solution is to sample independently for each table and basically assign a unique index to each sample tables.",
                    "label": 1
                },
                {
                    "sent": "So Switch Temple top assistant which sampled rows in the in the.",
                    "label": 0
                },
                {
                    "sent": "In the database.",
                    "label": 0
                },
                {
                    "sent": "So in the sample and only combination of tackle that have the same index will be actually member of the sample and we only count this when we compute the Cartesian.",
                    "label": 0
                },
                {
                    "sent": "The sample size to compute the selectivity in the sample.",
                    "label": 0
                },
                {
                    "sent": "And this is where the Fusion between because we only need to do a linear scan of the of the database of the output of the query to identify such a query to compute the selectivity on the sample.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We run a huge number of experiments using artificial data and our sample can always fit into my memory and this was one of the goals we had to achieve high speed.",
                    "label": 0
                },
                {
                    "sent": "When we execute the queries on the sample, our estimation of that of the selectivity are always within epsilon from the real value, which is another of the goal we were looking for.",
                    "label": 1
                },
                {
                    "sent": "We were looking to achieve and most importantly we actually outperform the two main.",
                    "label": 0
                },
                {
                    "sent": "Available.",
                    "label": 0
                },
                {
                    "sent": "Raise database system by orders of magnitude, so the figure shows the relative error of the prediction when the data in the table doesn't.",
                    "label": 1
                },
                {
                    "sent": "Don't don't respect the condition that so these two systems both use histograms, and here we see how bad they perform.",
                    "label": 0
                },
                {
                    "sent": "If the data in the table are correlated, that is when they are not independent anymore as the histograms request.",
                    "label": 0
                },
                {
                    "sent": "And how well instead the hour.",
                    "label": 0
                },
                {
                    "sent": "The metal platform said so.",
                    "label": 0
                },
                {
                    "sent": "The vertical scale the Y axis is actually a logarithmic scale, so both the year.",
                    "label": 0
                },
                {
                    "sent": "Sorry, this is a log log image and we see that we basically do much better orders of magnitude better than Postgres and those of SQL Server, so it's kind of surprising somehow that people still in commercial systems theory lie on on histograms to compute the selectivity, and even if they have to have to incur in this kind of.",
                    "label": 0
                },
                {
                    "sent": "Of huge error basically.",
                    "label": 0
                },
                {
                    "sent": "And given that the memory size is growing and growing, we believe that there is absolutely space to move to a sample based approach.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to sum up, what we did is basically we did.",
                    "label": 0
                },
                {
                    "sent": "We use are very tactical tool from statistical learning theory.",
                    "label": 1
                },
                {
                    "sent": "That is that baby see dimension and to solve efficiently a very practical problem and a very important one in databases that is selectivity estimation that is at the core of having.",
                    "label": 0
                },
                {
                    "sent": "Efficient query execution in databases.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This concludes my talk, so if you have any question and answer, thank you.",
                    "label": 0
                }
            ]
        }
    }
}