{
    "id": "bdx32rv5lpelaegdmq7jos3qhexmr3be",
    "title": "Theano: a Fast Python Library for Modelling and Training",
    "info": {
        "author": [
            "Pascal Lamblin, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Oct. 13, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computers->Software->Open-Source Software",
            "Top->Computers->Programming->Python",
            "Top->Computer Science->Programming Languages",
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning",
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/tcmm2014_lamblin_theano_python_library/",
    "segmentation": [
        [
            "Morning, so I'm here to present you library that we developed mostly in in our lab in Montreal.",
            "Those provision of professor your banjo so and that's what we are mainly using to do research on deep networks.",
            "Various kind of things that grant presented and also many other different things that we are experimenting with so.",
            "Um?"
        ],
        [
            "Starting just by giving you an overview of how to use the library and what that does, and then I'll get to more advanced examples and present in particular how we work with gradients, automatic differentiations, graph optimizations, how to use the GPU and then a couple of more advanced.",
            "Topics if so.",
            "Trying to give you like hands-on field of what it looks like to write code that uses Theano and so if you have any questions or if things are not clear, it's OK to raise your hand and interrupt and I'll try to clarify.",
            "I'll try to have an open Python session on the side if I can.",
            "If I have to compose some things and if there is still.",
            "Time at the end of the presentation we might.",
            "I mean, I might.",
            "Try some examples, especially to give you an idea of how.",
            "Things look like when it doesn't work when there are errors.",
            "If you have to debug things because it's not always.",
            "I mean if it works then everything is fine, but sometimes it can be a little bit confusing if not everything goes as expected.",
            "So."
        ],
        [
            "Let me check by the motivation and the objectives of piano, so there are no is a compiler for mathematical expressions.",
            "So what that means is that we define mathematical expressions as a graph of symbolic variables.",
            "We try to do that, so we do that in Python And we try to be really close to numb by semantic so that it's not confusing for people who already knew.",
            "No, NUM py and low syntax is not as far away from Matlab as it as it could be.",
            "Then when you define those mathematical expression, you can do manipulation of that expression graph so you can do substitution.",
            "You can extend the graph.",
            "You can do at more abstract manipulation.",
            "For instance, there are waiting gradients, so that's something that's done.",
            "Automatically for you going from the costs back through all the different.",
            "Nodes in your graph and computing the gradient of the cost with respect to the inputs of the of the nodes.",
            "Obviously going to talk further about that later.",
            "We also do optimizations for numerical stability.",
            "So for instance, if you take log of 1 plus exponential of a small number, then you obviously want that to be equivalent to.",
            "That number, if it's really close to 0.",
            "Scan those kind of things.",
            "We so that's a good way, especially when you work with log probabilities.",
            "Ann, you have things that have your say expectation for binary variable that's really close to 0 or really close to one.",
            "You still want the log of that to be correctly evaluated.",
            "Then we want it to be fast, because if we did not then you could just use NUM, PY or Matlab.",
            "So we want first of all to leverage fast backends.",
            "For instance blass.",
            "We want to be able to execute that on GPU, so we have a CUDA back end, and we've also written a lot of custom C code so that we have.",
            "The ability of having everything in C and not going back and forth between C and Python.",
            "And then, as I mentioned earlier, we also have a series of tools that help debugging understanding what's wrong and detecting if something's wrong.",
            "So for instance, all the graph optimizations they have been checked numerically on lots of inputs, the automatic differentiation as well as Graham said you can always like go back through each of your parameters, add epsilon, try to see how that affects the costs.",
            "And do the difference of the curve divided by epsilon you have an approximation of a gradient, so that's one good way of checking that.",
            "That the formula we have is correct."
        ],
        [
            "So what's the status of piano?",
            "So it's starting to become major project.",
            "It doesn't move that fast anymore.",
            "We started almost six years ago, yeah, six almost seven years ago.",
            "It has driven losts of research papers in our lab and also worldwide.",
            "We have a mailing list with lots of users from meaning labs and companies around the world.",
            "Some of them are contributors so we have.",
            "Yeah, a few couple of the main contributors right now are people from outside the lab.",
            "It's been used.",
            "Various companies like cloud ones and small as well as small startups.",
            "And so if you want to have a look at more detailed documentation and tutorials there online on our website thatdeeplearning.net.",
            "Yeah, so there is a documentation for piano and a couple of tutorials on how to implement deep networks and other machine learning algorithms in Indiana."
        ],
        [
            "So right, so Theano is.",
            "More or less ratings at the same time it's a language.",
            "That lets you define mathematical expressions.",
            "It's a compiler that can go from these expressions to functions.",
            "Python, callables that you can call.",
            "And then it's also a Python library which we optimized so that actually running these functions is fast.",
            "So."
        ],
        [
            "So here is how you go about defining expressions, so this should be.",
            "This is something that you can just copy and paste in Python interpreter.",
            "So you import Theano.",
            "Ann were mostly be working with for now is tensor which is one of the main types, the other ones.",
            "But this is like the.",
            "Most widely used.",
            "So you define inputs for your expressions.",
            "For instance a vector.",
            "This is just a label that's just the name, so that we can keep track of the variables matrix and another vector.",
            "You have to define them because we need to know the type in advance.",
            "So basically vector means that it's 1 dimensional array by default it's in floating point.",
            "We also have.",
            "All the types that Mumbai defines.",
            "And then you can build expressions that are also symbolic variables, but that depend on other ones.",
            "So for instance, you define, say that one that depends on X&W and define, say, Sigma made of the previous one plus that vector.",
            "So.",
            "So, so the results of these operations are in turn variables and that you can use and so on.",
            "So right now none of these variables have values.",
            "You cannot ask.",
            "OK, what's the value of X?",
            "We don't know yet.",
            "That's something you specify later."
        ],
        [
            "So yeah, here's a nice tool for printing graph and see what you're working with.",
            "Actually, I forgot the import statement here, but it's from TNO.",
            "Printing, import, debug, print.",
            "So say the dots variable.",
            "You see it's the dot product of its inputs X&W it's so this represents a tree and then you have the output that sigmoid.",
            "Of native noise addition.",
            "Of the dot and these so that's why it's useful to name the variables.",
            "'cause otherwise it would just print like vector or matrix and neutrino."
        ],
        [
            "So.",
            "What it was referring to?",
            "So then, once you define expressions, you can define, you can compile functions.",
            "So basically you ask for a function that goes from a set of inputs to an output or a set of outputs.",
            "So for instance, I can define the function F that computes the DOT product function G that computes the output.",
            "Or if you want to compute both it and, there's a common sub expression, then it's more efficient to have functions that compute both at the same time so that they can reuse intermediate values."
        ],
        [
            "So here is the graph for two of these ones, so it's after compilation, so some graph transformations and optimizations have been applied so you can see that the dot product has been replaced by Jim V. That in that case.",
            "So that's for the DOT product.",
            "We there's no subsequent addition of a vector, So what it does is allocate a vector or full of zeros that has the appropriate shape.",
            "And then it can work in place on that vector two, to avoid memory duplication and allocation.",
            "And since the you can also see that we specified the dot product not in the most convenient way.",
            "So actually to use GMV, we transpose the matrix W. Here when we also add a bias term on that, then it's no longer working in place.",
            "Because it tries not to overwrite inputs that you provided, there are some unsafe modes that let you do that, but by default we don't overwrite values provided and we don't.",
            "We don't overwrite outputs either.",
            "So here's a different kind."
        ],
        [
            "Of graph visualization, which yeah, it doesn't show up that well.",
            "Basically in green you have the inputs of the graph.",
            "And.",
            "Here are a couple of Bottlenose like the shape, a lot and here you have the output.",
            "So why are there 4 inputs but only but we only specify two is because constant values are also considered inputs, so there's one here.",
            "That's the scale factor for GMV, and there's a zero that's used for the lock."
        ],
        [
            "Here is the same function.",
            "I mean the same visualization for the second function that computes the full output, so after the sigmoid and here."
        ],
        [
            "Is the visualization for the function that computes both, so you can see that it's really like the same input, and then from that one we compute the sigmoid or not."
        ],
        [
            "Anne.",
            "So how do you execute those functions?",
            "Now?",
            "Let's say we have some values that you get from them by here.",
            "I'm just setting the seed so that it's reproducible, and if it's right, then you have the same results.",
            "Then we simply call the same way you call any type of callable function or object in Python with the values in the same order as you specify them.",
            "At the during the compilation phase.",
            "So for instance like you have ever."
        ],
        [
            "You specify that S in boots are X&W, and then when you call it."
        ],
        [
            "Then you have to specify value for X and value for W. And this returns the value for the output, or if there was a list of outputs than the list of values.",
            "So we the tensor type uses NUM PY arrays natively.",
            "If you specify if you call these functions with things like list of numbers or couple of numbers, or for scalars, just Python floats or integers, then they will be converted to an empire.",
            "Resan those would be."
        ],
        [
            "Boost.",
            "So let's go a little bit further.",
            "How you define graph of expressions.",
            "And.",
            "Expand little bit on what I said about the types and then a couple of things that can be confusing.",
            "The small differences that we have between the tiano syntax and what you can do in Mumbai."
        ],
        [
            "So I said all Tenno variables have a type that has to be defined in advance.",
            "There are different categories of types, most set types of types, so there's a tensor type.",
            "That's the one we've seen earlier that for N dimensional arrays that are represented that handle NUM PY arrays values, there's the CUDA injury type that does same kind of things, but.",
            "On device memory for, for CUDA there's a sparse type.",
            "For sparse matrices that uses Scipy sparse values you also have.",
            "Types for scalars.",
            "There's a generic type that can handle any Python object, but that's not widely used.",
            "There's now there's a typed list type, for instance sequence of tensors of varying dimensions that have been introduced recently.",
            "So for instance, for tensor and tensor like types, so like the code injury as well.",
            "The number of dimension is part of the type.",
            "The data type.",
            "For instance, in 64 float 32 and so on is also part of the type, and the third thing that's part of the type is the broadcastable pattern, and I'm going to get to that in the next slide.",
            "But the shape of variables, an memory layouts like is it see continuous contiguous, contiguous, completely strided.",
            "That's not part of the type, so that means that once you've compiled.",
            "A graph of when, once you've built a graph with those types, then whatever.",
            "Memory layout doesn't matter.",
            "You can call the same function with inputs of different shapes from one call to the next one, as long.",
            "Obviously as those shape work together and are consistent.",
            "So.",
            "The broadcasting."
        ],
        [
            "Operation is an implicit replication of a tensor along dimension, but the dimension has to be of length one.",
            "So for instance, you can define a row that's basically A1 by N matrix or column that's an end by one matrix, and the broadcast of all things.",
            "The broadcastable pattern will be true for the dimension that is guaranteed to be always one.",
            "And that enables us to do this broadcasting so that the replication over that.",
            "That dimension, whereas we cannot do that if we define, say, two matrices, an one happens to be one by one and the other one is 1 by M. If you try to add those together, that would be an error because it's a size mismatch.",
            "So that's something that we have to know at compile time.",
            "Now by just checks the size when you call it and so that's one thing that you have to beware if you're used to run by."
        ],
        [
            "Other limitations or that we have is that the symbolic graph is purely functional, so we don't have side effects we don't have.",
            "You cannot specify operation symbolic operations that change.",
            "A node are valuable in place because the expressions that were defined in terms of the previous value still needs to use that previous value.",
            "So what we can do is build a new node.",
            "For instance, if you do so, you have that operation that changes values of acipenser or that set value to acipenser.",
            "So you cannot simply do that.",
            "But what you can do.",
            "Is build these Inc sub tensor and sets of tensor variables that basically say.",
            "Here is a variable that will have the same value.",
            "As if I did change the original 1.",
            "So that's just at the time where we build the symbolic graph.",
            "Each time you have to build a new variable.",
            "But then when compiling that expression here, now will try to replace those expressions by inplace operation if possible.",
            "For instance, if the previous value is not used by any other node further in the graph, or if they already have computed.",
            "What they needed then they can have say they have you on the original data or work in place on the original data.",
            "Now there are a couple of exceptions.",
            "Operations that have side effects.",
            "There's for instance one up that prints statistiques or values of the value of a variable, so each time it's evaluated, the output of that node will have the same value as the input.",
            "It's just copying over that value or reusing it, but each time it's evaluated, it will.",
            "Print something on the CD out.",
            "There's also an assert of that will throw exceptions if some conditions are not met.",
            "The thing is that you always have to use the output of these instead of their inputs for the next 4 as input of the next node in the graph, 'cause otherwise it will just not be used.",
            "If it's not, it's output is not, is not.",
            "Needed.",
            "And these two OPS can disrupt some optimizations, because if you want to fuse two consecutive up in the same operation, but there's a print or insert in the middle, then that that will not happen."
        ],
        [
            "The.",
            "Something else that can be a little bit confusing if you're used to Python on them by.",
            "Is that we cannot really redefine the syntax of the language itself.",
            "Actually, now we can.",
            "As as Steven mentioned during his talk on Julia.",
            "Now Python has some introspection mechanism that can represent the code as an object and we could use that.",
            "But we're not for the moment and it would.",
            "It could have like more consequences.",
            "So basically if you try to evaluate a variable, a symbolic variable with like an IF statement that will always be true.",
            "It will be evaluated when you build the graph and say OK well is that variable none or false?",
            "Well no so evaluates to true and the first branch of your if will always be selected.",
            "Annual only.",
            "Build a graph that has that.",
            "You cannot iterate with a for loop over symbolic variable either.",
            "Oh yeah, so.",
            "So yeah, genitive is to use an op, that's called FLS, and that introduced into the graph symbolic switch.",
            "For loops you can use channel scan, which I will talk a little bit later in the advanced Topics Session section.",
            "In Python, you cannot redefine the length because it needs to be an int.",
            "So we can, but you can use like a shape over the dimension 0, and as I mentioned earlier, there's a print up if you want to print the value of a variable each time it's executed and not like print the symbolic variable that you're currently building."
        ],
        [
            "So the next topic is graph transformations.",
            "So once you've built your graph, there are."
        ],
        [
            "Different things you can do, for instance substitution and cloning.",
            "So the easiest.",
            "I mean the 1st way of doing substitution is basically so you define your graph that you can compile the first function on, But then you say, Oh well, I want to do like almost the same thing, but instead of the original input that I've used, I want to use a slightly different version of that input.",
            "So for instance, I want to do normalization on it over all the values of that vector.",
            "So what you can do is define a new.",
            "Input variable.",
            "And then define the normalization that that you want.",
            "And then when you call Tina Dot function to build your level, you say, well I want to start from X_instead of X and I want to substitute.",
            "X and I want to replace my origonal X by that expression.",
            "So and then it will perform the substitution in the graph and the compiler function.",
            "Which can then call, and while the the answer is different.",
            "Obviously."
        ],
        [
            "Different way of doing that and I didn't check that did not go over, but that's the same.",
            "That's the same expression as before is to build to clone with replacement the outputs of.",
            "Drive that we've already defined.",
            "So for instance, we have gotten out and I want to say, well, let's build new nodes that will compute the same computation as dot N out, but instead of starting from X.",
            "Replace X with this expression of X.",
            "And then you simply call the analog functions starting from X, because that was the X that's there and that computes, say, an.",
            "If you evaluated, you get the same result as as previously, so those are two things that can be helpful to transform graphs."
        ],
        [
            "Now something a little bit more more complex, how we So what the backpropagation algorithm is an how we implement it.",
            "So Graham introduced that earlier.",
            "So here is a slightly different explanation or a different take on what's going on.",
            "So let's say you have a cost function from RN to R that the output has to be.",
            "A scalar value.",
            "That's actually a composition of two functions F&G.",
            "Going from respectively RN to RN RN to RN.",
            "So the chain rule says that while the partial derivative of X with respect of C with respect to X, so that would be a vector because its derivative of a scalar with respect to a vector is the dot product between those two quantities.",
            "That one is the.",
            "The gradient so.",
            "With respect to.",
            "The output of G of X and this one is the Jacobian matrix, so this one is a matrix which is the Jacobian matrix of J, so that has all.",
            "That has all the element wise derivatives of all elements of.",
            "With respect to all elements of X.",
            "The thing is that usually we don't want to compute explicitly that whole Jacobian matrix, because can be huge and often it has some particular structure.",
            "For instance, if if G is in Elementwise operation that computes the same thing for all inputs, and in that case, like would be equal to M, then only the diagonal of the Jacobian matrix.",
            "Is actually non 0 so you can implement that.",
            "Implement that product much more efficiently.",
            "So what we need is a gradient function.",
            "That goes from a vector.",
            "And that represents the gradient of the cost with respect to the output.",
            "And that computes the dot product between that and the Jacobian.",
            "And The thing is that I said compute, but I should have said returns an expression for that.",
            "That that computation.",
            "So Indiana what we do is that each.",
            "Each operation it atomic operation, which is most of them defines a grad method that does exactly that.",
            "It takes a symbolic vector or tensor as input and returns a symbolic expression for that dot product.",
            "And then once we have defined that, we can simply start from the output, the cost of the graph an go backwards in the graph, computing those chains dot products representing those chain dot products and summing the contributions.",
            "If one output goes to different is an input of different nodes.",
            "Further an all those kind of things."
        ],
        [
            "So how do use that?",
            "Well, it's actually not that complex, so if you take the previous example and define an additional vector, why that would be a target?",
            "We compute a costs that basically just the squared error between our previous output and why.",
            "And then you simply call Tiano Dot grad.",
            "First element is.",
            "Your cost has to be a scalar, and the 2nd element is a valuable of the graph.",
            "It can be an input of the graph or an intermediate variable, or it can be a list of such variables.",
            "It can be more efficient to do it that way because then all the common subexpressions for both gradients are.",
            "Shared and it just speeds up the optimization a little bit later, so here these DCW and KDB are symbolic expressions that are defined from well XBW&Y.",
            "And there's no numerical value at that point a you can.",
            "I mean, you cannot ask.",
            "OK, So what is the value for?",
            "For that?",
            "Well, we don't know, because the inputs don't have value at that point."
        ],
        [
            "So how do you use the gradients?",
            "Well, you can use them as outputs of function, because there are symbolic variables that depend on the same input variables.",
            "And you can use them to build new expressions.",
            "For instance, if you want to do gradient descent, this is just stochastic gradient descent.",
            "You with a fixed learning rate so you can just compute new expressions for W&B using the gradient.",
            "An these are things that you can also output from a general function."
        ],
        [
            "So here is the whole graph that goes from all of the inputs are couple more constants and that has the three blue outputs which are the costan."
        ],
        [
            "Both partial derivatives like both both equations."
        ],
        [
            "Then you can update the values or two ways of doing that.",
            "The one that we already know is to say, well, OK, let's compute.",
            "Let's call that function.",
            "And get values for the costs.",
            "The gradient will respect the respect to the value and with respect to be and this will be an Empire race.",
            "And then you can simply in Mumbai update the non Pi values of those variables.",
            "So a different different take would be well, rather than do that update in NUM PY.",
            "Let's use Theano, so let's compute knew values for.",
            "4 WNB and simply reassign in Python, W Val and Val to point to those new values.",
            "It's cumbersome because you have to keep track of everything in empire as well an it can be inefficient, especially if the you want to.",
            "If you specify these values as NUM PY arrays, but you want the competition to be done on GPU, then you'll have unnecessary transfers between CPU and GPU's.",
            "That's not ideal, So what we can do instead?"
        ],
        [
            "Love.",
            "Is used what we call shared variables so shared variables are a knew knew kind of variables that are symbolic that you can use As for expressions, but they have to be input variables like the vector and matrix that we created.",
            "They cannot be the result of an operation.",
            "And those variables have a numerical value associated to them at each point.",
            "They are not constant, so this value can mutate, can be reassigned and so on and so.",
            "So why is that useful?",
            "Because the value persists between calls of the function, but also between all functions that compute expressions that need that value.",
            "So for instance we would define the MGH functions earlier.",
            "That start from the same graph, well, a shared valuable would have values that persists across and that are shared by all these by all these functions.",
            "Additional thing is that you do not have to.",
            "They are input variables, but you will not have to specify them in the inputs lists when you call channel function, they will be implicit inputs for all the functions that need them.",
            "So let's go to an example."
        ],
        [
            "Of that so here is like just a.",
            "Re implementation of the earlier example where X&Y are still symbolic, purely symbolic inputs without values, but the parameters of the model WNB will be shared variables and the origin of the initial values will be what we generated for W, Val and Val.",
            "Then we have to redefine dot and out because this resign.",
            "Just resigned the value the variable names in Python that one does not.",
            "Look, once it's built, it does not look forward to variable that's currently named.",
            "Be an that's currently name W, so we have to rebuild dot and out.",
            "Actually that would have been a good way of using the clone mechanism.",
            "And replacing the previous W by the new W and the previous B by the newbie.",
            "But so right?",
            "So then we redefine FNG to take only X as inputs, because here W an here WNB are implicit outputs an if you call the functions well, passing only XXL, then you get the same results as as previously.",
            "So that's the first advantage of using shared variable is that you don't have to always pass them to the functions.",
            "You can, once they are built, you can change when you can access and change the value currently stored in them from Python by these two methods."
        ],
        [
            "So the second thing shared variables are useful for.",
            "Is updating.",
            "So.",
            "And that's that's where we started.",
            "So here I redefined the costs and the gradients.",
            "And the symbolic expression for the updated value of WNB.",
            "And here I'm going to build a different channel function that will take inputs as previously, so, X&Y.",
            "That has only C as output.",
            "An that specifies update rules.",
            "Meaning I want to replace the value of W by whatever value will be computed.",
            "Mean will be associated for the W symbolic variable.",
            "And the same thing for be.",
            "So as As for earlier W&B are implicit inputs.",
            "But and update W an update B will be implicit outputs that will be computed when the channel function will be called.",
            "And then after these values are computed, the updates will be performed.",
            "So changing the value that's inside W an inside B to take the value of the that has been computed for these implicit outputs."
        ],
        [
            "So here is yeah, if I if I plot."
        ],
        [
            "That's great degree for that."
        ],
        [
            "Function here's what we have, and it's almost the same."
        ],
        [
            "Same one as that one.",
            "I think the only."
        ],
        [
            "Difference is that now, here's there's a jar method.",
            "So that's the rank 1 update for the weights and now it's performed in place because the fact that it's a shared variable allows it to to update its value in place because it's not explicit inputs that has to be that has to be preserved.",
            "So right so now you see."
        ],
        [
            "Seen a.",
            "Complete example of like how to define an expression gradient update rules and call them.",
            "So the next step is OK, I can do that, but how can I do that fast enough so that I can train my model before my next deadline?"
        ],
        [
            "So we have, as I said earlier, graph optimizations and you've seen the effect on the graph using the debug print statement earlier.",
            "So what optimizations are is basically rules for replacing a part of the graph with the different parts an.",
            "As for when you do the computation, the substitution by hand using clone or using the given keyword of function, while the types have to match.",
            "So we are.",
            "I mean there are really different categories of optimizations.",
            "Some are there to avoid duplicating work.",
            "So for instance, if you have the same expression computed twice at different places in your graph, they they will be merged.",
            "We have simplification of expressions, so for instance X over X, X -- X, U transpose matrix, and then you transpose it back.",
            "If you do like 3 copies, three successive.",
            "If you do 3 copies and then you never do anything with the intermediate values, then you can do only one.",
            "All those kind of things we have also to help with that we also have a few optimizations that reorder the graph.",
            "For instance, if you do an elementwise operation on the matrix and then transpose it, it's the same as if you transpose it first and then do the elementwise operation.",
            "That enables us, for instance, to have all the transposition one all one after the other, and merge them into only one operation, or remove it completely if it does not do anything.",
            "So try to avoid duplicating work and so that's something.",
            "So that's when first kind of optimizations.",
            "We have optimizations for numerical stability.",
            "As I said earlier, we want to simplify like log of X.",
            "Then you we want to simplify log of sigmoid into soft.",
            "Plus we want to simplify those kind of things, especially because in some cases we are going to want to work in single precision Floating Points because that's really much faster.",
            "On GPUs it's not that true now, but it was definitely true a couple years ago.",
            "The.",
            "The sum GPU didn't even support double precision float and some supported it, but it was like 5050 times slower than single precision.",
            "So in those cases we really want things to be as stable as possible.",
            "We have optimizations that insert in place operations instead of operations that do a copy or allocation for their outputs, so we have ways of.",
            "Of going through the.",
            "The computational graph and try to see OK. Is that node still needed?",
            "Can we overwrite its value?",
            "Is it an input that we should not that we should not destroy?",
            "Is it an output that we should try to copy because we don't want if the user still has it around?",
            "We don't want that value to be overwritten next time we call the function.",
            "Those kind of things.",
            "We had and then four.",
            "Purely conditional efficiency, we have bindings to high performance library like Glass.",
            "We use some high performance library on on GPU using welcome glass.",
            "For instance quick 50 and those kind of things we do elementwise look Fusion.",
            "So for instance, if you have a bunch of elementwise operation that you apply on the same array.",
            "Then you say I don't know.",
            "You add a few other constant and then take the log and then take the sigmoid and then take something you don't want to loop over your whole array once for each operation all the time.",
            "So we want to do only one loop where so we have only one memory access instead of many.",
            "We do shape inference to try to specialize some operations and to try to detect if there are some things that we can simplify.",
            "We do constant folding.",
            "So he remember Graham Taylor's example of the gradient of X squared we had in the original and optimized graph.",
            "You had things like constant of two minus constant of one, while all those kind of things get computed only once and then you see that if you multiply by one then you can remove that and so these are all things that optimizations do.",
            "And the same optimization framework is also used to transfer things to GPU.",
            "So what we actually have is that the competition done on GPU are usually different operations, different kind of nodes and the variables do not have the same type because they are could endure attack instead of tensor type.",
            "And we use the.",
            "The existing optimization mechanism to say OK. Well, if for instance I have one something that's on GPU.",
            "And that has been transferred to CPU.",
            "And that's as input to.",
            "Operation that we have implemented on GPU.",
            "Then we will transfer it back to GPU, apply the GPU operation and then insert another transfer so that when you do the substitution the types match.",
            "And then later the optimization that removes useless operations will replace like transfer from GPU to CPU and to CPU back to GPU with nothing.",
            "So basically, that's that's that's the way it happens.",
            "It starts from like all inputs that are on GPU and try to move like the other inputs of the same operation also to GPU and the outputs, and to have only.",
            "If possible, no transfers at all or just the transfer.",
            "Maybe at the beginning and at the end if you want the costs.",
            "Or only at the end if possible.",
            "So let's."
        ],
        [
            "Graduations as a user, you want to sometimes want to have some control over which optimizations are applied and when, because this graph optimization process can be long an in fact it scales super linearly with the number of nodes in the graph.",
            "So if you have a large graph, specially if you do a lot of small computations or.",
            "If you have loops that you've manually unrolled, then the competition times can be can be long.",
            "If you know if it's your final model and you know that's exactly what you want to do, then it's not that bad, because you can run experiments.",
            "For me, we usually have experiments that could run for a couple of days on GPU's.",
            "If it takes an additional 2 minutes to compute the graph at the beginning, well, it's not a big deal, but when you're prototyping and if you want to.",
            "Check that everything works OK and that you have all the necessary operations and that equations are correct and and all that.",
            "Then sometimes it can be bothersome.",
            "So we have a predefined different fine mode that's called fast compile that tries to minimize that compilation overhead but uses usually Python code instead of C code, because that's also something that we don't want to compile every time, and so yeah, so summations.",
            "Multiplied, but sometimes can be worth it.",
            "There's a different mode, like a different setting, that you can use that's called debug mode an it's another instance of the checking tools that we have, and basically what this does is it executes.",
            "The graph, and I mean each operation before and after each optimization and checks that the values are close.",
            "So for stability optimizations, well, it might not always be the case, because that's the point you don't want the wrong value.",
            "You want the right one, but for most optimizations it's it's really useful, but it's extremely slow because it will also check the Python code versus the C code.",
            "It will check a lot of things.",
            "An then there's also a mechanism to enable and disable or particular optimizations or set of optimizations you want.",
            "Sometimes you want to be able to use the GPU, but not transfer everything to GPU, but have more manual control over that, so you could disable the GPU optimizations and all that can be done either globally for the whole process or at the level of each.",
            "The handle function.",
            "So when you call tiano function inputs, outputs, updates, Givens and there's also parameter that's called mode and you can where you can specify OK which which optimizations I want to include or not."
        ],
        [
            "Yeah, so I was talking a little bit of about C code earlier, so basically each operation that we define in piano, that's a Python object that will represent, for instance, I say the addition reduction over that set of dimension transposition.",
            "All those kind of operations.",
            "You can define a method in Python that will take the NUM PY arrays as inputs and.",
            "Compute the output, set them at the right place where the rest of the runtime can find them, and then.",
            "Then it will be.",
            "Used by by the general function callable.",
            "There's also a different way, that is to specify C code.",
            "So how does that work?",
            "There's a method called C code that takes as input variable names that will represent the intermediate computations in C. And it outputs.",
            "A string that contains the code for a Python module implemented in C. So we can do that by just having a large template and substituting the variable names.",
            "It can be more than that.",
            "For instance, if you want to implement reduction over a variable number of axes, then you have like different for loops, a different number of implicated for loops and those kind of things so that you can do that simply by having for loops in Python And stitching steam strings together.",
            "And kind of things and.",
            "Once and there's also like boilerplate code that systematically added to check that the inputs have the right type that the dimensions match that extract them from the right place in memory, and set them, and so on.",
            "An once you have the code for that whole module, it gets written to disk in C++ or CU file.",
            "It's compiled by G+ Plus and then imported back in Python in temporary temporary module.",
            "So then that module defines a function that will like the perform method read.",
            "Python read input as Python object.",
            "From memory, do the computation and store the outputs at the right place.",
            "Those generated C files and compiled so or DLL's.",
            "Will be cashed from one run to the next one, so there's a persistent cache on the hard drive.",
            "Do not set that on the network drive.",
            "That's a bad idea.",
            "Have it's local in TMP or something and it has limited durations that if some.",
            "Some code is not used anymore than it will be clean.",
            "It will be cleaned up regularly and does so in order to use that you have to define versions for your C code and when you change it then you bump the version number and so the cash can be managed than this.",
            "I'm just wondering if you can actually cache a compiled piano graph.",
            "Not yet.",
            "We have a couple of things coming that way.",
            "There was also an experimental project where you could have a whole piano function where instead of having different modules for each operation, you have only one module that compile the whole.",
            "So from the from the input of the function to the output.",
            "And that you could save and reuse The thing is that if we try to cash that we have an exploding number of.",
            "If we did that all the time, then.",
            "The number of saved objects would be really huge and the overhead of going through them when you want to see if it has already been done could be larger than just re compiling it again because there are just too many modules to load and check the version and so on.",
            "So yeah, so we have this cache of compiled C module that will try to reuse if possible and for GPU code.",
            "Actually it's exactly the same process.",
            "Except that instead of reading non Pi injuries using NUM Py C API.",
            "Well, we'll read could endure Azor GPU arrays for the new GPU back end that's coming on and right into them.",
            "And we will compile that using MVC instead of G+ plus and.",
            "But that's that's really the same.",
            "The same mechanism."
        ],
        [
            "So then when we have these Python modules that do the actual computation, even if done is it's C code, while what the Python module interface exposes is a Python function.",
            "So we have what we call runtime environment or virtual machine that's responsible for executing the computation for 18 to function.",
            "So, So what that does is keep track of which operations there are to execute an which depend on which ones and then called what we call the thank all the functions that will.",
            "Perform the actual computation and set the result at the appropriate place.",
            "So we have different mentation of those, but what we realized is that even if the underlying implementation in C, the fact that if from Python we call the Python wrapper around that each time, sometimes it can be quite slow.",
            "It can take like there's a context switch between C and Python that can take in the orders of maybe, I think 50 NS or something like that.",
            "An when the underlying composition is really small thing, like take the length of an array or take the shape along that dimension, then that can take like 100 times.",
            "Less this time.",
            "So for all those cases, what we did is that it was actually James Box right that working mainly on that is that there is also a runtime that's written completely in C. And that will call the C pointer functions that defined by the the C modules.",
            "So you have before hand to set the addresses for all the intermediate computation values, you have to specify the ordering of the nodes in the graph.",
            "And you have to set all these function pointers.",
            "If if a couple of these elementary operations do not have implementation, then it's OK. We can also call the C the Python implementation from C which is.",
            "Again, a little bit slow because you have that overhead of context switching, but it's not that bad and at least it makes it possible to do that, whereas generating all the C code in only one module and compiling that will fail if even one operation do not have this implementation.",
            "An when you have set up that whole structure, then you only have to do 1.",
            "Call to see function from Python And it will run the whole thing from inputs to outputs.",
            "And if you have a function that do not have any explicit input and outputs, for instance, all the inputs are implicit shared variables an all outputs are the rules for the shared variables, then you can even.",
            "Lounge more than one call to the function at one so you can say execute the function end times and do not return to Python until you're completely finished.",
            "Anne, this can be quite a speedup if if you have lots of small computations and if your graph cannot be reduced to like a couple of gem calls that takes 90% of runtime.",
            "Oh yeah, and there's also lazy evaluation that's implemented in those virtual machines, so if you have an if else node, it will not try to compute all three inputs like the condition, the, then branch and the else branch.",
            "It will actually ask for the node.",
            "OK, So what do you need then?",
            "Would say, well, I need the value of the condition to compute whatever is needed to compute the value of the condition and then.",
            "Time will say, OK, here's your here's the value for your condition.",
            "Now what you need?",
            "Oh well, the condition was a true I need the first expression.",
            "OK so the CPM will all the other VM will compute the value for that and say, OK, well now your condition is that and your then branch.",
            "Is that what you need?",
            "If I say, OK, well, I have everything I need.",
            "My output is that and.",
            "So that was something that was in that had been missing for sometimes until James added that maybe three years ago."
        ],
        [
            "So how to use the GPU?",
            "So we want to make it as easy as possible to switch the GPU on and off.",
            "But The thing is that there are couple limitations currently.",
            "The first one is that.",
            "Since GPU's where I mean did not support float 64 at some point we the library that we use for arrays currently only supports float 32, so no integers, no complex of course and no double precision, only float 32.",
            "And these objects, these arrays that reside on GPU.",
            "We did not add a nice interface to interact with them from Python.",
            "So for instance if you try to print them and like NUM py, injuries will not see that content will just say, well, that's an could injury objects at address whatever.",
            "So we did not implement all the Python bindings for like say, set setting values and getting slices and so on, so that can be a pain if you want to have them as explicit inputs and outputs.",
            "So what we do is that if a GPU is selected and I'll talk a little bit later about how you do that.",
            "What happens is that we decide.",
            "That all shared variables that are created and that have the flow 32D type will be transferred to GPU as soon as they are created.",
            "And then it gives a starting point from the optimization process that I mentioned earlier, where if you have some.",
            "If you have an operation that has some inputs on GPU, then we move the operation as well as all the other inputs to the GPU as well.",
            "And that's that's the starting point.",
            "And then you you iterate through that from inputs to outputs.",
            "So that's the way we get that started.",
            "And obviously if no GPU is selected, then those variables will stay on CPU and you won't see any difference.",
            "So you want to be sure to use float 32 and there's an option to easily switch between float 32 and float 64 in Theano, which is the the type called float X.",
            "So by default it's aliased to float 64, but you can change that.",
            "With the config flag and that's the default type of tensors.",
            "So earlier when I created input variables with like T dot vector or matrix, by default if you don't specify data type then it will be float X so that you are 64.",
            "Depending on the current setting.",
            "But if you want to have explicit double precision or single precision regardless of that, then you can always like use.",
            "F matrix for floating point D matrix for double precision.",
            "Or specify explicitly what the data type should be.",
            "So."
        ],
        [
            "I was talking about configuration flags, so there are different ways of setting them.",
            "So those are three main ways are to use the environment variable.",
            "Let's call Tiano flags and you give pairs of valuable.",
            "Flag names and values and that's going to be interpreted just from the start.",
            "Another way is from from Python.",
            "So once once the annoys imported you can change the configuration value for some flags, but not all.",
            "For instance the path to the.",
            "Cache of compiled modules, while once you've imported 10 oh, it's already been loaded.",
            "It's already been configured.",
            "You cannot change that later, so it's not does not work for some for some flags, but if you try to set them, you'll get a meaningful error message.",
            "And the last way is with a configuration file, so we have first order first level flags that are in the global.",
            "Section you could also like subsections to specify more specific things.",
            "So if you want to use GPU, basically the two flags that you have to set or device which which you should set to GPU which means take the first available GPU or however could decide to get one or a specific one with GPU and index the new back end.",
            "That's I'll talk a little bit later, supports.",
            "No other syntax, so it's you say like say, CUDA zero or open CL one or.",
            "And you usually want to set float X2 float 32.",
            "Unless you've already been working with like F vectors and F matrixes from the salt.",
            "Ann once, once you do that, then your program will.",
            "I mean will run on GPU if it's available and if every operation is implemented on GPU."
        ],
        [
            "So it's time to have OK, so let's talk about with about a couple of more advanced topics.",
            "Not going to go into too much detail in there, but just to give you a sense of what more is possible."
        ],
        [
            "So.",
            "We cannot, so as I said earlier, if you use a for loop when you build your graph, it will iteratively build new variables.",
            "So it's equivalent of completely unrolling that loop.",
            "If you don't want to do that, because.",
            "Lots of different reasons there is an operation that can specify those loops completely symbolically.",
            "Which is called scan.",
            "And so it can perform like different.",
            "Different kinds of looping, so things like map reduce accumulations it can.",
            "It can look at inputs at, say, the previous time step, or more time steps before can also use as previous outputs an the number of iterations does not have to be constant.",
            "It could either be a symbolic variable that you evaluate.",
            "At that point in the graph and you say OK, have to do any loops and from the beginning of the execution the scan node will know how many loops it has.",
            "Or it could be a stopping condition that will be computed and evaluated at each step.",
            "And then if it returns true, then the.",
            "Decorating stops there.",
            "You still have to have a maximum number of epochs, so it's not completely so it's not completely as a while loop, it's more like break statements in the for loop.",
            "And it has to be executed at least once.",
            "I mean, the whole group has to be executed.",
            "That's added at least once on the way it works.",
            "Is that inside that scan operation?",
            "You actually define a general function.",
            "It will be compiled into at the end of function that will be executed at each step.",
            "So let's say if you want to.",
            "If my inputs are, say, one input at times T -- 1 and minus 20 -- 3, this would be like 3 different symbolic inputs.",
            "At each time step.",
            "And so the scan node dispatches all these slices of inputs and outputs to that function and call that in the loop and stop the loop if the stopping condition becomes true.",
            "So also if.",
            "So the scan node can also be moved to GPU.",
            "And in that case it will try to to move also the inner function to GPU and have everything run on GPU.",
            "The overhead of that is still quite high, so if you do like large recurrent neural Nets then it's not going to be large percentage of your runtime.",
            "But if you want to work on like small matrices or scalars then GPU is not using the GPU with Ken is is not going to be worth it.",
            "And the grid into the grid method of that scan operation implements backpropagation through time.",
            "So you can specify, so you have say gradients with respect to the output at each time step at the last time step, and then it will do like all the backpropagation and use actually a different scan operation that goes backwards and accumulates the right kind of gradients to the right kind of updates and return that.",
            "You can also, if you don't want the whole backpropagation through time, you can also.",
            "Kept it to a couple of epochs.",
            "So that's in that's the end of that scan.",
            "It's not easy to use because you have a lot of different things that."
        ],
        [
            "That you can specify.",
            "You can have to specify initial states, sometimes initial states for different for more than one time step.",
            "If you go, if you're looking back to more than one epoch.",
            "You won't be able to specify updates that are performed at each iterations for shared variables.",
            "For instance, if you use pseudorandom number generations, then you do not want to draw the same samples at each time step usually.",
            "You want to update the state of the pseudorandom number generator so that you can run you samples each time, so the syntax is a little bit complex.",
            "Here's one simple example that's just an accumulation and basically what you do is define a function.",
            "That takes different arguments, so depending on what you want to do, this will be so here.",
            "This is like the output at step T -- 1.",
            "And this is a non sequence input, so that's an input that will stay the same and that will be used at each time step.",
            "So that's not something that changes, but all that we iterate through.",
            "But just consider the constant for the evaluation of that and you specify and then here new results is like the Elementwise product of the prior result and apps.",
            "You specify that initially you start with once the same shape as a.",
            "A is a non sequence so that it will be fed as is for each time step.",
            "You could also specify a sequences and if it's a sequence that means.",
            "For instance, if you specify your matrix as a sequence, then each step will see a different role of that matrix and so.",
            "And how many steps you want?",
            "Sometime yeah here we only care about the last result, so that enables some optimizations that will for get the intermediate results as soon as they don't.",
            "They're not useful, and here compiled and functions.",
            "You specify the updates that would be specified by the scan function here.",
            "It's probably nothing that if you work with random numbers then you also have.",
            "To do that kind of updates.",
            "Otherwise it's not from step to step that you draw the same samples that it's from call to the channel function to the next one.",
            "So like say for the first mini batch will have some sequence of random numbers, and if you don't update your random number generator at the end, then that means that for the second mini batch you'll get like exactly the same numbers run in the same order, which is usually something that you do not want.",
            "And so here's here's a smaller.",
            "The example of a very."
        ],
        [
            "Now I'm going to talk about extending Theano quickly, so.",
            "No two ways of adding new operations.",
            "The easiest way is if you have a Python function that do what you want, then you can simply define a new operation an have the perform method that calls that, or if you do not need a gradient, there's even like a simple operator that's called as up where you can specify like the input.",
            "Types the output types an optionally infer shape function that will try to compute symbolically what the shapes of the output is given the shape of the inputs, and that's useful for the shape inference mechanism that I was mentioning earlier.",
            "There's no way of specifying ingredients using that decorator yet, but you can also simply inherit from up an defined performan grad method, and that's not much more complex.",
            "So that's the way.",
            "For instance, that recently someone implemented 3D convolution on GPU using Fast Fourier transform because just by calling bindings, Python bindings to existing software that computed that.",
            "Since it's a large operation and then the overhead of doing that in Python is not that high and it's a perfectly perfectly.",
            "Good way of extending channel for your needs."
        ],
        [
            "The harder way is using C code, so, but then you'd have to be a little bit familiar with the API of of Mumbai an how to access their pointers to arrays and so on.",
            "Or code ING arrays or GPU arrays.",
            "As I mentioned when I was talking about the type, those strides and the memory layout is not part of the types, so it can be arbitrary at at runtime.",
            "So you have to count for that or at least called GPU contiguous operation.",
            "That makes sure that everything is continuous and that you can use it.",
            "You have to manage reference counts because we use Python for our memory allocation and deallocation.",
            "But you don't have the overhead of Python context switching, and if you disable the internal garbage collector then you don't.",
            "You will not even go back through the to the Python interpreter so it can be really fast, and especially if it's on GPU, it can be a synchronous and you can gain some speed with that and that's the way that external contributors recently wrapped the cafe convolution.",
            "Into Theano to have fast implementation using German on GPU."
        ],
        [
            "So too.",
            "As we come to the conclusion what to expect in the next couple of months, what are the things that we are that we have been working on?",
            "So there's the new GPU back end, which I've mentioned a couple of times that will support all D types for GPU arrays and that will work both for CUDA and open CL.",
            "That's pretty exciting.",
            "Now we still have some things to tune and the support for CUDA is still much better than for open CL because we could more easily backport the previous.",
            "It's the previous operations to CUDA rather than an open CL.",
            "And this back end will also enable you to use more than one GPU in the same channel function.",
            "You'd have to be explicit about it, so unlike now where it moves everything it can to the GPU if there's only one.",
            "If you want to separate things and dispatch them across different GPU, then you'd have to specify explicitly.",
            "OK, well here's a transfer that I want to do from the host to this CPU to this GPU or from that GPU to these GPU and then back.",
            "And but then if you have like 2 pounds that are in the same GPU to try to have.",
            "All the information computation on the same.",
            "What we have also coming is the execution of unoptimized graph on GPU.",
            "So as I mentioned earlier there's the fast compile mode but one of the limitation of that is that since moving things to GPU is an optimization then it's not performed by by fast compile.",
            "So we're trying to have the minimal set of optimizations that are needed to transfer things over to GPU.",
            "In one mode that people can work with, we have serialization and disorganization of optimized function graph, so this was mentioning earlier we.",
            "Also have someone working on easier ways of writing C code for OPS, especially being able to have one C++ or the file apart from the Python source and be able to work with that instead of having to mingle with strings in Python so you can have like syntax highlighting using your favorite editor.",
            "Or you can have all those kind of things.",
            "It will not be applicable for everything.",
            "For instance, well if you have a variable number of dimensions and the variable number of implicated for loops, then obviously that's not going to help, but for the easiest cases, especially if you just want to wrap an external library, then it will.",
            "It would be much easier.",
            "Something else that's been needed for awhile and that we hope to finish in the next couple of months is the ability to serialize using pickle shared variables that are on GPU so that they are serialized as an employee injury and that if you try to reload a function or even just the shared variable on the machine that does not have a GPU then you're still able to access those values and two.",
            "For instance, train your model on a GPU because it's fast, but then use it for prediction on the machine that only has CPU's.",
            "So that's that's a lot.",
            "That's more complex that we thought, and we hope to be finished with that soon.",
            "And then there are like a lot of newer versions of faster versions of convolution or cross correlations for CPU and GPU.",
            "So I mentioned the one using FFT.",
            "There's a one using cafe that we're still perfecting and then couple of days ago.",
            "Yeah announced that there will be CUDA CUDA interface for convolutions directly in the Quad SDK.",
            "So we are planning on having implementation for that.",
            "Then we probably have a mechanism to enable choosing between all these implementations, because some are sometimes faster and sometimes not there.",
            "Also, like memory requirements that are not the same.",
            "So we want users to be able to say OK, this one is small, so I want to use a 50 because it's really fast, but I don't have enough memory to use a 50 on that one.",
            "So I want to use cafe.",
            "And.",
            "So."
        ],
        [
            "Yeah, we completely close to the end, so just wanted to thank all my colleagues and former colleagues at Lisa, especially the tender contributor.",
            "So here is like really small list of contributors here.",
            "That's really a small subset of people who did much work on on piano and associated software.",
            "Then compute Canada and accuracy.",
            "HP and Sorkin Canada research chairs for all the support either in funding or providing computing resources that helped us develop all those kind of things and take advantage of them.",
            "And of course the organizers and participants of this conference."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Morning, so I'm here to present you library that we developed mostly in in our lab in Montreal.",
                    "label": 0
                },
                {
                    "sent": "Those provision of professor your banjo so and that's what we are mainly using to do research on deep networks.",
                    "label": 0
                },
                {
                    "sent": "Various kind of things that grant presented and also many other different things that we are experimenting with so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starting just by giving you an overview of how to use the library and what that does, and then I'll get to more advanced examples and present in particular how we work with gradients, automatic differentiations, graph optimizations, how to use the GPU and then a couple of more advanced.",
                    "label": 0
                },
                {
                    "sent": "Topics if so.",
                    "label": 0
                },
                {
                    "sent": "Trying to give you like hands-on field of what it looks like to write code that uses Theano and so if you have any questions or if things are not clear, it's OK to raise your hand and interrupt and I'll try to clarify.",
                    "label": 0
                },
                {
                    "sent": "I'll try to have an open Python session on the side if I can.",
                    "label": 0
                },
                {
                    "sent": "If I have to compose some things and if there is still.",
                    "label": 0
                },
                {
                    "sent": "Time at the end of the presentation we might.",
                    "label": 0
                },
                {
                    "sent": "I mean, I might.",
                    "label": 0
                },
                {
                    "sent": "Try some examples, especially to give you an idea of how.",
                    "label": 0
                },
                {
                    "sent": "Things look like when it doesn't work when there are errors.",
                    "label": 0
                },
                {
                    "sent": "If you have to debug things because it's not always.",
                    "label": 0
                },
                {
                    "sent": "I mean if it works then everything is fine, but sometimes it can be a little bit confusing if not everything goes as expected.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me check by the motivation and the objectives of piano, so there are no is a compiler for mathematical expressions.",
                    "label": 0
                },
                {
                    "sent": "So what that means is that we define mathematical expressions as a graph of symbolic variables.",
                    "label": 0
                },
                {
                    "sent": "We try to do that, so we do that in Python And we try to be really close to numb by semantic so that it's not confusing for people who already knew.",
                    "label": 0
                },
                {
                    "sent": "No, NUM py and low syntax is not as far away from Matlab as it as it could be.",
                    "label": 0
                },
                {
                    "sent": "Then when you define those mathematical expression, you can do manipulation of that expression graph so you can do substitution.",
                    "label": 0
                },
                {
                    "sent": "You can extend the graph.",
                    "label": 0
                },
                {
                    "sent": "You can do at more abstract manipulation.",
                    "label": 0
                },
                {
                    "sent": "For instance, there are waiting gradients, so that's something that's done.",
                    "label": 0
                },
                {
                    "sent": "Automatically for you going from the costs back through all the different.",
                    "label": 0
                },
                {
                    "sent": "Nodes in your graph and computing the gradient of the cost with respect to the inputs of the of the nodes.",
                    "label": 0
                },
                {
                    "sent": "Obviously going to talk further about that later.",
                    "label": 0
                },
                {
                    "sent": "We also do optimizations for numerical stability.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you take log of 1 plus exponential of a small number, then you obviously want that to be equivalent to.",
                    "label": 0
                },
                {
                    "sent": "That number, if it's really close to 0.",
                    "label": 0
                },
                {
                    "sent": "Scan those kind of things.",
                    "label": 0
                },
                {
                    "sent": "We so that's a good way, especially when you work with log probabilities.",
                    "label": 0
                },
                {
                    "sent": "Ann, you have things that have your say expectation for binary variable that's really close to 0 or really close to one.",
                    "label": 0
                },
                {
                    "sent": "You still want the log of that to be correctly evaluated.",
                    "label": 0
                },
                {
                    "sent": "Then we want it to be fast, because if we did not then you could just use NUM, PY or Matlab.",
                    "label": 0
                },
                {
                    "sent": "So we want first of all to leverage fast backends.",
                    "label": 0
                },
                {
                    "sent": "For instance blass.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to execute that on GPU, so we have a CUDA back end, and we've also written a lot of custom C code so that we have.",
                    "label": 0
                },
                {
                    "sent": "The ability of having everything in C and not going back and forth between C and Python.",
                    "label": 0
                },
                {
                    "sent": "And then, as I mentioned earlier, we also have a series of tools that help debugging understanding what's wrong and detecting if something's wrong.",
                    "label": 0
                },
                {
                    "sent": "So for instance, all the graph optimizations they have been checked numerically on lots of inputs, the automatic differentiation as well as Graham said you can always like go back through each of your parameters, add epsilon, try to see how that affects the costs.",
                    "label": 0
                },
                {
                    "sent": "And do the difference of the curve divided by epsilon you have an approximation of a gradient, so that's one good way of checking that.",
                    "label": 0
                },
                {
                    "sent": "That the formula we have is correct.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the status of piano?",
                    "label": 0
                },
                {
                    "sent": "So it's starting to become major project.",
                    "label": 0
                },
                {
                    "sent": "It doesn't move that fast anymore.",
                    "label": 0
                },
                {
                    "sent": "We started almost six years ago, yeah, six almost seven years ago.",
                    "label": 0
                },
                {
                    "sent": "It has driven losts of research papers in our lab and also worldwide.",
                    "label": 1
                },
                {
                    "sent": "We have a mailing list with lots of users from meaning labs and companies around the world.",
                    "label": 1
                },
                {
                    "sent": "Some of them are contributors so we have.",
                    "label": 0
                },
                {
                    "sent": "Yeah, a few couple of the main contributors right now are people from outside the lab.",
                    "label": 0
                },
                {
                    "sent": "It's been used.",
                    "label": 0
                },
                {
                    "sent": "Various companies like cloud ones and small as well as small startups.",
                    "label": 0
                },
                {
                    "sent": "And so if you want to have a look at more detailed documentation and tutorials there online on our website thatdeeplearning.net.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there is a documentation for piano and a couple of tutorials on how to implement deep networks and other machine learning algorithms in Indiana.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So right, so Theano is.",
                    "label": 0
                },
                {
                    "sent": "More or less ratings at the same time it's a language.",
                    "label": 1
                },
                {
                    "sent": "That lets you define mathematical expressions.",
                    "label": 0
                },
                {
                    "sent": "It's a compiler that can go from these expressions to functions.",
                    "label": 1
                },
                {
                    "sent": "Python, callables that you can call.",
                    "label": 0
                },
                {
                    "sent": "And then it's also a Python library which we optimized so that actually running these functions is fast.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is how you go about defining expressions, so this should be.",
                    "label": 0
                },
                {
                    "sent": "This is something that you can just copy and paste in Python interpreter.",
                    "label": 0
                },
                {
                    "sent": "So you import Theano.",
                    "label": 0
                },
                {
                    "sent": "Ann were mostly be working with for now is tensor which is one of the main types, the other ones.",
                    "label": 0
                },
                {
                    "sent": "But this is like the.",
                    "label": 0
                },
                {
                    "sent": "Most widely used.",
                    "label": 0
                },
                {
                    "sent": "So you define inputs for your expressions.",
                    "label": 0
                },
                {
                    "sent": "For instance a vector.",
                    "label": 0
                },
                {
                    "sent": "This is just a label that's just the name, so that we can keep track of the variables matrix and another vector.",
                    "label": 0
                },
                {
                    "sent": "You have to define them because we need to know the type in advance.",
                    "label": 0
                },
                {
                    "sent": "So basically vector means that it's 1 dimensional array by default it's in floating point.",
                    "label": 0
                },
                {
                    "sent": "We also have.",
                    "label": 0
                },
                {
                    "sent": "All the types that Mumbai defines.",
                    "label": 0
                },
                {
                    "sent": "And then you can build expressions that are also symbolic variables, but that depend on other ones.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you define, say that one that depends on X&W and define, say, Sigma made of the previous one plus that vector.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So, so the results of these operations are in turn variables and that you can use and so on.",
                    "label": 0
                },
                {
                    "sent": "So right now none of these variables have values.",
                    "label": 0
                },
                {
                    "sent": "You cannot ask.",
                    "label": 0
                },
                {
                    "sent": "OK, what's the value of X?",
                    "label": 0
                },
                {
                    "sent": "We don't know yet.",
                    "label": 0
                },
                {
                    "sent": "That's something you specify later.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, here's a nice tool for printing graph and see what you're working with.",
                    "label": 0
                },
                {
                    "sent": "Actually, I forgot the import statement here, but it's from TNO.",
                    "label": 0
                },
                {
                    "sent": "Printing, import, debug, print.",
                    "label": 0
                },
                {
                    "sent": "So say the dots variable.",
                    "label": 0
                },
                {
                    "sent": "You see it's the dot product of its inputs X&W it's so this represents a tree and then you have the output that sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Of native noise addition.",
                    "label": 0
                },
                {
                    "sent": "Of the dot and these so that's why it's useful to name the variables.",
                    "label": 0
                },
                {
                    "sent": "'cause otherwise it would just print like vector or matrix and neutrino.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What it was referring to?",
                    "label": 0
                },
                {
                    "sent": "So then, once you define expressions, you can define, you can compile functions.",
                    "label": 0
                },
                {
                    "sent": "So basically you ask for a function that goes from a set of inputs to an output or a set of outputs.",
                    "label": 0
                },
                {
                    "sent": "So for instance, I can define the function F that computes the DOT product function G that computes the output.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to compute both it and, there's a common sub expression, then it's more efficient to have functions that compute both at the same time so that they can reuse intermediate values.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the graph for two of these ones, so it's after compilation, so some graph transformations and optimizations have been applied so you can see that the dot product has been replaced by Jim V. That in that case.",
                    "label": 0
                },
                {
                    "sent": "So that's for the DOT product.",
                    "label": 0
                },
                {
                    "sent": "We there's no subsequent addition of a vector, So what it does is allocate a vector or full of zeros that has the appropriate shape.",
                    "label": 0
                },
                {
                    "sent": "And then it can work in place on that vector two, to avoid memory duplication and allocation.",
                    "label": 0
                },
                {
                    "sent": "And since the you can also see that we specified the dot product not in the most convenient way.",
                    "label": 0
                },
                {
                    "sent": "So actually to use GMV, we transpose the matrix W. Here when we also add a bias term on that, then it's no longer working in place.",
                    "label": 0
                },
                {
                    "sent": "Because it tries not to overwrite inputs that you provided, there are some unsafe modes that let you do that, but by default we don't overwrite values provided and we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't overwrite outputs either.",
                    "label": 0
                },
                {
                    "sent": "So here's a different kind.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of graph visualization, which yeah, it doesn't show up that well.",
                    "label": 0
                },
                {
                    "sent": "Basically in green you have the inputs of the graph.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here are a couple of Bottlenose like the shape, a lot and here you have the output.",
                    "label": 0
                },
                {
                    "sent": "So why are there 4 inputs but only but we only specify two is because constant values are also considered inputs, so there's one here.",
                    "label": 0
                },
                {
                    "sent": "That's the scale factor for GMV, and there's a zero that's used for the lock.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the same function.",
                    "label": 0
                },
                {
                    "sent": "I mean the same visualization for the second function that computes the full output, so after the sigmoid and here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the visualization for the function that computes both, so you can see that it's really like the same input, and then from that one we compute the sigmoid or not.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So how do you execute those functions?",
                    "label": 0
                },
                {
                    "sent": "Now?",
                    "label": 0
                },
                {
                    "sent": "Let's say we have some values that you get from them by here.",
                    "label": 0
                },
                {
                    "sent": "I'm just setting the seed so that it's reproducible, and if it's right, then you have the same results.",
                    "label": 0
                },
                {
                    "sent": "Then we simply call the same way you call any type of callable function or object in Python with the values in the same order as you specify them.",
                    "label": 0
                },
                {
                    "sent": "At the during the compilation phase.",
                    "label": 0
                },
                {
                    "sent": "So for instance like you have ever.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You specify that S in boots are X&W, and then when you call it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you have to specify value for X and value for W. And this returns the value for the output, or if there was a list of outputs than the list of values.",
                    "label": 0
                },
                {
                    "sent": "So we the tensor type uses NUM PY arrays natively.",
                    "label": 0
                },
                {
                    "sent": "If you specify if you call these functions with things like list of numbers or couple of numbers, or for scalars, just Python floats or integers, then they will be converted to an empire.",
                    "label": 0
                },
                {
                    "sent": "Resan those would be.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Boost.",
                    "label": 0
                },
                {
                    "sent": "So let's go a little bit further.",
                    "label": 0
                },
                {
                    "sent": "How you define graph of expressions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Expand little bit on what I said about the types and then a couple of things that can be confusing.",
                    "label": 0
                },
                {
                    "sent": "The small differences that we have between the tiano syntax and what you can do in Mumbai.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I said all Tenno variables have a type that has to be defined in advance.",
                    "label": 1
                },
                {
                    "sent": "There are different categories of types, most set types of types, so there's a tensor type.",
                    "label": 1
                },
                {
                    "sent": "That's the one we've seen earlier that for N dimensional arrays that are represented that handle NUM PY arrays values, there's the CUDA injury type that does same kind of things, but.",
                    "label": 0
                },
                {
                    "sent": "On device memory for, for CUDA there's a sparse type.",
                    "label": 0
                },
                {
                    "sent": "For sparse matrices that uses Scipy sparse values you also have.",
                    "label": 0
                },
                {
                    "sent": "Types for scalars.",
                    "label": 0
                },
                {
                    "sent": "There's a generic type that can handle any Python object, but that's not widely used.",
                    "label": 0
                },
                {
                    "sent": "There's now there's a typed list type, for instance sequence of tensors of varying dimensions that have been introduced recently.",
                    "label": 0
                },
                {
                    "sent": "So for instance, for tensor and tensor like types, so like the code injury as well.",
                    "label": 0
                },
                {
                    "sent": "The number of dimension is part of the type.",
                    "label": 1
                },
                {
                    "sent": "The data type.",
                    "label": 0
                },
                {
                    "sent": "For instance, in 64 float 32 and so on is also part of the type, and the third thing that's part of the type is the broadcastable pattern, and I'm going to get to that in the next slide.",
                    "label": 0
                },
                {
                    "sent": "But the shape of variables, an memory layouts like is it see continuous contiguous, contiguous, completely strided.",
                    "label": 0
                },
                {
                    "sent": "That's not part of the type, so that means that once you've compiled.",
                    "label": 0
                },
                {
                    "sent": "A graph of when, once you've built a graph with those types, then whatever.",
                    "label": 0
                },
                {
                    "sent": "Memory layout doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You can call the same function with inputs of different shapes from one call to the next one, as long.",
                    "label": 0
                },
                {
                    "sent": "Obviously as those shape work together and are consistent.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The broadcasting.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operation is an implicit replication of a tensor along dimension, but the dimension has to be of length one.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you can define a row that's basically A1 by N matrix or column that's an end by one matrix, and the broadcast of all things.",
                    "label": 0
                },
                {
                    "sent": "The broadcastable pattern will be true for the dimension that is guaranteed to be always one.",
                    "label": 0
                },
                {
                    "sent": "And that enables us to do this broadcasting so that the replication over that.",
                    "label": 0
                },
                {
                    "sent": "That dimension, whereas we cannot do that if we define, say, two matrices, an one happens to be one by one and the other one is 1 by M. If you try to add those together, that would be an error because it's a size mismatch.",
                    "label": 0
                },
                {
                    "sent": "So that's something that we have to know at compile time.",
                    "label": 0
                },
                {
                    "sent": "Now by just checks the size when you call it and so that's one thing that you have to beware if you're used to run by.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other limitations or that we have is that the symbolic graph is purely functional, so we don't have side effects we don't have.",
                    "label": 0
                },
                {
                    "sent": "You cannot specify operation symbolic operations that change.",
                    "label": 0
                },
                {
                    "sent": "A node are valuable in place because the expressions that were defined in terms of the previous value still needs to use that previous value.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is build a new node.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you do so, you have that operation that changes values of acipenser or that set value to acipenser.",
                    "label": 0
                },
                {
                    "sent": "So you cannot simply do that.",
                    "label": 0
                },
                {
                    "sent": "But what you can do.",
                    "label": 0
                },
                {
                    "sent": "Is build these Inc sub tensor and sets of tensor variables that basically say.",
                    "label": 0
                },
                {
                    "sent": "Here is a variable that will have the same value.",
                    "label": 0
                },
                {
                    "sent": "As if I did change the original 1.",
                    "label": 0
                },
                {
                    "sent": "So that's just at the time where we build the symbolic graph.",
                    "label": 0
                },
                {
                    "sent": "Each time you have to build a new variable.",
                    "label": 1
                },
                {
                    "sent": "But then when compiling that expression here, now will try to replace those expressions by inplace operation if possible.",
                    "label": 0
                },
                {
                    "sent": "For instance, if the previous value is not used by any other node further in the graph, or if they already have computed.",
                    "label": 0
                },
                {
                    "sent": "What they needed then they can have say they have you on the original data or work in place on the original data.",
                    "label": 0
                },
                {
                    "sent": "Now there are a couple of exceptions.",
                    "label": 0
                },
                {
                    "sent": "Operations that have side effects.",
                    "label": 0
                },
                {
                    "sent": "There's for instance one up that prints statistiques or values of the value of a variable, so each time it's evaluated, the output of that node will have the same value as the input.",
                    "label": 0
                },
                {
                    "sent": "It's just copying over that value or reusing it, but each time it's evaluated, it will.",
                    "label": 0
                },
                {
                    "sent": "Print something on the CD out.",
                    "label": 0
                },
                {
                    "sent": "There's also an assert of that will throw exceptions if some conditions are not met.",
                    "label": 0
                },
                {
                    "sent": "The thing is that you always have to use the output of these instead of their inputs for the next 4 as input of the next node in the graph, 'cause otherwise it will just not be used.",
                    "label": 0
                },
                {
                    "sent": "If it's not, it's output is not, is not.",
                    "label": 0
                },
                {
                    "sent": "Needed.",
                    "label": 1
                },
                {
                    "sent": "And these two OPS can disrupt some optimizations, because if you want to fuse two consecutive up in the same operation, but there's a print or insert in the middle, then that that will not happen.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Something else that can be a little bit confusing if you're used to Python on them by.",
                    "label": 0
                },
                {
                    "sent": "Is that we cannot really redefine the syntax of the language itself.",
                    "label": 1
                },
                {
                    "sent": "Actually, now we can.",
                    "label": 0
                },
                {
                    "sent": "As as Steven mentioned during his talk on Julia.",
                    "label": 0
                },
                {
                    "sent": "Now Python has some introspection mechanism that can represent the code as an object and we could use that.",
                    "label": 0
                },
                {
                    "sent": "But we're not for the moment and it would.",
                    "label": 1
                },
                {
                    "sent": "It could have like more consequences.",
                    "label": 0
                },
                {
                    "sent": "So basically if you try to evaluate a variable, a symbolic variable with like an IF statement that will always be true.",
                    "label": 0
                },
                {
                    "sent": "It will be evaluated when you build the graph and say OK well is that variable none or false?",
                    "label": 0
                },
                {
                    "sent": "Well no so evaluates to true and the first branch of your if will always be selected.",
                    "label": 1
                },
                {
                    "sent": "Annual only.",
                    "label": 0
                },
                {
                    "sent": "Build a graph that has that.",
                    "label": 0
                },
                {
                    "sent": "You cannot iterate with a for loop over symbolic variable either.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so.",
                    "label": 0
                },
                {
                    "sent": "So yeah, genitive is to use an op, that's called FLS, and that introduced into the graph symbolic switch.",
                    "label": 0
                },
                {
                    "sent": "For loops you can use channel scan, which I will talk a little bit later in the advanced Topics Session section.",
                    "label": 1
                },
                {
                    "sent": "In Python, you cannot redefine the length because it needs to be an int.",
                    "label": 0
                },
                {
                    "sent": "So we can, but you can use like a shape over the dimension 0, and as I mentioned earlier, there's a print up if you want to print the value of a variable each time it's executed and not like print the symbolic variable that you're currently building.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next topic is graph transformations.",
                    "label": 0
                },
                {
                    "sent": "So once you've built your graph, there are.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different things you can do, for instance substitution and cloning.",
                    "label": 1
                },
                {
                    "sent": "So the easiest.",
                    "label": 0
                },
                {
                    "sent": "I mean the 1st way of doing substitution is basically so you define your graph that you can compile the first function on, But then you say, Oh well, I want to do like almost the same thing, but instead of the original input that I've used, I want to use a slightly different version of that input.",
                    "label": 0
                },
                {
                    "sent": "So for instance, I want to do normalization on it over all the values of that vector.",
                    "label": 0
                },
                {
                    "sent": "So what you can do is define a new.",
                    "label": 0
                },
                {
                    "sent": "Input variable.",
                    "label": 0
                },
                {
                    "sent": "And then define the normalization that that you want.",
                    "label": 0
                },
                {
                    "sent": "And then when you call Tina Dot function to build your level, you say, well I want to start from X_instead of X and I want to substitute.",
                    "label": 0
                },
                {
                    "sent": "X and I want to replace my origonal X by that expression.",
                    "label": 0
                },
                {
                    "sent": "So and then it will perform the substitution in the graph and the compiler function.",
                    "label": 0
                },
                {
                    "sent": "Which can then call, and while the the answer is different.",
                    "label": 0
                },
                {
                    "sent": "Obviously.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different way of doing that and I didn't check that did not go over, but that's the same.",
                    "label": 0
                },
                {
                    "sent": "That's the same expression as before is to build to clone with replacement the outputs of.",
                    "label": 0
                },
                {
                    "sent": "Drive that we've already defined.",
                    "label": 0
                },
                {
                    "sent": "So for instance, we have gotten out and I want to say, well, let's build new nodes that will compute the same computation as dot N out, but instead of starting from X.",
                    "label": 0
                },
                {
                    "sent": "Replace X with this expression of X.",
                    "label": 0
                },
                {
                    "sent": "And then you simply call the analog functions starting from X, because that was the X that's there and that computes, say, an.",
                    "label": 0
                },
                {
                    "sent": "If you evaluated, you get the same result as as previously, so those are two things that can be helpful to transform graphs.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now something a little bit more more complex, how we So what the backpropagation algorithm is an how we implement it.",
                    "label": 0
                },
                {
                    "sent": "So Graham introduced that earlier.",
                    "label": 0
                },
                {
                    "sent": "So here is a slightly different explanation or a different take on what's going on.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have a cost function from RN to R that the output has to be.",
                    "label": 1
                },
                {
                    "sent": "A scalar value.",
                    "label": 0
                },
                {
                    "sent": "That's actually a composition of two functions F&G.",
                    "label": 0
                },
                {
                    "sent": "Going from respectively RN to RN RN to RN.",
                    "label": 0
                },
                {
                    "sent": "So the chain rule says that while the partial derivative of X with respect of C with respect to X, so that would be a vector because its derivative of a scalar with respect to a vector is the dot product between those two quantities.",
                    "label": 0
                },
                {
                    "sent": "That one is the.",
                    "label": 0
                },
                {
                    "sent": "The gradient so.",
                    "label": 0
                },
                {
                    "sent": "With respect to.",
                    "label": 0
                },
                {
                    "sent": "The output of G of X and this one is the Jacobian matrix, so this one is a matrix which is the Jacobian matrix of J, so that has all.",
                    "label": 0
                },
                {
                    "sent": "That has all the element wise derivatives of all elements of.",
                    "label": 0
                },
                {
                    "sent": "With respect to all elements of X.",
                    "label": 0
                },
                {
                    "sent": "The thing is that usually we don't want to compute explicitly that whole Jacobian matrix, because can be huge and often it has some particular structure.",
                    "label": 0
                },
                {
                    "sent": "For instance, if if G is in Elementwise operation that computes the same thing for all inputs, and in that case, like would be equal to M, then only the diagonal of the Jacobian matrix.",
                    "label": 0
                },
                {
                    "sent": "Is actually non 0 so you can implement that.",
                    "label": 0
                },
                {
                    "sent": "Implement that product much more efficiently.",
                    "label": 0
                },
                {
                    "sent": "So what we need is a gradient function.",
                    "label": 0
                },
                {
                    "sent": "That goes from a vector.",
                    "label": 0
                },
                {
                    "sent": "And that represents the gradient of the cost with respect to the output.",
                    "label": 0
                },
                {
                    "sent": "And that computes the dot product between that and the Jacobian.",
                    "label": 0
                },
                {
                    "sent": "And The thing is that I said compute, but I should have said returns an expression for that.",
                    "label": 0
                },
                {
                    "sent": "That that computation.",
                    "label": 0
                },
                {
                    "sent": "So Indiana what we do is that each.",
                    "label": 0
                },
                {
                    "sent": "Each operation it atomic operation, which is most of them defines a grad method that does exactly that.",
                    "label": 0
                },
                {
                    "sent": "It takes a symbolic vector or tensor as input and returns a symbolic expression for that dot product.",
                    "label": 0
                },
                {
                    "sent": "And then once we have defined that, we can simply start from the output, the cost of the graph an go backwards in the graph, computing those chains dot products representing those chain dot products and summing the contributions.",
                    "label": 0
                },
                {
                    "sent": "If one output goes to different is an input of different nodes.",
                    "label": 0
                },
                {
                    "sent": "Further an all those kind of things.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do use that?",
                    "label": 0
                },
                {
                    "sent": "Well, it's actually not that complex, so if you take the previous example and define an additional vector, why that would be a target?",
                    "label": 0
                },
                {
                    "sent": "We compute a costs that basically just the squared error between our previous output and why.",
                    "label": 0
                },
                {
                    "sent": "And then you simply call Tiano Dot grad.",
                    "label": 0
                },
                {
                    "sent": "First element is.",
                    "label": 0
                },
                {
                    "sent": "Your cost has to be a scalar, and the 2nd element is a valuable of the graph.",
                    "label": 0
                },
                {
                    "sent": "It can be an input of the graph or an intermediate variable, or it can be a list of such variables.",
                    "label": 0
                },
                {
                    "sent": "It can be more efficient to do it that way because then all the common subexpressions for both gradients are.",
                    "label": 0
                },
                {
                    "sent": "Shared and it just speeds up the optimization a little bit later, so here these DCW and KDB are symbolic expressions that are defined from well XBW&Y.",
                    "label": 1
                },
                {
                    "sent": "And there's no numerical value at that point a you can.",
                    "label": 0
                },
                {
                    "sent": "I mean, you cannot ask.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the value for?",
                    "label": 0
                },
                {
                    "sent": "For that?",
                    "label": 0
                },
                {
                    "sent": "Well, we don't know, because the inputs don't have value at that point.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do you use the gradients?",
                    "label": 1
                },
                {
                    "sent": "Well, you can use them as outputs of function, because there are symbolic variables that depend on the same input variables.",
                    "label": 0
                },
                {
                    "sent": "And you can use them to build new expressions.",
                    "label": 1
                },
                {
                    "sent": "For instance, if you want to do gradient descent, this is just stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "You with a fixed learning rate so you can just compute new expressions for W&B using the gradient.",
                    "label": 0
                },
                {
                    "sent": "An these are things that you can also output from a general function.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the whole graph that goes from all of the inputs are couple more constants and that has the three blue outputs which are the costan.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both partial derivatives like both both equations.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can update the values or two ways of doing that.",
                    "label": 0
                },
                {
                    "sent": "The one that we already know is to say, well, OK, let's compute.",
                    "label": 0
                },
                {
                    "sent": "Let's call that function.",
                    "label": 0
                },
                {
                    "sent": "And get values for the costs.",
                    "label": 0
                },
                {
                    "sent": "The gradient will respect the respect to the value and with respect to be and this will be an Empire race.",
                    "label": 0
                },
                {
                    "sent": "And then you can simply in Mumbai update the non Pi values of those variables.",
                    "label": 0
                },
                {
                    "sent": "So a different different take would be well, rather than do that update in NUM PY.",
                    "label": 0
                },
                {
                    "sent": "Let's use Theano, so let's compute knew values for.",
                    "label": 0
                },
                {
                    "sent": "4 WNB and simply reassign in Python, W Val and Val to point to those new values.",
                    "label": 0
                },
                {
                    "sent": "It's cumbersome because you have to keep track of everything in empire as well an it can be inefficient, especially if the you want to.",
                    "label": 0
                },
                {
                    "sent": "If you specify these values as NUM PY arrays, but you want the competition to be done on GPU, then you'll have unnecessary transfers between CPU and GPU's.",
                    "label": 0
                },
                {
                    "sent": "That's not ideal, So what we can do instead?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Love.",
                    "label": 0
                },
                {
                    "sent": "Is used what we call shared variables so shared variables are a knew knew kind of variables that are symbolic that you can use As for expressions, but they have to be input variables like the vector and matrix that we created.",
                    "label": 1
                },
                {
                    "sent": "They cannot be the result of an operation.",
                    "label": 0
                },
                {
                    "sent": "And those variables have a numerical value associated to them at each point.",
                    "label": 1
                },
                {
                    "sent": "They are not constant, so this value can mutate, can be reassigned and so on and so.",
                    "label": 0
                },
                {
                    "sent": "So why is that useful?",
                    "label": 1
                },
                {
                    "sent": "Because the value persists between calls of the function, but also between all functions that compute expressions that need that value.",
                    "label": 0
                },
                {
                    "sent": "So for instance we would define the MGH functions earlier.",
                    "label": 0
                },
                {
                    "sent": "That start from the same graph, well, a shared valuable would have values that persists across and that are shared by all these by all these functions.",
                    "label": 0
                },
                {
                    "sent": "Additional thing is that you do not have to.",
                    "label": 0
                },
                {
                    "sent": "They are input variables, but you will not have to specify them in the inputs lists when you call channel function, they will be implicit inputs for all the functions that need them.",
                    "label": 0
                },
                {
                    "sent": "So let's go to an example.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of that so here is like just a.",
                    "label": 0
                },
                {
                    "sent": "Re implementation of the earlier example where X&Y are still symbolic, purely symbolic inputs without values, but the parameters of the model WNB will be shared variables and the origin of the initial values will be what we generated for W, Val and Val.",
                    "label": 0
                },
                {
                    "sent": "Then we have to redefine dot and out because this resign.",
                    "label": 0
                },
                {
                    "sent": "Just resigned the value the variable names in Python that one does not.",
                    "label": 0
                },
                {
                    "sent": "Look, once it's built, it does not look forward to variable that's currently named.",
                    "label": 0
                },
                {
                    "sent": "Be an that's currently name W, so we have to rebuild dot and out.",
                    "label": 0
                },
                {
                    "sent": "Actually that would have been a good way of using the clone mechanism.",
                    "label": 0
                },
                {
                    "sent": "And replacing the previous W by the new W and the previous B by the newbie.",
                    "label": 0
                },
                {
                    "sent": "But so right?",
                    "label": 0
                },
                {
                    "sent": "So then we redefine FNG to take only X as inputs, because here W an here WNB are implicit outputs an if you call the functions well, passing only XXL, then you get the same results as as previously.",
                    "label": 0
                },
                {
                    "sent": "So that's the first advantage of using shared variable is that you don't have to always pass them to the functions.",
                    "label": 0
                },
                {
                    "sent": "You can, once they are built, you can change when you can access and change the value currently stored in them from Python by these two methods.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the second thing shared variables are useful for.",
                    "label": 1
                },
                {
                    "sent": "Is updating.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And that's that's where we started.",
                    "label": 0
                },
                {
                    "sent": "So here I redefined the costs and the gradients.",
                    "label": 0
                },
                {
                    "sent": "And the symbolic expression for the updated value of WNB.",
                    "label": 0
                },
                {
                    "sent": "And here I'm going to build a different channel function that will take inputs as previously, so, X&Y.",
                    "label": 0
                },
                {
                    "sent": "That has only C as output.",
                    "label": 0
                },
                {
                    "sent": "An that specifies update rules.",
                    "label": 0
                },
                {
                    "sent": "Meaning I want to replace the value of W by whatever value will be computed.",
                    "label": 0
                },
                {
                    "sent": "Mean will be associated for the W symbolic variable.",
                    "label": 0
                },
                {
                    "sent": "And the same thing for be.",
                    "label": 0
                },
                {
                    "sent": "So as As for earlier W&B are implicit inputs.",
                    "label": 1
                },
                {
                    "sent": "But and update W an update B will be implicit outputs that will be computed when the channel function will be called.",
                    "label": 1
                },
                {
                    "sent": "And then after these values are computed, the updates will be performed.",
                    "label": 0
                },
                {
                    "sent": "So changing the value that's inside W an inside B to take the value of the that has been computed for these implicit outputs.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is yeah, if I if I plot.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's great degree for that.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function here's what we have, and it's almost the same.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same one as that one.",
                    "label": 0
                },
                {
                    "sent": "I think the only.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difference is that now, here's there's a jar method.",
                    "label": 0
                },
                {
                    "sent": "So that's the rank 1 update for the weights and now it's performed in place because the fact that it's a shared variable allows it to to update its value in place because it's not explicit inputs that has to be that has to be preserved.",
                    "label": 0
                },
                {
                    "sent": "So right so now you see.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seen a.",
                    "label": 0
                },
                {
                    "sent": "Complete example of like how to define an expression gradient update rules and call them.",
                    "label": 0
                },
                {
                    "sent": "So the next step is OK, I can do that, but how can I do that fast enough so that I can train my model before my next deadline?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have, as I said earlier, graph optimizations and you've seen the effect on the graph using the debug print statement earlier.",
                    "label": 0
                },
                {
                    "sent": "So what optimizations are is basically rules for replacing a part of the graph with the different parts an.",
                    "label": 1
                },
                {
                    "sent": "As for when you do the computation, the substitution by hand using clone or using the given keyword of function, while the types have to match.",
                    "label": 0
                },
                {
                    "sent": "So we are.",
                    "label": 0
                },
                {
                    "sent": "I mean there are really different categories of optimizations.",
                    "label": 0
                },
                {
                    "sent": "Some are there to avoid duplicating work.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you have the same expression computed twice at different places in your graph, they they will be merged.",
                    "label": 0
                },
                {
                    "sent": "We have simplification of expressions, so for instance X over X, X -- X, U transpose matrix, and then you transpose it back.",
                    "label": 0
                },
                {
                    "sent": "If you do like 3 copies, three successive.",
                    "label": 0
                },
                {
                    "sent": "If you do 3 copies and then you never do anything with the intermediate values, then you can do only one.",
                    "label": 0
                },
                {
                    "sent": "All those kind of things we have also to help with that we also have a few optimizations that reorder the graph.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you do an elementwise operation on the matrix and then transpose it, it's the same as if you transpose it first and then do the elementwise operation.",
                    "label": 0
                },
                {
                    "sent": "That enables us, for instance, to have all the transposition one all one after the other, and merge them into only one operation, or remove it completely if it does not do anything.",
                    "label": 0
                },
                {
                    "sent": "So try to avoid duplicating work and so that's something.",
                    "label": 1
                },
                {
                    "sent": "So that's when first kind of optimizations.",
                    "label": 0
                },
                {
                    "sent": "We have optimizations for numerical stability.",
                    "label": 0
                },
                {
                    "sent": "As I said earlier, we want to simplify like log of X.",
                    "label": 0
                },
                {
                    "sent": "Then you we want to simplify log of sigmoid into soft.",
                    "label": 0
                },
                {
                    "sent": "Plus we want to simplify those kind of things, especially because in some cases we are going to want to work in single precision Floating Points because that's really much faster.",
                    "label": 0
                },
                {
                    "sent": "On GPUs it's not that true now, but it was definitely true a couple years ago.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The sum GPU didn't even support double precision float and some supported it, but it was like 5050 times slower than single precision.",
                    "label": 0
                },
                {
                    "sent": "So in those cases we really want things to be as stable as possible.",
                    "label": 0
                },
                {
                    "sent": "We have optimizations that insert in place operations instead of operations that do a copy or allocation for their outputs, so we have ways of.",
                    "label": 0
                },
                {
                    "sent": "Of going through the.",
                    "label": 0
                },
                {
                    "sent": "The computational graph and try to see OK. Is that node still needed?",
                    "label": 0
                },
                {
                    "sent": "Can we overwrite its value?",
                    "label": 0
                },
                {
                    "sent": "Is it an input that we should not that we should not destroy?",
                    "label": 0
                },
                {
                    "sent": "Is it an output that we should try to copy because we don't want if the user still has it around?",
                    "label": 0
                },
                {
                    "sent": "We don't want that value to be overwritten next time we call the function.",
                    "label": 0
                },
                {
                    "sent": "Those kind of things.",
                    "label": 0
                },
                {
                    "sent": "We had and then four.",
                    "label": 0
                },
                {
                    "sent": "Purely conditional efficiency, we have bindings to high performance library like Glass.",
                    "label": 0
                },
                {
                    "sent": "We use some high performance library on on GPU using welcome glass.",
                    "label": 0
                },
                {
                    "sent": "For instance quick 50 and those kind of things we do elementwise look Fusion.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if you have a bunch of elementwise operation that you apply on the same array.",
                    "label": 0
                },
                {
                    "sent": "Then you say I don't know.",
                    "label": 0
                },
                {
                    "sent": "You add a few other constant and then take the log and then take the sigmoid and then take something you don't want to loop over your whole array once for each operation all the time.",
                    "label": 0
                },
                {
                    "sent": "So we want to do only one loop where so we have only one memory access instead of many.",
                    "label": 0
                },
                {
                    "sent": "We do shape inference to try to specialize some operations and to try to detect if there are some things that we can simplify.",
                    "label": 0
                },
                {
                    "sent": "We do constant folding.",
                    "label": 1
                },
                {
                    "sent": "So he remember Graham Taylor's example of the gradient of X squared we had in the original and optimized graph.",
                    "label": 0
                },
                {
                    "sent": "You had things like constant of two minus constant of one, while all those kind of things get computed only once and then you see that if you multiply by one then you can remove that and so these are all things that optimizations do.",
                    "label": 0
                },
                {
                    "sent": "And the same optimization framework is also used to transfer things to GPU.",
                    "label": 0
                },
                {
                    "sent": "So what we actually have is that the competition done on GPU are usually different operations, different kind of nodes and the variables do not have the same type because they are could endure attack instead of tensor type.",
                    "label": 0
                },
                {
                    "sent": "And we use the.",
                    "label": 0
                },
                {
                    "sent": "The existing optimization mechanism to say OK. Well, if for instance I have one something that's on GPU.",
                    "label": 0
                },
                {
                    "sent": "And that has been transferred to CPU.",
                    "label": 0
                },
                {
                    "sent": "And that's as input to.",
                    "label": 0
                },
                {
                    "sent": "Operation that we have implemented on GPU.",
                    "label": 0
                },
                {
                    "sent": "Then we will transfer it back to GPU, apply the GPU operation and then insert another transfer so that when you do the substitution the types match.",
                    "label": 0
                },
                {
                    "sent": "And then later the optimization that removes useless operations will replace like transfer from GPU to CPU and to CPU back to GPU with nothing.",
                    "label": 0
                },
                {
                    "sent": "So basically, that's that's that's the way it happens.",
                    "label": 0
                },
                {
                    "sent": "It starts from like all inputs that are on GPU and try to move like the other inputs of the same operation also to GPU and the outputs, and to have only.",
                    "label": 0
                },
                {
                    "sent": "If possible, no transfers at all or just the transfer.",
                    "label": 0
                },
                {
                    "sent": "Maybe at the beginning and at the end if you want the costs.",
                    "label": 0
                },
                {
                    "sent": "Or only at the end if possible.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Graduations as a user, you want to sometimes want to have some control over which optimizations are applied and when, because this graph optimization process can be long an in fact it scales super linearly with the number of nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "So if you have a large graph, specially if you do a lot of small computations or.",
                    "label": 0
                },
                {
                    "sent": "If you have loops that you've manually unrolled, then the competition times can be can be long.",
                    "label": 0
                },
                {
                    "sent": "If you know if it's your final model and you know that's exactly what you want to do, then it's not that bad, because you can run experiments.",
                    "label": 0
                },
                {
                    "sent": "For me, we usually have experiments that could run for a couple of days on GPU's.",
                    "label": 0
                },
                {
                    "sent": "If it takes an additional 2 minutes to compute the graph at the beginning, well, it's not a big deal, but when you're prototyping and if you want to.",
                    "label": 0
                },
                {
                    "sent": "Check that everything works OK and that you have all the necessary operations and that equations are correct and and all that.",
                    "label": 0
                },
                {
                    "sent": "Then sometimes it can be bothersome.",
                    "label": 1
                },
                {
                    "sent": "So we have a predefined different fine mode that's called fast compile that tries to minimize that compilation overhead but uses usually Python code instead of C code, because that's also something that we don't want to compile every time, and so yeah, so summations.",
                    "label": 0
                },
                {
                    "sent": "Multiplied, but sometimes can be worth it.",
                    "label": 0
                },
                {
                    "sent": "There's a different mode, like a different setting, that you can use that's called debug mode an it's another instance of the checking tools that we have, and basically what this does is it executes.",
                    "label": 0
                },
                {
                    "sent": "The graph, and I mean each operation before and after each optimization and checks that the values are close.",
                    "label": 0
                },
                {
                    "sent": "So for stability optimizations, well, it might not always be the case, because that's the point you don't want the wrong value.",
                    "label": 0
                },
                {
                    "sent": "You want the right one, but for most optimizations it's it's really useful, but it's extremely slow because it will also check the Python code versus the C code.",
                    "label": 0
                },
                {
                    "sent": "It will check a lot of things.",
                    "label": 0
                },
                {
                    "sent": "An then there's also a mechanism to enable and disable or particular optimizations or set of optimizations you want.",
                    "label": 1
                },
                {
                    "sent": "Sometimes you want to be able to use the GPU, but not transfer everything to GPU, but have more manual control over that, so you could disable the GPU optimizations and all that can be done either globally for the whole process or at the level of each.",
                    "label": 0
                },
                {
                    "sent": "The handle function.",
                    "label": 0
                },
                {
                    "sent": "So when you call tiano function inputs, outputs, updates, Givens and there's also parameter that's called mode and you can where you can specify OK which which optimizations I want to include or not.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so I was talking a little bit of about C code earlier, so basically each operation that we define in piano, that's a Python object that will represent, for instance, I say the addition reduction over that set of dimension transposition.",
                    "label": 0
                },
                {
                    "sent": "All those kind of operations.",
                    "label": 0
                },
                {
                    "sent": "You can define a method in Python that will take the NUM PY arrays as inputs and.",
                    "label": 0
                },
                {
                    "sent": "Compute the output, set them at the right place where the rest of the runtime can find them, and then.",
                    "label": 0
                },
                {
                    "sent": "Then it will be.",
                    "label": 0
                },
                {
                    "sent": "Used by by the general function callable.",
                    "label": 0
                },
                {
                    "sent": "There's also a different way, that is to specify C code.",
                    "label": 0
                },
                {
                    "sent": "So how does that work?",
                    "label": 0
                },
                {
                    "sent": "There's a method called C code that takes as input variable names that will represent the intermediate computations in C. And it outputs.",
                    "label": 0
                },
                {
                    "sent": "A string that contains the code for a Python module implemented in C. So we can do that by just having a large template and substituting the variable names.",
                    "label": 1
                },
                {
                    "sent": "It can be more than that.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you want to implement reduction over a variable number of axes, then you have like different for loops, a different number of implicated for loops and those kind of things so that you can do that simply by having for loops in Python And stitching steam strings together.",
                    "label": 0
                },
                {
                    "sent": "And kind of things and.",
                    "label": 0
                },
                {
                    "sent": "Once and there's also like boilerplate code that systematically added to check that the inputs have the right type that the dimensions match that extract them from the right place in memory, and set them, and so on.",
                    "label": 0
                },
                {
                    "sent": "An once you have the code for that whole module, it gets written to disk in C++ or CU file.",
                    "label": 0
                },
                {
                    "sent": "It's compiled by G+ Plus and then imported back in Python in temporary temporary module.",
                    "label": 0
                },
                {
                    "sent": "So then that module defines a function that will like the perform method read.",
                    "label": 0
                },
                {
                    "sent": "Python read input as Python object.",
                    "label": 0
                },
                {
                    "sent": "From memory, do the computation and store the outputs at the right place.",
                    "label": 0
                },
                {
                    "sent": "Those generated C files and compiled so or DLL's.",
                    "label": 0
                },
                {
                    "sent": "Will be cashed from one run to the next one, so there's a persistent cache on the hard drive.",
                    "label": 0
                },
                {
                    "sent": "Do not set that on the network drive.",
                    "label": 0
                },
                {
                    "sent": "That's a bad idea.",
                    "label": 0
                },
                {
                    "sent": "Have it's local in TMP or something and it has limited durations that if some.",
                    "label": 0
                },
                {
                    "sent": "Some code is not used anymore than it will be clean.",
                    "label": 0
                },
                {
                    "sent": "It will be cleaned up regularly and does so in order to use that you have to define versions for your C code and when you change it then you bump the version number and so the cash can be managed than this.",
                    "label": 0
                },
                {
                    "sent": "I'm just wondering if you can actually cache a compiled piano graph.",
                    "label": 0
                },
                {
                    "sent": "Not yet.",
                    "label": 0
                },
                {
                    "sent": "We have a couple of things coming that way.",
                    "label": 0
                },
                {
                    "sent": "There was also an experimental project where you could have a whole piano function where instead of having different modules for each operation, you have only one module that compile the whole.",
                    "label": 0
                },
                {
                    "sent": "So from the from the input of the function to the output.",
                    "label": 0
                },
                {
                    "sent": "And that you could save and reuse The thing is that if we try to cash that we have an exploding number of.",
                    "label": 0
                },
                {
                    "sent": "If we did that all the time, then.",
                    "label": 0
                },
                {
                    "sent": "The number of saved objects would be really huge and the overhead of going through them when you want to see if it has already been done could be larger than just re compiling it again because there are just too many modules to load and check the version and so on.",
                    "label": 1
                },
                {
                    "sent": "So yeah, so we have this cache of compiled C module that will try to reuse if possible and for GPU code.",
                    "label": 0
                },
                {
                    "sent": "Actually it's exactly the same process.",
                    "label": 0
                },
                {
                    "sent": "Except that instead of reading non Pi injuries using NUM Py C API.",
                    "label": 0
                },
                {
                    "sent": "Well, we'll read could endure Azor GPU arrays for the new GPU back end that's coming on and right into them.",
                    "label": 0
                },
                {
                    "sent": "And we will compile that using MVC instead of G+ plus and.",
                    "label": 0
                },
                {
                    "sent": "But that's that's really the same.",
                    "label": 0
                },
                {
                    "sent": "The same mechanism.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then when we have these Python modules that do the actual computation, even if done is it's C code, while what the Python module interface exposes is a Python function.",
                    "label": 0
                },
                {
                    "sent": "So we have what we call runtime environment or virtual machine that's responsible for executing the computation for 18 to function.",
                    "label": 1
                },
                {
                    "sent": "So, So what that does is keep track of which operations there are to execute an which depend on which ones and then called what we call the thank all the functions that will.",
                    "label": 0
                },
                {
                    "sent": "Perform the actual computation and set the result at the appropriate place.",
                    "label": 0
                },
                {
                    "sent": "So we have different mentation of those, but what we realized is that even if the underlying implementation in C, the fact that if from Python we call the Python wrapper around that each time, sometimes it can be quite slow.",
                    "label": 1
                },
                {
                    "sent": "It can take like there's a context switch between C and Python that can take in the orders of maybe, I think 50 NS or something like that.",
                    "label": 0
                },
                {
                    "sent": "An when the underlying composition is really small thing, like take the length of an array or take the shape along that dimension, then that can take like 100 times.",
                    "label": 0
                },
                {
                    "sent": "Less this time.",
                    "label": 0
                },
                {
                    "sent": "So for all those cases, what we did is that it was actually James Box right that working mainly on that is that there is also a runtime that's written completely in C. And that will call the C pointer functions that defined by the the C modules.",
                    "label": 0
                },
                {
                    "sent": "So you have before hand to set the addresses for all the intermediate computation values, you have to specify the ordering of the nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "And you have to set all these function pointers.",
                    "label": 0
                },
                {
                    "sent": "If if a couple of these elementary operations do not have implementation, then it's OK. We can also call the C the Python implementation from C which is.",
                    "label": 0
                },
                {
                    "sent": "Again, a little bit slow because you have that overhead of context switching, but it's not that bad and at least it makes it possible to do that, whereas generating all the C code in only one module and compiling that will fail if even one operation do not have this implementation.",
                    "label": 0
                },
                {
                    "sent": "An when you have set up that whole structure, then you only have to do 1.",
                    "label": 0
                },
                {
                    "sent": "Call to see function from Python And it will run the whole thing from inputs to outputs.",
                    "label": 1
                },
                {
                    "sent": "And if you have a function that do not have any explicit input and outputs, for instance, all the inputs are implicit shared variables an all outputs are the rules for the shared variables, then you can even.",
                    "label": 0
                },
                {
                    "sent": "Lounge more than one call to the function at one so you can say execute the function end times and do not return to Python until you're completely finished.",
                    "label": 0
                },
                {
                    "sent": "Anne, this can be quite a speedup if if you have lots of small computations and if your graph cannot be reduced to like a couple of gem calls that takes 90% of runtime.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, and there's also lazy evaluation that's implemented in those virtual machines, so if you have an if else node, it will not try to compute all three inputs like the condition, the, then branch and the else branch.",
                    "label": 0
                },
                {
                    "sent": "It will actually ask for the node.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do you need then?",
                    "label": 0
                },
                {
                    "sent": "Would say, well, I need the value of the condition to compute whatever is needed to compute the value of the condition and then.",
                    "label": 0
                },
                {
                    "sent": "Time will say, OK, here's your here's the value for your condition.",
                    "label": 0
                },
                {
                    "sent": "Now what you need?",
                    "label": 0
                },
                {
                    "sent": "Oh well, the condition was a true I need the first expression.",
                    "label": 0
                },
                {
                    "sent": "OK so the CPM will all the other VM will compute the value for that and say, OK, well now your condition is that and your then branch.",
                    "label": 0
                },
                {
                    "sent": "Is that what you need?",
                    "label": 0
                },
                {
                    "sent": "If I say, OK, well, I have everything I need.",
                    "label": 0
                },
                {
                    "sent": "My output is that and.",
                    "label": 0
                },
                {
                    "sent": "So that was something that was in that had been missing for sometimes until James added that maybe three years ago.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how to use the GPU?",
                    "label": 1
                },
                {
                    "sent": "So we want to make it as easy as possible to switch the GPU on and off.",
                    "label": 1
                },
                {
                    "sent": "But The thing is that there are couple limitations currently.",
                    "label": 0
                },
                {
                    "sent": "The first one is that.",
                    "label": 0
                },
                {
                    "sent": "Since GPU's where I mean did not support float 64 at some point we the library that we use for arrays currently only supports float 32, so no integers, no complex of course and no double precision, only float 32.",
                    "label": 0
                },
                {
                    "sent": "And these objects, these arrays that reside on GPU.",
                    "label": 0
                },
                {
                    "sent": "We did not add a nice interface to interact with them from Python.",
                    "label": 0
                },
                {
                    "sent": "So for instance if you try to print them and like NUM py, injuries will not see that content will just say, well, that's an could injury objects at address whatever.",
                    "label": 0
                },
                {
                    "sent": "So we did not implement all the Python bindings for like say, set setting values and getting slices and so on, so that can be a pain if you want to have them as explicit inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that if a GPU is selected and I'll talk a little bit later about how you do that.",
                    "label": 0
                },
                {
                    "sent": "What happens is that we decide.",
                    "label": 0
                },
                {
                    "sent": "That all shared variables that are created and that have the flow 32D type will be transferred to GPU as soon as they are created.",
                    "label": 0
                },
                {
                    "sent": "And then it gives a starting point from the optimization process that I mentioned earlier, where if you have some.",
                    "label": 0
                },
                {
                    "sent": "If you have an operation that has some inputs on GPU, then we move the operation as well as all the other inputs to the GPU as well.",
                    "label": 0
                },
                {
                    "sent": "And that's that's the starting point.",
                    "label": 0
                },
                {
                    "sent": "And then you you iterate through that from inputs to outputs.",
                    "label": 0
                },
                {
                    "sent": "So that's the way we get that started.",
                    "label": 0
                },
                {
                    "sent": "And obviously if no GPU is selected, then those variables will stay on CPU and you won't see any difference.",
                    "label": 0
                },
                {
                    "sent": "So you want to be sure to use float 32 and there's an option to easily switch between float 32 and float 64 in Theano, which is the the type called float X.",
                    "label": 0
                },
                {
                    "sent": "So by default it's aliased to float 64, but you can change that.",
                    "label": 1
                },
                {
                    "sent": "With the config flag and that's the default type of tensors.",
                    "label": 0
                },
                {
                    "sent": "So earlier when I created input variables with like T dot vector or matrix, by default if you don't specify data type then it will be float X so that you are 64.",
                    "label": 0
                },
                {
                    "sent": "Depending on the current setting.",
                    "label": 1
                },
                {
                    "sent": "But if you want to have explicit double precision or single precision regardless of that, then you can always like use.",
                    "label": 0
                },
                {
                    "sent": "F matrix for floating point D matrix for double precision.",
                    "label": 0
                },
                {
                    "sent": "Or specify explicitly what the data type should be.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was talking about configuration flags, so there are different ways of setting them.",
                    "label": 0
                },
                {
                    "sent": "So those are three main ways are to use the environment variable.",
                    "label": 0
                },
                {
                    "sent": "Let's call Tiano flags and you give pairs of valuable.",
                    "label": 0
                },
                {
                    "sent": "Flag names and values and that's going to be interpreted just from the start.",
                    "label": 0
                },
                {
                    "sent": "Another way is from from Python.",
                    "label": 0
                },
                {
                    "sent": "So once once the annoys imported you can change the configuration value for some flags, but not all.",
                    "label": 0
                },
                {
                    "sent": "For instance the path to the.",
                    "label": 0
                },
                {
                    "sent": "Cache of compiled modules, while once you've imported 10 oh, it's already been loaded.",
                    "label": 0
                },
                {
                    "sent": "It's already been configured.",
                    "label": 0
                },
                {
                    "sent": "You cannot change that later, so it's not does not work for some for some flags, but if you try to set them, you'll get a meaningful error message.",
                    "label": 0
                },
                {
                    "sent": "And the last way is with a configuration file, so we have first order first level flags that are in the global.",
                    "label": 0
                },
                {
                    "sent": "Section you could also like subsections to specify more specific things.",
                    "label": 0
                },
                {
                    "sent": "So if you want to use GPU, basically the two flags that you have to set or device which which you should set to GPU which means take the first available GPU or however could decide to get one or a specific one with GPU and index the new back end.",
                    "label": 0
                },
                {
                    "sent": "That's I'll talk a little bit later, supports.",
                    "label": 0
                },
                {
                    "sent": "No other syntax, so it's you say like say, CUDA zero or open CL one or.",
                    "label": 0
                },
                {
                    "sent": "And you usually want to set float X2 float 32.",
                    "label": 0
                },
                {
                    "sent": "Unless you've already been working with like F vectors and F matrixes from the salt.",
                    "label": 0
                },
                {
                    "sent": "Ann once, once you do that, then your program will.",
                    "label": 0
                },
                {
                    "sent": "I mean will run on GPU if it's available and if every operation is implemented on GPU.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's time to have OK, so let's talk about with about a couple of more advanced topics.",
                    "label": 0
                },
                {
                    "sent": "Not going to go into too much detail in there, but just to give you a sense of what more is possible.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We cannot, so as I said earlier, if you use a for loop when you build your graph, it will iteratively build new variables.",
                    "label": 0
                },
                {
                    "sent": "So it's equivalent of completely unrolling that loop.",
                    "label": 0
                },
                {
                    "sent": "If you don't want to do that, because.",
                    "label": 0
                },
                {
                    "sent": "Lots of different reasons there is an operation that can specify those loops completely symbolically.",
                    "label": 0
                },
                {
                    "sent": "Which is called scan.",
                    "label": 0
                },
                {
                    "sent": "And so it can perform like different.",
                    "label": 1
                },
                {
                    "sent": "Different kinds of looping, so things like map reduce accumulations it can.",
                    "label": 0
                },
                {
                    "sent": "It can look at inputs at, say, the previous time step, or more time steps before can also use as previous outputs an the number of iterations does not have to be constant.",
                    "label": 0
                },
                {
                    "sent": "It could either be a symbolic variable that you evaluate.",
                    "label": 1
                },
                {
                    "sent": "At that point in the graph and you say OK, have to do any loops and from the beginning of the execution the scan node will know how many loops it has.",
                    "label": 0
                },
                {
                    "sent": "Or it could be a stopping condition that will be computed and evaluated at each step.",
                    "label": 0
                },
                {
                    "sent": "And then if it returns true, then the.",
                    "label": 0
                },
                {
                    "sent": "Decorating stops there.",
                    "label": 0
                },
                {
                    "sent": "You still have to have a maximum number of epochs, so it's not completely so it's not completely as a while loop, it's more like break statements in the for loop.",
                    "label": 0
                },
                {
                    "sent": "And it has to be executed at least once.",
                    "label": 0
                },
                {
                    "sent": "I mean, the whole group has to be executed.",
                    "label": 0
                },
                {
                    "sent": "That's added at least once on the way it works.",
                    "label": 0
                },
                {
                    "sent": "Is that inside that scan operation?",
                    "label": 0
                },
                {
                    "sent": "You actually define a general function.",
                    "label": 0
                },
                {
                    "sent": "It will be compiled into at the end of function that will be executed at each step.",
                    "label": 0
                },
                {
                    "sent": "So let's say if you want to.",
                    "label": 0
                },
                {
                    "sent": "If my inputs are, say, one input at times T -- 1 and minus 20 -- 3, this would be like 3 different symbolic inputs.",
                    "label": 0
                },
                {
                    "sent": "At each time step.",
                    "label": 0
                },
                {
                    "sent": "And so the scan node dispatches all these slices of inputs and outputs to that function and call that in the loop and stop the loop if the stopping condition becomes true.",
                    "label": 0
                },
                {
                    "sent": "So also if.",
                    "label": 0
                },
                {
                    "sent": "So the scan node can also be moved to GPU.",
                    "label": 1
                },
                {
                    "sent": "And in that case it will try to to move also the inner function to GPU and have everything run on GPU.",
                    "label": 0
                },
                {
                    "sent": "The overhead of that is still quite high, so if you do like large recurrent neural Nets then it's not going to be large percentage of your runtime.",
                    "label": 0
                },
                {
                    "sent": "But if you want to work on like small matrices or scalars then GPU is not using the GPU with Ken is is not going to be worth it.",
                    "label": 1
                },
                {
                    "sent": "And the grid into the grid method of that scan operation implements backpropagation through time.",
                    "label": 0
                },
                {
                    "sent": "So you can specify, so you have say gradients with respect to the output at each time step at the last time step, and then it will do like all the backpropagation and use actually a different scan operation that goes backwards and accumulates the right kind of gradients to the right kind of updates and return that.",
                    "label": 0
                },
                {
                    "sent": "You can also, if you don't want the whole backpropagation through time, you can also.",
                    "label": 0
                },
                {
                    "sent": "Kept it to a couple of epochs.",
                    "label": 0
                },
                {
                    "sent": "So that's in that's the end of that scan.",
                    "label": 0
                },
                {
                    "sent": "It's not easy to use because you have a lot of different things that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you can specify.",
                    "label": 0
                },
                {
                    "sent": "You can have to specify initial states, sometimes initial states for different for more than one time step.",
                    "label": 0
                },
                {
                    "sent": "If you go, if you're looking back to more than one epoch.",
                    "label": 0
                },
                {
                    "sent": "You won't be able to specify updates that are performed at each iterations for shared variables.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you use pseudorandom number generations, then you do not want to draw the same samples at each time step usually.",
                    "label": 0
                },
                {
                    "sent": "You want to update the state of the pseudorandom number generator so that you can run you samples each time, so the syntax is a little bit complex.",
                    "label": 1
                },
                {
                    "sent": "Here's one simple example that's just an accumulation and basically what you do is define a function.",
                    "label": 0
                },
                {
                    "sent": "That takes different arguments, so depending on what you want to do, this will be so here.",
                    "label": 0
                },
                {
                    "sent": "This is like the output at step T -- 1.",
                    "label": 0
                },
                {
                    "sent": "And this is a non sequence input, so that's an input that will stay the same and that will be used at each time step.",
                    "label": 0
                },
                {
                    "sent": "So that's not something that changes, but all that we iterate through.",
                    "label": 1
                },
                {
                    "sent": "But just consider the constant for the evaluation of that and you specify and then here new results is like the Elementwise product of the prior result and apps.",
                    "label": 0
                },
                {
                    "sent": "You specify that initially you start with once the same shape as a.",
                    "label": 0
                },
                {
                    "sent": "A is a non sequence so that it will be fed as is for each time step.",
                    "label": 0
                },
                {
                    "sent": "You could also specify a sequences and if it's a sequence that means.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you specify your matrix as a sequence, then each step will see a different role of that matrix and so.",
                    "label": 0
                },
                {
                    "sent": "And how many steps you want?",
                    "label": 0
                },
                {
                    "sent": "Sometime yeah here we only care about the last result, so that enables some optimizations that will for get the intermediate results as soon as they don't.",
                    "label": 1
                },
                {
                    "sent": "They're not useful, and here compiled and functions.",
                    "label": 0
                },
                {
                    "sent": "You specify the updates that would be specified by the scan function here.",
                    "label": 0
                },
                {
                    "sent": "It's probably nothing that if you work with random numbers then you also have.",
                    "label": 0
                },
                {
                    "sent": "To do that kind of updates.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's not from step to step that you draw the same samples that it's from call to the channel function to the next one.",
                    "label": 0
                },
                {
                    "sent": "So like say for the first mini batch will have some sequence of random numbers, and if you don't update your random number generator at the end, then that means that for the second mini batch you'll get like exactly the same numbers run in the same order, which is usually something that you do not want.",
                    "label": 0
                },
                {
                    "sent": "And so here's here's a smaller.",
                    "label": 0
                },
                {
                    "sent": "The example of a very.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to talk about extending Theano quickly, so.",
                    "label": 0
                },
                {
                    "sent": "No two ways of adding new operations.",
                    "label": 0
                },
                {
                    "sent": "The easiest way is if you have a Python function that do what you want, then you can simply define a new operation an have the perform method that calls that, or if you do not need a gradient, there's even like a simple operator that's called as up where you can specify like the input.",
                    "label": 0
                },
                {
                    "sent": "Types the output types an optionally infer shape function that will try to compute symbolically what the shapes of the output is given the shape of the inputs, and that's useful for the shape inference mechanism that I was mentioning earlier.",
                    "label": 0
                },
                {
                    "sent": "There's no way of specifying ingredients using that decorator yet, but you can also simply inherit from up an defined performan grad method, and that's not much more complex.",
                    "label": 0
                },
                {
                    "sent": "So that's the way.",
                    "label": 0
                },
                {
                    "sent": "For instance, that recently someone implemented 3D convolution on GPU using Fast Fourier transform because just by calling bindings, Python bindings to existing software that computed that.",
                    "label": 0
                },
                {
                    "sent": "Since it's a large operation and then the overhead of doing that in Python is not that high and it's a perfectly perfectly.",
                    "label": 0
                },
                {
                    "sent": "Good way of extending channel for your needs.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The harder way is using C code, so, but then you'd have to be a little bit familiar with the API of of Mumbai an how to access their pointers to arrays and so on.",
                    "label": 0
                },
                {
                    "sent": "Or code ING arrays or GPU arrays.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned when I was talking about the type, those strides and the memory layout is not part of the types, so it can be arbitrary at at runtime.",
                    "label": 0
                },
                {
                    "sent": "So you have to count for that or at least called GPU contiguous operation.",
                    "label": 0
                },
                {
                    "sent": "That makes sure that everything is continuous and that you can use it.",
                    "label": 0
                },
                {
                    "sent": "You have to manage reference counts because we use Python for our memory allocation and deallocation.",
                    "label": 0
                },
                {
                    "sent": "But you don't have the overhead of Python context switching, and if you disable the internal garbage collector then you don't.",
                    "label": 1
                },
                {
                    "sent": "You will not even go back through the to the Python interpreter so it can be really fast, and especially if it's on GPU, it can be a synchronous and you can gain some speed with that and that's the way that external contributors recently wrapped the cafe convolution.",
                    "label": 1
                },
                {
                    "sent": "Into Theano to have fast implementation using German on GPU.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So too.",
                    "label": 0
                },
                {
                    "sent": "As we come to the conclusion what to expect in the next couple of months, what are the things that we are that we have been working on?",
                    "label": 1
                },
                {
                    "sent": "So there's the new GPU back end, which I've mentioned a couple of times that will support all D types for GPU arrays and that will work both for CUDA and open CL.",
                    "label": 0
                },
                {
                    "sent": "That's pretty exciting.",
                    "label": 0
                },
                {
                    "sent": "Now we still have some things to tune and the support for CUDA is still much better than for open CL because we could more easily backport the previous.",
                    "label": 0
                },
                {
                    "sent": "It's the previous operations to CUDA rather than an open CL.",
                    "label": 0
                },
                {
                    "sent": "And this back end will also enable you to use more than one GPU in the same channel function.",
                    "label": 0
                },
                {
                    "sent": "You'd have to be explicit about it, so unlike now where it moves everything it can to the GPU if there's only one.",
                    "label": 0
                },
                {
                    "sent": "If you want to separate things and dispatch them across different GPU, then you'd have to specify explicitly.",
                    "label": 0
                },
                {
                    "sent": "OK, well here's a transfer that I want to do from the host to this CPU to this GPU or from that GPU to these GPU and then back.",
                    "label": 0
                },
                {
                    "sent": "And but then if you have like 2 pounds that are in the same GPU to try to have.",
                    "label": 0
                },
                {
                    "sent": "All the information computation on the same.",
                    "label": 0
                },
                {
                    "sent": "What we have also coming is the execution of unoptimized graph on GPU.",
                    "label": 1
                },
                {
                    "sent": "So as I mentioned earlier there's the fast compile mode but one of the limitation of that is that since moving things to GPU is an optimization then it's not performed by by fast compile.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to have the minimal set of optimizations that are needed to transfer things over to GPU.",
                    "label": 0
                },
                {
                    "sent": "In one mode that people can work with, we have serialization and disorganization of optimized function graph, so this was mentioning earlier we.",
                    "label": 0
                },
                {
                    "sent": "Also have someone working on easier ways of writing C code for OPS, especially being able to have one C++ or the file apart from the Python source and be able to work with that instead of having to mingle with strings in Python so you can have like syntax highlighting using your favorite editor.",
                    "label": 0
                },
                {
                    "sent": "Or you can have all those kind of things.",
                    "label": 0
                },
                {
                    "sent": "It will not be applicable for everything.",
                    "label": 0
                },
                {
                    "sent": "For instance, well if you have a variable number of dimensions and the variable number of implicated for loops, then obviously that's not going to help, but for the easiest cases, especially if you just want to wrap an external library, then it will.",
                    "label": 0
                },
                {
                    "sent": "It would be much easier.",
                    "label": 0
                },
                {
                    "sent": "Something else that's been needed for awhile and that we hope to finish in the next couple of months is the ability to serialize using pickle shared variables that are on GPU so that they are serialized as an employee injury and that if you try to reload a function or even just the shared variable on the machine that does not have a GPU then you're still able to access those values and two.",
                    "label": 0
                },
                {
                    "sent": "For instance, train your model on a GPU because it's fast, but then use it for prediction on the machine that only has CPU's.",
                    "label": 0
                },
                {
                    "sent": "So that's that's a lot.",
                    "label": 1
                },
                {
                    "sent": "That's more complex that we thought, and we hope to be finished with that soon.",
                    "label": 0
                },
                {
                    "sent": "And then there are like a lot of newer versions of faster versions of convolution or cross correlations for CPU and GPU.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned the one using FFT.",
                    "label": 0
                },
                {
                    "sent": "There's a one using cafe that we're still perfecting and then couple of days ago.",
                    "label": 0
                },
                {
                    "sent": "Yeah announced that there will be CUDA CUDA interface for convolutions directly in the Quad SDK.",
                    "label": 0
                },
                {
                    "sent": "So we are planning on having implementation for that.",
                    "label": 0
                },
                {
                    "sent": "Then we probably have a mechanism to enable choosing between all these implementations, because some are sometimes faster and sometimes not there.",
                    "label": 0
                },
                {
                    "sent": "Also, like memory requirements that are not the same.",
                    "label": 0
                },
                {
                    "sent": "So we want users to be able to say OK, this one is small, so I want to use a 50 because it's really fast, but I don't have enough memory to use a 50 on that one.",
                    "label": 0
                },
                {
                    "sent": "So I want to use cafe.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, we completely close to the end, so just wanted to thank all my colleagues and former colleagues at Lisa, especially the tender contributor.",
                    "label": 0
                },
                {
                    "sent": "So here is like really small list of contributors here.",
                    "label": 0
                },
                {
                    "sent": "That's really a small subset of people who did much work on on piano and associated software.",
                    "label": 0
                },
                {
                    "sent": "Then compute Canada and accuracy.",
                    "label": 1
                },
                {
                    "sent": "HP and Sorkin Canada research chairs for all the support either in funding or providing computing resources that helped us develop all those kind of things and take advantage of them.",
                    "label": 1
                },
                {
                    "sent": "And of course the organizers and participants of this conference.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}