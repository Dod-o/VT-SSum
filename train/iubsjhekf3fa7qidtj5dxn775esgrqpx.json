{
    "id": "iubsjhekf3fa7qidtj5dxn775esgrqpx",
    "title": "Learning Prediction Suffix Trees with Winnow",
    "info": {
        "author": [
            "Nikos Karampatziakis, Department of Computer Science, Cornell University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_karampatziakis_lpst/",
    "segmentation": [
        [
            "So yeah, this talk is about 6."
        ],
        [
            "So prediction and sequential prediction means predicting the next item in a sequence using the previous items.",
            "So."
        ],
        [
            "So in this talk I will present an algorithm that learns small and accurate prediction suffix trees, which is a model for doing sequential produce."
        ],
        [
            "And another task we have before I explain things about this algorithm and more formally about the problem, I want to say a few things about our motivational or motivating application.",
            "So we want to know if a program is behaving normally and what that means is we are running the program in a controlled environment and we're making a model of the system, calls the sequences of system calls it.",
            "Execute and then."
        ],
        [
            "We monitor the program outside this controlled environment and see what kind of system calls it actually executes and compare again."
        ],
        [
            "The predicted ones and then we see.",
            "Further we look for deviations which may signify a bug, a security problem, etc."
        ],
        [
            "So what is sequential prediction?",
            "So we have a known alphabet, like the set of nucleotides or minus 1 + 1 that I will give him the examples or the sequence a set of system calls R as in our motivating application."
        ],
        [
            "And then what we do is in its around T we want to predict item YT given the prefix Y one up to Y T -- 1.",
            "I know why I belong to this alphabet that we know."
        ],
        [
            "And prediction suffix trees, which are also known as context trees or variable length Markov models are popular and effective models for this task.",
            "I'm.",
            "So I will."
        ],
        [
            "Describe them through an example and I will be using a version that has been popular by Yoram Singer and his students.",
            "So assume that YT is in the set minus 1 + 1 and in."
        ],
        [
            "This case prediction suffix tree is a binary tree and further."
        ],
        [
            "For every node has label has a value, a real number real number, positive or negative."
        ],
        [
            "And the links on the edges are labeled either minus one or plus one they are labeled with the symbols from the alphabet.",
            "In this talk I will link to the left.",
            "Children will be labeled minus one.",
            "The links to the right children will be labeled plus one.",
            "So to make a prediction for item YT we will fall we will start at the root and follow the path labeled YT minus one writing minus two and so on.",
            "So until the next child in the sequel."
        ],
        [
            "It does not exist.",
            "So what we do is we start at the root with scanner sequence backwards.",
            "And I can relate to these values that we collect."
        ],
        [
            "From this different nodes and YT is the sign of a weighted sum of the values in the visited nodes, and this weighted sum is such that early."
        ],
        [
            "Symbols in the sequence are discounted more than recent ones.",
            "The weights of this weighted sum will be such that, so let me."
        ],
        [
            "Give you an example of how this works.",
            "Suppose that we will be using this type of discounting this exponential discounting when I know that the depth D will be discounted by 1/2 to the D. Then we have this prediction suffix tree.",
            "So if our sequence ends in plus one, then."
        ],
        [
            "We will follow this right link here and we will have 1/2 * -- 2 so we."
        ],
        [
            "Predict minus one in this case.",
            "If the sequence ends in."
        ],
        [
            "Plus 1 -- 1.",
            "Then"
        ],
        [
            "We will follow the minus run first, so we are scanning backwards and then we will see that we cannot."
        ],
        [
            "There is no right child here, so we'll stop there and we."
        ],
        [
            "What we have so far.",
            "So minus one.",
            "And finally, if it's if the sequence is."
        ],
        [
            "Minus 1 -- 1 we just follow the two minus ones and."
        ],
        [
            "Producer the this weighted sum of."
        ],
        [
            "Values.",
            "So now will show the connection between this."
        ],
        [
            "Prediction problem and linear classification.",
            "Every node in the tree is indexed by the suffix that leads to this node.",
            "And if we collect all."
        ],
        [
            "As nodes, all this node values in a big vector at a time T, There are some nodes in the tree and their values can be collected in a vector."
        ],
        [
            "WT that way will be indexing by this strings is and then the decision can be written as a sign of the of this vector WT with another vector XD.",
            "And next is defined as follows.",
            "X + T is defined as follows.",
            "If S is a suffix of the sequence, then we will take the feature value to be better to the length of the suffix, but there will be a number between zero and one that.",
            "So this means that it will be decaying exponentially quickly and this is very common for prediction suffix trees, and if it is not a suffix of the sequence, then.",
            "It will just not affect the decision.",
            "So what we?"
        ],
        [
            "Will do is very interested in learning this W vector.",
            "The values in the tree in the online setting because we expect that the system calls we observe from an executable or not coming from a stationary process.",
            "So we want an online algorithm to keep track of the best tree.",
            "And here is an algorithm for online learn."
        ],
        [
            "And I will explain more about the motivation for choosing this particular algorithm.",
            "Yeah, in a way.",
            "So I have written it in this form so that it looks a lot like perceptron.",
            "We have a vector of parameters Theta.",
            "If we make a mistake we update using the Perceptron update and when we end our weight vector our actual weight vector is exponentiating the parameters Theta.",
            "Um?",
            "And therefore you can see that this weight vector all the values of it are positive.",
            "For this algorithm to be able to make both negative positive and negative predictions, it gives us a special form of these features.",
            "It has a features have a positive part and the negative part and the."
        ],
        [
            "First part are is our original feature vector and the negative part is just.",
            "Minus the the positive part and if we write our our features in this way, we can also split our parameter vector in the same way to have a positive part in the negative part.",
            "And then we can write the inner product between W an X this thing here that this is all we need to be able to evaluate.",
            "The sign of this quantity is all we need to be able to evaluate to run this algorithm to make decisions.",
            "And this can be converted to be proportional to the sum over all positive parameters of the hyperbolic sine of the parameter times the.",
            "Thanks, they their corresponding feature.",
            "So.",
            "Furthermore, I want like the hyperbolic sine is probably a function that you don't see very often.",
            "Iproperty hyperbolic sine is that it has a root at zero.",
            "The only route of hyperbolic sine is at 0.",
            "So when the parameter is zero, it doesn't affect the decision.",
            "Um?"
        ],
        [
            "So what we will do is we will use window to learn good.",
            "We will store this set of values in our tree and we will use Windows Update.",
            "We will make predictions by this previous relation.",
            "So we want to have a small tree and to do that."
        ],
        [
            "This data vector has to be sparse.",
            "Because if it's sparse, then the corresponding node in the tree doesn't have to be stored.",
            "If a parameter for a particular suffix.",
            "If a parameter for a particular suffix is 0, then."
        ],
        [
            "The."
        ],
        [
            "Thanks.",
            "It's your curves.",
            "So yeah, I mean if it's a fix, it has a parameter equal to 0, then the suffix that leads to a node doesn't know doesn't have to be stored in the tree.",
            "So initially we start with a zero vector.",
            "And therefore nothing.",
            "None of the nodes in the tree has to be stored.",
            "We just have the root of the tree, which is like a dummy node, doesn't correspond to any suffix.",
            "And when we make mistakes, we have data parameters and therefore some of these parameters become non zero and we have to.",
            "So allocate nodes in the tree."
        ],
        [
            "Accommodate experiment.",
            "And what is happening is like if we use an algorithm like window or perceptron for keeping track of our hypothesis then this.",
            "This update rule that they use quickly destroy the sparsity of the trade it leads.",
            "They lead to large trees, so here's an illustration of the."
        ],
        [
            "Problem suppose that our features are looking like this.",
            "These are the regular features.",
            "If it sees a suffix, then we have 1/2 to the length of the suffix and 0 otherwise four.",
            "And we have this input sequence.",
            "We want to predict the value of the question mark.",
            "Um?",
            "For this three here on the left."
        ],
        [
            "Our prediction the prediction that it makes is just it follows the minus one here."
        ],
        [
            "And it produced."
        ],
        [
            "Is something positive?",
            "And now suppose that."
        ],
        [
            "So actually we make a mistake that the value of the question was minus one.",
            "What we have to do is we have to make an update and the update says that for every suffix of this sequence will have to change the parameter.",
            "The parameter vector that we implicitly keep with history.",
            "So we have."
        ],
        [
            "Allocate nodes for all these suffixes and we have to make the update according to."
        ],
        [
            "So what happens is that when we."
        ],
        [
            "Make a mistake at time T we have to allocate order T nodes and therefore this leads to a quadratic dependence of the size of the tree with the length of the sequence.",
            "And that is not just hard to learn.",
            "It's also hard to store because we will be dealing with sequences with millions of.",
            "What items so this happens becausw?"
        ],
        [
            "Our features are such that they are non 0 even for very long suffixes and there are two ways to fix that."
        ],
        [
            "Problem.",
            "The first way is like we can change our features to look only on suffixes of small length, like small suffixes.",
            "Of short length."
        ],
        [
            "This is a bad idea because we would like our algorithm to be able to compete against arbitrarily deep and complex trees.",
            "And if we do that, we cannot change our features."
        ],
        [
            "Cannot do that anymore.",
            "Another idea is to have an adaptive bound DT on the depth after which the tree can grow in round T. So you can think of DT some quantity that will be growing slowly if necessary.",
            "Oh, and if we make mistakes, did they will be growing?",
            "And if they, if we if we are not making mistakes that they will stay small.",
            "So."
        ],
        [
            "There is a window for prediction.",
            "Suffix trees is the only changes like the update rule here has is the previous update rule plus this X."
        ],
        [
            "The term here NT and you can think of 20 or something that will prove that Ray it's a noise term that cancels the update for all those suffixes that are linked whose length is greater than DT.",
            "So with this you can think conceptually as growing the full tree, growing full path in the tree and then pruning.",
            "So.",
            "Here is the main effect of the noise in the analysis.",
            "This term PT here."
        ],
        [
            "Is the sum overall rounds up to round T on which mistakes are made of the Infinity norm of the noise vectors, and this is exactly why we picked window because the noise vectors, the effect of the noise comes through the Infinity norm of the of this noise term.",
            "So this is like the smallest.",
            "Norm among them.",
            "Among the norms like they could use so that their analysis gives the effect of the noise bounded and what we do is."
        ],
        [
            "We set the pity.",
            "We maintain an invariant that PT has to be less than the number of mistakes to some power, and you can have many powers here that give a mistake bound.",
            "But we use this one which works well in practice.",
            "So we said it in this way.",
            "This is by induction from this way from this.",
            "From this quantity from this invariant.",
            "And they would do that.",
            "We can get mistake bound that if there is another three that."
        ],
        [
            "So our algorithm could have found then that attains small loss L on the input sequence and this loss is like a modified hinge loss.",
            "Then our mistakes will be at most the number of this quantity here.",
            "At most this many mistakes.",
            "Yeah, it's a Delta this way, but this part does not depend on the loss in the log.",
            "So.",
            "We can also prove that the the trade that will learn will not be too deep.",
            "That means that it will have at most logarithm of the number of mistakes plus 4 levels.",
            "And this is when we said better to this quantity, we change this quantity.",
            "We get another.",
            "We get a number here in front.",
            "Oh"
        ],
        [
            "So I will quickly go over the proof.",
            "We have.",
            "The growth bound is straightforward algebra from the definition of this DT value."
        ],
        [
            "The mistake bound is using a potential function argument."
        ],
        [
            "This is the potential function we're using the relative entropy as usual for for multiple."
        ],
        [
            "Active updates and what we do is we upper bound initial potential and we saw that the decreasing potential with its mistake can be decomposed in the effect of the full update minus the effect of the noise and the effect of the full update is upper bound lower bounded by a function of the learning rate and loss of the optimal vector.",
            "So here is the proof."
        ],
        [
            "Victoria Lee will take an example.",
            "Suppose we."
        ],
        [
            "Mega mistake we make the full update and then buy prune."
        ],
        [
            "The three we introduce."
        ],
        [
            "Some noise here, but takes away some of our progress."
        ],
        [
            "And then."
        ],
        [
            "Another example, the same story happens.",
            "The noise takes away."
        ],
        [
            "Progress by pruning."
        ],
        [
            "If we make correct prediction, there is no."
        ],
        [
            "Oh no progress."
        ],
        [
            "And finally, this is how things are going."
        ],
        [
            "The."
        ],
        [
            "If is that the length of the blue bar is always less than cannot be more than."
        ],
        [
            "The initial potential and the blue bar is just the sum of the green blocks minus the.",
            "Minus the length."
        ],
        [
            "The Brown birds effect of the noise there."
        ],
        [
            "So it was lower bounded in some previous slide by the number of mistakes times the minimum size of the green block, the Brown, the effect of the noise is we have an invariant that says it should be less than the number of mistakes to the 2/3, and then we have this in the."
        ],
        [
            "If we solve this inequality will get our mistake bound."
        ],
        [
            "So quickly, some results.",
            "We had three programs for these sequences of system calls for every program, 120 sequences of."
        ],
        [
            "System calls and we run our window variant and the bounded perception for prediction suffix trees by Decker salvage parts and Singer.",
            "And we here is the average."
        ],
        [
            "The error rate over there for the sequence is number of mistakes divided by the length of the sequence, and you see that we have like."
        ],
        [
            "Fewer mistakes, and Furthermore, the number of nodes in the tree is always smaller for for a variant, and not only that."
        ],
        [
            "But we can show that we actually have seen that window makes fewer mistakes and grow smaller trees for all sequences individually."
        ],
        [
            "Oh so some related work that I probably don't know if I have time to cover."
        ],
        [
            "Is this?",
            "OK, so."
        ],
        [
            "Let me skip to the summary."
        ],
        [
            "I introduced an online learning algorithm for learning prediction suffix."
        ],
        [
            "And I saw that it is competitive with the best fixed."
        ],
        [
            "Fix and suffix tree in hindsight and the resulting trees grow slowly if necessary."
        ],
        [
            "And then our task is made fewer mistakes and grow smaller trees than other state of the art algorithms.",
            "Thank you very much.",
            "Mission."
        ],
        [
            "Yes.",
            "True regret bounds.",
            "Do you get lots of the algorithm minus loss of the best tree is found?",
            "Or is it constant in front of the loss of balance or something?",
            "I don't have regret once I have mistake Bones.",
            "Constant on you.",
            "Is it is it I mean the the now it's with numbers here, right?",
            "Like in the theorem?",
            "This the two times the loss divided.",
            "Oh you can trade these two with a 64.",
            "You know, like you can make this model, but this will become bigger when you randomize this algorithm.",
            "I see.",
            "Uh huh, OK. And he has a terministic.",
            "So probably you can, yeah OK. One more quick question.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, this talk is about 6.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So prediction and sequential prediction means predicting the next item in a sequence using the previous items.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this talk I will present an algorithm that learns small and accurate prediction suffix trees, which is a model for doing sequential produce.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And another task we have before I explain things about this algorithm and more formally about the problem, I want to say a few things about our motivational or motivating application.",
                    "label": 0
                },
                {
                    "sent": "So we want to know if a program is behaving normally and what that means is we are running the program in a controlled environment and we're making a model of the system, calls the sequences of system calls it.",
                    "label": 1
                },
                {
                    "sent": "Execute and then.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We monitor the program outside this controlled environment and see what kind of system calls it actually executes and compare again.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The predicted ones and then we see.",
                    "label": 0
                },
                {
                    "sent": "Further we look for deviations which may signify a bug, a security problem, etc.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is sequential prediction?",
                    "label": 0
                },
                {
                    "sent": "So we have a known alphabet, like the set of nucleotides or minus 1 + 1 that I will give him the examples or the sequence a set of system calls R as in our motivating application.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then what we do is in its around T we want to predict item YT given the prefix Y one up to Y T -- 1.",
                    "label": 0
                },
                {
                    "sent": "I know why I belong to this alphabet that we know.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And prediction suffix trees, which are also known as context trees or variable length Markov models are popular and effective models for this task.",
                    "label": 1
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So I will.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Describe them through an example and I will be using a version that has been popular by Yoram Singer and his students.",
                    "label": 0
                },
                {
                    "sent": "So assume that YT is in the set minus 1 + 1 and in.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This case prediction suffix tree is a binary tree and further.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For every node has label has a value, a real number real number, positive or negative.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the links on the edges are labeled either minus one or plus one they are labeled with the symbols from the alphabet.",
                    "label": 0
                },
                {
                    "sent": "In this talk I will link to the left.",
                    "label": 0
                },
                {
                    "sent": "Children will be labeled minus one.",
                    "label": 0
                },
                {
                    "sent": "The links to the right children will be labeled plus one.",
                    "label": 0
                },
                {
                    "sent": "So to make a prediction for item YT we will fall we will start at the root and follow the path labeled YT minus one writing minus two and so on.",
                    "label": 0
                },
                {
                    "sent": "So until the next child in the sequel.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It does not exist.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we start at the root with scanner sequence backwards.",
                    "label": 0
                },
                {
                    "sent": "And I can relate to these values that we collect.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From this different nodes and YT is the sign of a weighted sum of the values in the visited nodes, and this weighted sum is such that early.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Symbols in the sequence are discounted more than recent ones.",
                    "label": 0
                },
                {
                    "sent": "The weights of this weighted sum will be such that, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give you an example of how this works.",
                    "label": 0
                },
                {
                    "sent": "Suppose that we will be using this type of discounting this exponential discounting when I know that the depth D will be discounted by 1/2 to the D. Then we have this prediction suffix tree.",
                    "label": 1
                },
                {
                    "sent": "So if our sequence ends in plus one, then.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will follow this right link here and we will have 1/2 * -- 2 so we.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Predict minus one in this case.",
                    "label": 0
                },
                {
                    "sent": "If the sequence ends in.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plus 1 -- 1.",
                    "label": 0
                },
                {
                    "sent": "Then",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will follow the minus run first, so we are scanning backwards and then we will see that we cannot.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is no right child here, so we'll stop there and we.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we have so far.",
                    "label": 0
                },
                {
                    "sent": "So minus one.",
                    "label": 0
                },
                {
                    "sent": "And finally, if it's if the sequence is.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Minus 1 -- 1 we just follow the two minus ones and.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Producer the this weighted sum of.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Values.",
                    "label": 0
                },
                {
                    "sent": "So now will show the connection between this.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prediction problem and linear classification.",
                    "label": 1
                },
                {
                    "sent": "Every node in the tree is indexed by the suffix that leads to this node.",
                    "label": 1
                },
                {
                    "sent": "And if we collect all.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As nodes, all this node values in a big vector at a time T, There are some nodes in the tree and their values can be collected in a vector.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "WT that way will be indexing by this strings is and then the decision can be written as a sign of the of this vector WT with another vector XD.",
                    "label": 0
                },
                {
                    "sent": "And next is defined as follows.",
                    "label": 0
                },
                {
                    "sent": "X + T is defined as follows.",
                    "label": 0
                },
                {
                    "sent": "If S is a suffix of the sequence, then we will take the feature value to be better to the length of the suffix, but there will be a number between zero and one that.",
                    "label": 1
                },
                {
                    "sent": "So this means that it will be decaying exponentially quickly and this is very common for prediction suffix trees, and if it is not a suffix of the sequence, then.",
                    "label": 0
                },
                {
                    "sent": "It will just not affect the decision.",
                    "label": 0
                },
                {
                    "sent": "So what we?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will do is very interested in learning this W vector.",
                    "label": 0
                },
                {
                    "sent": "The values in the tree in the online setting because we expect that the system calls we observe from an executable or not coming from a stationary process.",
                    "label": 1
                },
                {
                    "sent": "So we want an online algorithm to keep track of the best tree.",
                    "label": 0
                },
                {
                    "sent": "And here is an algorithm for online learn.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I will explain more about the motivation for choosing this particular algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in a way.",
                    "label": 0
                },
                {
                    "sent": "So I have written it in this form so that it looks a lot like perceptron.",
                    "label": 0
                },
                {
                    "sent": "We have a vector of parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "If we make a mistake we update using the Perceptron update and when we end our weight vector our actual weight vector is exponentiating the parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And therefore you can see that this weight vector all the values of it are positive.",
                    "label": 0
                },
                {
                    "sent": "For this algorithm to be able to make both negative positive and negative predictions, it gives us a special form of these features.",
                    "label": 0
                },
                {
                    "sent": "It has a features have a positive part and the negative part and the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First part are is our original feature vector and the negative part is just.",
                    "label": 0
                },
                {
                    "sent": "Minus the the positive part and if we write our our features in this way, we can also split our parameter vector in the same way to have a positive part in the negative part.",
                    "label": 0
                },
                {
                    "sent": "And then we can write the inner product between W an X this thing here that this is all we need to be able to evaluate.",
                    "label": 0
                },
                {
                    "sent": "The sign of this quantity is all we need to be able to evaluate to run this algorithm to make decisions.",
                    "label": 0
                },
                {
                    "sent": "And this can be converted to be proportional to the sum over all positive parameters of the hyperbolic sine of the parameter times the.",
                    "label": 0
                },
                {
                    "sent": "Thanks, they their corresponding feature.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, I want like the hyperbolic sine is probably a function that you don't see very often.",
                    "label": 0
                },
                {
                    "sent": "Iproperty hyperbolic sine is that it has a root at zero.",
                    "label": 0
                },
                {
                    "sent": "The only route of hyperbolic sine is at 0.",
                    "label": 0
                },
                {
                    "sent": "So when the parameter is zero, it doesn't affect the decision.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we will do is we will use window to learn good.",
                    "label": 1
                },
                {
                    "sent": "We will store this set of values in our tree and we will use Windows Update.",
                    "label": 0
                },
                {
                    "sent": "We will make predictions by this previous relation.",
                    "label": 0
                },
                {
                    "sent": "So we want to have a small tree and to do that.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This data vector has to be sparse.",
                    "label": 0
                },
                {
                    "sent": "Because if it's sparse, then the corresponding node in the tree doesn't have to be stored.",
                    "label": 0
                },
                {
                    "sent": "If a parameter for a particular suffix.",
                    "label": 0
                },
                {
                    "sent": "If a parameter for a particular suffix is 0, then.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "It's your curves.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I mean if it's a fix, it has a parameter equal to 0, then the suffix that leads to a node doesn't know doesn't have to be stored in the tree.",
                    "label": 0
                },
                {
                    "sent": "So initially we start with a zero vector.",
                    "label": 0
                },
                {
                    "sent": "And therefore nothing.",
                    "label": 0
                },
                {
                    "sent": "None of the nodes in the tree has to be stored.",
                    "label": 1
                },
                {
                    "sent": "We just have the root of the tree, which is like a dummy node, doesn't correspond to any suffix.",
                    "label": 0
                },
                {
                    "sent": "And when we make mistakes, we have data parameters and therefore some of these parameters become non zero and we have to.",
                    "label": 1
                },
                {
                    "sent": "So allocate nodes in the tree.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Accommodate experiment.",
                    "label": 0
                },
                {
                    "sent": "And what is happening is like if we use an algorithm like window or perceptron for keeping track of our hypothesis then this.",
                    "label": 0
                },
                {
                    "sent": "This update rule that they use quickly destroy the sparsity of the trade it leads.",
                    "label": 0
                },
                {
                    "sent": "They lead to large trees, so here's an illustration of the.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem suppose that our features are looking like this.",
                    "label": 0
                },
                {
                    "sent": "These are the regular features.",
                    "label": 0
                },
                {
                    "sent": "If it sees a suffix, then we have 1/2 to the length of the suffix and 0 otherwise four.",
                    "label": 1
                },
                {
                    "sent": "And we have this input sequence.",
                    "label": 0
                },
                {
                    "sent": "We want to predict the value of the question mark.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "For this three here on the left.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our prediction the prediction that it makes is just it follows the minus one here.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it produced.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is something positive?",
                    "label": 0
                },
                {
                    "sent": "And now suppose that.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually we make a mistake that the value of the question was minus one.",
                    "label": 0
                },
                {
                    "sent": "What we have to do is we have to make an update and the update says that for every suffix of this sequence will have to change the parameter.",
                    "label": 0
                },
                {
                    "sent": "The parameter vector that we implicitly keep with history.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Allocate nodes for all these suffixes and we have to make the update according to.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what happens is that when we.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make a mistake at time T we have to allocate order T nodes and therefore this leads to a quadratic dependence of the size of the tree with the length of the sequence.",
                    "label": 1
                },
                {
                    "sent": "And that is not just hard to learn.",
                    "label": 0
                },
                {
                    "sent": "It's also hard to store because we will be dealing with sequences with millions of.",
                    "label": 0
                },
                {
                    "sent": "What items so this happens becausw?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our features are such that they are non 0 even for very long suffixes and there are two ways to fix that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "The first way is like we can change our features to look only on suffixes of small length, like small suffixes.",
                    "label": 0
                },
                {
                    "sent": "Of short length.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a bad idea because we would like our algorithm to be able to compete against arbitrarily deep and complex trees.",
                    "label": 0
                },
                {
                    "sent": "And if we do that, we cannot change our features.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cannot do that anymore.",
                    "label": 0
                },
                {
                    "sent": "Another idea is to have an adaptive bound DT on the depth after which the tree can grow in round T. So you can think of DT some quantity that will be growing slowly if necessary.",
                    "label": 1
                },
                {
                    "sent": "Oh, and if we make mistakes, did they will be growing?",
                    "label": 0
                },
                {
                    "sent": "And if they, if we if we are not making mistakes that they will stay small.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a window for prediction.",
                    "label": 0
                },
                {
                    "sent": "Suffix trees is the only changes like the update rule here has is the previous update rule plus this X.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The term here NT and you can think of 20 or something that will prove that Ray it's a noise term that cancels the update for all those suffixes that are linked whose length is greater than DT.",
                    "label": 0
                },
                {
                    "sent": "So with this you can think conceptually as growing the full tree, growing full path in the tree and then pruning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here is the main effect of the noise in the analysis.",
                    "label": 0
                },
                {
                    "sent": "This term PT here.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the sum overall rounds up to round T on which mistakes are made of the Infinity norm of the noise vectors, and this is exactly why we picked window because the noise vectors, the effect of the noise comes through the Infinity norm of the of this noise term.",
                    "label": 1
                },
                {
                    "sent": "So this is like the smallest.",
                    "label": 0
                },
                {
                    "sent": "Norm among them.",
                    "label": 0
                },
                {
                    "sent": "Among the norms like they could use so that their analysis gives the effect of the noise bounded and what we do is.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We set the pity.",
                    "label": 0
                },
                {
                    "sent": "We maintain an invariant that PT has to be less than the number of mistakes to some power, and you can have many powers here that give a mistake bound.",
                    "label": 0
                },
                {
                    "sent": "But we use this one which works well in practice.",
                    "label": 0
                },
                {
                    "sent": "So we said it in this way.",
                    "label": 0
                },
                {
                    "sent": "This is by induction from this way from this.",
                    "label": 0
                },
                {
                    "sent": "From this quantity from this invariant.",
                    "label": 0
                },
                {
                    "sent": "And they would do that.",
                    "label": 0
                },
                {
                    "sent": "We can get mistake bound that if there is another three that.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our algorithm could have found then that attains small loss L on the input sequence and this loss is like a modified hinge loss.",
                    "label": 1
                },
                {
                    "sent": "Then our mistakes will be at most the number of this quantity here.",
                    "label": 1
                },
                {
                    "sent": "At most this many mistakes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a Delta this way, but this part does not depend on the loss in the log.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can also prove that the the trade that will learn will not be too deep.",
                    "label": 0
                },
                {
                    "sent": "That means that it will have at most logarithm of the number of mistakes plus 4 levels.",
                    "label": 0
                },
                {
                    "sent": "And this is when we said better to this quantity, we change this quantity.",
                    "label": 0
                },
                {
                    "sent": "We get another.",
                    "label": 0
                },
                {
                    "sent": "We get a number here in front.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will quickly go over the proof.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "The growth bound is straightforward algebra from the definition of this DT value.",
                    "label": 1
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The mistake bound is using a potential function argument.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the potential function we're using the relative entropy as usual for for multiple.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Active updates and what we do is we upper bound initial potential and we saw that the decreasing potential with its mistake can be decomposed in the effect of the full update minus the effect of the noise and the effect of the full update is upper bound lower bounded by a function of the learning rate and loss of the optimal vector.",
                    "label": 0
                },
                {
                    "sent": "So here is the proof.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Victoria Lee will take an example.",
                    "label": 0
                },
                {
                    "sent": "Suppose we.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mega mistake we make the full update and then buy prune.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The three we introduce.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some noise here, but takes away some of our progress.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example, the same story happens.",
                    "label": 0
                },
                {
                    "sent": "The noise takes away.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Progress by pruning.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we make correct prediction, there is no.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh no progress.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, this is how things are going.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If is that the length of the blue bar is always less than cannot be more than.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The initial potential and the blue bar is just the sum of the green blocks minus the.",
                    "label": 0
                },
                {
                    "sent": "Minus the length.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Brown birds effect of the noise there.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it was lower bounded in some previous slide by the number of mistakes times the minimum size of the green block, the Brown, the effect of the noise is we have an invariant that says it should be less than the number of mistakes to the 2/3, and then we have this in the.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we solve this inequality will get our mistake bound.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So quickly, some results.",
                    "label": 0
                },
                {
                    "sent": "We had three programs for these sequences of system calls for every program, 120 sequences of.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "System calls and we run our window variant and the bounded perception for prediction suffix trees by Decker salvage parts and Singer.",
                    "label": 0
                },
                {
                    "sent": "And we here is the average.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The error rate over there for the sequence is number of mistakes divided by the length of the sequence, and you see that we have like.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fewer mistakes, and Furthermore, the number of nodes in the tree is always smaller for for a variant, and not only that.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can show that we actually have seen that window makes fewer mistakes and grow smaller trees for all sequences individually.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh so some related work that I probably don't know if I have time to cover.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me skip to the summary.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I introduced an online learning algorithm for learning prediction suffix.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I saw that it is competitive with the best fixed.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fix and suffix tree in hindsight and the resulting trees grow slowly if necessary.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then our task is made fewer mistakes and grow smaller trees than other state of the art algorithms.",
                    "label": 1
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Mission.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "True regret bounds.",
                    "label": 0
                },
                {
                    "sent": "Do you get lots of the algorithm minus loss of the best tree is found?",
                    "label": 0
                },
                {
                    "sent": "Or is it constant in front of the loss of balance or something?",
                    "label": 0
                },
                {
                    "sent": "I don't have regret once I have mistake Bones.",
                    "label": 0
                },
                {
                    "sent": "Constant on you.",
                    "label": 0
                },
                {
                    "sent": "Is it is it I mean the the now it's with numbers here, right?",
                    "label": 0
                },
                {
                    "sent": "Like in the theorem?",
                    "label": 0
                },
                {
                    "sent": "This the two times the loss divided.",
                    "label": 0
                },
                {
                    "sent": "Oh you can trade these two with a 64.",
                    "label": 0
                },
                {
                    "sent": "You know, like you can make this model, but this will become bigger when you randomize this algorithm.",
                    "label": 0
                },
                {
                    "sent": "I see.",
                    "label": 0
                },
                {
                    "sent": "Uh huh, OK. And he has a terministic.",
                    "label": 0
                },
                {
                    "sent": "So probably you can, yeah OK. One more quick question.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}