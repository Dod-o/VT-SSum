{
    "id": "hozrtifsqftal76xqa6vednf7hkovexs",
    "title": "Latent Space Domain Transfer between High Dimensional Overlapping Distributions",
    "info": {
        "author": [
            "Sihong Xie, Department of Computer Science, University of Illinois at Chicago",
            "Wei Fan, Baidu, Inc.",
            "Jing Peng, Montclair State University",
            "Olivier Verscheure, IBM Thomas J. Watson Research Center",
            "Jiangtao Ren, Sun Yat-Sen University"
        ],
        "published": "May 20, 2009",
        "recorded": "April 2009",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/www09_xie_lsdt/",
    "segmentation": [
        [
            "About four"
        ],
        [
            "Categorize were talking model for New York Times.",
            "So ideally OK go and look for label information from New York Times.",
            "Then you apply a multiple choice like SPN, largest regression and various Daniel.",
            "Play the model on your desired testing data.",
            "You get 85% accuracy and that's not that.",
            "And but you."
        ],
        [
            "That it is you mean you want to apply a model on your times, but you may not be able to get any labeled information in your dimension interest.",
            "Southwestern WH I have a I don't have any label information from New York Times, but what I have is Reuters.",
            "So apply the same algorithm as VN logic regression.",
            "Are you based on Reuters data trigger model apply on the same testing data lab, the accuracy drops down to 64% and that's a does not too good.",
            "OK so the."
        ],
        [
            "The challenge we answer we want to dress in a learning is in the ideal setting.",
            "You know you want to get them out of your domain, interests you go and look for label information in the domain, but in reality you may not be able to because it may be costly and you won't be able to get information in time so that you have a little information from a dramatic of your related domain.",
            "So you have the accuracy job.",
            "So the difference here is a 20% accuracy differ.",
            "So the challenge of his learning is how can we, you know, improve our learning algorithms or come up with new ones so we can get accuracy as good as you know from training the information in the same domain.",
            "Like 80% OK?",
            "So."
        ],
        [
            "This paper that's a quick summary of what we mean by a stressful learning, so this papers talking about high dimensional data transfer so high dimensional data is very common for test capitalizations.",
            "Also us for image data and in the corpus we used in our study we have the dimension is about 4000 and what's interesting is in the study we have the number of labeled information is.",
            "So the much smaller than you are playing with dimension and so you easy right into our fitting problem.",
            "If you are very careful with the features and extras you use.",
            "And this is particularly interesting for the in terms of data mining and machine learning is forceful is a high dimensionality problem.",
            "And as I said before, the number of features are usually more than number between examples.",
            "And also when the high dimension there are too many features, we can.",
            "There's a previous for most studies showing that the Euclidean distance among the different examples.",
            "Obviously very, very similar as the dimensionality Brooks High.",
            "So which means that using why you have high dimension using without using all the features calculating distance is not a good measure.",
            "You are going to confuse these greeting.",
            "Discrete machine labels belong to different class labels.",
            "And another interesting point is also our elaborate more is because they have dimensional and data coming from different distributions they do not share the same feature sets.",
            "So for example they may only share 80% features in total and those examples from the source domain and title domain may have another 20% features of their own which are not shared by the other data in the separate.",
            "Different domain.",
            "So so given this, you know the situation.",
            "So the question we want to ask is there mushrooms module distributions of the two datasets do related?",
            "If so, how ready they are?",
            "If not, how can we come up with the right approach to improve it?",
            "And because they are high dimensional also they are having overlapping features and it's very hard to find the right structures transfer between the source and target domain.",
            "And because they're high dimensional, we really have to come up with the right definition of what's being similar and use it to distinguish.",
            "Examples belong to different class.",
            "So he."
        ],
        [
            "Here is the.",
            "Illustration what we mean exactly by overlapping distribution.",
            "Here we have a.",
            "3 examples and two.",
            "The two dark blue are from the out domain or your training domain.",
            "And they want the top.",
            "The light blue is your your thought, your target man where you want to apply your Model 1 so here you can see that the only features they have in common is a Z.",
            "We are all the examples, have a value there, and so the example a only have value for Y which B&C do not have a value.",
            "So similarly BNC have a value in the Y dimension where the have the X dimension we are a doesn't have a value.",
            "So here is like a particular datasets we have used in the study and the top one you look at the last column, the.",
            "The last column here, the 25%, is the percentage of features not shared by the source and target domain."
        ],
        [
            "The problem is the following.",
            "When you only use the single F3, the overlapping feature to make a model.",
            "It's not good because here can see that if you using your clean distance to a distinguish the examples belong to different class is very hard to say which class the top example should belong to because it had same distance to either the B or C because the distance your .1.",
            "The idea the information here is because we are now using some additional information that is not overlapping across two domains.",
            "So the challenge here is how we are able to use this information to help us.",
            "So one very simple idea is, so how about for those missing values?",
            "We just fill up with zeros because we don't have any value anyway, so using zeros, that's the most you know straightforward approach to solve the problem.",
            "But the question is, does it really help?"
        ],
        [
            "So here is 1 example illustration for the same example shown before.",
            "So have plotted the three examples in the three dimensional space OK and the two examples on the top, the Green one.",
            "Sorry my my laser pointer is not powerful enough and so this one green one and this right one.",
            "Are they labeled examples from the your training data?",
            "OK, and this one is your target domain example, you want to classify this example and the red color, meaning that if you know the true answer, they belong to the same class.",
            "OK, so if you are with zero, so actually the target domain example actually is closer to the green on the top.",
            "So then actually the example would be misclassified to have the same class as C instead of B.",
            "Ideally the actual label should be be now see, but because we use it, we fill out with zeros and that's your own mess up the distance measure.",
            "So actually we're giving it wrong.",
            "A classification."
        ],
        [
            "So we show in the paper formally that when one uses union of overlapping features and not overlapping features and leave the missing values, euros, we show that in the paper formally that the distance of too much distributions can become very very large as a function of the nonliving features.",
            "So this is not good because that will become a very dominant factor in your similarity measure.",
            "Like you can distance."
        ],
        [
            "And also high dimensionality can early underpin important features and for example, here we have two classes and we have example from in domain of domain and the Blues.",
            "Are closer to the Greens and then read, but actually the blue and red should belong to the same class because we're using too many features and some features are irrelevant and they mess up these multi measure so you end up actually giving the wrong class label."
        ],
        [
            "So the.",
            "The methods we are proposing is a two step correction approach.",
            "So first we use a miss.",
            "You missing value regression approach to a brings the marginal distribution closer.",
            "By filling up some values but not euros into this non overlapping features, then we use the latent space dimension reduction and we show formally that they can bring some margin distribution even closer and we can also ignore those non important noisy and errors due to the feature importation from the first step.",
            "And we can also easily identify those transferable structures across the two domains."
        ],
        [
            "So here is illustration.",
            "It's about missing value regression.",
            "So how we use it in the transfer learning scenario?",
            "So we remember the same chart or have some before and so this example in the middle.",
            "Here is the ultimate example.",
            "OK, and we have as she is only shares one feature with another example.",
            "So first we projected.",
            "To the same plan the same.",
            "Same space as your out domain example.",
            "OK, so your first project it, then you use a regression model to map it into the domain, so that will projection here.",
            "Then goes here.",
            "So after this process the the distance of the image after projection is closer to the red than to the green, so that will give it the right class label as it should be."
        ],
        [
            "And then the second step in the is for the dimension reduction where user singular value decomposition and so the beginning.",
            "The first part are the auto main word vectors or feature vectors.",
            "These are the in domain feature vectors and in the beginning because these are the those features, you don't have any value in common.",
            "Then you use your missing value regression approach to fill this up."
        ],
        [
            "Then you project the.",
            "The combination of in domain out domain, the auto main int are manual few up the.",
            "Missing values with the regression approach.",
            "Then you are using an SVD to convert it into a lower subspace."
        ],
        [
            "So here is, uh, see how the SVD helps us.",
            "These are the same examples in before, and the Blues are closer to the Greens than to the Reds, and we ideally it actually the blue and the red should belong to the same class.",
            "So after SVD, the Blues and red are closer, so it will be given the right class label using the right similarity measure."
        ],
        [
            "And there are some interesting properties and we have shown in the paper and here I just give a quick summary.",
            "Not going to the details or like formulas.",
            "So we show that.",
            "The proposed approach can bring the margin distributions of two domains close.",
            "And this particular true using the support vector regression approach and we will give a bound on the difference of the month distribution after regression approach is applied.",
            "And then actually the after the SVD, they dimension reduction and we show that the mass distribution is the difference.",
            "Image traditions even minimized more in the low dimensional space and prove is based on a similarity measure between the K means and SVD.",
            "And we also showed that in the paper, because in this scenario we don't have any labeled information from your target domain, but based on the clustering manifold assumption and we show formally that the nearby 2 instances should have a similar condition distribution given certain restrictions.",
            "And we also show that the approach we have proposed can reduce the domain transfer risk and we give a bound like to show that the distance in the new sniper approach used vacation is related to this risk to to make a mistake."
        ],
        [
            "And we have a in a paper presented like three sets of results, and we use the 20 newsgroup datasets and as well as the SRA is simulated real auto aviation datasets which contains about 7000 articles from 4 different discussion groups and we also use the Reuters datasets.",
            "So each of the datasets the concepts are is as a hierarchy and for example in the 20 newsgroup and higher hierarchy can include compute computers and recreation.",
            "OK, so for the each of the datasets the out domain OK, your training data.",
            "We take examples from the same.",
            "Higher hierarchy, often computers.",
            "One classroom computer and for example systems and the other from a recreation for the most sports.",
            "The in domain data.",
            "The class labels are from the same high level hierarchy, but from a different lower level hierarchy.",
            "For example, in domain data, one class coming from computer graphics, which is also in computer and the other class coming from recreation but subclasses auto.",
            "And as the baseline methods, we use an obvious and largest version SVN, which are known to handle high dimensional data well and to justify the proposed approach.",
            "The little nap approach.",
            "We have two more baselines, one sucking regression.",
            "So the difference is we also use the missing value regression approach to fill up the non overlapping features but without SVD.",
            "And appealing to nap and the difference from the proposed approaches is used as VD, but missing value will be, uh, will be treated as zeros.",
            "So basically we want to show why we want to do the regression to fill up as well as use SVD two dimension reduction."
        ],
        [
            "And here is a summary of the datasets we have and they are all at the number of examples are enough use out in usually less than 2000 a number of features can be as many as like 5000."
        ],
        [
            "So here is a quick summary of the oral performance.",
            "So comparing with the different baselines, and we have a ten wins and one loss, and these are averaged with a 10 fold CV."
        ],
        [
            "And comparing with the key and regression just to remind ourselves in regression is we have also have the missing value filled up by regression.",
            "But we do not use SVD, so we want to specify.",
            "Using missing value regression is not enough.",
            "We also want to use SVD, so comparing with sucking regression the proposed approach have eight wins and three losses.",
            "And comparing with appeal it in the map, the difference from the proposed approach is it uses SVD, but missing value would be will be used will be filled with zeros.",
            "So we have eight wins, three losses.",
            "So here's a quick."
        ],
        [
            "Alright, so the problems we addressed is.",
            "A transfer learning problems between high dimensional overlapping distributions and examples, including text and image data.",
            "And the approach we have is a two step approach and the first step is we are few up missing values by your regression and the the purpose is to bring the the domains margin distribution closer and step by step is to use SVD for dimension reduction and has two purposes.",
            "First of all can also bring the distribution model solution even closer and we can also show that with we also can reduce.",
            "The difference in conditional probability distributions as well as minimal risk in domain transfer and the code and datasets reported in the data available from the our research websites."
        ],
        [
            "Thank you.",
            "We have time for a few questions.",
            "I'll start out with the question how does the method scale as the number of missing as the size of the overlap?",
            "We yeah we number features yeah OK just one results we did with synthetic datasets like because when we we actually have it, snow is about 80% overlapping thermostat so we did some synthetic like just using same data and removing more and more features and it goes wrong.",
            "We tried from anywhere from 5% up to like 80% so the methods really has advantage when the Member of Missing values are really a lot.",
            "And the advantage gets smaller when the overlapping is more.",
            "Do you know why you approach lose?",
            "An in that case, in those cases that you show, yeah, we we're not sure yet.",
            "Yeah, it's something it's good to find out.",
            "Yeah, we have.",
            "I think comparing the baseline, we have a 10 win, another one loss.",
            "Yeah, but you already here is like in machine learning like you probably we can never find an approach that you know can always be a can always be the winner, but it's good to find out exactly, but these are not synthetic data set using synthetic that says it's easier because we can really find we can be to our call to create the true function.",
            "Then this study the difference in there.",
            "But thank you.",
            "What is the two datasets were separate gathered at different points in time?",
            "Do you think the technique would work?",
            "Then you're making of data gathered.",
            "This happened date of time.",
            "For example, you have data last year and had this data.",
            "Yeah, I think well, because usually in immersion learning like there are three domains of you know just recently related one is streaming while the sample selection bias once transfer learning.",
            "So I think what you mean is streaming so you have a data from the past which are probably related to your current data.",
            "But then maybe concept drift.",
            "And so some post election biases.",
            "You are, for example, you know you want to do a testing on an effective drug.",
            "But people self select.",
            "So obviously your sample people self select will be different from the general population, so you won't do a correction by interest.",
            "Learning is the assumptions is much looser than that, so they're just roughly related.",
            "So basically we don't have in this paper, but in separate paper coming in KDD this year is we define similarity.",
            "On the joint distribution, so one that is 20 distributions within, we use the hypothesis space to measure the difference within some some limits on the distance.",
            "Then the approach proposed will work, but if you see the difference too high then you're is very hard to bound the difference when you actually do the modeling.",
            "Thanks.",
            "Into speaker.",
            "Again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About four",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Categorize were talking model for New York Times.",
                    "label": 1
                },
                {
                    "sent": "So ideally OK go and look for label information from New York Times.",
                    "label": 0
                },
                {
                    "sent": "Then you apply a multiple choice like SPN, largest regression and various Daniel.",
                    "label": 0
                },
                {
                    "sent": "Play the model on your desired testing data.",
                    "label": 0
                },
                {
                    "sent": "You get 85% accuracy and that's not that.",
                    "label": 0
                },
                {
                    "sent": "And but you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That it is you mean you want to apply a model on your times, but you may not be able to get any labeled information in your dimension interest.",
                    "label": 0
                },
                {
                    "sent": "Southwestern WH I have a I don't have any label information from New York Times, but what I have is Reuters.",
                    "label": 1
                },
                {
                    "sent": "So apply the same algorithm as VN logic regression.",
                    "label": 0
                },
                {
                    "sent": "Are you based on Reuters data trigger model apply on the same testing data lab, the accuracy drops down to 64% and that's a does not too good.",
                    "label": 0
                },
                {
                    "sent": "OK so the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The challenge we answer we want to dress in a learning is in the ideal setting.",
                    "label": 0
                },
                {
                    "sent": "You know you want to get them out of your domain, interests you go and look for label information in the domain, but in reality you may not be able to because it may be costly and you won't be able to get information in time so that you have a little information from a dramatic of your related domain.",
                    "label": 0
                },
                {
                    "sent": "So you have the accuracy job.",
                    "label": 0
                },
                {
                    "sent": "So the difference here is a 20% accuracy differ.",
                    "label": 0
                },
                {
                    "sent": "So the challenge of his learning is how can we, you know, improve our learning algorithms or come up with new ones so we can get accuracy as good as you know from training the information in the same domain.",
                    "label": 0
                },
                {
                    "sent": "Like 80% OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This paper that's a quick summary of what we mean by a stressful learning, so this papers talking about high dimensional data transfer so high dimensional data is very common for test capitalizations.",
                    "label": 1
                },
                {
                    "sent": "Also us for image data and in the corpus we used in our study we have the dimension is about 4000 and what's interesting is in the study we have the number of labeled information is.",
                    "label": 0
                },
                {
                    "sent": "So the much smaller than you are playing with dimension and so you easy right into our fitting problem.",
                    "label": 0
                },
                {
                    "sent": "If you are very careful with the features and extras you use.",
                    "label": 0
                },
                {
                    "sent": "And this is particularly interesting for the in terms of data mining and machine learning is forceful is a high dimensionality problem.",
                    "label": 0
                },
                {
                    "sent": "And as I said before, the number of features are usually more than number between examples.",
                    "label": 1
                },
                {
                    "sent": "And also when the high dimension there are too many features, we can.",
                    "label": 0
                },
                {
                    "sent": "There's a previous for most studies showing that the Euclidean distance among the different examples.",
                    "label": 0
                },
                {
                    "sent": "Obviously very, very similar as the dimensionality Brooks High.",
                    "label": 0
                },
                {
                    "sent": "So which means that using why you have high dimension using without using all the features calculating distance is not a good measure.",
                    "label": 0
                },
                {
                    "sent": "You are going to confuse these greeting.",
                    "label": 0
                },
                {
                    "sent": "Discrete machine labels belong to different class labels.",
                    "label": 0
                },
                {
                    "sent": "And another interesting point is also our elaborate more is because they have dimensional and data coming from different distributions they do not share the same feature sets.",
                    "label": 0
                },
                {
                    "sent": "So for example they may only share 80% features in total and those examples from the source domain and title domain may have another 20% features of their own which are not shared by the other data in the separate.",
                    "label": 0
                },
                {
                    "sent": "Different domain.",
                    "label": 0
                },
                {
                    "sent": "So so given this, you know the situation.",
                    "label": 0
                },
                {
                    "sent": "So the question we want to ask is there mushrooms module distributions of the two datasets do related?",
                    "label": 0
                },
                {
                    "sent": "If so, how ready they are?",
                    "label": 0
                },
                {
                    "sent": "If not, how can we come up with the right approach to improve it?",
                    "label": 0
                },
                {
                    "sent": "And because they are high dimensional also they are having overlapping features and it's very hard to find the right structures transfer between the source and target domain.",
                    "label": 0
                },
                {
                    "sent": "And because they're high dimensional, we really have to come up with the right definition of what's being similar and use it to distinguish.",
                    "label": 0
                },
                {
                    "sent": "Examples belong to different class.",
                    "label": 0
                },
                {
                    "sent": "So he.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is the.",
                    "label": 0
                },
                {
                    "sent": "Illustration what we mean exactly by overlapping distribution.",
                    "label": 0
                },
                {
                    "sent": "Here we have a.",
                    "label": 0
                },
                {
                    "sent": "3 examples and two.",
                    "label": 0
                },
                {
                    "sent": "The two dark blue are from the out domain or your training domain.",
                    "label": 0
                },
                {
                    "sent": "And they want the top.",
                    "label": 0
                },
                {
                    "sent": "The light blue is your your thought, your target man where you want to apply your Model 1 so here you can see that the only features they have in common is a Z.",
                    "label": 0
                },
                {
                    "sent": "We are all the examples, have a value there, and so the example a only have value for Y which B&C do not have a value.",
                    "label": 0
                },
                {
                    "sent": "So similarly BNC have a value in the Y dimension where the have the X dimension we are a doesn't have a value.",
                    "label": 0
                },
                {
                    "sent": "So here is like a particular datasets we have used in the study and the top one you look at the last column, the.",
                    "label": 0
                },
                {
                    "sent": "The last column here, the 25%, is the percentage of features not shared by the source and target domain.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is the following.",
                    "label": 0
                },
                {
                    "sent": "When you only use the single F3, the overlapping feature to make a model.",
                    "label": 0
                },
                {
                    "sent": "It's not good because here can see that if you using your clean distance to a distinguish the examples belong to different class is very hard to say which class the top example should belong to because it had same distance to either the B or C because the distance your .1.",
                    "label": 0
                },
                {
                    "sent": "The idea the information here is because we are now using some additional information that is not overlapping across two domains.",
                    "label": 0
                },
                {
                    "sent": "So the challenge here is how we are able to use this information to help us.",
                    "label": 0
                },
                {
                    "sent": "So one very simple idea is, so how about for those missing values?",
                    "label": 0
                },
                {
                    "sent": "We just fill up with zeros because we don't have any value anyway, so using zeros, that's the most you know straightforward approach to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "But the question is, does it really help?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is 1 example illustration for the same example shown before.",
                    "label": 0
                },
                {
                    "sent": "So have plotted the three examples in the three dimensional space OK and the two examples on the top, the Green one.",
                    "label": 0
                },
                {
                    "sent": "Sorry my my laser pointer is not powerful enough and so this one green one and this right one.",
                    "label": 0
                },
                {
                    "sent": "Are they labeled examples from the your training data?",
                    "label": 0
                },
                {
                    "sent": "OK, and this one is your target domain example, you want to classify this example and the red color, meaning that if you know the true answer, they belong to the same class.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you are with zero, so actually the target domain example actually is closer to the green on the top.",
                    "label": 0
                },
                {
                    "sent": "So then actually the example would be misclassified to have the same class as C instead of B.",
                    "label": 1
                },
                {
                    "sent": "Ideally the actual label should be be now see, but because we use it, we fill out with zeros and that's your own mess up the distance measure.",
                    "label": 0
                },
                {
                    "sent": "So actually we're giving it wrong.",
                    "label": 0
                },
                {
                    "sent": "A classification.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we show in the paper formally that when one uses union of overlapping features and not overlapping features and leave the missing values, euros, we show that in the paper formally that the distance of too much distributions can become very very large as a function of the nonliving features.",
                    "label": 1
                },
                {
                    "sent": "So this is not good because that will become a very dominant factor in your similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Like you can distance.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also high dimensionality can early underpin important features and for example, here we have two classes and we have example from in domain of domain and the Blues.",
                    "label": 0
                },
                {
                    "sent": "Are closer to the Greens and then read, but actually the blue and red should belong to the same class because we're using too many features and some features are irrelevant and they mess up these multi measure so you end up actually giving the wrong class label.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The methods we are proposing is a two step correction approach.",
                    "label": 1
                },
                {
                    "sent": "So first we use a miss.",
                    "label": 0
                },
                {
                    "sent": "You missing value regression approach to a brings the marginal distribution closer.",
                    "label": 1
                },
                {
                    "sent": "By filling up some values but not euros into this non overlapping features, then we use the latent space dimension reduction and we show formally that they can bring some margin distribution even closer and we can also ignore those non important noisy and errors due to the feature importation from the first step.",
                    "label": 0
                },
                {
                    "sent": "And we can also easily identify those transferable structures across the two domains.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is illustration.",
                    "label": 0
                },
                {
                    "sent": "It's about missing value regression.",
                    "label": 1
                },
                {
                    "sent": "So how we use it in the transfer learning scenario?",
                    "label": 0
                },
                {
                    "sent": "So we remember the same chart or have some before and so this example in the middle.",
                    "label": 0
                },
                {
                    "sent": "Here is the ultimate example.",
                    "label": 0
                },
                {
                    "sent": "OK, and we have as she is only shares one feature with another example.",
                    "label": 0
                },
                {
                    "sent": "So first we projected.",
                    "label": 1
                },
                {
                    "sent": "To the same plan the same.",
                    "label": 0
                },
                {
                    "sent": "Same space as your out domain example.",
                    "label": 0
                },
                {
                    "sent": "OK, so your first project it, then you use a regression model to map it into the domain, so that will projection here.",
                    "label": 0
                },
                {
                    "sent": "Then goes here.",
                    "label": 0
                },
                {
                    "sent": "So after this process the the distance of the image after projection is closer to the red than to the green, so that will give it the right class label as it should be.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the second step in the is for the dimension reduction where user singular value decomposition and so the beginning.",
                    "label": 0
                },
                {
                    "sent": "The first part are the auto main word vectors or feature vectors.",
                    "label": 1
                },
                {
                    "sent": "These are the in domain feature vectors and in the beginning because these are the those features, you don't have any value in common.",
                    "label": 0
                },
                {
                    "sent": "Then you use your missing value regression approach to fill this up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you project the.",
                    "label": 0
                },
                {
                    "sent": "The combination of in domain out domain, the auto main int are manual few up the.",
                    "label": 0
                },
                {
                    "sent": "Missing values with the regression approach.",
                    "label": 0
                },
                {
                    "sent": "Then you are using an SVD to convert it into a lower subspace.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is, uh, see how the SVD helps us.",
                    "label": 0
                },
                {
                    "sent": "These are the same examples in before, and the Blues are closer to the Greens than to the Reds, and we ideally it actually the blue and the red should belong to the same class.",
                    "label": 1
                },
                {
                    "sent": "So after SVD, the Blues and red are closer, so it will be given the right class label using the right similarity measure.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are some interesting properties and we have shown in the paper and here I just give a quick summary.",
                    "label": 0
                },
                {
                    "sent": "Not going to the details or like formulas.",
                    "label": 0
                },
                {
                    "sent": "So we show that.",
                    "label": 0
                },
                {
                    "sent": "The proposed approach can bring the margin distributions of two domains close.",
                    "label": 1
                },
                {
                    "sent": "And this particular true using the support vector regression approach and we will give a bound on the difference of the month distribution after regression approach is applied.",
                    "label": 0
                },
                {
                    "sent": "And then actually the after the SVD, they dimension reduction and we show that the mass distribution is the difference.",
                    "label": 1
                },
                {
                    "sent": "Image traditions even minimized more in the low dimensional space and prove is based on a similarity measure between the K means and SVD.",
                    "label": 1
                },
                {
                    "sent": "And we also showed that in the paper, because in this scenario we don't have any labeled information from your target domain, but based on the clustering manifold assumption and we show formally that the nearby 2 instances should have a similar condition distribution given certain restrictions.",
                    "label": 0
                },
                {
                    "sent": "And we also show that the approach we have proposed can reduce the domain transfer risk and we give a bound like to show that the distance in the new sniper approach used vacation is related to this risk to to make a mistake.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we have a in a paper presented like three sets of results, and we use the 20 newsgroup datasets and as well as the SRA is simulated real auto aviation datasets which contains about 7000 articles from 4 different discussion groups and we also use the Reuters datasets.",
                    "label": 1
                },
                {
                    "sent": "So each of the datasets the concepts are is as a hierarchy and for example in the 20 newsgroup and higher hierarchy can include compute computers and recreation.",
                    "label": 0
                },
                {
                    "sent": "OK, so for the each of the datasets the out domain OK, your training data.",
                    "label": 0
                },
                {
                    "sent": "We take examples from the same.",
                    "label": 0
                },
                {
                    "sent": "Higher hierarchy, often computers.",
                    "label": 0
                },
                {
                    "sent": "One classroom computer and for example systems and the other from a recreation for the most sports.",
                    "label": 0
                },
                {
                    "sent": "The in domain data.",
                    "label": 0
                },
                {
                    "sent": "The class labels are from the same high level hierarchy, but from a different lower level hierarchy.",
                    "label": 0
                },
                {
                    "sent": "For example, in domain data, one class coming from computer graphics, which is also in computer and the other class coming from recreation but subclasses auto.",
                    "label": 1
                },
                {
                    "sent": "And as the baseline methods, we use an obvious and largest version SVN, which are known to handle high dimensional data well and to justify the proposed approach.",
                    "label": 0
                },
                {
                    "sent": "The little nap approach.",
                    "label": 1
                },
                {
                    "sent": "We have two more baselines, one sucking regression.",
                    "label": 1
                },
                {
                    "sent": "So the difference is we also use the missing value regression approach to fill up the non overlapping features but without SVD.",
                    "label": 0
                },
                {
                    "sent": "And appealing to nap and the difference from the proposed approaches is used as VD, but missing value will be, uh, will be treated as zeros.",
                    "label": 0
                },
                {
                    "sent": "So basically we want to show why we want to do the regression to fill up as well as use SVD two dimension reduction.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is a summary of the datasets we have and they are all at the number of examples are enough use out in usually less than 2000 a number of features can be as many as like 5000.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a quick summary of the oral performance.",
                    "label": 0
                },
                {
                    "sent": "So comparing with the different baselines, and we have a ten wins and one loss, and these are averaged with a 10 fold CV.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And comparing with the key and regression just to remind ourselves in regression is we have also have the missing value filled up by regression.",
                    "label": 0
                },
                {
                    "sent": "But we do not use SVD, so we want to specify.",
                    "label": 0
                },
                {
                    "sent": "Using missing value regression is not enough.",
                    "label": 0
                },
                {
                    "sent": "We also want to use SVD, so comparing with sucking regression the proposed approach have eight wins and three losses.",
                    "label": 0
                },
                {
                    "sent": "And comparing with appeal it in the map, the difference from the proposed approach is it uses SVD, but missing value would be will be used will be filled with zeros.",
                    "label": 0
                },
                {
                    "sent": "So we have eight wins, three losses.",
                    "label": 0
                },
                {
                    "sent": "So here's a quick.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so the problems we addressed is.",
                    "label": 0
                },
                {
                    "sent": "A transfer learning problems between high dimensional overlapping distributions and examples, including text and image data.",
                    "label": 1
                },
                {
                    "sent": "And the approach we have is a two step approach and the first step is we are few up missing values by your regression and the the purpose is to bring the the domains margin distribution closer and step by step is to use SVD for dimension reduction and has two purposes.",
                    "label": 0
                },
                {
                    "sent": "First of all can also bring the distribution model solution even closer and we can also show that with we also can reduce.",
                    "label": 0
                },
                {
                    "sent": "The difference in conditional probability distributions as well as minimal risk in domain transfer and the code and datasets reported in the data available from the our research websites.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "We have time for a few questions.",
                    "label": 0
                },
                {
                    "sent": "I'll start out with the question how does the method scale as the number of missing as the size of the overlap?",
                    "label": 0
                },
                {
                    "sent": "We yeah we number features yeah OK just one results we did with synthetic datasets like because when we we actually have it, snow is about 80% overlapping thermostat so we did some synthetic like just using same data and removing more and more features and it goes wrong.",
                    "label": 0
                },
                {
                    "sent": "We tried from anywhere from 5% up to like 80% so the methods really has advantage when the Member of Missing values are really a lot.",
                    "label": 0
                },
                {
                    "sent": "And the advantage gets smaller when the overlapping is more.",
                    "label": 0
                },
                {
                    "sent": "Do you know why you approach lose?",
                    "label": 0
                },
                {
                    "sent": "An in that case, in those cases that you show, yeah, we we're not sure yet.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's something it's good to find out.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we have.",
                    "label": 0
                },
                {
                    "sent": "I think comparing the baseline, we have a 10 win, another one loss.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you already here is like in machine learning like you probably we can never find an approach that you know can always be a can always be the winner, but it's good to find out exactly, but these are not synthetic data set using synthetic that says it's easier because we can really find we can be to our call to create the true function.",
                    "label": 0
                },
                {
                    "sent": "Then this study the difference in there.",
                    "label": 0
                },
                {
                    "sent": "But thank you.",
                    "label": 0
                },
                {
                    "sent": "What is the two datasets were separate gathered at different points in time?",
                    "label": 0
                },
                {
                    "sent": "Do you think the technique would work?",
                    "label": 0
                },
                {
                    "sent": "Then you're making of data gathered.",
                    "label": 0
                },
                {
                    "sent": "This happened date of time.",
                    "label": 0
                },
                {
                    "sent": "For example, you have data last year and had this data.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think well, because usually in immersion learning like there are three domains of you know just recently related one is streaming while the sample selection bias once transfer learning.",
                    "label": 0
                },
                {
                    "sent": "So I think what you mean is streaming so you have a data from the past which are probably related to your current data.",
                    "label": 0
                },
                {
                    "sent": "But then maybe concept drift.",
                    "label": 0
                },
                {
                    "sent": "And so some post election biases.",
                    "label": 0
                },
                {
                    "sent": "You are, for example, you know you want to do a testing on an effective drug.",
                    "label": 0
                },
                {
                    "sent": "But people self select.",
                    "label": 0
                },
                {
                    "sent": "So obviously your sample people self select will be different from the general population, so you won't do a correction by interest.",
                    "label": 0
                },
                {
                    "sent": "Learning is the assumptions is much looser than that, so they're just roughly related.",
                    "label": 0
                },
                {
                    "sent": "So basically we don't have in this paper, but in separate paper coming in KDD this year is we define similarity.",
                    "label": 0
                },
                {
                    "sent": "On the joint distribution, so one that is 20 distributions within, we use the hypothesis space to measure the difference within some some limits on the distance.",
                    "label": 0
                },
                {
                    "sent": "Then the approach proposed will work, but if you see the difference too high then you're is very hard to bound the difference when you actually do the modeling.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Into speaker.",
                    "label": 0
                },
                {
                    "sent": "Again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}