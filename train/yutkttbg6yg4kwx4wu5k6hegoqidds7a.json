{
    "id": "yutkttbg6yg4kwx4wu5k6hegoqidds7a",
    "title": "Convex Optimization",
    "info": {
        "author": [
            "Lieven Vandenberghe, Electrical Engineering Department, University of California, Los Angeles, UCLA"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Mathematics->Optimization"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_vandenberghe_co/",
    "segmentation": [
        [
            "This will be 2 tutorial lectures on convex optimization.",
            "Most of the material in these lectures is developed together with Stephen Boyd at Stanford as part of a book, and then also as part of.",
            "Their slides from different courses at UCLA and then also Stevens courses at Stanford.",
            "I'd like to start with.",
            "Some general."
        ],
        [
            "Production to explain why convex optimization and convexity is so important and optimization?",
            "And.",
            "I think everyone is familiar with the general."
        ],
        [
            "Mathematical optimization problem of minimizing some nonlinear function of N variables subject to several non linear inequality constraints.",
            "This we call this a mathematical optimization problem or nonlinear optimization problem.",
            "It's extremely general, it's a general model of could be any design problem or a decision problem or estimation problem.",
            "It's extremely general, it's almost universal.",
            "Inapplicability at least in theory and practice.",
            "Of course, there are limit."
        ],
        [
            "As to what you can do with mathematical optimization, and basically they are determined by the answers to these two questions.",
            "1st, it's only a mathematical model of some actual optimization problem, so it's only useful if the mathematical model is sufficiently accurate.",
            "Anne.",
            "Also, very important optimizations the accuracy of the data that describe the model or that are needed in the model.",
            "So because in practice often there is substantial uncertainty in the parameters that describe the model, and that's since we in this optimization we optimize.",
            "Assuming certain parameters, any uncertainty in the problem can have can make the answer very sensitive to small changes in the data.",
            "And then the second, of course, is.",
            "It's one thing to write down a mathematical problem.",
            "It's another to solve it.",
            "So the question is, can we actually solve the problem by existing methods?",
            "And then that leads to to sort of main activities and optimization research.",
            "One is what people in optimization called model."
        ],
        [
            "And there we are trying to formulate generic methods for formulating optimization problems in applications.",
            "For example, generic methods for dealing with uncertainty an optimization problems not just in machine learning, but also control or circuit design.",
            "And I think of course this research on algorithms that tries to expand the class of problems that can be handled.",
            "Scale or the complexity of the problems that can be handled in practice.",
            "And convexity is important because it's a central concept in sort of modeling and algorithms.",
            "So we see that roughly speaking, the convex optimization problems are, roughly speaking, the optimization problems that can be handled efficiently that are tractable.",
            "And at the same time, there is quite general, and it's a class of problems sufficiently rich to be useful in practice.",
            "So that's the reason why convexity is such an important topic.",
            "The, as I mentioned, the general optimization problem unfortunately is intractable, so we don't know algorithms for solving the general nonlinear optimization problem, and that's unfortunate, but it's just a fact.",
            "And at least a few for people who are new to this area.",
            "It's often true that even simple optimization problems.",
            "Can be very hard to solve.",
            "And two examples of that are, for example, quadratic optimization.",
            "So we know that if a quadratic optimization problem with no constraints, it's very easy to solve this at the derivatives equal to 0 and you get a set of linear equations that you can solve to get cancer.",
            "As soon as you add constraints, quadratic optimization, quadratic constraints, it becomes much harder with one inequality constraint or one constraint can still be solved efficiently.",
            "It's more involved, but it can still be solved efficiently.",
            "But as soon as you have several constraints, it becomes very difficult to solve.",
            "Another example is polynomial optimization, so if we're just interested in minimizing and polynomial in one variable, then we can just take the derivative, which is also polynomial, compute the roots of the derivative, and find the one that defines the optimum.",
            "If as soon as I have more than several variables, it becomes also very difficult to minimize a polynomial.",
            "So in general, the optimization T general nonlinear problem is difficult to solve.",
            "There is one of all known and."
        ],
        [
            "Famous exception that's the linear programming problem.",
            "It's a special case when the objective and all the constraints are linear.",
            "It was introduced by.",
            "Danzig, or come see formulated and practical methods for solving it in the 1940s, known as the Simplex method.",
            "And then once that method or that algorithm became widely known, people started to use linear programming for modeling.",
            "Different applications, not just in operations research which were the applications that don't.",
            "They had in mind initially, but also many other applications that.",
            "Go beyond the applications he."
        ],
        [
            "Intended initially in engineering and finance and so on.",
            "So it's a very easy problem to solve in a theoretical sense.",
            "There exist polynomial time methods for solving it.",
            "Also, in a practical sense.",
            "In the sense that it's easy to find good software for linear programming, there's free and very good software for solving linear optimization problems.",
            "There's also extensive theory that relates to duality of linear programming.",
            "One important point, there is no closed form expression in general.",
            "But that's not really a problem because we can solve it efficiently in practice using numerical methods, and so people some methods have can be proved to have polynomial complexity, and in practice people also solve linear programs with several 100 thousands or even millions of variables and constraints.",
            "So convex optimization."
        ],
        [
            "Is more general, so here we have a non linear cost function and linear constraints.",
            "But we restrict the problem too.",
            "Objective and constraint functions that are convex that satisfies this inequality.",
            "That I'll say more about in a few minutes.",
            "So this includes linear programming as a special case if all the constraints are linear or the functions are linear and the properties are very similar to linear programming.",
            "So the algorithms are very similar.",
            "Complexity is similar.",
            "But it's also much more general and many more problems can be written in this form than using linear programming."
        ],
        [
            "The history a brief history.",
            "So it all started with linear programming in the 1940s.",
            "Then in the 1950s, for awhile, the development followed on more less predictable path.",
            "So first extension that people consider those quadratic programming.",
            "And which we replace the linear objectives with the quadratic function.",
            "In a quadratic program, you keep the constraints linear, and the reason why that's the first problem that people looked at as an extension is that the simplex method for linear programming exploits the polyhedral structure of the constraints.",
            "And that's very so that's related to the fact that they are linear inequalities.",
            "Few replace the linear inequalities by non linear or quadratic inequalities then.",
            "Simplex method becomes.",
            "It's difficult to generalize the simplex method.",
            "In the 1960s or some other extension, it's smaller, but also fits in this development, known as geometric programming, that I'll introduce later.",
            "And then for awhile not much happened in this area of convex optimization.",
            "But then at the beginning of the 1990s there was a burst of."
        ],
        [
            "Activity and people looked at.",
            "Several extensions are similar to linear and quadratic programming and then known as semi definite and 2nd order cone programming.",
            "Also, quadratically constrained quadratic programming, so this problem if you also replace the constraints with quadratic and so on.",
            "And I will see what happened around 1990 to trigger this activity.",
            "We see the same application so many new applications were discovered since the 1990s, so in control theory semidefinite programming was used extensively starting around 1990 geometric programming found new applications in circuit design.",
            "Then in machine learning, I think everyone knows about support vector machines and quadratic programming.",
            "Also, L1 norm optimization, which is a form of convex optimization, is now widely used for sparse signal reconstruction and for actually solving very difficult combinatorial optimization problems.",
            "And the list goes on.",
            "There are applications in computer vision and finance and so on.",
            "So again, most of this happened after 1990.",
            "And the reason why people became more interested in convex optimization around 1990 was the development of."
        ],
        [
            "Interior point methods for convex optimization.",
            "And these were not new methods that were used for linear programming since Karmarkar's famous algorithm for linear programming in 1984.",
            "But then later around 1990, Necromorphs key extended linear interior point methods for linear programming to general nonlinear convex optimization.",
            "And that also made these problems more easy to solve.",
            "It also.",
            "Motivated people to look at convex optimization as an extension of linear programming because the methods that were used.",
            "These interior point methods feast or very similar to linear programming methods.",
            "So then also motivated people to look.",
            "For applications and to try to use convex optimization for modeling different applications as an extension of linear programming.",
            "So in algorithms, most of the research in the 1990s focused on interior point methods.",
            "Since the last five or ten years.",
            "There's also increased interest in convex optimization and what people call a first order methods.",
            "So they're basically.",
            "Methods are similar to the basic gradient descent method, so the most elementary algorithm for optimization.",
            "But people have formulated new types of grading methods with better convergence properties.",
            "And I'll discuss them at the end of the next lecture.",
            "So they're useful in very large scale applications because there.",
            "Much easier to explain, much easier to implement, and much less expensive than interior Point methods.",
            "So that's the introduction.",
            "So in these two lectures try to cover."
        ],
        [
            "So basic theory about convex sets and functions on basic definitions.",
            "Then we look at these program classes.",
            "This newer problem classes that were developed since the 1990s.",
            "And then also look at some developments in algorithms at least since 1990.",
            "So I'll discuss interior point methods and then these fast gradient methods.",
            "So today will probably get 2 second order cone programming problems and the rest will do tomorrow.",
            "20 questions."
        ],
        [
            "So I start with a very quick introduction to the theory of convex optimization.",
            "So we start with convex sets.",
            "So a surface convex if all line segments defined by points in the."
        ],
        [
            "Is are included in this set, so this is a convex set.",
            "This is not convex because there is a line segment that is not included in the set.",
            "This is also not convex because the line segment defined by two corner points of this square also not entirely included.",
            "So that's the definition, and that's the mathematical description of a line segment between two points X1 and X2."
        ],
        [
            "So these are some common examples that will encounter, so the solution set of a set of linear equations is always convex.",
            "It's actually more than convex, it's all fine.",
            "And it's not just every line segment defined by two points, but the entire line defined by any two points in the set is included in an affine set.",
            "A set of linear inequalities defines a convex set known as a polyhedron.",
            "And I'll use this notation with this inequality to denote componentwise inequality between vectors.",
            "So this is a component inequality between a vector X and a vector B.",
            "A normal, so the set of vectors X with norm lesson given under R is always convex, and that's true for any norm that's follows from the definition of norms.",
            "And there is a related set known as a norm cone where we look at vectors XT where X is some vector in RNT is a scalar.",
            "And Norm Cone is defined as this point 60.",
            "Where the normal vector is less than or equal to T. So that's a cone because every positive multiple of XT that lies in this set is also in this set.",
            "And you can also show it's convex if this is a norm and that's true for any type of norm.",
            "And we also encountered a set of positive semidefinite matrices, and that's our notation.",
            "So as in.",
            "And denotes the order of the matrices.",
            "Plus stands for positive semidefinite, and then I'll use again this generalized inequality for in the case of a matrix will denote positive semidefinite knus.",
            "So this means that the symmetric matrix is positive semidefinite.",
            "And that's also a convex set.",
            "And then there are some simple properties that you can use to show that or to derive convex sets from simpler convex sets.",
            "So if you take the image of a convex set under a linear transformation, you obtain a convex set.",
            "And that's easy to see because line segments are mapped to line segments by linear transformation.",
            "That's true for the inverse image under linear transformation.",
            "And also the intersection of any number of convex sets is convex and that's follows directly from the definition.",
            "If you think about the definition.",
            "And that's a very useful property.",
            "Allows you, for example, to immediately see that this set is convex without even applying the definition.",
            "So here I define a set C of."
        ],
        [
            "As in RN.",
            "And the vectors the components of X are the coefficients of a cosine polynomial of order.",
            "So X1, cosine T, X2, cosine, 2 T and so on.",
            "So every every vector X defines a course on polynomial and this shows three.",
            "It's not very visible, but this shows 3.",
            "Of those cosine polynomials, for three choices of X.",
            "Then I define a set C as the polynomials or the coefficients of polynomials for which this polynomial is between one and negative one on the interval of T between 0 and \u03c0 / 3.",
            "So these three examples satisfy this property.",
            "So the corresponding coefficient vectors are in C. Well, this C is always convex and that follows from the OR is easily seen from the intersection property, because if you look at this condition for a fixed T. Then the condition that P of T is between one and negative one defines two linear inequalities in the coefficients X.",
            "If T is fixed.",
            "So it defines two parallel half spaces.",
            "And the solution set for a fixed is actually a slab between two parallel half spaces.",
            "And then in this definition we define X or the set C as the intersection of infinitely infinitely many of those convex slabs.",
            "So this is convex because it's intersection of.",
            "In this case infinitely many convex sets.",
            "And so, in an example for R2, it looks like the intersection of all those spaces.",
            "And clearly it's convex."
        ],
        [
            "So that's all I'll say about convex sets, so then next can define convex functions.",
            "So function F is convex if first of all its domain is convex.",
            "So the set of points X where it's defined as convex.",
            "And on its domain it satisfies this inequality known as Jensen's inequality.",
            "So this says that if you take two points X or Y.",
            "Then on the line segment, if you consider the function on the line segment defined by XLI, then the graph of the function is below the linear interpolation between the function values at X&Y.",
            "So the graph of the function lies below this linear segment.",
            "So function that satisfies this is convex and if minus F satisfies it's, the function is concave.",
            "So we can also relate this to the theory of convex sets, so the epigraph of a function F in general is defined as a set in effectors XD.",
            "So in dimension the dimension one higher than the domain of F. Anna Point XT is in the epigraph if F of X is less than or equal to T, so it's the graph of the function and everything above the graph.",
            "That's called the epigraph order function, so this is the epigraph of function that's obviously not convex.",
            "But for convex functions we have the property that the epigraph is also convex if and only if.",
            "So that relates the theory of convex sets with convex functions.",
            "There is a related definition of sublevel set of a function F. Is the set of all vectors X that have a function value less than or equal to a given number?",
            "And then convex functions have the property that all their sublevel sets are convex.",
            "But the converse is not true.",
            "So function can have convex sublevel sets, but it's not necessarily convex."
        ],
        [
            "So these are some basic examples or examples that will encounter of convex functions for functions of 1 variable.",
            "You can just look at the graph of the function and see their convex.",
            "For example the exponential.",
            "Or minus log X is a convex or convex functions.",
            "Certain powers of X are convex, so X to the power of 5A is greater than one and you restrict X to the positive real axis.",
            "For example, some negative powers, for example 1 / X is convex for positive X.",
            "This is a very useful function of quadratic over linear, so if you take X transpose X.",
            "Divided by T, where T is restricted to be positive.",
            "Then this is a convex function of ex ante joining ex ante.",
            "The geometric mean of N vectors is concave.",
            "Log of the determinant of a positive definite matrix is a concave function that will encounter.",
            "The log of a sum of exponentials is convex.",
            "Norms are always convex, linear and affine functions are always convex, and so on."
        ],
        [
            "For differentiable functions, there are some.",
            "Other characterizations so function is twice differentiable, then its Hessian is always positive semidefinite, so the Hessian is a symmetric matrix with the partial second partial derivatives, that's always.",
            "Positive semidefinite.",
            "So that's probably the best known characterization of convex functions.",
            "But it assumes that F is differentiable, twice differentiable, and.",
            "For a function, you can also.",
            "There's also property that only uses the first derivative, so if the gradients with the gradient I mean the vector of the first partial derivatives.",
            "So the gradient of any function F defines a linear approximation, local linear approximation around X.",
            "So if you evaluate at some point X the function and its gradient, then the.",
            "This straight line, the 1st order approximation is a local linear approximation to the function value is a function is convex.",
            "Then this approximation also has a property that it's a lower bound on the function value everywhere.",
            "And not just a local approximation.",
            "And that's if and only if, if the function is differentiable.",
            "Then, in practice, if you try to use."
        ],
        [
            "Convex formulations and applications.",
            "It's useful to have sort of a set of techniques for.",
            "Proving convexity or easily establishing convexity of functions.",
            "And so there are several techniques we can use.",
            "So one is we can just use a definition that works.",
            "Chances inequality that works in.",
            "Some cases, in some cases it's quite complicated.",
            "Can use for differentiable twice differentiable functions.",
            "You can check that the Hessian is positive semidefinite.",
            "But also that can be quite painful if.",
            "This session is complicated.",
            "And there's also a set of techniques that often allow you to easily show convexity.",
            "From by showing that the function is derived from simpler functions, for example, the list that I showed two slides ago and some basic calculus rules for that preserve convexity.",
            "So I'll go through these different rules.",
            "With some examples.",
            "So the first one is quite straightforward.",
            "If F is convex, then a positive or a negative multiple is convex or a sum of two convex functions."
        ],
        [
            "Convex.",
            "Or if you have a function that's convex and replace its argument with a linear mapping of some variable, then the resulting function is convex in X.",
            "So in some examples of this are for example the norm of X + B is always convex.",
            "And that's true for any norm, because norms are convex and then we use this third property.",
            "This function that's known as a logarithmic barrier function in linear programming is convex because minus log X is convex.",
            "Here we replace the argument of the function by linear or non function.",
            "So that gives us a convex function.",
            "And then we add these functions for.",
            "I want to, so that's automatically convex, right?",
            "You don't need to worry about the Hessian or the derivatives to establish convexity.",
            "This is one of the most."
        ],
        [
            "Full of these calculus rules.",
            "A pointwise maximum of a set of convex functions as convex.",
            "So if F1 through FM are all convex in X.",
            "And I define F of X as a new function that has the maximum of this function values.",
            "Then that new function is convex.",
            "And this is an example of this.",
            "Suppose I define for a vector in RN.",
            "A function as a sum of the R largest components in X.",
            "So it's easy to compute it so well defined function of XI can compute it by sorting the components of X and descending order.",
            "And I'm just adding the R leading coefficients so it's easy to compute.",
            "It's well defined function of X.",
            "And it's also convex, and that's easy to see from this maximum rule, because I can also write it as a maximum of a very large number of linear functions of X.",
            "So I take all groups of R subsets of coefficients of X for each subset that take the sum.",
            "And then if I take the maximum of all these some stand, I'll be the maximum this some of the our largest values, right?",
            "That's not a practical rule expression for computing F of X because it's a maximum of a very large number of linear functions.",
            "But it shows that it's convex, right?",
            "So the maximum component of F is convex to some of the largest two, and so on.",
            "This."
        ],
        [
            "Also extends to an infinite maximization.",
            "So here we have a finite maximum over M coefficients.",
            "And then so in.",
            "Our notation will use a super supremum for a maximization over sets infinite or possibly infinite, where it's not specified.",
            "So here I define a function G of X.",
            "As in so.",
            "Take a function F of two variables X&Y.",
            "And F has a property that for fixed why the function is convex in X.",
            "And then I define a function I maximize over Y, subject to some constraints.",
            "And that gives me a function of XG of X.",
            "And.",
            "Well, this function G has a property that GS convex if if has this property that it's convex in X for fixed, why?",
            "Right?",
            "So that's the extension of this maximization rule.",
            "In the previous example we just had while I was just an index that runs from 1:00 to NI, will take a maximum over a finite set, right?",
            "But it's true in general.",
            "This doesn't depend on the set a, so they could be non convex or even discrete or unbounded.",
            "There's no assumptions about A and there are no assumptions about how this function F depends on why it doesn't have to be convex, or there's no assumption on how it depends on why the only property is that for fixed, why has to be convex in X.",
            "So again, that's very useful because in some cases that allows us to immediately conclude that the function is convex just by from the definition.",
            "For example, if I define a function of matrix symmetric matrix X as the maximum eigenvalue of X.",
            "Then that's quite complicated.",
            "You could compute F of X by, for example, computing all the eigenvalues and taking the maximum one.",
            "It's also defined if you think of the definition of the maximum eigenvalue in terms of the characteristic polynomial of matrix X.",
            "Then that would mean that you.",
            "Consider the characteristic polynomial of X.",
            "That's a polynomial.",
            "And it's depends on all the coefficients in X.",
            "And then here we actually are interested in largest root of that polynomial.",
            "So if you look at that as a function of the entries in X, that certainly is a quite complicated function of X.",
            "But this convection can easily see it from another definition.",
            "That's from linear algebra.",
            "So the maximum market value of a matrix X is also the maximum of the Y transpose, XY maximized over all vectors with Euclidean norm equal to 1.",
            "That's an expression from linear algebra.",
            "And if you know this fact, then it's actually easy to see that this is convex by applying this supremum rule, because if you fix Y in this expression, then you get a linear function of X, right?",
            "It's just a quadratic form.",
            "Usually we look at functions of this form.",
            "And why is the variable and X is given?",
            "And then of course it's quadratic, but here if I fix why and look at this as a function of X of all the entries in X, then it's obviously linear.",
            "And linear functions are convex so.",
            "Soup over Y and it gives us a convex function of X.",
            "So in this example, why is the set A is just unit sphere in RN?",
            "An extra few composition rules that allow you to, for example."
        ],
        [
            "Clue that functions like the exponential of G of X is convex if GS convex and it follows from the properties of G and then convexity or concavity properties of 1st function H and monotonicity properties.",
            "So that's basically we have two rules.",
            "If G is convex and H is convex and nondecreasing than the result is convex.",
            "Or she could be concave and H is convex and increasing.",
            "Then the result is convex.",
            "And that's not difficult to show from the definition.",
            "And there is one sort of tricky point here.",
            "If we define nondecreasing or nonincreasing.",
            "And the function H is not defined for all X, so it has a domain that's not all X.",
            "Then implicitly have to assume that the function H is Infinity is infinite outside the domain.",
            "And then non decreasing and increasing.",
            "Assume that it's infinite.",
            "So for example 1 / X for positive X would be a decreasing function because for negative X we assume it's plus Infinity.",
            "Um?",
            "So these are some typical examples.",
            "Annex"
        ],
        [
            "Ends to vector composition, so you could have a factor from an H of more than one variable and then different functions GI, and then you could have combinations of these theorems where H is.",
            "Decreasing and some components and increasing and others, and then the GK also are convex or concave.",
            "So for example, the sum of the logarithm of G of X is concave.",
            "If the functions G are positive and concave, that follows from these rules.",
            "Log of the son of the exponential of G of X is convex if the functions GI are convex.",
            "And that follows from the fact that log sum of X is a convex function.",
            "That's one of the examples I gave, and then you apply one of these composition rules, and this is a complicated function, but it's often used as a smooth.",
            "Approximation of the maximum of these functions GI because log sum of Xbox several variables.",
            "Is a smooth function and it's often used as a smooth approximation for the maximum.",
            "Then there's two more I think.",
            "The next one is a minimization rule that looks very similar to the supremum property that I gave.",
            "Three slides ago, so in this case again we start with a function of two variables X&Y.",
            "We minimize over the second variable and that defines a new function G of X.",
            "And in the previous case we had maximization of the function of two variables.",
            "Are we maximized over the second one?",
            "So the difference here is that this function F. So under certain conditions, resulting function G is convex.",
            "And the properties are much more restrictive than in the other case, so here F must be a convex function jointly in X&Y.",
            "And in the other case, in this maximization rule, there were no assumptions on how it depends on why or fixed, why had to be convex in X.",
            "Here it's convex jointly in X or Y.",
            "And also the set of which we minimize is must be a convex set.",
            "So if that's true, then the resulting function here is convex.",
            "So an example is for example this the distance to a convex set is always convex, and that's true in any norm.",
            "To see this, we just choose this, apply this property so we can write the distance as you take the distance of X2 point.",
            "Why in the set using any any particular norm?",
            "So that's always convex, because norms are convex.",
            "And then that's just a linear combination of two XY.",
            "So this is a convex function jointly in XY.",
            "And then how do we minimize over why so if Y is CS convex, then this is convex, so the distance to convex set is always a convex function.",
            "And that's true in any norm and also for any set as long as it's convex.",
            "Another application of this that's very useful is.",
            "In sensitivity analysis.",
            "For example, suppose we consider.",
            "Here we consider a linear program an.",
            "Why is variable?",
            "So we have a linear objective C transpose Y.",
            "The inner product of CNY.",
            "And then constraints on why.",
            "With right hand side X.",
            "So the constraints on why that are that.",
            "Ay the vector ay is componentwise less than X.",
            "So that's a linear program in Y.",
            "And this is the optimal value.",
            "Of the linear program, but if you minimize over widen, this is the optimal value.",
            "And I can define a new function of X that takes as its value the optimal value of the LP as a function of the right hand side X.",
            "And in many applications, it's interesting to know how the optimal value of an optimization problem as an LP varies with the parameters.",
            "For example, the right hand sides.",
            "Because, for example, this could be resource constraints as in some.",
            "Application and I'm very interested in how well this function looks like.",
            "If we change the upper limits on resources.",
            "How quickly does the?",
            "Optimal value function change.",
            "So one thing we can say, of course we can always compute this function by solving the LP forgiven X.",
            "We can solve this to find G of X.",
            "But we also know that it's a convex function of X.",
            "Another going follows from the minimization rule, and to apply this we have to define F. So in this case, F of XY would be just C transpose Y.",
            "And as its domain of the function, we take the pairs XY that satisfy the inequality.",
            "And outside the domain we define a function F plus Infinity.",
            "So this is a convex function jointly an XL.",
            "Why?",
            "Because its domain is convex, so that's just linear linear inequalities in XY.",
            "So man is convex an on its domain, it's linear.",
            "So it's also convex, so this function is convex jointly in XY.",
            "And then here we minimize over Y.",
            "With no constraints, actually an online, so the result is always necessarily convex in the remaining variable X, right?",
            "Another last proper."
        ],
        [
            "This.",
            "Called perspective.",
            "So suppose I have a convex function of F of XF of X.",
            "And then I can define a new function G of an X and an extra variable T. By replacing X in the argument of FBX over T. And then multiplying effetti.",
            "And it's called the perspective of the function F. And it has the nice property that if F is convex.",
            "And I restrict this to positive values of T. Then this perspective is always convex.",
            "And that, for example, allows us to immediately see that this quadratic over linear function that I gave as an example, is convex.",
            "Because I take F of X just accessible sex, that's convex, fairy place X over by X / T. And multiply with T and just get expose X / T four positive T, so that's convex.",
            "Another example from information theory is negative entropy.",
            "The negative logarithm if I use.",
            "F of X.",
            "Minus log X is a convex function of X, so if I use this apply this construction, they replace X over by X / T and I multiply the result with T, then that gives us this function.",
            "T times log of T / X.",
            "And so because of this, that's convex, jointly SMT.",
            "So that's called the relative entropy.",
            "And it's not immediately obvious because the first term is convex, T log T is convex because that's the entropy, But the second term separately has no interesting convexity properties, but a combination of the two is convex.",
            "Because of this, 'cause it's the perspective of a convex function.",
            "So."
        ],
        [
            "That's all I wanted to say in terms of theory of convex sets and functions.",
            "Are there any questions about this?",
            "So then next we'll look at some problems, definitions and examples of convex optimization problems.",
            "So we'll define a convex optimization problem is a function of the problem with the convex objective convex inequality constraints and linear equality constraints.",
            "So it's the general mathematical optimization problem that I started with, but with the F is restricted to be convex.",
            "So obviously the feasible set is convex cause it's the intersection of an affine set solution of the linear equations and."
        ],
        [
            "Sublevel sets of different convex functions so it's convex set.",
            "You can also easily show that locally optimal points unnecessarily globally optimal.",
            "And so the most important property of this is that it's easy to solve in theory.",
            "Terms of complexity theory, but also in practice because there exists more and more."
        ],
        [
            "We obtained software for solving optimization and convex optimization problems.",
            "So the.",
            "Simplest example is an LP.",
            "If we restrict all the inequality constraints to linear.",
            "The solution set looks like this is a polyhedron defined by different inequalities.",
            "And then the constraint is the objective is to minimize C transpose X.",
            "So you can easily see that the optimal value or the optimal solution will be at boundary of the polyhedron.",
            "And it's typically at one of these extreme points of support here and.",
            "Any kind of visualizer solution by just considering?",
            "A level occurs or sets of constant value of C. Transpose X + D. So these level curves are hyperplanes with normal vector minus C. So those are the sets of constant objective value.",
            "The decrease in the direction of minus C. And then we look look for the point in the polyhedron with the smallest value of C. Transpose X.",
            "So in this case, that's the optimum.",
            "So."
        ],
        [
            "In practice, it's not too difficult to recognize convex functions.",
            "So if linear optimization linear programming problems in practice, so typically there arise from if they're not immediately or obviously LP's, they're typically come from piecewise linear functions.",
            "For example, if I want to minimize a function F of X that's defined as the maximum pointwise maximum of M linear functions of X.",
            "Then we already know that's convex because each of the functions is convex.",
            "It's not differentiable, so it's not.",
            "Immediately obvious how to solve it using gradient descent or or Newton's method, but it is the sold as an LP.",
            "By using a very common trick, we introduce a new variable T and they will minimize T. This new variable subject to the constraint that AI transpose experts by is less than T for each eye.",
            "And that's an LP and two variables X&T, because the objective is linear, all the constraints are linear.",
            "It's easy to see that it's equivalent.",
            "And.",
            "This for example, the easiest way to see this is to in this problem, if you fix X.",
            "And you just optimize over T. And that's a very simple LP, because there's only one variable.",
            "You minimize the subject to M lower bounds on TI, so obviously the answer is to take T equal to the largest of those lower bounds, and that's this maximum.",
            "So for fixty the optimal values is expression, and if you're allowed to optimize over X anti jointly then you choose that value of T and Forex.",
            "You choose the optimum of this problem.",
            "So there are some."
        ],
        [
            "Applications in machine learning that you probably all know about that are very well known.",
            "So basically, if we want to separate so the basic one basic problem is if you want to linearly separate two sets of points in RN.",
            "Dark circles and the open circles.",
            "And we define a hyperplane or accepting separating hyperplane like this.",
            "So we look for coefficient vector A and a constant B that takes for, which is a find function, takes positive values on the axis and negative values on the rise.",
            "Then that's a strict set of a set of strict linear inequalities.",
            "With variables A&B.",
            "And the exercise and wires are known, so this is a set of linear inequalities and AMB.",
            "It's not strict, but you can easily.",
            "It's not a strict inequality, but you can easily make it into a non strict inequality.",
            "By noting that this is homogeneous or if AMB satisfy these conditions that any positive multiple of AMB satisfy them.",
            "So you can also make this right hand side equal to 1 or negative one and then just replace it with a non strict inequality.",
            "And then you have a set of linear inequalities in A&B.",
            "Any solution if it's solvable, defines a separating hyperplane.",
            "As an extension, you can look at problems where."
        ],
        [
            "Two sets are not separable.",
            "And then try to define an approximate separating hyperplane.",
            "That separates most of the sets points X&Y, so one formulation would be the following, so this is a piecewise linear optimization problem in the same variables A&B.",
            "So the first time you look at the Max of zero and then for each point XI the slack 1 -- A transpose XI minus BI.",
            "So if XI lies on the correct side of the hyperplane.",
            "That's zero.",
            "And then there's Max is zero and there is no contribution to this sum.",
            "If X is on the negative on the wrong side of the hyperplane, if it's misclassified by that hyperplane, then we add a penalty equal to the slack.",
            "In the inequality.",
            "And then we do the same with the other set of points and we use this as a penalty on the separating hyperplane that we can minimize to find an approximate separating hyperplane.",
            "Now that's obviously piecewise linear because it's Max of linear functions of a B, and it can easily write it as an LP using the same trick as before.",
            "Yeah.",
            "So the one is because I used one in the right hand side here.",
            "At the one doesn't really.",
            "Any positive number will be equally good here.",
            "Boys so if I scaled."
        ],
        [
            "One, then the A&B would scale homogeneously.",
            "Sorry, but there's only there's no other term in this objective, so if.",
            "A scale one, then A&B.",
            "If A&B are up."
        ],
        [
            "Golden A&B will be scaled.",
            "From that's because he redifined.",
            "Separation of strict inequality.",
            "So if A&B satisfy this then it can always scale A and B2."
        ],
        [
            "Greater than any positive number on the right, and if it separates it with right hand side one and negative one, then it obviously satisfies.",
            "This would be different if I define separation by a non strict inequality.",
            "Here an I just don't strict inequality and then also exclude 0-AB as trivial solution.",
            "Then this step wouldn't be equivalent.",
            "So two other common."
        ],
        [
            "Elements that are result in LP's are Infinity and one norm optimization.",
            "So if I take the one norm of X minus be with the one norm is defined as the sum of the absolute values, then that can be written as an LP again by introducing extra variables and Huawei affected Y and then linear inequalities.",
            "And again it's easy to see that they are equivalent because if you fix X and this problem and this LP.",
            "Then you get an easy problem and why?",
            "Because these are componentwise inequalities, so on each component why I there is 2 bounds lower bounds?",
            "On why I an upper bound on minus Yi?",
            "So to combine these inequality, say that why I is greater than the absolute value of the IT component of X -- B?",
            "So that component is between Y and minus Y, so that's an upper bound on the absolute value.",
            "This objective function is separable, so we can minimize over each iy separately.",
            "Because the constraints are also and coupled, they don't couple the different entries and why.",
            "So the answer for this problem fixed X will be to choose why I equal to the absolute value of the right component of X -- B.",
            "If X is fixed.",
            "And then so that's the known as one norm.",
            "And then if you're allowed to minimize over X&Y, then you choose this, make the same choice for awhile, and for actual data points with a minimum one node.",
            "So on the right hand side you're minimizing over X&Y, yeah?",
            "So here we minimize over X.",
            "That's a nonlinear but convex optimization problem.",
            "Here I introduce a new vector variable Y, and actually that should be MFAN is here the role dimension of a?",
            "And I minimize jointly over XY, so that makes this into an LP because the objective is linear in all those variables and the constraints are also linear.",
            "They're just linear inequalities in X&Y.",
            "There's a similar trick, actually.",
            "It's easier for the Chebyshev norm or the Infinity norm if I'm interested in minimizing the maximum absolute value of X -- B.",
            "Then this would be the equivalent and equivalent LP.",
            "So here the extra variable introduces scalar instead of a vector.",
            "And then the constraints are that.",
            "So the one here, the BF one is a vector of ones.",
            "So each constraint here says that the components of X -- B must be less than Y.",
            "Each component of X -- B.",
            "And greater than minus Y for each eye.",
            "So that means that the absolute value of the components of X -- B are bounded by Y.",
            "And again to see the equivalence, we can fix X here and then we minimize over the scalar variable Y.",
            "That's a very easy optimization problem because you just have a bunch of lower bounds on Y and you take Y equal to the largest lower bound, and in this case the largest lower bound is the maximum absolute value of the entries of X -- B, right?",
            "And then if you minimize joint deal for X&Y, you get something equivalent to this.",
            "So that's."
        ],
        [
            "A few examples for linear programming.",
            "So the next extension down is quadratic programming, and that's something else goes back to the 50s.",
            "So here we keep the same types of constraints, linear inequalities and equalities, and we take a convex quadratic cost function of X.",
            "So P is a symmetric matrix and its positive semidefinite, so it's the Hessian of this quadratic function.",
            "So the constraints that are still polyhedral, as in the LP case.",
            "But level curves of F have changed there now.",
            "FS quadratic function, so these are... And then, depending where the midpoint, the center of these... or the unconstrained minimum of the objective function is, we find an optimum that can be on the boundary.",
            "Can be on the corner point or it can be in the interior of the polyhedron if that's the unconstrained minimum.",
            "So that's a different linear programming.",
            "So very."
        ],
        [
            "Um?",
            "Example that actually motivated the development of quadratic programming in the 50s.",
            "Is a linear program it today, but people call linear programming with uncertainty or a robust linear program.",
            "So this comes up investment.",
            "Science, so this was a basic markovits portfolio problem.",
            "So in any case, we have an LP.",
            "We assume that data and the constraints are exactly known, so G&H are exactly given, but we assume that C is uncertain.",
            "The objective isn't certain.",
            "So and we model it as a random variable.",
            "Then obviously that makes C transpose X the objective function and random variable.",
            "And then we have to define what we mean by minimizing a random variable of X random variable.",
            "That depends on X.",
            "Well, if C has mean seebaran covariance Sigma.",
            "Then we'll have a tradeoff between the expected value of C transpose X, which is C bar transpose X, and the variance X transpose SEMA X.",
            "So an investment I will be the expected.",
            "We minimize so we would minimize expected loss, so we mean would be the expected loss and the variance would be the risk on the portfolio.",
            "And there is a tradeoff between these two by just minimizing the expected value of C transpose X, we might have a solution with high variance.",
            "That's very sensitive to changes in C. So one way to model this is to look at the linear convex combination or a weighted combination of the two objectives.",
            "So suppose we minimize the combination of weighted sum of the expected optimal value, expected objective function and the variance.",
            "Extensible Sigma X with a weighting parameters that expresses our risk aversion right?",
            "So if, is 0, then we are don't care about the risk or the variance in the solution.",
            "If we are very.",
            "Risk averse that we would choose a larger value of gamma and put a higher penalty on high variance.",
            "But for positive, any positive value of X, this is a convex quadratic function of X, so this is a quadratic programming problem.",
            "And it's one of the first applications of quadratic programming.",
            "From the 50s."
        ],
        [
            "Another famous one is just to continue this finger discrimination example from linear programming.",
            "So again, if the solution if you have a two sets of points that are separable by these two hyperplanes, then we can also try to find that in general there are multiple supporting or separating hyperplanes, but we can try to find the one that maximizes the distance between these two sets.",
            "Or the convex holes of these two sets.",
            "And the distances between these two hyperplanes is given by two over the Euclidean norm of a.",
            "That's the distance between this hyperplane that defines.",
            "Separates the first set and the second one.",
            "So if you want to maximize the distance you want to minimize the Euclidean norm of a or the square of the Euclidean norm of A and that gives us a quadratic programming problem.",
            "Because again, as before, the variables are A&B, the exercise and the Y eyes are given, so we have linear inequalities in A&B and a convex quadratic function of.",
            "And then you can define a combine the two week and if."
        ],
        [
            "Find combined this penalty on misclassification that we used for as an example of linear programming and then this maximum margin objective and look at the linear combination or a weighted sum of the two and minimize this.",
            "Function of A&B.",
            "And then again, you can easily write it as a quadratic programming problem, and that's the support vector classifier.",
            "Which one or two?",
            "Yeah, that's a very good.",
            "So that's the distance between these two."
        ],
        [
            "Arthur paints the.",
            "So if you define a different distance.",
            "Then in general, you would take the so we can look at the convex Hull of this first set of points and the other point, and then they find the distance in any norm and that defined that as a margin between the two sets.",
            "And then in general you can do this, but you get a different norm.",
            "I think you get the dual norm here instead of the.",
            "Get a problem that relates to the dual norm of the normally choose to define the distance so it's not quite, uh, it won't be a QP, but still convex.",
            "So in other words, you can maximize the distance of the convex Hull between these two sets, and you know.",
            "Or computer distance and then from the distance computers."
        ],
        [
            "Creating hyperlink on this picture you don't have any points between the margin.",
            "Yeah, so here I define that there's assume they're separable.",
            "And I define the margin.",
            "And the next thing we have these two hyperplanes, and they're not separable, so that we take this objective function.",
            "So this would be a sort of some kind of convex penalty on the misclassification, and this term maximizes the margin between the two.",
            "These two are dash type of lens.",
            "So C4 if you increase the penalty.",
            "You put more weight on the margin and increase the margin between the hyperplanes.",
            "So another."
        ],
        [
            "Important application is in of 1 norms, an Infinity norms?",
            "One norms and is in single reconstruction, so this is just one example.",
            "Suppose we take a signal of length 1000 and we know it's sparse, so here's a signal with 10 nonzero components.",
            "And of course, unknown positions and also the range is changed, but there are 10 zero components and the rest are zero.",
            "And then the question is, can we reconstruct this signal from a small number of measurements, for example 100 measurements?",
            "Model, like this linear measurements so we have an exact signal X hat.",
            "We make linear measurements.",
            "X gives us a vector X hat.",
            "We add random noise and it gives us 100 measurements.",
            "And then from this 100 measurements on this 1000 dimensional Vector X.",
            "The question is, can we reconstruct this signal by exploiting the fact that we know that it's sparse?",
            "So here in this experiment we generate a randomly, so that's actually important.",
            "Just from a caution distribution and then the noise is also random.",
            "So one common, so that's an underdetermined set of equations, so we have only 100 conditions on X and we try to estimate the signal of dimension 1000.",
            "So in general you need a regularization term to make that well post.",
            "So one popular regularization would be to add the Euclidean norm to take the two norm of X -- B."
        ],
        [
            "The error in the.",
            "Reconstruction and put a penalty on the size of X.",
            "And the most common choice would be the two norm that gives you so called Tikhonov regularization, but obviously that doesn't work because you get a solution at small everywhere.",
            "And minimize this.",
            "This convex combination of the two, so it doesn't of course reconstruct the exact signal.",
            "However, if you use the one norm as regularization and you take this."
        ],
        [
            "Trade off between the error and X model.",
            "The estimated X and the measurements be and I put a penalty on the one norm instead of the two norm.",
            "Then in this case it reconstructs the signal perfectly, so that's the general property of 1 norms that it also.",
            "Um and.",
            "So in the optimization problem with penalty on the one or more constraint on the one norm will encourage the solution to be sparse many zero components.",
            "And that's very useful in applications like this.",
            "We have 1000.",
            "You have 1000 plane.",
            "Most of them are zero and just several of them are are not zero.",
            "And then you have just 100 points from which you want to reconstruct these non 0.",
            "Wooden 0 signal yeah, but if you have like probably looks to me like it's quite probable that all these 100 points will be zero.",
            "Well, it depends.",
            "That's why it's important that a is.",
            "It depends on a also.",
            "So here I generate a all the entries of a are randomly generated, so it's a random matrix.",
            "So for example, sorry.",
            "So A is the measurement, so you have a signal."
        ],
        [
            "Length one thousands and suppose you have a linear measurement on X at linear.",
            "You observe a linear combination of the components of X.",
            "For example, another choice of a would be to pick 100 random components of X and that would work.",
            "Had a problem that you describe, because then most of the components will be 0 and your reconstruction will be 0.",
            "So here it works.",
            "For example, for randomly generated a with caution distribution because all the non zero entries on the signal will be mixed in the measurement.",
            "Another choice would be for example take a DfT and then pick random.",
            "Random subset of the DfT of X, right?",
            "So this is known in the sparse reconstruction, so people have actually very interesting theory that characterizes the types of matrices where this works, and water properties are of a that make it work, and it's certainly not true for any.",
            "Matrix a.",
            "But so in cases like this, it allows you to use a convex.",
            "Technique to solve a problem that actually would be very difficult to solve otherwise because one.",
            "Combinatorial way of solving this problem would be to just consider different subsets of X and then try to find the best reconstruction with different sparse different subsets of X to find the minimal subset, right?",
            "So those are some examples of quadratic programming problems, then maybe very quickly we can.",
            "An interesting set of problems.",
            "Maybe it's less important here is geometric programming.",
            "So in a geometric program, you look at different nonlinear.",
            "Optimization problems like this, so we have a variable X that."
        ],
        [
            "Restricted to be positive.",
            "And each of these functions as a so-called polynomial function of X opposing normal is defined as the.",
            "Function like this so it's a sum.",
            "Of powers of X products of powers of X.",
            "So the powers of the exponents of the each X can be any real number can be positive, negative.",
            "They don't have to be integers.",
            "And the coefficients have to be positive, so that's called a polynomial.",
            "So it looks a little bit like a polynomial, but of course the powers here can be positive, negative and non integer.",
            "And also a restrict X to be positive so all the components of expensive positive.",
            "So that's called a geometric program and it's not convex, but there is a simple transformation that makes it convex."
        ],
        [
            "So if you use instead of X, the logarithm of X is variable.",
            "Then it transforms into a convex problem.",
            "Because this would be the result and it involves these functions that we've seen are convex."
        ],
        [
            "So to finish this section, I'd like to say something about modeling software, so we've seen that.",
            "Often convex functions can be derived from simpler convex functions using some calculus rules from convex analysis.",
            "For example, this maximization rule, etc.",
            "And often formulating a problem in a standard form, for example in linear program or a quadratic program involves some transformations.",
            "You have to introduce new variables.",
            "You have to add extra constraints, do some transformations.",
            "And recently soft more software has become available to automate some of these tasks as called modeling software.",
            "So there are some packages in Matlab, for example CVS annual map that make it much easier to formulate convex optimization problems in practice.",
            "And they automate actually two tasks that.",
            "Or needed to apply this in practice one is you have to recognize complexity and verify it from these calculus rules.",
            "For example, maximization or.",
            "Linear transformations, and Secondly these packages will transform your problem in an input format that's required by optimization solvers.",
            "For example, in LP solver.",
            "So for example, if you have a piecewise linear maximization problem than a modeling language will do the transformation for you, and you don't have to transform the maximization into a maximization problem into an LP.",
            "So it's easier to explain with an example.",
            "So suppose we minimize it, then one norm of X -- B subject to appeal lower bounds on the variables."
        ],
        [
            "It's a very simple.",
            "Problem we can easily write it as an LP by introducing a new variable and moving this to the constraints.",
            "But that's not necessary.",
            "Becausw can also be automated, and this is what the code in Matlab would be for CX, which is a model modeling package.",
            "So you describe to write generating A&B and everything between these two statements, civics begin and civic send.",
            "Is specific to CVX.",
            "So first we define a variable of dimension 3.",
            "Then we give the objective and the natural form is just say minimize the one norm of X -- B and mop notation.",
            "And then we give the constraints.",
            "So this is a componentwise inequality of a vector, and so on.",
            "And then after you execute this statement than CVS will transform this into an LP caller solver and linear programming solver and then.",
            "Export results to the variables over defining the problem.",
            "So inside these two statements, X is in CVS variable.",
            "After completion the problem will be solved and X will have the values of the optimal solution.",
            "And this is just a basic optimization problem, but you can do this includes predefined functions for very wide variety of convex functions.",
            "So.",
            "Examples."
        ],
        [
            "Example that you gave you slides back on the trying to you know the probabilistic programming one would be in the variances in trade off that example.",
            "I mean one thing that I'd be interested in is.",
            "Instead of, you know, training off the meaning of variance of each.",
            "To know what can I find a variable for my model so that you know the probability of something happening have something in it?",
            "An outlier happening is really really small.",
            "Can you say something about?",
            "You know this idea?",
            "You know using you know, I guess it can open density functions or something.",
            "You know if I mean what conditions that could be solved you know formulated as a comics optimization problem?",
            "Yeah, actually, that's one of the examples.",
            "I'll come in a few slides, so that's called.",
            "Chance constraints you want to put a limit on the probability that a constraint is violated.",
            "And in general, that can be very difficult except in special cases.",
            "Depending on the density and constraints.",
            "But we will actually see one practical examples where it can be.",
            "So then so the next topic is cone programming and will continue this tomorrow.",
            "So Cone programming is general.",
            "As defined like this, and it's a general format for convex optimization that has become very popular since the early 1990s.",
            "And it basically makes any convex optimization problem look like a linear program, so it just takes a linear objective.",
            "Linear equality, constraints, linear inequalities.",
            "And the only difference is they replace the standard inequality between vectors."
        ],
        [
            "By a generalized inequality.",
            "And generalized generalized inequality.",
            "I mean that I define a cone K convex cone K. And then this inequality means that H -- G X lies in the cone K. So if I take forcada nonnegative orthant.",
            "Then this is just another way of saying that the constraints hold componentwise right 'cause componentwise H -- G, X lies in a negative audit.",
            "But if you replace the negative order by other convex cones, you get interesting.",
            "Convex optimization problems that are extend linear programming.",
            "So that's called called linear programming.",
            "As popular as a standard format because it makes the general nonlinear problem look like an LP.",
            "And then it's important in the also theory and algorithms becausw.",
            "It turns out that duality theory, for example, is almost exactly the same As for linear programming.",
            "And also the algorithms for linear programming, at least interior point methods are very similar or can be extended to this case.",
            "So it's a convex set.",
            "That's also a cone, so if you take an element in the cone and all non negative multiples of that element or in the entire half line through that element is in the code.",
            "So we see actually this is a very general format, but will be interested in two specific cases, or three if you include the nonnegative organs.",
            "So the first one is the 2nd order cone, so this is."
        ],
        [
            "Cone that's called a second order cone.",
            "So the 2nd order cone is defined like this in general, so you have P. It's a, it's a cone.",
            "In RP then you look at the sub vector of the first P -- 1 entries.",
            "And then that vector Y is in the cone if the Euclidean norm of the first P -- 1 entries is less than the last entry.",
            "So the 2nd order cone in R3 looks like this is also known as the ice cream cone, so.",
            "Three vector, three variable three vector wise in the cone.",
            "If Y-3 is greater than the Euclidean norm of one and two.",
            "So for each Y three we have this constraint defines disk.",
            "Of points this could radius Y 3 and then if you increase Y 3, the radius of the disk increases that goes from zero to Infinity, so it's a convex set, that's example.",
            "It's easy to see that it's convex.",
            "It's also a cone because it includes the origin and an older lines.",
            "Through any points in the cone.",
            "The constraint function is not differentiable.",
            "Normal.",
            "Yeah, so the norm.",
            "So this is also the graph of the Norman in R2, so the norm is non differentiable at the origin.",
            "It's differentiable everywhere else, but if the argument of enormous zero then that's not differentiable.",
            "So this is a second order cone program and sort of the abstract way of viewing it is as a cone program and say, well, look at linear inequalities with respect to this cone.",
            "But at all actually means something that's very straightforward.",
            "It just means that Euclidean norm of a linear function of X.",
            "Is less than a scalar linear function of X.",
            "And that's a second order constraint.",
            "So it means that.",
            "If you look at it geometrically that this.",
            "Image of X and it is linear.",
            "Mapping lies in the 2nd order code.",
            "But to appreciate a constraint, you can also just look at it like this so it's a convex function of X, because that's a convex function of X.",
            "It says are equal to a linear function of X and it's not differentiable, but it's convex.",
            "It's known as an SCP.",
            "And includes linear programming.",
            "For example, if you take A and B0 or the dimension of this first: zero, that is just a set of linear inequalities.",
            "So these are some basic examples that actually can be written in that form.",
            "First, if you have a general quadratic constraint with opposed."
        ],
        [
            "This definite matrix A then you can easily write it as a second order cone constraint by factoring A.",
            "And this extends to semi definite and singular a, but it gets a little more complicated.",
            "It also works for certain quadratic inequality's that are actually indefinite.",
            "So here we have a vector X.",
            "And two scalar variables Y&Z that are non negative and a constraint is that the square of the norm of X is less than the product of Y&Z.",
            "So it's called a hyperbolic constraint.",
            "Ann, it's if your account that's a quadratic function of these variables XY and Z, But the Hessian is actually indefinite if you bring this to the left hand side and you workout the Hessian of the constraints indefinite as a negative eigenvalue.",
            "But it defines a convex set and they can be written as a second order cone constraint like this.",
            "If you workout this norm squared this norm and square the right hand side, you'll see it's equivalent to this.",
            "We call 2nd order cone representable.",
            "Constraints that can be written in this form by the correct choice of ABC and D. And the second refers to the user to do exactly, so the one norm."
        ],
        [
            "One norm corn program would be something like this with one note.",
            "So basically what this is saying is that we can.",
            "2nd Order cone representable constraints are constraint sets.",
            "That can be interpreted as different slices of 2nd order cones and different dimensions.",
            "So if you look at this in higher dimensions then intersected with affine sets, then depending on your choice of ABC and D, then depending on how you choose this Efron sets.",
            "You can actually create or obtain different convex intersections.",
            "And this would be one example where you have this nonlinear constraints and."
        ],
        [
            "It's equivalent to a second order cone constraint."
        ],
        [
            "Certain powers of X.",
            "For example, if they X to the power 1.5 lisanti restricted to non negative X. Gambrinus hyperbolic constraints like this.",
            "And you can easily verify this inequality.",
            "So here we introduce an extra variable Z.",
            "And this says that she is less than square root of X.",
            "And then if you plug it in here, you get the first constraint.",
            "So then you can convert these hyperbolic constraints and the 2nd order cone constraints by using the trick of the previous page.",
            "And this actually works for any rational P greater than one.",
            "So instead of 1.5 you could take any P as long as it's rational and then by introducing new variables you can write it as a second order cone problem.",
            "And also some negative powers as long as they're rational.",
            "And these are the types of things that are modeling language would do for you.",
            "The modeling language would accept constraints like this and then make these conversions for you.",
            "So this is actually the extension that of this risk."
        ],
        [
            "Expected value tradeoff problem in linear programming.",
            "So in general.",
            "We can consider what's called a robust linear program.",
            "So and here is a stochastic formulation of robust linear program.",
            "You call it stochastic because we assume here that Azar uncertain.",
            "For simplicity, assume that C&B are given.",
            "And we modeled as random variables with some distribution.",
            "In this case normal distribution with given mean and given covariance.",
            "And then we have to decide what we mean by the constraint.",
            "If the coefficient is uncertain or random.",
            "So here we specify on certain minimum probability of satisfying the constraint.",
            "So you say access feasible for this robust version of the LP if it satisfies each constraint with probability at least at a, for example 90%.",
            "So that's called a chance constrained and stochastic optimization.",
            "And the question is, can we actually solve this?",
            "Guy will call this probability and then handle it is at a convex constraint on X or not.",
            "And it's actually interesting to look at before we look at the expressions to look at the convex set of this robust LP for different choices of ETA.",
            "So we assume that A is a normal random vector with certain mean and covariance.",
            "Suppose we pick it as 50%.",
            "Then I will look at one constraint inequality, A transpose X -- B.",
            "So if we require that the constraint is satisfied with probability 50%.",
            "Then it just means that X satisfies the constraint with a equal to the mean of AI.",
            "Because a satisfies the constraint for the average AI.",
            "Then for half of the cases, half of the coefficient vectors will be outside the feasible set, and for half of the insights will be exactly satisfied with the 50%.",
            "So for 50% the solution set of this robust LP is exactly the solution set for the LP with the normal or the mean with a replaced by the mean.",
            "If we increase at a, we require that excess satisfy each constraint with probability higher than 50%.",
            "Then the solution such shrinks to something smaller.",
            "Because if you pick an X and near the boundary of the original feasible set, then with high probability the AES random.",
            "A transpose X will be outside will be greater than be.",
            "I'll be infeasible so you have to stay away from the boundary, and it turns out that that's actually gives you a non polyhedral but convex set X.",
            "On the other hand, if you look at an 8 or less than 50% and it turns out that the set is non convex even if it is very close to 50%.",
            "And the set is larger than the normal case.",
            "And the reason is that now we are happy with X satisfying the constraint with low probability.",
            "And then it turns out that you can actually obtain a solution, so it looks like this, and it's nonconvex, and in general, this problem with eight or 10% would be very difficult to solve.",
            "Because there are many local Optima.",
            "For example, if C points in this direction.",
            "Then these two are local optimal points and it's in general very difficult to find the corner point that gives you the global optimum.",
            "Whereas here is convex.",
            "I will see it in a second order constraint.",
            "So here is the probability constraint with the single ADA.",
            "Is it straightforward to generalize to Ada I for each?",
            "Yeah, that's still sit for us, so here it's a different error.",
            "Or at a holds for each constraint separately, so it's not a constraint of satisfying all the constraint jointly.",
            "It's constant for each other, but you could have a different data in each.",
            "As long as they are created at 50%.",
            "Yeah.",
            "Do I explicitly on the fact that is normal?",
            "It does actually.",
            "This is for a normal distribution.",
            "This.",
            "You know with the mean and everything less.",
            "Being a larger everything you know larger than 50%.",
            "Being a smaller set, wouldn't that apply to any distribution that's still true for many of its symmetric and so on symmetric distribution.",
            "Yeah, it's more complicated.",
            "So in general these chance constraint is actually very difficult except for very simple distribute like the normal distribution and linear constraints.",
            "So there they become intractable in general.",
            "But I can just quickly say why it's convex.",
            "So basically just workout."
        ],
        [
            "The probability for each constraint.",
            "So a is normal, so we can easily workout in terms of the cumulative density of ocean water prob."
        ],
        [
            "That is of exceeding X.",
            "And you get an expression like this."
        ],
        [
            "So you get the normal so X satisfy this with probability at a.",
            "If this constraint is satisfied, so a bar is the normal value of AI, Sigma was a covariance, fires a cumulative density of the Gaussian unit variance.",
            "Caution, caution.",
            "And then the reason why after 50% is important is that the coefficient in front of the norm here is the inverse of inverse applied to enter.",
            "So it's the inverse of the cumulative density, so forever exceeding 50%.",
            "It's a positive number.",
            "For A to less than 50%, it's a negative value is far innovator.",
            "So if it's greater than 50%, you get a positive coefficient in front of the norm and you get a second order cone problem.",
            "If it's less than 50%, you will have a negative coefficient here and then it makes constraints a non convex because that's a non convex function.",
            "Yeah.",
            "So.",
            "Is it obvious that?"
        ],
        [
            "Yes, because the objective is still linear.",
            "So, for example, suppose at the points up, then the lines of constant C transpose X would be horizontal lines.",
            "And any decrease C transpose X as much as you can.",
            "So in that case that would be the optimum.",
            "But it can easily see if you change the orientation of C, then the solution will jump to other corner points, and often there could be local minima.",
            "Let's say 50 points this way, for example then.",
            "You know you have you have cases where two points are actually local optimal, a neighborhood, their minima.",
            "But another global minima.",
            "Even if the curvature is very weak.",
            "So we should stop here, but this is the reason why this is an SCP and this works for a crossing distribution.",
            "For linear constraints, and that's.",
            "So it makes it a convex problem.",
            "But in general these chance constraints are very difficult, and then people actually replace them by convex constraints are weaker, but guarantee this probability, but are more conservative and there's a lot of theory about those formulations.",
            "Change the constraints.",
            "People also thought about.",
            "It's not normal to approximate the distribution by normal and then applied.",
            "In theory that seems to work, yeah?",
            "And also there are ways to actually just upper bound this.",
            "Or looking at different types of constraints and different distributions to find constraints that actually stronger and guarantee this that are convex and easy to.",
            "Handling a convex problem.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This will be 2 tutorial lectures on convex optimization.",
                    "label": 1
                },
                {
                    "sent": "Most of the material in these lectures is developed together with Stephen Boyd at Stanford as part of a book, and then also as part of.",
                    "label": 0
                },
                {
                    "sent": "Their slides from different courses at UCLA and then also Stevens courses at Stanford.",
                    "label": 0
                },
                {
                    "sent": "I'd like to start with.",
                    "label": 0
                },
                {
                    "sent": "Some general.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Production to explain why convex optimization and convexity is so important and optimization?",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I think everyone is familiar with the general.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mathematical optimization problem of minimizing some nonlinear function of N variables subject to several non linear inequality constraints.",
                    "label": 0
                },
                {
                    "sent": "This we call this a mathematical optimization problem or nonlinear optimization problem.",
                    "label": 1
                },
                {
                    "sent": "It's extremely general, it's a general model of could be any design problem or a decision problem or estimation problem.",
                    "label": 1
                },
                {
                    "sent": "It's extremely general, it's almost universal.",
                    "label": 0
                },
                {
                    "sent": "Inapplicability at least in theory and practice.",
                    "label": 0
                },
                {
                    "sent": "Of course, there are limit.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As to what you can do with mathematical optimization, and basically they are determined by the answers to these two questions.",
                    "label": 1
                },
                {
                    "sent": "1st, it's only a mathematical model of some actual optimization problem, so it's only useful if the mathematical model is sufficiently accurate.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Also, very important optimizations the accuracy of the data that describe the model or that are needed in the model.",
                    "label": 0
                },
                {
                    "sent": "So because in practice often there is substantial uncertainty in the parameters that describe the model, and that's since we in this optimization we optimize.",
                    "label": 1
                },
                {
                    "sent": "Assuming certain parameters, any uncertainty in the problem can have can make the answer very sensitive to small changes in the data.",
                    "label": 0
                },
                {
                    "sent": "And then the second, of course, is.",
                    "label": 0
                },
                {
                    "sent": "It's one thing to write down a mathematical problem.",
                    "label": 0
                },
                {
                    "sent": "It's another to solve it.",
                    "label": 1
                },
                {
                    "sent": "So the question is, can we actually solve the problem by existing methods?",
                    "label": 0
                },
                {
                    "sent": "And then that leads to to sort of main activities and optimization research.",
                    "label": 0
                },
                {
                    "sent": "One is what people in optimization called model.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there we are trying to formulate generic methods for formulating optimization problems in applications.",
                    "label": 0
                },
                {
                    "sent": "For example, generic methods for dealing with uncertainty an optimization problems not just in machine learning, but also control or circuit design.",
                    "label": 0
                },
                {
                    "sent": "And I think of course this research on algorithms that tries to expand the class of problems that can be handled.",
                    "label": 0
                },
                {
                    "sent": "Scale or the complexity of the problems that can be handled in practice.",
                    "label": 0
                },
                {
                    "sent": "And convexity is important because it's a central concept in sort of modeling and algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we see that roughly speaking, the convex optimization problems are, roughly speaking, the optimization problems that can be handled efficiently that are tractable.",
                    "label": 0
                },
                {
                    "sent": "And at the same time, there is quite general, and it's a class of problems sufficiently rich to be useful in practice.",
                    "label": 0
                },
                {
                    "sent": "So that's the reason why convexity is such an important topic.",
                    "label": 0
                },
                {
                    "sent": "The, as I mentioned, the general optimization problem unfortunately is intractable, so we don't know algorithms for solving the general nonlinear optimization problem, and that's unfortunate, but it's just a fact.",
                    "label": 1
                },
                {
                    "sent": "And at least a few for people who are new to this area.",
                    "label": 0
                },
                {
                    "sent": "It's often true that even simple optimization problems.",
                    "label": 1
                },
                {
                    "sent": "Can be very hard to solve.",
                    "label": 0
                },
                {
                    "sent": "And two examples of that are, for example, quadratic optimization.",
                    "label": 0
                },
                {
                    "sent": "So we know that if a quadratic optimization problem with no constraints, it's very easy to solve this at the derivatives equal to 0 and you get a set of linear equations that you can solve to get cancer.",
                    "label": 0
                },
                {
                    "sent": "As soon as you add constraints, quadratic optimization, quadratic constraints, it becomes much harder with one inequality constraint or one constraint can still be solved efficiently.",
                    "label": 0
                },
                {
                    "sent": "It's more involved, but it can still be solved efficiently.",
                    "label": 0
                },
                {
                    "sent": "But as soon as you have several constraints, it becomes very difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "Another example is polynomial optimization, so if we're just interested in minimizing and polynomial in one variable, then we can just take the derivative, which is also polynomial, compute the roots of the derivative, and find the one that defines the optimum.",
                    "label": 0
                },
                {
                    "sent": "If as soon as I have more than several variables, it becomes also very difficult to minimize a polynomial.",
                    "label": 0
                },
                {
                    "sent": "So in general, the optimization T general nonlinear problem is difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "There is one of all known and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Famous exception that's the linear programming problem.",
                    "label": 1
                },
                {
                    "sent": "It's a special case when the objective and all the constraints are linear.",
                    "label": 0
                },
                {
                    "sent": "It was introduced by.",
                    "label": 1
                },
                {
                    "sent": "Danzig, or come see formulated and practical methods for solving it in the 1940s, known as the Simplex method.",
                    "label": 0
                },
                {
                    "sent": "And then once that method or that algorithm became widely known, people started to use linear programming for modeling.",
                    "label": 1
                },
                {
                    "sent": "Different applications, not just in operations research which were the applications that don't.",
                    "label": 0
                },
                {
                    "sent": "They had in mind initially, but also many other applications that.",
                    "label": 0
                },
                {
                    "sent": "Go beyond the applications he.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Intended initially in engineering and finance and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's a very easy problem to solve in a theoretical sense.",
                    "label": 0
                },
                {
                    "sent": "There exist polynomial time methods for solving it.",
                    "label": 1
                },
                {
                    "sent": "Also, in a practical sense.",
                    "label": 0
                },
                {
                    "sent": "In the sense that it's easy to find good software for linear programming, there's free and very good software for solving linear optimization problems.",
                    "label": 1
                },
                {
                    "sent": "There's also extensive theory that relates to duality of linear programming.",
                    "label": 0
                },
                {
                    "sent": "One important point, there is no closed form expression in general.",
                    "label": 0
                },
                {
                    "sent": "But that's not really a problem because we can solve it efficiently in practice using numerical methods, and so people some methods have can be proved to have polynomial complexity, and in practice people also solve linear programs with several 100 thousands or even millions of variables and constraints.",
                    "label": 0
                },
                {
                    "sent": "So convex optimization.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is more general, so here we have a non linear cost function and linear constraints.",
                    "label": 0
                },
                {
                    "sent": "But we restrict the problem too.",
                    "label": 0
                },
                {
                    "sent": "Objective and constraint functions that are convex that satisfies this inequality.",
                    "label": 0
                },
                {
                    "sent": "That I'll say more about in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "So this includes linear programming as a special case if all the constraints are linear or the functions are linear and the properties are very similar to linear programming.",
                    "label": 0
                },
                {
                    "sent": "So the algorithms are very similar.",
                    "label": 0
                },
                {
                    "sent": "Complexity is similar.",
                    "label": 0
                },
                {
                    "sent": "But it's also much more general and many more problems can be written in this form than using linear programming.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The history a brief history.",
                    "label": 0
                },
                {
                    "sent": "So it all started with linear programming in the 1940s.",
                    "label": 1
                },
                {
                    "sent": "Then in the 1950s, for awhile, the development followed on more less predictable path.",
                    "label": 0
                },
                {
                    "sent": "So first extension that people consider those quadratic programming.",
                    "label": 1
                },
                {
                    "sent": "And which we replace the linear objectives with the quadratic function.",
                    "label": 0
                },
                {
                    "sent": "In a quadratic program, you keep the constraints linear, and the reason why that's the first problem that people looked at as an extension is that the simplex method for linear programming exploits the polyhedral structure of the constraints.",
                    "label": 0
                },
                {
                    "sent": "And that's very so that's related to the fact that they are linear inequalities.",
                    "label": 0
                },
                {
                    "sent": "Few replace the linear inequalities by non linear or quadratic inequalities then.",
                    "label": 0
                },
                {
                    "sent": "Simplex method becomes.",
                    "label": 1
                },
                {
                    "sent": "It's difficult to generalize the simplex method.",
                    "label": 0
                },
                {
                    "sent": "In the 1960s or some other extension, it's smaller, but also fits in this development, known as geometric programming, that I'll introduce later.",
                    "label": 0
                },
                {
                    "sent": "And then for awhile not much happened in this area of convex optimization.",
                    "label": 0
                },
                {
                    "sent": "But then at the beginning of the 1990s there was a burst of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Activity and people looked at.",
                    "label": 0
                },
                {
                    "sent": "Several extensions are similar to linear and quadratic programming and then known as semi definite and 2nd order cone programming.",
                    "label": 0
                },
                {
                    "sent": "Also, quadratically constrained quadratic programming, so this problem if you also replace the constraints with quadratic and so on.",
                    "label": 0
                },
                {
                    "sent": "And I will see what happened around 1990 to trigger this activity.",
                    "label": 0
                },
                {
                    "sent": "We see the same application so many new applications were discovered since the 1990s, so in control theory semidefinite programming was used extensively starting around 1990 geometric programming found new applications in circuit design.",
                    "label": 1
                },
                {
                    "sent": "Then in machine learning, I think everyone knows about support vector machines and quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "Also, L1 norm optimization, which is a form of convex optimization, is now widely used for sparse signal reconstruction and for actually solving very difficult combinatorial optimization problems.",
                    "label": 0
                },
                {
                    "sent": "And the list goes on.",
                    "label": 0
                },
                {
                    "sent": "There are applications in computer vision and finance and so on.",
                    "label": 0
                },
                {
                    "sent": "So again, most of this happened after 1990.",
                    "label": 0
                },
                {
                    "sent": "And the reason why people became more interested in convex optimization around 1990 was the development of.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interior point methods for convex optimization.",
                    "label": 0
                },
                {
                    "sent": "And these were not new methods that were used for linear programming since Karmarkar's famous algorithm for linear programming in 1984.",
                    "label": 0
                },
                {
                    "sent": "But then later around 1990, Necromorphs key extended linear interior point methods for linear programming to general nonlinear convex optimization.",
                    "label": 1
                },
                {
                    "sent": "And that also made these problems more easy to solve.",
                    "label": 0
                },
                {
                    "sent": "It also.",
                    "label": 0
                },
                {
                    "sent": "Motivated people to look at convex optimization as an extension of linear programming because the methods that were used.",
                    "label": 0
                },
                {
                    "sent": "These interior point methods feast or very similar to linear programming methods.",
                    "label": 0
                },
                {
                    "sent": "So then also motivated people to look.",
                    "label": 0
                },
                {
                    "sent": "For applications and to try to use convex optimization for modeling different applications as an extension of linear programming.",
                    "label": 0
                },
                {
                    "sent": "So in algorithms, most of the research in the 1990s focused on interior point methods.",
                    "label": 0
                },
                {
                    "sent": "Since the last five or ten years.",
                    "label": 0
                },
                {
                    "sent": "There's also increased interest in convex optimization and what people call a first order methods.",
                    "label": 0
                },
                {
                    "sent": "So they're basically.",
                    "label": 1
                },
                {
                    "sent": "Methods are similar to the basic gradient descent method, so the most elementary algorithm for optimization.",
                    "label": 0
                },
                {
                    "sent": "But people have formulated new types of grading methods with better convergence properties.",
                    "label": 1
                },
                {
                    "sent": "And I'll discuss them at the end of the next lecture.",
                    "label": 0
                },
                {
                    "sent": "So they're useful in very large scale applications because there.",
                    "label": 0
                },
                {
                    "sent": "Much easier to explain, much easier to implement, and much less expensive than interior Point methods.",
                    "label": 0
                },
                {
                    "sent": "So that's the introduction.",
                    "label": 0
                },
                {
                    "sent": "So in these two lectures try to cover.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basic theory about convex sets and functions on basic definitions.",
                    "label": 1
                },
                {
                    "sent": "Then we look at these program classes.",
                    "label": 0
                },
                {
                    "sent": "This newer problem classes that were developed since the 1990s.",
                    "label": 1
                },
                {
                    "sent": "And then also look at some developments in algorithms at least since 1990.",
                    "label": 0
                },
                {
                    "sent": "So I'll discuss interior point methods and then these fast gradient methods.",
                    "label": 0
                },
                {
                    "sent": "So today will probably get 2 second order cone programming problems and the rest will do tomorrow.",
                    "label": 0
                },
                {
                    "sent": "20 questions.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I start with a very quick introduction to the theory of convex optimization.",
                    "label": 1
                },
                {
                    "sent": "So we start with convex sets.",
                    "label": 1
                },
                {
                    "sent": "So a surface convex if all line segments defined by points in the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is are included in this set, so this is a convex set.",
                    "label": 1
                },
                {
                    "sent": "This is not convex because there is a line segment that is not included in the set.",
                    "label": 1
                },
                {
                    "sent": "This is also not convex because the line segment defined by two corner points of this square also not entirely included.",
                    "label": 1
                },
                {
                    "sent": "So that's the definition, and that's the mathematical description of a line segment between two points X1 and X2.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are some common examples that will encounter, so the solution set of a set of linear equations is always convex.",
                    "label": 1
                },
                {
                    "sent": "It's actually more than convex, it's all fine.",
                    "label": 0
                },
                {
                    "sent": "And it's not just every line segment defined by two points, but the entire line defined by any two points in the set is included in an affine set.",
                    "label": 0
                },
                {
                    "sent": "A set of linear inequalities defines a convex set known as a polyhedron.",
                    "label": 0
                },
                {
                    "sent": "And I'll use this notation with this inequality to denote componentwise inequality between vectors.",
                    "label": 0
                },
                {
                    "sent": "So this is a component inequality between a vector X and a vector B.",
                    "label": 0
                },
                {
                    "sent": "A normal, so the set of vectors X with norm lesson given under R is always convex, and that's true for any norm that's follows from the definition of norms.",
                    "label": 0
                },
                {
                    "sent": "And there is a related set known as a norm cone where we look at vectors XT where X is some vector in RNT is a scalar.",
                    "label": 0
                },
                {
                    "sent": "And Norm Cone is defined as this point 60.",
                    "label": 0
                },
                {
                    "sent": "Where the normal vector is less than or equal to T. So that's a cone because every positive multiple of XT that lies in this set is also in this set.",
                    "label": 0
                },
                {
                    "sent": "And you can also show it's convex if this is a norm and that's true for any type of norm.",
                    "label": 1
                },
                {
                    "sent": "And we also encountered a set of positive semidefinite matrices, and that's our notation.",
                    "label": 0
                },
                {
                    "sent": "So as in.",
                    "label": 0
                },
                {
                    "sent": "And denotes the order of the matrices.",
                    "label": 0
                },
                {
                    "sent": "Plus stands for positive semidefinite, and then I'll use again this generalized inequality for in the case of a matrix will denote positive semidefinite knus.",
                    "label": 0
                },
                {
                    "sent": "So this means that the symmetric matrix is positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "And that's also a convex set.",
                    "label": 0
                },
                {
                    "sent": "And then there are some simple properties that you can use to show that or to derive convex sets from simpler convex sets.",
                    "label": 1
                },
                {
                    "sent": "So if you take the image of a convex set under a linear transformation, you obtain a convex set.",
                    "label": 1
                },
                {
                    "sent": "And that's easy to see because line segments are mapped to line segments by linear transformation.",
                    "label": 1
                },
                {
                    "sent": "That's true for the inverse image under linear transformation.",
                    "label": 0
                },
                {
                    "sent": "And also the intersection of any number of convex sets is convex and that's follows directly from the definition.",
                    "label": 0
                },
                {
                    "sent": "If you think about the definition.",
                    "label": 0
                },
                {
                    "sent": "And that's a very useful property.",
                    "label": 0
                },
                {
                    "sent": "Allows you, for example, to immediately see that this set is convex without even applying the definition.",
                    "label": 0
                },
                {
                    "sent": "So here I define a set C of.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As in RN.",
                    "label": 0
                },
                {
                    "sent": "And the vectors the components of X are the coefficients of a cosine polynomial of order.",
                    "label": 0
                },
                {
                    "sent": "So X1, cosine T, X2, cosine, 2 T and so on.",
                    "label": 1
                },
                {
                    "sent": "So every every vector X defines a course on polynomial and this shows three.",
                    "label": 0
                },
                {
                    "sent": "It's not very visible, but this shows 3.",
                    "label": 0
                },
                {
                    "sent": "Of those cosine polynomials, for three choices of X.",
                    "label": 0
                },
                {
                    "sent": "Then I define a set C as the polynomials or the coefficients of polynomials for which this polynomial is between one and negative one on the interval of T between 0 and \u03c0 / 3.",
                    "label": 0
                },
                {
                    "sent": "So these three examples satisfy this property.",
                    "label": 0
                },
                {
                    "sent": "So the corresponding coefficient vectors are in C. Well, this C is always convex and that follows from the OR is easily seen from the intersection property, because if you look at this condition for a fixed T. Then the condition that P of T is between one and negative one defines two linear inequalities in the coefficients X.",
                    "label": 0
                },
                {
                    "sent": "If T is fixed.",
                    "label": 0
                },
                {
                    "sent": "So it defines two parallel half spaces.",
                    "label": 0
                },
                {
                    "sent": "And the solution set for a fixed is actually a slab between two parallel half spaces.",
                    "label": 0
                },
                {
                    "sent": "And then in this definition we define X or the set C as the intersection of infinitely infinitely many of those convex slabs.",
                    "label": 1
                },
                {
                    "sent": "So this is convex because it's intersection of.",
                    "label": 1
                },
                {
                    "sent": "In this case infinitely many convex sets.",
                    "label": 0
                },
                {
                    "sent": "And so, in an example for R2, it looks like the intersection of all those spaces.",
                    "label": 0
                },
                {
                    "sent": "And clearly it's convex.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's all I'll say about convex sets, so then next can define convex functions.",
                    "label": 0
                },
                {
                    "sent": "So function F is convex if first of all its domain is convex.",
                    "label": 1
                },
                {
                    "sent": "So the set of points X where it's defined as convex.",
                    "label": 0
                },
                {
                    "sent": "And on its domain it satisfies this inequality known as Jensen's inequality.",
                    "label": 0
                },
                {
                    "sent": "So this says that if you take two points X or Y.",
                    "label": 0
                },
                {
                    "sent": "Then on the line segment, if you consider the function on the line segment defined by XLI, then the graph of the function is below the linear interpolation between the function values at X&Y.",
                    "label": 0
                },
                {
                    "sent": "So the graph of the function lies below this linear segment.",
                    "label": 0
                },
                {
                    "sent": "So function that satisfies this is convex and if minus F satisfies it's, the function is concave.",
                    "label": 0
                },
                {
                    "sent": "So we can also relate this to the theory of convex sets, so the epigraph of a function F in general is defined as a set in effectors XD.",
                    "label": 0
                },
                {
                    "sent": "So in dimension the dimension one higher than the domain of F. Anna Point XT is in the epigraph if F of X is less than or equal to T, so it's the graph of the function and everything above the graph.",
                    "label": 0
                },
                {
                    "sent": "That's called the epigraph order function, so this is the epigraph of function that's obviously not convex.",
                    "label": 0
                },
                {
                    "sent": "But for convex functions we have the property that the epigraph is also convex if and only if.",
                    "label": 0
                },
                {
                    "sent": "So that relates the theory of convex sets with convex functions.",
                    "label": 0
                },
                {
                    "sent": "There is a related definition of sublevel set of a function F. Is the set of all vectors X that have a function value less than or equal to a given number?",
                    "label": 0
                },
                {
                    "sent": "And then convex functions have the property that all their sublevel sets are convex.",
                    "label": 0
                },
                {
                    "sent": "But the converse is not true.",
                    "label": 0
                },
                {
                    "sent": "So function can have convex sublevel sets, but it's not necessarily convex.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are some basic examples or examples that will encounter of convex functions for functions of 1 variable.",
                    "label": 0
                },
                {
                    "sent": "You can just look at the graph of the function and see their convex.",
                    "label": 0
                },
                {
                    "sent": "For example the exponential.",
                    "label": 0
                },
                {
                    "sent": "Or minus log X is a convex or convex functions.",
                    "label": 1
                },
                {
                    "sent": "Certain powers of X are convex, so X to the power of 5A is greater than one and you restrict X to the positive real axis.",
                    "label": 0
                },
                {
                    "sent": "For example, some negative powers, for example 1 / X is convex for positive X.",
                    "label": 1
                },
                {
                    "sent": "This is a very useful function of quadratic over linear, so if you take X transpose X.",
                    "label": 0
                },
                {
                    "sent": "Divided by T, where T is restricted to be positive.",
                    "label": 0
                },
                {
                    "sent": "Then this is a convex function of ex ante joining ex ante.",
                    "label": 1
                },
                {
                    "sent": "The geometric mean of N vectors is concave.",
                    "label": 0
                },
                {
                    "sent": "Log of the determinant of a positive definite matrix is a concave function that will encounter.",
                    "label": 0
                },
                {
                    "sent": "The log of a sum of exponentials is convex.",
                    "label": 0
                },
                {
                    "sent": "Norms are always convex, linear and affine functions are always convex, and so on.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For differentiable functions, there are some.",
                    "label": 0
                },
                {
                    "sent": "Other characterizations so function is twice differentiable, then its Hessian is always positive semidefinite, so the Hessian is a symmetric matrix with the partial second partial derivatives, that's always.",
                    "label": 0
                },
                {
                    "sent": "Positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "So that's probably the best known characterization of convex functions.",
                    "label": 0
                },
                {
                    "sent": "But it assumes that F is differentiable, twice differentiable, and.",
                    "label": 1
                },
                {
                    "sent": "For a function, you can also.",
                    "label": 0
                },
                {
                    "sent": "There's also property that only uses the first derivative, so if the gradients with the gradient I mean the vector of the first partial derivatives.",
                    "label": 0
                },
                {
                    "sent": "So the gradient of any function F defines a linear approximation, local linear approximation around X.",
                    "label": 0
                },
                {
                    "sent": "So if you evaluate at some point X the function and its gradient, then the.",
                    "label": 0
                },
                {
                    "sent": "This straight line, the 1st order approximation is a local linear approximation to the function value is a function is convex.",
                    "label": 0
                },
                {
                    "sent": "Then this approximation also has a property that it's a lower bound on the function value everywhere.",
                    "label": 0
                },
                {
                    "sent": "And not just a local approximation.",
                    "label": 0
                },
                {
                    "sent": "And that's if and only if, if the function is differentiable.",
                    "label": 1
                },
                {
                    "sent": "Then, in practice, if you try to use.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Convex formulations and applications.",
                    "label": 0
                },
                {
                    "sent": "It's useful to have sort of a set of techniques for.",
                    "label": 0
                },
                {
                    "sent": "Proving convexity or easily establishing convexity of functions.",
                    "label": 1
                },
                {
                    "sent": "And so there are several techniques we can use.",
                    "label": 0
                },
                {
                    "sent": "So one is we can just use a definition that works.",
                    "label": 0
                },
                {
                    "sent": "Chances inequality that works in.",
                    "label": 0
                },
                {
                    "sent": "Some cases, in some cases it's quite complicated.",
                    "label": 0
                },
                {
                    "sent": "Can use for differentiable twice differentiable functions.",
                    "label": 1
                },
                {
                    "sent": "You can check that the Hessian is positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "But also that can be quite painful if.",
                    "label": 0
                },
                {
                    "sent": "This session is complicated.",
                    "label": 0
                },
                {
                    "sent": "And there's also a set of techniques that often allow you to easily show convexity.",
                    "label": 0
                },
                {
                    "sent": "From by showing that the function is derived from simpler functions, for example, the list that I showed two slides ago and some basic calculus rules for that preserve convexity.",
                    "label": 0
                },
                {
                    "sent": "So I'll go through these different rules.",
                    "label": 0
                },
                {
                    "sent": "With some examples.",
                    "label": 1
                },
                {
                    "sent": "So the first one is quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "If F is convex, then a positive or a negative multiple is convex or a sum of two convex functions.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Convex.",
                    "label": 0
                },
                {
                    "sent": "Or if you have a function that's convex and replace its argument with a linear mapping of some variable, then the resulting function is convex in X.",
                    "label": 1
                },
                {
                    "sent": "So in some examples of this are for example the norm of X + B is always convex.",
                    "label": 1
                },
                {
                    "sent": "And that's true for any norm, because norms are convex and then we use this third property.",
                    "label": 1
                },
                {
                    "sent": "This function that's known as a logarithmic barrier function in linear programming is convex because minus log X is convex.",
                    "label": 0
                },
                {
                    "sent": "Here we replace the argument of the function by linear or non function.",
                    "label": 0
                },
                {
                    "sent": "So that gives us a convex function.",
                    "label": 0
                },
                {
                    "sent": "And then we add these functions for.",
                    "label": 0
                },
                {
                    "sent": "I want to, so that's automatically convex, right?",
                    "label": 0
                },
                {
                    "sent": "You don't need to worry about the Hessian or the derivatives to establish convexity.",
                    "label": 0
                },
                {
                    "sent": "This is one of the most.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Full of these calculus rules.",
                    "label": 0
                },
                {
                    "sent": "A pointwise maximum of a set of convex functions as convex.",
                    "label": 0
                },
                {
                    "sent": "So if F1 through FM are all convex in X.",
                    "label": 1
                },
                {
                    "sent": "And I define F of X as a new function that has the maximum of this function values.",
                    "label": 0
                },
                {
                    "sent": "Then that new function is convex.",
                    "label": 0
                },
                {
                    "sent": "And this is an example of this.",
                    "label": 0
                },
                {
                    "sent": "Suppose I define for a vector in RN.",
                    "label": 0
                },
                {
                    "sent": "A function as a sum of the R largest components in X.",
                    "label": 1
                },
                {
                    "sent": "So it's easy to compute it so well defined function of XI can compute it by sorting the components of X and descending order.",
                    "label": 0
                },
                {
                    "sent": "And I'm just adding the R leading coefficients so it's easy to compute.",
                    "label": 0
                },
                {
                    "sent": "It's well defined function of X.",
                    "label": 0
                },
                {
                    "sent": "And it's also convex, and that's easy to see from this maximum rule, because I can also write it as a maximum of a very large number of linear functions of X.",
                    "label": 0
                },
                {
                    "sent": "So I take all groups of R subsets of coefficients of X for each subset that take the sum.",
                    "label": 0
                },
                {
                    "sent": "And then if I take the maximum of all these some stand, I'll be the maximum this some of the our largest values, right?",
                    "label": 0
                },
                {
                    "sent": "That's not a practical rule expression for computing F of X because it's a maximum of a very large number of linear functions.",
                    "label": 0
                },
                {
                    "sent": "But it shows that it's convex, right?",
                    "label": 0
                },
                {
                    "sent": "So the maximum component of F is convex to some of the largest two, and so on.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also extends to an infinite maximization.",
                    "label": 0
                },
                {
                    "sent": "So here we have a finite maximum over M coefficients.",
                    "label": 0
                },
                {
                    "sent": "And then so in.",
                    "label": 0
                },
                {
                    "sent": "Our notation will use a super supremum for a maximization over sets infinite or possibly infinite, where it's not specified.",
                    "label": 0
                },
                {
                    "sent": "So here I define a function G of X.",
                    "label": 0
                },
                {
                    "sent": "As in so.",
                    "label": 0
                },
                {
                    "sent": "Take a function F of two variables X&Y.",
                    "label": 0
                },
                {
                    "sent": "And F has a property that for fixed why the function is convex in X.",
                    "label": 0
                },
                {
                    "sent": "And then I define a function I maximize over Y, subject to some constraints.",
                    "label": 0
                },
                {
                    "sent": "And that gives me a function of XG of X.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, this function G has a property that GS convex if if has this property that it's convex in X for fixed, why?",
                    "label": 1
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So that's the extension of this maximization rule.",
                    "label": 0
                },
                {
                    "sent": "In the previous example we just had while I was just an index that runs from 1:00 to NI, will take a maximum over a finite set, right?",
                    "label": 0
                },
                {
                    "sent": "But it's true in general.",
                    "label": 0
                },
                {
                    "sent": "This doesn't depend on the set a, so they could be non convex or even discrete or unbounded.",
                    "label": 0
                },
                {
                    "sent": "There's no assumptions about A and there are no assumptions about how this function F depends on why it doesn't have to be convex, or there's no assumption on how it depends on why the only property is that for fixed, why has to be convex in X.",
                    "label": 0
                },
                {
                    "sent": "So again, that's very useful because in some cases that allows us to immediately conclude that the function is convex just by from the definition.",
                    "label": 0
                },
                {
                    "sent": "For example, if I define a function of matrix symmetric matrix X as the maximum eigenvalue of X.",
                    "label": 0
                },
                {
                    "sent": "Then that's quite complicated.",
                    "label": 0
                },
                {
                    "sent": "You could compute F of X by, for example, computing all the eigenvalues and taking the maximum one.",
                    "label": 0
                },
                {
                    "sent": "It's also defined if you think of the definition of the maximum eigenvalue in terms of the characteristic polynomial of matrix X.",
                    "label": 0
                },
                {
                    "sent": "Then that would mean that you.",
                    "label": 0
                },
                {
                    "sent": "Consider the characteristic polynomial of X.",
                    "label": 0
                },
                {
                    "sent": "That's a polynomial.",
                    "label": 0
                },
                {
                    "sent": "And it's depends on all the coefficients in X.",
                    "label": 0
                },
                {
                    "sent": "And then here we actually are interested in largest root of that polynomial.",
                    "label": 0
                },
                {
                    "sent": "So if you look at that as a function of the entries in X, that certainly is a quite complicated function of X.",
                    "label": 0
                },
                {
                    "sent": "But this convection can easily see it from another definition.",
                    "label": 0
                },
                {
                    "sent": "That's from linear algebra.",
                    "label": 0
                },
                {
                    "sent": "So the maximum market value of a matrix X is also the maximum of the Y transpose, XY maximized over all vectors with Euclidean norm equal to 1.",
                    "label": 0
                },
                {
                    "sent": "That's an expression from linear algebra.",
                    "label": 0
                },
                {
                    "sent": "And if you know this fact, then it's actually easy to see that this is convex by applying this supremum rule, because if you fix Y in this expression, then you get a linear function of X, right?",
                    "label": 0
                },
                {
                    "sent": "It's just a quadratic form.",
                    "label": 0
                },
                {
                    "sent": "Usually we look at functions of this form.",
                    "label": 0
                },
                {
                    "sent": "And why is the variable and X is given?",
                    "label": 0
                },
                {
                    "sent": "And then of course it's quadratic, but here if I fix why and look at this as a function of X of all the entries in X, then it's obviously linear.",
                    "label": 0
                },
                {
                    "sent": "And linear functions are convex so.",
                    "label": 0
                },
                {
                    "sent": "Soup over Y and it gives us a convex function of X.",
                    "label": 0
                },
                {
                    "sent": "So in this example, why is the set A is just unit sphere in RN?",
                    "label": 0
                },
                {
                    "sent": "An extra few composition rules that allow you to, for example.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clue that functions like the exponential of G of X is convex if GS convex and it follows from the properties of G and then convexity or concavity properties of 1st function H and monotonicity properties.",
                    "label": 0
                },
                {
                    "sent": "So that's basically we have two rules.",
                    "label": 0
                },
                {
                    "sent": "If G is convex and H is convex and nondecreasing than the result is convex.",
                    "label": 1
                },
                {
                    "sent": "Or she could be concave and H is convex and increasing.",
                    "label": 0
                },
                {
                    "sent": "Then the result is convex.",
                    "label": 0
                },
                {
                    "sent": "And that's not difficult to show from the definition.",
                    "label": 0
                },
                {
                    "sent": "And there is one sort of tricky point here.",
                    "label": 0
                },
                {
                    "sent": "If we define nondecreasing or nonincreasing.",
                    "label": 0
                },
                {
                    "sent": "And the function H is not defined for all X, so it has a domain that's not all X.",
                    "label": 0
                },
                {
                    "sent": "Then implicitly have to assume that the function H is Infinity is infinite outside the domain.",
                    "label": 0
                },
                {
                    "sent": "And then non decreasing and increasing.",
                    "label": 0
                },
                {
                    "sent": "Assume that it's infinite.",
                    "label": 0
                },
                {
                    "sent": "So for example 1 / X for positive X would be a decreasing function because for negative X we assume it's plus Infinity.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So these are some typical examples.",
                    "label": 0
                },
                {
                    "sent": "Annex",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ends to vector composition, so you could have a factor from an H of more than one variable and then different functions GI, and then you could have combinations of these theorems where H is.",
                    "label": 0
                },
                {
                    "sent": "Decreasing and some components and increasing and others, and then the GK also are convex or concave.",
                    "label": 0
                },
                {
                    "sent": "So for example, the sum of the logarithm of G of X is concave.",
                    "label": 0
                },
                {
                    "sent": "If the functions G are positive and concave, that follows from these rules.",
                    "label": 0
                },
                {
                    "sent": "Log of the son of the exponential of G of X is convex if the functions GI are convex.",
                    "label": 1
                },
                {
                    "sent": "And that follows from the fact that log sum of X is a convex function.",
                    "label": 0
                },
                {
                    "sent": "That's one of the examples I gave, and then you apply one of these composition rules, and this is a complicated function, but it's often used as a smooth.",
                    "label": 0
                },
                {
                    "sent": "Approximation of the maximum of these functions GI because log sum of Xbox several variables.",
                    "label": 0
                },
                {
                    "sent": "Is a smooth function and it's often used as a smooth approximation for the maximum.",
                    "label": 0
                },
                {
                    "sent": "Then there's two more I think.",
                    "label": 0
                },
                {
                    "sent": "The next one is a minimization rule that looks very similar to the supremum property that I gave.",
                    "label": 0
                },
                {
                    "sent": "Three slides ago, so in this case again we start with a function of two variables X&Y.",
                    "label": 0
                },
                {
                    "sent": "We minimize over the second variable and that defines a new function G of X.",
                    "label": 0
                },
                {
                    "sent": "And in the previous case we had maximization of the function of two variables.",
                    "label": 0
                },
                {
                    "sent": "Are we maximized over the second one?",
                    "label": 0
                },
                {
                    "sent": "So the difference here is that this function F. So under certain conditions, resulting function G is convex.",
                    "label": 0
                },
                {
                    "sent": "And the properties are much more restrictive than in the other case, so here F must be a convex function jointly in X&Y.",
                    "label": 0
                },
                {
                    "sent": "And in the other case, in this maximization rule, there were no assumptions on how it depends on why or fixed, why had to be convex in X.",
                    "label": 0
                },
                {
                    "sent": "Here it's convex jointly in X or Y.",
                    "label": 0
                },
                {
                    "sent": "And also the set of which we minimize is must be a convex set.",
                    "label": 0
                },
                {
                    "sent": "So if that's true, then the resulting function here is convex.",
                    "label": 0
                },
                {
                    "sent": "So an example is for example this the distance to a convex set is always convex, and that's true in any norm.",
                    "label": 0
                },
                {
                    "sent": "To see this, we just choose this, apply this property so we can write the distance as you take the distance of X2 point.",
                    "label": 0
                },
                {
                    "sent": "Why in the set using any any particular norm?",
                    "label": 0
                },
                {
                    "sent": "So that's always convex, because norms are convex.",
                    "label": 0
                },
                {
                    "sent": "And then that's just a linear combination of two XY.",
                    "label": 0
                },
                {
                    "sent": "So this is a convex function jointly in XY.",
                    "label": 0
                },
                {
                    "sent": "And then how do we minimize over why so if Y is CS convex, then this is convex, so the distance to convex set is always a convex function.",
                    "label": 0
                },
                {
                    "sent": "And that's true in any norm and also for any set as long as it's convex.",
                    "label": 0
                },
                {
                    "sent": "Another application of this that's very useful is.",
                    "label": 0
                },
                {
                    "sent": "In sensitivity analysis.",
                    "label": 0
                },
                {
                    "sent": "For example, suppose we consider.",
                    "label": 0
                },
                {
                    "sent": "Here we consider a linear program an.",
                    "label": 0
                },
                {
                    "sent": "Why is variable?",
                    "label": 0
                },
                {
                    "sent": "So we have a linear objective C transpose Y.",
                    "label": 0
                },
                {
                    "sent": "The inner product of CNY.",
                    "label": 0
                },
                {
                    "sent": "And then constraints on why.",
                    "label": 0
                },
                {
                    "sent": "With right hand side X.",
                    "label": 0
                },
                {
                    "sent": "So the constraints on why that are that.",
                    "label": 0
                },
                {
                    "sent": "Ay the vector ay is componentwise less than X.",
                    "label": 0
                },
                {
                    "sent": "So that's a linear program in Y.",
                    "label": 0
                },
                {
                    "sent": "And this is the optimal value.",
                    "label": 0
                },
                {
                    "sent": "Of the linear program, but if you minimize over widen, this is the optimal value.",
                    "label": 0
                },
                {
                    "sent": "And I can define a new function of X that takes as its value the optimal value of the LP as a function of the right hand side X.",
                    "label": 0
                },
                {
                    "sent": "And in many applications, it's interesting to know how the optimal value of an optimization problem as an LP varies with the parameters.",
                    "label": 0
                },
                {
                    "sent": "For example, the right hand sides.",
                    "label": 0
                },
                {
                    "sent": "Because, for example, this could be resource constraints as in some.",
                    "label": 0
                },
                {
                    "sent": "Application and I'm very interested in how well this function looks like.",
                    "label": 0
                },
                {
                    "sent": "If we change the upper limits on resources.",
                    "label": 0
                },
                {
                    "sent": "How quickly does the?",
                    "label": 0
                },
                {
                    "sent": "Optimal value function change.",
                    "label": 0
                },
                {
                    "sent": "So one thing we can say, of course we can always compute this function by solving the LP forgiven X.",
                    "label": 0
                },
                {
                    "sent": "We can solve this to find G of X.",
                    "label": 0
                },
                {
                    "sent": "But we also know that it's a convex function of X.",
                    "label": 0
                },
                {
                    "sent": "Another going follows from the minimization rule, and to apply this we have to define F. So in this case, F of XY would be just C transpose Y.",
                    "label": 0
                },
                {
                    "sent": "And as its domain of the function, we take the pairs XY that satisfy the inequality.",
                    "label": 0
                },
                {
                    "sent": "And outside the domain we define a function F plus Infinity.",
                    "label": 0
                },
                {
                    "sent": "So this is a convex function jointly an XL.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because its domain is convex, so that's just linear linear inequalities in XY.",
                    "label": 0
                },
                {
                    "sent": "So man is convex an on its domain, it's linear.",
                    "label": 0
                },
                {
                    "sent": "So it's also convex, so this function is convex jointly in XY.",
                    "label": 0
                },
                {
                    "sent": "And then here we minimize over Y.",
                    "label": 0
                },
                {
                    "sent": "With no constraints, actually an online, so the result is always necessarily convex in the remaining variable X, right?",
                    "label": 0
                },
                {
                    "sent": "Another last proper.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Called perspective.",
                    "label": 0
                },
                {
                    "sent": "So suppose I have a convex function of F of XF of X.",
                    "label": 0
                },
                {
                    "sent": "And then I can define a new function G of an X and an extra variable T. By replacing X in the argument of FBX over T. And then multiplying effetti.",
                    "label": 0
                },
                {
                    "sent": "And it's called the perspective of the function F. And it has the nice property that if F is convex.",
                    "label": 1
                },
                {
                    "sent": "And I restrict this to positive values of T. Then this perspective is always convex.",
                    "label": 0
                },
                {
                    "sent": "And that, for example, allows us to immediately see that this quadratic over linear function that I gave as an example, is convex.",
                    "label": 0
                },
                {
                    "sent": "Because I take F of X just accessible sex, that's convex, fairy place X over by X / T. And multiply with T and just get expose X / T four positive T, so that's convex.",
                    "label": 0
                },
                {
                    "sent": "Another example from information theory is negative entropy.",
                    "label": 0
                },
                {
                    "sent": "The negative logarithm if I use.",
                    "label": 0
                },
                {
                    "sent": "F of X.",
                    "label": 0
                },
                {
                    "sent": "Minus log X is a convex function of X, so if I use this apply this construction, they replace X over by X / T and I multiply the result with T, then that gives us this function.",
                    "label": 0
                },
                {
                    "sent": "T times log of T / X.",
                    "label": 0
                },
                {
                    "sent": "And so because of this, that's convex, jointly SMT.",
                    "label": 0
                },
                {
                    "sent": "So that's called the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "And it's not immediately obvious because the first term is convex, T log T is convex because that's the entropy, But the second term separately has no interesting convexity properties, but a combination of the two is convex.",
                    "label": 0
                },
                {
                    "sent": "Because of this, 'cause it's the perspective of a convex function.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's all I wanted to say in terms of theory of convex sets and functions.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions about this?",
                    "label": 0
                },
                {
                    "sent": "So then next we'll look at some problems, definitions and examples of convex optimization problems.",
                    "label": 1
                },
                {
                    "sent": "So we'll define a convex optimization problem is a function of the problem with the convex objective convex inequality constraints and linear equality constraints.",
                    "label": 0
                },
                {
                    "sent": "So it's the general mathematical optimization problem that I started with, but with the F is restricted to be convex.",
                    "label": 0
                },
                {
                    "sent": "So obviously the feasible set is convex cause it's the intersection of an affine set solution of the linear equations and.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sublevel sets of different convex functions so it's convex set.",
                    "label": 1
                },
                {
                    "sent": "You can also easily show that locally optimal points unnecessarily globally optimal.",
                    "label": 1
                },
                {
                    "sent": "And so the most important property of this is that it's easy to solve in theory.",
                    "label": 0
                },
                {
                    "sent": "Terms of complexity theory, but also in practice because there exists more and more.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We obtained software for solving optimization and convex optimization problems.",
                    "label": 1
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Simplest example is an LP.",
                    "label": 0
                },
                {
                    "sent": "If we restrict all the inequality constraints to linear.",
                    "label": 1
                },
                {
                    "sent": "The solution set looks like this is a polyhedron defined by different inequalities.",
                    "label": 0
                },
                {
                    "sent": "And then the constraint is the objective is to minimize C transpose X.",
                    "label": 0
                },
                {
                    "sent": "So you can easily see that the optimal value or the optimal solution will be at boundary of the polyhedron.",
                    "label": 0
                },
                {
                    "sent": "And it's typically at one of these extreme points of support here and.",
                    "label": 0
                },
                {
                    "sent": "Any kind of visualizer solution by just considering?",
                    "label": 0
                },
                {
                    "sent": "A level occurs or sets of constant value of C. Transpose X + D. So these level curves are hyperplanes with normal vector minus C. So those are the sets of constant objective value.",
                    "label": 0
                },
                {
                    "sent": "The decrease in the direction of minus C. And then we look look for the point in the polyhedron with the smallest value of C. Transpose X.",
                    "label": 0
                },
                {
                    "sent": "So in this case, that's the optimum.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In practice, it's not too difficult to recognize convex functions.",
                    "label": 0
                },
                {
                    "sent": "So if linear optimization linear programming problems in practice, so typically there arise from if they're not immediately or obviously LP's, they're typically come from piecewise linear functions.",
                    "label": 0
                },
                {
                    "sent": "For example, if I want to minimize a function F of X that's defined as the maximum pointwise maximum of M linear functions of X.",
                    "label": 0
                },
                {
                    "sent": "Then we already know that's convex because each of the functions is convex.",
                    "label": 0
                },
                {
                    "sent": "It's not differentiable, so it's not.",
                    "label": 0
                },
                {
                    "sent": "Immediately obvious how to solve it using gradient descent or or Newton's method, but it is the sold as an LP.",
                    "label": 0
                },
                {
                    "sent": "By using a very common trick, we introduce a new variable T and they will minimize T. This new variable subject to the constraint that AI transpose experts by is less than T for each eye.",
                    "label": 1
                },
                {
                    "sent": "And that's an LP and two variables X&T, because the objective is linear, all the constraints are linear.",
                    "label": 0
                },
                {
                    "sent": "It's easy to see that it's equivalent.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This for example, the easiest way to see this is to in this problem, if you fix X.",
                    "label": 0
                },
                {
                    "sent": "And you just optimize over T. And that's a very simple LP, because there's only one variable.",
                    "label": 0
                },
                {
                    "sent": "You minimize the subject to M lower bounds on TI, so obviously the answer is to take T equal to the largest of those lower bounds, and that's this maximum.",
                    "label": 0
                },
                {
                    "sent": "So for fixty the optimal values is expression, and if you're allowed to optimize over X anti jointly then you choose that value of T and Forex.",
                    "label": 0
                },
                {
                    "sent": "You choose the optimum of this problem.",
                    "label": 0
                },
                {
                    "sent": "So there are some.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Applications in machine learning that you probably all know about that are very well known.",
                    "label": 0
                },
                {
                    "sent": "So basically, if we want to separate so the basic one basic problem is if you want to linearly separate two sets of points in RN.",
                    "label": 1
                },
                {
                    "sent": "Dark circles and the open circles.",
                    "label": 1
                },
                {
                    "sent": "And we define a hyperplane or accepting separating hyperplane like this.",
                    "label": 0
                },
                {
                    "sent": "So we look for coefficient vector A and a constant B that takes for, which is a find function, takes positive values on the axis and negative values on the rise.",
                    "label": 0
                },
                {
                    "sent": "Then that's a strict set of a set of strict linear inequalities.",
                    "label": 0
                },
                {
                    "sent": "With variables A&B.",
                    "label": 0
                },
                {
                    "sent": "And the exercise and wires are known, so this is a set of linear inequalities and AMB.",
                    "label": 0
                },
                {
                    "sent": "It's not strict, but you can easily.",
                    "label": 0
                },
                {
                    "sent": "It's not a strict inequality, but you can easily make it into a non strict inequality.",
                    "label": 0
                },
                {
                    "sent": "By noting that this is homogeneous or if AMB satisfy these conditions that any positive multiple of AMB satisfy them.",
                    "label": 0
                },
                {
                    "sent": "So you can also make this right hand side equal to 1 or negative one and then just replace it with a non strict inequality.",
                    "label": 1
                },
                {
                    "sent": "And then you have a set of linear inequalities in A&B.",
                    "label": 0
                },
                {
                    "sent": "Any solution if it's solvable, defines a separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "As an extension, you can look at problems where.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two sets are not separable.",
                    "label": 0
                },
                {
                    "sent": "And then try to define an approximate separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "That separates most of the sets points X&Y, so one formulation would be the following, so this is a piecewise linear optimization problem in the same variables A&B.",
                    "label": 0
                },
                {
                    "sent": "So the first time you look at the Max of zero and then for each point XI the slack 1 -- A transpose XI minus BI.",
                    "label": 0
                },
                {
                    "sent": "So if XI lies on the correct side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "That's zero.",
                    "label": 0
                },
                {
                    "sent": "And then there's Max is zero and there is no contribution to this sum.",
                    "label": 0
                },
                {
                    "sent": "If X is on the negative on the wrong side of the hyperplane, if it's misclassified by that hyperplane, then we add a penalty equal to the slack.",
                    "label": 0
                },
                {
                    "sent": "In the inequality.",
                    "label": 0
                },
                {
                    "sent": "And then we do the same with the other set of points and we use this as a penalty on the separating hyperplane that we can minimize to find an approximate separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Now that's obviously piecewise linear because it's Max of linear functions of a B, and it can easily write it as an LP using the same trick as before.",
                    "label": 1
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So the one is because I used one in the right hand side here.",
                    "label": 0
                },
                {
                    "sent": "At the one doesn't really.",
                    "label": 0
                },
                {
                    "sent": "Any positive number will be equally good here.",
                    "label": 0
                },
                {
                    "sent": "Boys so if I scaled.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One, then the A&B would scale homogeneously.",
                    "label": 0
                },
                {
                    "sent": "Sorry, but there's only there's no other term in this objective, so if.",
                    "label": 0
                },
                {
                    "sent": "A scale one, then A&B.",
                    "label": 0
                },
                {
                    "sent": "If A&B are up.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Golden A&B will be scaled.",
                    "label": 0
                },
                {
                    "sent": "From that's because he redifined.",
                    "label": 0
                },
                {
                    "sent": "Separation of strict inequality.",
                    "label": 0
                },
                {
                    "sent": "So if A&B satisfy this then it can always scale A and B2.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Greater than any positive number on the right, and if it separates it with right hand side one and negative one, then it obviously satisfies.",
                    "label": 0
                },
                {
                    "sent": "This would be different if I define separation by a non strict inequality.",
                    "label": 0
                },
                {
                    "sent": "Here an I just don't strict inequality and then also exclude 0-AB as trivial solution.",
                    "label": 0
                },
                {
                    "sent": "Then this step wouldn't be equivalent.",
                    "label": 0
                },
                {
                    "sent": "So two other common.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Elements that are result in LP's are Infinity and one norm optimization.",
                    "label": 0
                },
                {
                    "sent": "So if I take the one norm of X minus be with the one norm is defined as the sum of the absolute values, then that can be written as an LP again by introducing extra variables and Huawei affected Y and then linear inequalities.",
                    "label": 0
                },
                {
                    "sent": "And again it's easy to see that they are equivalent because if you fix X and this problem and this LP.",
                    "label": 0
                },
                {
                    "sent": "Then you get an easy problem and why?",
                    "label": 0
                },
                {
                    "sent": "Because these are componentwise inequalities, so on each component why I there is 2 bounds lower bounds?",
                    "label": 0
                },
                {
                    "sent": "On why I an upper bound on minus Yi?",
                    "label": 0
                },
                {
                    "sent": "So to combine these inequality, say that why I is greater than the absolute value of the IT component of X -- B?",
                    "label": 0
                },
                {
                    "sent": "So that component is between Y and minus Y, so that's an upper bound on the absolute value.",
                    "label": 0
                },
                {
                    "sent": "This objective function is separable, so we can minimize over each iy separately.",
                    "label": 0
                },
                {
                    "sent": "Because the constraints are also and coupled, they don't couple the different entries and why.",
                    "label": 0
                },
                {
                    "sent": "So the answer for this problem fixed X will be to choose why I equal to the absolute value of the right component of X -- B.",
                    "label": 0
                },
                {
                    "sent": "If X is fixed.",
                    "label": 0
                },
                {
                    "sent": "And then so that's the known as one norm.",
                    "label": 0
                },
                {
                    "sent": "And then if you're allowed to minimize over X&Y, then you choose this, make the same choice for awhile, and for actual data points with a minimum one node.",
                    "label": 0
                },
                {
                    "sent": "So on the right hand side you're minimizing over X&Y, yeah?",
                    "label": 0
                },
                {
                    "sent": "So here we minimize over X.",
                    "label": 0
                },
                {
                    "sent": "That's a nonlinear but convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Here I introduce a new vector variable Y, and actually that should be MFAN is here the role dimension of a?",
                    "label": 0
                },
                {
                    "sent": "And I minimize jointly over XY, so that makes this into an LP because the objective is linear in all those variables and the constraints are also linear.",
                    "label": 0
                },
                {
                    "sent": "They're just linear inequalities in X&Y.",
                    "label": 0
                },
                {
                    "sent": "There's a similar trick, actually.",
                    "label": 0
                },
                {
                    "sent": "It's easier for the Chebyshev norm or the Infinity norm if I'm interested in minimizing the maximum absolute value of X -- B.",
                    "label": 0
                },
                {
                    "sent": "Then this would be the equivalent and equivalent LP.",
                    "label": 0
                },
                {
                    "sent": "So here the extra variable introduces scalar instead of a vector.",
                    "label": 0
                },
                {
                    "sent": "And then the constraints are that.",
                    "label": 0
                },
                {
                    "sent": "So the one here, the BF one is a vector of ones.",
                    "label": 0
                },
                {
                    "sent": "So each constraint here says that the components of X -- B must be less than Y.",
                    "label": 0
                },
                {
                    "sent": "Each component of X -- B.",
                    "label": 0
                },
                {
                    "sent": "And greater than minus Y for each eye.",
                    "label": 0
                },
                {
                    "sent": "So that means that the absolute value of the components of X -- B are bounded by Y.",
                    "label": 0
                },
                {
                    "sent": "And again to see the equivalence, we can fix X here and then we minimize over the scalar variable Y.",
                    "label": 0
                },
                {
                    "sent": "That's a very easy optimization problem because you just have a bunch of lower bounds on Y and you take Y equal to the largest lower bound, and in this case the largest lower bound is the maximum absolute value of the entries of X -- B, right?",
                    "label": 0
                },
                {
                    "sent": "And then if you minimize joint deal for X&Y, you get something equivalent to this.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A few examples for linear programming.",
                    "label": 0
                },
                {
                    "sent": "So the next extension down is quadratic programming, and that's something else goes back to the 50s.",
                    "label": 0
                },
                {
                    "sent": "So here we keep the same types of constraints, linear inequalities and equalities, and we take a convex quadratic cost function of X.",
                    "label": 1
                },
                {
                    "sent": "So P is a symmetric matrix and its positive semidefinite, so it's the Hessian of this quadratic function.",
                    "label": 0
                },
                {
                    "sent": "So the constraints that are still polyhedral, as in the LP case.",
                    "label": 0
                },
                {
                    "sent": "But level curves of F have changed there now.",
                    "label": 0
                },
                {
                    "sent": "FS quadratic function, so these are... And then, depending where the midpoint, the center of these... or the unconstrained minimum of the objective function is, we find an optimum that can be on the boundary.",
                    "label": 0
                },
                {
                    "sent": "Can be on the corner point or it can be in the interior of the polyhedron if that's the unconstrained minimum.",
                    "label": 0
                },
                {
                    "sent": "So that's a different linear programming.",
                    "label": 0
                },
                {
                    "sent": "So very.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Example that actually motivated the development of quadratic programming in the 50s.",
                    "label": 0
                },
                {
                    "sent": "Is a linear program it today, but people call linear programming with uncertainty or a robust linear program.",
                    "label": 1
                },
                {
                    "sent": "So this comes up investment.",
                    "label": 0
                },
                {
                    "sent": "Science, so this was a basic markovits portfolio problem.",
                    "label": 0
                },
                {
                    "sent": "So in any case, we have an LP.",
                    "label": 1
                },
                {
                    "sent": "We assume that data and the constraints are exactly known, so G&H are exactly given, but we assume that C is uncertain.",
                    "label": 0
                },
                {
                    "sent": "The objective isn't certain.",
                    "label": 0
                },
                {
                    "sent": "So and we model it as a random variable.",
                    "label": 1
                },
                {
                    "sent": "Then obviously that makes C transpose X the objective function and random variable.",
                    "label": 0
                },
                {
                    "sent": "And then we have to define what we mean by minimizing a random variable of X random variable.",
                    "label": 0
                },
                {
                    "sent": "That depends on X.",
                    "label": 0
                },
                {
                    "sent": "Well, if C has mean seebaran covariance Sigma.",
                    "label": 0
                },
                {
                    "sent": "Then we'll have a tradeoff between the expected value of C transpose X, which is C bar transpose X, and the variance X transpose SEMA X.",
                    "label": 0
                },
                {
                    "sent": "So an investment I will be the expected.",
                    "label": 0
                },
                {
                    "sent": "We minimize so we would minimize expected loss, so we mean would be the expected loss and the variance would be the risk on the portfolio.",
                    "label": 0
                },
                {
                    "sent": "And there is a tradeoff between these two by just minimizing the expected value of C transpose X, we might have a solution with high variance.",
                    "label": 0
                },
                {
                    "sent": "That's very sensitive to changes in C. So one way to model this is to look at the linear convex combination or a weighted combination of the two objectives.",
                    "label": 0
                },
                {
                    "sent": "So suppose we minimize the combination of weighted sum of the expected optimal value, expected objective function and the variance.",
                    "label": 1
                },
                {
                    "sent": "Extensible Sigma X with a weighting parameters that expresses our risk aversion right?",
                    "label": 0
                },
                {
                    "sent": "So if, is 0, then we are don't care about the risk or the variance in the solution.",
                    "label": 0
                },
                {
                    "sent": "If we are very.",
                    "label": 0
                },
                {
                    "sent": "Risk averse that we would choose a larger value of gamma and put a higher penalty on high variance.",
                    "label": 0
                },
                {
                    "sent": "But for positive, any positive value of X, this is a convex quadratic function of X, so this is a quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "And it's one of the first applications of quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "From the 50s.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another famous one is just to continue this finger discrimination example from linear programming.",
                    "label": 0
                },
                {
                    "sent": "So again, if the solution if you have a two sets of points that are separable by these two hyperplanes, then we can also try to find that in general there are multiple supporting or separating hyperplanes, but we can try to find the one that maximizes the distance between these two sets.",
                    "label": 1
                },
                {
                    "sent": "Or the convex holes of these two sets.",
                    "label": 1
                },
                {
                    "sent": "And the distances between these two hyperplanes is given by two over the Euclidean norm of a.",
                    "label": 0
                },
                {
                    "sent": "That's the distance between this hyperplane that defines.",
                    "label": 0
                },
                {
                    "sent": "Separates the first set and the second one.",
                    "label": 0
                },
                {
                    "sent": "So if you want to maximize the distance you want to minimize the Euclidean norm of a or the square of the Euclidean norm of A and that gives us a quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "Because again, as before, the variables are A&B, the exercise and the Y eyes are given, so we have linear inequalities in A&B and a convex quadratic function of.",
                    "label": 0
                },
                {
                    "sent": "And then you can define a combine the two week and if.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Find combined this penalty on misclassification that we used for as an example of linear programming and then this maximum margin objective and look at the linear combination or a weighted sum of the two and minimize this.",
                    "label": 0
                },
                {
                    "sent": "Function of A&B.",
                    "label": 0
                },
                {
                    "sent": "And then again, you can easily write it as a quadratic programming problem, and that's the support vector classifier.",
                    "label": 1
                },
                {
                    "sent": "Which one or two?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a very good.",
                    "label": 0
                },
                {
                    "sent": "So that's the distance between these two.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arthur paints the.",
                    "label": 0
                },
                {
                    "sent": "So if you define a different distance.",
                    "label": 0
                },
                {
                    "sent": "Then in general, you would take the so we can look at the convex Hull of this first set of points and the other point, and then they find the distance in any norm and that defined that as a margin between the two sets.",
                    "label": 0
                },
                {
                    "sent": "And then in general you can do this, but you get a different norm.",
                    "label": 0
                },
                {
                    "sent": "I think you get the dual norm here instead of the.",
                    "label": 0
                },
                {
                    "sent": "Get a problem that relates to the dual norm of the normally choose to define the distance so it's not quite, uh, it won't be a QP, but still convex.",
                    "label": 0
                },
                {
                    "sent": "So in other words, you can maximize the distance of the convex Hull between these two sets, and you know.",
                    "label": 0
                },
                {
                    "sent": "Or computer distance and then from the distance computers.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Creating hyperlink on this picture you don't have any points between the margin.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so here I define that there's assume they're separable.",
                    "label": 0
                },
                {
                    "sent": "And I define the margin.",
                    "label": 0
                },
                {
                    "sent": "And the next thing we have these two hyperplanes, and they're not separable, so that we take this objective function.",
                    "label": 0
                },
                {
                    "sent": "So this would be a sort of some kind of convex penalty on the misclassification, and this term maximizes the margin between the two.",
                    "label": 0
                },
                {
                    "sent": "These two are dash type of lens.",
                    "label": 0
                },
                {
                    "sent": "So C4 if you increase the penalty.",
                    "label": 0
                },
                {
                    "sent": "You put more weight on the margin and increase the margin between the hyperplanes.",
                    "label": 0
                },
                {
                    "sent": "So another.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Important application is in of 1 norms, an Infinity norms?",
                    "label": 0
                },
                {
                    "sent": "One norms and is in single reconstruction, so this is just one example.",
                    "label": 0
                },
                {
                    "sent": "Suppose we take a signal of length 1000 and we know it's sparse, so here's a signal with 10 nonzero components.",
                    "label": 1
                },
                {
                    "sent": "And of course, unknown positions and also the range is changed, but there are 10 zero components and the rest are zero.",
                    "label": 1
                },
                {
                    "sent": "And then the question is, can we reconstruct this signal from a small number of measurements, for example 100 measurements?",
                    "label": 0
                },
                {
                    "sent": "Model, like this linear measurements so we have an exact signal X hat.",
                    "label": 0
                },
                {
                    "sent": "We make linear measurements.",
                    "label": 0
                },
                {
                    "sent": "X gives us a vector X hat.",
                    "label": 0
                },
                {
                    "sent": "We add random noise and it gives us 100 measurements.",
                    "label": 0
                },
                {
                    "sent": "And then from this 100 measurements on this 1000 dimensional Vector X.",
                    "label": 0
                },
                {
                    "sent": "The question is, can we reconstruct this signal by exploiting the fact that we know that it's sparse?",
                    "label": 0
                },
                {
                    "sent": "So here in this experiment we generate a randomly, so that's actually important.",
                    "label": 0
                },
                {
                    "sent": "Just from a caution distribution and then the noise is also random.",
                    "label": 0
                },
                {
                    "sent": "So one common, so that's an underdetermined set of equations, so we have only 100 conditions on X and we try to estimate the signal of dimension 1000.",
                    "label": 0
                },
                {
                    "sent": "So in general you need a regularization term to make that well post.",
                    "label": 0
                },
                {
                    "sent": "So one popular regularization would be to add the Euclidean norm to take the two norm of X -- B.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The error in the.",
                    "label": 0
                },
                {
                    "sent": "Reconstruction and put a penalty on the size of X.",
                    "label": 0
                },
                {
                    "sent": "And the most common choice would be the two norm that gives you so called Tikhonov regularization, but obviously that doesn't work because you get a solution at small everywhere.",
                    "label": 0
                },
                {
                    "sent": "And minimize this.",
                    "label": 0
                },
                {
                    "sent": "This convex combination of the two, so it doesn't of course reconstruct the exact signal.",
                    "label": 0
                },
                {
                    "sent": "However, if you use the one norm as regularization and you take this.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trade off between the error and X model.",
                    "label": 0
                },
                {
                    "sent": "The estimated X and the measurements be and I put a penalty on the one norm instead of the two norm.",
                    "label": 0
                },
                {
                    "sent": "Then in this case it reconstructs the signal perfectly, so that's the general property of 1 norms that it also.",
                    "label": 0
                },
                {
                    "sent": "Um and.",
                    "label": 0
                },
                {
                    "sent": "So in the optimization problem with penalty on the one or more constraint on the one norm will encourage the solution to be sparse many zero components.",
                    "label": 0
                },
                {
                    "sent": "And that's very useful in applications like this.",
                    "label": 0
                },
                {
                    "sent": "We have 1000.",
                    "label": 0
                },
                {
                    "sent": "You have 1000 plane.",
                    "label": 0
                },
                {
                    "sent": "Most of them are zero and just several of them are are not zero.",
                    "label": 0
                },
                {
                    "sent": "And then you have just 100 points from which you want to reconstruct these non 0.",
                    "label": 0
                },
                {
                    "sent": "Wooden 0 signal yeah, but if you have like probably looks to me like it's quite probable that all these 100 points will be zero.",
                    "label": 0
                },
                {
                    "sent": "Well, it depends.",
                    "label": 0
                },
                {
                    "sent": "That's why it's important that a is.",
                    "label": 0
                },
                {
                    "sent": "It depends on a also.",
                    "label": 0
                },
                {
                    "sent": "So here I generate a all the entries of a are randomly generated, so it's a random matrix.",
                    "label": 0
                },
                {
                    "sent": "So for example, sorry.",
                    "label": 0
                },
                {
                    "sent": "So A is the measurement, so you have a signal.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Length one thousands and suppose you have a linear measurement on X at linear.",
                    "label": 0
                },
                {
                    "sent": "You observe a linear combination of the components of X.",
                    "label": 0
                },
                {
                    "sent": "For example, another choice of a would be to pick 100 random components of X and that would work.",
                    "label": 0
                },
                {
                    "sent": "Had a problem that you describe, because then most of the components will be 0 and your reconstruction will be 0.",
                    "label": 0
                },
                {
                    "sent": "So here it works.",
                    "label": 0
                },
                {
                    "sent": "For example, for randomly generated a with caution distribution because all the non zero entries on the signal will be mixed in the measurement.",
                    "label": 0
                },
                {
                    "sent": "Another choice would be for example take a DfT and then pick random.",
                    "label": 0
                },
                {
                    "sent": "Random subset of the DfT of X, right?",
                    "label": 0
                },
                {
                    "sent": "So this is known in the sparse reconstruction, so people have actually very interesting theory that characterizes the types of matrices where this works, and water properties are of a that make it work, and it's certainly not true for any.",
                    "label": 0
                },
                {
                    "sent": "Matrix a.",
                    "label": 0
                },
                {
                    "sent": "But so in cases like this, it allows you to use a convex.",
                    "label": 0
                },
                {
                    "sent": "Technique to solve a problem that actually would be very difficult to solve otherwise because one.",
                    "label": 0
                },
                {
                    "sent": "Combinatorial way of solving this problem would be to just consider different subsets of X and then try to find the best reconstruction with different sparse different subsets of X to find the minimal subset, right?",
                    "label": 0
                },
                {
                    "sent": "So those are some examples of quadratic programming problems, then maybe very quickly we can.",
                    "label": 0
                },
                {
                    "sent": "An interesting set of problems.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's less important here is geometric programming.",
                    "label": 0
                },
                {
                    "sent": "So in a geometric program, you look at different nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Optimization problems like this, so we have a variable X that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Restricted to be positive.",
                    "label": 0
                },
                {
                    "sent": "And each of these functions as a so-called polynomial function of X opposing normal is defined as the.",
                    "label": 0
                },
                {
                    "sent": "Function like this so it's a sum.",
                    "label": 0
                },
                {
                    "sent": "Of powers of X products of powers of X.",
                    "label": 0
                },
                {
                    "sent": "So the powers of the exponents of the each X can be any real number can be positive, negative.",
                    "label": 0
                },
                {
                    "sent": "They don't have to be integers.",
                    "label": 0
                },
                {
                    "sent": "And the coefficients have to be positive, so that's called a polynomial.",
                    "label": 0
                },
                {
                    "sent": "So it looks a little bit like a polynomial, but of course the powers here can be positive, negative and non integer.",
                    "label": 0
                },
                {
                    "sent": "And also a restrict X to be positive so all the components of expensive positive.",
                    "label": 0
                },
                {
                    "sent": "So that's called a geometric program and it's not convex, but there is a simple transformation that makes it convex.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you use instead of X, the logarithm of X is variable.",
                    "label": 0
                },
                {
                    "sent": "Then it transforms into a convex problem.",
                    "label": 0
                },
                {
                    "sent": "Because this would be the result and it involves these functions that we've seen are convex.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to finish this section, I'd like to say something about modeling software, so we've seen that.",
                    "label": 0
                },
                {
                    "sent": "Often convex functions can be derived from simpler convex functions using some calculus rules from convex analysis.",
                    "label": 1
                },
                {
                    "sent": "For example, this maximization rule, etc.",
                    "label": 0
                },
                {
                    "sent": "And often formulating a problem in a standard form, for example in linear program or a quadratic program involves some transformations.",
                    "label": 0
                },
                {
                    "sent": "You have to introduce new variables.",
                    "label": 0
                },
                {
                    "sent": "You have to add extra constraints, do some transformations.",
                    "label": 0
                },
                {
                    "sent": "And recently soft more software has become available to automate some of these tasks as called modeling software.",
                    "label": 1
                },
                {
                    "sent": "So there are some packages in Matlab, for example CVS annual map that make it much easier to formulate convex optimization problems in practice.",
                    "label": 1
                },
                {
                    "sent": "And they automate actually two tasks that.",
                    "label": 0
                },
                {
                    "sent": "Or needed to apply this in practice one is you have to recognize complexity and verify it from these calculus rules.",
                    "label": 0
                },
                {
                    "sent": "For example, maximization or.",
                    "label": 0
                },
                {
                    "sent": "Linear transformations, and Secondly these packages will transform your problem in an input format that's required by optimization solvers.",
                    "label": 1
                },
                {
                    "sent": "For example, in LP solver.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have a piecewise linear maximization problem than a modeling language will do the transformation for you, and you don't have to transform the maximization into a maximization problem into an LP.",
                    "label": 0
                },
                {
                    "sent": "So it's easier to explain with an example.",
                    "label": 0
                },
                {
                    "sent": "So suppose we minimize it, then one norm of X -- B subject to appeal lower bounds on the variables.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a very simple.",
                    "label": 0
                },
                {
                    "sent": "Problem we can easily write it as an LP by introducing a new variable and moving this to the constraints.",
                    "label": 0
                },
                {
                    "sent": "But that's not necessary.",
                    "label": 0
                },
                {
                    "sent": "Becausw can also be automated, and this is what the code in Matlab would be for CX, which is a model modeling package.",
                    "label": 0
                },
                {
                    "sent": "So you describe to write generating A&B and everything between these two statements, civics begin and civic send.",
                    "label": 0
                },
                {
                    "sent": "Is specific to CVX.",
                    "label": 0
                },
                {
                    "sent": "So first we define a variable of dimension 3.",
                    "label": 0
                },
                {
                    "sent": "Then we give the objective and the natural form is just say minimize the one norm of X -- B and mop notation.",
                    "label": 0
                },
                {
                    "sent": "And then we give the constraints.",
                    "label": 0
                },
                {
                    "sent": "So this is a componentwise inequality of a vector, and so on.",
                    "label": 0
                },
                {
                    "sent": "And then after you execute this statement than CVS will transform this into an LP caller solver and linear programming solver and then.",
                    "label": 0
                },
                {
                    "sent": "Export results to the variables over defining the problem.",
                    "label": 0
                },
                {
                    "sent": "So inside these two statements, X is in CVS variable.",
                    "label": 0
                },
                {
                    "sent": "After completion the problem will be solved and X will have the values of the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "And this is just a basic optimization problem, but you can do this includes predefined functions for very wide variety of convex functions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Examples.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example that you gave you slides back on the trying to you know the probabilistic programming one would be in the variances in trade off that example.",
                    "label": 0
                },
                {
                    "sent": "I mean one thing that I'd be interested in is.",
                    "label": 0
                },
                {
                    "sent": "Instead of, you know, training off the meaning of variance of each.",
                    "label": 0
                },
                {
                    "sent": "To know what can I find a variable for my model so that you know the probability of something happening have something in it?",
                    "label": 0
                },
                {
                    "sent": "An outlier happening is really really small.",
                    "label": 0
                },
                {
                    "sent": "Can you say something about?",
                    "label": 0
                },
                {
                    "sent": "You know this idea?",
                    "label": 0
                },
                {
                    "sent": "You know using you know, I guess it can open density functions or something.",
                    "label": 0
                },
                {
                    "sent": "You know if I mean what conditions that could be solved you know formulated as a comics optimization problem?",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually, that's one of the examples.",
                    "label": 0
                },
                {
                    "sent": "I'll come in a few slides, so that's called.",
                    "label": 0
                },
                {
                    "sent": "Chance constraints you want to put a limit on the probability that a constraint is violated.",
                    "label": 0
                },
                {
                    "sent": "And in general, that can be very difficult except in special cases.",
                    "label": 0
                },
                {
                    "sent": "Depending on the density and constraints.",
                    "label": 0
                },
                {
                    "sent": "But we will actually see one practical examples where it can be.",
                    "label": 0
                },
                {
                    "sent": "So then so the next topic is cone programming and will continue this tomorrow.",
                    "label": 0
                },
                {
                    "sent": "So Cone programming is general.",
                    "label": 1
                },
                {
                    "sent": "As defined like this, and it's a general format for convex optimization that has become very popular since the early 1990s.",
                    "label": 0
                },
                {
                    "sent": "And it basically makes any convex optimization problem look like a linear program, so it just takes a linear objective.",
                    "label": 0
                },
                {
                    "sent": "Linear equality, constraints, linear inequalities.",
                    "label": 0
                },
                {
                    "sent": "And the only difference is they replace the standard inequality between vectors.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By a generalized inequality.",
                    "label": 0
                },
                {
                    "sent": "And generalized generalized inequality.",
                    "label": 0
                },
                {
                    "sent": "I mean that I define a cone K convex cone K. And then this inequality means that H -- G X lies in the cone K. So if I take forcada nonnegative orthant.",
                    "label": 0
                },
                {
                    "sent": "Then this is just another way of saying that the constraints hold componentwise right 'cause componentwise H -- G, X lies in a negative audit.",
                    "label": 0
                },
                {
                    "sent": "But if you replace the negative order by other convex cones, you get interesting.",
                    "label": 0
                },
                {
                    "sent": "Convex optimization problems that are extend linear programming.",
                    "label": 1
                },
                {
                    "sent": "So that's called called linear programming.",
                    "label": 1
                },
                {
                    "sent": "As popular as a standard format because it makes the general nonlinear problem look like an LP.",
                    "label": 1
                },
                {
                    "sent": "And then it's important in the also theory and algorithms becausw.",
                    "label": 0
                },
                {
                    "sent": "It turns out that duality theory, for example, is almost exactly the same As for linear programming.",
                    "label": 0
                },
                {
                    "sent": "And also the algorithms for linear programming, at least interior point methods are very similar or can be extended to this case.",
                    "label": 0
                },
                {
                    "sent": "So it's a convex set.",
                    "label": 0
                },
                {
                    "sent": "That's also a cone, so if you take an element in the cone and all non negative multiples of that element or in the entire half line through that element is in the code.",
                    "label": 0
                },
                {
                    "sent": "So we see actually this is a very general format, but will be interested in two specific cases, or three if you include the nonnegative organs.",
                    "label": 0
                },
                {
                    "sent": "So the first one is the 2nd order cone, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cone that's called a second order cone.",
                    "label": 0
                },
                {
                    "sent": "So the 2nd order cone is defined like this in general, so you have P. It's a, it's a cone.",
                    "label": 0
                },
                {
                    "sent": "In RP then you look at the sub vector of the first P -- 1 entries.",
                    "label": 0
                },
                {
                    "sent": "And then that vector Y is in the cone if the Euclidean norm of the first P -- 1 entries is less than the last entry.",
                    "label": 0
                },
                {
                    "sent": "So the 2nd order cone in R3 looks like this is also known as the ice cream cone, so.",
                    "label": 0
                },
                {
                    "sent": "Three vector, three variable three vector wise in the cone.",
                    "label": 0
                },
                {
                    "sent": "If Y-3 is greater than the Euclidean norm of one and two.",
                    "label": 0
                },
                {
                    "sent": "So for each Y three we have this constraint defines disk.",
                    "label": 0
                },
                {
                    "sent": "Of points this could radius Y 3 and then if you increase Y 3, the radius of the disk increases that goes from zero to Infinity, so it's a convex set, that's example.",
                    "label": 0
                },
                {
                    "sent": "It's easy to see that it's convex.",
                    "label": 0
                },
                {
                    "sent": "It's also a cone because it includes the origin and an older lines.",
                    "label": 0
                },
                {
                    "sent": "Through any points in the cone.",
                    "label": 0
                },
                {
                    "sent": "The constraint function is not differentiable.",
                    "label": 0
                },
                {
                    "sent": "Normal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the norm.",
                    "label": 0
                },
                {
                    "sent": "So this is also the graph of the Norman in R2, so the norm is non differentiable at the origin.",
                    "label": 0
                },
                {
                    "sent": "It's differentiable everywhere else, but if the argument of enormous zero then that's not differentiable.",
                    "label": 0
                },
                {
                    "sent": "So this is a second order cone program and sort of the abstract way of viewing it is as a cone program and say, well, look at linear inequalities with respect to this cone.",
                    "label": 0
                },
                {
                    "sent": "But at all actually means something that's very straightforward.",
                    "label": 0
                },
                {
                    "sent": "It just means that Euclidean norm of a linear function of X.",
                    "label": 0
                },
                {
                    "sent": "Is less than a scalar linear function of X.",
                    "label": 0
                },
                {
                    "sent": "And that's a second order constraint.",
                    "label": 0
                },
                {
                    "sent": "So it means that.",
                    "label": 0
                },
                {
                    "sent": "If you look at it geometrically that this.",
                    "label": 0
                },
                {
                    "sent": "Image of X and it is linear.",
                    "label": 0
                },
                {
                    "sent": "Mapping lies in the 2nd order code.",
                    "label": 0
                },
                {
                    "sent": "But to appreciate a constraint, you can also just look at it like this so it's a convex function of X, because that's a convex function of X.",
                    "label": 0
                },
                {
                    "sent": "It says are equal to a linear function of X and it's not differentiable, but it's convex.",
                    "label": 0
                },
                {
                    "sent": "It's known as an SCP.",
                    "label": 0
                },
                {
                    "sent": "And includes linear programming.",
                    "label": 0
                },
                {
                    "sent": "For example, if you take A and B0 or the dimension of this first: zero, that is just a set of linear inequalities.",
                    "label": 0
                },
                {
                    "sent": "So these are some basic examples that actually can be written in that form.",
                    "label": 0
                },
                {
                    "sent": "First, if you have a general quadratic constraint with opposed.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This definite matrix A then you can easily write it as a second order cone constraint by factoring A.",
                    "label": 0
                },
                {
                    "sent": "And this extends to semi definite and singular a, but it gets a little more complicated.",
                    "label": 1
                },
                {
                    "sent": "It also works for certain quadratic inequality's that are actually indefinite.",
                    "label": 0
                },
                {
                    "sent": "So here we have a vector X.",
                    "label": 0
                },
                {
                    "sent": "And two scalar variables Y&Z that are non negative and a constraint is that the square of the norm of X is less than the product of Y&Z.",
                    "label": 0
                },
                {
                    "sent": "So it's called a hyperbolic constraint.",
                    "label": 1
                },
                {
                    "sent": "Ann, it's if your account that's a quadratic function of these variables XY and Z, But the Hessian is actually indefinite if you bring this to the left hand side and you workout the Hessian of the constraints indefinite as a negative eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "But it defines a convex set and they can be written as a second order cone constraint like this.",
                    "label": 0
                },
                {
                    "sent": "If you workout this norm squared this norm and square the right hand side, you'll see it's equivalent to this.",
                    "label": 0
                },
                {
                    "sent": "We call 2nd order cone representable.",
                    "label": 0
                },
                {
                    "sent": "Constraints that can be written in this form by the correct choice of ABC and D. And the second refers to the user to do exactly, so the one norm.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One norm corn program would be something like this with one note.",
                    "label": 0
                },
                {
                    "sent": "So basically what this is saying is that we can.",
                    "label": 0
                },
                {
                    "sent": "2nd Order cone representable constraints are constraint sets.",
                    "label": 0
                },
                {
                    "sent": "That can be interpreted as different slices of 2nd order cones and different dimensions.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this in higher dimensions then intersected with affine sets, then depending on your choice of ABC and D, then depending on how you choose this Efron sets.",
                    "label": 0
                },
                {
                    "sent": "You can actually create or obtain different convex intersections.",
                    "label": 0
                },
                {
                    "sent": "And this would be one example where you have this nonlinear constraints and.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's equivalent to a second order cone constraint.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Certain powers of X.",
                    "label": 0
                },
                {
                    "sent": "For example, if they X to the power 1.5 lisanti restricted to non negative X. Gambrinus hyperbolic constraints like this.",
                    "label": 0
                },
                {
                    "sent": "And you can easily verify this inequality.",
                    "label": 0
                },
                {
                    "sent": "So here we introduce an extra variable Z.",
                    "label": 0
                },
                {
                    "sent": "And this says that she is less than square root of X.",
                    "label": 0
                },
                {
                    "sent": "And then if you plug it in here, you get the first constraint.",
                    "label": 0
                },
                {
                    "sent": "So then you can convert these hyperbolic constraints and the 2nd order cone constraints by using the trick of the previous page.",
                    "label": 0
                },
                {
                    "sent": "And this actually works for any rational P greater than one.",
                    "label": 0
                },
                {
                    "sent": "So instead of 1.5 you could take any P as long as it's rational and then by introducing new variables you can write it as a second order cone problem.",
                    "label": 0
                },
                {
                    "sent": "And also some negative powers as long as they're rational.",
                    "label": 0
                },
                {
                    "sent": "And these are the types of things that are modeling language would do for you.",
                    "label": 0
                },
                {
                    "sent": "The modeling language would accept constraints like this and then make these conversions for you.",
                    "label": 0
                },
                {
                    "sent": "So this is actually the extension that of this risk.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expected value tradeoff problem in linear programming.",
                    "label": 0
                },
                {
                    "sent": "So in general.",
                    "label": 0
                },
                {
                    "sent": "We can consider what's called a robust linear program.",
                    "label": 1
                },
                {
                    "sent": "So and here is a stochastic formulation of robust linear program.",
                    "label": 0
                },
                {
                    "sent": "You call it stochastic because we assume here that Azar uncertain.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, assume that C&B are given.",
                    "label": 0
                },
                {
                    "sent": "And we modeled as random variables with some distribution.",
                    "label": 0
                },
                {
                    "sent": "In this case normal distribution with given mean and given covariance.",
                    "label": 0
                },
                {
                    "sent": "And then we have to decide what we mean by the constraint.",
                    "label": 0
                },
                {
                    "sent": "If the coefficient is uncertain or random.",
                    "label": 0
                },
                {
                    "sent": "So here we specify on certain minimum probability of satisfying the constraint.",
                    "label": 0
                },
                {
                    "sent": "So you say access feasible for this robust version of the LP if it satisfies each constraint with probability at least at a, for example 90%.",
                    "label": 0
                },
                {
                    "sent": "So that's called a chance constrained and stochastic optimization.",
                    "label": 0
                },
                {
                    "sent": "And the question is, can we actually solve this?",
                    "label": 0
                },
                {
                    "sent": "Guy will call this probability and then handle it is at a convex constraint on X or not.",
                    "label": 0
                },
                {
                    "sent": "And it's actually interesting to look at before we look at the expressions to look at the convex set of this robust LP for different choices of ETA.",
                    "label": 0
                },
                {
                    "sent": "So we assume that A is a normal random vector with certain mean and covariance.",
                    "label": 0
                },
                {
                    "sent": "Suppose we pick it as 50%.",
                    "label": 0
                },
                {
                    "sent": "Then I will look at one constraint inequality, A transpose X -- B.",
                    "label": 0
                },
                {
                    "sent": "So if we require that the constraint is satisfied with probability 50%.",
                    "label": 1
                },
                {
                    "sent": "Then it just means that X satisfies the constraint with a equal to the mean of AI.",
                    "label": 0
                },
                {
                    "sent": "Because a satisfies the constraint for the average AI.",
                    "label": 0
                },
                {
                    "sent": "Then for half of the cases, half of the coefficient vectors will be outside the feasible set, and for half of the insights will be exactly satisfied with the 50%.",
                    "label": 0
                },
                {
                    "sent": "So for 50% the solution set of this robust LP is exactly the solution set for the LP with the normal or the mean with a replaced by the mean.",
                    "label": 0
                },
                {
                    "sent": "If we increase at a, we require that excess satisfy each constraint with probability higher than 50%.",
                    "label": 1
                },
                {
                    "sent": "Then the solution such shrinks to something smaller.",
                    "label": 0
                },
                {
                    "sent": "Because if you pick an X and near the boundary of the original feasible set, then with high probability the AES random.",
                    "label": 0
                },
                {
                    "sent": "A transpose X will be outside will be greater than be.",
                    "label": 0
                },
                {
                    "sent": "I'll be infeasible so you have to stay away from the boundary, and it turns out that that's actually gives you a non polyhedral but convex set X.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you look at an 8 or less than 50% and it turns out that the set is non convex even if it is very close to 50%.",
                    "label": 0
                },
                {
                    "sent": "And the set is larger than the normal case.",
                    "label": 0
                },
                {
                    "sent": "And the reason is that now we are happy with X satisfying the constraint with low probability.",
                    "label": 0
                },
                {
                    "sent": "And then it turns out that you can actually obtain a solution, so it looks like this, and it's nonconvex, and in general, this problem with eight or 10% would be very difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "Because there are many local Optima.",
                    "label": 0
                },
                {
                    "sent": "For example, if C points in this direction.",
                    "label": 0
                },
                {
                    "sent": "Then these two are local optimal points and it's in general very difficult to find the corner point that gives you the global optimum.",
                    "label": 0
                },
                {
                    "sent": "Whereas here is convex.",
                    "label": 0
                },
                {
                    "sent": "I will see it in a second order constraint.",
                    "label": 0
                },
                {
                    "sent": "So here is the probability constraint with the single ADA.",
                    "label": 0
                },
                {
                    "sent": "Is it straightforward to generalize to Ada I for each?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's still sit for us, so here it's a different error.",
                    "label": 0
                },
                {
                    "sent": "Or at a holds for each constraint separately, so it's not a constraint of satisfying all the constraint jointly.",
                    "label": 0
                },
                {
                    "sent": "It's constant for each other, but you could have a different data in each.",
                    "label": 0
                },
                {
                    "sent": "As long as they are created at 50%.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Do I explicitly on the fact that is normal?",
                    "label": 0
                },
                {
                    "sent": "It does actually.",
                    "label": 0
                },
                {
                    "sent": "This is for a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "You know with the mean and everything less.",
                    "label": 0
                },
                {
                    "sent": "Being a larger everything you know larger than 50%.",
                    "label": 0
                },
                {
                    "sent": "Being a smaller set, wouldn't that apply to any distribution that's still true for many of its symmetric and so on symmetric distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's more complicated.",
                    "label": 0
                },
                {
                    "sent": "So in general these chance constraint is actually very difficult except for very simple distribute like the normal distribution and linear constraints.",
                    "label": 0
                },
                {
                    "sent": "So there they become intractable in general.",
                    "label": 0
                },
                {
                    "sent": "But I can just quickly say why it's convex.",
                    "label": 0
                },
                {
                    "sent": "So basically just workout.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The probability for each constraint.",
                    "label": 0
                },
                {
                    "sent": "So a is normal, so we can easily workout in terms of the cumulative density of ocean water prob.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is of exceeding X.",
                    "label": 0
                },
                {
                    "sent": "And you get an expression like this.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you get the normal so X satisfy this with probability at a.",
                    "label": 0
                },
                {
                    "sent": "If this constraint is satisfied, so a bar is the normal value of AI, Sigma was a covariance, fires a cumulative density of the Gaussian unit variance.",
                    "label": 1
                },
                {
                    "sent": "Caution, caution.",
                    "label": 0
                },
                {
                    "sent": "And then the reason why after 50% is important is that the coefficient in front of the norm here is the inverse of inverse applied to enter.",
                    "label": 0
                },
                {
                    "sent": "So it's the inverse of the cumulative density, so forever exceeding 50%.",
                    "label": 0
                },
                {
                    "sent": "It's a positive number.",
                    "label": 0
                },
                {
                    "sent": "For A to less than 50%, it's a negative value is far innovator.",
                    "label": 0
                },
                {
                    "sent": "So if it's greater than 50%, you get a positive coefficient in front of the norm and you get a second order cone problem.",
                    "label": 0
                },
                {
                    "sent": "If it's less than 50%, you will have a negative coefficient here and then it makes constraints a non convex because that's a non convex function.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Is it obvious that?",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, because the objective is still linear.",
                    "label": 0
                },
                {
                    "sent": "So, for example, suppose at the points up, then the lines of constant C transpose X would be horizontal lines.",
                    "label": 0
                },
                {
                    "sent": "And any decrease C transpose X as much as you can.",
                    "label": 0
                },
                {
                    "sent": "So in that case that would be the optimum.",
                    "label": 0
                },
                {
                    "sent": "But it can easily see if you change the orientation of C, then the solution will jump to other corner points, and often there could be local minima.",
                    "label": 0
                },
                {
                    "sent": "Let's say 50 points this way, for example then.",
                    "label": 0
                },
                {
                    "sent": "You know you have you have cases where two points are actually local optimal, a neighborhood, their minima.",
                    "label": 0
                },
                {
                    "sent": "But another global minima.",
                    "label": 0
                },
                {
                    "sent": "Even if the curvature is very weak.",
                    "label": 0
                },
                {
                    "sent": "So we should stop here, but this is the reason why this is an SCP and this works for a crossing distribution.",
                    "label": 0
                },
                {
                    "sent": "For linear constraints, and that's.",
                    "label": 0
                },
                {
                    "sent": "So it makes it a convex problem.",
                    "label": 0
                },
                {
                    "sent": "But in general these chance constraints are very difficult, and then people actually replace them by convex constraints are weaker, but guarantee this probability, but are more conservative and there's a lot of theory about those formulations.",
                    "label": 0
                },
                {
                    "sent": "Change the constraints.",
                    "label": 0
                },
                {
                    "sent": "People also thought about.",
                    "label": 0
                },
                {
                    "sent": "It's not normal to approximate the distribution by normal and then applied.",
                    "label": 0
                },
                {
                    "sent": "In theory that seems to work, yeah?",
                    "label": 0
                },
                {
                    "sent": "And also there are ways to actually just upper bound this.",
                    "label": 0
                },
                {
                    "sent": "Or looking at different types of constraints and different distributions to find constraints that actually stronger and guarantee this that are convex and easy to.",
                    "label": 0
                },
                {
                    "sent": "Handling a convex problem.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}