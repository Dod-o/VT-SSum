{
    "id": "4x2ykub6uepe4rmsu6p7m6hayp6vxe46",
    "title": "Learning Probabilistic Stochastic Models from Probabilistic Examples",
    "info": {
        "author": [
            "Stephen Muggleton, Imperial College London"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Human Language Technology",
            "Top->Computer Science->Machine Learning->Inductive Logic Programming"
        ]
    },
    "url": "http://videolectures.net/icml07_muggleton_lpsm/",
    "segmentation": [
        [
            "Yeah, OK, so um.",
            "This talk is on.",
            "A topic related to probabilistic IO P and stochastic relation learning.",
            "And the issue that we are investigating here is.",
            "Whether or not it makes sense in some settings to learn probabilistic models from probabilistic data, and I mean, it's maybe it struck us as a slight anomaly that the standard systems that are out there in SRL and and PLP.",
            "The labels that are on the examples are.",
            "Consist of true false values, typically.",
            "When you think about this for a while, you may think, well, that's very natural because we're getting these from observations of the way things are or they are not in fact, in many settings, and in particular will show in this.",
            "In scientific settings there's quite a natural way of dealing with the noise and degree of.",
            "Of of a truth of of, whether or not examples in this case are up or down, regulated, which is not an all or nothing thing, can be represented as a as a as a probability, and can be learned from effectively."
        ],
        [
            "OK, so the talk is going to consist of some background and motivation.",
            "We're going to.",
            "I'm going to reintroduce.",
            "The framework of abductive SLP's which was presented in LP 2006, but it was presented there as a theoretical framework which was not implemented so since then we have a form of implementation for that.",
            "And it also turned out to be a natural thing to apply for this.",
            "To this to this particular application area.",
            "So the application area is that of learning from metabolic learning to fill gaps in the descriptions of metabolic networks.",
            "And again this is work that was presented in IRP 2005 and accepted for the special issue of that conference.",
            "So we're doing a comparison against existing and existing LP approach, so one of the other merits of what we're doing is that.",
            "We are doing a direct comparison of LP versus probabilistic.",
            "I'll pee a head to head and Pat Langley made a point earlier in the conference that people should be doing this.",
            "Well, that's that's what we've done in this in this paper.",
            "So in order to deal with this issue of how to introduce probabilities into the into the examples, we need to have a formal model of how to do this, and we can do it in such a way that the probabilities are interpretable.",
            "So again we'll I'll describe how that's done in this case, and then I'll show the experiments that we've carried out so far.",
            "We're testing a hypothesis that it actually gives you an advantage to learn from probabilistic data as opposed to not learning from probabilistic data.",
            "So I'll show you how that works out and then discuss and conclude."
        ],
        [
            "OK so um.",
            "The variety of different approaches to probabilistic I LP where we're using stochastic logic programs as our representation language.",
            "That in itself is slightly awkward since previous descriptions of stochastic logic programs have a semantics of distributional semantics, which is not natural in some cases too.",
            "To represent truth status to statements that are true about the world, probabilists probabilistic iobst approaches that have a semantics in possible worlds.",
            "Possible world semantics are easier to interpret.",
            "One of the advances in last year's approach of abductor vessel pieces that if you treat SLP's as a mechanism for deriving hypothesis, probabilistic hypothesis in fact that does turn out to be quite a natural possible world semantics.",
            "And I'll show you how that works.",
            "So the issue that we're looking at is this comparison of IOP versus probabilistic IOP and as part of that the issue is to weather in the probabilistic IOP setting we should.",
            "We should also have probabilistic examples.",
            "And I've already mentioned the the application."
        ],
        [
            "Area, but I need to give you a bit more detail on these topics.",
            "OK, so starting with the rich, going back to abductive SLP's, what is it that is novel here?",
            "Well, to begin with, you might.",
            "Find it strange that you could have a new representation which you could call abductive SLP's.",
            "They actually the difference between an SLP and an abductor vessel.",
            "P is not very great.",
            "The only difference is the inclusion of Abdou Cibles and a given fact that needs to be explained.",
            "OK, so AB duction normally works on the basis that you're given some case that you're trying to analyze and get an explanation for, and the abductor adduced fact.",
            "Provide you with that explanation.",
            "Now in a logical framework there is.",
            "It's very hard to compare what the merits of the various possible explanations.",
            "If you get multiple explanations with abductive SLP's you get probabilities associated with these various different.",
            "Explanations and so the probability value that's associated with each clause in an SLP in this context.",
            "Is it can be interpreted not as the probability of the head given the body, but rather the probability of the body given the head.",
            "OK, which at first sight that may sound crazy, but in fact that's a natural way to think of explanations.",
            "You're given the head right and you need to find an explanation which is somehow comes from the body.",
            "Now David Poole pointed out to me at one of the stores that there's something weird about stochastic logic programs, in that the bodies are not disjoint.",
            "OK, if you have all of these probabilities summing to one, then how do you interpret that the way that that's done in this context is that every time you form an explanation, you treat it as a world by taking the set of things which are part of that proof as being true and making a closed world assumption for everything else.",
            "OK.",
            "So in the circumstance that only in the case the two bodies are identical, do you end up with the same world?",
            "OK, because all of the other facts that are not proved turn out to be false, so so it's quite reasonable under these situations to assume that the explanations are disjoint.",
            "OK, because the explanations assume that everything that's not part of the proof is false.",
            "OK, so in this setting then we have basically the standard way then of computing the probabilities so derivations are treated as a Markov chain.",
            "So you multiply the probabilities in that chain together to find individual explanations and then if you find there are multiple ways of explaining a particular fact then you can get the overall probability for that for that fact as being the sum of of.",
            "Products OK, so in the normal way.",
            "And the learning setting is is is 1 in which we assume that the examples are all independent, so we're looking for.",
            "A likelihood function in which you take the product of probabilities for the of the individual explanations.",
            "OK, so that's the obvious little SLP."
        ],
        [
            "The setting.",
            "The application which we are using this on is this one which was described before.",
            "But just to remind you of it.",
            "So we are.",
            "We worked with a group that does toxin studies at Imperial College and they have data, nuclear, magnetic resonance data of the effect of making of giving toxins in small amounts to rodents.",
            "So here this poor animal up here is getting a toxin.",
            "Well actually rather weak talks and put in its bloodstream.",
            "And measurements are then made of the urine of the animal, and from those you can interpret the NMR signals which are spectral spectral traces.",
            "Overtime you can you can use those to extract the fluctuation of concentrations of metabolites.",
            "OK, so this graph here shows you the way in which the system is thrown out of out of balance.",
            "And you can see that in the initial when the initial toxin is introduced, then certain metabolites become overexpressed.",
            "Other metabolites become under expressed overtime.",
            "In fact, the whole system settles, so you can see this is a bit like a kind of spring.",
            "That system that's been perturbed.",
            "And the aim of the learning is to take background knowledge, descriptions of the what's known about the metabolic network.",
            "And develop explanations.",
            "Abductive explanations in our case that allow you to explain why things are being upregulated or downregulated by the toxin in terms of the toxins inhibiting various different enzyme.",
            "So we're trying to look for inhibition labels within a network and doing that using AB DUCTION and this is what we did in the previous IOP study."
        ],
        [
            "OK, so the one of the well the overall outcome of that was a map.",
            "A bit like this.",
            "So this is one of the solutions that we found.",
            "And just to describe what's going on here, the rectangles are enzymes.",
            "So sorry, the rectangles are metabolites.",
            "These are essential molecules in the activity of the cell, and between these you have enzymes which regulate reaction, so this is a small section out of out of keg that our experts helped us to focus in on finding explanations for.",
            "The IO P solutions to this.",
            "So there are set of red labels that you see there which are claims as to inhibition points.",
            "You can think of these as a bit like traffic lights that are stopping the flow and at various different points and these give us an explanation of the full set of up and down regulation.",
            "So you notice these up and down triangles.",
            "Those are up and down regulated points of the metabolites.",
            "So our experts really like this and in fact some new.",
            "Suggestions came out of this that they went on to test.",
            "One thing though that they pointed out that they didn't like was that it was very brittle.",
            "It was very all or nothing OK and that didn't fit the circumstances well.",
            "So in fact these up and down regulations are true to some degree and in some cases.",
            "So for instance with create creatinine they kept flipping between saying well this is not really upregulated or it's not downregulated.",
            "If you look at the graphs it's unclear whether it's upregulated or downregulated.",
            "And they also found it disturbing that again, these inhibition points were all or nothing, because that doesn't really fit the situation in which the inhibition holds to some degree.",
            "OK, so.",
            "We were looking already for some way of including a softer description that would give better insight to the experts.",
            "OK, so the idea is to increase the insight by introducing probabilities.",
            "Might seem strange that you think, well these are nasty numbers, but actually the numbers give you some extra information that's useful for interpreting what."
        ],
        [
            "Going on for the experts.",
            "So the question was how do we?",
            "How do we get a soft description of the degree of up and down regulation?",
            "OK, now it turns out that there is a formal way of doing this that makes sense in the context of a scientific experiment.",
            "So when you carry out normal scientifics experiment, you don't come up with positive and negative examples as you do with as you required to with machine learning.",
            "In fact, what you typically do is you have two sets of.",
            "Of of treatments, one of which is a control, you just make sure that everything is the same with those set of subjects.",
            "So those set of rats in this case and the other is a treated situation.",
            "OK, so the ones that were injected with the toxin act differently to the control and then to decide the degree to which the treatment has an effect.",
            "You compare it to the control animals.",
            "So in fact we used this fact in order to extract the probability labels.",
            "We took the control data, which is fluctuate ING at a low level, and we use that to derive well.",
            "We made an assumption that we that the data is normally distributed, we derived the mean and standard deviation of that of the.",
            "Control data and then we use the values of the treatment data in order to get a probability which is based on the integral of the of the normal distribution.",
            "OK, so this is a standard idea from statistics, so if you take the point X there and you ask yourself to what degree is that upregulated?",
            "Well, it's actually the integral up to the point X of the of the Gaussian distribution.",
            "And that gives you a probability value which corresponds to if you like to a degree to which that is upregulated with respect to the control data.",
            "OK, so this algorithm simply does that.",
            "OK, so we went through an re labeled all of the data in terms of probability labels in this fashion, and it's quite straightforward to interpret what those probabilities mean in the context.",
            "OK."
        ],
        [
            "So the experiments we carried out we had a null hypothesis, which is that by using the probability labels you get better accuracy than if you don't.",
            "We took the we learned the.",
            "We used a learning method that was a mixture of standard IO P, program 5, abduction engine, together with learning of the probability labels using James Cousins's failure adjusted maximization algorithm, which is an EM type algorithm and we then ran ran these experiments.",
            "OK So what?"
        ],
        [
            "With the outcome.",
            "So we're doing a comparison here of the previous I LP results OK against categorical Learning and SLP learning.",
            "In fact, I've listed out every individual prediction value that was given for for leave.",
            "One out prediction for all of these.",
            "So the first column is the the name of the metabolites involved, then the concentration, which is a coarse grained up or down regulation that was used for the LP learning.",
            "The empirical probability calculated in the fashion that I've just shown you.",
            "Then the leave one hour prediction for the individual cases that I LP get gave the leave one out predictions of probability for the categorical learning and Lastly leave one out predictions for the probabilistic learning and this is summarized at the bottom and the overall effect of this is that if you treat this as the.",
            "Calculate the expected error, then the expected accuracy.",
            "Expected accuracy overall is significantly higher in the case of SLP, of the probability learning than both the categorical and the LP learning OK, so."
        ],
        [
            "That refutes the hypothesis of the experiment.",
            "What we having worked out there where we're doing better in terms of accuracy.",
            "It's then worth looking in more detail as to what kind of explanations were getting out.",
            "In fact, you'll find that the explanations are similar in many cases.",
            "They differ in critical cases and in cases that we were finding hard to explain previously, we're now getting probability relations between the interpretation of the various different numbers.",
            "So the difference between this slide in the previous one is that we have.",
            "Numerical probabilities throughout here the interpretations of these must be made under the under the closed world assumption, with the with where you you can think of this as being a system in which one of only one event, one inhibiting event, is able to happen at any one time.",
            "So take each one of these probabilities as the probability that would be necessary for explain the data as best as possible, given that the probability of inhibition is as it is here.",
            "OK, so."
        ],
        [
            "The detail of what we have.",
            "We have a set of abuse obols which have associated probability values that are learned by fam probabilistic background knowledge, which has again is updated by farm in order to provide the explanations.",
            "Then we have some non probabilistic background knowledge.",
            "So these are pure prolog clauses and last of all the observations where the probabilities are attached at the end.",
            "OK, so we had to.",
            "Play around with farm in order to force it to be able to do this.",
            "We did that by a mechanism of replicating in the initial examples, which gives the effect of introducing probabilities.",
            "So if you have a ratio of 700 to two in the between 2 examples, then that makes makes a probability, which is 50 times higher for the for the set."
        ],
        [
            "Guy OK so the overall conclusions are that we can indeed learn from probabilistic examples and that to do so actually produces higher accuracy in the hydrazine case that we tested.",
            "In fact, we looked at a second network after submitting the paper, which the experts had told us was harder to discriminate the Anet.",
            "Data set, and in that case, although we had higher accuracies, they were not significantly higher.",
            "OK, so we can't make a very strong claim in the second case, so there's still more more work to do.",
            "The improvement that we were able to find was not only in predictive accuracy, not only does this sort of probabilistic logical approach improve the accuracy, it actually improves the interpretability, surprisingly.",
            "We had problems with overfitting which were related to the to the fact that we have more parameters than we have data and so it would be nice to be able to look at other systems in which this was not the case.",
            "And we're also hoping to see whether these effects hold out using alternative systems other than pro goal and SLP's.",
            "OK.",
            "So at first sight the problem here seems to be that progression that you need to write because of regulation and regulation.",
            "It's a continuous variable, not so much.",
            "At the example is probabilistic.",
            "So why did you go this step of turning the continuous value into probability by assuming a normal distribution instead of just?",
            "You know, trying to use it predicted continuous value directly.",
            "OK, regression would have been another way to do it and I think maybe it would be interesting to have a different experiment in which we tested whether or not you can.",
            "And improve the performance.",
            "It's actually would be quite hard to compare that against standard IO P in the way that we did becausw the value is so unlike a truth value, so I would prefer to do that as a kind of 2nd experiment to this one.",
            "But it would be.",
            "It would certainly be interesting.",
            "OK, so the the the technical difficulties I spoke OK so the question was what are the technical difficulties in using probabilistic examples within the program?",
            "And the one of the issues I think that has to be considered is that you if you only have limited amounts of data and you're trying to estimate a probability distribution from those, you actually get your own fairly shaky ground unless you've got more than 30 examples to make assumptions about normal distributions and so on.",
            "So that's one of one of the issues.",
            "And I think that's one of the things that would push us to looking at, you know further.",
            "Further, applications further experimental applications.",
            "Because we were pretty much on the border of of what we can do and get away with mathematically.",
            "Oh yeah, yeah yeah, I mean, this is in every stat package.",
            "You can calculate these probabilities in this way.",
            "Yeah, yeah that's true.",
            "I mean, if you had enough examples it would wash out.",
            "So yeah, but it does introduce an extra element of noise.",
            "Hello.",
            "Sing just.",
            "It seems to me.",
            "Regular regulation.",
            "This influences.",
            "You want to do regression.",
            "There are some.",
            "Something else as well.",
            "OK, so this is a good question.",
            "I mean part of the reason that I would maintain that probabilistic angle makes sense here is because of the sparseness of the data.",
            "We've got not.",
            "We know we have noise in the state of becausw.",
            "We're comparing it again against a control signal, which is clearly noisy.",
            "OK, so we do have an issue about whether or not we believe the degree to which we appear to have upregulation Israel OK, because in certain cases.",
            "You value will look high even though really what you're measuring is something that's low.",
            "OK, so there is a probabilistic element to this which needs to be represented.",
            "I take your point that we might want to find some, say, a decision theoretic way of mixing, producing an expectation or expectation with variance or something of that kind.",
            "In fact, we can take the probability and go back to the initial values and turn it back into.",
            "An actual value that's being predicted because we have that mapping.",
            "We've had to calculate it.",
            "It's a convenience that we are.",
            "We're putting this in a single value as a degree, but it's not the only way to do it.",
            "Should should.",
            "Yeah, for.",
            "Yeah, that's a good point."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, OK, so um.",
                    "label": 0
                },
                {
                    "sent": "This talk is on.",
                    "label": 0
                },
                {
                    "sent": "A topic related to probabilistic IO P and stochastic relation learning.",
                    "label": 0
                },
                {
                    "sent": "And the issue that we are investigating here is.",
                    "label": 0
                },
                {
                    "sent": "Whether or not it makes sense in some settings to learn probabilistic models from probabilistic data, and I mean, it's maybe it struck us as a slight anomaly that the standard systems that are out there in SRL and and PLP.",
                    "label": 1
                },
                {
                    "sent": "The labels that are on the examples are.",
                    "label": 0
                },
                {
                    "sent": "Consist of true false values, typically.",
                    "label": 0
                },
                {
                    "sent": "When you think about this for a while, you may think, well, that's very natural because we're getting these from observations of the way things are or they are not in fact, in many settings, and in particular will show in this.",
                    "label": 0
                },
                {
                    "sent": "In scientific settings there's quite a natural way of dealing with the noise and degree of.",
                    "label": 0
                },
                {
                    "sent": "Of of a truth of of, whether or not examples in this case are up or down, regulated, which is not an all or nothing thing, can be represented as a as a as a probability, and can be learned from effectively.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the talk is going to consist of some background and motivation.",
                    "label": 1
                },
                {
                    "sent": "We're going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to reintroduce.",
                    "label": 0
                },
                {
                    "sent": "The framework of abductive SLP's which was presented in LP 2006, but it was presented there as a theoretical framework which was not implemented so since then we have a form of implementation for that.",
                    "label": 0
                },
                {
                    "sent": "And it also turned out to be a natural thing to apply for this.",
                    "label": 0
                },
                {
                    "sent": "To this to this particular application area.",
                    "label": 0
                },
                {
                    "sent": "So the application area is that of learning from metabolic learning to fill gaps in the descriptions of metabolic networks.",
                    "label": 0
                },
                {
                    "sent": "And again this is work that was presented in IRP 2005 and accepted for the special issue of that conference.",
                    "label": 0
                },
                {
                    "sent": "So we're doing a comparison against existing and existing LP approach, so one of the other merits of what we're doing is that.",
                    "label": 0
                },
                {
                    "sent": "We are doing a direct comparison of LP versus probabilistic.",
                    "label": 0
                },
                {
                    "sent": "I'll pee a head to head and Pat Langley made a point earlier in the conference that people should be doing this.",
                    "label": 0
                },
                {
                    "sent": "Well, that's that's what we've done in this in this paper.",
                    "label": 0
                },
                {
                    "sent": "So in order to deal with this issue of how to introduce probabilities into the into the examples, we need to have a formal model of how to do this, and we can do it in such a way that the probabilities are interpretable.",
                    "label": 0
                },
                {
                    "sent": "So again we'll I'll describe how that's done in this case, and then I'll show the experiments that we've carried out so far.",
                    "label": 0
                },
                {
                    "sent": "We're testing a hypothesis that it actually gives you an advantage to learn from probabilistic data as opposed to not learning from probabilistic data.",
                    "label": 0
                },
                {
                    "sent": "So I'll show you how that works out and then discuss and conclude.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so um.",
                    "label": 0
                },
                {
                    "sent": "The variety of different approaches to probabilistic I LP where we're using stochastic logic programs as our representation language.",
                    "label": 1
                },
                {
                    "sent": "That in itself is slightly awkward since previous descriptions of stochastic logic programs have a semantics of distributional semantics, which is not natural in some cases too.",
                    "label": 1
                },
                {
                    "sent": "To represent truth status to statements that are true about the world, probabilists probabilistic iobst approaches that have a semantics in possible worlds.",
                    "label": 0
                },
                {
                    "sent": "Possible world semantics are easier to interpret.",
                    "label": 0
                },
                {
                    "sent": "One of the advances in last year's approach of abductor vessel pieces that if you treat SLP's as a mechanism for deriving hypothesis, probabilistic hypothesis in fact that does turn out to be quite a natural possible world semantics.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you how that works.",
                    "label": 0
                },
                {
                    "sent": "So the issue that we're looking at is this comparison of IOP versus probabilistic IOP and as part of that the issue is to weather in the probabilistic IOP setting we should.",
                    "label": 0
                },
                {
                    "sent": "We should also have probabilistic examples.",
                    "label": 0
                },
                {
                    "sent": "And I've already mentioned the the application.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Area, but I need to give you a bit more detail on these topics.",
                    "label": 0
                },
                {
                    "sent": "OK, so starting with the rich, going back to abductive SLP's, what is it that is novel here?",
                    "label": 0
                },
                {
                    "sent": "Well, to begin with, you might.",
                    "label": 0
                },
                {
                    "sent": "Find it strange that you could have a new representation which you could call abductive SLP's.",
                    "label": 0
                },
                {
                    "sent": "They actually the difference between an SLP and an abductor vessel.",
                    "label": 0
                },
                {
                    "sent": "P is not very great.",
                    "label": 1
                },
                {
                    "sent": "The only difference is the inclusion of Abdou Cibles and a given fact that needs to be explained.",
                    "label": 0
                },
                {
                    "sent": "OK, so AB duction normally works on the basis that you're given some case that you're trying to analyze and get an explanation for, and the abductor adduced fact.",
                    "label": 0
                },
                {
                    "sent": "Provide you with that explanation.",
                    "label": 0
                },
                {
                    "sent": "Now in a logical framework there is.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to compare what the merits of the various possible explanations.",
                    "label": 0
                },
                {
                    "sent": "If you get multiple explanations with abductive SLP's you get probabilities associated with these various different.",
                    "label": 0
                },
                {
                    "sent": "Explanations and so the probability value that's associated with each clause in an SLP in this context.",
                    "label": 0
                },
                {
                    "sent": "Is it can be interpreted not as the probability of the head given the body, but rather the probability of the body given the head.",
                    "label": 1
                },
                {
                    "sent": "OK, which at first sight that may sound crazy, but in fact that's a natural way to think of explanations.",
                    "label": 0
                },
                {
                    "sent": "You're given the head right and you need to find an explanation which is somehow comes from the body.",
                    "label": 0
                },
                {
                    "sent": "Now David Poole pointed out to me at one of the stores that there's something weird about stochastic logic programs, in that the bodies are not disjoint.",
                    "label": 0
                },
                {
                    "sent": "OK, if you have all of these probabilities summing to one, then how do you interpret that the way that that's done in this context is that every time you form an explanation, you treat it as a world by taking the set of things which are part of that proof as being true and making a closed world assumption for everything else.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in the circumstance that only in the case the two bodies are identical, do you end up with the same world?",
                    "label": 0
                },
                {
                    "sent": "OK, because all of the other facts that are not proved turn out to be false, so so it's quite reasonable under these situations to assume that the explanations are disjoint.",
                    "label": 0
                },
                {
                    "sent": "OK, because the explanations assume that everything that's not part of the proof is false.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this setting then we have basically the standard way then of computing the probabilities so derivations are treated as a Markov chain.",
                    "label": 0
                },
                {
                    "sent": "So you multiply the probabilities in that chain together to find individual explanations and then if you find there are multiple ways of explaining a particular fact then you can get the overall probability for that for that fact as being the sum of of.",
                    "label": 0
                },
                {
                    "sent": "Products OK, so in the normal way.",
                    "label": 0
                },
                {
                    "sent": "And the learning setting is is is 1 in which we assume that the examples are all independent, so we're looking for.",
                    "label": 0
                },
                {
                    "sent": "A likelihood function in which you take the product of probabilities for the of the individual explanations.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the obvious little SLP.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The setting.",
                    "label": 0
                },
                {
                    "sent": "The application which we are using this on is this one which was described before.",
                    "label": 0
                },
                {
                    "sent": "But just to remind you of it.",
                    "label": 0
                },
                {
                    "sent": "So we are.",
                    "label": 0
                },
                {
                    "sent": "We worked with a group that does toxin studies at Imperial College and they have data, nuclear, magnetic resonance data of the effect of making of giving toxins in small amounts to rodents.",
                    "label": 0
                },
                {
                    "sent": "So here this poor animal up here is getting a toxin.",
                    "label": 0
                },
                {
                    "sent": "Well actually rather weak talks and put in its bloodstream.",
                    "label": 0
                },
                {
                    "sent": "And measurements are then made of the urine of the animal, and from those you can interpret the NMR signals which are spectral spectral traces.",
                    "label": 0
                },
                {
                    "sent": "Overtime you can you can use those to extract the fluctuation of concentrations of metabolites.",
                    "label": 0
                },
                {
                    "sent": "OK, so this graph here shows you the way in which the system is thrown out of out of balance.",
                    "label": 0
                },
                {
                    "sent": "And you can see that in the initial when the initial toxin is introduced, then certain metabolites become overexpressed.",
                    "label": 0
                },
                {
                    "sent": "Other metabolites become under expressed overtime.",
                    "label": 0
                },
                {
                    "sent": "In fact, the whole system settles, so you can see this is a bit like a kind of spring.",
                    "label": 0
                },
                {
                    "sent": "That system that's been perturbed.",
                    "label": 0
                },
                {
                    "sent": "And the aim of the learning is to take background knowledge, descriptions of the what's known about the metabolic network.",
                    "label": 1
                },
                {
                    "sent": "And develop explanations.",
                    "label": 0
                },
                {
                    "sent": "Abductive explanations in our case that allow you to explain why things are being upregulated or downregulated by the toxin in terms of the toxins inhibiting various different enzyme.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to look for inhibition labels within a network and doing that using AB DUCTION and this is what we did in the previous IOP study.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the one of the well the overall outcome of that was a map.",
                    "label": 0
                },
                {
                    "sent": "A bit like this.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the solutions that we found.",
                    "label": 0
                },
                {
                    "sent": "And just to describe what's going on here, the rectangles are enzymes.",
                    "label": 0
                },
                {
                    "sent": "So sorry, the rectangles are metabolites.",
                    "label": 0
                },
                {
                    "sent": "These are essential molecules in the activity of the cell, and between these you have enzymes which regulate reaction, so this is a small section out of out of keg that our experts helped us to focus in on finding explanations for.",
                    "label": 0
                },
                {
                    "sent": "The IO P solutions to this.",
                    "label": 0
                },
                {
                    "sent": "So there are set of red labels that you see there which are claims as to inhibition points.",
                    "label": 0
                },
                {
                    "sent": "You can think of these as a bit like traffic lights that are stopping the flow and at various different points and these give us an explanation of the full set of up and down regulation.",
                    "label": 0
                },
                {
                    "sent": "So you notice these up and down triangles.",
                    "label": 0
                },
                {
                    "sent": "Those are up and down regulated points of the metabolites.",
                    "label": 0
                },
                {
                    "sent": "So our experts really like this and in fact some new.",
                    "label": 0
                },
                {
                    "sent": "Suggestions came out of this that they went on to test.",
                    "label": 0
                },
                {
                    "sent": "One thing though that they pointed out that they didn't like was that it was very brittle.",
                    "label": 0
                },
                {
                    "sent": "It was very all or nothing OK and that didn't fit the circumstances well.",
                    "label": 0
                },
                {
                    "sent": "So in fact these up and down regulations are true to some degree and in some cases.",
                    "label": 0
                },
                {
                    "sent": "So for instance with create creatinine they kept flipping between saying well this is not really upregulated or it's not downregulated.",
                    "label": 0
                },
                {
                    "sent": "If you look at the graphs it's unclear whether it's upregulated or downregulated.",
                    "label": 0
                },
                {
                    "sent": "And they also found it disturbing that again, these inhibition points were all or nothing, because that doesn't really fit the situation in which the inhibition holds to some degree.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We were looking already for some way of including a softer description that would give better insight to the experts.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is to increase the insight by introducing probabilities.",
                    "label": 0
                },
                {
                    "sent": "Might seem strange that you think, well these are nasty numbers, but actually the numbers give you some extra information that's useful for interpreting what.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going on for the experts.",
                    "label": 0
                },
                {
                    "sent": "So the question was how do we?",
                    "label": 0
                },
                {
                    "sent": "How do we get a soft description of the degree of up and down regulation?",
                    "label": 0
                },
                {
                    "sent": "OK, now it turns out that there is a formal way of doing this that makes sense in the context of a scientific experiment.",
                    "label": 1
                },
                {
                    "sent": "So when you carry out normal scientifics experiment, you don't come up with positive and negative examples as you do with as you required to with machine learning.",
                    "label": 0
                },
                {
                    "sent": "In fact, what you typically do is you have two sets of.",
                    "label": 0
                },
                {
                    "sent": "Of of treatments, one of which is a control, you just make sure that everything is the same with those set of subjects.",
                    "label": 1
                },
                {
                    "sent": "So those set of rats in this case and the other is a treated situation.",
                    "label": 1
                },
                {
                    "sent": "OK, so the ones that were injected with the toxin act differently to the control and then to decide the degree to which the treatment has an effect.",
                    "label": 1
                },
                {
                    "sent": "You compare it to the control animals.",
                    "label": 0
                },
                {
                    "sent": "So in fact we used this fact in order to extract the probability labels.",
                    "label": 0
                },
                {
                    "sent": "We took the control data, which is fluctuate ING at a low level, and we use that to derive well.",
                    "label": 0
                },
                {
                    "sent": "We made an assumption that we that the data is normally distributed, we derived the mean and standard deviation of that of the.",
                    "label": 0
                },
                {
                    "sent": "Control data and then we use the values of the treatment data in order to get a probability which is based on the integral of the of the normal distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a standard idea from statistics, so if you take the point X there and you ask yourself to what degree is that upregulated?",
                    "label": 1
                },
                {
                    "sent": "Well, it's actually the integral up to the point X of the of the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "And that gives you a probability value which corresponds to if you like to a degree to which that is upregulated with respect to the control data.",
                    "label": 0
                },
                {
                    "sent": "OK, so this algorithm simply does that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we went through an re labeled all of the data in terms of probability labels in this fashion, and it's quite straightforward to interpret what those probabilities mean in the context.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the experiments we carried out we had a null hypothesis, which is that by using the probability labels you get better accuracy than if you don't.",
                    "label": 0
                },
                {
                    "sent": "We took the we learned the.",
                    "label": 0
                },
                {
                    "sent": "We used a learning method that was a mixture of standard IO P, program 5, abduction engine, together with learning of the probability labels using James Cousins's failure adjusted maximization algorithm, which is an EM type algorithm and we then ran ran these experiments.",
                    "label": 0
                },
                {
                    "sent": "OK So what?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the outcome.",
                    "label": 0
                },
                {
                    "sent": "So we're doing a comparison here of the previous I LP results OK against categorical Learning and SLP learning.",
                    "label": 0
                },
                {
                    "sent": "In fact, I've listed out every individual prediction value that was given for for leave.",
                    "label": 0
                },
                {
                    "sent": "One out prediction for all of these.",
                    "label": 0
                },
                {
                    "sent": "So the first column is the the name of the metabolites involved, then the concentration, which is a coarse grained up or down regulation that was used for the LP learning.",
                    "label": 0
                },
                {
                    "sent": "The empirical probability calculated in the fashion that I've just shown you.",
                    "label": 0
                },
                {
                    "sent": "Then the leave one hour prediction for the individual cases that I LP get gave the leave one out predictions of probability for the categorical learning and Lastly leave one out predictions for the probabilistic learning and this is summarized at the bottom and the overall effect of this is that if you treat this as the.",
                    "label": 0
                },
                {
                    "sent": "Calculate the expected error, then the expected accuracy.",
                    "label": 0
                },
                {
                    "sent": "Expected accuracy overall is significantly higher in the case of SLP, of the probability learning than both the categorical and the LP learning OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That refutes the hypothesis of the experiment.",
                    "label": 0
                },
                {
                    "sent": "What we having worked out there where we're doing better in terms of accuracy.",
                    "label": 0
                },
                {
                    "sent": "It's then worth looking in more detail as to what kind of explanations were getting out.",
                    "label": 0
                },
                {
                    "sent": "In fact, you'll find that the explanations are similar in many cases.",
                    "label": 0
                },
                {
                    "sent": "They differ in critical cases and in cases that we were finding hard to explain previously, we're now getting probability relations between the interpretation of the various different numbers.",
                    "label": 0
                },
                {
                    "sent": "So the difference between this slide in the previous one is that we have.",
                    "label": 0
                },
                {
                    "sent": "Numerical probabilities throughout here the interpretations of these must be made under the under the closed world assumption, with the with where you you can think of this as being a system in which one of only one event, one inhibiting event, is able to happen at any one time.",
                    "label": 0
                },
                {
                    "sent": "So take each one of these probabilities as the probability that would be necessary for explain the data as best as possible, given that the probability of inhibition is as it is here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The detail of what we have.",
                    "label": 0
                },
                {
                    "sent": "We have a set of abuse obols which have associated probability values that are learned by fam probabilistic background knowledge, which has again is updated by farm in order to provide the explanations.",
                    "label": 0
                },
                {
                    "sent": "Then we have some non probabilistic background knowledge.",
                    "label": 1
                },
                {
                    "sent": "So these are pure prolog clauses and last of all the observations where the probabilities are attached at the end.",
                    "label": 0
                },
                {
                    "sent": "OK, so we had to.",
                    "label": 0
                },
                {
                    "sent": "Play around with farm in order to force it to be able to do this.",
                    "label": 0
                },
                {
                    "sent": "We did that by a mechanism of replicating in the initial examples, which gives the effect of introducing probabilities.",
                    "label": 0
                },
                {
                    "sent": "So if you have a ratio of 700 to two in the between 2 examples, then that makes makes a probability, which is 50 times higher for the for the set.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Guy OK so the overall conclusions are that we can indeed learn from probabilistic examples and that to do so actually produces higher accuracy in the hydrazine case that we tested.",
                    "label": 1
                },
                {
                    "sent": "In fact, we looked at a second network after submitting the paper, which the experts had told us was harder to discriminate the Anet.",
                    "label": 0
                },
                {
                    "sent": "Data set, and in that case, although we had higher accuracies, they were not significantly higher.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can't make a very strong claim in the second case, so there's still more more work to do.",
                    "label": 0
                },
                {
                    "sent": "The improvement that we were able to find was not only in predictive accuracy, not only does this sort of probabilistic logical approach improve the accuracy, it actually improves the interpretability, surprisingly.",
                    "label": 0
                },
                {
                    "sent": "We had problems with overfitting which were related to the to the fact that we have more parameters than we have data and so it would be nice to be able to look at other systems in which this was not the case.",
                    "label": 0
                },
                {
                    "sent": "And we're also hoping to see whether these effects hold out using alternative systems other than pro goal and SLP's.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So at first sight the problem here seems to be that progression that you need to write because of regulation and regulation.",
                    "label": 0
                },
                {
                    "sent": "It's a continuous variable, not so much.",
                    "label": 0
                },
                {
                    "sent": "At the example is probabilistic.",
                    "label": 0
                },
                {
                    "sent": "So why did you go this step of turning the continuous value into probability by assuming a normal distribution instead of just?",
                    "label": 0
                },
                {
                    "sent": "You know, trying to use it predicted continuous value directly.",
                    "label": 0
                },
                {
                    "sent": "OK, regression would have been another way to do it and I think maybe it would be interesting to have a different experiment in which we tested whether or not you can.",
                    "label": 0
                },
                {
                    "sent": "And improve the performance.",
                    "label": 0
                },
                {
                    "sent": "It's actually would be quite hard to compare that against standard IO P in the way that we did becausw the value is so unlike a truth value, so I would prefer to do that as a kind of 2nd experiment to this one.",
                    "label": 0
                },
                {
                    "sent": "But it would be.",
                    "label": 0
                },
                {
                    "sent": "It would certainly be interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, so the the the technical difficulties I spoke OK so the question was what are the technical difficulties in using probabilistic examples within the program?",
                    "label": 0
                },
                {
                    "sent": "And the one of the issues I think that has to be considered is that you if you only have limited amounts of data and you're trying to estimate a probability distribution from those, you actually get your own fairly shaky ground unless you've got more than 30 examples to make assumptions about normal distributions and so on.",
                    "label": 0
                },
                {
                    "sent": "So that's one of one of the issues.",
                    "label": 0
                },
                {
                    "sent": "And I think that's one of the things that would push us to looking at, you know further.",
                    "label": 0
                },
                {
                    "sent": "Further, applications further experimental applications.",
                    "label": 0
                },
                {
                    "sent": "Because we were pretty much on the border of of what we can do and get away with mathematically.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, yeah yeah, I mean, this is in every stat package.",
                    "label": 0
                },
                {
                    "sent": "You can calculate these probabilities in this way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah that's true.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you had enough examples it would wash out.",
                    "label": 0
                },
                {
                    "sent": "So yeah, but it does introduce an extra element of noise.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "Sing just.",
                    "label": 0
                },
                {
                    "sent": "It seems to me.",
                    "label": 0
                },
                {
                    "sent": "Regular regulation.",
                    "label": 0
                },
                {
                    "sent": "This influences.",
                    "label": 0
                },
                {
                    "sent": "You want to do regression.",
                    "label": 0
                },
                {
                    "sent": "There are some.",
                    "label": 0
                },
                {
                    "sent": "Something else as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a good question.",
                    "label": 1
                },
                {
                    "sent": "I mean part of the reason that I would maintain that probabilistic angle makes sense here is because of the sparseness of the data.",
                    "label": 0
                },
                {
                    "sent": "We've got not.",
                    "label": 0
                },
                {
                    "sent": "We know we have noise in the state of becausw.",
                    "label": 0
                },
                {
                    "sent": "We're comparing it again against a control signal, which is clearly noisy.",
                    "label": 0
                },
                {
                    "sent": "OK, so we do have an issue about whether or not we believe the degree to which we appear to have upregulation Israel OK, because in certain cases.",
                    "label": 0
                },
                {
                    "sent": "You value will look high even though really what you're measuring is something that's low.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a probabilistic element to this which needs to be represented.",
                    "label": 0
                },
                {
                    "sent": "I take your point that we might want to find some, say, a decision theoretic way of mixing, producing an expectation or expectation with variance or something of that kind.",
                    "label": 0
                },
                {
                    "sent": "In fact, we can take the probability and go back to the initial values and turn it back into.",
                    "label": 0
                },
                {
                    "sent": "An actual value that's being predicted because we have that mapping.",
                    "label": 0
                },
                {
                    "sent": "We've had to calculate it.",
                    "label": 0
                },
                {
                    "sent": "It's a convenience that we are.",
                    "label": 0
                },
                {
                    "sent": "We're putting this in a single value as a degree, but it's not the only way to do it.",
                    "label": 0
                },
                {
                    "sent": "Should should.",
                    "label": 0
                },
                {
                    "sent": "Yeah, for.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good point.",
                    "label": 0
                }
            ]
        }
    }
}