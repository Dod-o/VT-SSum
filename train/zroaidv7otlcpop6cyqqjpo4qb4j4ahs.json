{
    "id": "zroaidv7otlcpop6cyqqjpo4qb4j4ahs",
    "title": "Improving Morphosyntactic Tagging of Slovene by Tagger Combination",
    "info": {
        "author": [
            "Jan Rupnik, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "coauthor": [
            "Miha Gr\u010dar, Jo\u017eef Stefan Institute",
            "Toma\u017e Erjavec, Jo\u017eef Stefan Institute"
        ],
        "published": "Nov. 7, 2008",
        "recorded": "October 2008",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Machine Learning->Human Language Technology",
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/sikdd08_rupnik_imt/",
    "segmentation": [
        [
            "Combining part of speech taggers.",
            "Team learning, combining the nightly constructed one which automatically constructed one, so this is joint work with me here, Gurcharan Tamasha rabbits, and the purpose of this work is to improve this tagging part of speech.",
            "Tagging accuracy for Slovenian language by using two taggers that were state of the art and at the moment and the combine them to get a better."
        ],
        [
            "Dagger, so first I'll give you the introduction and some motivation.",
            "Then I'll describe the approach of stagger combination and some experiments."
        ],
        [
            "So part of speech tagging is assigning morphosyntactic in descriptor store words in a sentence.",
            "For instance, here we have nouns, verbs, and prepositions and adjectives."
        ],
        [
            "But um, for, let's say Slovenian part of speech tagging is quite a difficult problem, because there are 2002 unique tags.",
            "Uh, that are defined for Slovenian language because it is a highly inflectional language.",
            "For instance, we have A tag, or they're also called M as this this age you FPA, which which means that the category is adjective type is general degrees undefined, and so on.",
            "So all these.",
            "Combinations give us this large set of classes possible.",
            "Classes where, for example, in English there's only 50 or 660 usually."
        ],
        [
            "It's a lot easier.",
            "So the two of state of the art dagger Sarah maybe studio proprietary attacker.",
            "It's based on handcrafted rules how to.",
            "And the Italy.",
            "It's performance is about 85%, so the accuracy and the second one is TNT tagger, which is based on the statistical modeling of sentences in their part of speech tags.",
            "And it uses hidden Markov Model 3 gram technology.",
            "So this one has to be trained on the large corpus.",
            "So how these two taggers are created there are very independent so.",
            "Um and TNT achieves about 86% accuracy as well, which is still.",
            "Too low for, let's say for English we have over 95 usually, so there's some.",
            "These things need to be improved and I'll describe the approach."
        ],
        [
            "In this direction.",
            "So first, first of all we have this data set of 100,000 words.",
            "The from this is the use corpus.",
            "It's uh, it's 100,000 words of in.",
            "In Slovenian.",
            "We with their M as this, so there are annotated and when we ran the two Tigers we can analyze how how their predictions in the correct tags, in what correspondence is, they are, for instance the green part of the circle means that roughly 3/4 of the of the words were correctly classified by both.",
            "Classifiers the, the blue part means that both daggers predicted incorrect, but different tax and it's a small portion of the words and the yellow one represents the boat staggers, predicted the same and incorrect tag.",
            "And what is interesting for us?",
            "Or are these two parts so one is the Scion part where, uh, maybe stagger.",
            "Cooper predicted the correct tag and TNT predicted the four stag and the purple part where the situation is reversed so.",
            "If we take only TNT, we lose the Scion part.",
            "If we take only a maybe sweet lose this purple part and our idea is to try to work on this subset of the words and try to try to get a larger portion than just one one of those two parts, so."
        ],
        [
            "And this is an example.",
            "So our training corpus consists of truth.",
            "This sentences with their two annotations that were hand validated and TNT and maybes predictions.",
            "So in the first case we see that TNT predicted for predicted correct tag and maybe didn't end the cell.",
            "The other cases seen cases where maybe predicted the correct one.",
            "So we will only use these words for our training over this meta tagger, which will be a combination of these two Tigers."
        ],
        [
            "So this is a.",
            "This is the high level description of our approach, so we have a sequence of words.",
            "So we have these two taggers.",
            "Each tagger provides A tag and are we build our our meta tagger on top of those two taggers which decides for every word which tagger we should listen to.",
            "So which tag we should take into account."
        ],
        [
            "And this tagger is is, basically it's a binary classifier so in.",
            "So little to go into a little bit more details we have for every word.",
            "We have two tags that were predicted, then we compute some features from those two tags and feed them into our meta tagger, which is a binary classifier and then the binary classifier basically predicts TNT or a maybe so which tag we should take.",
            "And the, let's say in this case, the true the tag was there, maybe stack, and then the meta tagger predicts the memes."
        ],
        [
            "So now now a bit about the future vector construction.",
            "So the first set of features are for every MSD we we come, we decompose it completely into all those sub attributes like type, gender, part of speech.",
            "And so every one of those sub attributes will will define one feature, one feature.",
            "And we do so for, uh, maybe San T&T.",
            "We join those features into one large feature vector, and then we also had agreement features which tell us forever for every subtype of of the tag they tell us if they the taggers agreed.",
            "For instance, here we see that they didn't agree in the case.",
            "So the case agreement was no missing this.",
            "So, and the labels are just since I already showed you that we had this gold standard here this validated text.",
            "So these are these represent the labels in arlar."
        ],
        [
            "Problem.",
            "Oh, OK, and the second thing is that we we also added some context features in some, some other because we have two types of experiments and in context features we look not only at the central work but also the two surrounding words and just added the TNT.",
            "And then maybe those decomposition features for the leftward and for the right words so.",
            "So I I think that the this this constitutes around 100 features and in the end or something."
        ],
        [
            "So then, as we said, the meta tagger is binary classifier and we experimented with three classifiers.",
            "So one was naive Bayes classifier which is a probabilistic classifier and assumes strong independence sample assumptions.",
            "And it's it's.",
            "It's sometimes works really well.",
            "So for instance in text specification.",
            "So we we wanted to see what happens here.",
            "No, the second one RCN 2 rules.",
            "It's a covering algorithm and the nice thing about it is that the predictions the rules are easily interpretable for for the human reader.",
            "And but the problem is that the algorithm is quite slow, so if we have a large corpus, training is really computationally expensive.",
            "The last one is the well known decision Tree C. 4.5 is based on information entropy.",
            "It's a splitting algorithm, and it's also easily interpretable by the human.",
            "And on top of that, it's also very efficient and fast so."
        ],
        [
            "OK, now I want to experiment.",
            "We were we are using the use corpus which which is composed of 250 texts.",
            "It's a, it's a subsample of the feeder plus large corpus.",
            "So there are 100,000 words or 120,000.",
            "If we take a punctuation into account as well.",
            "And the the first thing is that we trained TNT on nine.",
            "We split it into 10 folds with trained TNT on 9 and attacked the last 10 fold and used those tags as input to our meta tagger."
        ],
        [
            "So experiments the first baseline is just the majority classifier.",
            "So since TNT will perform slightly better than a, maybe the majority classifier says always pick the tag that TNT predicted.",
            "So on that subset that I showed you in the beginning, so roughly 15,000 words where we train our algorithm, we get 53% accuracy accuracy, so the classes are quite balanced.",
            "So the second baseline is naive Bayes classifier with only one feature, and that's a maybe full MSD, so we don't even take the future.",
            "The compositions, we just take a feature that says the tag is equal to this MSD.",
            "So and we got quite a high accuracy with this simple baseline because."
        ],
        [
            "So we can interpret this this baseline as naive Bayes classifier is simplified in this case of 1 future, it's it's all it does is for every MSDF we count the number of times.",
            "Maybe is predicted F and it was correct and the number of times.",
            "And maybe maybe it's predicted F and was incorrect.",
            "So then when we encounter and you knew.",
            "So the rule is is as follows.",
            "If if we have this new predictions, maybe same as D and TNT MSD, and we'd like to choose between them, then will you look at basically how reliable are maybe SIS when he's predicting this?",
            "M is there maybe that he's just predicted.",
            "So if the number of correct predictions for this particular M is this lower the number of wrong predictions, then we take the TNT M is the.",
            "Otherwise we take a maybe?",
            "So this is good.",
            "This is quite a.",
            "This is a very simple approach, but it's it's less trivial than naive that the majority class classifier.",
            "So and we all already got a 71%."
        ],
        [
            "Percy with this one.",
            "So now to experiment.",
            "So we we use three classifiers, then about feature sets we used four scenarios.",
            "For that one is just to use full M as DTX Sonoda compositions.",
            "The second one is the one that I showed you on the slide with those, let's say 45 features.",
            "So TNT the compositions are maybe is the compositions, an agreement features.",
            "So the third one is a subset of this decomposed features.",
            "Which are just some 55 very basic features like category type, number, gender and case.",
            "And the last one is a union of Decomposed M as decent fullem.",
            "Is this so?",
            "These are just four different features.",
            "Feature selection scenarios.",
            "So and also we we experimented with no context models and context, which are the features with context and in one case we used punctuation and in the other case we discarded punctuation by using punctuation I mean that every every.",
            "I sign every character punctuation character gets, it gets the tag, MSD, it's just it's identity.",
            "So, gets MSD, and then we use that for train."
        ],
        [
            "So these are the results are our findings are that the context does help in that punctuation slightly improves classification so that so the first one is no context, and we see that we get a 74%.",
            "Accuracy with C 4.5 if we use context but without punctuation, we're on 79 and if you use punctuation and context, we get an extra half a percent.",
            "So, so that's just slight improvement.",
            "And so the we found out that the C 4.5 works the best in this situation and if we on the the basic set of features was a good choice for it.",
            "Becausw.",
            "Yeah, it had less problems with pruning and.",
            "No, yeah, we simplified the work for for the."
        ],
        [
            "Algorithm.",
            "So and see, I'm so so, so far I've been talking about the class dismissed attacker meta tagger performance on the subset of words.",
            "Let's say 15% worse where we were optimizing, where exactly 1 tagger was correct, and so let's look at how this this 30% boosts how this looks at the whole classification accuracy on the whole set and we see that, uh, maybe was.",
            "Let's say 86% in the big percent accurate TNT was 87, so 13% error in the second baseline decreases the error for 3%, and the best model that we found decreases the whole error for four and half percent, which is, which is a significant improvement.",
            "So it's so our classifiers now work over 90% are now over 90% accurate so.",
            "And this is the final.",
            "Slide."
        ],
        [
            "The commenter question yes, you mentioned earlier that you have agreements, features.",
            "What do you mean?",
            "Do you mean agreement between?",
            "Surrender number Oh yeah."
        ],
        [
            "Adjective so agreement features we see we have, let's say the number case animasi, and for every this attribute we look at.",
            "If the predictions of TNT and maybe sword the same.",
            "So the all the agreement features were a yes in this case, except for case for the different features.",
            "No, no, no.",
            "It's not that sophisticated.",
            "It's just you just take the case of the two classifiers and check whether the classifiers, because that could be useful.",
            "Yes, exactly.",
            "In front of two.",
            "The weather the agreement features between the words you're looking at in the middle."
        ],
        [
            "Reusing some of features with the neighboring words that could help you as well, I think for languages like.",
            "Any?",
            "It's for both, so actually we are now looking at what kind of additional features we could potentially using this kind of, let's say, domain knowledge in a way which which attributes should shoot match on the previous on the next word and so on.",
            "This is certainly something we did.",
            "People typical case for the rule based.",
            "Yeah, exactly chunking.",
            "For instance when you look at the greement between negative and now and then you you find the boundaries of a chunk.",
            "Also for tagging I think they have this kind of rules.",
            "For example maybe stagger would need to have this kind of rules implemented.",
            "And there's another extension that we want to do right now, you you, you could see that we we only take features from the from the text right?",
            "We decompose the Texan.",
            "We create features, but we could also take features from the from the text.",
            "So from we could take words and context of the words and I've known some other.",
            "It is good to complications or if you have statistically based publications and you can use the publications as units.",
            "Because.",
            "If you want to go to deeper models like cognitive modeling, it is absolutely true my psycholinguistic research proven that we are not.",
            "You know our memory.",
            "Our mental lexicon does not work on single word yeah troopers on bunch of words or or you know like it would groups like so.",
            "Just a question.",
            "Text is now this line.",
            "Parable, so TNT on English.",
            "I think it's so over 95 percentage.",
            "See even more language could be 9798 yeah, but English that is not so.",
            "This is a much more difficult.",
            "I have another question.",
            "Let's take this.",
            "I think it's you could you with a specification into a number of classes.",
            "A specification problem.",
            "Maybe your approach with this method classifier could be generalized to multiclass classification in general, so it would be very interesting to see.",
            "And we could use more if we used more than two taggers then we have a multi class or problem and we will use all the software tools that are available.",
            "Is coming out of their butt to amplification.",
            "Plus so this could be really neat.",
            "To train another classifier to know which classifier is correct.",
            "So this would be the idea, but I'm I'm sure people already try.",
            "I mean this is maybe maybe they could use the same type of purchases.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Combining part of speech taggers.",
                    "label": 0
                },
                {
                    "sent": "Team learning, combining the nightly constructed one which automatically constructed one, so this is joint work with me here, Gurcharan Tamasha rabbits, and the purpose of this work is to improve this tagging part of speech.",
                    "label": 0
                },
                {
                    "sent": "Tagging accuracy for Slovenian language by using two taggers that were state of the art and at the moment and the combine them to get a better.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dagger, so first I'll give you the introduction and some motivation.",
                    "label": 0
                },
                {
                    "sent": "Then I'll describe the approach of stagger combination and some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So part of speech tagging is assigning morphosyntactic in descriptor store words in a sentence.",
                    "label": 0
                },
                {
                    "sent": "For instance, here we have nouns, verbs, and prepositions and adjectives.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But um, for, let's say Slovenian part of speech tagging is quite a difficult problem, because there are 2002 unique tags.",
                    "label": 0
                },
                {
                    "sent": "Uh, that are defined for Slovenian language because it is a highly inflectional language.",
                    "label": 0
                },
                {
                    "sent": "For instance, we have A tag, or they're also called M as this this age you FPA, which which means that the category is adjective type is general degrees undefined, and so on.",
                    "label": 0
                },
                {
                    "sent": "So all these.",
                    "label": 0
                },
                {
                    "sent": "Combinations give us this large set of classes possible.",
                    "label": 0
                },
                {
                    "sent": "Classes where, for example, in English there's only 50 or 660 usually.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a lot easier.",
                    "label": 0
                },
                {
                    "sent": "So the two of state of the art dagger Sarah maybe studio proprietary attacker.",
                    "label": 1
                },
                {
                    "sent": "It's based on handcrafted rules how to.",
                    "label": 1
                },
                {
                    "sent": "And the Italy.",
                    "label": 1
                },
                {
                    "sent": "It's performance is about 85%, so the accuracy and the second one is TNT tagger, which is based on the statistical modeling of sentences in their part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "And it uses hidden Markov Model 3 gram technology.",
                    "label": 0
                },
                {
                    "sent": "So this one has to be trained on the large corpus.",
                    "label": 0
                },
                {
                    "sent": "So how these two taggers are created there are very independent so.",
                    "label": 0
                },
                {
                    "sent": "Um and TNT achieves about 86% accuracy as well, which is still.",
                    "label": 0
                },
                {
                    "sent": "Too low for, let's say for English we have over 95 usually, so there's some.",
                    "label": 0
                },
                {
                    "sent": "These things need to be improved and I'll describe the approach.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this direction.",
                    "label": 0
                },
                {
                    "sent": "So first, first of all we have this data set of 100,000 words.",
                    "label": 0
                },
                {
                    "sent": "The from this is the use corpus.",
                    "label": 0
                },
                {
                    "sent": "It's uh, it's 100,000 words of in.",
                    "label": 0
                },
                {
                    "sent": "In Slovenian.",
                    "label": 0
                },
                {
                    "sent": "We with their M as this, so there are annotated and when we ran the two Tigers we can analyze how how their predictions in the correct tags, in what correspondence is, they are, for instance the green part of the circle means that roughly 3/4 of the of the words were correctly classified by both.",
                    "label": 0
                },
                {
                    "sent": "Classifiers the, the blue part means that both daggers predicted incorrect, but different tax and it's a small portion of the words and the yellow one represents the boat staggers, predicted the same and incorrect tag.",
                    "label": 1
                },
                {
                    "sent": "And what is interesting for us?",
                    "label": 0
                },
                {
                    "sent": "Or are these two parts so one is the Scion part where, uh, maybe stagger.",
                    "label": 0
                },
                {
                    "sent": "Cooper predicted the correct tag and TNT predicted the four stag and the purple part where the situation is reversed so.",
                    "label": 0
                },
                {
                    "sent": "If we take only TNT, we lose the Scion part.",
                    "label": 0
                },
                {
                    "sent": "If we take only a maybe sweet lose this purple part and our idea is to try to work on this subset of the words and try to try to get a larger portion than just one one of those two parts, so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is an example.",
                    "label": 0
                },
                {
                    "sent": "So our training corpus consists of truth.",
                    "label": 0
                },
                {
                    "sent": "This sentences with their two annotations that were hand validated and TNT and maybes predictions.",
                    "label": 0
                },
                {
                    "sent": "So in the first case we see that TNT predicted for predicted correct tag and maybe didn't end the cell.",
                    "label": 0
                },
                {
                    "sent": "The other cases seen cases where maybe predicted the correct one.",
                    "label": 0
                },
                {
                    "sent": "So we will only use these words for our training over this meta tagger, which will be a combination of these two Tigers.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "This is the high level description of our approach, so we have a sequence of words.",
                    "label": 0
                },
                {
                    "sent": "So we have these two taggers.",
                    "label": 0
                },
                {
                    "sent": "Each tagger provides A tag and are we build our our meta tagger on top of those two taggers which decides for every word which tagger we should listen to.",
                    "label": 0
                },
                {
                    "sent": "So which tag we should take into account.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this tagger is is, basically it's a binary classifier so in.",
                    "label": 1
                },
                {
                    "sent": "So little to go into a little bit more details we have for every word.",
                    "label": 0
                },
                {
                    "sent": "We have two tags that were predicted, then we compute some features from those two tags and feed them into our meta tagger, which is a binary classifier and then the binary classifier basically predicts TNT or a maybe so which tag we should take.",
                    "label": 0
                },
                {
                    "sent": "And the, let's say in this case, the true the tag was there, maybe stack, and then the meta tagger predicts the memes.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now now a bit about the future vector construction.",
                    "label": 1
                },
                {
                    "sent": "So the first set of features are for every MSD we we come, we decompose it completely into all those sub attributes like type, gender, part of speech.",
                    "label": 0
                },
                {
                    "sent": "And so every one of those sub attributes will will define one feature, one feature.",
                    "label": 0
                },
                {
                    "sent": "And we do so for, uh, maybe San T&T.",
                    "label": 0
                },
                {
                    "sent": "We join those features into one large feature vector, and then we also had agreement features which tell us forever for every subtype of of the tag they tell us if they the taggers agreed.",
                    "label": 1
                },
                {
                    "sent": "For instance, here we see that they didn't agree in the case.",
                    "label": 0
                },
                {
                    "sent": "So the case agreement was no missing this.",
                    "label": 0
                },
                {
                    "sent": "So, and the labels are just since I already showed you that we had this gold standard here this validated text.",
                    "label": 0
                },
                {
                    "sent": "So these are these represent the labels in arlar.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK, and the second thing is that we we also added some context features in some, some other because we have two types of experiments and in context features we look not only at the central work but also the two surrounding words and just added the TNT.",
                    "label": 0
                },
                {
                    "sent": "And then maybe those decomposition features for the leftward and for the right words so.",
                    "label": 0
                },
                {
                    "sent": "So I I think that the this this constitutes around 100 features and in the end or something.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then, as we said, the meta tagger is binary classifier and we experimented with three classifiers.",
                    "label": 0
                },
                {
                    "sent": "So one was naive Bayes classifier which is a probabilistic classifier and assumes strong independence sample assumptions.",
                    "label": 1
                },
                {
                    "sent": "And it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's sometimes works really well.",
                    "label": 0
                },
                {
                    "sent": "So for instance in text specification.",
                    "label": 0
                },
                {
                    "sent": "So we we wanted to see what happens here.",
                    "label": 0
                },
                {
                    "sent": "No, the second one RCN 2 rules.",
                    "label": 0
                },
                {
                    "sent": "It's a covering algorithm and the nice thing about it is that the predictions the rules are easily interpretable for for the human reader.",
                    "label": 0
                },
                {
                    "sent": "And but the problem is that the algorithm is quite slow, so if we have a large corpus, training is really computationally expensive.",
                    "label": 0
                },
                {
                    "sent": "The last one is the well known decision Tree C. 4.5 is based on information entropy.",
                    "label": 1
                },
                {
                    "sent": "It's a splitting algorithm, and it's also easily interpretable by the human.",
                    "label": 0
                },
                {
                    "sent": "And on top of that, it's also very efficient and fast so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now I want to experiment.",
                    "label": 0
                },
                {
                    "sent": "We were we are using the use corpus which which is composed of 250 texts.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a subsample of the feeder plus large corpus.",
                    "label": 0
                },
                {
                    "sent": "So there are 100,000 words or 120,000.",
                    "label": 0
                },
                {
                    "sent": "If we take a punctuation into account as well.",
                    "label": 1
                },
                {
                    "sent": "And the the first thing is that we trained TNT on nine.",
                    "label": 0
                },
                {
                    "sent": "We split it into 10 folds with trained TNT on 9 and attacked the last 10 fold and used those tags as input to our meta tagger.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So experiments the first baseline is just the majority classifier.",
                    "label": 0
                },
                {
                    "sent": "So since TNT will perform slightly better than a, maybe the majority classifier says always pick the tag that TNT predicted.",
                    "label": 0
                },
                {
                    "sent": "So on that subset that I showed you in the beginning, so roughly 15,000 words where we train our algorithm, we get 53% accuracy accuracy, so the classes are quite balanced.",
                    "label": 0
                },
                {
                    "sent": "So the second baseline is naive Bayes classifier with only one feature, and that's a maybe full MSD, so we don't even take the future.",
                    "label": 1
                },
                {
                    "sent": "The compositions, we just take a feature that says the tag is equal to this MSD.",
                    "label": 0
                },
                {
                    "sent": "So and we got quite a high accuracy with this simple baseline because.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can interpret this this baseline as naive Bayes classifier is simplified in this case of 1 future, it's it's all it does is for every MSDF we count the number of times.",
                    "label": 1
                },
                {
                    "sent": "Maybe is predicted F and it was correct and the number of times.",
                    "label": 1
                },
                {
                    "sent": "And maybe maybe it's predicted F and was incorrect.",
                    "label": 0
                },
                {
                    "sent": "So then when we encounter and you knew.",
                    "label": 0
                },
                {
                    "sent": "So the rule is is as follows.",
                    "label": 0
                },
                {
                    "sent": "If if we have this new predictions, maybe same as D and TNT MSD, and we'd like to choose between them, then will you look at basically how reliable are maybe SIS when he's predicting this?",
                    "label": 0
                },
                {
                    "sent": "M is there maybe that he's just predicted.",
                    "label": 0
                },
                {
                    "sent": "So if the number of correct predictions for this particular M is this lower the number of wrong predictions, then we take the TNT M is the.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we take a maybe?",
                    "label": 0
                },
                {
                    "sent": "So this is good.",
                    "label": 0
                },
                {
                    "sent": "This is quite a.",
                    "label": 0
                },
                {
                    "sent": "This is a very simple approach, but it's it's less trivial than naive that the majority class classifier.",
                    "label": 0
                },
                {
                    "sent": "So and we all already got a 71%.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Percy with this one.",
                    "label": 0
                },
                {
                    "sent": "So now to experiment.",
                    "label": 0
                },
                {
                    "sent": "So we we use three classifiers, then about feature sets we used four scenarios.",
                    "label": 0
                },
                {
                    "sent": "For that one is just to use full M as DTX Sonoda compositions.",
                    "label": 0
                },
                {
                    "sent": "The second one is the one that I showed you on the slide with those, let's say 45 features.",
                    "label": 0
                },
                {
                    "sent": "So TNT the compositions are maybe is the compositions, an agreement features.",
                    "label": 1
                },
                {
                    "sent": "So the third one is a subset of this decomposed features.",
                    "label": 1
                },
                {
                    "sent": "Which are just some 55 very basic features like category type, number, gender and case.",
                    "label": 1
                },
                {
                    "sent": "And the last one is a union of Decomposed M as decent fullem.",
                    "label": 0
                },
                {
                    "sent": "Is this so?",
                    "label": 0
                },
                {
                    "sent": "These are just four different features.",
                    "label": 0
                },
                {
                    "sent": "Feature selection scenarios.",
                    "label": 0
                },
                {
                    "sent": "So and also we we experimented with no context models and context, which are the features with context and in one case we used punctuation and in the other case we discarded punctuation by using punctuation I mean that every every.",
                    "label": 0
                },
                {
                    "sent": "I sign every character punctuation character gets, it gets the tag, MSD, it's just it's identity.",
                    "label": 0
                },
                {
                    "sent": "So, gets MSD, and then we use that for train.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are the results are our findings are that the context does help in that punctuation slightly improves classification so that so the first one is no context, and we see that we get a 74%.",
                    "label": 1
                },
                {
                    "sent": "Accuracy with C 4.5 if we use context but without punctuation, we're on 79 and if you use punctuation and context, we get an extra half a percent.",
                    "label": 0
                },
                {
                    "sent": "So, so that's just slight improvement.",
                    "label": 0
                },
                {
                    "sent": "And so the we found out that the C 4.5 works the best in this situation and if we on the the basic set of features was a good choice for it.",
                    "label": 0
                },
                {
                    "sent": "Becausw.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it had less problems with pruning and.",
                    "label": 0
                },
                {
                    "sent": "No, yeah, we simplified the work for for the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "So and see, I'm so so, so far I've been talking about the class dismissed attacker meta tagger performance on the subset of words.",
                    "label": 0
                },
                {
                    "sent": "Let's say 15% worse where we were optimizing, where exactly 1 tagger was correct, and so let's look at how this this 30% boosts how this looks at the whole classification accuracy on the whole set and we see that, uh, maybe was.",
                    "label": 0
                },
                {
                    "sent": "Let's say 86% in the big percent accurate TNT was 87, so 13% error in the second baseline decreases the error for 3%, and the best model that we found decreases the whole error for four and half percent, which is, which is a significant improvement.",
                    "label": 0
                },
                {
                    "sent": "So it's so our classifiers now work over 90% are now over 90% accurate so.",
                    "label": 0
                },
                {
                    "sent": "And this is the final.",
                    "label": 0
                },
                {
                    "sent": "Slide.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The commenter question yes, you mentioned earlier that you have agreements, features.",
                    "label": 0
                },
                {
                    "sent": "What do you mean?",
                    "label": 0
                },
                {
                    "sent": "Do you mean agreement between?",
                    "label": 0
                },
                {
                    "sent": "Surrender number Oh yeah.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adjective so agreement features we see we have, let's say the number case animasi, and for every this attribute we look at.",
                    "label": 0
                },
                {
                    "sent": "If the predictions of TNT and maybe sword the same.",
                    "label": 0
                },
                {
                    "sent": "So the all the agreement features were a yes in this case, except for case for the different features.",
                    "label": 1
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "It's not that sophisticated.",
                    "label": 0
                },
                {
                    "sent": "It's just you just take the case of the two classifiers and check whether the classifiers, because that could be useful.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly.",
                    "label": 0
                },
                {
                    "sent": "In front of two.",
                    "label": 0
                },
                {
                    "sent": "The weather the agreement features between the words you're looking at in the middle.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reusing some of features with the neighboring words that could help you as well, I think for languages like.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                },
                {
                    "sent": "It's for both, so actually we are now looking at what kind of additional features we could potentially using this kind of, let's say, domain knowledge in a way which which attributes should shoot match on the previous on the next word and so on.",
                    "label": 0
                },
                {
                    "sent": "This is certainly something we did.",
                    "label": 0
                },
                {
                    "sent": "People typical case for the rule based.",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly chunking.",
                    "label": 0
                },
                {
                    "sent": "For instance when you look at the greement between negative and now and then you you find the boundaries of a chunk.",
                    "label": 0
                },
                {
                    "sent": "Also for tagging I think they have this kind of rules.",
                    "label": 0
                },
                {
                    "sent": "For example maybe stagger would need to have this kind of rules implemented.",
                    "label": 0
                },
                {
                    "sent": "And there's another extension that we want to do right now, you you, you could see that we we only take features from the from the text right?",
                    "label": 0
                },
                {
                    "sent": "We decompose the Texan.",
                    "label": 0
                },
                {
                    "sent": "We create features, but we could also take features from the from the text.",
                    "label": 0
                },
                {
                    "sent": "So from we could take words and context of the words and I've known some other.",
                    "label": 0
                },
                {
                    "sent": "It is good to complications or if you have statistically based publications and you can use the publications as units.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "If you want to go to deeper models like cognitive modeling, it is absolutely true my psycholinguistic research proven that we are not.",
                    "label": 0
                },
                {
                    "sent": "You know our memory.",
                    "label": 0
                },
                {
                    "sent": "Our mental lexicon does not work on single word yeah troopers on bunch of words or or you know like it would groups like so.",
                    "label": 0
                },
                {
                    "sent": "Just a question.",
                    "label": 0
                },
                {
                    "sent": "Text is now this line.",
                    "label": 0
                },
                {
                    "sent": "Parable, so TNT on English.",
                    "label": 0
                },
                {
                    "sent": "I think it's so over 95 percentage.",
                    "label": 0
                },
                {
                    "sent": "See even more language could be 9798 yeah, but English that is not so.",
                    "label": 0
                },
                {
                    "sent": "This is a much more difficult.",
                    "label": 0
                },
                {
                    "sent": "I have another question.",
                    "label": 0
                },
                {
                    "sent": "Let's take this.",
                    "label": 0
                },
                {
                    "sent": "I think it's you could you with a specification into a number of classes.",
                    "label": 0
                },
                {
                    "sent": "A specification problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe your approach with this method classifier could be generalized to multiclass classification in general, so it would be very interesting to see.",
                    "label": 0
                },
                {
                    "sent": "And we could use more if we used more than two taggers then we have a multi class or problem and we will use all the software tools that are available.",
                    "label": 0
                },
                {
                    "sent": "Is coming out of their butt to amplification.",
                    "label": 0
                },
                {
                    "sent": "Plus so this could be really neat.",
                    "label": 0
                },
                {
                    "sent": "To train another classifier to know which classifier is correct.",
                    "label": 0
                },
                {
                    "sent": "So this would be the idea, but I'm I'm sure people already try.",
                    "label": 0
                },
                {
                    "sent": "I mean this is maybe maybe they could use the same type of purchases.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}