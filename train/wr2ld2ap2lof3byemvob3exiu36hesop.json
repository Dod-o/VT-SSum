{
    "id": "wr2ld2ap2lof3byemvob3exiu36hesop",
    "title": "Multitask learning: the Bayesian way",
    "info": {
        "author": [
            "Tom Heskes, Radboud University Nijmegen"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/oh06_heskes_bw/",
    "segmentation": [
        [
            "OK, OK, thanks marcie.",
            "Thanks for inviting me over here.",
            "It's really nice to be here and So what I will do is kind of perhaps different from some of the other talks.",
            "This is not kind of recent work, so most of this work.",
            "I've done quite some time ago, so I have to apologize for that.",
            "Also, I think it's it's maybe a little bit more problem problem oriented, so it really starts from a problem and that's the reason why I became interested in in multitask learning and I will basically show you what kind of solutions we sort of me implementing.",
            "How do I go further here?",
            "Just enter."
        ],
        [
            "OK, so this is kind of the outline of the talk.",
            "I will give the motivation for this kind of work, which is a problem predicting single copy newspaper sales.",
            "So we'll say a little bit about it about the data first.",
            "What we did is that we tried kind of the classical multi task learning kind of a huge neural network in which we combine all these different tasks.",
            "It seems to make a lot of sense to apply kind of Bayesian framework on top of that, so that's what we did after that.",
            "And if you talk about Bayesian saying they have to define priors, you can define price in different ways.",
            "You get kind of different answers.",
            "You have to optimize this prior to the hyperparameters of those priors, and you can do it with simple Kobe.",
            "So my main argument will be that it's kind of a really nice framework which which in particular for multi task learning works really well.",
            "Anne will say little bit about different priors and I have some questions that I'd like to ask you.",
            "OK, so what's the?",
            "What's the motivation for this work?",
            "There is this.",
            "Newspaper company in Holland, which is called the tail half so the people from Holland.",
            "No, it's not kind of."
        ],
        [
            "The highest quality newspaper, probably it's compareable here and in the UK with the Sun or something.",
            "So it's not kind of top of the bill.",
            "If you are, if you.",
            "If you're in academic but.",
            "As usual, it sells pretty well and the circulation in Holland is over 1,000,000 copies.",
            "Today they have around 15,000 outlets throughout the country, and since about a year they have newspapers every day of the week, so seven days a week before that they had six days so they didn't have newspapers on Sunday and was also relevant here is that.",
            "That they have quite a lot of single copy sales, so most of the other newspapers really live by their subscriptions and they don't have a lot of single copy, but for for that will have this single copy for is really important.",
            "So one of the things that they have implemented is called the right of return, which basically means that an outlet that didn't sell the copies that they got doesn't have to pay for that.",
            "So basically these are collected.",
            "They go back to the newspaper company.",
            "Somebody is basically counting them and keeps track of how.",
            "How much has been solved with me.",
            "So that's kind of nice because then you really know what has been sold, which is quite different from any other newspaper companies where the outlet basically chooses itself how many copies they want to buy and then it just have to pay for that.",
            "And that's kind of different.",
            "So they have collected all these sales, figured they keep them in memory for about for about five years, but hope process of collecting copies takes him about four weeks at least.",
            "Took them about four weeks at a time that we did this kind of study.",
            "So when we talk about recent sales figures, the most recent selfies, sales figure that we can really use like 4 weeks ago.",
            "So not from last week, but we need it takes in four weeks to collect all this and put it into the system.",
            "OK yeah, so that's kind of.",
            "The problem, so we want to."
        ],
        [
            "Please listen newspaper sales.",
            "This is data and I should say it's from before the time that our system was implemented there and was really used there.",
            "So what we see here is for one particular point point of sale, week number and sales.",
            "At these different weeks, I forgot to mention is that we will treat a newspapers for particular days of the week is kind of separate weekly magazines.",
            "So Saturday newspaper completely different from Monday's newspaper and so on.",
            "So basically we will treat them as kind of separate magazines.",
            "So this is sales on Saturday.",
            "So this is the solid line, is the actual sales.",
            "The Dash line is the deliveries that they.",
            "They had at that particular time and if you see start means that there is a sell out so you don't want to have sellouts because maybe you could have sold more.",
            "You also don't want to have a lot of returns which you have in this case here so.",
            "15 years papers have been delivered and only three have been sold, so it's not what you would like to have.",
            "But this is also what you do not like to have this kind of delve.",
            "This is dawn Spike which is pretty close to the beach.",
            "And what you see here is kind of nice because there was really some kind of pattern in some kind of pattern in there strongly seasonally dependent cells.",
            "To sell, so there's much more sales during summer and much less in winter.",
            "And this is what I always have to show him, because if people see this then they they get worried that the sales is really increasing.",
            "So at some points of sale is also decreasing.",
            "OK so I want you to."
        ],
        [
            "Realize that this data is really noisy, so if you're looking at kind of single points of sale, then it's pretty terrible, so there's a lot of noise there is.",
            "I think if you have a really good system you can explain maybe 10% of the variance in the data.",
            "You just trying to explain it better when the base of the date without one of the parameters that you use it on, you want to predict the sales right?",
            "Exactly what are the attributes that you use only the week of the no no no.",
            "So it's like, yeah, so when you talk to this newspaper people they have lots of variables that they would like to include into the model so they think about within sales.",
            "So if the sales recently has behind then it will be higher next next week, right?",
            "I mean, that's going to sink the same for sell out, so you have to do something with that.",
            "Same for last year sales.",
            "So if you're in the same period last year sales was high, they will think well it will be high again.",
            "They also want to incorporate season Holidays is important information, so if there is a holiday then probably at railway stations you need less sales because most people don't go there anymore.",
            "Better is important if it's really nice weather you have to send more newspapers to the beach to the outlets that are close to the beach.",
            "They would also really like to incorporate news content, so if you have any ideas how they should do that then.",
            "Then tell them you know we need to send in that, but we don't have any good ideas here how to do it.",
            "So this is just a picture out of a set of about 300 outlets.",
            "We just it kind of a simple analysis.",
            "What?",
            "What outlets?",
            "Would sell more if you have nice weather.",
            "So and the red ones are the 10 outlets that that that have kind of the most.",
            "The most positive effect of nice weather on the sales.",
            "And you can see that they are all kind of close to the to the beach and it is available which is, which is a touristic area.",
            "So there is there is some information in there that you would like to use.",
            "And the blue ones is not so clear, and those are the ones with that will show less lower sales with nice weather.",
            "And these outlets are kind of in the major countries for we don't have really have major countries in Indianapolis except for Amsterdam.",
            "Open them.",
            "Nice weather in one place and the bad weather in another place for sure.",
            "So we were just looking at these individual points of sale.",
            "We kind of regressing nice weather against sale for each of them individually.",
            "We see increased sales for for the red ones with nice weather and lower sales.",
            "For the blue ones with nice matter, but it could just be an artifact of the, you know, during the summer it's nice weather and during the summer people happen to be on the seaside so sure, but I wouldn't think of this as an artifact.",
            "I mean, it's kind of.",
            "I mean the nice guy that might not be causing this out.",
            "There could be mainly season and not so much nicer than my problem.",
            "So I cannot exclude that."
        ],
        [
            "OK, so the point is, these newspaper people really want to have models that can incorporate many variables if you try to do that by viewing all these points of sales is kind of individual tasks, so predicting the sales for each point of sale, it's a single task.",
            "Then there is no way we can find good model.",
            "So we will.",
            "We will always.",
            "We will always overfit if we try to incorporate all these variables.",
            "So when we thought about dead, well, we said this could be kind of a nice idea and we looked into the area of multitask learning and what we saw was that people.",
            "Proposed to combine only starts in kind of a huge network, but the first layer of weights from the inputs to the hidden units was kind of shared.",
            "Was the same for all these.",
            "For all these outlets, in my case of all the points of sale.",
            "So so for all the different tasks but the.",
            "The weight from the from the heating units to the output would would be specific for each outlet, so the advantage of that is that you can use kind of all the information for all these points of sale to learn this part of the model.",
            "And now for each point of sale we have only three three parameters here instead of the maybe 20 or 30 that we would have.",
            "A couple of them directly from the inputs to outputs, so that's kind of the kind of classical idea behind multitask learning.",
            "If you do it kind of in a known network, so these are the equations which are not so important, which is that you know, kind of this kind of clear right?",
            "For those few.",
            "Anything else I would like to say, yeah, so some.",
            "In some cases we use we use a hyperbolic tension here.",
            "Sometimes it's just plain linear and then it's very similar to the things that Muscian unveils have done."
        ],
        [
            "So the first question is, does it really help?",
            "And it does so, but it should should mention is that before we really started doing this we had another system and we kind of handcrafted features.",
            "So we thought really hard how how to do some kind of moving average of recent sales figures, some kind of average over last year's sales.",
            "And so we constructed kind of several features and then did single task learning, basically using these features.",
            "And that's the solution here given by the district.",
            "These are kind of different error measures.",
            "It basically means the higher the higher the worse, so the lower the better and.",
            "So what you see here is that if we start increasing the number of hidden units, then at first it decreases and then it already starts overfitting.",
            "So in this case 2 hidden units was kind of the optimal thing to do, and the solution that we got was really better than solution that we had before with those handcrafted features.",
            "So it made it made a lot of sense to use the neural network is kind of an automatic feature extractor for these for these multiple tasks that should be 98,000, But anyway."
        ],
        [
            "The other thing we would like to, we always tend to do is to to look into the system and see if it really makes sense what's going on.",
            "And so we did a kind of analysis of variance.",
            "We group the inputs into into kind of different.",
            "So we group the inputs as we have different groups of inputs, lost his sales losses, sellout Sweden sales reason, sellers, weather figures and seasonal variables.",
            "So better features.",
            "We have five different inputs that say something about whether and we kind of group them together and then we try to.",
            "To compute.",
            "To what extent each group available helps too?",
            "To predict some of the variants right, so so to get rid of some of the variance in there in the outputs.",
            "So recent sales is by far kind of the most important group of group of input variables.",
            "And then last year sales is important and seasonal variables also start to become important.",
            "So what I think is kind of nice here is that if you for example, look at last year's Ella, so up to 400 units, it doesn't do anything.",
            "And then when we go from 4 to 5 from 4 to 5 minutes, it really starts to enter.",
            "So it's not that it's kind of a smooth line.",
            "And this constantly increasing.",
            "But it really is kind of a jumper.",
            "At some point it starts to enter the equation and not just right away.",
            "So that seems to make a lot of sense, this is.",
            "Sort of noise artifacts, because something interfacing at that point.",
            "Yeah, it is, yeah.",
            "Yeah, it is definitely.",
            "Yeah, I'll probably come back to that.",
            "Random will be talking about maximum posterior solutions, so then it's kind of easier to see that.",
            "OK, this is kind of another."
        ],
        [
            "Slightly different data that we use for this same problem, so we had a little bit less inputs here.",
            "This is a Hinton diagram.",
            "You'll know Hinton diagrams, so the larger the box, the higher the absolute value and what means positive and black means negative.",
            "And so this is a representation of the weights from the inputs to the hidden units.",
            "So this first layer of this of this neural network.",
            "In this case we had two hidden units.",
            "So the first, the first thing it really looks looks more into short term effects are kind of recent sales is pretty important here.",
            "The second one mainly focuses on kind of a combination of weather information and seasonal variables.",
            "It's kind of the interpretation that I'd like to get so this kind of similar to the analysis or to the picture that Andreas showed yesterday for his for his experiment, where he showed it for just one 100 unit.",
            "Common features, yeah, so in this case, the first sending unit is kind of this feature and the second thing is kind of this feature."
        ],
        [
            "OK.",
            "So what we did?",
            "What we did then is just to compute.",
            "For all these points of sales and typically work with a set of of about 404 hundred points of sale and not not not the 15,000 because it's quite a lot.",
            "But we can compute for all these points of sales there.",
            "The best value basically for the.",
            "Weights from the head into the output units and there will be different.",
            "They can be different for each point of sale.",
            "We can make a distribution of those so so basically we can make a histogram of those and we get this kind of histogram.",
            "So for the first feature this again for three hidden units we get this kind of physical RAM and then for the second one something like this server on something like this.",
            "So this already gives a kind of a flavor of a density or distribution, so when you look at the maximum likelihood solutions such as maximizing the likelihood for for the of the data given these parameters.",
            "Then you get a distribution.",
            "If you look at the different tasks basically.",
            "So that gives a suggestion that what you might want to do is really treat these star specific parameters.",
            "As as random variables, which brings you kind of do the basic machinery so you would have to define priors on based on that and then you can use the whole basic machinery to compute posteriors.",
            "So the thing to do?",
            "So the thing that we thought that that that's kind of another way of saying that is, suppose that you've seen this and now there is a new outlet for which you haven't seen anything, so you don't have any sales figures, but you do have to make predictions.",
            "How would you set the parameters of that particular outlet?",
            "So I suppose you're not a Bayesian, which is you have to make a choice for the for the parameters of that particular outlet for which you haven't seen any sales.",
            "Then you.",
            "Then you probably would set the.",
            "The parameter for that outlet close to one 1/2 for the first city unit and probably close to 0 or many slightly higher for the other two hidden units.",
            "So you would really want to use that kind of information, but you would also kind of realize that it might not be optimal and that there is kind of spread that you would like to incorporate, perhaps also well, then you're really into kind of debate here now.",
            "The basic framework."
        ],
        [
            "So this is kind of not the way that we.",
            "That we thought about it, and so this is kind of I didn't have this before, and it's not kind of the way that we thought about it at first, but then kind of later on when we start start developing this whole basic machinery, then this is kind of the basic model that we probably have.",
            "Which is, I think, this kind of general to many of the multi task learning approaches.",
            "If you would do it in kind of a Bayesian way.",
            "Are you familiar with this kind of this kind of graph is kind of busy networking and attention, because if not then probably have to explain it more carefully.",
            "So the idea is the following.",
            "So we have.",
            "We have outputs a bit like to predict where this classification or regression is is not so important.",
            "We call them why so init init training set.",
            "These are these are.",
            "Observed, that's why we kind of color them.",
            "We have we have input, so those are the explanatory variables and basically this this.",
            "This part of the model is kind of copied for all data points, which is why there is a box here and then data here.",
            "So for all data points is this kind of copied?",
            "This this this this model for the output depends both on kind of shared parameters that are the same for all tasks, which is why they are outside this box.",
            "But there also does the specific parameters and if we can see to hear an within this box.",
            "So these are the specific parameters and.",
            "Together with the shared parameters they they predict or they model the outputs given the inputs.",
            "An what it wants me yeah so so this one is really kind of copied again, but then number of tasks right?",
            "So I have different parameters for each different task here and this is a different parameter for each different task for each different data point.",
            "So this whole boxes again copied end task.",
            "So if we're doing it basically right now, we're treating this one also as a random variable, so we have to define a prior for that random variable, and so the parameters of the prior will indicate with fine here.",
            "So these are also kind of shared between the tasks, so there are outside this box, and it might be, and they will give an example of that later on that.",
            "Specification meaning of this prior also depends on properties of the tasks.",
            "So for example, what we will do is we will make this depend on the location of the outlet.",
            "Right, so so that's a property that is not within this box, because it doesn't change overtime.",
            "It's the same for data points, but it it is different for the different outlets, so it's kind of a different thing.",
            "And this one.",
            "Suzette is really just saying which outlet?",
            "Yeah, and then specific property of that that you tried to use in your model to model the prior of that.",
            "Sort of evidence in your theater, right?",
            "That would be then specific parameters, or there will be additional inputs.",
            "I will I will.",
            "I will get to that I think I'm on the next slide so we got this picture.",
            "More less.",
            "Could be put together with X. Yeah, you could do that, but you wouldn't really.",
            "Yeah you could.",
            "Excluding yeah, but I wouldn't like to do that and I will come back to that one.",
            "I hope it will become clear.",
            "Place the thick lines other place.",
            "Yeah, this in line is an artifact of the figure.",
            "So I I didn't have this on my own, but yeah, sorry so it's in line doesn't mean anything.",
            "There is another type of here, so this size should be fine if we want to relate it to them.",
            "So on this whole slide."
        ],
        [
            "Size 5 for those that are really into quick.",
            "So one of his choice.",
            "Also, if you look at the maximum likelihood solution that I showed before, is simply to take a Gaussian prior so we specify a probability distribution over these weights from the form from the hidden to the output units with a mean mean an end.",
            "Variance matrix, so then these parameters sigh of fire.",
            "Or if you want to call them consists of a mean and covariance matrix.",
            "Have you been using weight DK in your original neural network?",
            "Yeah, we did.",
            "That implies already a away Gaussian prior exactly observe it, but that's not really as informative anymore right after you put a Bayesian prior on it.",
            "So I think that.",
            "If you would think about that, the main difference is that I don't want to decay towards 0, but I want to decay or shrink to watch a kind of mean that might be different from zero.",
            "I will come back to that and pictures later, so yeah, so that's one thing.",
            "And the other thing is that I don't.",
            "I might not shrink in each direction in in the same amount, so my strength in different directions.",
            "So I have a whole matrix here and not just a diagonal matrix.",
            "Is this the same?",
            "The same term on each of the of the elements.",
            "OK, so so this is one thing that you could do.",
            "Another thing that you could do is say, well, I don't think that there is kind of a kind of coherent set set of tasks, but maybe they are kind of different classes of tasks and one way to implement that is by modeling your prior, not as just a single Gaussian, but there's a mixture of Gaussians, so really similar to what you would do if you would just talk about kind of single task learning and then compare fitting a single Gaussian to do a mixture of Gaussians.",
            "So then you would have this kind of model, so these are the different clusters and these are kind of prior assignments of.",
            "Do a particular list so you might have two different clusters in prior assignments.",
            "Might be 0.4 and 0.6, meaning that with probabilities 0.6 at task assigned to the first cluster, and will see a .4 to the second cluster.",
            "So that's one model that you could you could have and, but then you could think, well, maybe.",
            "Pretty good enough because I have more information about dots, so more task.",
            "Sometimes I expect to be more similar than other tasks and I might do.",
            "I might want to incorporate it as well and so the thing that you could do then is make this prior assignment dependent on properties of this particular tasks.",
            "So now I don't want to make this dependent on inputs or anything like that, so the inputs are used for predicting sales.",
            "In my case, at each different week.",
            "So each different week.",
            "But I really want to make them dependent on kind of high level properties, kind of constant properties of this this task.",
            "So what we will do later on is to make this.",
            "Depends on the location of the outlet.",
            "So if it's close to the beach, would expect that the probability is higher that it belongs to kind of a seasonal cluster than to kind of a cluster which might consist of mainly kind of the larger cities in the country or something.",
            "You got the idea.",
            "OK. And so you can.",
            "Yeah, you could basically take any model that you that you like, but it's just an example of a model that you could take.",
            "OK."
        ],
        [
            "So now we have this model.",
            "Maybe I should should go back?",
            "So for for awhile."
        ],
        [
            "Consider this one kind of fixed.",
            "You can estimate everything together, but I will focus on kind of this part so we we have to do something with these parameters Phi so that specify the hyperparameters.",
            "And if you're a real Bayesian, you would probably specify a hyper prior on the prior and hyperparameters are hyper hyper parameters over that and we have to integrate and the whole crowd, so we're not real basis and we want to be kind of pragmatic.",
            "And then there is an approach which is called.",
            "Local base and Impoco base bows down to doing maximum likelihood.",
            "Basically on this parameter, but not on this parameter, so we still keep integrating out.",
            "So treating this one really is.",
            "As a random variable, so we integrate over it, but we will do maximum likelihood at this level for this particular parameter, so we will optimize the hyperparameters that integrate out.",
            "Anne.",
            "Did dust, dust specific parameters, and I think that there is a very good reason for doing so, because if you would really kind of do do the realbasic real basin.",
            "The real basic machinery and compute the posterior of these hyperparameters given all the data that we observe, then it would be very close to a Delta pick because we have so much data because we have kind of a copy of an data times and task number of data points to really compute that will all appear in the posterior for this one and then we have kind of the same argument as we can be based in media.",
            "You have a lot of data, then Bayesian and maximum likelihood.",
            "It's the same, right?",
            "So we can use kind of a similar argument here.",
            "While we can optimize this one, the same is not true for these parameters because for each of these we only have data data points, so we have much less data points in this argument that we might have.",
            "Well, we have a lot of data, so maximum likelihood in Bayes more less.",
            "The same doesn't apply for those parameters.",
            "OK so my."
        ],
        [
            "My argument is well, if we we want to optimize these these kind of these parameters that are shared between the tasks, then we can do maximum likelihood and then it's called empirical base or also.",
            "Statistic literature it's called Type 2 maximum likelihood procedure, so that too because type one would be doing maximum likelihood on all the parameters in the model.",
            "OK, so.",
            "Then this is the thing that you would have to compute, so you have to integrate over over the specific parameters and you would have to do is integral and then just maximize it.",
            "So one way to do that is with expectation maximization, as you probably."
        ],
        [
            "Million with, but in this case it's really nice.",
            "That's why I want to spend a little bit time on it.",
            "So what it boils down to is that you have to do the following.",
            "So for each of these tasks, you would have to compute the probability of the.",
            "The specific parameters given the data for this task and the old setting of the hyperparameters.",
            "So if everything is linear, is everything Gaussian, and if my prior is just a single Gaussian, then it would boil down to something like this, right?",
            "So we would have a Gaussian for that one, and then I would have Gaussians for each of these different tasks.",
            "Well, have a bunch of Gaussians.",
            "If you look at what you have to do in the end step, you would have to find the new hyperparameter that that that maximizes this term, and so this is again, if you have just a simple Gaussian prior, this is just a Gaussian and then you look at this term and what it basically says.",
            "I have to add up the Gaussians that I get so these posterior Gaussians for each of these different tasks.",
            "So I have all these Gaussians, Adam applicator, mixture of Gaussians and then I have to fit.",
            "I got sent to that which is really simple right?",
            "So if that makes you now since then I have to compute the mean and variance of that of that mixture and then I have my kind of best fit.",
            "Gaussian to that mixture of Gaussian.",
            "So this is really, really easy algorithm and you can very easily implement it.",
            "It will converge to the maximum 2.",
            "The maximum likelihood it will converge to a local optimum of this.",
            "Of the of the locking.",
            "So kind of similar analysis is also in in a reason NIPS paper, by swipe over and focus recipe will probably tell about more about this tomorrow.",
            "OK, you will build me here, OK?",
            "So."
        ],
        [
            "Doesn't really help, so does this machine.",
            "This basic machinery on top of the the kind of classical multitask learning doesn't really add anything, and it does.",
            "So what we have here is again, with increasing number of hidden units, we look at the mean squared error in this case.",
            "So this is maximum likelihood solution.",
            "So without any of the basic machine without any priors, and this is the same again for the maximum posteriori solution.",
            "So we do incorporate all the basic machinery.",
            "And we kind of regularize the maximum likelihood solutions based on the prior that we've learned.",
            "So what you can see here is that the best solution here is, well, maybe 400 units, but it's not significantly different from 3 or 5 or 600 units, but it's clearly better than the best solution that began without this.",
            "This Bayesian machinery and you can also see that we really need both, so we cannot say in this case.",
            "And I was not so clear.",
            "So in this case I had nine inputs.",
            "So if I would have 900 units, it wouldn't be really different from having no hidden units, so.",
            "So in this case I would would have only kind of basic machinery, but no kind of bottleneck of hidden units, and this seems to be better than this one over here.",
            "So it seems that you really need both in this case to get kind of the best performance.",
            "Another thing to show that it really helps is."
        ],
        [
            "That we've implemented this an made made a commercial product out of it, which is called Jason of delivery, so it runs.",
            "An advance in Holland for this newspaper company that tells you about before, but it also has been implemented for a newspaper in Portugal.",
            "And we did several studies for for different newspaper companies and magazine distributors and those people and always checked and compare it with the system that they had before to kind of convince them that they should buy our product.",
            "So let me explain just what you can see in these figures.",
            "So one thing to realize is that there was one knob basically to the system that is really up to the to the company itself.",
            "To really said that.",
            "So you can either choose for kind of a strategy that is expensive so that you don't care so much about about having a lot of returns, but you want to minimize the probability of sellouts, but you can also be slightly more more more more more conservative, and that you don't don't want to have a lot of returns, and you don't care so much about sellouts.",
            "It really depends on the price of the newspaper and the cost of printing a newspaper and delivering it and and.",
            "What what the market is about and things like that?",
            "So there's basically a whole line that.",
            "That you can choose to be on, depending on what your strategy is.",
            "I kind of normalized the results for the company itself, so so that they had with the system themselves and set it to 100.",
            "So this is the total number of of delivering this kind of normalized to 100 and the sales as well do not give you any information about about what it really was, and So what you can see here is that there are two kind of break even point.",
            "So one thing you could do is to.",
            "Keep the total amount of deliveries fixed and then see how much more you can sell with our system and then in this case it's about 1% in.",
            "This came about 2% in here, slightly less than 1%.",
            "The other thing you could do is you could ask yourself.",
            "Well suppose that I want to have the same total amount of sales.",
            "How much less deliveries do I need for that?",
            "And that's slightly better than it looks better because this curve is kind and because of the curve basically.",
            "So if you would like to have the same amount of sales and here I can do with like 4% less deliveries.",
            "Here, even with 15 or something like that and here again 3 or 4.",
            "So it really really helps, and I think this is really nice.",
            "In real world application of kind of multi task learning, in that you can beat it, beat existing systems with it.",
            "Again, we I always like to look into the solution and see where it would make sense and so this is kind of the answer to your regularization.",
            "So you indeed see if you see the maximum likelihood solutions here."
        ],
        [
            "So without any prior, then you learn the prior and you look at the maximum possible solutions and make a histogram of that.",
            "So they basically shrink and she would expand but they don't swing.",
            "Towards 0, but it's trying to watch that they're kind of join me.",
            "More, less so to 0.5 more less in this case, and this is this mean happens to be close to zero, and this also pretty close to 0, but still it's different from just playing regularization.",
            "456 yeah, so this is just just saying the more features, so this is a model with history hidden units.",
            "In here I showed the results so I didn't change.",
            "Any units here?",
            "This picture you've seen before, so we looked at at the percentage of explained variance that we could attribute to each of the of the different groups of inputs and what you see here which is."
        ],
        [
            "Is also kind of nice.",
            "Kind of similar behavior that you've seen yesterday.",
            "Is that that the groups that come in later also kind of recognized away by the by the prior so we don't have?",
            "So basically, with the Gaussian priors it's closer to an L2 norm rather than L1 or L0 norm?",
            "So, so it's not fully shut down basically, but it's kind of close to so you get more or less the same behavior, but not as strongly, so you don't get really sparse solutions, but you get kind of a similar kind of effect.",
            "So this one really doesn't enter anymore.",
            "Does this one?",
            "He said something about veterans as well.",
            "It seems like last year sales already incorporate last year seasonal variables which have the same exactly one year later.",
            "So yeah, so the yeah.",
            "I didn't look at that governments.",
            "I could have done that, but yeah, so I don't have any pictures, but definitely that will be the case so.",
            "We did do analysis in the sense of what if we only incorporate seasonal variables and not last year sales, do we get more less the same performance and then the answer is yes.",
            "So we don't really need last year sales, which is why in the actual system we don't.",
            "We don't use it because we can use more more data.",
            "Basically we don't have to go back.",
            "For some of the variables, would it make sense to have a mixture model on the prior when you have a Gaussian and also a Delta function and they have some pie that's going to mix between the two?",
            "So you can maybe make some of them.",
            "Yeah, so so spiking slap kind of prior.",
            "I would love to but I haven't done it yet, but definitely.",
            "OK, so this is."
        ],
        [
            "Now I have to go back to this case where we had two in the units where one was more kind of short term effects and the other one was more seasonal.",
            "Since seasonal effects and it's kind of nice because then you can make 2 dimensional pictures of of what the maximum likelihood solutions are for different outlets and then we had Gaussian prior and then you get that kind of string through which is to watch.",
            "It's harder to watch each other so you get this kind of solution and this is kind of indicating.",
            "The... indicating the covariance matrix.",
            "We also implemented.",
            "A mixture of two Gaussians as a prior.",
            "So what we would have expected was to find kind of two separate clusters, one more seasonal cluster, one more than one cluster.",
            "Looking more into within cells is not exactly what happens, although it's kind of close to it.",
            "But what happens is that there is one cluster.",
            "Here this one which has large hardly any variation in the short term effects.",
            "So the weight of the first sitting unit to the outputs is more less constant and it runs in zero point 5, but has a large variance in the second one.",
            "So in the seasonal effect, which means that some of the points of sales will sell more young winter, an orders will sell more during summer, so there was this kind of spread that you have there.",
            "And the other cluster that we found was kind of the opposite.",
            "What was not so much variation in kind of the seasonal effects, but there is a lot of there was quite a lot of variation in in short term effects does not what we expected, but still it seems to make sense.",
            "So you can also do task gating and then you basically make this prior assignment a function of properties of this particular task of this particular point of sale.",
            "So what we did was.",
            "I think we use two variables.",
            "One was distance to the beach and the other one was whether it was a touristic city yes or no.",
            "So you can do it and then try to incorporate that in your prior distribution.",
            "So you get something like cascading.",
            "But it looks quite similar to the things that you get this task list ring, and if you look at performance improvement we didn't find any significant performance improvement in this case, which if you look at the distribution of the maximum likelihood solutions, kind of what you would expect, probably because you don't see any obvious clusters here.",
            "So I would say if you see if you just do kind of simple first analysis and see really see clusters in the maximum.",
            "Solutions, then it makes sense to apply either this one or maybe even this one.",
            "But in our case it didn't really give any better performance, so it's kind of nice and makes sense, but it doesn't help.",
            "Every dot.",
            "Here is a data point is an outlet and outlet and to access are the weights.",
            "Basically if you talk about the network so that the parameters connected to the two hidden units.",
            "So I had this huge neural network.",
            "Each outlet is in a separate.",
            "An output in this in this neural network do any units to wait and I can plot the Max market solutions.",
            "So so these are all different tasks basically.",
            "Yeah, I guess.",
            "Is it going to be?",
            "Given the parameters, is it going to be in some sense formally the identifiable model in that, given the particular math, or a particular likelihood, you will have a unique specification of the parameters that will give you that.",
            "I guess so, but I I wouldn't know why.",
            "Why not so so?",
            "So the results you consented before work for no task, 18 new class clustering they want.",
            "So, so the results for for these two more let's say so no, no significant difference.",
            "OK."
        ],
        [
            "So.",
            "I hope I've shown you or convince you, perhaps even that basic machinery lends itself really nicely for formal task learning.",
            "Even if you're not a Bayesian, if you're really frequentist and you look at the maximum likelihood solution, you see that there's a distribution of of the different parameters that are specific to each task.",
            "So I think you can give a quite direct interpretation of the prior, which is kind of different from single task learning, so there you always have to have some kind of argument is subjective, or whatever.",
            "There's not so much subjective here, I think.",
            "I hope I've also kind of argued why you why.",
            "Empirical base is kind of a good idea, especially if you have many tasks.",
            "I'm not talking bout 2 tasks.",
            "If you have two tasks and I don't know, then the argument kind of fills.",
            "But if you have like 100 different tasks and relatively few data points per task, then I think you can.",
            "You can have a pretty strong argument why to optimize the kind of parameters that are shared between the tasks, but not the parameters that are specific for each task.",
            "So we treat those as random variables in this in this basic framework.",
            "And we really obtained good performance.",
            "So we really could make a commercially.",
            "Melephant system out of debt.",
            "So of course there's still a lot of work to do to do, and we've already talked about this yesterday, so we have to think about appropriate models, and I've been looking a little bit more into to time series analysis.",
            "Kind of other problems as well.",
            "So really hot area in the basin machine in the world is doing approximate inference, so in this case, if you keep everything kind of linear and Gaussian, then you can do all the intervals kind of easily and.",
            "And and analytically, if you go to more complex models then it's no longer doable and you need kind of approximate inference techniques to really get them to work.",
            "Perhaps the reason that I haven't done a lot of work on multi task learning the last couple of years is that once you kind of translate it and draw this graph of kind of the Bayesian network graph that I showed, then it's not so different anymore from kind of standard Bayesian machinery, so so really basing people would say, well who cares?",
            "I mean it's just that you have a plate which is copied to end task times and then just do it and go for it, but it's technically.",
            "It's not much, much different then, so all the techniques that that apply to in single task learning.",
            "You can try to transfer to a multitasker.",
            "Maybe a couple of?"
        ],
        [
            "Of questions that I had when I looked into the recent literature.",
            "I have to apologize here because I have to leave later this afternoon, so I'm not sure when I will hear the answer still, but this is what I was kind of interested in.",
            "So what I've seen is that that people have made a translation from kind of the basic machinery to the kernel approach.",
            "So if everything is linear an if you have Gaussian priors and basically what you have is a Gaussian process so that kind of work by mozzy and also by value and transparent and strike over.",
            "So, so that brings you kind of from from the.",
            "From the Bayesian multitask.",
            "Anwyl to that kind of kernel, an machine world which I think is kind of nice.",
            "But then I started thinking what's kind of the better, easier framework.",
            "So the first question is so if you have this bottleneck MLP maybe not not linear but it was hyped up with hyperbolic tension.",
            "So what would it mean for the kernel?",
            "It's probably kind of an easy exercising, probably do that, but you can probably also derive kernels for task list ring engaging so you start with kind of basing machine.",
            "We do the calculations.",
            "And see what kind of girls you get.",
            "If you want to implement that clustering, engaging in the kind of kernel machine framework, I don't know with any money has done that, but it shouldn't be too difficult to think we actually arrived to the kind of approach from the literature, no, but you can make the connection right.",
            "Designer and presentation, which also and it made the task.",
            "Yeah, I've used Nebulization framework.",
            "Then you just minimize the functional.",
            "Yeah no.",
            "Yeah I can see it, but it's not so difficult to connect the two I think.",
            "And so.",
            "So maybe this one clearly makes the connection.",
            "Yeah, yeah.",
            "Yeah, but the things that he they arrive at a very similar to the signature to describe here is very different computational algorithms.",
            "I guess one of the issue is.",
            "Yeah, how it scales.",
            "To train each other?",
            "Yeah, that's definitely it's.",
            "It's also a point messages on there.",
            "Common formulation, yeah.",
            "Yep, quite appealing.",
            "But I think you would also have it here if you just have a linear.",
            "If you have linear linear.",
            "Linear transfer functions then I haven't looked at it, but then it's very similar and I bet it's also convex.",
            "You have any M approach?",
            "No, but that's just one way to do it right?",
            "It's just just alternate maximization, so if you apply, it doesn't mean that it is not convex, right?",
            "Can also apply EM to complex problems.",
            "I think you're probably exhausted lyrics.",
            "I'm not sure.",
            "OK, we can.",
            "We can check that.",
            "Yeah so, but yeah, so one way to go is to start with the basic approach and then do the calculations and see what kind of kernels come out of that and so you could also probably do that for task blistering, engaging and look at the kernel set come out so.",
            "And the question is, what's kind of the better approach, if there, if any, of them is better then perhaps it boils down to whether it's easier to think about distances and kernels if we want to define.",
            "Kind of across difference between tasks or when it's easier to think about models and then he was kind of basic machinery.",
            "I don't know.",
            "So so for me it's easier because of my background.",
            "To define basing models which other people might be easier to think about distances and kernels.",
            "Another question is what which approach is most sensitive to a sip optimal choice, so I don't think we can claim that.",
            "Assumptions that we make are always valid in practice, but then so probably any choice will be sub optimal.",
            "But then the question is which the optimal choice is most sensitive to being being sub optimal.",
            "And of course there is the question.",
            "What's the most efficient approach?",
            "But yeah, so there I don't really know.",
            "They have to sort it out then I have some technical questions but yeah, so technical that I'll leave this.",
            "That's a big question, that an ultimate one, but it's pretty hard to formulate in any exact way.",
            "I would've thought it 'cause.",
            "I mean, you know sub optimal is going to be measured in different scales if you're talking about Bayesian or.",
            "Or kernel design innocence.",
            "So when you might have a sort of objective measure performance at the end, that's actually sort of say.",
            "To what extent is more suboptimal?",
            "Or yeah, so how hard the Bayesian tried?",
            "Did they try out of just joking or did the you know the kernel guys try harder?",
            "Just imagine I think it depends.",
            "A lot of what you want to do with the output of your inference.",
            "If you have a subsequent decision phase where you want to make about measuring the output qualities, OK, it's a question of what I'm saying is, you know, you could have sort of a fair comparison of how good they are at the end.",
            "But the question is, how do you measure the quality that the suboptimality at the input?",
            "Yeah yeah yeah.",
            "My my my opinion is the if you could please correctly it will be pretty good, but usually people don't patient correctly.",
            "Therefore it's actually pretty simple.",
            "Yeah.",
            "Like yeah.",
            "He claims that these empirical Bayes approach is a good approximation.",
            "It's kind of reasonable is not basically the only difference is it.",
            "Without that there is a issue of whether intervals are better.",
            "Better approach essentially may not their reason for that.",
            "The other issue about integration out is computationally some more difficult question is I mean for example email starting with logistic regression.",
            "This time will be more money then it will be more difficult, sure.",
            "Sure, sure, definitely, but if it's linear in Gaussian then is not more difficult.",
            "And Lastly vaginal system you want to optimize hyperparameters of.",
            "Your kernel is still difficult is not as difficult as logistic regression, but still is difficult.",
            "It is starting to optimize your half of any other kernel integral.",
            "Each individual ways.",
            "Yeah, but in my case whether I optimize the.",
            "The specific parameters or integrate amount is not more difficult.",
            "Yeah it could be, yeah.",
            "I think one important point guys.",
            "If you do this fully Bayesian method then you set things up that you have nice conjugate priors and you can run full MCM.",
            "The issue of Convexity's not so much of a big deal because you're going to be taking the vision average at the end of the day.",
            "You really have to worry about is is your posterior going to be very multimodal or not with these type of models it shouldn't be so nasty because of the type of construction.",
            "So fun fact.",
            "So if you're willing to wait.",
            "I mean, that's a big issue if you're willing to wait around for Markov chain.",
            "Wonderful vision process, then you have the benefit of being necessary.",
            "Yeah, so so in my case I'm not willing to wait right?",
            "Because I have to predict for 15,000 points of sale every day and I'm not going to wait for Markov chain to in the convergence equilibrium.",
            "Sure.",
            "Take Me Out so those things you could do, but I don't want to do MCMC all the way for these kind of problems.",
            "Yeah, something you mentioned that.",
            "And it does seem that stores some population means, so just a common algorithm.",
            "Whether you check how much the same that's is and whether it's closed.",
            "Welcome again to find buildings, for example, checking in Ohio with test data.",
            "No, I haven't.",
            "I haven't really done that.",
            "Like a standard regularization W square right now, which will exist.",
            "I think it was zero to something.",
            "Yeah, but I I definitely do not want to string towards 0 so I don't want to.",
            "Telephone zero.",
            "It's the same idea.",
            "It's a similar idea, so so one thing is which I probably didn't emphasize that I don't have to do cross validation to determine what the optimal string change, because it kind of follows from the basic machinery.",
            "What?",
            "I haven't checked it, so I think I'm too much a Bayesian to then do cross validation to come check it.",
            "You said you weren't invited, no.",
            "I mean, I'm kind of an empirical Bayesian and at that level I kind of trust the things actually.",
            "Ask us about the moment.",
            "Question is not space typical base active right now.",
            "We don't ask this question.",
            "OK, so maybe I'm not.",
            "We don't think about everything software.",
            "But I think that people realize that.",
            "That's my minus ten, OK?",
            "I think that most Bayesians realized that the assumptions that they make are not really valid, but it's the best thing that they can do, and then they go from there and so.",
            "No, I mean, there's a whole theory of these factors in many different extensive theory about suboptimality and OK. Actually even cross validation I think can be satisfied, right?",
            "Just.",
            "Invasion thing.",
            "Anymore questions from the agents.",
            "OK thanks, I hope it was actually one of the interesting topics for discussion.",
            "I cannot be certain about that.",
            "So thankfully."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, OK, thanks marcie.",
                    "label": 0
                },
                {
                    "sent": "Thanks for inviting me over here.",
                    "label": 0
                },
                {
                    "sent": "It's really nice to be here and So what I will do is kind of perhaps different from some of the other talks.",
                    "label": 0
                },
                {
                    "sent": "This is not kind of recent work, so most of this work.",
                    "label": 0
                },
                {
                    "sent": "I've done quite some time ago, so I have to apologize for that.",
                    "label": 0
                },
                {
                    "sent": "Also, I think it's it's maybe a little bit more problem problem oriented, so it really starts from a problem and that's the reason why I became interested in in multitask learning and I will basically show you what kind of solutions we sort of me implementing.",
                    "label": 0
                },
                {
                    "sent": "How do I go further here?",
                    "label": 0
                },
                {
                    "sent": "Just enter.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is kind of the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will give the motivation for this kind of work, which is a problem predicting single copy newspaper sales.",
                    "label": 1
                },
                {
                    "sent": "So we'll say a little bit about it about the data first.",
                    "label": 0
                },
                {
                    "sent": "What we did is that we tried kind of the classical multi task learning kind of a huge neural network in which we combine all these different tasks.",
                    "label": 0
                },
                {
                    "sent": "It seems to make a lot of sense to apply kind of Bayesian framework on top of that, so that's what we did after that.",
                    "label": 0
                },
                {
                    "sent": "And if you talk about Bayesian saying they have to define priors, you can define price in different ways.",
                    "label": 0
                },
                {
                    "sent": "You get kind of different answers.",
                    "label": 0
                },
                {
                    "sent": "You have to optimize this prior to the hyperparameters of those priors, and you can do it with simple Kobe.",
                    "label": 0
                },
                {
                    "sent": "So my main argument will be that it's kind of a really nice framework which which in particular for multi task learning works really well.",
                    "label": 0
                },
                {
                    "sent": "Anne will say little bit about different priors and I have some questions that I'd like to ask you.",
                    "label": 1
                },
                {
                    "sent": "OK, so what's the?",
                    "label": 0
                },
                {
                    "sent": "What's the motivation for this work?",
                    "label": 0
                },
                {
                    "sent": "There is this.",
                    "label": 0
                },
                {
                    "sent": "Newspaper company in Holland, which is called the tail half so the people from Holland.",
                    "label": 0
                },
                {
                    "sent": "No, it's not kind of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The highest quality newspaper, probably it's compareable here and in the UK with the Sun or something.",
                    "label": 0
                },
                {
                    "sent": "So it's not kind of top of the bill.",
                    "label": 0
                },
                {
                    "sent": "If you are, if you.",
                    "label": 0
                },
                {
                    "sent": "If you're in academic but.",
                    "label": 0
                },
                {
                    "sent": "As usual, it sells pretty well and the circulation in Holland is over 1,000,000 copies.",
                    "label": 0
                },
                {
                    "sent": "Today they have around 15,000 outlets throughout the country, and since about a year they have newspapers every day of the week, so seven days a week before that they had six days so they didn't have newspapers on Sunday and was also relevant here is that.",
                    "label": 1
                },
                {
                    "sent": "That they have quite a lot of single copy sales, so most of the other newspapers really live by their subscriptions and they don't have a lot of single copy, but for for that will have this single copy for is really important.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that they have implemented is called the right of return, which basically means that an outlet that didn't sell the copies that they got doesn't have to pay for that.",
                    "label": 0
                },
                {
                    "sent": "So basically these are collected.",
                    "label": 0
                },
                {
                    "sent": "They go back to the newspaper company.",
                    "label": 0
                },
                {
                    "sent": "Somebody is basically counting them and keeps track of how.",
                    "label": 0
                },
                {
                    "sent": "How much has been solved with me.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of nice because then you really know what has been sold, which is quite different from any other newspaper companies where the outlet basically chooses itself how many copies they want to buy and then it just have to pay for that.",
                    "label": 0
                },
                {
                    "sent": "And that's kind of different.",
                    "label": 0
                },
                {
                    "sent": "So they have collected all these sales, figured they keep them in memory for about for about five years, but hope process of collecting copies takes him about four weeks at least.",
                    "label": 0
                },
                {
                    "sent": "Took them about four weeks at a time that we did this kind of study.",
                    "label": 0
                },
                {
                    "sent": "So when we talk about recent sales figures, the most recent selfies, sales figure that we can really use like 4 weeks ago.",
                    "label": 1
                },
                {
                    "sent": "So not from last week, but we need it takes in four weeks to collect all this and put it into the system.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, so that's kind of.",
                    "label": 0
                },
                {
                    "sent": "The problem, so we want to.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please listen newspaper sales.",
                    "label": 0
                },
                {
                    "sent": "This is data and I should say it's from before the time that our system was implemented there and was really used there.",
                    "label": 0
                },
                {
                    "sent": "So what we see here is for one particular point point of sale, week number and sales.",
                    "label": 0
                },
                {
                    "sent": "At these different weeks, I forgot to mention is that we will treat a newspapers for particular days of the week is kind of separate weekly magazines.",
                    "label": 0
                },
                {
                    "sent": "So Saturday newspaper completely different from Monday's newspaper and so on.",
                    "label": 0
                },
                {
                    "sent": "So basically we will treat them as kind of separate magazines.",
                    "label": 0
                },
                {
                    "sent": "So this is sales on Saturday.",
                    "label": 0
                },
                {
                    "sent": "So this is the solid line, is the actual sales.",
                    "label": 0
                },
                {
                    "sent": "The Dash line is the deliveries that they.",
                    "label": 0
                },
                {
                    "sent": "They had at that particular time and if you see start means that there is a sell out so you don't want to have sellouts because maybe you could have sold more.",
                    "label": 0
                },
                {
                    "sent": "You also don't want to have a lot of returns which you have in this case here so.",
                    "label": 0
                },
                {
                    "sent": "15 years papers have been delivered and only three have been sold, so it's not what you would like to have.",
                    "label": 0
                },
                {
                    "sent": "But this is also what you do not like to have this kind of delve.",
                    "label": 0
                },
                {
                    "sent": "This is dawn Spike which is pretty close to the beach.",
                    "label": 0
                },
                {
                    "sent": "And what you see here is kind of nice because there was really some kind of pattern in some kind of pattern in there strongly seasonally dependent cells.",
                    "label": 0
                },
                {
                    "sent": "To sell, so there's much more sales during summer and much less in winter.",
                    "label": 0
                },
                {
                    "sent": "And this is what I always have to show him, because if people see this then they they get worried that the sales is really increasing.",
                    "label": 0
                },
                {
                    "sent": "So at some points of sale is also decreasing.",
                    "label": 0
                },
                {
                    "sent": "OK so I want you to.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Realize that this data is really noisy, so if you're looking at kind of single points of sale, then it's pretty terrible, so there's a lot of noise there is.",
                    "label": 0
                },
                {
                    "sent": "I think if you have a really good system you can explain maybe 10% of the variance in the data.",
                    "label": 0
                },
                {
                    "sent": "You just trying to explain it better when the base of the date without one of the parameters that you use it on, you want to predict the sales right?",
                    "label": 0
                },
                {
                    "sent": "Exactly what are the attributes that you use only the week of the no no no.",
                    "label": 0
                },
                {
                    "sent": "So it's like, yeah, so when you talk to this newspaper people they have lots of variables that they would like to include into the model so they think about within sales.",
                    "label": 0
                },
                {
                    "sent": "So if the sales recently has behind then it will be higher next next week, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, that's going to sink the same for sell out, so you have to do something with that.",
                    "label": 0
                },
                {
                    "sent": "Same for last year sales.",
                    "label": 0
                },
                {
                    "sent": "So if you're in the same period last year sales was high, they will think well it will be high again.",
                    "label": 0
                },
                {
                    "sent": "They also want to incorporate season Holidays is important information, so if there is a holiday then probably at railway stations you need less sales because most people don't go there anymore.",
                    "label": 0
                },
                {
                    "sent": "Better is important if it's really nice weather you have to send more newspapers to the beach to the outlets that are close to the beach.",
                    "label": 0
                },
                {
                    "sent": "They would also really like to incorporate news content, so if you have any ideas how they should do that then.",
                    "label": 0
                },
                {
                    "sent": "Then tell them you know we need to send in that, but we don't have any good ideas here how to do it.",
                    "label": 0
                },
                {
                    "sent": "So this is just a picture out of a set of about 300 outlets.",
                    "label": 0
                },
                {
                    "sent": "We just it kind of a simple analysis.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What outlets?",
                    "label": 0
                },
                {
                    "sent": "Would sell more if you have nice weather.",
                    "label": 0
                },
                {
                    "sent": "So and the red ones are the 10 outlets that that that have kind of the most.",
                    "label": 0
                },
                {
                    "sent": "The most positive effect of nice weather on the sales.",
                    "label": 0
                },
                {
                    "sent": "And you can see that they are all kind of close to the to the beach and it is available which is, which is a touristic area.",
                    "label": 0
                },
                {
                    "sent": "So there is there is some information in there that you would like to use.",
                    "label": 0
                },
                {
                    "sent": "And the blue ones is not so clear, and those are the ones with that will show less lower sales with nice weather.",
                    "label": 1
                },
                {
                    "sent": "And these outlets are kind of in the major countries for we don't have really have major countries in Indianapolis except for Amsterdam.",
                    "label": 0
                },
                {
                    "sent": "Open them.",
                    "label": 0
                },
                {
                    "sent": "Nice weather in one place and the bad weather in another place for sure.",
                    "label": 0
                },
                {
                    "sent": "So we were just looking at these individual points of sale.",
                    "label": 0
                },
                {
                    "sent": "We kind of regressing nice weather against sale for each of them individually.",
                    "label": 1
                },
                {
                    "sent": "We see increased sales for for the red ones with nice weather and lower sales.",
                    "label": 0
                },
                {
                    "sent": "For the blue ones with nice matter, but it could just be an artifact of the, you know, during the summer it's nice weather and during the summer people happen to be on the seaside so sure, but I wouldn't think of this as an artifact.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's kind of.",
                    "label": 0
                },
                {
                    "sent": "I mean the nice guy that might not be causing this out.",
                    "label": 0
                },
                {
                    "sent": "There could be mainly season and not so much nicer than my problem.",
                    "label": 0
                },
                {
                    "sent": "So I cannot exclude that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the point is, these newspaper people really want to have models that can incorporate many variables if you try to do that by viewing all these points of sales is kind of individual tasks, so predicting the sales for each point of sale, it's a single task.",
                    "label": 1
                },
                {
                    "sent": "Then there is no way we can find good model.",
                    "label": 0
                },
                {
                    "sent": "So we will.",
                    "label": 0
                },
                {
                    "sent": "We will always.",
                    "label": 0
                },
                {
                    "sent": "We will always overfit if we try to incorporate all these variables.",
                    "label": 0
                },
                {
                    "sent": "So when we thought about dead, well, we said this could be kind of a nice idea and we looked into the area of multitask learning and what we saw was that people.",
                    "label": 1
                },
                {
                    "sent": "Proposed to combine only starts in kind of a huge network, but the first layer of weights from the inputs to the hidden units was kind of shared.",
                    "label": 1
                },
                {
                    "sent": "Was the same for all these.",
                    "label": 0
                },
                {
                    "sent": "For all these outlets, in my case of all the points of sale.",
                    "label": 0
                },
                {
                    "sent": "So so for all the different tasks but the.",
                    "label": 0
                },
                {
                    "sent": "The weight from the from the heating units to the output would would be specific for each outlet, so the advantage of that is that you can use kind of all the information for all these points of sale to learn this part of the model.",
                    "label": 0
                },
                {
                    "sent": "And now for each point of sale we have only three three parameters here instead of the maybe 20 or 30 that we would have.",
                    "label": 0
                },
                {
                    "sent": "A couple of them directly from the inputs to outputs, so that's kind of the kind of classical idea behind multitask learning.",
                    "label": 0
                },
                {
                    "sent": "If you do it kind of in a known network, so these are the equations which are not so important, which is that you know, kind of this kind of clear right?",
                    "label": 0
                },
                {
                    "sent": "For those few.",
                    "label": 0
                },
                {
                    "sent": "Anything else I would like to say, yeah, so some.",
                    "label": 0
                },
                {
                    "sent": "In some cases we use we use a hyperbolic tension here.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's just plain linear and then it's very similar to the things that Muscian unveils have done.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first question is, does it really help?",
                    "label": 1
                },
                {
                    "sent": "And it does so, but it should should mention is that before we really started doing this we had another system and we kind of handcrafted features.",
                    "label": 0
                },
                {
                    "sent": "So we thought really hard how how to do some kind of moving average of recent sales figures, some kind of average over last year's sales.",
                    "label": 0
                },
                {
                    "sent": "And so we constructed kind of several features and then did single task learning, basically using these features.",
                    "label": 0
                },
                {
                    "sent": "And that's the solution here given by the district.",
                    "label": 0
                },
                {
                    "sent": "These are kind of different error measures.",
                    "label": 0
                },
                {
                    "sent": "It basically means the higher the higher the worse, so the lower the better and.",
                    "label": 1
                },
                {
                    "sent": "So what you see here is that if we start increasing the number of hidden units, then at first it decreases and then it already starts overfitting.",
                    "label": 1
                },
                {
                    "sent": "So in this case 2 hidden units was kind of the optimal thing to do, and the solution that we got was really better than solution that we had before with those handcrafted features.",
                    "label": 0
                },
                {
                    "sent": "So it made it made a lot of sense to use the neural network is kind of an automatic feature extractor for these for these multiple tasks that should be 98,000, But anyway.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing we would like to, we always tend to do is to to look into the system and see if it really makes sense what's going on.",
                    "label": 0
                },
                {
                    "sent": "And so we did a kind of analysis of variance.",
                    "label": 0
                },
                {
                    "sent": "We group the inputs into into kind of different.",
                    "label": 0
                },
                {
                    "sent": "So we group the inputs as we have different groups of inputs, lost his sales losses, sellout Sweden sales reason, sellers, weather figures and seasonal variables.",
                    "label": 0
                },
                {
                    "sent": "So better features.",
                    "label": 0
                },
                {
                    "sent": "We have five different inputs that say something about whether and we kind of group them together and then we try to.",
                    "label": 0
                },
                {
                    "sent": "To compute.",
                    "label": 0
                },
                {
                    "sent": "To what extent each group available helps too?",
                    "label": 0
                },
                {
                    "sent": "To predict some of the variants right, so so to get rid of some of the variance in there in the outputs.",
                    "label": 0
                },
                {
                    "sent": "So recent sales is by far kind of the most important group of group of input variables.",
                    "label": 0
                },
                {
                    "sent": "And then last year sales is important and seasonal variables also start to become important.",
                    "label": 0
                },
                {
                    "sent": "So what I think is kind of nice here is that if you for example, look at last year's Ella, so up to 400 units, it doesn't do anything.",
                    "label": 0
                },
                {
                    "sent": "And then when we go from 4 to 5 from 4 to 5 minutes, it really starts to enter.",
                    "label": 0
                },
                {
                    "sent": "So it's not that it's kind of a smooth line.",
                    "label": 0
                },
                {
                    "sent": "And this constantly increasing.",
                    "label": 0
                },
                {
                    "sent": "But it really is kind of a jumper.",
                    "label": 0
                },
                {
                    "sent": "At some point it starts to enter the equation and not just right away.",
                    "label": 0
                },
                {
                    "sent": "So that seems to make a lot of sense, this is.",
                    "label": 0
                },
                {
                    "sent": "Sort of noise artifacts, because something interfacing at that point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it is, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it is definitely.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll probably come back to that.",
                    "label": 0
                },
                {
                    "sent": "Random will be talking about maximum posterior solutions, so then it's kind of easier to see that.",
                    "label": 0
                },
                {
                    "sent": "OK, this is kind of another.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slightly different data that we use for this same problem, so we had a little bit less inputs here.",
                    "label": 0
                },
                {
                    "sent": "This is a Hinton diagram.",
                    "label": 0
                },
                {
                    "sent": "You'll know Hinton diagrams, so the larger the box, the higher the absolute value and what means positive and black means negative.",
                    "label": 0
                },
                {
                    "sent": "And so this is a representation of the weights from the inputs to the hidden units.",
                    "label": 0
                },
                {
                    "sent": "So this first layer of this of this neural network.",
                    "label": 0
                },
                {
                    "sent": "In this case we had two hidden units.",
                    "label": 0
                },
                {
                    "sent": "So the first, the first thing it really looks looks more into short term effects are kind of recent sales is pretty important here.",
                    "label": 0
                },
                {
                    "sent": "The second one mainly focuses on kind of a combination of weather information and seasonal variables.",
                    "label": 0
                },
                {
                    "sent": "It's kind of the interpretation that I'd like to get so this kind of similar to the analysis or to the picture that Andreas showed yesterday for his for his experiment, where he showed it for just one 100 unit.",
                    "label": 0
                },
                {
                    "sent": "Common features, yeah, so in this case, the first sending unit is kind of this feature and the second thing is kind of this feature.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what we did?",
                    "label": 0
                },
                {
                    "sent": "What we did then is just to compute.",
                    "label": 0
                },
                {
                    "sent": "For all these points of sales and typically work with a set of of about 404 hundred points of sale and not not not the 15,000 because it's quite a lot.",
                    "label": 0
                },
                {
                    "sent": "But we can compute for all these points of sales there.",
                    "label": 0
                },
                {
                    "sent": "The best value basically for the.",
                    "label": 1
                },
                {
                    "sent": "Weights from the head into the output units and there will be different.",
                    "label": 0
                },
                {
                    "sent": "They can be different for each point of sale.",
                    "label": 0
                },
                {
                    "sent": "We can make a distribution of those so so basically we can make a histogram of those and we get this kind of histogram.",
                    "label": 1
                },
                {
                    "sent": "So for the first feature this again for three hidden units we get this kind of physical RAM and then for the second one something like this server on something like this.",
                    "label": 0
                },
                {
                    "sent": "So this already gives a kind of a flavor of a density or distribution, so when you look at the maximum likelihood solutions such as maximizing the likelihood for for the of the data given these parameters.",
                    "label": 0
                },
                {
                    "sent": "Then you get a distribution.",
                    "label": 0
                },
                {
                    "sent": "If you look at the different tasks basically.",
                    "label": 0
                },
                {
                    "sent": "So that gives a suggestion that what you might want to do is really treat these star specific parameters.",
                    "label": 0
                },
                {
                    "sent": "As as random variables, which brings you kind of do the basic machinery so you would have to define priors on based on that and then you can use the whole basic machinery to compute posteriors.",
                    "label": 1
                },
                {
                    "sent": "So the thing to do?",
                    "label": 0
                },
                {
                    "sent": "So the thing that we thought that that that's kind of another way of saying that is, suppose that you've seen this and now there is a new outlet for which you haven't seen anything, so you don't have any sales figures, but you do have to make predictions.",
                    "label": 0
                },
                {
                    "sent": "How would you set the parameters of that particular outlet?",
                    "label": 0
                },
                {
                    "sent": "So I suppose you're not a Bayesian, which is you have to make a choice for the for the parameters of that particular outlet for which you haven't seen any sales.",
                    "label": 0
                },
                {
                    "sent": "Then you.",
                    "label": 0
                },
                {
                    "sent": "Then you probably would set the.",
                    "label": 0
                },
                {
                    "sent": "The parameter for that outlet close to one 1/2 for the first city unit and probably close to 0 or many slightly higher for the other two hidden units.",
                    "label": 0
                },
                {
                    "sent": "So you would really want to use that kind of information, but you would also kind of realize that it might not be optimal and that there is kind of spread that you would like to incorporate, perhaps also well, then you're really into kind of debate here now.",
                    "label": 0
                },
                {
                    "sent": "The basic framework.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is kind of not the way that we.",
                    "label": 0
                },
                {
                    "sent": "That we thought about it, and so this is kind of I didn't have this before, and it's not kind of the way that we thought about it at first, but then kind of later on when we start start developing this whole basic machinery, then this is kind of the basic model that we probably have.",
                    "label": 0
                },
                {
                    "sent": "Which is, I think, this kind of general to many of the multi task learning approaches.",
                    "label": 0
                },
                {
                    "sent": "If you would do it in kind of a Bayesian way.",
                    "label": 1
                },
                {
                    "sent": "Are you familiar with this kind of this kind of graph is kind of busy networking and attention, because if not then probably have to explain it more carefully.",
                    "label": 0
                },
                {
                    "sent": "So the idea is the following.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "We have outputs a bit like to predict where this classification or regression is is not so important.",
                    "label": 0
                },
                {
                    "sent": "We call them why so init init training set.",
                    "label": 0
                },
                {
                    "sent": "These are these are.",
                    "label": 0
                },
                {
                    "sent": "Observed, that's why we kind of color them.",
                    "label": 0
                },
                {
                    "sent": "We have we have input, so those are the explanatory variables and basically this this.",
                    "label": 0
                },
                {
                    "sent": "This part of the model is kind of copied for all data points, which is why there is a box here and then data here.",
                    "label": 0
                },
                {
                    "sent": "So for all data points is this kind of copied?",
                    "label": 0
                },
                {
                    "sent": "This this this this model for the output depends both on kind of shared parameters that are the same for all tasks, which is why they are outside this box.",
                    "label": 0
                },
                {
                    "sent": "But there also does the specific parameters and if we can see to hear an within this box.",
                    "label": 0
                },
                {
                    "sent": "So these are the specific parameters and.",
                    "label": 0
                },
                {
                    "sent": "Together with the shared parameters they they predict or they model the outputs given the inputs.",
                    "label": 0
                },
                {
                    "sent": "An what it wants me yeah so so this one is really kind of copied again, but then number of tasks right?",
                    "label": 0
                },
                {
                    "sent": "So I have different parameters for each different task here and this is a different parameter for each different task for each different data point.",
                    "label": 0
                },
                {
                    "sent": "So this whole boxes again copied end task.",
                    "label": 0
                },
                {
                    "sent": "So if we're doing it basically right now, we're treating this one also as a random variable, so we have to define a prior for that random variable, and so the parameters of the prior will indicate with fine here.",
                    "label": 0
                },
                {
                    "sent": "So these are also kind of shared between the tasks, so there are outside this box, and it might be, and they will give an example of that later on that.",
                    "label": 0
                },
                {
                    "sent": "Specification meaning of this prior also depends on properties of the tasks.",
                    "label": 0
                },
                {
                    "sent": "So for example, what we will do is we will make this depend on the location of the outlet.",
                    "label": 1
                },
                {
                    "sent": "Right, so so that's a property that is not within this box, because it doesn't change overtime.",
                    "label": 0
                },
                {
                    "sent": "It's the same for data points, but it it is different for the different outlets, so it's kind of a different thing.",
                    "label": 0
                },
                {
                    "sent": "And this one.",
                    "label": 0
                },
                {
                    "sent": "Suzette is really just saying which outlet?",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then specific property of that that you tried to use in your model to model the prior of that.",
                    "label": 0
                },
                {
                    "sent": "Sort of evidence in your theater, right?",
                    "label": 0
                },
                {
                    "sent": "That would be then specific parameters, or there will be additional inputs.",
                    "label": 0
                },
                {
                    "sent": "I will I will.",
                    "label": 0
                },
                {
                    "sent": "I will get to that I think I'm on the next slide so we got this picture.",
                    "label": 0
                },
                {
                    "sent": "More less.",
                    "label": 0
                },
                {
                    "sent": "Could be put together with X. Yeah, you could do that, but you wouldn't really.",
                    "label": 0
                },
                {
                    "sent": "Yeah you could.",
                    "label": 0
                },
                {
                    "sent": "Excluding yeah, but I wouldn't like to do that and I will come back to that one.",
                    "label": 0
                },
                {
                    "sent": "I hope it will become clear.",
                    "label": 0
                },
                {
                    "sent": "Place the thick lines other place.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this in line is an artifact of the figure.",
                    "label": 0
                },
                {
                    "sent": "So I I didn't have this on my own, but yeah, sorry so it's in line doesn't mean anything.",
                    "label": 0
                },
                {
                    "sent": "There is another type of here, so this size should be fine if we want to relate it to them.",
                    "label": 0
                },
                {
                    "sent": "So on this whole slide.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Size 5 for those that are really into quick.",
                    "label": 0
                },
                {
                    "sent": "So one of his choice.",
                    "label": 0
                },
                {
                    "sent": "Also, if you look at the maximum likelihood solution that I showed before, is simply to take a Gaussian prior so we specify a probability distribution over these weights from the form from the hidden to the output units with a mean mean an end.",
                    "label": 0
                },
                {
                    "sent": "Variance matrix, so then these parameters sigh of fire.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to call them consists of a mean and covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "Have you been using weight DK in your original neural network?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we did.",
                    "label": 0
                },
                {
                    "sent": "That implies already a away Gaussian prior exactly observe it, but that's not really as informative anymore right after you put a Bayesian prior on it.",
                    "label": 0
                },
                {
                    "sent": "So I think that.",
                    "label": 0
                },
                {
                    "sent": "If you would think about that, the main difference is that I don't want to decay towards 0, but I want to decay or shrink to watch a kind of mean that might be different from zero.",
                    "label": 0
                },
                {
                    "sent": "I will come back to that and pictures later, so yeah, so that's one thing.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that I don't.",
                    "label": 0
                },
                {
                    "sent": "I might not shrink in each direction in in the same amount, so my strength in different directions.",
                    "label": 0
                },
                {
                    "sent": "So I have a whole matrix here and not just a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "Is this the same?",
                    "label": 0
                },
                {
                    "sent": "The same term on each of the of the elements.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is one thing that you could do.",
                    "label": 0
                },
                {
                    "sent": "Another thing that you could do is say, well, I don't think that there is kind of a kind of coherent set set of tasks, but maybe they are kind of different classes of tasks and one way to implement that is by modeling your prior, not as just a single Gaussian, but there's a mixture of Gaussians, so really similar to what you would do if you would just talk about kind of single task learning and then compare fitting a single Gaussian to do a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So then you would have this kind of model, so these are the different clusters and these are kind of prior assignments of.",
                    "label": 0
                },
                {
                    "sent": "Do a particular list so you might have two different clusters in prior assignments.",
                    "label": 0
                },
                {
                    "sent": "Might be 0.4 and 0.6, meaning that with probabilities 0.6 at task assigned to the first cluster, and will see a .4 to the second cluster.",
                    "label": 0
                },
                {
                    "sent": "So that's one model that you could you could have and, but then you could think, well, maybe.",
                    "label": 0
                },
                {
                    "sent": "Pretty good enough because I have more information about dots, so more task.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I expect to be more similar than other tasks and I might do.",
                    "label": 0
                },
                {
                    "sent": "I might want to incorporate it as well and so the thing that you could do then is make this prior assignment dependent on properties of this particular tasks.",
                    "label": 0
                },
                {
                    "sent": "So now I don't want to make this dependent on inputs or anything like that, so the inputs are used for predicting sales.",
                    "label": 0
                },
                {
                    "sent": "In my case, at each different week.",
                    "label": 0
                },
                {
                    "sent": "So each different week.",
                    "label": 0
                },
                {
                    "sent": "But I really want to make them dependent on kind of high level properties, kind of constant properties of this this task.",
                    "label": 0
                },
                {
                    "sent": "So what we will do later on is to make this.",
                    "label": 1
                },
                {
                    "sent": "Depends on the location of the outlet.",
                    "label": 0
                },
                {
                    "sent": "So if it's close to the beach, would expect that the probability is higher that it belongs to kind of a seasonal cluster than to kind of a cluster which might consist of mainly kind of the larger cities in the country or something.",
                    "label": 0
                },
                {
                    "sent": "You got the idea.",
                    "label": 0
                },
                {
                    "sent": "OK. And so you can.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could basically take any model that you that you like, but it's just an example of a model that you could take.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we have this model.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should should go back?",
                    "label": 0
                },
                {
                    "sent": "So for for awhile.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consider this one kind of fixed.",
                    "label": 0
                },
                {
                    "sent": "You can estimate everything together, but I will focus on kind of this part so we we have to do something with these parameters Phi so that specify the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "And if you're a real Bayesian, you would probably specify a hyper prior on the prior and hyperparameters are hyper hyper parameters over that and we have to integrate and the whole crowd, so we're not real basis and we want to be kind of pragmatic.",
                    "label": 0
                },
                {
                    "sent": "And then there is an approach which is called.",
                    "label": 0
                },
                {
                    "sent": "Local base and Impoco base bows down to doing maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Basically on this parameter, but not on this parameter, so we still keep integrating out.",
                    "label": 0
                },
                {
                    "sent": "So treating this one really is.",
                    "label": 0
                },
                {
                    "sent": "As a random variable, so we integrate over it, but we will do maximum likelihood at this level for this particular parameter, so we will optimize the hyperparameters that integrate out.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Did dust, dust specific parameters, and I think that there is a very good reason for doing so, because if you would really kind of do do the realbasic real basin.",
                    "label": 0
                },
                {
                    "sent": "The real basic machinery and compute the posterior of these hyperparameters given all the data that we observe, then it would be very close to a Delta pick because we have so much data because we have kind of a copy of an data times and task number of data points to really compute that will all appear in the posterior for this one and then we have kind of the same argument as we can be based in media.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of data, then Bayesian and maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "It's the same, right?",
                    "label": 0
                },
                {
                    "sent": "So we can use kind of a similar argument here.",
                    "label": 0
                },
                {
                    "sent": "While we can optimize this one, the same is not true for these parameters because for each of these we only have data data points, so we have much less data points in this argument that we might have.",
                    "label": 0
                },
                {
                    "sent": "Well, we have a lot of data, so maximum likelihood in Bayes more less.",
                    "label": 0
                },
                {
                    "sent": "The same doesn't apply for those parameters.",
                    "label": 0
                },
                {
                    "sent": "OK so my.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My argument is well, if we we want to optimize these these kind of these parameters that are shared between the tasks, then we can do maximum likelihood and then it's called empirical base or also.",
                    "label": 1
                },
                {
                    "sent": "Statistic literature it's called Type 2 maximum likelihood procedure, so that too because type one would be doing maximum likelihood on all the parameters in the model.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 1
                },
                {
                    "sent": "Then this is the thing that you would have to compute, so you have to integrate over over the specific parameters and you would have to do is integral and then just maximize it.",
                    "label": 0
                },
                {
                    "sent": "So one way to do that is with expectation maximization, as you probably.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Million with, but in this case it's really nice.",
                    "label": 0
                },
                {
                    "sent": "That's why I want to spend a little bit time on it.",
                    "label": 0
                },
                {
                    "sent": "So what it boils down to is that you have to do the following.",
                    "label": 0
                },
                {
                    "sent": "So for each of these tasks, you would have to compute the probability of the.",
                    "label": 0
                },
                {
                    "sent": "The specific parameters given the data for this task and the old setting of the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So if everything is linear, is everything Gaussian, and if my prior is just a single Gaussian, then it would boil down to something like this, right?",
                    "label": 0
                },
                {
                    "sent": "So we would have a Gaussian for that one, and then I would have Gaussians for each of these different tasks.",
                    "label": 0
                },
                {
                    "sent": "Well, have a bunch of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "If you look at what you have to do in the end step, you would have to find the new hyperparameter that that that maximizes this term, and so this is again, if you have just a simple Gaussian prior, this is just a Gaussian and then you look at this term and what it basically says.",
                    "label": 0
                },
                {
                    "sent": "I have to add up the Gaussians that I get so these posterior Gaussians for each of these different tasks.",
                    "label": 0
                },
                {
                    "sent": "So I have all these Gaussians, Adam applicator, mixture of Gaussians and then I have to fit.",
                    "label": 0
                },
                {
                    "sent": "I got sent to that which is really simple right?",
                    "label": 0
                },
                {
                    "sent": "So if that makes you now since then I have to compute the mean and variance of that of that mixture and then I have my kind of best fit.",
                    "label": 0
                },
                {
                    "sent": "Gaussian to that mixture of Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So this is really, really easy algorithm and you can very easily implement it.",
                    "label": 0
                },
                {
                    "sent": "It will converge to the maximum 2.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood it will converge to a local optimum of this.",
                    "label": 0
                },
                {
                    "sent": "Of the of the locking.",
                    "label": 0
                },
                {
                    "sent": "So kind of similar analysis is also in in a reason NIPS paper, by swipe over and focus recipe will probably tell about more about this tomorrow.",
                    "label": 0
                },
                {
                    "sent": "OK, you will build me here, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doesn't really help, so does this machine.",
                    "label": 0
                },
                {
                    "sent": "This basic machinery on top of the the kind of classical multitask learning doesn't really add anything, and it does.",
                    "label": 1
                },
                {
                    "sent": "So what we have here is again, with increasing number of hidden units, we look at the mean squared error in this case.",
                    "label": 1
                },
                {
                    "sent": "So this is maximum likelihood solution.",
                    "label": 0
                },
                {
                    "sent": "So without any of the basic machine without any priors, and this is the same again for the maximum posteriori solution.",
                    "label": 0
                },
                {
                    "sent": "So we do incorporate all the basic machinery.",
                    "label": 0
                },
                {
                    "sent": "And we kind of regularize the maximum likelihood solutions based on the prior that we've learned.",
                    "label": 0
                },
                {
                    "sent": "So what you can see here is that the best solution here is, well, maybe 400 units, but it's not significantly different from 3 or 5 or 600 units, but it's clearly better than the best solution that began without this.",
                    "label": 0
                },
                {
                    "sent": "This Bayesian machinery and you can also see that we really need both, so we cannot say in this case.",
                    "label": 0
                },
                {
                    "sent": "And I was not so clear.",
                    "label": 0
                },
                {
                    "sent": "So in this case I had nine inputs.",
                    "label": 0
                },
                {
                    "sent": "So if I would have 900 units, it wouldn't be really different from having no hidden units, so.",
                    "label": 0
                },
                {
                    "sent": "So in this case I would would have only kind of basic machinery, but no kind of bottleneck of hidden units, and this seems to be better than this one over here.",
                    "label": 1
                },
                {
                    "sent": "So it seems that you really need both in this case to get kind of the best performance.",
                    "label": 0
                },
                {
                    "sent": "Another thing to show that it really helps is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we've implemented this an made made a commercial product out of it, which is called Jason of delivery, so it runs.",
                    "label": 0
                },
                {
                    "sent": "An advance in Holland for this newspaper company that tells you about before, but it also has been implemented for a newspaper in Portugal.",
                    "label": 0
                },
                {
                    "sent": "And we did several studies for for different newspaper companies and magazine distributors and those people and always checked and compare it with the system that they had before to kind of convince them that they should buy our product.",
                    "label": 0
                },
                {
                    "sent": "So let me explain just what you can see in these figures.",
                    "label": 0
                },
                {
                    "sent": "So one thing to realize is that there was one knob basically to the system that is really up to the to the company itself.",
                    "label": 0
                },
                {
                    "sent": "To really said that.",
                    "label": 0
                },
                {
                    "sent": "So you can either choose for kind of a strategy that is expensive so that you don't care so much about about having a lot of returns, but you want to minimize the probability of sellouts, but you can also be slightly more more more more more conservative, and that you don't don't want to have a lot of returns, and you don't care so much about sellouts.",
                    "label": 0
                },
                {
                    "sent": "It really depends on the price of the newspaper and the cost of printing a newspaper and delivering it and and.",
                    "label": 0
                },
                {
                    "sent": "What what the market is about and things like that?",
                    "label": 0
                },
                {
                    "sent": "So there's basically a whole line that.",
                    "label": 0
                },
                {
                    "sent": "That you can choose to be on, depending on what your strategy is.",
                    "label": 0
                },
                {
                    "sent": "I kind of normalized the results for the company itself, so so that they had with the system themselves and set it to 100.",
                    "label": 0
                },
                {
                    "sent": "So this is the total number of of delivering this kind of normalized to 100 and the sales as well do not give you any information about about what it really was, and So what you can see here is that there are two kind of break even point.",
                    "label": 0
                },
                {
                    "sent": "So one thing you could do is to.",
                    "label": 0
                },
                {
                    "sent": "Keep the total amount of deliveries fixed and then see how much more you can sell with our system and then in this case it's about 1% in.",
                    "label": 0
                },
                {
                    "sent": "This came about 2% in here, slightly less than 1%.",
                    "label": 0
                },
                {
                    "sent": "The other thing you could do is you could ask yourself.",
                    "label": 0
                },
                {
                    "sent": "Well suppose that I want to have the same total amount of sales.",
                    "label": 0
                },
                {
                    "sent": "How much less deliveries do I need for that?",
                    "label": 0
                },
                {
                    "sent": "And that's slightly better than it looks better because this curve is kind and because of the curve basically.",
                    "label": 0
                },
                {
                    "sent": "So if you would like to have the same amount of sales and here I can do with like 4% less deliveries.",
                    "label": 0
                },
                {
                    "sent": "Here, even with 15 or something like that and here again 3 or 4.",
                    "label": 0
                },
                {
                    "sent": "So it really really helps, and I think this is really nice.",
                    "label": 0
                },
                {
                    "sent": "In real world application of kind of multi task learning, in that you can beat it, beat existing systems with it.",
                    "label": 0
                },
                {
                    "sent": "Again, we I always like to look into the solution and see where it would make sense and so this is kind of the answer to your regularization.",
                    "label": 0
                },
                {
                    "sent": "So you indeed see if you see the maximum likelihood solutions here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So without any prior, then you learn the prior and you look at the maximum possible solutions and make a histogram of that.",
                    "label": 0
                },
                {
                    "sent": "So they basically shrink and she would expand but they don't swing.",
                    "label": 0
                },
                {
                    "sent": "Towards 0, but it's trying to watch that they're kind of join me.",
                    "label": 0
                },
                {
                    "sent": "More, less so to 0.5 more less in this case, and this is this mean happens to be close to zero, and this also pretty close to 0, but still it's different from just playing regularization.",
                    "label": 0
                },
                {
                    "sent": "456 yeah, so this is just just saying the more features, so this is a model with history hidden units.",
                    "label": 0
                },
                {
                    "sent": "In here I showed the results so I didn't change.",
                    "label": 0
                },
                {
                    "sent": "Any units here?",
                    "label": 0
                },
                {
                    "sent": "This picture you've seen before, so we looked at at the percentage of explained variance that we could attribute to each of the of the different groups of inputs and what you see here which is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is also kind of nice.",
                    "label": 0
                },
                {
                    "sent": "Kind of similar behavior that you've seen yesterday.",
                    "label": 0
                },
                {
                    "sent": "Is that that the groups that come in later also kind of recognized away by the by the prior so we don't have?",
                    "label": 0
                },
                {
                    "sent": "So basically, with the Gaussian priors it's closer to an L2 norm rather than L1 or L0 norm?",
                    "label": 0
                },
                {
                    "sent": "So, so it's not fully shut down basically, but it's kind of close to so you get more or less the same behavior, but not as strongly, so you don't get really sparse solutions, but you get kind of a similar kind of effect.",
                    "label": 0
                },
                {
                    "sent": "So this one really doesn't enter anymore.",
                    "label": 0
                },
                {
                    "sent": "Does this one?",
                    "label": 0
                },
                {
                    "sent": "He said something about veterans as well.",
                    "label": 0
                },
                {
                    "sent": "It seems like last year sales already incorporate last year seasonal variables which have the same exactly one year later.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so the yeah.",
                    "label": 0
                },
                {
                    "sent": "I didn't look at that governments.",
                    "label": 0
                },
                {
                    "sent": "I could have done that, but yeah, so I don't have any pictures, but definitely that will be the case so.",
                    "label": 0
                },
                {
                    "sent": "We did do analysis in the sense of what if we only incorporate seasonal variables and not last year sales, do we get more less the same performance and then the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "So we don't really need last year sales, which is why in the actual system we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't use it because we can use more more data.",
                    "label": 0
                },
                {
                    "sent": "Basically we don't have to go back.",
                    "label": 0
                },
                {
                    "sent": "For some of the variables, would it make sense to have a mixture model on the prior when you have a Gaussian and also a Delta function and they have some pie that's going to mix between the two?",
                    "label": 0
                },
                {
                    "sent": "So you can maybe make some of them.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so spiking slap kind of prior.",
                    "label": 0
                },
                {
                    "sent": "I would love to but I haven't done it yet, but definitely.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I have to go back to this case where we had two in the units where one was more kind of short term effects and the other one was more seasonal.",
                    "label": 0
                },
                {
                    "sent": "Since seasonal effects and it's kind of nice because then you can make 2 dimensional pictures of of what the maximum likelihood solutions are for different outlets and then we had Gaussian prior and then you get that kind of string through which is to watch.",
                    "label": 0
                },
                {
                    "sent": "It's harder to watch each other so you get this kind of solution and this is kind of indicating.",
                    "label": 0
                },
                {
                    "sent": "The... indicating the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "We also implemented.",
                    "label": 0
                },
                {
                    "sent": "A mixture of two Gaussians as a prior.",
                    "label": 0
                },
                {
                    "sent": "So what we would have expected was to find kind of two separate clusters, one more seasonal cluster, one more than one cluster.",
                    "label": 0
                },
                {
                    "sent": "Looking more into within cells is not exactly what happens, although it's kind of close to it.",
                    "label": 0
                },
                {
                    "sent": "But what happens is that there is one cluster.",
                    "label": 0
                },
                {
                    "sent": "Here this one which has large hardly any variation in the short term effects.",
                    "label": 0
                },
                {
                    "sent": "So the weight of the first sitting unit to the outputs is more less constant and it runs in zero point 5, but has a large variance in the second one.",
                    "label": 0
                },
                {
                    "sent": "So in the seasonal effect, which means that some of the points of sales will sell more young winter, an orders will sell more during summer, so there was this kind of spread that you have there.",
                    "label": 0
                },
                {
                    "sent": "And the other cluster that we found was kind of the opposite.",
                    "label": 0
                },
                {
                    "sent": "What was not so much variation in kind of the seasonal effects, but there is a lot of there was quite a lot of variation in in short term effects does not what we expected, but still it seems to make sense.",
                    "label": 0
                },
                {
                    "sent": "So you can also do task gating and then you basically make this prior assignment a function of properties of this particular task of this particular point of sale.",
                    "label": 0
                },
                {
                    "sent": "So what we did was.",
                    "label": 0
                },
                {
                    "sent": "I think we use two variables.",
                    "label": 0
                },
                {
                    "sent": "One was distance to the beach and the other one was whether it was a touristic city yes or no.",
                    "label": 0
                },
                {
                    "sent": "So you can do it and then try to incorporate that in your prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So you get something like cascading.",
                    "label": 0
                },
                {
                    "sent": "But it looks quite similar to the things that you get this task list ring, and if you look at performance improvement we didn't find any significant performance improvement in this case, which if you look at the distribution of the maximum likelihood solutions, kind of what you would expect, probably because you don't see any obvious clusters here.",
                    "label": 0
                },
                {
                    "sent": "So I would say if you see if you just do kind of simple first analysis and see really see clusters in the maximum.",
                    "label": 0
                },
                {
                    "sent": "Solutions, then it makes sense to apply either this one or maybe even this one.",
                    "label": 0
                },
                {
                    "sent": "But in our case it didn't really give any better performance, so it's kind of nice and makes sense, but it doesn't help.",
                    "label": 0
                },
                {
                    "sent": "Every dot.",
                    "label": 0
                },
                {
                    "sent": "Here is a data point is an outlet and outlet and to access are the weights.",
                    "label": 1
                },
                {
                    "sent": "Basically if you talk about the network so that the parameters connected to the two hidden units.",
                    "label": 0
                },
                {
                    "sent": "So I had this huge neural network.",
                    "label": 0
                },
                {
                    "sent": "Each outlet is in a separate.",
                    "label": 1
                },
                {
                    "sent": "An output in this in this neural network do any units to wait and I can plot the Max market solutions.",
                    "label": 0
                },
                {
                    "sent": "So so these are all different tasks basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess.",
                    "label": 0
                },
                {
                    "sent": "Is it going to be?",
                    "label": 0
                },
                {
                    "sent": "Given the parameters, is it going to be in some sense formally the identifiable model in that, given the particular math, or a particular likelihood, you will have a unique specification of the parameters that will give you that.",
                    "label": 1
                },
                {
                    "sent": "I guess so, but I I wouldn't know why.",
                    "label": 0
                },
                {
                    "sent": "Why not so so?",
                    "label": 0
                },
                {
                    "sent": "So the results you consented before work for no task, 18 new class clustering they want.",
                    "label": 0
                },
                {
                    "sent": "So, so the results for for these two more let's say so no, no significant difference.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I hope I've shown you or convince you, perhaps even that basic machinery lends itself really nicely for formal task learning.",
                    "label": 1
                },
                {
                    "sent": "Even if you're not a Bayesian, if you're really frequentist and you look at the maximum likelihood solution, you see that there's a distribution of of the different parameters that are specific to each task.",
                    "label": 0
                },
                {
                    "sent": "So I think you can give a quite direct interpretation of the prior, which is kind of different from single task learning, so there you always have to have some kind of argument is subjective, or whatever.",
                    "label": 1
                },
                {
                    "sent": "There's not so much subjective here, I think.",
                    "label": 0
                },
                {
                    "sent": "I hope I've also kind of argued why you why.",
                    "label": 0
                },
                {
                    "sent": "Empirical base is kind of a good idea, especially if you have many tasks.",
                    "label": 0
                },
                {
                    "sent": "I'm not talking bout 2 tasks.",
                    "label": 0
                },
                {
                    "sent": "If you have two tasks and I don't know, then the argument kind of fills.",
                    "label": 1
                },
                {
                    "sent": "But if you have like 100 different tasks and relatively few data points per task, then I think you can.",
                    "label": 0
                },
                {
                    "sent": "You can have a pretty strong argument why to optimize the kind of parameters that are shared between the tasks, but not the parameters that are specific for each task.",
                    "label": 0
                },
                {
                    "sent": "So we treat those as random variables in this in this basic framework.",
                    "label": 1
                },
                {
                    "sent": "And we really obtained good performance.",
                    "label": 0
                },
                {
                    "sent": "So we really could make a commercially.",
                    "label": 1
                },
                {
                    "sent": "Melephant system out of debt.",
                    "label": 0
                },
                {
                    "sent": "So of course there's still a lot of work to do to do, and we've already talked about this yesterday, so we have to think about appropriate models, and I've been looking a little bit more into to time series analysis.",
                    "label": 1
                },
                {
                    "sent": "Kind of other problems as well.",
                    "label": 0
                },
                {
                    "sent": "So really hot area in the basin machine in the world is doing approximate inference, so in this case, if you keep everything kind of linear and Gaussian, then you can do all the intervals kind of easily and.",
                    "label": 0
                },
                {
                    "sent": "And and analytically, if you go to more complex models then it's no longer doable and you need kind of approximate inference techniques to really get them to work.",
                    "label": 0
                },
                {
                    "sent": "Perhaps the reason that I haven't done a lot of work on multi task learning the last couple of years is that once you kind of translate it and draw this graph of kind of the Bayesian network graph that I showed, then it's not so different anymore from kind of standard Bayesian machinery, so so really basing people would say, well who cares?",
                    "label": 0
                },
                {
                    "sent": "I mean it's just that you have a plate which is copied to end task times and then just do it and go for it, but it's technically.",
                    "label": 0
                },
                {
                    "sent": "It's not much, much different then, so all the techniques that that apply to in single task learning.",
                    "label": 0
                },
                {
                    "sent": "You can try to transfer to a multitasker.",
                    "label": 0
                },
                {
                    "sent": "Maybe a couple of?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of questions that I had when I looked into the recent literature.",
                    "label": 0
                },
                {
                    "sent": "I have to apologize here because I have to leave later this afternoon, so I'm not sure when I will hear the answer still, but this is what I was kind of interested in.",
                    "label": 0
                },
                {
                    "sent": "So what I've seen is that that people have made a translation from kind of the basic machinery to the kernel approach.",
                    "label": 0
                },
                {
                    "sent": "So if everything is linear an if you have Gaussian priors and basically what you have is a Gaussian process so that kind of work by mozzy and also by value and transparent and strike over.",
                    "label": 0
                },
                {
                    "sent": "So, so that brings you kind of from from the.",
                    "label": 0
                },
                {
                    "sent": "From the Bayesian multitask.",
                    "label": 0
                },
                {
                    "sent": "Anwyl to that kind of kernel, an machine world which I think is kind of nice.",
                    "label": 0
                },
                {
                    "sent": "But then I started thinking what's kind of the better, easier framework.",
                    "label": 0
                },
                {
                    "sent": "So the first question is so if you have this bottleneck MLP maybe not not linear but it was hyped up with hyperbolic tension.",
                    "label": 0
                },
                {
                    "sent": "So what would it mean for the kernel?",
                    "label": 1
                },
                {
                    "sent": "It's probably kind of an easy exercising, probably do that, but you can probably also derive kernels for task list ring engaging so you start with kind of basing machine.",
                    "label": 0
                },
                {
                    "sent": "We do the calculations.",
                    "label": 0
                },
                {
                    "sent": "And see what kind of girls you get.",
                    "label": 0
                },
                {
                    "sent": "If you want to implement that clustering, engaging in the kind of kernel machine framework, I don't know with any money has done that, but it shouldn't be too difficult to think we actually arrived to the kind of approach from the literature, no, but you can make the connection right.",
                    "label": 0
                },
                {
                    "sent": "Designer and presentation, which also and it made the task.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I've used Nebulization framework.",
                    "label": 0
                },
                {
                    "sent": "Then you just minimize the functional.",
                    "label": 0
                },
                {
                    "sent": "Yeah no.",
                    "label": 0
                },
                {
                    "sent": "Yeah I can see it, but it's not so difficult to connect the two I think.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "So maybe this one clearly makes the connection.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but the things that he they arrive at a very similar to the signature to describe here is very different computational algorithms.",
                    "label": 0
                },
                {
                    "sent": "I guess one of the issue is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, how it scales.",
                    "label": 0
                },
                {
                    "sent": "To train each other?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's definitely it's.",
                    "label": 0
                },
                {
                    "sent": "It's also a point messages on there.",
                    "label": 0
                },
                {
                    "sent": "Common formulation, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yep, quite appealing.",
                    "label": 0
                },
                {
                    "sent": "But I think you would also have it here if you just have a linear.",
                    "label": 0
                },
                {
                    "sent": "If you have linear linear.",
                    "label": 0
                },
                {
                    "sent": "Linear transfer functions then I haven't looked at it, but then it's very similar and I bet it's also convex.",
                    "label": 0
                },
                {
                    "sent": "You have any M approach?",
                    "label": 0
                },
                {
                    "sent": "No, but that's just one way to do it right?",
                    "label": 0
                },
                {
                    "sent": "It's just just alternate maximization, so if you apply, it doesn't mean that it is not convex, right?",
                    "label": 0
                },
                {
                    "sent": "Can also apply EM to complex problems.",
                    "label": 0
                },
                {
                    "sent": "I think you're probably exhausted lyrics.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "OK, we can.",
                    "label": 0
                },
                {
                    "sent": "We can check that.",
                    "label": 0
                },
                {
                    "sent": "Yeah so, but yeah, so one way to go is to start with the basic approach and then do the calculations and see what kind of kernels come out of that and so you could also probably do that for task blistering, engaging and look at the kernel set come out so.",
                    "label": 0
                },
                {
                    "sent": "And the question is, what's kind of the better approach, if there, if any, of them is better then perhaps it boils down to whether it's easier to think about distances and kernels if we want to define.",
                    "label": 0
                },
                {
                    "sent": "Kind of across difference between tasks or when it's easier to think about models and then he was kind of basic machinery.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "So so for me it's easier because of my background.",
                    "label": 0
                },
                {
                    "sent": "To define basing models which other people might be easier to think about distances and kernels.",
                    "label": 0
                },
                {
                    "sent": "Another question is what which approach is most sensitive to a sip optimal choice, so I don't think we can claim that.",
                    "label": 1
                },
                {
                    "sent": "Assumptions that we make are always valid in practice, but then so probably any choice will be sub optimal.",
                    "label": 0
                },
                {
                    "sent": "But then the question is which the optimal choice is most sensitive to being being sub optimal.",
                    "label": 1
                },
                {
                    "sent": "And of course there is the question.",
                    "label": 1
                },
                {
                    "sent": "What's the most efficient approach?",
                    "label": 0
                },
                {
                    "sent": "But yeah, so there I don't really know.",
                    "label": 0
                },
                {
                    "sent": "They have to sort it out then I have some technical questions but yeah, so technical that I'll leave this.",
                    "label": 0
                },
                {
                    "sent": "That's a big question, that an ultimate one, but it's pretty hard to formulate in any exact way.",
                    "label": 0
                },
                {
                    "sent": "I would've thought it 'cause.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know sub optimal is going to be measured in different scales if you're talking about Bayesian or.",
                    "label": 1
                },
                {
                    "sent": "Or kernel design innocence.",
                    "label": 0
                },
                {
                    "sent": "So when you might have a sort of objective measure performance at the end, that's actually sort of say.",
                    "label": 0
                },
                {
                    "sent": "To what extent is more suboptimal?",
                    "label": 0
                },
                {
                    "sent": "Or yeah, so how hard the Bayesian tried?",
                    "label": 0
                },
                {
                    "sent": "Did they try out of just joking or did the you know the kernel guys try harder?",
                    "label": 0
                },
                {
                    "sent": "Just imagine I think it depends.",
                    "label": 0
                },
                {
                    "sent": "A lot of what you want to do with the output of your inference.",
                    "label": 0
                },
                {
                    "sent": "If you have a subsequent decision phase where you want to make about measuring the output qualities, OK, it's a question of what I'm saying is, you know, you could have sort of a fair comparison of how good they are at the end.",
                    "label": 0
                },
                {
                    "sent": "But the question is, how do you measure the quality that the suboptimality at the input?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "My my my opinion is the if you could please correctly it will be pretty good, but usually people don't patient correctly.",
                    "label": 0
                },
                {
                    "sent": "Therefore it's actually pretty simple.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Like yeah.",
                    "label": 0
                },
                {
                    "sent": "He claims that these empirical Bayes approach is a good approximation.",
                    "label": 0
                },
                {
                    "sent": "It's kind of reasonable is not basically the only difference is it.",
                    "label": 0
                },
                {
                    "sent": "Without that there is a issue of whether intervals are better.",
                    "label": 0
                },
                {
                    "sent": "Better approach essentially may not their reason for that.",
                    "label": 0
                },
                {
                    "sent": "The other issue about integration out is computationally some more difficult question is I mean for example email starting with logistic regression.",
                    "label": 0
                },
                {
                    "sent": "This time will be more money then it will be more difficult, sure.",
                    "label": 0
                },
                {
                    "sent": "Sure, sure, definitely, but if it's linear in Gaussian then is not more difficult.",
                    "label": 0
                },
                {
                    "sent": "And Lastly vaginal system you want to optimize hyperparameters of.",
                    "label": 0
                },
                {
                    "sent": "Your kernel is still difficult is not as difficult as logistic regression, but still is difficult.",
                    "label": 0
                },
                {
                    "sent": "It is starting to optimize your half of any other kernel integral.",
                    "label": 0
                },
                {
                    "sent": "Each individual ways.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but in my case whether I optimize the.",
                    "label": 0
                },
                {
                    "sent": "The specific parameters or integrate amount is not more difficult.",
                    "label": 0
                },
                {
                    "sent": "Yeah it could be, yeah.",
                    "label": 0
                },
                {
                    "sent": "I think one important point guys.",
                    "label": 0
                },
                {
                    "sent": "If you do this fully Bayesian method then you set things up that you have nice conjugate priors and you can run full MCM.",
                    "label": 0
                },
                {
                    "sent": "The issue of Convexity's not so much of a big deal because you're going to be taking the vision average at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "You really have to worry about is is your posterior going to be very multimodal or not with these type of models it shouldn't be so nasty because of the type of construction.",
                    "label": 0
                },
                {
                    "sent": "So fun fact.",
                    "label": 0
                },
                {
                    "sent": "So if you're willing to wait.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's a big issue if you're willing to wait around for Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Wonderful vision process, then you have the benefit of being necessary.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so in my case I'm not willing to wait right?",
                    "label": 0
                },
                {
                    "sent": "Because I have to predict for 15,000 points of sale every day and I'm not going to wait for Markov chain to in the convergence equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Take Me Out so those things you could do, but I don't want to do MCMC all the way for these kind of problems.",
                    "label": 0
                },
                {
                    "sent": "Yeah, something you mentioned that.",
                    "label": 0
                },
                {
                    "sent": "And it does seem that stores some population means, so just a common algorithm.",
                    "label": 0
                },
                {
                    "sent": "Whether you check how much the same that's is and whether it's closed.",
                    "label": 0
                },
                {
                    "sent": "Welcome again to find buildings, for example, checking in Ohio with test data.",
                    "label": 0
                },
                {
                    "sent": "No, I haven't.",
                    "label": 0
                },
                {
                    "sent": "I haven't really done that.",
                    "label": 0
                },
                {
                    "sent": "Like a standard regularization W square right now, which will exist.",
                    "label": 0
                },
                {
                    "sent": "I think it was zero to something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I I definitely do not want to string towards 0 so I don't want to.",
                    "label": 0
                },
                {
                    "sent": "Telephone zero.",
                    "label": 0
                },
                {
                    "sent": "It's the same idea.",
                    "label": 0
                },
                {
                    "sent": "It's a similar idea, so so one thing is which I probably didn't emphasize that I don't have to do cross validation to determine what the optimal string change, because it kind of follows from the basic machinery.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "I haven't checked it, so I think I'm too much a Bayesian to then do cross validation to come check it.",
                    "label": 0
                },
                {
                    "sent": "You said you weren't invited, no.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm kind of an empirical Bayesian and at that level I kind of trust the things actually.",
                    "label": 0
                },
                {
                    "sent": "Ask us about the moment.",
                    "label": 0
                },
                {
                    "sent": "Question is not space typical base active right now.",
                    "label": 0
                },
                {
                    "sent": "We don't ask this question.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe I'm not.",
                    "label": 0
                },
                {
                    "sent": "We don't think about everything software.",
                    "label": 0
                },
                {
                    "sent": "But I think that people realize that.",
                    "label": 0
                },
                {
                    "sent": "That's my minus ten, OK?",
                    "label": 0
                },
                {
                    "sent": "I think that most Bayesians realized that the assumptions that they make are not really valid, but it's the best thing that they can do, and then they go from there and so.",
                    "label": 0
                },
                {
                    "sent": "No, I mean, there's a whole theory of these factors in many different extensive theory about suboptimality and OK. Actually even cross validation I think can be satisfied, right?",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Invasion thing.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions from the agents.",
                    "label": 0
                },
                {
                    "sent": "OK thanks, I hope it was actually one of the interesting topics for discussion.",
                    "label": 0
                },
                {
                    "sent": "I cannot be certain about that.",
                    "label": 0
                },
                {
                    "sent": "So thankfully.",
                    "label": 0
                }
            ]
        }
    }
}