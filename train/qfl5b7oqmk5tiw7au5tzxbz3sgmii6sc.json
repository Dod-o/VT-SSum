{
    "id": "qfl5b7oqmk5tiw7au5tzxbz3sgmii6sc",
    "title": "Fast Nearest Neighbor Retrieval for Bregman Divergences",
    "info": {
        "author": [
            "Lawrence Cayton, /Department of Computer Science and Engineering, Institute for Genomics and Bioinformatics, University of California"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Instance-based Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_cayton_fnn/",
    "segmentation": [
        [
            "OK, so I think my title is fairly self fix."
        ],
        [
            "Lana Tori, but let me just give a quick overview of quick introduction of what I'll be talking about.",
            "So the problem is the basics nearest neighbor problem and so we have these very large databases and we have some query Q and we just want to find the best match of queued in the database.",
            "So this is a database of images and so that's particularly important example for us.",
            "OK, so I hardly need to say it, but nearest neighbor methods, especially machine learning or ubiquitous, but also in other areas of computer science.",
            "The problem is, is that they're very slow, and so if you want to see which what's the best match in the database, you have to compare Q to virtually every element of the database, and once these databases get larger and larger and larger, it's just too expensive.",
            "So there's been a lot of work on trying to try to speed this up, so there's a lot of kind of intelligent data structures that help one.",
            "Only compute similarity to a few elements of the database rather than the whole thing, and so that's the hope to speed things up.",
            "The thing is, the issue is is that most of these methods and this is a huge research area in of itself.",
            "Most of these methods assume some kind of metric structure, so maybe L2, or maybe you know general metric spaces, but usually there's a metric assumption in there and there are a few exceptions.",
            "Now, in learning in vision in text and so forth, we use a lot of different sort of similarity measures, and some of these things are some of these things satisfy the triangle inequality.",
            "Some of these things are metrics, but some of these things are not, and the really important one that this work sort of got started under was KL divergences.",
            "OK, so the KL divergent is kind of the natural notion to compare probability distributions, and it's also the natural notion too well to compare probability distributions or anytime you have sort of histogram features or non negative features.",
            "KL divergences kind of the standard.",
            "OK, but it's not a metric, so in this work I'm going to be talking about data structure design for arbitrary Bregman divergences, and so Cal divergences.",
            "One particular example, but it's, but we'll just take out the whole Bregman divergent family."
        ],
        [
            "So let me do a quick review of what Bregman divergences are.",
            "So this is the mathematical definition of 'em.",
            "You start with some convex function F. OK and then you define the divergent this way and the basic thing is you have F and then you have the linear approximation of F, and so Bregman divergences, essentially the distance between the actual function F and the approximation.",
            "The linear approximation of F so.",
            "You can see that since F is assumed to be convex, this is always non negative.",
            "OK."
        ],
        [
            "OK, so here's a few examples and these are the sort of standard examples.",
            "The first ones, L2 squared distance.",
            "So anytime you have the pictures is like a one ball of these various divergences, so the first one is standard L2 squared, second one Mahalanobis distance.",
            "We've been talking about this already, so just an ellipsoid.",
            "And in the bottom two are kind of interesting ones.",
            "We have failed emergence and so this is sort of what the ball looks like.",
            "But depending on where the origin is centered, it gets shaped very differently, and then this is at the core of cyto and this is used in sound processing.",
            "So these bottom two are much less well behaved in the top two, but they are quite useful."
        ],
        [
            "OK, so I've been saying a lot of the nearest neighbour workers for metrics and so let's just do a sort of a rundown of Bregman divergent versus metrics.",
            "OK, so metrics satisfy three basic properties, right?",
            "Nonnegativity symmetry, and the triangle inequality, so symmetry is mostly just for convenience.",
            "It's not a big deal.",
            "But the triangle inequality is like the workhorse of nearest neighbor search.",
            "OK, so this is used extensively to sort of prune out large portions of the space from consideration.",
            "So this is really what's driving the huge preponderance of the algorithms for fast nearest neighbor search.",
            "So what do we get?"
        ],
        [
            "Bregman divergences well, we get nonnegativity, as I mentioned.",
            "However, they are not symmetric, so we have to in general, so we have to take care of that problem.",
            "We have to deal with the fact that the distance from X to Y might be different from the distance from white X, and we don't get the triangle inequality.",
            "So that's sort of unfortunate, because the kind of basic thing that we're using to drive these nearest neighbor search algorithms doesn't hold for Bregman divergences.",
            "So we're going to instead rely on."
        ],
        [
            "Some geometric properties.",
            "OK, so let me do a very quick one slide overview, and so if you're here for the previous talks talked about this a little bit too, but let me just give the basic kind of scheme of many of the nearest neighbor search data structures and search algorithms.",
            "We're going to follow the same kind of basic program, but we're going to swap out some elements, so the basic and this is a KD trees and metric trees follow the same format as do just a huge variety of these things.",
            "So the basic idea is is we take our space.",
            "And we carve it up into this week with the space, the decomposition.",
            "It's hierarchical, so the hope is that when we go to find the nearest neighbor, we're not going to look at the whole space.",
            "We're just going to be able to look at the small portion of it.",
            "And this corresponds to a binary tree or ternary tree or whatever, and then we go to search that you do some sort of branch inbound exploration.",
            "So this is the basic format that's behind metric treason, KD trees, etc.",
            "All details being omitted.",
            "So we're going to follow this basic same format in the Bregman Divergent's case, and this has some.",
            "Entries of this type are used extensively in practice, and so that's why we follow this kind of basic basic scheme."
        ],
        [
            "So, So what I'm going to be introducing is it's called a Bregman baltray OK, and so there's a sort of artists rendition over here now.",
            "Now, what's our fundamental geometric unit?",
            "So as I said, you know, you basically want to do is hierarchical decomposition, so the fundamental geometric unit we're going to be working with Bregman ball and so defined this way.",
            "And as I said, the order of these matter OK, and so the reason I defined it this way instead of putting you in the first position, is because Bregman divergences are all convex in the first argument, but not necessarily in this second.",
            "OK, so by defining it this way, the Bregman ball could be kind of shaped weirdly.",
            "It's not going to be like a.",
            "A circle, but it's at least going to be convex body, so we need to come up with a reasonable build heuristic.",
            "We can't use the triangle inequality and we're going to handle the asymmetry of these things.",
            "OK, so these are kind of challenges.",
            "I had the last one.",
            "Dealt with in the."
        ],
        [
            "Sure, but not not today.",
            "OK, so here's just a simple build heuristic to give an idea of how this works, so let me just motivate this with a little bit of intuition.",
            "You know the what you want out of this hierarchical decomposition is sort of the same thing you want out of a hierarchical clustering.",
            "OK, you want you want balls that are going to well separated.",
            "An compact at each level.",
            "OK, and the reason for this is when you have a query.",
            "If their wall separating compact like there are over here, odds are that any query is going to be much closer to one side to the other.",
            "OK, so after you've explored this side then you're going to be able to print out the left side, and that's the intuition.",
            "However, if we use these balls instead, it's the same.",
            "The same point set.",
            "You see they overlap.",
            "So now Q is basically in both of the balls, and so we're going to explore both sides."
        ],
        [
            "So very simple heuristic to build these things is just to use K means, so at each level we just we just use 2 means we just find 2 means at each level.",
            "And Fortunately for US K means was generalized to the whole Bregman divergences family a couple years ago, and that's in Jamila.",
            "OK."
        ],
        [
            "So that's the build.",
            "And now let's go through the search.",
            "OK, so this is the kind of high level of how you search these things, and then we're going to have to deal with one big detail later.",
            "So we're looking for the nearest neighbour, so here's how you do it.",
            "First, you were looking for the nearest neighbor of QQ.",
            "Is our query?",
            "OK, So what we're going to do is to send the tree at each level we're going to have to pick which child to go too, so we're going to do is go to the closer child so whoever is child, whichever child whose mean is closest, we're going to we're going or the sibling, but we're going to come back to it, so once we hit a leaf, we're going to compute distances to all the points in that leaf, and we're going to the candidate.",
            "The closest one our candidate nearest neighbor.",
            "So call the taxi.",
            "Now we're going to reverse back up the tree and we're going to check all those nodes we ignored before.",
            "So at each step, what we need to do is compare the distance to our current candidate nearest neighbor, so the distance to the Bregman Bowl of the ignored node.",
            "Now, if the candidate nearest neighbor is further away than this Bregman ball, then our two nearest neighbor could be in this node, so we need to explore it.",
            "So this is the basic basic search routine.",
            "The main trick is computing this, so this is just some fixed number, but this thing on the right.",
            "Is kind of the main technical hang up here, so if you have a metric tree then you can use some kind of triangle inequality.",
            "Get around on this.",
            "In this case we can't go so this is the kind of Maine."
        ],
        [
            "Goal challenge here.",
            "So we need to check if this triangle system if this inequality holds and again this thing is just some number but this thing is."
        ],
        [
            "We need to compute.",
            "OK, So what is this exactly?",
            "So this is the Bregman projection onto a Bregman ball.",
            "So here we have a Bregman ball, and we're trying to find the kind of closest point on this Baltic.",
            "You OK, but close.",
            "This is measured by Bregman divergences by Bregman, Divergent, and not by L2 distance or something like that.",
            "So this is a convex program, right?",
            "So this is a convex constraint, and again this is a pregnant versus convex in the first argument.",
            "But the fact that it's convex doesn't help that much 'cause we need it to be really efficient.",
            "So to give you an idea, when we're searching for a single queries nearest neighbor, we're going to have to compute many many, many, many of these projections.",
            "So like at least log in or something like that.",
            "So saying something like you know Interior Point method is just the wrong ballpark altogether, so we're going to come fast too."
        ],
        [
            "Evaluate this.",
            "OK, so since we have come up something fast, let's start with an easy case and build up a little intuition and hopefully that will build out.",
            "So in the L2 squared case.",
            "Why is this so easy?",
            "So nail 2 squared case we have Q we have you and the projections right here, so you can actually compute analytically.",
            "There's no program or anything like that.",
            "You need to run.",
            "So what is the what is the key property that we're using?",
            "What's so nice about it is that if you draw a line between Q and you, you know the projection is going to lie on that line, so you don't have to explore this whole big D dimensional convex body.",
            "Just look along this one line and you know that the answer is going to be on that line.",
            "OK, So what about the general case?"
        ],
        [
            "OK, so in the general case what we show is that something similar actually holds OK.",
            "It's not quite as."
        ],
        [
            "Simple, but it's pretty close, so we can show is that if you look at the instead in the gradient of F space, so just look at the gradient map that the projection is actually going to lie between you and Q again.",
            "But now everything's with this gradient operation in front of it.",
            "And now the L2 squared relationship that we just saw is actually a special case of this, 'cause in that case the gradient is just the identity function.",
            "So as this is useful."
        ],
        [
            "And the answer is yes.",
            "So since F is strictly convex and that's basic feature of Bregman divergences, the gradient map is going to be 1 to one.",
            "OK, so that means that we can go between the gradient F of F at X and regular X and the way you go between these two spaces is something that's well known.",
            "You use what's called the convex conjugate.",
            "So so the convex conjugate basically gives you a way to go between these two points.",
            "So my point is, is that if you can find this derivative of the projection, then you can get back to the original point by using this conjugate function so we can recover XP from the great enough XP."
        ],
        [
            "So just the picture here.",
            "So let mu prime Q prime denote those gradients.",
            "So what we know is that if we want to project Q onto this Bregman ball, it's not a line between Q and mu, but it's this curve, and we have an actual formula for this curve, and so as data runs between zero and one, we can just look along this curve and it's if you're not familiar with this conjugate stuff, it's no big deal, but just know that we kind of have the form for it, so we know that the solution lies along this curve.",
            "So this basic fact, along with like a couple of other little facts.",
            "Not going to go into it gives us."
        ],
        [
            "Really simple algorithm to compute the projection and the algorithm is just to use by section search.",
            "So basically we just need to look along this curve and you can show that it's actually monotonic along this curve and so all we need to do is a simple line search along here and so by section search just a simple by section search over Theta will get us the projection."
        ],
        [
            "So this is really nice because we get a solution and only log of one over epsilon steps and each step is really cheap.",
            "It just requires one gradient evaluation and one divergent evaluation.",
            "So this actually works very fast.",
            "OK."
        ],
        [
            "So.",
            "We can actually do a little bit better, and that's because we don't really need an exact solution to this projection.",
            "What we just need to do is evaluate whether this inequality holds.",
            "OK, so the way to do that the way we."
        ],
        [
            "Can do that is just come up with upper and lower bounds on the projection.",
            "So let me sort of skip over this a little bit, but basically you just use a lower bound given by weak duality and upper bound given by the primal function and using kind of evaluating these two bounds at each iteration allows you to stop really early so.",
            "This doesn't give you a better asymptotic bound on how many iterations you need, but practically speaking it works really well.",
            "OK."
        ],
        [
            "So let me just do a couple of quick experiments here so all the experiments are for the KL divergent San.",
            "That's because there isn't a good scheme out there for Cal.",
            "Vergence and it's practical."
        ],
        [
            "So what are the datasets we use?",
            "So the first one is MCV, so this is a text data set, half a million documents and these are all represented as Adidas.",
            "Excuse me?",
            "D dimensional distribution over topics, so this topic distribution is based by Randall D. Latent deer Slayer allocation to generate these.",
            "So when I tried to build up datasets of different dimensionalities.",
            "OK, second one Carl histograms.",
            "That's a standard data set.",
            "Third one, this is from content content based image retrieval.",
            "Literature is 371 dimensional representation.",
            "5000 Images final one is from vision signatures.",
            "These are quantized histogram features from Pascal data.",
            "So I notice the bottom two are very high dimensional and Karel is a reasonably high dimensional MCV summer dimensional.",
            "So these tree based methods typically start breaking down around here.",
            "Will just over there."
        ],
        [
            "OK, so the way these tree based methods are often deployed in practice is sometimes you use him to get an exact solution, but often that doesn't give you enough, so they're just sort of deployed approximately the way you do.",
            "That is, you run them and then you just examine a few leaves and then sort of cut the search off early.",
            "So spend as much time as you can and then just cut it off so to evaluate when you run it.",
            "In this situation we show the speedup over brute force versus number closer, so number closer is how many.",
            "Closer points are there, so if I return the 4th nearest neighbor, there's three closer.",
            "So you want that to be small."
        ],
        [
            "OK, so this is for 128 dimensional representation of dark data set.",
            "the Y axis is the speed up the exponent.",
            "This is log log.",
            "The exponent of speedup ranging from 10 to 0, so it's no speed up to 10 to the 5th, so that's 100,000 times speedup and the number closer ranges from 10 to the negative 2:00 to 1:50.",
            "OK, so to give you a couple of highlights, right?",
            "So it tend to zero.",
            "That means we're on average getting the second nearest neighbor first or second nearest neighbor out of half a million points.",
            "And we're getting a speedup of about 100 X.",
            "If you need a if you don't need that kind of approximation with one and so this is giving so this is we're getting one of the top 10 nearest neighbors again out of half a million points.",
            "We're getting about 1000 XP up.",
            "OK, so."
        ],
        [
            "We get similar sort of results for the various dimensionalities of the MCV data set, so it starts at 8 and goes to 256.",
            "Want to look at these more closely?"
        ],
        [
            "Take a look at the poster now for corelle, we're also getting very good results.",
            "Semantic space.",
            "It starts deteriorating a little bit, but we're still getting useful results, so 10X speedup at almost the exact nearest neighbor 100X speed up on one of the top 10 and so.",
            "The 1111 Dimensional thing is where we're starting to sort of pan out.",
            "It's just getting too high dimensional, although this is still gives you some useful speedup."
        ],
        [
            "OK, so just one more experiment.",
            "What if we use it for exact search now for exact search this is well known it when you use these sort of methods they get hurt with the dimensionality, but we still do pretty well.",
            "So all the way up to 256 dimensions we get a 3 1/2 times speedup.",
            "So these are all pretty useful numbers.",
            "And then again on these really high dimensional ones.",
            "No speedup or even a little worse, but these are still pretty useful speedups all the way up to there.",
            "OK."
        ],
        [
            "So I want to thank all these folks for some helpful recommendations.",
            "And thanks for listening.",
            "Question.",
            "Permission edition point goes to the center of the Berlin Wall.",
            "How do you represent it?",
            "Great excuse me.",
            "Look at the gradient instrument.",
            "With respect I mean the formula for the curve is.",
            "Is that gradient of the conjugate function?",
            "Event.",
            "Oh, as data runs from between zero and one.",
            "You don't actually need to compute the gradient of that curve and less you want to do Newton's method or something.",
            "Then you would need it.",
            "Even 01.",
            "Yeah yeah, yeah.",
            "In practice, balls overlap.",
            "They will overlap some.",
            "Yeah, they're not going to be disjoint.",
            "You do get a bit of overlap, yeah?",
            "I mean, I took the form is really impressive with you.",
            "Could you please repeat it even further?",
            "Still paid over at all?",
            "Yeah, I mean the build method is just a heuristic, right?",
            "So pretty much the build methods for all these guys are heuristics.",
            "But yeah, you should be able to do a bit better.",
            "I think if you were to so I was originally working with different heuristic and then I went over this K means thing and that actually bumped performance up significantly.",
            "So I think if you were to come up with a bit more principled method then you would help it out a bit more.",
            "I don't think I think you might be able to get like 2X improvement, not like a order of magnitude or something like that, but I think a little bit of improvements possible.",
            "Yeah, I mean another talks often.",
            "See these hierarchical topic models where they essentially got a hierarchical way.",
            "Choosing these artistic Rams over words.",
            "Yeah, that seems like it's gotten better.",
            "Oh, I see.",
            "So actually, actually using the mixture hierarchy like in LDA sort of things.",
            "Yeah, that might give you some better separation."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I think my title is fairly self fix.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lana Tori, but let me just give a quick overview of quick introduction of what I'll be talking about.",
                    "label": 0
                },
                {
                    "sent": "So the problem is the basics nearest neighbor problem and so we have these very large databases and we have some query Q and we just want to find the best match of queued in the database.",
                    "label": 1
                },
                {
                    "sent": "So this is a database of images and so that's particularly important example for us.",
                    "label": 0
                },
                {
                    "sent": "OK, so I hardly need to say it, but nearest neighbor methods, especially machine learning or ubiquitous, but also in other areas of computer science.",
                    "label": 0
                },
                {
                    "sent": "The problem is, is that they're very slow, and so if you want to see which what's the best match in the database, you have to compare Q to virtually every element of the database, and once these databases get larger and larger and larger, it's just too expensive.",
                    "label": 0
                },
                {
                    "sent": "So there's been a lot of work on trying to try to speed this up, so there's a lot of kind of intelligent data structures that help one.",
                    "label": 0
                },
                {
                    "sent": "Only compute similarity to a few elements of the database rather than the whole thing, and so that's the hope to speed things up.",
                    "label": 0
                },
                {
                    "sent": "The thing is, the issue is is that most of these methods and this is a huge research area in of itself.",
                    "label": 0
                },
                {
                    "sent": "Most of these methods assume some kind of metric structure, so maybe L2, or maybe you know general metric spaces, but usually there's a metric assumption in there and there are a few exceptions.",
                    "label": 0
                },
                {
                    "sent": "Now, in learning in vision in text and so forth, we use a lot of different sort of similarity measures, and some of these things are some of these things satisfy the triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "Some of these things are metrics, but some of these things are not, and the really important one that this work sort of got started under was KL divergences.",
                    "label": 0
                },
                {
                    "sent": "OK, so the KL divergent is kind of the natural notion to compare probability distributions, and it's also the natural notion too well to compare probability distributions or anytime you have sort of histogram features or non negative features.",
                    "label": 0
                },
                {
                    "sent": "KL divergences kind of the standard.",
                    "label": 1
                },
                {
                    "sent": "OK, but it's not a metric, so in this work I'm going to be talking about data structure design for arbitrary Bregman divergences, and so Cal divergences.",
                    "label": 0
                },
                {
                    "sent": "One particular example, but it's, but we'll just take out the whole Bregman divergent family.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me do a quick review of what Bregman divergences are.",
                    "label": 0
                },
                {
                    "sent": "So this is the mathematical definition of 'em.",
                    "label": 0
                },
                {
                    "sent": "You start with some convex function F. OK and then you define the divergent this way and the basic thing is you have F and then you have the linear approximation of F, and so Bregman divergences, essentially the distance between the actual function F and the approximation.",
                    "label": 0
                },
                {
                    "sent": "The linear approximation of F so.",
                    "label": 0
                },
                {
                    "sent": "You can see that since F is assumed to be convex, this is always non negative.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's a few examples and these are the sort of standard examples.",
                    "label": 0
                },
                {
                    "sent": "The first ones, L2 squared distance.",
                    "label": 0
                },
                {
                    "sent": "So anytime you have the pictures is like a one ball of these various divergences, so the first one is standard L2 squared, second one Mahalanobis distance.",
                    "label": 0
                },
                {
                    "sent": "We've been talking about this already, so just an ellipsoid.",
                    "label": 0
                },
                {
                    "sent": "And in the bottom two are kind of interesting ones.",
                    "label": 0
                },
                {
                    "sent": "We have failed emergence and so this is sort of what the ball looks like.",
                    "label": 0
                },
                {
                    "sent": "But depending on where the origin is centered, it gets shaped very differently, and then this is at the core of cyto and this is used in sound processing.",
                    "label": 0
                },
                {
                    "sent": "So these bottom two are much less well behaved in the top two, but they are quite useful.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I've been saying a lot of the nearest neighbour workers for metrics and so let's just do a sort of a rundown of Bregman divergent versus metrics.",
                    "label": 0
                },
                {
                    "sent": "OK, so metrics satisfy three basic properties, right?",
                    "label": 0
                },
                {
                    "sent": "Nonnegativity symmetry, and the triangle inequality, so symmetry is mostly just for convenience.",
                    "label": 0
                },
                {
                    "sent": "It's not a big deal.",
                    "label": 0
                },
                {
                    "sent": "But the triangle inequality is like the workhorse of nearest neighbor search.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is used extensively to sort of prune out large portions of the space from consideration.",
                    "label": 0
                },
                {
                    "sent": "So this is really what's driving the huge preponderance of the algorithms for fast nearest neighbor search.",
                    "label": 0
                },
                {
                    "sent": "So what do we get?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bregman divergences well, we get nonnegativity, as I mentioned.",
                    "label": 0
                },
                {
                    "sent": "However, they are not symmetric, so we have to in general, so we have to take care of that problem.",
                    "label": 0
                },
                {
                    "sent": "We have to deal with the fact that the distance from X to Y might be different from the distance from white X, and we don't get the triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of unfortunate, because the kind of basic thing that we're using to drive these nearest neighbor search algorithms doesn't hold for Bregman divergences.",
                    "label": 0
                },
                {
                    "sent": "So we're going to instead rely on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some geometric properties.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me do a very quick one slide overview, and so if you're here for the previous talks talked about this a little bit too, but let me just give the basic kind of scheme of many of the nearest neighbor search data structures and search algorithms.",
                    "label": 0
                },
                {
                    "sent": "We're going to follow the same kind of basic program, but we're going to swap out some elements, so the basic and this is a KD trees and metric trees follow the same format as do just a huge variety of these things.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is is we take our space.",
                    "label": 0
                },
                {
                    "sent": "And we carve it up into this week with the space, the decomposition.",
                    "label": 0
                },
                {
                    "sent": "It's hierarchical, so the hope is that when we go to find the nearest neighbor, we're not going to look at the whole space.",
                    "label": 0
                },
                {
                    "sent": "We're just going to be able to look at the small portion of it.",
                    "label": 0
                },
                {
                    "sent": "And this corresponds to a binary tree or ternary tree or whatever, and then we go to search that you do some sort of branch inbound exploration.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic format that's behind metric treason, KD trees, etc.",
                    "label": 0
                },
                {
                    "sent": "All details being omitted.",
                    "label": 0
                },
                {
                    "sent": "So we're going to follow this basic same format in the Bregman Divergent's case, and this has some.",
                    "label": 0
                },
                {
                    "sent": "Entries of this type are used extensively in practice, and so that's why we follow this kind of basic basic scheme.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, So what I'm going to be introducing is it's called a Bregman baltray OK, and so there's a sort of artists rendition over here now.",
                    "label": 0
                },
                {
                    "sent": "Now, what's our fundamental geometric unit?",
                    "label": 1
                },
                {
                    "sent": "So as I said, you know, you basically want to do is hierarchical decomposition, so the fundamental geometric unit we're going to be working with Bregman ball and so defined this way.",
                    "label": 0
                },
                {
                    "sent": "And as I said, the order of these matter OK, and so the reason I defined it this way instead of putting you in the first position, is because Bregman divergences are all convex in the first argument, but not necessarily in this second.",
                    "label": 0
                },
                {
                    "sent": "OK, so by defining it this way, the Bregman ball could be kind of shaped weirdly.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be like a.",
                    "label": 1
                },
                {
                    "sent": "A circle, but it's at least going to be convex body, so we need to come up with a reasonable build heuristic.",
                    "label": 0
                },
                {
                    "sent": "We can't use the triangle inequality and we're going to handle the asymmetry of these things.",
                    "label": 1
                },
                {
                    "sent": "OK, so these are kind of challenges.",
                    "label": 0
                },
                {
                    "sent": "I had the last one.",
                    "label": 0
                },
                {
                    "sent": "Dealt with in the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure, but not not today.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's just a simple build heuristic to give an idea of how this works, so let me just motivate this with a little bit of intuition.",
                    "label": 0
                },
                {
                    "sent": "You know the what you want out of this hierarchical decomposition is sort of the same thing you want out of a hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "OK, you want you want balls that are going to well separated.",
                    "label": 1
                },
                {
                    "sent": "An compact at each level.",
                    "label": 0
                },
                {
                    "sent": "OK, and the reason for this is when you have a query.",
                    "label": 0
                },
                {
                    "sent": "If their wall separating compact like there are over here, odds are that any query is going to be much closer to one side to the other.",
                    "label": 0
                },
                {
                    "sent": "OK, so after you've explored this side then you're going to be able to print out the left side, and that's the intuition.",
                    "label": 0
                },
                {
                    "sent": "However, if we use these balls instead, it's the same.",
                    "label": 0
                },
                {
                    "sent": "The same point set.",
                    "label": 0
                },
                {
                    "sent": "You see they overlap.",
                    "label": 0
                },
                {
                    "sent": "So now Q is basically in both of the balls, and so we're going to explore both sides.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very simple heuristic to build these things is just to use K means, so at each level we just we just use 2 means we just find 2 means at each level.",
                    "label": 0
                },
                {
                    "sent": "And Fortunately for US K means was generalized to the whole Bregman divergences family a couple years ago, and that's in Jamila.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the build.",
                    "label": 0
                },
                {
                    "sent": "And now let's go through the search.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the kind of high level of how you search these things, and then we're going to have to deal with one big detail later.",
                    "label": 0
                },
                {
                    "sent": "So we're looking for the nearest neighbour, so here's how you do it.",
                    "label": 0
                },
                {
                    "sent": "First, you were looking for the nearest neighbor of QQ.",
                    "label": 0
                },
                {
                    "sent": "Is our query?",
                    "label": 0
                },
                {
                    "sent": "OK, So what we're going to do is to send the tree at each level we're going to have to pick which child to go too, so we're going to do is go to the closer child so whoever is child, whichever child whose mean is closest, we're going to we're going or the sibling, but we're going to come back to it, so once we hit a leaf, we're going to compute distances to all the points in that leaf, and we're going to the candidate.",
                    "label": 1
                },
                {
                    "sent": "The closest one our candidate nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "So call the taxi.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to reverse back up the tree and we're going to check all those nodes we ignored before.",
                    "label": 0
                },
                {
                    "sent": "So at each step, what we need to do is compare the distance to our current candidate nearest neighbor, so the distance to the Bregman Bowl of the ignored node.",
                    "label": 1
                },
                {
                    "sent": "Now, if the candidate nearest neighbor is further away than this Bregman ball, then our two nearest neighbor could be in this node, so we need to explore it.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic basic search routine.",
                    "label": 0
                },
                {
                    "sent": "The main trick is computing this, so this is just some fixed number, but this thing on the right.",
                    "label": 0
                },
                {
                    "sent": "Is kind of the main technical hang up here, so if you have a metric tree then you can use some kind of triangle inequality.",
                    "label": 0
                },
                {
                    "sent": "Get around on this.",
                    "label": 0
                },
                {
                    "sent": "In this case we can't go so this is the kind of Maine.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Goal challenge here.",
                    "label": 0
                },
                {
                    "sent": "So we need to check if this triangle system if this inequality holds and again this thing is just some number but this thing is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We need to compute.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is this exactly?",
                    "label": 0
                },
                {
                    "sent": "So this is the Bregman projection onto a Bregman ball.",
                    "label": 1
                },
                {
                    "sent": "So here we have a Bregman ball, and we're trying to find the kind of closest point on this Baltic.",
                    "label": 0
                },
                {
                    "sent": "You OK, but close.",
                    "label": 0
                },
                {
                    "sent": "This is measured by Bregman divergences by Bregman, Divergent, and not by L2 distance or something like that.",
                    "label": 0
                },
                {
                    "sent": "So this is a convex program, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a convex constraint, and again this is a pregnant versus convex in the first argument.",
                    "label": 0
                },
                {
                    "sent": "But the fact that it's convex doesn't help that much 'cause we need it to be really efficient.",
                    "label": 0
                },
                {
                    "sent": "So to give you an idea, when we're searching for a single queries nearest neighbor, we're going to have to compute many many, many, many of these projections.",
                    "label": 0
                },
                {
                    "sent": "So like at least log in or something like that.",
                    "label": 0
                },
                {
                    "sent": "So saying something like you know Interior Point method is just the wrong ballpark altogether, so we're going to come fast too.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluate this.",
                    "label": 0
                },
                {
                    "sent": "OK, so since we have come up something fast, let's start with an easy case and build up a little intuition and hopefully that will build out.",
                    "label": 0
                },
                {
                    "sent": "So in the L2 squared case.",
                    "label": 0
                },
                {
                    "sent": "Why is this so easy?",
                    "label": 0
                },
                {
                    "sent": "So nail 2 squared case we have Q we have you and the projections right here, so you can actually compute analytically.",
                    "label": 0
                },
                {
                    "sent": "There's no program or anything like that.",
                    "label": 0
                },
                {
                    "sent": "You need to run.",
                    "label": 0
                },
                {
                    "sent": "So what is the what is the key property that we're using?",
                    "label": 0
                },
                {
                    "sent": "What's so nice about it is that if you draw a line between Q and you, you know the projection is going to lie on that line, so you don't have to explore this whole big D dimensional convex body.",
                    "label": 0
                },
                {
                    "sent": "Just look along this one line and you know that the answer is going to be on that line.",
                    "label": 0
                },
                {
                    "sent": "OK, So what about the general case?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in the general case what we show is that something similar actually holds OK.",
                    "label": 0
                },
                {
                    "sent": "It's not quite as.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple, but it's pretty close, so we can show is that if you look at the instead in the gradient of F space, so just look at the gradient map that the projection is actually going to lie between you and Q again.",
                    "label": 0
                },
                {
                    "sent": "But now everything's with this gradient operation in front of it.",
                    "label": 0
                },
                {
                    "sent": "And now the L2 squared relationship that we just saw is actually a special case of this, 'cause in that case the gradient is just the identity function.",
                    "label": 1
                },
                {
                    "sent": "So as this is useful.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "So since F is strictly convex and that's basic feature of Bregman divergences, the gradient map is going to be 1 to one.",
                    "label": 1
                },
                {
                    "sent": "OK, so that means that we can go between the gradient F of F at X and regular X and the way you go between these two spaces is something that's well known.",
                    "label": 0
                },
                {
                    "sent": "You use what's called the convex conjugate.",
                    "label": 0
                },
                {
                    "sent": "So so the convex conjugate basically gives you a way to go between these two points.",
                    "label": 0
                },
                {
                    "sent": "So my point is, is that if you can find this derivative of the projection, then you can get back to the original point by using this conjugate function so we can recover XP from the great enough XP.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just the picture here.",
                    "label": 0
                },
                {
                    "sent": "So let mu prime Q prime denote those gradients.",
                    "label": 0
                },
                {
                    "sent": "So what we know is that if we want to project Q onto this Bregman ball, it's not a line between Q and mu, but it's this curve, and we have an actual formula for this curve, and so as data runs between zero and one, we can just look along this curve and it's if you're not familiar with this conjugate stuff, it's no big deal, but just know that we kind of have the form for it, so we know that the solution lies along this curve.",
                    "label": 0
                },
                {
                    "sent": "So this basic fact, along with like a couple of other little facts.",
                    "label": 0
                },
                {
                    "sent": "Not going to go into it gives us.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really simple algorithm to compute the projection and the algorithm is just to use by section search.",
                    "label": 0
                },
                {
                    "sent": "So basically we just need to look along this curve and you can show that it's actually monotonic along this curve and so all we need to do is a simple line search along here and so by section search just a simple by section search over Theta will get us the projection.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is really nice because we get a solution and only log of one over epsilon steps and each step is really cheap.",
                    "label": 1
                },
                {
                    "sent": "It just requires one gradient evaluation and one divergent evaluation.",
                    "label": 1
                },
                {
                    "sent": "So this actually works very fast.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can actually do a little bit better, and that's because we don't really need an exact solution to this projection.",
                    "label": 1
                },
                {
                    "sent": "What we just need to do is evaluate whether this inequality holds.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way to do that the way we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can do that is just come up with upper and lower bounds on the projection.",
                    "label": 1
                },
                {
                    "sent": "So let me sort of skip over this a little bit, but basically you just use a lower bound given by weak duality and upper bound given by the primal function and using kind of evaluating these two bounds at each iteration allows you to stop really early so.",
                    "label": 1
                },
                {
                    "sent": "This doesn't give you a better asymptotic bound on how many iterations you need, but practically speaking it works really well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just do a couple of quick experiments here so all the experiments are for the KL divergent San.",
                    "label": 0
                },
                {
                    "sent": "That's because there isn't a good scheme out there for Cal.",
                    "label": 1
                },
                {
                    "sent": "Vergence and it's practical.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the datasets we use?",
                    "label": 0
                },
                {
                    "sent": "So the first one is MCV, so this is a text data set, half a million documents and these are all represented as Adidas.",
                    "label": 0
                },
                {
                    "sent": "Excuse me?",
                    "label": 0
                },
                {
                    "sent": "D dimensional distribution over topics, so this topic distribution is based by Randall D. Latent deer Slayer allocation to generate these.",
                    "label": 0
                },
                {
                    "sent": "So when I tried to build up datasets of different dimensionalities.",
                    "label": 0
                },
                {
                    "sent": "OK, second one Carl histograms.",
                    "label": 0
                },
                {
                    "sent": "That's a standard data set.",
                    "label": 0
                },
                {
                    "sent": "Third one, this is from content content based image retrieval.",
                    "label": 0
                },
                {
                    "sent": "Literature is 371 dimensional representation.",
                    "label": 0
                },
                {
                    "sent": "5000 Images final one is from vision signatures.",
                    "label": 0
                },
                {
                    "sent": "These are quantized histogram features from Pascal data.",
                    "label": 1
                },
                {
                    "sent": "So I notice the bottom two are very high dimensional and Karel is a reasonably high dimensional MCV summer dimensional.",
                    "label": 0
                },
                {
                    "sent": "So these tree based methods typically start breaking down around here.",
                    "label": 0
                },
                {
                    "sent": "Will just over there.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the way these tree based methods are often deployed in practice is sometimes you use him to get an exact solution, but often that doesn't give you enough, so they're just sort of deployed approximately the way you do.",
                    "label": 0
                },
                {
                    "sent": "That is, you run them and then you just examine a few leaves and then sort of cut the search off early.",
                    "label": 1
                },
                {
                    "sent": "So spend as much time as you can and then just cut it off so to evaluate when you run it.",
                    "label": 0
                },
                {
                    "sent": "In this situation we show the speedup over brute force versus number closer, so number closer is how many.",
                    "label": 1
                },
                {
                    "sent": "Closer points are there, so if I return the 4th nearest neighbor, there's three closer.",
                    "label": 1
                },
                {
                    "sent": "So you want that to be small.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is for 128 dimensional representation of dark data set.",
                    "label": 0
                },
                {
                    "sent": "the Y axis is the speed up the exponent.",
                    "label": 0
                },
                {
                    "sent": "This is log log.",
                    "label": 0
                },
                {
                    "sent": "The exponent of speedup ranging from 10 to 0, so it's no speed up to 10 to the 5th, so that's 100,000 times speedup and the number closer ranges from 10 to the negative 2:00 to 1:50.",
                    "label": 0
                },
                {
                    "sent": "OK, so to give you a couple of highlights, right?",
                    "label": 0
                },
                {
                    "sent": "So it tend to zero.",
                    "label": 0
                },
                {
                    "sent": "That means we're on average getting the second nearest neighbor first or second nearest neighbor out of half a million points.",
                    "label": 0
                },
                {
                    "sent": "And we're getting a speedup of about 100 X.",
                    "label": 0
                },
                {
                    "sent": "If you need a if you don't need that kind of approximation with one and so this is giving so this is we're getting one of the top 10 nearest neighbors again out of half a million points.",
                    "label": 0
                },
                {
                    "sent": "We're getting about 1000 XP up.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We get similar sort of results for the various dimensionalities of the MCV data set, so it starts at 8 and goes to 256.",
                    "label": 0
                },
                {
                    "sent": "Want to look at these more closely?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take a look at the poster now for corelle, we're also getting very good results.",
                    "label": 0
                },
                {
                    "sent": "Semantic space.",
                    "label": 0
                },
                {
                    "sent": "It starts deteriorating a little bit, but we're still getting useful results, so 10X speedup at almost the exact nearest neighbor 100X speed up on one of the top 10 and so.",
                    "label": 0
                },
                {
                    "sent": "The 1111 Dimensional thing is where we're starting to sort of pan out.",
                    "label": 0
                },
                {
                    "sent": "It's just getting too high dimensional, although this is still gives you some useful speedup.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just one more experiment.",
                    "label": 0
                },
                {
                    "sent": "What if we use it for exact search now for exact search this is well known it when you use these sort of methods they get hurt with the dimensionality, but we still do pretty well.",
                    "label": 0
                },
                {
                    "sent": "So all the way up to 256 dimensions we get a 3 1/2 times speedup.",
                    "label": 0
                },
                {
                    "sent": "So these are all pretty useful numbers.",
                    "label": 0
                },
                {
                    "sent": "And then again on these really high dimensional ones.",
                    "label": 0
                },
                {
                    "sent": "No speedup or even a little worse, but these are still pretty useful speedups all the way up to there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to thank all these folks for some helpful recommendations.",
                    "label": 0
                },
                {
                    "sent": "And thanks for listening.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Permission edition point goes to the center of the Berlin Wall.",
                    "label": 0
                },
                {
                    "sent": "How do you represent it?",
                    "label": 0
                },
                {
                    "sent": "Great excuse me.",
                    "label": 0
                },
                {
                    "sent": "Look at the gradient instrument.",
                    "label": 0
                },
                {
                    "sent": "With respect I mean the formula for the curve is.",
                    "label": 0
                },
                {
                    "sent": "Is that gradient of the conjugate function?",
                    "label": 0
                },
                {
                    "sent": "Event.",
                    "label": 0
                },
                {
                    "sent": "Oh, as data runs from between zero and one.",
                    "label": 0
                },
                {
                    "sent": "You don't actually need to compute the gradient of that curve and less you want to do Newton's method or something.",
                    "label": 0
                },
                {
                    "sent": "Then you would need it.",
                    "label": 0
                },
                {
                    "sent": "Even 01.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "In practice, balls overlap.",
                    "label": 0
                },
                {
                    "sent": "They will overlap some.",
                    "label": 0
                },
                {
                    "sent": "Yeah, they're not going to be disjoint.",
                    "label": 0
                },
                {
                    "sent": "You do get a bit of overlap, yeah?",
                    "label": 0
                },
                {
                    "sent": "I mean, I took the form is really impressive with you.",
                    "label": 0
                },
                {
                    "sent": "Could you please repeat it even further?",
                    "label": 0
                },
                {
                    "sent": "Still paid over at all?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean the build method is just a heuristic, right?",
                    "label": 0
                },
                {
                    "sent": "So pretty much the build methods for all these guys are heuristics.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you should be able to do a bit better.",
                    "label": 0
                },
                {
                    "sent": "I think if you were to so I was originally working with different heuristic and then I went over this K means thing and that actually bumped performance up significantly.",
                    "label": 0
                },
                {
                    "sent": "So I think if you were to come up with a bit more principled method then you would help it out a bit more.",
                    "label": 0
                },
                {
                    "sent": "I don't think I think you might be able to get like 2X improvement, not like a order of magnitude or something like that, but I think a little bit of improvements possible.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean another talks often.",
                    "label": 0
                },
                {
                    "sent": "See these hierarchical topic models where they essentially got a hierarchical way.",
                    "label": 0
                },
                {
                    "sent": "Choosing these artistic Rams over words.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that seems like it's gotten better.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see.",
                    "label": 0
                },
                {
                    "sent": "So actually, actually using the mixture hierarchy like in LDA sort of things.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that might give you some better separation.",
                    "label": 0
                }
            ]
        }
    }
}