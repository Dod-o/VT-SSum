{
    "id": "a56jdzjzm3565jzddonkuvkuvgyjfkmj",
    "title": "Label Embedding for Text Recognition",
    "info": {
        "author": [
            "Jos\u00e9 A. Rodriguez, Xerox Research Centre Europe, Xerox"
        ],
        "published": "April 3, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_rodriguez_text_recognition/",
    "segmentation": [
        [
            "My name is Jose Rodriguez.",
            "I'm a permanent researcher at Xerox.",
            "The title of this dog is level embedding for text recognition and this is joint work with Logan Paramel."
        ],
        [
            "This presentation is structured in three parts, motivation, label embedding, model, an experiment, some conclusions and."
        ],
        [
            "Start with the motivation."
        ],
        [
            "The problem of interest in this work is text recognition and natural images.",
            "This is illustrated with images of recent scene text recognition data set.",
            "As you can appreciate the difficulty of doing text recognition in this type of images is significantly higher than performing text recognition in clean documents.",
            "This is due to the variety of colors, fonts, backgrounds and character configurations.",
            "The applications of such an approach would be automatic navigation, automatic transcription, or assistance."
        ],
        [
            "As in previous works, the goal we will frame our work in Word recognition.",
            "This is the problem where the input is a corrupt image of our work and the output is the sequence of characters that are present in that work.",
            "We have to define."
        ],
        [
            "And also the lexicon, which is the list of valid words and this is application dependent, so there're applications like word spotting, where lexicons can be very small and there is applications like recognition where lexicon could be open."
        ],
        [
            "So how do we in general detect text in images to the classical strategy?",
            "Follows a bottom up approach as illustrated here.",
            "We have a word image and we will run a character detector, for example based on Hog features.",
            "Two propose some character hypothesis.",
            "This character hypothesis leads to some plausible world hypothesis, and then there is a high level model such as a CRF that will select what is the most likely hypothesis.",
            "The advantage of such an approach?",
            "Is that it's scalable to large lexicons?",
            "It enables recognizing any word.",
            "As long as you can compose it from the individual characters of your alphabet.",
            "But the downside is that it relies on a character detector, so it's sensitive to the errors made by the character detector, and usually to build a robust character detector, we will perform very heavy preprocessing and postprocessing.",
            "As opposed."
        ],
        [
            "That type of approaches we could ask ourselves, is it possible to not try to detect characters in the image and this is what some works have been doing, which are known as holistic approaches.",
            "So here the image will be represented as a single feature vector and then we will do a nearest neighbor match with a set of templates from a database.",
            "And then this approach is simple and efficient.",
            "Some works have been shown that this is feasible for small, but also for not so small lexicons, so they can go up off to several thousands of words.",
            "The main drawback, however, is that in order to be able to recognize one word, you have to have seen at least one annotated example, so we don't have the O shot learning property."
        ],
        [
            "In our work, we still want to perform text recognition without having to detect the characters.",
            "And that means we will extract a single feature vector per image we want to compute the match between an image Anna label using a simple function, But we want to preserve the sure shot learning property.",
            "And the."
        ],
        [
            "Rest of the presentation will describe how this is achieved."
        ],
        [
            "Here we show a schematic representation of our approach.",
            "On the one side, we have an image.",
            "The pixel values of that image are denoted as X.",
            "We apply an operation to that image that we called image embedding, which results in a vector that we call Theta.",
            "Sorry yeah fit of X, which is nothing more than the feature extraction.",
            "But we call it in this way because on the other side which read the label in exactly the same way, so a label is a sequence of characters that within out why and we apply an operation that in this case is called level embedding that produces a vector called Phi of Y. OK, and what we would like ideally is to be able to compare this set of X with this fire Y.",
            "So one way to do that is to bring this set of X in the space of the five Y using a suitable projection with a matrix W transpose.",
            "Now here.",
            "If we manage to express these two, both images and labels in the same space, we can compute the match between an image and label with a simple similarity, fun."
        ],
        [
            "For example, we could use this bilinear form here and then the recognition of an image.",
            "So recognition of the image represented by this point just amounts to finding the closest label.",
            "OK, so we have a simple approach that is quite different from what has been done before in text recognition.",
            "Here there are two key points.",
            "First, that we do not only embed the features as in most of the approaches, but we also embed the labels.",
            "So we embed the input and output space and.",
            "We will have to define how to learn a suitable projection matrix, so in."
        ],
        [
            "Summary to materialize this framework, we need three things.",
            "One defined image embedding, second defined label embedding and 3rd define how to learn.",
            "Sorry the projection matrix.",
            "In fact, what I have not mentioned is the very fact that we do this label embedding is what will enable the zero shot learning, because once the model is learned you can project any new label into that space and the method will work."
        ],
        [
            "So for image features, this is the first point we have to define.",
            "Well, this is simple.",
            "We are computer vision people.",
            "We know many ways of coding pixels into vectors, so very simply I will just use the feature vector because it obtains state of the art results in image retrieval, Ann, we know this task resembles retrieval alot.",
            "So question one was easy to solve."
        ],
        [
            "Question two, we have to define how to embed a sequence of characters into a vector space.",
            "To that end, we propose what we call the special pyramid of characters.",
            "This is basically taking the sequence of characters and doing a bag of characters with a linear special pyramid.",
            "OK to illustrate it with particular example, imagine we have the sequence of characters ABC DE OK.",
            "This is a sequence of characters, not an image.",
            "OK, this is important to mention.",
            "Now what we will do is so also imagine the alphabet is only consists of these 5 characters.",
            "For this simple example.",
            "So what we will do is in a first level we just count how many times each character appears.",
            "So in each case each character counts only once to the histogram.",
            "OK, so in a second level we split the sequence into two parts and we count the number of characters in each part.",
            "So in the first part we have a B and half of a C. OK because she falls into the two parts.",
            "And in the second half we have half a CDN and so we could go down any number of levels we wanted an we would in the end concatenate all these characters histograms an for example L2 normalize them and this yields are representation, a fixed length representation for a variable length sequence."
        ],
        [
            "So now we had embeddings of input and output spaces.",
            "Now we have to define how to compute the similarity between those and we have several options here.",
            "So one option is we could optimize our ranking criterion.",
            "Like the one shown here, for example, we can say that the similarity between an image XN and its corresponding label or its ground truth label YN should be higher than the similarity between the image and any other label.",
            "OK, so this is the type of three plus ranking we want to impose, and in the paper we show the details of how to formulate this in a structure SVM."
        ],
        [
            "Framework where we have our risk here.",
            "With the typical 01 loss, but we bound this risk with with this bound.",
            "Here the details are in the paper and basically by doing so we managed to encode this type of criterion in terms of ranking triplets and this can be easily optimized using stochastic gradient descent.",
            "So this is a ranking criteria."
        ],
        [
            "Another option is to use a reconstruction criterion.",
            "So since we are projecting the images into the space of label, we could actually measure how big is the reconstruction error, because ideally we would like the projection to fall exactly on the same point as the label embedding.",
            "And then we could minimize the square sum of errors, for example, which would lead to our rich regression formulation, which has a closed form solution.",
            "And in experiments we will use these two.",
            "These two options and we will see."
        ],
        [
            "Sorry, so the third part of the talk is actually the experiment."
        ],
        [
            "The first experiment is performed on a license plate recognition task.",
            "We have a database of 45,000 US license plate images.",
            "You can perceive license plate recognition as an easy task, but if you think it is easy then you should go to the United States where the plates contain graphical background symbols and each state has many different plate types allowed which define different templates with different numbering system.",
            "So license plate recognition is not trivial at all in the US.",
            "Unfortunately I cannot show real images of this database because this is private data and the presentation is being recorded.",
            "But I have put here some Internet images just to give an illustration.",
            "Here these are the parameters of the sprinkle settings for this database.",
            "For this particular case, we use the structure at SVM framework."
        ],
        [
            "We see some results here we compare to the baselines.",
            "First commercial license plate recognition system that is in the market and which uses a bottom up approach so it detects tries to detect the characters.",
            "2nd, we also compared to a holistic approach that uses nearest neighbor match and that obtained promising results in the same database.",
            "Here we show the accuracy versus reject characteristic.",
            "Our system obtains this blue curve here, which is significantly better than the one on the commercial license plate recognition system.",
            "As we set the bottom up approach is dependent on a lot of the errors made by the character detectors and it turns out that this character detector fails a lot in graphical symbols.",
            "But in our case, since we are not attempting to recognize characters, we don't have that.",
            "That limitation also if we compare to the nearest neighbor approach where you can see that it obtains a very high position, but only at a very high reject rate.",
            "Why is that?",
            "Because this is very natural in the test set, there are only a few labels that had been seen in the training set, so the system at best can only recognize this labels and the rest it needs to be rejected.",
            "Here on the right we do a fair comparison with the nearest neighbor by evaluating only on the scene labels and.",
            "Still we have an edge on the with respect to the nearest neighbor, an we attribute that to the fact that we do this learning jointly for labels and there is information that is shared between labels, which is not the case in the nearest neighbor approach.",
            "So we have a small advantage here."
        ],
        [
            "A second type of experiments is run on scene text recognition.",
            "We use the triple it set, which is a reason database or 5000 word images from street scenes.",
            "In that case, the structured learning doesn't work that well, and we had to resort to rich regression.",
            "The results are shown here in the table.",
            "We compared to the work of his right Al who provided a baseline for this database and we evaluate 2 cases.",
            "One where we have a lexicon of 50 words and one where we where we have actually got 1000 words in the 1000 words case.",
            "We have a comparable accuracy to the bottom up approach of miserable.",
            "But when we go to a small lexicon, we have a very significant improvement.",
            "We"
        ],
        [
            "And analyze some correctly recognized work we can see here that despite the fact that we are not attempting at detecting characters, we still do quite well in challenging cases.",
            "In it's also interesting to see."
        ],
        [
            "Some failure cases we have, sometimes confusions with similar words."
        ],
        [
            "Something which is very particular to our approach is that sometimes work words get confused with other labels that share engrams.",
            "So if you have a strong engram like a 3 gram or a four gram plus some other common characters, this can easily lead to errors.",
            "Like in this case, services and serious or States and St."
        ],
        [
            "And we also observe more errors than we want it in numbers.",
            "This is, I think, because the space of numbers is less sparse on the space of words, and there is not so much constrained by a language model, the numbers.",
            "And because our method is holistic and it does as it learns, a single projection, it could just be that you know.",
            "Sometimes projections are just bad and the result you get has nothing to do with the original image."
        ],
        [
            "So conclusions the contribution of this work is a label embedding framework for text recognition.",
            "We highlight that we don't need to localize and classify individual characters.",
            "Recognition just proceeds as a linear search that can be expressed as a cosine similarity.",
            "I have not mentioned that by the time of searches of the order of magnitude of 100 milliseconds and we obtained the O shot learning property.",
            "That means once we have learned our model.",
            "We can still check if an image matches with labels that were not present in the training set.",
            "As future work, I think this is a promising direction for tax recognition, but we would like to scale to large lexicons.",
            "To that end, we will leverage abundant literature and efficient large image retrieval, such as some of the words that have been presented during the keynote speech on product quantization or hashing."
        ],
        [
            "Thank you very much for your attention and I'm open for questions now.",
            "You said that you are using a structured SVM for yes for recognition, but I I didn't get very this structure is sitting so to say sorry I didn't see the structure.",
            "OK, could you explain?",
            "Yes yes.",
            "So."
        ],
        [
            "So for the time constraints I have not gone into the details.",
            "But here, so in the structure SVM usually you have here loss plus the regularizer.",
            "So we choose the 01 loss which means we want to penalize when the predicted.",
            "Label doesn't match the the.",
            "You know the ground truth label and if we call that L then in the paper we give details on how to bound this L with such a bound and.",
            "It's in the end, it's it's just about checking whether the triplet, albeit this ordering or not OK, and the structure you can introduce it in this variable here, which in our case is still 01 loss as which is the most basic structure.",
            "But you could add other structures.",
            "For example you could add Levenshtein distance between words in order to.",
            "Favor that, given an image, the result should be, you know, result 2.",
            "Labels with similar Levenshtein distance should not be too close.",
            "With respect to the score.",
            "Have you with the fully projected text?",
            "Have you tried any rectification?",
            "Is that a trivial task or is it more complicated?",
            "It looks like early not since I mean we follow the.",
            "Classical philosophy of the Fisher vectors which."
        ],
        [
            "Is we do not do any preprocessing today image we just extract.",
            "This representation, which has shown some success in the past and we will trust this representation to to deal all the deformations or distortions that you could have their line of future work, and especially when I mentioned we want to extend to larger scale.",
            "To large scale lexicons, so one challenge is that I think if you will use a large lexicons then you need a lot of training data.",
            "Otherwise the problem is not consistent and to get more training data, we could follow approaches like.",
            "Distorts synthetically the training data to obtain more samples in the same way that is being done now in deep learning, for example.",
            "Which is the opposite to what you are asking.",
            "Instead of rectifying the images I think I would prefer to apply distortions to it's raining images and then train a system, but we haven't tried that yet.",
            "OK, now this is the end of this session.",
            "Thanks, OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Jose Rodriguez.",
                    "label": 0
                },
                {
                    "sent": "I'm a permanent researcher at Xerox.",
                    "label": 0
                },
                {
                    "sent": "The title of this dog is level embedding for text recognition and this is joint work with Logan Paramel.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This presentation is structured in three parts, motivation, label embedding, model, an experiment, some conclusions and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start with the motivation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem of interest in this work is text recognition and natural images.",
                    "label": 1
                },
                {
                    "sent": "This is illustrated with images of recent scene text recognition data set.",
                    "label": 0
                },
                {
                    "sent": "As you can appreciate the difficulty of doing text recognition in this type of images is significantly higher than performing text recognition in clean documents.",
                    "label": 1
                },
                {
                    "sent": "This is due to the variety of colors, fonts, backgrounds and character configurations.",
                    "label": 0
                },
                {
                    "sent": "The applications of such an approach would be automatic navigation, automatic transcription, or assistance.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As in previous works, the goal we will frame our work in Word recognition.",
                    "label": 1
                },
                {
                    "sent": "This is the problem where the input is a corrupt image of our work and the output is the sequence of characters that are present in that work.",
                    "label": 1
                },
                {
                    "sent": "We have to define.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also the lexicon, which is the list of valid words and this is application dependent, so there're applications like word spotting, where lexicons can be very small and there is applications like recognition where lexicon could be open.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we in general detect text in images to the classical strategy?",
                    "label": 0
                },
                {
                    "sent": "Follows a bottom up approach as illustrated here.",
                    "label": 0
                },
                {
                    "sent": "We have a word image and we will run a character detector, for example based on Hog features.",
                    "label": 0
                },
                {
                    "sent": "Two propose some character hypothesis.",
                    "label": 0
                },
                {
                    "sent": "This character hypothesis leads to some plausible world hypothesis, and then there is a high level model such as a CRF that will select what is the most likely hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The advantage of such an approach?",
                    "label": 0
                },
                {
                    "sent": "Is that it's scalable to large lexicons?",
                    "label": 1
                },
                {
                    "sent": "It enables recognizing any word.",
                    "label": 0
                },
                {
                    "sent": "As long as you can compose it from the individual characters of your alphabet.",
                    "label": 0
                },
                {
                    "sent": "But the downside is that it relies on a character detector, so it's sensitive to the errors made by the character detector, and usually to build a robust character detector, we will perform very heavy preprocessing and postprocessing.",
                    "label": 1
                },
                {
                    "sent": "As opposed.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That type of approaches we could ask ourselves, is it possible to not try to detect characters in the image and this is what some works have been doing, which are known as holistic approaches.",
                    "label": 0
                },
                {
                    "sent": "So here the image will be represented as a single feature vector and then we will do a nearest neighbor match with a set of templates from a database.",
                    "label": 0
                },
                {
                    "sent": "And then this approach is simple and efficient.",
                    "label": 0
                },
                {
                    "sent": "Some works have been shown that this is feasible for small, but also for not so small lexicons, so they can go up off to several thousands of words.",
                    "label": 0
                },
                {
                    "sent": "The main drawback, however, is that in order to be able to recognize one word, you have to have seen at least one annotated example, so we don't have the O shot learning property.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In our work, we still want to perform text recognition without having to detect the characters.",
                    "label": 0
                },
                {
                    "sent": "And that means we will extract a single feature vector per image we want to compute the match between an image Anna label using a simple function, But we want to preserve the sure shot learning property.",
                    "label": 1
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rest of the presentation will describe how this is achieved.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we show a schematic representation of our approach.",
                    "label": 0
                },
                {
                    "sent": "On the one side, we have an image.",
                    "label": 0
                },
                {
                    "sent": "The pixel values of that image are denoted as X.",
                    "label": 0
                },
                {
                    "sent": "We apply an operation to that image that we called image embedding, which results in a vector that we call Theta.",
                    "label": 0
                },
                {
                    "sent": "Sorry yeah fit of X, which is nothing more than the feature extraction.",
                    "label": 0
                },
                {
                    "sent": "But we call it in this way because on the other side which read the label in exactly the same way, so a label is a sequence of characters that within out why and we apply an operation that in this case is called level embedding that produces a vector called Phi of Y. OK, and what we would like ideally is to be able to compare this set of X with this fire Y.",
                    "label": 0
                },
                {
                    "sent": "So one way to do that is to bring this set of X in the space of the five Y using a suitable projection with a matrix W transpose.",
                    "label": 0
                },
                {
                    "sent": "Now here.",
                    "label": 0
                },
                {
                    "sent": "If we manage to express these two, both images and labels in the same space, we can compute the match between an image and label with a simple similarity, fun.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, we could use this bilinear form here and then the recognition of an image.",
                    "label": 0
                },
                {
                    "sent": "So recognition of the image represented by this point just amounts to finding the closest label.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a simple approach that is quite different from what has been done before in text recognition.",
                    "label": 0
                },
                {
                    "sent": "Here there are two key points.",
                    "label": 0
                },
                {
                    "sent": "First, that we do not only embed the features as in most of the approaches, but we also embed the labels.",
                    "label": 0
                },
                {
                    "sent": "So we embed the input and output space and.",
                    "label": 1
                },
                {
                    "sent": "We will have to define how to learn a suitable projection matrix, so in.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summary to materialize this framework, we need three things.",
                    "label": 0
                },
                {
                    "sent": "One defined image embedding, second defined label embedding and 3rd define how to learn.",
                    "label": 1
                },
                {
                    "sent": "Sorry the projection matrix.",
                    "label": 0
                },
                {
                    "sent": "In fact, what I have not mentioned is the very fact that we do this label embedding is what will enable the zero shot learning, because once the model is learned you can project any new label into that space and the method will work.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for image features, this is the first point we have to define.",
                    "label": 0
                },
                {
                    "sent": "Well, this is simple.",
                    "label": 0
                },
                {
                    "sent": "We are computer vision people.",
                    "label": 0
                },
                {
                    "sent": "We know many ways of coding pixels into vectors, so very simply I will just use the feature vector because it obtains state of the art results in image retrieval, Ann, we know this task resembles retrieval alot.",
                    "label": 1
                },
                {
                    "sent": "So question one was easy to solve.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Question two, we have to define how to embed a sequence of characters into a vector space.",
                    "label": 1
                },
                {
                    "sent": "To that end, we propose what we call the special pyramid of characters.",
                    "label": 1
                },
                {
                    "sent": "This is basically taking the sequence of characters and doing a bag of characters with a linear special pyramid.",
                    "label": 0
                },
                {
                    "sent": "OK to illustrate it with particular example, imagine we have the sequence of characters ABC DE OK.",
                    "label": 0
                },
                {
                    "sent": "This is a sequence of characters, not an image.",
                    "label": 0
                },
                {
                    "sent": "OK, this is important to mention.",
                    "label": 0
                },
                {
                    "sent": "Now what we will do is so also imagine the alphabet is only consists of these 5 characters.",
                    "label": 0
                },
                {
                    "sent": "For this simple example.",
                    "label": 0
                },
                {
                    "sent": "So what we will do is in a first level we just count how many times each character appears.",
                    "label": 0
                },
                {
                    "sent": "So in each case each character counts only once to the histogram.",
                    "label": 0
                },
                {
                    "sent": "OK, so in a second level we split the sequence into two parts and we count the number of characters in each part.",
                    "label": 0
                },
                {
                    "sent": "So in the first part we have a B and half of a C. OK because she falls into the two parts.",
                    "label": 0
                },
                {
                    "sent": "And in the second half we have half a CDN and so we could go down any number of levels we wanted an we would in the end concatenate all these characters histograms an for example L2 normalize them and this yields are representation, a fixed length representation for a variable length sequence.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we had embeddings of input and output spaces.",
                    "label": 0
                },
                {
                    "sent": "Now we have to define how to compute the similarity between those and we have several options here.",
                    "label": 0
                },
                {
                    "sent": "So one option is we could optimize our ranking criterion.",
                    "label": 1
                },
                {
                    "sent": "Like the one shown here, for example, we can say that the similarity between an image XN and its corresponding label or its ground truth label YN should be higher than the similarity between the image and any other label.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is the type of three plus ranking we want to impose, and in the paper we show the details of how to formulate this in a structure SVM.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Framework where we have our risk here.",
                    "label": 0
                },
                {
                    "sent": "With the typical 01 loss, but we bound this risk with with this bound.",
                    "label": 0
                },
                {
                    "sent": "Here the details are in the paper and basically by doing so we managed to encode this type of criterion in terms of ranking triplets and this can be easily optimized using stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So this is a ranking criteria.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another option is to use a reconstruction criterion.",
                    "label": 0
                },
                {
                    "sent": "So since we are projecting the images into the space of label, we could actually measure how big is the reconstruction error, because ideally we would like the projection to fall exactly on the same point as the label embedding.",
                    "label": 0
                },
                {
                    "sent": "And then we could minimize the square sum of errors, for example, which would lead to our rich regression formulation, which has a closed form solution.",
                    "label": 0
                },
                {
                    "sent": "And in experiments we will use these two.",
                    "label": 0
                },
                {
                    "sent": "These two options and we will see.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, so the third part of the talk is actually the experiment.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first experiment is performed on a license plate recognition task.",
                    "label": 0
                },
                {
                    "sent": "We have a database of 45,000 US license plate images.",
                    "label": 1
                },
                {
                    "sent": "You can perceive license plate recognition as an easy task, but if you think it is easy then you should go to the United States where the plates contain graphical background symbols and each state has many different plate types allowed which define different templates with different numbering system.",
                    "label": 1
                },
                {
                    "sent": "So license plate recognition is not trivial at all in the US.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately I cannot show real images of this database because this is private data and the presentation is being recorded.",
                    "label": 0
                },
                {
                    "sent": "But I have put here some Internet images just to give an illustration.",
                    "label": 0
                },
                {
                    "sent": "Here these are the parameters of the sprinkle settings for this database.",
                    "label": 0
                },
                {
                    "sent": "For this particular case, we use the structure at SVM framework.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We see some results here we compare to the baselines.",
                    "label": 0
                },
                {
                    "sent": "First commercial license plate recognition system that is in the market and which uses a bottom up approach so it detects tries to detect the characters.",
                    "label": 0
                },
                {
                    "sent": "2nd, we also compared to a holistic approach that uses nearest neighbor match and that obtained promising results in the same database.",
                    "label": 1
                },
                {
                    "sent": "Here we show the accuracy versus reject characteristic.",
                    "label": 0
                },
                {
                    "sent": "Our system obtains this blue curve here, which is significantly better than the one on the commercial license plate recognition system.",
                    "label": 1
                },
                {
                    "sent": "As we set the bottom up approach is dependent on a lot of the errors made by the character detectors and it turns out that this character detector fails a lot in graphical symbols.",
                    "label": 0
                },
                {
                    "sent": "But in our case, since we are not attempting to recognize characters, we don't have that.",
                    "label": 0
                },
                {
                    "sent": "That limitation also if we compare to the nearest neighbor approach where you can see that it obtains a very high position, but only at a very high reject rate.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Because this is very natural in the test set, there are only a few labels that had been seen in the training set, so the system at best can only recognize this labels and the rest it needs to be rejected.",
                    "label": 0
                },
                {
                    "sent": "Here on the right we do a fair comparison with the nearest neighbor by evaluating only on the scene labels and.",
                    "label": 0
                },
                {
                    "sent": "Still we have an edge on the with respect to the nearest neighbor, an we attribute that to the fact that we do this learning jointly for labels and there is information that is shared between labels, which is not the case in the nearest neighbor approach.",
                    "label": 0
                },
                {
                    "sent": "So we have a small advantage here.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A second type of experiments is run on scene text recognition.",
                    "label": 1
                },
                {
                    "sent": "We use the triple it set, which is a reason database or 5000 word images from street scenes.",
                    "label": 1
                },
                {
                    "sent": "In that case, the structured learning doesn't work that well, and we had to resort to rich regression.",
                    "label": 0
                },
                {
                    "sent": "The results are shown here in the table.",
                    "label": 0
                },
                {
                    "sent": "We compared to the work of his right Al who provided a baseline for this database and we evaluate 2 cases.",
                    "label": 0
                },
                {
                    "sent": "One where we have a lexicon of 50 words and one where we where we have actually got 1000 words in the 1000 words case.",
                    "label": 0
                },
                {
                    "sent": "We have a comparable accuracy to the bottom up approach of miserable.",
                    "label": 0
                },
                {
                    "sent": "But when we go to a small lexicon, we have a very significant improvement.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And analyze some correctly recognized work we can see here that despite the fact that we are not attempting at detecting characters, we still do quite well in challenging cases.",
                    "label": 0
                },
                {
                    "sent": "In it's also interesting to see.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some failure cases we have, sometimes confusions with similar words.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something which is very particular to our approach is that sometimes work words get confused with other labels that share engrams.",
                    "label": 0
                },
                {
                    "sent": "So if you have a strong engram like a 3 gram or a four gram plus some other common characters, this can easily lead to errors.",
                    "label": 0
                },
                {
                    "sent": "Like in this case, services and serious or States and St.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also observe more errors than we want it in numbers.",
                    "label": 0
                },
                {
                    "sent": "This is, I think, because the space of numbers is less sparse on the space of words, and there is not so much constrained by a language model, the numbers.",
                    "label": 0
                },
                {
                    "sent": "And because our method is holistic and it does as it learns, a single projection, it could just be that you know.",
                    "label": 0
                },
                {
                    "sent": "Sometimes projections are just bad and the result you get has nothing to do with the original image.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conclusions the contribution of this work is a label embedding framework for text recognition.",
                    "label": 1
                },
                {
                    "sent": "We highlight that we don't need to localize and classify individual characters.",
                    "label": 0
                },
                {
                    "sent": "Recognition just proceeds as a linear search that can be expressed as a cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "I have not mentioned that by the time of searches of the order of magnitude of 100 milliseconds and we obtained the O shot learning property.",
                    "label": 0
                },
                {
                    "sent": "That means once we have learned our model.",
                    "label": 0
                },
                {
                    "sent": "We can still check if an image matches with labels that were not present in the training set.",
                    "label": 1
                },
                {
                    "sent": "As future work, I think this is a promising direction for tax recognition, but we would like to scale to large lexicons.",
                    "label": 0
                },
                {
                    "sent": "To that end, we will leverage abundant literature and efficient large image retrieval, such as some of the words that have been presented during the keynote speech on product quantization or hashing.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much for your attention and I'm open for questions now.",
                    "label": 0
                },
                {
                    "sent": "You said that you are using a structured SVM for yes for recognition, but I I didn't get very this structure is sitting so to say sorry I didn't see the structure.",
                    "label": 0
                },
                {
                    "sent": "OK, could you explain?",
                    "label": 0
                },
                {
                    "sent": "Yes yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the time constraints I have not gone into the details.",
                    "label": 0
                },
                {
                    "sent": "But here, so in the structure SVM usually you have here loss plus the regularizer.",
                    "label": 0
                },
                {
                    "sent": "So we choose the 01 loss which means we want to penalize when the predicted.",
                    "label": 0
                },
                {
                    "sent": "Label doesn't match the the.",
                    "label": 0
                },
                {
                    "sent": "You know the ground truth label and if we call that L then in the paper we give details on how to bound this L with such a bound and.",
                    "label": 0
                },
                {
                    "sent": "It's in the end, it's it's just about checking whether the triplet, albeit this ordering or not OK, and the structure you can introduce it in this variable here, which in our case is still 01 loss as which is the most basic structure.",
                    "label": 0
                },
                {
                    "sent": "But you could add other structures.",
                    "label": 0
                },
                {
                    "sent": "For example you could add Levenshtein distance between words in order to.",
                    "label": 0
                },
                {
                    "sent": "Favor that, given an image, the result should be, you know, result 2.",
                    "label": 0
                },
                {
                    "sent": "Labels with similar Levenshtein distance should not be too close.",
                    "label": 0
                },
                {
                    "sent": "With respect to the score.",
                    "label": 0
                },
                {
                    "sent": "Have you with the fully projected text?",
                    "label": 0
                },
                {
                    "sent": "Have you tried any rectification?",
                    "label": 0
                },
                {
                    "sent": "Is that a trivial task or is it more complicated?",
                    "label": 0
                },
                {
                    "sent": "It looks like early not since I mean we follow the.",
                    "label": 0
                },
                {
                    "sent": "Classical philosophy of the Fisher vectors which.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is we do not do any preprocessing today image we just extract.",
                    "label": 0
                },
                {
                    "sent": "This representation, which has shown some success in the past and we will trust this representation to to deal all the deformations or distortions that you could have their line of future work, and especially when I mentioned we want to extend to larger scale.",
                    "label": 0
                },
                {
                    "sent": "To large scale lexicons, so one challenge is that I think if you will use a large lexicons then you need a lot of training data.",
                    "label": 0
                },
                {
                    "sent": "Otherwise the problem is not consistent and to get more training data, we could follow approaches like.",
                    "label": 0
                },
                {
                    "sent": "Distorts synthetically the training data to obtain more samples in the same way that is being done now in deep learning, for example.",
                    "label": 0
                },
                {
                    "sent": "Which is the opposite to what you are asking.",
                    "label": 0
                },
                {
                    "sent": "Instead of rectifying the images I think I would prefer to apply distortions to it's raining images and then train a system, but we haven't tried that yet.",
                    "label": 0
                },
                {
                    "sent": "OK, now this is the end of this session.",
                    "label": 0
                },
                {
                    "sent": "Thanks, OK.",
                    "label": 0
                }
            ]
        }
    }
}