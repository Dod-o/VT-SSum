{
    "id": "43sqjkcpwznljyjzzujdy3egsfxjv5ep",
    "title": "libDAI",
    "info": {
        "author": [
            "Joris Mooij, Radboud University Nijmegen"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/mloss08_mooij_libdai/",
    "segmentation": [
        [
            "OK, so well actually my name is pronounced Euless Moy but so.",
            "And so left Eye is is a library about that implements various methods for doing approximate inference in graphical models and.",
            "It was actually a byproduct of my PhD research and I thought maybe other people can can profit from this work.",
            "So I decided to make it open source.",
            "So the D in Lip DI stands for discretes.",
            "So basically all the algorithms assume that the variables in the factor graph are all discrete."
        ],
        [
            "OK, so graphical models are often you have different kinds of graphical models and some communities use prefer to use a Bayesian networks.",
            "Others use Markov random fields."
        ],
        [
            "So Bayesian networks are also called directed random graphs, sorry, directed graphical models and Markov random fields are undirected.",
            "Gray"
        ],
        [
            "Local models, but from the point of view of doing inference, the distinction between these two classes of graphical models.",
            "This I think it's not so important, and if you want to do inference then it's I think more natural to look at the slide generalize generalization of of Markov random fields actually, and that is the class of vector graphs.",
            "So basically if you have for those of you who don't know this, if you have a probability distribution on a set of random variables.",
            "So the set X which which.",
            "Which are different random variables which are assumed to be discreet.",
            "Here an if you can write this probability distribution as as a product of factors.",
            "So these price over there and each factor is actually a non negative function of hopefully a small subset of the variables.",
            "Then this is basically the.",
            "What would you call a factor graph or actually the graph itself?",
            "You can get it by.",
            "It's a bipartite graph which has variable notes, Inspector notes, and there's a connection between effector and a variable.",
            "If the factor depends on that variable, so it's.",
            "Destruction to the right.",
            "So that's basically the clouds affected graphs, and you can already see that both Bayesian networks and Markov random fields are trivially factor graphs."
        ],
        [
            "OK, so in this context so.",
            "So lip tie you can you give it to a factor graph.",
            "You specify the factors and then it can do different things for you.",
            "So interesting basic approximate inference tasks that are relevant are first of all to calculate the partition sum.",
            "So there's this normalizing constant C in there, and usually if you specify the factor graphs, you don't actually specify the value of Z that is necessary to make it a probability distribution.",
            "So."
        ],
        [
            "One of the task is to calculate this partition.",
            "Some another task is to calculate the marginal distribution of some.",
            "Subsets."
        ],
        [
            "Some variables and also what is also important is to calculate the map state so joint configuration of the very."
        ],
        [
            "That has maximum probability.",
            "OK, so now what kind of algorithms are currently implemented to solve these tasks?",
            "And these are the following algorithms and so we have also some exact inference methods.",
            "So first of all by brute force enumeration, which is not very sophisticated but.",
            "For comparison, and then there is a junction tree methods.",
            "Then there and the other methods are all approximate methods, so there is mean field for factor graphs.",
            "There's loopy belief propagation.",
            "There's three expectation propagation by Minka Anki.",
            "An generalized belief propagation is in there, which was proposed by UDF, Freeman and voice, and there is a double loop version of that algorithm which has guaranteed convergence, biasca, salverson Cup, and there are also methods for doing loop corrected belief propagation, so.",
            "Belief propagation gives you an approximation, and you can try to make it better by correcting for the influence of loops in your factor graph, and there are some methods by myself and get a Cup and an also related methods by Montanari and Rita.",
            "And there is a simple Gibbs sampler."
        ],
        [
            "And there are I plan to add some more methods which are currently in the pipeline but not yet there.",
            "So for the next release I hope to add iterative join graph propagation, tree related approximations and bounds and some methods for bounding marginals.",
            "So the first one called box propagation, we proposed it to this conference, so you may may have seen the poster.",
            "Then there's related work by Tyler.",
            "And also."
        ],
        [
            "It's called bound propagation.",
            "Now some key features of lip tie that it's obviously it's.",
            "It's free and open source, otherwise I would not be here and it's GPL.",
            "The license is GPL, is written in C++, so compared to for example Matlab, which would be a more obvious choice.",
            "Maybe if you want to maximize the audience of a piece of software, but you can get easily get a factor of 1000 or 10,000 speed up for this kind of methods.",
            "If you just write it in C++ instead of in Matlab.",
            "So there is a.",
            "You don't write if you don't know C++.",
            "You can use the Command line interface that is provided by lip tie or the sort of rudimentary Matlab interface to use the existing methods and use them on your problems on your effective graphs.",
            "So I tried to give it a modular design so that it is easy to extend it and to add new algorithms or to use to use the basic building blocks.",
            "The documentation is not as complete as I would like it to be, but its target for the next release that everything will become become fully documented.",
            "I use doxygen for the documentation and why it compiles under Linux and under."
        ],
        [
            "Windows out of the books.",
            "So what is the target audience?",
            "The target audience is basically researchers that have a good understanding of graphical models and of inference in graphical models.",
            "And optimally if you want to get the most out of lip, tie it, you need a good knowledge of C++, but you can also, as I said, use the other interfaces.",
            "And well, if you can just use the existing algorithms to if you have a new algorithm, for example, use these existing algorithms to compare your method with it.",
            "Or ideally you could implement a new method in lip tie and contribute it so that the collection of methods will grow so things that are not in liptai are.",
            "The following things.",
            "There are no, there's no learning algorithms in it, so no parameter learning the structure learning there is limited file format supports, there's no GUI, no visualization.",
            "So basically it's just an inference."
        ],
        [
            "Engine, so it's really a library.",
            "OK, so we're at the MLOS website you can download.",
            "At Liptai there, but you can also go to the public Git tree which has the newer."
        ],
        [
            "Code.",
            "So OK related software.",
            "So if I select it all the open source software which was able to deal with both directors and undirected graphs and I found.",
            "Basically I found these two other packages, the Bayes net toolbox written by Kevin Murphy which is written in mainly in Matlab and the Probabilistic Networks library from Intel.",
            "And well, if you look here, you see that there is some overlap between those three packages, and there are some algorithms which are only in lip Diane.",
            "There are some features which are in the other packages, but which are not elliptic."
        ],
        [
            "So.",
            "OK, so the shorts demo.",
            "How is the time?",
            "OK. An so OK so so I do a quick demo of the Command line interface, so I used the alarm network which is a well known Bayesian network.",
            "And it is.",
            "So this is how the file format looks like, so we can even human readable a bit.",
            "OK, so there's this this command line interface, so this program that you can use to quickly compare different methods on a given vector graph, say so it has some arguments.",
            "And well, so you can hear you specify the file name and then you specify a bunch of methods and the first method is to run that that the others will be compared against.",
            "So this is.",
            "In this case I choose a junction tree so that we can so that we get the exact results and the other methods are approximate methods so we can compare how good the approximate results are.",
            "And so, for example, here we have to be P versions with different update schedules.",
            "And here is 3 P and mean fields and generalized BP methods, and so in this file aliases dot conf you have the default parameters.",
            "So basically but yeah, so they're defined there.",
            "You can edit that, but you can also on the command line change the parameters.",
            "Of the methods for by example for setting the maximum number of iterations, and this is not.",
            "This is.",
            "Well, maybe then I have another resolution.",
            "OK, so here you see the results an.",
            "Basically.",
            "You see, the time that the algorithms needed the number of iterations stated.",
            "The maximum error in the marginals of the variables so.",
            "And the average error.",
            "In the marginals and here is the error in the log partition sum.",
            "And here you see basically whether it has converged or not, so you see that the last methods, generalized belief propagation, has not converged because it didn't have enough iterations, so.",
            "We can take another parameter and so basically this this office here quick methods to to compare various methods on various models and if you.",
            "If you would like to know that the marginals then.",
            "You can also get the details marginals and well yeah, so basically this is the current command line interface an if you can do all kinds of other things.",
            "Of course if you just edit this or write your own C++ code to call the library.",
            "So yeah, that's it."
        ],
        [
            "Thanks for your attention.",
            "On what data sizes?",
            "If you tested your algorithm because?"
        ],
        [
            "Obviously, knowing that it would be having problem data size.",
            "So this heavily OK.",
            "So which algorithm you can use for a large vector graph?",
            "1000 No yeah, so 10,000 notes is sort of routine, so for some algorithms not for all but.",
            "And if you look after so, so I know that that it's used for example in computer vision by some people.",
            "And I myself.",
            "I usually play around with quite small networks, but sometimes I always try some larger ones.",
            "Lately.",
            "Because that's one of the issues.",
            "Floating point is finite.",
            "So usually this is not really such a large problem, but if it is then for example.",
            "Anne.",
            "So here you see some some of the default settings.",
            "So for example, if you want to do belief propagation then you can do the updates in the log domain or in the real domain and in the lock domain it will have higher stability and there is a parameter such as.",
            "You can do it in the log domain and ideally.",
            "You could do this, so currently you can only do this for belief propagation, but an I'm doing some want to extend it a bit so that you have a factor which works in the log domain and that it just works too transparently for all the algorithms.",
            "Speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so well actually my name is pronounced Euless Moy but so.",
                    "label": 0
                },
                {
                    "sent": "And so left Eye is is a library about that implements various methods for doing approximate inference in graphical models and.",
                    "label": 0
                },
                {
                    "sent": "It was actually a byproduct of my PhD research and I thought maybe other people can can profit from this work.",
                    "label": 0
                },
                {
                    "sent": "So I decided to make it open source.",
                    "label": 0
                },
                {
                    "sent": "So the D in Lip DI stands for discretes.",
                    "label": 0
                },
                {
                    "sent": "So basically all the algorithms assume that the variables in the factor graph are all discrete.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so graphical models are often you have different kinds of graphical models and some communities use prefer to use a Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "Others use Markov random fields.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Bayesian networks are also called directed random graphs, sorry, directed graphical models and Markov random fields are undirected.",
                    "label": 0
                },
                {
                    "sent": "Gray",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Local models, but from the point of view of doing inference, the distinction between these two classes of graphical models.",
                    "label": 0
                },
                {
                    "sent": "This I think it's not so important, and if you want to do inference then it's I think more natural to look at the slide generalize generalization of of Markov random fields actually, and that is the class of vector graphs.",
                    "label": 0
                },
                {
                    "sent": "So basically if you have for those of you who don't know this, if you have a probability distribution on a set of random variables.",
                    "label": 0
                },
                {
                    "sent": "So the set X which which.",
                    "label": 0
                },
                {
                    "sent": "Which are different random variables which are assumed to be discreet.",
                    "label": 0
                },
                {
                    "sent": "Here an if you can write this probability distribution as as a product of factors.",
                    "label": 0
                },
                {
                    "sent": "So these price over there and each factor is actually a non negative function of hopefully a small subset of the variables.",
                    "label": 0
                },
                {
                    "sent": "Then this is basically the.",
                    "label": 0
                },
                {
                    "sent": "What would you call a factor graph or actually the graph itself?",
                    "label": 0
                },
                {
                    "sent": "You can get it by.",
                    "label": 0
                },
                {
                    "sent": "It's a bipartite graph which has variable notes, Inspector notes, and there's a connection between effector and a variable.",
                    "label": 0
                },
                {
                    "sent": "If the factor depends on that variable, so it's.",
                    "label": 0
                },
                {
                    "sent": "Destruction to the right.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the clouds affected graphs, and you can already see that both Bayesian networks and Markov random fields are trivially factor graphs.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in this context so.",
                    "label": 0
                },
                {
                    "sent": "So lip tie you can you give it to a factor graph.",
                    "label": 1
                },
                {
                    "sent": "You specify the factors and then it can do different things for you.",
                    "label": 0
                },
                {
                    "sent": "So interesting basic approximate inference tasks that are relevant are first of all to calculate the partition sum.",
                    "label": 1
                },
                {
                    "sent": "So there's this normalizing constant C in there, and usually if you specify the factor graphs, you don't actually specify the value of Z that is necessary to make it a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the task is to calculate this partition.",
                    "label": 0
                },
                {
                    "sent": "Some another task is to calculate the marginal distribution of some.",
                    "label": 1
                },
                {
                    "sent": "Subsets.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some variables and also what is also important is to calculate the map state so joint configuration of the very.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That has maximum probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what kind of algorithms are currently implemented to solve these tasks?",
                    "label": 1
                },
                {
                    "sent": "And these are the following algorithms and so we have also some exact inference methods.",
                    "label": 1
                },
                {
                    "sent": "So first of all by brute force enumeration, which is not very sophisticated but.",
                    "label": 1
                },
                {
                    "sent": "For comparison, and then there is a junction tree methods.",
                    "label": 0
                },
                {
                    "sent": "Then there and the other methods are all approximate methods, so there is mean field for factor graphs.",
                    "label": 0
                },
                {
                    "sent": "There's loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "There's three expectation propagation by Minka Anki.",
                    "label": 0
                },
                {
                    "sent": "An generalized belief propagation is in there, which was proposed by UDF, Freeman and voice, and there is a double loop version of that algorithm which has guaranteed convergence, biasca, salverson Cup, and there are also methods for doing loop corrected belief propagation, so.",
                    "label": 1
                },
                {
                    "sent": "Belief propagation gives you an approximation, and you can try to make it better by correcting for the influence of loops in your factor graph, and there are some methods by myself and get a Cup and an also related methods by Montanari and Rita.",
                    "label": 0
                },
                {
                    "sent": "And there is a simple Gibbs sampler.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are I plan to add some more methods which are currently in the pipeline but not yet there.",
                    "label": 0
                },
                {
                    "sent": "So for the next release I hope to add iterative join graph propagation, tree related approximations and bounds and some methods for bounding marginals.",
                    "label": 1
                },
                {
                    "sent": "So the first one called box propagation, we proposed it to this conference, so you may may have seen the poster.",
                    "label": 0
                },
                {
                    "sent": "Then there's related work by Tyler.",
                    "label": 0
                },
                {
                    "sent": "And also.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's called bound propagation.",
                    "label": 0
                },
                {
                    "sent": "Now some key features of lip tie that it's obviously it's.",
                    "label": 0
                },
                {
                    "sent": "It's free and open source, otherwise I would not be here and it's GPL.",
                    "label": 0
                },
                {
                    "sent": "The license is GPL, is written in C++, so compared to for example Matlab, which would be a more obvious choice.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you want to maximize the audience of a piece of software, but you can get easily get a factor of 1000 or 10,000 speed up for this kind of methods.",
                    "label": 0
                },
                {
                    "sent": "If you just write it in C++ instead of in Matlab.",
                    "label": 0
                },
                {
                    "sent": "So there is a.",
                    "label": 0
                },
                {
                    "sent": "You don't write if you don't know C++.",
                    "label": 0
                },
                {
                    "sent": "You can use the Command line interface that is provided by lip tie or the sort of rudimentary Matlab interface to use the existing methods and use them on your problems on your effective graphs.",
                    "label": 1
                },
                {
                    "sent": "So I tried to give it a modular design so that it is easy to extend it and to add new algorithms or to use to use the basic building blocks.",
                    "label": 0
                },
                {
                    "sent": "The documentation is not as complete as I would like it to be, but its target for the next release that everything will become become fully documented.",
                    "label": 0
                },
                {
                    "sent": "I use doxygen for the documentation and why it compiles under Linux and under.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Windows out of the books.",
                    "label": 0
                },
                {
                    "sent": "So what is the target audience?",
                    "label": 0
                },
                {
                    "sent": "The target audience is basically researchers that have a good understanding of graphical models and of inference in graphical models.",
                    "label": 1
                },
                {
                    "sent": "And optimally if you want to get the most out of lip, tie it, you need a good knowledge of C++, but you can also, as I said, use the other interfaces.",
                    "label": 0
                },
                {
                    "sent": "And well, if you can just use the existing algorithms to if you have a new algorithm, for example, use these existing algorithms to compare your method with it.",
                    "label": 0
                },
                {
                    "sent": "Or ideally you could implement a new method in lip tie and contribute it so that the collection of methods will grow so things that are not in liptai are.",
                    "label": 0
                },
                {
                    "sent": "The following things.",
                    "label": 1
                },
                {
                    "sent": "There are no, there's no learning algorithms in it, so no parameter learning the structure learning there is limited file format supports, there's no GUI, no visualization.",
                    "label": 0
                },
                {
                    "sent": "So basically it's just an inference.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Engine, so it's really a library.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're at the MLOS website you can download.",
                    "label": 0
                },
                {
                    "sent": "At Liptai there, but you can also go to the public Git tree which has the newer.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Code.",
                    "label": 0
                },
                {
                    "sent": "So OK related software.",
                    "label": 0
                },
                {
                    "sent": "So if I select it all the open source software which was able to deal with both directors and undirected graphs and I found.",
                    "label": 0
                },
                {
                    "sent": "Basically I found these two other packages, the Bayes net toolbox written by Kevin Murphy which is written in mainly in Matlab and the Probabilistic Networks library from Intel.",
                    "label": 1
                },
                {
                    "sent": "And well, if you look here, you see that there is some overlap between those three packages, and there are some algorithms which are only in lip Diane.",
                    "label": 0
                },
                {
                    "sent": "There are some features which are in the other packages, but which are not elliptic.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so the shorts demo.",
                    "label": 0
                },
                {
                    "sent": "How is the time?",
                    "label": 0
                },
                {
                    "sent": "OK. An so OK so so I do a quick demo of the Command line interface, so I used the alarm network which is a well known Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "And it is.",
                    "label": 0
                },
                {
                    "sent": "So this is how the file format looks like, so we can even human readable a bit.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's this this command line interface, so this program that you can use to quickly compare different methods on a given vector graph, say so it has some arguments.",
                    "label": 0
                },
                {
                    "sent": "And well, so you can hear you specify the file name and then you specify a bunch of methods and the first method is to run that that the others will be compared against.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "In this case I choose a junction tree so that we can so that we get the exact results and the other methods are approximate methods so we can compare how good the approximate results are.",
                    "label": 0
                },
                {
                    "sent": "And so, for example, here we have to be P versions with different update schedules.",
                    "label": 0
                },
                {
                    "sent": "And here is 3 P and mean fields and generalized BP methods, and so in this file aliases dot conf you have the default parameters.",
                    "label": 0
                },
                {
                    "sent": "So basically but yeah, so they're defined there.",
                    "label": 0
                },
                {
                    "sent": "You can edit that, but you can also on the command line change the parameters.",
                    "label": 0
                },
                {
                    "sent": "Of the methods for by example for setting the maximum number of iterations, and this is not.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe then I have another resolution.",
                    "label": 0
                },
                {
                    "sent": "OK, so here you see the results an.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "You see, the time that the algorithms needed the number of iterations stated.",
                    "label": 0
                },
                {
                    "sent": "The maximum error in the marginals of the variables so.",
                    "label": 0
                },
                {
                    "sent": "And the average error.",
                    "label": 0
                },
                {
                    "sent": "In the marginals and here is the error in the log partition sum.",
                    "label": 0
                },
                {
                    "sent": "And here you see basically whether it has converged or not, so you see that the last methods, generalized belief propagation, has not converged because it didn't have enough iterations, so.",
                    "label": 0
                },
                {
                    "sent": "We can take another parameter and so basically this this office here quick methods to to compare various methods on various models and if you.",
                    "label": 0
                },
                {
                    "sent": "If you would like to know that the marginals then.",
                    "label": 0
                },
                {
                    "sent": "You can also get the details marginals and well yeah, so basically this is the current command line interface an if you can do all kinds of other things.",
                    "label": 0
                },
                {
                    "sent": "Of course if you just edit this or write your own C++ code to call the library.",
                    "label": 0
                },
                {
                    "sent": "So yeah, that's it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "On what data sizes?",
                    "label": 0
                },
                {
                    "sent": "If you tested your algorithm because?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Obviously, knowing that it would be having problem data size.",
                    "label": 0
                },
                {
                    "sent": "So this heavily OK.",
                    "label": 0
                },
                {
                    "sent": "So which algorithm you can use for a large vector graph?",
                    "label": 0
                },
                {
                    "sent": "1000 No yeah, so 10,000 notes is sort of routine, so for some algorithms not for all but.",
                    "label": 0
                },
                {
                    "sent": "And if you look after so, so I know that that it's used for example in computer vision by some people.",
                    "label": 0
                },
                {
                    "sent": "And I myself.",
                    "label": 0
                },
                {
                    "sent": "I usually play around with quite small networks, but sometimes I always try some larger ones.",
                    "label": 0
                },
                {
                    "sent": "Lately.",
                    "label": 0
                },
                {
                    "sent": "Because that's one of the issues.",
                    "label": 0
                },
                {
                    "sent": "Floating point is finite.",
                    "label": 0
                },
                {
                    "sent": "So usually this is not really such a large problem, but if it is then for example.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So here you see some some of the default settings.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you want to do belief propagation then you can do the updates in the log domain or in the real domain and in the lock domain it will have higher stability and there is a parameter such as.",
                    "label": 0
                },
                {
                    "sent": "You can do it in the log domain and ideally.",
                    "label": 0
                },
                {
                    "sent": "You could do this, so currently you can only do this for belief propagation, but an I'm doing some want to extend it a bit so that you have a factor which works in the log domain and that it just works too transparently for all the algorithms.",
                    "label": 0
                },
                {
                    "sent": "Speak again.",
                    "label": 0
                }
            ]
        }
    }
}