{
    "id": "o3gu3d6cfdcktuymde5apg72fwqo25oa",
    "title": "Boosting Products of Base Classi\ufb01ers",
    "info": {
        "author": [
            "Bal\u00e1zs K\u00e9gl, Laboratoire de l Acc\u00e9l\u00e9rateur Lin\u00e9aire, University of Paris-Sud 11"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Classification",
            "Top->Computer Science->Machine Learning->Boosting"
        ]
    },
    "url": "http://videolectures.net/icml09_kegl_bpbc/",
    "segmentation": [
        [
            "So I will show you how to boost product or base classifiers."
        ],
        [
            "There will be 2 parts of my talk.",
            "In the first I will show you the generic algorithm and I'll show you results we obtained by boosting products of decision stumps.",
            "And in the second part of the talk I will introduce a new base learner for nominal features and I will show how the products of this new base learners are related to maximum margin metrics factor."
        ],
        [
            "Station so I put up the code of other boost, not to explain, just to show you the notations."
        ],
        [
            "Are using so.",
            "We have a data set that contains of pairs of observation vectors and labels.",
            "For technical simplicity, I will explain the algorithm for binary doubles.",
            "So why I are either plus one or minus one?",
            "But the algorithm itself works for multiclass classification."
        ],
        [
            "Second parameter is the base classifier, generic."
        ],
        [
            "Algorithm and the number of iterations."
        ],
        [
            "So that will starts by initializing a weight vector on the data sample uniform."
        ],
        [
            "And then it starts iterations.",
            "In each iteration it starts by calling the base learner using the data set and the weight vector."
        ],
        [
            "The goal of the base learner is to minimize the weighted error, which is equivalent to maximizing the edge and for technical reason it's actually simpler to work on the edge and then on the error.",
            "So the edge basically just the product of the labels.",
            "The output of the base classifier and the weights.",
            "Since the output of the way base classifiers and Y are both plus minus one and the weights are normalized, this is a number between minus 1 + 1 and.",
            "For boosting to be able to continue, it has to be bigger than zero, so the edge is between zero and one, and the better it is."
        ],
        [
            "The.",
            "The higher it is the better, so then the coefficient is set based on the edge."
        ],
        [
            "And then we wait the example.",
            "So four points that are misclassified.",
            "This factor is higher than one.",
            "So the weight."
        ],
        [
            "You go up and correctly classified points.",
            "The weight will go down."
        ],
        [
            "And at the end we returned.",
            "The strong classifier."
        ],
        [
            "So I will talk about Bayes learning, so the simplest based classifier used on real valued features are the decision stumps basically parameterized by the feature we will cut and the threshold.",
            "So these are just one decision to leave decision trees.",
            "They can be trained pretty efficiently by an exhaustive search that looks at all features and all data points in linear time.",
            "In both of those parameters.",
            "If the features are presorted, but that can be done before other boost starts, so.",
            "Although these are pretty good base learners for a lot of applications, it turns out that you can do better.",
            "By using more complicated base learners and one of the ways to construct more complicated base learners is to construct trees based on your base learner.",
            "If you actually use decision stamps, this will be classical decision cheese.",
            "Construct it slightly differently to strong C 4.5 but for.",
            "For all practical goals, it's pretty equivalent, so if you can just call your base the decision stump classifier, recursively construct trees that will give you actually state of the art performance in a lot of data."
        ],
        [
            "It's so.",
            "So our contribution is a different kind of base learner that in the same way as trees use base learners and compose them, but in a different way.",
            "So we will.",
            "Learn products of base classifiers.",
            "WHI we had this idea I will show it in the second part of the talk.",
            "So for now I just show how to change these classifiers so the key point is that the edge what we want to maximize is multiplicative in the labels and based classifiers outputs so.",
            "If we fix all the base classifiers in the product except for one, you can do a trick basically.",
            "Take the product of all the fixed the outputs of the order fixed base classifiers multiplied by the labor and code this virtual labels.",
            "OK, so in this product you will have the base classifier want to optimize times.",
            "This virtual labels times the weight, which looks exactly what the base classifiers are supposed to maximize.",
            "So by calling your pre implemented based classifier with this virtual labels you can optimize each term of the product in iterative manner.",
            "So formally we initialize all the products to constant one.",
            "And then we iterate over the terms until convergence.",
            "So couple of remarks.",
            "So basically, if you want technically what we do is, we just call the stamp algorithm.",
            "We flip the labels where the stumps is minus one and when we recall the stamp for these labels again flip the labels where the output is minus one etc into convergence.",
            "So obviously this won't learn the optimal product in the product space.",
            "Because it's a greedy algorithm, but it's the same way as trees don't really learn the best tree in the tree space.",
            "So since we boost this base, classifiers does not actually a problem.",
            "The other remark is that the edge actually either stays the same as in the previous iteration, in which case we stop or it increases by an amount that's bounded away from zero, because it can only increase by these weights.",
            "So.",
            "In any case, we stop in finite number of iterations this loop.",
            "So in the algorithm actually we let to let.",
            "To cycle through these terms, but in practice we rarely observe more than one or maybe two cycles or over the.",
            "Terms of the product.",
            "So it's pretty fast.",
            "It doesn't add a lot of overhead to the other boost algorithm.",
            "On the other hand, it will converge faster because the base classifier."
        ],
        [
            "Space is richer.",
            "So I show you the results that are more proud of on me, so I missed this.",
            "Handwritten character that is used a lot, maybe overused, but the advantage is that there are a lot of Apple implementation of standards algorithms that are well documented and then compare your results.",
            "So we tried two different based learners, one was.",
            "Single stamps on the pixels, which are pretty terrible if you just boost them, they get to like 7% error rate.",
            "So we tried products over stumps over the pixels and trees over the stumps over the pixels.",
            "You see that we have an edge and actually if you look compared to generate methods.",
            "But I'm calling generic methods.",
            "The methods that don't know that the data are images.",
            "So basically the test is that people compare mute all the pixels and it will give you the same solution.",
            "So among these generic methods, SVM did Gaussian kernel.",
            "Neural Nets with backpropagation.",
            "These are between the two and product really beats them and this is actually the best generic method if you don't count deep networks which are being presented in another room.",
            "The other set of the experiments we used a specific image in image specific based learner called hard filters which were.",
            "Proposed by village in Johns for face classification.",
            "So these are rectangular filters that can be pre with some steps to reset some data structures that can be evaluated pretty fast on images.",
            "The point here is that this is a huge feature space.",
            "You can have 100,000 filters, potential filters that overreach you boost, and usually in this huge spaces you would expect that the single stamp boosted Stample work well and it's true actually works pretty well, it's around 1%.",
            "We were really surprised that when we tried boosting products over this huge space, we could improve.",
            "The classification error and this is in the ballpark of other algorithms that use image specific.",
            "Features but not character specific features so they don't distort the character.",
            "They don't know it's images of characters that they know.",
            "These are images.",
            "So these are the.",
            "Posting"
        ],
        [
            "Portant experimental results.",
            "We run it on four other large datasets because our goal was to try it on large datasets.",
            "You can see that we run it for a long time.",
            "We observe no overfitting.",
            "Which is pretty nice on trees.",
            "Sometimes we observe a little bit more overfitting, so our intuition is that products are a little bit more.",
            "Safe in these terms.",
            "This is the only set where cheese are slightly better than products."
        ],
        [
            "So to conclude, this first part as I said, the version of the paper is described for multiclass to the boost of course, or this sets for multiclass classification sets.",
            "So why nobody thought about doing products before as based learning?",
            "One of the reasons is that the products are not strong learners, so we tried to set M to Infinity and see what happened with the first product.",
            "It turned out that they were pretty bad underfitting so.",
            "So as a standalone learner, they cannot be used, unlike trees.",
            "It might be that if you do some other optimization, not this greedy optimization, you can get better, but I have an intuition that it's there pretty.",
            "A low complexity so you cannot get strong learners with simple products.",
            "The other indication why they might be better than cheese is that these are nominal complexity, which was validated on hold out sets.",
            "So the number of terms in the product is much less than in the trees, so you get 2, three, maybe go to up to 10 on certain datasets, but in the trees you can get 2030 forty leaves.",
            "If you validate the number of leaves.",
            "So that the complexity of the base learner is simpler, is lower, and actually an interesting, more theoretical kind of statement is that.",
            "If you take linear combination of products.",
            "Then you can prove that they are universal approximators, so you can approximate any function if and only if the number of terms is bigger than number of dimension, and the hint is that.",
            "If you take stamps, for example, take linear Commission of stamps, you cannot learn the X or the XYZ or problem.",
            "Same way if you take M term products, you cannot learn the N plus one term parity problem.",
            "OK so if you are indeed dimension and M is less than D, then if you take the parity.",
            "Over the binary vectors as label, you cannot learn and be even with the linear combination of the product, so boosting products will not give you.",
            "Completely correct classification on that set, so that gives you an idea what the what this M represents is a complexity of the function class.",
            "So this concludes the first part."
        ],
        [
            "The talk now.",
            "So in the second part I will talk about nominal features.",
            "So far you stamps and these are real valued features, so nominal features are just a set of values which are unordered.",
            "Here I'm using integers, but the point is that you cannot use their order, so usually what's used on this kind of features is this kind of base learners.",
            "So this kind of baseless is that I could selector because they select one value of the feature it's plus one is if the data point has that value.",
            "Minus one is if it's not that value, so you get this kind of representations.",
            "These represent the blue weak learner that gives plus one for the blue and minus one for the others.",
            "Actually, you get this if you one hot encode your nominal features and use stamps on that encoding.",
            "So probably that's why it's a popular quick learner because you can convert your nominal features into numerical features and use your stamps.",
            "It takes this number of steps to learn.",
            "'cause you have to look at all the data points and all the features and then you have to select the best feature and the best value of that feature that you will head to your pool.",
            "So it turns out that you can learn much richer features using the same number of iterations, which I call indicator because they indicate a subset of a value values for a given nominal feature.",
            "OK, so you have a binary vector that represents a binary vector over the value.",
            "So this week learner will say plus one if the value is red or blue or yellow and minus one if it's blue or green.",
            "So the running time of this week learner is the same now.",
            "So why is it in chess?"
        ],
        [
            "Sting hits I show you a common example that's used a lot nowadays, which is the collaborative filtering, except so imagine that your data set.",
            "Each data point has two indices.",
            "Nominal features in this is the order, one is the movie ID, the other is the user ID.",
            "So if you look at the form of products of indicator learners.",
            "Over this example is what you get is basically each base learner is an outer product of two vectors, one vector over newbies and another vector over the users plus the coefficient.",
            "OK, so if you move this indicator learners the product over this indicator learners, what you get actually is much is factorization and cause other boost is maximizing the margin you basically it's a maximum margin matrix factorization method.",
            "Of course L1 margin because that's what other boost is pushing up.",
            "OK, so.",
            "To give you an idea why we thought about boosting, it was boosting products.",
            "It was exactly this example.",
            "So what happened is actually that we tried on."
        ],
        [
            "A small data set.",
            "So basically this is a.",
            "Table that gives you some analogies.",
            "So what is Adaboost?",
            "SVM is the same other boosted products of subset indicators to the semidefinite programming based maximum margin, factorization of srebro and the collaborators so.",
            "As I said, we tried it on the small data set that they used in that paper and it was compareable when we went to high large datasets.",
            "It actually didn't really work.",
            "So.",
            "I think that I mean the weak learner must be improved.",
            "My intuition is that it's actually two complex, have too much freedom in filling out all the values of the vectors freely.",
            "In any case it formalizes the match.",
            "Maxima geomatics, factorization.",
            "Problem in the L1 regime.",
            "So what I wanted to say.",
            "What's ironic about this is that we did this whole idea of boosting products was because we wanted to formalize this problem, and it turned out that by accident I tried it on stumps and it turned out that practically it's a much more significant result to boost products of stumps that boost productive indicator learners.",
            "We haven't gave given up on this.",
            "MTX factorization yet, but for the time being on the larger datasets it doesn't relieve."
        ],
        [
            "So to conclude.",
            "Boosting product seem to outperform boosted trees seem to less prone to overfitting.",
            "And then what's interesting that we can improve over stamps even in very high dimensional feature spaces.",
            "And if you both products of subset indicators, we formalize the Maxima G. Magic factorization problem with L1 margin.",
            "But we have to improve the weak learner for better practical performance.",
            "Thank you.",
            "Cute.",
            "So you say that Adams solves the maximum margin matrix factorization problem.",
            "Is that a well known result, or is that something you guys proved?",
            "Or well it is by the results on classical other boost it's maximizing the margin on the on the examples that are filled out in the matrix in actually log number of filled out example steps.",
            "Times emerging third product features.",
            "Optimize this margin more or.",
            "This is the way to maximize the margin using products of two matches.",
            "Otherwise, you could use other weak learners to fill out that matches, but.",
            "That's.",
            "Yes, can you go back to the slide there was wearing the training curve before plots or the.",
            "This.",
            "Glass your product.",
            "The subscribe is actually delay.",
            "There seems to be like in the second yeah, because it's less complex based classifier so cheese can go faster in the beginning.",
            "Yeah, we we always validate the hyperparameters.",
            "The number of leaves and the number of terms, and it turns out that we have much more leaves than terms.",
            "OK, so it starts lower.",
            "Yeah, it's a good observation.",
            "Is that it was?",
            "Make a fraction constant amount slower or as the data is bigger or dimensionality diagnosis that difference.",
            "Actually we didn't.",
            "Yeah.",
            "Not explicitly maximized, I know, I know.",
            "Close to half of the optimum.",
            "I know, I know, but it still I wanted to compare it to SVM so I that's why CSL 1 not 2.",
            "Maximize this one.",
            "Yeah, I know.",
            "More questions.",
            "We had some results.",
            "Pause.",
            "Most valuable off camera.",
            "These are all multiclass problems and I used yeah, yeah, and I use multiclass based classifiers and I use that doubles MH.",
            "I just didn't describe it because it's more technical, but it's the same idea.",
            "So typically having a product based classifier and then you make the product basically.",
            "The number of iterations here doesn't mean that you have only that much stamps for that much classifiers, because in each in each iteration you have more classifiers, more stamps to level you, so there also counts how many times do you know face detection and and you want to actually classify an example.",
            "If it comes how many stacks.",
            "Typically, you know typically the numbers.",
            "It's well for use USPS, and MNIST.",
            "Was it two or three for some of them I don't remember which they are.",
            "The biggest was 10.",
            "OK, and cheese were much bigger and they were proportionally bigger too.",
            "I'm actually interested.",
            "OK, thanks the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will show you how to boost product or base classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There will be 2 parts of my talk.",
                    "label": 0
                },
                {
                    "sent": "In the first I will show you the generic algorithm and I'll show you results we obtained by boosting products of decision stumps.",
                    "label": 0
                },
                {
                    "sent": "And in the second part of the talk I will introduce a new base learner for nominal features and I will show how the products of this new base learners are related to maximum margin metrics factor.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Station so I put up the code of other boost, not to explain, just to show you the notations.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are using so.",
                    "label": 0
                },
                {
                    "sent": "We have a data set that contains of pairs of observation vectors and labels.",
                    "label": 0
                },
                {
                    "sent": "For technical simplicity, I will explain the algorithm for binary doubles.",
                    "label": 0
                },
                {
                    "sent": "So why I are either plus one or minus one?",
                    "label": 0
                },
                {
                    "sent": "But the algorithm itself works for multiclass classification.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second parameter is the base classifier, generic.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm and the number of iterations.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that will starts by initializing a weight vector on the data sample uniform.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then it starts iterations.",
                    "label": 0
                },
                {
                    "sent": "In each iteration it starts by calling the base learner using the data set and the weight vector.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The goal of the base learner is to minimize the weighted error, which is equivalent to maximizing the edge and for technical reason it's actually simpler to work on the edge and then on the error.",
                    "label": 0
                },
                {
                    "sent": "So the edge basically just the product of the labels.",
                    "label": 0
                },
                {
                    "sent": "The output of the base classifier and the weights.",
                    "label": 0
                },
                {
                    "sent": "Since the output of the way base classifiers and Y are both plus minus one and the weights are normalized, this is a number between minus 1 + 1 and.",
                    "label": 0
                },
                {
                    "sent": "For boosting to be able to continue, it has to be bigger than zero, so the edge is between zero and one, and the better it is.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The higher it is the better, so then the coefficient is set based on the edge.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we wait the example.",
                    "label": 0
                },
                {
                    "sent": "So four points that are misclassified.",
                    "label": 0
                },
                {
                    "sent": "This factor is higher than one.",
                    "label": 0
                },
                {
                    "sent": "So the weight.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You go up and correctly classified points.",
                    "label": 0
                },
                {
                    "sent": "The weight will go down.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And at the end we returned.",
                    "label": 0
                },
                {
                    "sent": "The strong classifier.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will talk about Bayes learning, so the simplest based classifier used on real valued features are the decision stumps basically parameterized by the feature we will cut and the threshold.",
                    "label": 0
                },
                {
                    "sent": "So these are just one decision to leave decision trees.",
                    "label": 1
                },
                {
                    "sent": "They can be trained pretty efficiently by an exhaustive search that looks at all features and all data points in linear time.",
                    "label": 0
                },
                {
                    "sent": "In both of those parameters.",
                    "label": 0
                },
                {
                    "sent": "If the features are presorted, but that can be done before other boost starts, so.",
                    "label": 1
                },
                {
                    "sent": "Although these are pretty good base learners for a lot of applications, it turns out that you can do better.",
                    "label": 0
                },
                {
                    "sent": "By using more complicated base learners and one of the ways to construct more complicated base learners is to construct trees based on your base learner.",
                    "label": 0
                },
                {
                    "sent": "If you actually use decision stamps, this will be classical decision cheese.",
                    "label": 0
                },
                {
                    "sent": "Construct it slightly differently to strong C 4.5 but for.",
                    "label": 0
                },
                {
                    "sent": "For all practical goals, it's pretty equivalent, so if you can just call your base the decision stump classifier, recursively construct trees that will give you actually state of the art performance in a lot of data.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's so.",
                    "label": 0
                },
                {
                    "sent": "So our contribution is a different kind of base learner that in the same way as trees use base learners and compose them, but in a different way.",
                    "label": 1
                },
                {
                    "sent": "So we will.",
                    "label": 1
                },
                {
                    "sent": "Learn products of base classifiers.",
                    "label": 0
                },
                {
                    "sent": "WHI we had this idea I will show it in the second part of the talk.",
                    "label": 0
                },
                {
                    "sent": "So for now I just show how to change these classifiers so the key point is that the edge what we want to maximize is multiplicative in the labels and based classifiers outputs so.",
                    "label": 0
                },
                {
                    "sent": "If we fix all the base classifiers in the product except for one, you can do a trick basically.",
                    "label": 0
                },
                {
                    "sent": "Take the product of all the fixed the outputs of the order fixed base classifiers multiplied by the labor and code this virtual labels.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this product you will have the base classifier want to optimize times.",
                    "label": 1
                },
                {
                    "sent": "This virtual labels times the weight, which looks exactly what the base classifiers are supposed to maximize.",
                    "label": 0
                },
                {
                    "sent": "So by calling your pre implemented based classifier with this virtual labels you can optimize each term of the product in iterative manner.",
                    "label": 0
                },
                {
                    "sent": "So formally we initialize all the products to constant one.",
                    "label": 1
                },
                {
                    "sent": "And then we iterate over the terms until convergence.",
                    "label": 0
                },
                {
                    "sent": "So couple of remarks.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you want technically what we do is, we just call the stamp algorithm.",
                    "label": 0
                },
                {
                    "sent": "We flip the labels where the stumps is minus one and when we recall the stamp for these labels again flip the labels where the output is minus one etc into convergence.",
                    "label": 0
                },
                {
                    "sent": "So obviously this won't learn the optimal product in the product space.",
                    "label": 0
                },
                {
                    "sent": "Because it's a greedy algorithm, but it's the same way as trees don't really learn the best tree in the tree space.",
                    "label": 0
                },
                {
                    "sent": "So since we boost this base, classifiers does not actually a problem.",
                    "label": 0
                },
                {
                    "sent": "The other remark is that the edge actually either stays the same as in the previous iteration, in which case we stop or it increases by an amount that's bounded away from zero, because it can only increase by these weights.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In any case, we stop in finite number of iterations this loop.",
                    "label": 0
                },
                {
                    "sent": "So in the algorithm actually we let to let.",
                    "label": 0
                },
                {
                    "sent": "To cycle through these terms, but in practice we rarely observe more than one or maybe two cycles or over the.",
                    "label": 0
                },
                {
                    "sent": "Terms of the product.",
                    "label": 0
                },
                {
                    "sent": "So it's pretty fast.",
                    "label": 0
                },
                {
                    "sent": "It doesn't add a lot of overhead to the other boost algorithm.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, it will converge faster because the base classifier.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Space is richer.",
                    "label": 0
                },
                {
                    "sent": "So I show you the results that are more proud of on me, so I missed this.",
                    "label": 0
                },
                {
                    "sent": "Handwritten character that is used a lot, maybe overused, but the advantage is that there are a lot of Apple implementation of standards algorithms that are well documented and then compare your results.",
                    "label": 0
                },
                {
                    "sent": "So we tried two different based learners, one was.",
                    "label": 0
                },
                {
                    "sent": "Single stamps on the pixels, which are pretty terrible if you just boost them, they get to like 7% error rate.",
                    "label": 0
                },
                {
                    "sent": "So we tried products over stumps over the pixels and trees over the stumps over the pixels.",
                    "label": 0
                },
                {
                    "sent": "You see that we have an edge and actually if you look compared to generate methods.",
                    "label": 0
                },
                {
                    "sent": "But I'm calling generic methods.",
                    "label": 1
                },
                {
                    "sent": "The methods that don't know that the data are images.",
                    "label": 0
                },
                {
                    "sent": "So basically the test is that people compare mute all the pixels and it will give you the same solution.",
                    "label": 0
                },
                {
                    "sent": "So among these generic methods, SVM did Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Neural Nets with backpropagation.",
                    "label": 0
                },
                {
                    "sent": "These are between the two and product really beats them and this is actually the best generic method if you don't count deep networks which are being presented in another room.",
                    "label": 0
                },
                {
                    "sent": "The other set of the experiments we used a specific image in image specific based learner called hard filters which were.",
                    "label": 0
                },
                {
                    "sent": "Proposed by village in Johns for face classification.",
                    "label": 0
                },
                {
                    "sent": "So these are rectangular filters that can be pre with some steps to reset some data structures that can be evaluated pretty fast on images.",
                    "label": 0
                },
                {
                    "sent": "The point here is that this is a huge feature space.",
                    "label": 0
                },
                {
                    "sent": "You can have 100,000 filters, potential filters that overreach you boost, and usually in this huge spaces you would expect that the single stamp boosted Stample work well and it's true actually works pretty well, it's around 1%.",
                    "label": 1
                },
                {
                    "sent": "We were really surprised that when we tried boosting products over this huge space, we could improve.",
                    "label": 0
                },
                {
                    "sent": "The classification error and this is in the ballpark of other algorithms that use image specific.",
                    "label": 0
                },
                {
                    "sent": "Features but not character specific features so they don't distort the character.",
                    "label": 0
                },
                {
                    "sent": "They don't know it's images of characters that they know.",
                    "label": 0
                },
                {
                    "sent": "These are images.",
                    "label": 0
                },
                {
                    "sent": "So these are the.",
                    "label": 0
                },
                {
                    "sent": "Posting",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Portant experimental results.",
                    "label": 0
                },
                {
                    "sent": "We run it on four other large datasets because our goal was to try it on large datasets.",
                    "label": 0
                },
                {
                    "sent": "You can see that we run it for a long time.",
                    "label": 0
                },
                {
                    "sent": "We observe no overfitting.",
                    "label": 0
                },
                {
                    "sent": "Which is pretty nice on trees.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we observe a little bit more overfitting, so our intuition is that products are a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Safe in these terms.",
                    "label": 0
                },
                {
                    "sent": "This is the only set where cheese are slightly better than products.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, this first part as I said, the version of the paper is described for multiclass to the boost of course, or this sets for multiclass classification sets.",
                    "label": 0
                },
                {
                    "sent": "So why nobody thought about doing products before as based learning?",
                    "label": 0
                },
                {
                    "sent": "One of the reasons is that the products are not strong learners, so we tried to set M to Infinity and see what happened with the first product.",
                    "label": 1
                },
                {
                    "sent": "It turned out that they were pretty bad underfitting so.",
                    "label": 0
                },
                {
                    "sent": "So as a standalone learner, they cannot be used, unlike trees.",
                    "label": 0
                },
                {
                    "sent": "It might be that if you do some other optimization, not this greedy optimization, you can get better, but I have an intuition that it's there pretty.",
                    "label": 0
                },
                {
                    "sent": "A low complexity so you cannot get strong learners with simple products.",
                    "label": 0
                },
                {
                    "sent": "The other indication why they might be better than cheese is that these are nominal complexity, which was validated on hold out sets.",
                    "label": 0
                },
                {
                    "sent": "So the number of terms in the product is much less than in the trees, so you get 2, three, maybe go to up to 10 on certain datasets, but in the trees you can get 2030 forty leaves.",
                    "label": 0
                },
                {
                    "sent": "If you validate the number of leaves.",
                    "label": 0
                },
                {
                    "sent": "So that the complexity of the base learner is simpler, is lower, and actually an interesting, more theoretical kind of statement is that.",
                    "label": 0
                },
                {
                    "sent": "If you take linear combination of products.",
                    "label": 1
                },
                {
                    "sent": "Then you can prove that they are universal approximators, so you can approximate any function if and only if the number of terms is bigger than number of dimension, and the hint is that.",
                    "label": 1
                },
                {
                    "sent": "If you take stamps, for example, take linear Commission of stamps, you cannot learn the X or the XYZ or problem.",
                    "label": 0
                },
                {
                    "sent": "Same way if you take M term products, you cannot learn the N plus one term parity problem.",
                    "label": 0
                },
                {
                    "sent": "OK so if you are indeed dimension and M is less than D, then if you take the parity.",
                    "label": 0
                },
                {
                    "sent": "Over the binary vectors as label, you cannot learn and be even with the linear combination of the product, so boosting products will not give you.",
                    "label": 0
                },
                {
                    "sent": "Completely correct classification on that set, so that gives you an idea what the what this M represents is a complexity of the function class.",
                    "label": 0
                },
                {
                    "sent": "So this concludes the first part.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The talk now.",
                    "label": 0
                },
                {
                    "sent": "So in the second part I will talk about nominal features.",
                    "label": 1
                },
                {
                    "sent": "So far you stamps and these are real valued features, so nominal features are just a set of values which are unordered.",
                    "label": 0
                },
                {
                    "sent": "Here I'm using integers, but the point is that you cannot use their order, so usually what's used on this kind of features is this kind of base learners.",
                    "label": 1
                },
                {
                    "sent": "So this kind of baseless is that I could selector because they select one value of the feature it's plus one is if the data point has that value.",
                    "label": 0
                },
                {
                    "sent": "Minus one is if it's not that value, so you get this kind of representations.",
                    "label": 0
                },
                {
                    "sent": "These represent the blue weak learner that gives plus one for the blue and minus one for the others.",
                    "label": 1
                },
                {
                    "sent": "Actually, you get this if you one hot encode your nominal features and use stamps on that encoding.",
                    "label": 0
                },
                {
                    "sent": "So probably that's why it's a popular quick learner because you can convert your nominal features into numerical features and use your stamps.",
                    "label": 0
                },
                {
                    "sent": "It takes this number of steps to learn.",
                    "label": 1
                },
                {
                    "sent": "'cause you have to look at all the data points and all the features and then you have to select the best feature and the best value of that feature that you will head to your pool.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that you can learn much richer features using the same number of iterations, which I call indicator because they indicate a subset of a value values for a given nominal feature.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have a binary vector that represents a binary vector over the value.",
                    "label": 0
                },
                {
                    "sent": "So this week learner will say plus one if the value is red or blue or yellow and minus one if it's blue or green.",
                    "label": 0
                },
                {
                    "sent": "So the running time of this week learner is the same now.",
                    "label": 0
                },
                {
                    "sent": "So why is it in chess?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sting hits I show you a common example that's used a lot nowadays, which is the collaborative filtering, except so imagine that your data set.",
                    "label": 0
                },
                {
                    "sent": "Each data point has two indices.",
                    "label": 0
                },
                {
                    "sent": "Nominal features in this is the order, one is the movie ID, the other is the user ID.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the form of products of indicator learners.",
                    "label": 1
                },
                {
                    "sent": "Over this example is what you get is basically each base learner is an outer product of two vectors, one vector over newbies and another vector over the users plus the coefficient.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you move this indicator learners the product over this indicator learners, what you get actually is much is factorization and cause other boost is maximizing the margin you basically it's a maximum margin matrix factorization method.",
                    "label": 0
                },
                {
                    "sent": "Of course L1 margin because that's what other boost is pushing up.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "To give you an idea why we thought about boosting, it was boosting products.",
                    "label": 0
                },
                {
                    "sent": "It was exactly this example.",
                    "label": 0
                },
                {
                    "sent": "So what happened is actually that we tried on.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A small data set.",
                    "label": 0
                },
                {
                    "sent": "So basically this is a.",
                    "label": 0
                },
                {
                    "sent": "Table that gives you some analogies.",
                    "label": 0
                },
                {
                    "sent": "So what is Adaboost?",
                    "label": 0
                },
                {
                    "sent": "SVM is the same other boosted products of subset indicators to the semidefinite programming based maximum margin, factorization of srebro and the collaborators so.",
                    "label": 1
                },
                {
                    "sent": "As I said, we tried it on the small data set that they used in that paper and it was compareable when we went to high large datasets.",
                    "label": 0
                },
                {
                    "sent": "It actually didn't really work.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think that I mean the weak learner must be improved.",
                    "label": 1
                },
                {
                    "sent": "My intuition is that it's actually two complex, have too much freedom in filling out all the values of the vectors freely.",
                    "label": 0
                },
                {
                    "sent": "In any case it formalizes the match.",
                    "label": 0
                },
                {
                    "sent": "Maxima geomatics, factorization.",
                    "label": 0
                },
                {
                    "sent": "Problem in the L1 regime.",
                    "label": 0
                },
                {
                    "sent": "So what I wanted to say.",
                    "label": 0
                },
                {
                    "sent": "What's ironic about this is that we did this whole idea of boosting products was because we wanted to formalize this problem, and it turned out that by accident I tried it on stumps and it turned out that practically it's a much more significant result to boost products of stumps that boost productive indicator learners.",
                    "label": 0
                },
                {
                    "sent": "We haven't gave given up on this.",
                    "label": 0
                },
                {
                    "sent": "MTX factorization yet, but for the time being on the larger datasets it doesn't relieve.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "Boosting product seem to outperform boosted trees seem to less prone to overfitting.",
                    "label": 1
                },
                {
                    "sent": "And then what's interesting that we can improve over stamps even in very high dimensional feature spaces.",
                    "label": 1
                },
                {
                    "sent": "And if you both products of subset indicators, we formalize the Maxima G. Magic factorization problem with L1 margin.",
                    "label": 1
                },
                {
                    "sent": "But we have to improve the weak learner for better practical performance.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Cute.",
                    "label": 0
                },
                {
                    "sent": "So you say that Adams solves the maximum margin matrix factorization problem.",
                    "label": 0
                },
                {
                    "sent": "Is that a well known result, or is that something you guys proved?",
                    "label": 0
                },
                {
                    "sent": "Or well it is by the results on classical other boost it's maximizing the margin on the on the examples that are filled out in the matrix in actually log number of filled out example steps.",
                    "label": 0
                },
                {
                    "sent": "Times emerging third product features.",
                    "label": 0
                },
                {
                    "sent": "Optimize this margin more or.",
                    "label": 0
                },
                {
                    "sent": "This is the way to maximize the margin using products of two matches.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, you could use other weak learners to fill out that matches, but.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                },
                {
                    "sent": "Yes, can you go back to the slide there was wearing the training curve before plots or the.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Glass your product.",
                    "label": 0
                },
                {
                    "sent": "The subscribe is actually delay.",
                    "label": 0
                },
                {
                    "sent": "There seems to be like in the second yeah, because it's less complex based classifier so cheese can go faster in the beginning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we we always validate the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "The number of leaves and the number of terms, and it turns out that we have much more leaves than terms.",
                    "label": 0
                },
                {
                    "sent": "OK, so it starts lower.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's a good observation.",
                    "label": 0
                },
                {
                    "sent": "Is that it was?",
                    "label": 0
                },
                {
                    "sent": "Make a fraction constant amount slower or as the data is bigger or dimensionality diagnosis that difference.",
                    "label": 0
                },
                {
                    "sent": "Actually we didn't.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Not explicitly maximized, I know, I know.",
                    "label": 0
                },
                {
                    "sent": "Close to half of the optimum.",
                    "label": 0
                },
                {
                    "sent": "I know, I know, but it still I wanted to compare it to SVM so I that's why CSL 1 not 2.",
                    "label": 0
                },
                {
                    "sent": "Maximize this one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "We had some results.",
                    "label": 0
                },
                {
                    "sent": "Pause.",
                    "label": 0
                },
                {
                    "sent": "Most valuable off camera.",
                    "label": 0
                },
                {
                    "sent": "These are all multiclass problems and I used yeah, yeah, and I use multiclass based classifiers and I use that doubles MH.",
                    "label": 0
                },
                {
                    "sent": "I just didn't describe it because it's more technical, but it's the same idea.",
                    "label": 0
                },
                {
                    "sent": "So typically having a product based classifier and then you make the product basically.",
                    "label": 0
                },
                {
                    "sent": "The number of iterations here doesn't mean that you have only that much stamps for that much classifiers, because in each in each iteration you have more classifiers, more stamps to level you, so there also counts how many times do you know face detection and and you want to actually classify an example.",
                    "label": 0
                },
                {
                    "sent": "If it comes how many stacks.",
                    "label": 0
                },
                {
                    "sent": "Typically, you know typically the numbers.",
                    "label": 0
                },
                {
                    "sent": "It's well for use USPS, and MNIST.",
                    "label": 0
                },
                {
                    "sent": "Was it two or three for some of them I don't remember which they are.",
                    "label": 0
                },
                {
                    "sent": "The biggest was 10.",
                    "label": 0
                },
                {
                    "sent": "OK, and cheese were much bigger and they were proportionally bigger too.",
                    "label": 0
                },
                {
                    "sent": "I'm actually interested.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}