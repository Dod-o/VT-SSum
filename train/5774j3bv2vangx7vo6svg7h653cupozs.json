{
    "id": "5774j3bv2vangx7vo6svg7h653cupozs",
    "title": "Learning Nash Equilibrium for General-Sum Markov Games from Batch Dat",
    "info": {
        "author": [
            "Julien Pe\u0301rolat, Universit\u00e9 de Lille 1"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_perolat_learning_nash/",
    "segmentation": [
        [
            "So this is joint work with a flowing struggle.",
            "It comes from the University of Deal.",
            "So you heard a lot of talks about."
        ],
        [
            "Markov decision processes and reinforcement learning this morning.",
            "So this is 1 typical case from the Atari and that's it.",
            "So what is the goal?",
            "You have one player trying to interact with the environment and.",
            "OK, so receiving.",
            "Directing with an environment and receiving a reward as a result of the elections.",
            "So the goal here is to optimize some long-term sum of rewards, so to find some policy to optimize this.",
            "So."
        ],
        [
            "Purities usually for value based algorithm.",
            "You try to find an optimal key functions, meaning you try to find the sum value.",
            "Some value of an action considering your state.",
            "And once you find this function you act greedy according to the function and retrieve a good policy."
        ],
        [
            "So.",
            "What what could we do in this case, like we have we're playing phone, there is two players there, which kind of policy would we like to find?",
            "So.",
            "Some what we could say is reduces to some Markov decision process problem and to say OK I have an agent having a strategy and I try to find.",
            "A best responsive policy to beating.",
            "But maybe the agent can change, and maybe you want some more conservative notion of of a Poly."
        ],
        [
            "And this can be cast to the framework of Zero 7 two player Markov game.",
            "And here you might want to find the min Max strategy, meaning you try to find a strategy that would maximize your reward considering the opponent will minimize it.",
            "So.",
            "So.",
            "2."
        ],
        [
            "This like so you share both players and a way to interact with the environment they are in the same state space and they receive the same reward and one is trying to maximize it where the other is trying to minimize it.",
            "So how can we handle this problem, it's?",
            "It's like for my court decision processes.",
            "You can find the optimal value of Q function of that game and act greedy according to this Q function.",
            "But here acting really will not be like maximizing the action, but taking a min Max over this this matrix game."
        ],
        [
            "More complicated so.",
            "What could we do in this case where we have 4 four agents?",
            "OK, so.",
            "Which kind of notion of optimality would we like to find?",
            "OK so.",
            "The point here is you could also take it as a marketing process.",
            "Both every player would be with some policy and try to find the best response to this.",
            "To those policies of the other players.",
            "But I mean those can adapt and of their own objectives.",
            "So maybe it's not the notion you want to.",
            "You want to find?",
            "You could cast it as a kind of a 0 sum market game, considers that every player.",
            "Or are you playing some worst case scenario and are going to?",
            "Try to beat you and kiss it this way, but maybe you want to team apart with this guy.",
            "Always this guy and this way you too.",
            "This is a way to increase your world so."
        ],
        [
            "What's up in, uh, this is more like Mplayer general submarket game, so each player receives his own reward signal, and what you would like to find the Nash equilibrium.",
            "So you should consider that you play with independent strategy.",
            "That the notion is a game theoretic notion that you want to find."
        ],
        [
            "I'm sure you've heard about the prisoners dilemma.",
            "So what is a Nash equilibrium?",
            "It's you want to find a strategy for each player.",
            "Such as no player would benefit from modifying their current strategy."
        ],
        [
            "So.",
            "I mean, could we work on the key function as in previous previous examples?",
            "The answer is."
        ],
        [
            "No, you cannot retrieve a Nash equilibrium from the function, which is a key function of the Nash equilibrium, so that's that's one surprise there is not enough information in the key function to retrieve a good strategy."
        ],
        [
            "And that's kind of a problem, since every batch reinforcement learning based algorithm.",
            "Only work on key functions and only rely on this key function, so there is a gap between.",
            "Going from batch, they only in zero center player Market Games and or MVP's, two generals and lack of games."
        ],
        [
            "So our goal here is going to be to find epsilon Nash equilibrium, so it's a little weaker notion of optimality for in N player general submarket games from historical later with function approximation.",
            "So as I said, there were a lot of work in Markov decision processes and EPS, such as the some algorithms such as fitted to iteration from else or no GPU.",
            "Another algorithm which is up more of the policy iteration like algorithm is that works with much data or we love work on extending those algorithm on the two player 0 sum markup game setting.",
            "But this is not enough to go to an player general some games.",
            "So to learn Nash equilibrium in general, some Markov games there are there is some work, but it's limited to the online case online learning case.",
            "Or to model based area.",
            "Meaning you have the model.",
            "What about learning it?",
            "Sorry about learning it and other algorithms that are not to learn.",
            "For SEO approaches here, it's more of an actor critic.",
            "Methods for general Markov games.",
            "But still, I'm not.",
            "It's difficult to extend it to the batch case where you have samples.",
            "Difficult in theory.",
            "I I'm not.",
            "I don't know algorithms that work.",
            "Actor critic methods that work for the batch case.",
            "If I."
        ],
        [
            "OK, so.",
            "What we do is we define a weaker notion of Nash equilibrium and we reduce the problem of learning.",
            "And Ashley Graham, two mutation of a surrogate loss, which is a sum of Bellman residual and then we perform an empirical evaluation of this this method on winner in network as a function of."
        ],
        [
            "Summation, so a Markov game is specified by end players a state space.",
            "And action space per player, meaning we have each player has is an action space and the dynamic of the game is going to be.",
            "Will depend on the joint action of old players.",
            "Each player will have an influence on the dynamic, so for the rest of the talk I will note like a -- 8 like the action of every other player and actions involved, which is a joint action of old player.",
            "So just for reservation.",
            "So each player receives his reward signal.",
            "And we will work on that medicine case.",
            "So we needed this connector.",
            "So the goal is to find a strategy for each player considering batch samples."
        ],
        [
            "So as you see in this morning the differences between the online scenario in the batch scenarios that we do not allow ourselves to re sample mean we only have logs of interactions with the.",
            "With the from the game, it's convenient in situations such as an industry where you have lots of interactions and you want to retrieve the strategy from those and.",
            "I mean, it's hard to convince somebody to let you try your policy on a real system and to collect more data.",
            "This might be sensitive and you'd better work on logs and other algorithms that work on logs of data."
        ],
        [
            "So.",
            "I mean, you can still define key functions, or at least a key function according to your policy.",
            "So what is it in that case?",
            "So the state action value function is simply considered that you start each player play action the joint action A and then that every player follow their own strategy.",
            "So this is going to be the joint, the value of the joint policy of every player and as in.",
            "As in the Markov chain processes, this is a.",
            "You can find the bellman operator and this Q function is going to be a fixed point of.",
            "This Belmont prater."
        ],
        [
            "And the other key for key function we're going to is the best response of player I.",
            "So imagine you fix the policy of all other players.",
            "And this is a Markov decision problem and you want to find an optimal policy according to that Mark of decision problem.",
            "So you it simply is a maximum of all your policy considering the policy of the other player is fixed for that function and it's also fixed point of this bellman operator and it satisfied the fixed point equation."
        ],
        [
            "So what is the Nash equilibrium in the definition?",
            "So player has no incentive to modify this current strategy, meaning the key function of his best response equals the key function of the joint policy.",
            "Forward player, right.",
            "And if you rewrite it in terms of Bellman equations, you just need to find.",
            "Joint strategy.",
            "Such that there exists on key functions.",
            "Function for each player that is simultaneously the fixed point of this bellman operator and the fixed point of this operator."
        ],
        [
            "OK, so we're not going to learn the optimal control here.",
            "We have batch data, so we have a limited knowledge on the game we're going to.",
            "We try to learn from.",
            "And so.",
            "What an epsilon week epsilon Nash equilibrium is is basically for all player or for a certain measure over player.",
            "You try to minimize the insub, nor here is the two norm.",
            "The difference between the Q function of your best response and the key function of the joint policy.",
            "So the idea of this this work is well if I can find a policy such NQ functions such as I have an approximate fixed point for the bellman operator corresponding to the policy and for the bellman operator come corresponding to the best response.",
            "Maybe I can get something that is like fair enough or close to a Nash equilibrium so.",
            "I want this norm to be close to 0 for all player.",
            "Can just do the sum.",
            "And some of our players so.",
            "This is going to be surrogate loss that we're going to try to optimize so it's a one step problem within an implicit key functions you're trying to learn, so we're going to optimize.",
            "This this this sum of reward and trying to get it close to 0."
        ],
        [
            "And what can we guarantee?",
            "So the difference between the key function of the joint policy and the key function of the best response is going to be controlled by this sum of Bellman residuals.",
            "With respect to some constant which depend on the game and on one minus the horizon.",
            "So.",
            "Where we have a sum of normier to minimize.",
            "But remember, we're trying to find a batch scenario, meaning we only know the game through logs of interactions.",
            "So what we're going to do is minimize an empirical estimate of this loss.",
            "Of this sum of Bellman residual with parameterized strategy and parameterized functions."
        ],
        [
            "So I'm not going to go too deep in those estimators, but mainly you can cast it into some supervised learning problem where you just have a ear transformation over your key functions and policies.",
            "Which is here for the estimate of the minimum.",
            "Schedule an the optimal bellman residual.",
            "Here."
        ],
        [
            "So the learning process is now reduced to a minimizing the sum of these empirical behrmann residuals and So what we have is a sum of loss and we want to optimize over a set of parameters.",
            "So we have one policy parameter per policy per player and the parameter of function parallel Q functions per player.",
            "And we want to minimize this loss.",
            "So what we do here is we use in our experiment with stochastic gradient descent and the parameterization of the key function in the policy is done with neural network so."
        ],
        [
            "We did some.",
            "Some experiment on randomly generated Markov games, so we sampled the dynamic of the game.",
            "So here we took a game with a third taking game, meaning only one player, the control on the.",
            "Each state.",
            "And mainly so this is the difference between the value of the policy, the value of the response through the training, normalized by the value of the best response, and we can see the this gap keeps increasing as the training goes on so.",
            "That, but still it took us a lot of time to get it to converge.",
            "But anyway, it performs better than our baseline, which was random policy."
        ],
        [
            "So.",
            "To sum up.",
            "What we did here is defined a new, weaker definition of epsilon Nash equilibrium.",
            "We didn't need it in order to learn.",
            "The our policy.",
            "We replace the problem of learning a Nash equilibrium and confusion by just minimizing a surrogate loss.",
            "And we in particular they should using internal network.",
            "So there is a lot of work that can go out from here like.",
            "So the main the main one is the extension.",
            "Two similar tables, games and two large scale state state space.",
            "There is a lot of work also on the optimization techniques.",
            "This one is not.",
            "Convert slowly and there is a lot of things to catch from the actor critic methods such as taking different timescale on policies or functions.",
            "And I mean we could use.",
            "The goal is to minimize some loss, so we could use a parameterized policy or key function.",
            "But maybe we can use other classes, function approximations such as trees and search for other optimization techniques such as functional gradient descent or.",
            "Or is it the case?",
            "So thanks for your attentions and feel free to ask questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with a flowing struggle.",
                    "label": 0
                },
                {
                    "sent": "It comes from the University of Deal.",
                    "label": 0
                },
                {
                    "sent": "So you heard a lot of talks about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Markov decision processes and reinforcement learning this morning.",
                    "label": 1
                },
                {
                    "sent": "So this is 1 typical case from the Atari and that's it.",
                    "label": 0
                },
                {
                    "sent": "So what is the goal?",
                    "label": 0
                },
                {
                    "sent": "You have one player trying to interact with the environment and.",
                    "label": 0
                },
                {
                    "sent": "OK, so receiving.",
                    "label": 0
                },
                {
                    "sent": "Directing with an environment and receiving a reward as a result of the elections.",
                    "label": 0
                },
                {
                    "sent": "So the goal here is to optimize some long-term sum of rewards, so to find some policy to optimize this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Purities usually for value based algorithm.",
                    "label": 0
                },
                {
                    "sent": "You try to find an optimal key functions, meaning you try to find the sum value.",
                    "label": 1
                },
                {
                    "sent": "Some value of an action considering your state.",
                    "label": 1
                },
                {
                    "sent": "And once you find this function you act greedy according to the function and retrieve a good policy.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What what could we do in this case, like we have we're playing phone, there is two players there, which kind of policy would we like to find?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Some what we could say is reduces to some Markov decision process problem and to say OK I have an agent having a strategy and I try to find.",
                    "label": 0
                },
                {
                    "sent": "A best responsive policy to beating.",
                    "label": 0
                },
                {
                    "sent": "But maybe the agent can change, and maybe you want some more conservative notion of of a Poly.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this can be cast to the framework of Zero 7 two player Markov game.",
                    "label": 0
                },
                {
                    "sent": "And here you might want to find the min Max strategy, meaning you try to find a strategy that would maximize your reward considering the opponent will minimize it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This like so you share both players and a way to interact with the environment they are in the same state space and they receive the same reward and one is trying to maximize it where the other is trying to minimize it.",
                    "label": 1
                },
                {
                    "sent": "So how can we handle this problem, it's?",
                    "label": 0
                },
                {
                    "sent": "It's like for my court decision processes.",
                    "label": 1
                },
                {
                    "sent": "You can find the optimal value of Q function of that game and act greedy according to this Q function.",
                    "label": 0
                },
                {
                    "sent": "But here acting really will not be like maximizing the action, but taking a min Max over this this matrix game.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More complicated so.",
                    "label": 0
                },
                {
                    "sent": "What could we do in this case where we have 4 four agents?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Which kind of notion of optimality would we like to find?",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "The point here is you could also take it as a marketing process.",
                    "label": 0
                },
                {
                    "sent": "Both every player would be with some policy and try to find the best response to this.",
                    "label": 0
                },
                {
                    "sent": "To those policies of the other players.",
                    "label": 0
                },
                {
                    "sent": "But I mean those can adapt and of their own objectives.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's not the notion you want to.",
                    "label": 0
                },
                {
                    "sent": "You want to find?",
                    "label": 0
                },
                {
                    "sent": "You could cast it as a kind of a 0 sum market game, considers that every player.",
                    "label": 0
                },
                {
                    "sent": "Or are you playing some worst case scenario and are going to?",
                    "label": 0
                },
                {
                    "sent": "Try to beat you and kiss it this way, but maybe you want to team apart with this guy.",
                    "label": 0
                },
                {
                    "sent": "Always this guy and this way you too.",
                    "label": 0
                },
                {
                    "sent": "This is a way to increase your world so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's up in, uh, this is more like Mplayer general submarket game, so each player receives his own reward signal, and what you would like to find the Nash equilibrium.",
                    "label": 1
                },
                {
                    "sent": "So you should consider that you play with independent strategy.",
                    "label": 0
                },
                {
                    "sent": "That the notion is a game theoretic notion that you want to find.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm sure you've heard about the prisoners dilemma.",
                    "label": 0
                },
                {
                    "sent": "So what is a Nash equilibrium?",
                    "label": 1
                },
                {
                    "sent": "It's you want to find a strategy for each player.",
                    "label": 0
                },
                {
                    "sent": "Such as no player would benefit from modifying their current strategy.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean, could we work on the key function as in previous previous examples?",
                    "label": 1
                },
                {
                    "sent": "The answer is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, you cannot retrieve a Nash equilibrium from the function, which is a key function of the Nash equilibrium, so that's that's one surprise there is not enough information in the key function to retrieve a good strategy.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's kind of a problem, since every batch reinforcement learning based algorithm.",
                    "label": 1
                },
                {
                    "sent": "Only work on key functions and only rely on this key function, so there is a gap between.",
                    "label": 0
                },
                {
                    "sent": "Going from batch, they only in zero center player Market Games and or MVP's, two generals and lack of games.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our goal here is going to be to find epsilon Nash equilibrium, so it's a little weaker notion of optimality for in N player general submarket games from historical later with function approximation.",
                    "label": 0
                },
                {
                    "sent": "So as I said, there were a lot of work in Markov decision processes and EPS, such as the some algorithms such as fitted to iteration from else or no GPU.",
                    "label": 0
                },
                {
                    "sent": "Another algorithm which is up more of the policy iteration like algorithm is that works with much data or we love work on extending those algorithm on the two player 0 sum markup game setting.",
                    "label": 0
                },
                {
                    "sent": "But this is not enough to go to an player general some games.",
                    "label": 0
                },
                {
                    "sent": "So to learn Nash equilibrium in general, some Markov games there are there is some work, but it's limited to the online case online learning case.",
                    "label": 1
                },
                {
                    "sent": "Or to model based area.",
                    "label": 0
                },
                {
                    "sent": "Meaning you have the model.",
                    "label": 0
                },
                {
                    "sent": "What about learning it?",
                    "label": 0
                },
                {
                    "sent": "Sorry about learning it and other algorithms that are not to learn.",
                    "label": 0
                },
                {
                    "sent": "For SEO approaches here, it's more of an actor critic.",
                    "label": 0
                },
                {
                    "sent": "Methods for general Markov games.",
                    "label": 0
                },
                {
                    "sent": "But still, I'm not.",
                    "label": 0
                },
                {
                    "sent": "It's difficult to extend it to the batch case where you have samples.",
                    "label": 0
                },
                {
                    "sent": "Difficult in theory.",
                    "label": 0
                },
                {
                    "sent": "I I'm not.",
                    "label": 0
                },
                {
                    "sent": "I don't know algorithms that work.",
                    "label": 0
                },
                {
                    "sent": "Actor critic methods that work for the batch case.",
                    "label": 0
                },
                {
                    "sent": "If I.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What we do is we define a weaker notion of Nash equilibrium and we reduce the problem of learning.",
                    "label": 1
                },
                {
                    "sent": "And Ashley Graham, two mutation of a surrogate loss, which is a sum of Bellman residual and then we perform an empirical evaluation of this this method on winner in network as a function of.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summation, so a Markov game is specified by end players a state space.",
                    "label": 1
                },
                {
                    "sent": "And action space per player, meaning we have each player has is an action space and the dynamic of the game is going to be.",
                    "label": 0
                },
                {
                    "sent": "Will depend on the joint action of old players.",
                    "label": 0
                },
                {
                    "sent": "Each player will have an influence on the dynamic, so for the rest of the talk I will note like a -- 8 like the action of every other player and actions involved, which is a joint action of old player.",
                    "label": 1
                },
                {
                    "sent": "So just for reservation.",
                    "label": 0
                },
                {
                    "sent": "So each player receives his reward signal.",
                    "label": 0
                },
                {
                    "sent": "And we will work on that medicine case.",
                    "label": 0
                },
                {
                    "sent": "So we needed this connector.",
                    "label": 1
                },
                {
                    "sent": "So the goal is to find a strategy for each player considering batch samples.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as you see in this morning the differences between the online scenario in the batch scenarios that we do not allow ourselves to re sample mean we only have logs of interactions with the.",
                    "label": 0
                },
                {
                    "sent": "With the from the game, it's convenient in situations such as an industry where you have lots of interactions and you want to retrieve the strategy from those and.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's hard to convince somebody to let you try your policy on a real system and to collect more data.",
                    "label": 0
                },
                {
                    "sent": "This might be sensitive and you'd better work on logs and other algorithms that work on logs of data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can still define key functions, or at least a key function according to your policy.",
                    "label": 0
                },
                {
                    "sent": "So what is it in that case?",
                    "label": 0
                },
                {
                    "sent": "So the state action value function is simply considered that you start each player play action the joint action A and then that every player follow their own strategy.",
                    "label": 1
                },
                {
                    "sent": "So this is going to be the joint, the value of the joint policy of every player and as in.",
                    "label": 0
                },
                {
                    "sent": "As in the Markov chain processes, this is a.",
                    "label": 0
                },
                {
                    "sent": "You can find the bellman operator and this Q function is going to be a fixed point of.",
                    "label": 1
                },
                {
                    "sent": "This Belmont prater.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the other key for key function we're going to is the best response of player I.",
                    "label": 1
                },
                {
                    "sent": "So imagine you fix the policy of all other players.",
                    "label": 0
                },
                {
                    "sent": "And this is a Markov decision problem and you want to find an optimal policy according to that Mark of decision problem.",
                    "label": 0
                },
                {
                    "sent": "So you it simply is a maximum of all your policy considering the policy of the other player is fixed for that function and it's also fixed point of this bellman operator and it satisfied the fixed point equation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is the Nash equilibrium in the definition?",
                    "label": 0
                },
                {
                    "sent": "So player has no incentive to modify this current strategy, meaning the key function of his best response equals the key function of the joint policy.",
                    "label": 0
                },
                {
                    "sent": "Forward player, right.",
                    "label": 0
                },
                {
                    "sent": "And if you rewrite it in terms of Bellman equations, you just need to find.",
                    "label": 0
                },
                {
                    "sent": "Joint strategy.",
                    "label": 0
                },
                {
                    "sent": "Such that there exists on key functions.",
                    "label": 0
                },
                {
                    "sent": "Function for each player that is simultaneously the fixed point of this bellman operator and the fixed point of this operator.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we're not going to learn the optimal control here.",
                    "label": 0
                },
                {
                    "sent": "We have batch data, so we have a limited knowledge on the game we're going to.",
                    "label": 0
                },
                {
                    "sent": "We try to learn from.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "What an epsilon week epsilon Nash equilibrium is is basically for all player or for a certain measure over player.",
                    "label": 0
                },
                {
                    "sent": "You try to minimize the insub, nor here is the two norm.",
                    "label": 0
                },
                {
                    "sent": "The difference between the Q function of your best response and the key function of the joint policy.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this this work is well if I can find a policy such NQ functions such as I have an approximate fixed point for the bellman operator corresponding to the policy and for the bellman operator come corresponding to the best response.",
                    "label": 1
                },
                {
                    "sent": "Maybe I can get something that is like fair enough or close to a Nash equilibrium so.",
                    "label": 0
                },
                {
                    "sent": "I want this norm to be close to 0 for all player.",
                    "label": 0
                },
                {
                    "sent": "Can just do the sum.",
                    "label": 0
                },
                {
                    "sent": "And some of our players so.",
                    "label": 0
                },
                {
                    "sent": "This is going to be surrogate loss that we're going to try to optimize so it's a one step problem within an implicit key functions you're trying to learn, so we're going to optimize.",
                    "label": 0
                },
                {
                    "sent": "This this this sum of reward and trying to get it close to 0.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what can we guarantee?",
                    "label": 1
                },
                {
                    "sent": "So the difference between the key function of the joint policy and the key function of the best response is going to be controlled by this sum of Bellman residuals.",
                    "label": 0
                },
                {
                    "sent": "With respect to some constant which depend on the game and on one minus the horizon.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Where we have a sum of normier to minimize.",
                    "label": 0
                },
                {
                    "sent": "But remember, we're trying to find a batch scenario, meaning we only know the game through logs of interactions.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is minimize an empirical estimate of this loss.",
                    "label": 1
                },
                {
                    "sent": "Of this sum of Bellman residual with parameterized strategy and parameterized functions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm not going to go too deep in those estimators, but mainly you can cast it into some supervised learning problem where you just have a ear transformation over your key functions and policies.",
                    "label": 0
                },
                {
                    "sent": "Which is here for the estimate of the minimum.",
                    "label": 0
                },
                {
                    "sent": "Schedule an the optimal bellman residual.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the learning process is now reduced to a minimizing the sum of these empirical behrmann residuals and So what we have is a sum of loss and we want to optimize over a set of parameters.",
                    "label": 1
                },
                {
                    "sent": "So we have one policy parameter per policy per player and the parameter of function parallel Q functions per player.",
                    "label": 0
                },
                {
                    "sent": "And we want to minimize this loss.",
                    "label": 0
                },
                {
                    "sent": "So what we do here is we use in our experiment with stochastic gradient descent and the parameterization of the key function in the policy is done with neural network so.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We did some.",
                    "label": 0
                },
                {
                    "sent": "Some experiment on randomly generated Markov games, so we sampled the dynamic of the game.",
                    "label": 1
                },
                {
                    "sent": "So here we took a game with a third taking game, meaning only one player, the control on the.",
                    "label": 0
                },
                {
                    "sent": "Each state.",
                    "label": 1
                },
                {
                    "sent": "And mainly so this is the difference between the value of the policy, the value of the response through the training, normalized by the value of the best response, and we can see the this gap keeps increasing as the training goes on so.",
                    "label": 1
                },
                {
                    "sent": "That, but still it took us a lot of time to get it to converge.",
                    "label": 0
                },
                {
                    "sent": "But anyway, it performs better than our baseline, which was random policy.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To sum up.",
                    "label": 0
                },
                {
                    "sent": "What we did here is defined a new, weaker definition of epsilon Nash equilibrium.",
                    "label": 1
                },
                {
                    "sent": "We didn't need it in order to learn.",
                    "label": 1
                },
                {
                    "sent": "The our policy.",
                    "label": 0
                },
                {
                    "sent": "We replace the problem of learning a Nash equilibrium and confusion by just minimizing a surrogate loss.",
                    "label": 0
                },
                {
                    "sent": "And we in particular they should using internal network.",
                    "label": 0
                },
                {
                    "sent": "So there is a lot of work that can go out from here like.",
                    "label": 1
                },
                {
                    "sent": "So the main the main one is the extension.",
                    "label": 0
                },
                {
                    "sent": "Two similar tables, games and two large scale state state space.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of work also on the optimization techniques.",
                    "label": 0
                },
                {
                    "sent": "This one is not.",
                    "label": 0
                },
                {
                    "sent": "Convert slowly and there is a lot of things to catch from the actor critic methods such as taking different timescale on policies or functions.",
                    "label": 1
                },
                {
                    "sent": "And I mean we could use.",
                    "label": 0
                },
                {
                    "sent": "The goal is to minimize some loss, so we could use a parameterized policy or key function.",
                    "label": 1
                },
                {
                    "sent": "But maybe we can use other classes, function approximations such as trees and search for other optimization techniques such as functional gradient descent or.",
                    "label": 0
                },
                {
                    "sent": "Or is it the case?",
                    "label": 0
                },
                {
                    "sent": "So thanks for your attentions and feel free to ask questions.",
                    "label": 0
                }
            ]
        }
    }
}