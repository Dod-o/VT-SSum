{
    "id": "ogz4trlszi5gghxgzjm4ptwyjjhvefor",
    "title": "Bayesian Modelling",
    "info": {
        "author": [
            "Zoubin Ghahramani, Department of Engineering, University of Cambridge"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2012_ghahramani_bayesian_modelling/",
    "segmentation": [
        [
            "OK, I've been losing my voice over the last few days, so.",
            "Hopefully by the end of the two hours I'll still have some voice, but the magic microphones will help out that way.",
            "OK, I'm going to be talking about Bayesian modeling and first of all, I just want to mention I put the slides online just right now it's mlg.nj.com.ac.uk/zubin talks, lect, onebase.pdf.",
            "So if you all download it now, then nobody will be able to use the Internet, probably for the rest of the day, OK?",
            "So I'm going to be talking about Bayesian modeling an I'm I know that you've had quite a lot of advanced materials so far, but.",
            "So especially following from yesterday talking about Kingman's coalescent and things like that, it's going to seem very basic.",
            "What I'm going to say, but I think it's still very important to start from the basics and try to really understand what's going on.",
            "Why do you know Bayesian machine learning?",
            "People do things the way that they do.",
            "What's the philosophy behind it?",
            "What is the formal theoretical foundations behind it and?",
            "How does it relate to the rest of machine learning?",
            "So in particular, I'm talking about Bayesian modeling and I want to focus on these two words.",
            "Modeling and Bayesian for at the beginning and try to just very clearly describe to you what I mean by these things.",
            "And then I'll talk about more specifics and more and more technical material as I go on through.",
            "I think I have maybe 6 hours of lectures.",
            "So.",
            "This usually happens."
        ],
        [
            "OK, so just as motivation, it's a really exciting time to be doing machine learning or modeling.",
            "We're in the middle of an information revolution.",
            "There's huge amounts of data everywhere affecting society.",
            "The way Sciences are done, the way business is done so you know, all of us will hopefully have nice jobs because there's a lot of demand for what we are trying to do.",
            "But of course with huge amounts of data, we really need tools for dealing with that data, and it's not.",
            "I'm not going to focus on the largeness.",
            "Quantity of tools of the data.",
            "I'm going to focus on the interesting things that we can do with that data, so we need tools for modeling, searching, visualizing, and understanding large datasets."
        ],
        [
            "And, uh, our modeling tools need to, I feel, be able to do certain things.",
            "The modeling tools should be able to capture some form of uncertainty about our model structure.",
            "They should be able to capture uncertainty in the in the data.",
            "The noise processes in the data data, especially stuff that you get nowadays on the web, isn't very clean experimental data.",
            "It's incredibly messy.",
            "We want tools that are automated that adapt that are robust, that scale well, etc."
        ],
        [
            "Now the framework I'm going to be talking about is probabilistic modeling framework, so let me focus on the word model.",
            "What's a model to me?",
            "Well, to me, a model is a description of possible data one could observe from a system.",
            "OK.",
            "So funny way of describing a model, but I've come to sort of like this way of thinking about models.",
            "So model is a way of describing possible data you could observe.",
            "In any kind of domain you're trying to model.",
            "And if it's not that, then I don't understand what a model could be OK if a model can't predict data, then there's no way of falsifying a model.",
            "There's no way of telling whether model is good or bad, because ultimately we have to evaluate models with respect to our real world data.",
            "Of course, models serve other purposes.",
            "Models might also be much more interpretable or understandable than the raw data, and those are kind of side benefits of models, but ultimately their descriptions of possible data one could observe.",
            "Now the framework we're going to take is to use the mathematics of probability theory to express all the forms of uncertainty and noise that we have in our model.",
            "If we do that, the nice thing is then we can use the same rules of probability theory.",
            "What used to be called?",
            "You know, over 100 years ago, inverse probability or Bayes rule, but it's basically the application of the rules of probability theory to allow us to infer unknown quantities, adapt our models, learn parameters, learn from data, etc, make predictions.",
            "Etc.",
            "Now I want to.",
            "I want to draw an analogy for you.",
            "You know a few 100 years ago, Newton and Leibnitz, and people like that developed calculus.",
            "Calculus was a mathematical language for talking about rates of change.",
            "Probability theory is a mathematical language for talking about uncertainty.",
            "OK, so it's basically our way of expressing uncertainty."
        ],
        [
            "OK, so this is the probabilistic modeling framework that I'm going to take, and you know you've seen this stuff to death, but basically.",
            "You know that's probably not base as Neil said.",
            "I think in his talk there's some arguments.",
            "It's the only image of Bayes that anybody claims could be of base.",
            "But there's some dispute about it.",
            "Um?",
            "The famous Bayes rules just a basic rule of probability theory.",
            "But when you use terms like hypothesis and data, you can write it down in this form that says it's a way of turning uncertainty about some set of hypothesis before observing the data.",
            "That's the prior is there it?",
            "Does anybody have pointer?",
            "OK, this looks suspiciously like a pointer.",
            "OK, this looks like Swiss issues like pointer that doesn't work, sorry.",
            "Oh yeah.",
            "Wow, I'm gonna I'm gonna be in pain.",
            "OK yeah alright get stronger overtime.",
            "OK, here's a mouse that's easier.",
            "OK, so the Bayes rule tells us how to turn our uncertainty about a bunch of hypothesis before observing the data.",
            "That's the prior into our uncertainty about the hypothesis.",
            "After observing the data and the key ingredient that we need to be able to do this transformation is this likelihood.",
            "Which basically says that every hypothesis has to be able to give probabilities to data.",
            "So hypothesis is only well formulated if it can actually assign a probability to your observed data.",
            "Let me just elaborate on this point.",
            "Let's say my hypothesis was that the relationship between X&Y is a linear regression.",
            "That's not a well formulated hypothesis.",
            "OK, because I can't, I can't assign probabilities data if I say it's a linear regression, let's say with Gaussian noise, that's still not a well formulated hypothesis because I haven't told you what range the parameters can take.",
            "OK, a linear regression that can take parameter ranges between, let's say, slopes between minus one and one is a completely different model than a linear regression that can take slopes between minus 100 and 100.",
            "OK, so if I tediously and laboriously write down all of my assumptions, let's say it's a linear regression model, my soaps are going to arrange my parameters are going to range between minus 100 and 100.",
            "I'm going to give a uniform distribution to those parameters.",
            "My nose is going to be Gaussian with some variance Sigma squared, that Sigma squared is going to come from an exponential distribution, etc.",
            "I write down all of those assumptions at some point Bingo, I have a model.",
            "That actually could predict data, and only at that point am I going to call that a model.",
            "OK before that, it's an incomplete model.",
            "So if you ask, for example, an you some of you will already be thinking where does the prior come from.",
            "It's the same as, where does the likelihood come from?",
            "It's a part of the model.",
            "If you don't specify as a Bayesian if you don't specify all the parts, it's like building a car without all the components.",
            "It won't drive until you put all the components on it OK.",
            "So Bayes rule tells us how to do inference about hypothesis from data learning.",
            "Prediction can be seen as forms of inference."
        ],
        [
            "Will elaborate on this a lot more, so let me contrast 2 views of machine learning.",
            "The view that I'm going to be kind of mostly talking about is this goal of machine learning as learning models of data.",
            "So we define a space of possible models.",
            "We learn the parameters and structure of these models from data.",
            "We make predictions and decisions.",
            "The alternative, and I would say perhaps dominant view of machine learning is.",
            "That machine learning is a toolbox of methods for processing data that's maybe not a very nice way of putting it, but I think that's essentially how a lot of practitioners out there in the real world end up using machine learning is they download.",
            "They have some data there like I need to do some clustering, they download a tool for doing clustering.",
            "They run it on their algorithm, they process their data in some way, and they're going to.",
            "Then they say, OK, what do I do now with the clusters, OK?",
            "You feed the data into many possible one of many possible methods.",
            "You choose methods.",
            "Of course you know people are very clever.",
            "There are good and bad methods and you choose methods that have excellent theoretical properties or excellent empirical properties, and that's a good way of getting a lot of stuff done.",
            "You can make predictions, decisions that way as well."
        ],
        [
            "Let's see if this works OK, so here is my plan for for today for at least these first 2 hours I'm going to talk about some of the foundations I'm going to talk about intractability and I'm going to give a brief overview of approximation tools.",
            "Now, again, this is all a bit backwards.",
            "You've heard a lot about Hamiltonian Monte Carlo, right?",
            "But I'm going to just give you the overview and maybe you can slot some of what you've learned into the bigger picture.",
            "Unless you probably already had that in your head.",
            "Then I'm going to talk about some advanced topics and some limitations and discussions because I think it's also very important to place the Bayesian framework alongside the Classical an optimization based frameworks and say what are the advantages and disadvantages.",
            "OK."
        ],
        [
            "So in more detail.",
            "This is sort of the detailed outline of what I'm going to talk about.",
            "I'm I'm going to skip parts of this.",
            "This is actually vaguely based on a nice email tutorial I gave a bunch of years ago, but it's evolved overtime and I've edited some things out basically overtime."
        ],
        [
            "OK, so just to be on the same page, I'm going to introduce some very Canonical machine learning problems and just show them to you in, you know, unified notate."
        ],
        [
            "So here is linear classification.",
            "I have some end data points points in some plane and their class label plus or minus one.",
            "I want to learn a linear boundary.",
            "I write down a model which in this case is a hard classifier, but written as a likelihood function.",
            "That's fine, OK, I can write a likelihood function.",
            "This deterministic in this particular case, it's a linear classifier.",
            "We have some parameters Theta and.",
            "Our goal.",
            "Is going to be to infer these parameters data from the data so as to be able to generalize to predict future data OK?"
        ],
        [
            "Nope.",
            "So here is polynomial regression.",
            "Again, we have a bunch of data points.",
            "Pairs of X is and wise, our model is going to be this polynomial with some noise and I have to specify some.",
            "Distribution for the noise.",
            "So let's make that Gaussian.",
            "In the simplest case, and we have some parameters, our goal is going to be done for the parameters data from the data so as to predict future outputs."
        ],
        [
            "OK, and here is clustering.",
            "With Gaussian mixture models.",
            "Or you could think of it also is a density estimation problem.",
            "We have a bunch of data point these blue dots.",
            "We have a model which is a.",
            "In this case, the finite mixture model with M components.",
            "Each of these, let's say, is Gaussian with each component is a Gaussian with some mean and some covariance matrix denoted as these... And you have some mixing coefficients for the relative sizes or masses of these Gaussians.",
            "Now you can write down your parameters and again our goal is going to be to infer the parameters data from the data, so it's to predict either.",
            "For example, the density of new data points, or to infer whether two points belong to the same cluster or not OK.",
            "So any questions about that?"
        ],
        [
            "OK.",
            "So the Bayesian framework to machine learning says just basically apply the rules of probability theory.",
            "OK, that's it.",
            "That's essentially all we need to do.",
            "Well, first of all, you know there's a couple of more caveats here.",
            "First of all, write down everything you know using the language of probability theory on certainty as probabilities.",
            "Then use the rules of probability theory on your data.",
            "OK, so the good news is.",
            "You don't have to memorize a lot of stuff, we only have to know 2 rules.",
            "Essentially, the sum rule in the product rule.",
            "Some rule says the probability of X can be written as the sum or integral if Y is continuous of the joint probability of X&Y.",
            "The product rule says that the joint probability of X&Y can be decomposed into the probability of X times the probability of Y given X or the other way round, OK.",
            "So it's great for people like me.",
            "I can't memorize a lot of things if I'm if I'm.",
            "In the middle of a derivation and I'm confused what to do.",
            "I just sit back and say, alright, I just have to follow the summer on the product rule, OK?",
            "It's true you can do incredible things with the sum rule in the product rule, belief propagation.",
            "How many people here have studied belief propagation?",
            "OK, it's just the sum rule in the product rule.",
            "OK, it's just done that on a graph.",
            "Alright.",
            "So.",
            "You know the same with Kalman filtering.",
            "I took a two semester class.",
            "On Kalman filtering.",
            "If they had just told me it's the summer on the product rule for Gaussians, it would have saved me a lot of time.",
            "OK, so so from this summer in the product rule we get Bayes rule.",
            "Obviously by just applying these two rules here.",
            "And I just replace the symbols X&Y with Theta and D to represent data being the parameters of our model and D being the data OK.",
            "So that's the prior.",
            "This is the likelihood function.",
            "This is called the marginal likelihood.",
            "I'll come back to it later and this is the posterior over the parameters given the data.",
            "Now.",
            "If we want to do prediction, let's say what does prediction mean?",
            "Prediction means.",
            "Assume I'm in some particular model.",
            "I've observed some data D. And I want to be able to predict new data points X.",
            "So the question is, what is the probability of a new data point X given the observed data D and our current model assumption?",
            "Well, find out I apply the same rule in the product rule.",
            "Basically I can write that as a sum or integral over all possible parameters Theta of the product of.",
            "The predictions for that particular parameter times the posterior over the parameters given the data.",
            "And now.",
            "This just looks like boring math, but it actually is very, very intuitive and very nice with this.",
            "Basically says is if I want to know the predictions of my model.",
            "What I do is I take the predictions for any particular parameter and I average those predictions with respect to my posterior distribution over the parameters.",
            "So here it is.",
            "This is like some rule product rule giving you the kind of idea of ensemble learning methods, which is that you're averaging your predictions over multiple models.",
            "OK, I have more to say about that.",
            "It's more subtle if you push that to the end, but essentially this is what is going on here.",
            "Averaging the key idea in Bayesian methods is if you don't know something average over it, OK?",
            "Now a model comparison again also follows from this.",
            "If I wanted the know, the probability of a model given the data, I could apply these rules.",
            "The key quantity here is what's called the marginal likelihood.",
            "This term here and that is the integral of over the parameters of the likelihood times the prior.",
            "Integrate with respect to the parameters.",
            "So."
        ],
        [
            "So let's.",
            "Just step back.",
            "That's it.",
            "OK really.",
            "You know the rest of the stuff is.",
            "Well, what's left, OK?",
            "So do you have any questions?",
            "Should we stop there?",
            "It's not good weather to go to the beach I guess, so maybe we could stay a little longer.",
            "I don't know.",
            "Any questions?",
            "Maybe I meant I'm going to answer.",
            "I do have more material.",
            "But I want to see if I could pre empt some of that.",
            "You're all sold.",
            "Has it been a long week?",
            "Yeah.",
            "I'm not going on until somebody asked the question.",
            "Who's going to be the brave one?",
            "The previous slide prompt questions."
        ],
        [
            "That's not a question.",
            "Request no.",
            "OK question excellent.",
            "Mining and machine learning.",
            "OK, that's a good question.",
            "Science is made of people, and people tend to form communities.",
            "So I I came into this field from a particular community.",
            "I used to go for example to the NIPS conference, because when I started my PhD, the really exciting thing to me was neural networks.",
            "I'm afraid to admit.",
            "And you know our community people Mike Jordan and Jeff Hinton.",
            "And you know I'm Neil Lawrence and myself.",
            "You know we we were, you know we can't.",
            "We came in.",
            "I think if interested in or were you interested in all networks and I wanted to know how to break.",
            "I went to the extent of actually getting a PhD in neuroscience 'cause I was interested in how the brain works.",
            "I don't care how the brain works anymore.",
            "This is much more fun to do machine learning.",
            "Anyway, so there was a community that came into the field through that route.",
            "There was a community that came into the field from databases where they were had lots of databases and they wanted to do interesting operations on those databases.",
            "There are more computer science in their thinking, so they're more the data mining community community there is.",
            "The uncertainty in artificial intelligence community which came at it from graphical models.",
            "There is the ICL community which is machine.",
            "AI people who wanted to do machine learning with rule based machine learning originally.",
            "And the interesting thing is that.",
            "Well, and then there's the statisticians.",
            "Let's not forget them, OK?",
            "The interesting thing is that all of these ships were on a collision course.",
            "OK, so now they're doing much more similar things, but when you go into a data mining conference, the flavor of things that you get is much more historically based on the idea of big databases.",
            "Large scale but simple forms of machine learning.",
            "OK. Yeah.",
            "That's one question I can move on.",
            "I don't need two questions.",
            "Go ahead for model comparison.",
            "You assume that you can integrate the models, right?",
            "Yeah, what does it mean exactly?",
            "How do you define integrating out?",
            "OK, this is a very good question.",
            "I'll come to this in a minute in model comparison here.",
            "At least if I want to compare two models, would I need to be able to do is integrate out the parameters for those two models?",
            "OK, but if I have a larger space of models, then one of the tricky things about Bayesian inference is that I need to be able to somehow enumerate.",
            "Either implicitly or explicitly, all the models I'm willing to consider.",
            "This is sometimes called the closed world hypothesis.",
            "Basically, you know.",
            "I have to somehow specify a generative process over models now that could be very sophisticated.",
            "For example, you could have a grammar over models that would generate more and more complicated models as long as you're the probability mass that you associate to all possible models in your in your closed world or universe of models sums to one or at most one.",
            "So it's a very good question actually.",
            "Alright, yeah, another question.",
            "Likelihood of three times the next thing you know, the probability of data so.",
            "Can I interchange likelihood and probability?",
            "OK, I haven't.",
            "I use the symbol P uniformly on this page OK, which means there are probabilities.",
            "This is the probability of the data given Theta.",
            "A lot of people, confusingly say the likelihood of the data because it you know they use the word likelihood colloquially for probability.",
            "Formally, if you look at any statistics textbook, this is the likelihood of Theta, which is.",
            "It's a function of Theta.",
            "That's not a probability function, OK?",
            "But you know, Alternatively, for a Bayesian, the likelihood of Theta is just the probability of the data given Theta.",
            "Sorry.",
            "Prior likelihood of the time to say prior probability is it.",
            "Yeah it's it's this is this is a probability over theater.",
            "Low likelihood is not a probability over Theta.",
            "The likelihood is not a probability distribution.",
            "OK. OK."
        ],
        [
            "Alright, let me move on.",
            "OK so that's it, but it's not to."
        ],
        [
            "Yeah, so here are the questions I want to address.",
            "So first of all you know what are the?",
            "What are the foundations of bayesianism?",
            "Why are some people using this basic methodology?",
            "I actually think it's weird.",
            "I don't like the term Bayesian, it makes it sound different somehow.",
            "Like it's some choice.",
            "You know, we don't say why are we all calculation Ists right?",
            "Because we use calculus.",
            "You know, I, I think that this basically you know this comes down to why are we using probability theory to answer questions from probability theory.",
            "Well, you know what else would we use right?",
            "Of course there is the question.",
            "Are there other ways of doing data analysis and modeling?",
            "And that's certainly true, right?",
            "There are many other ways of doing data analysis and machine learning.",
            "So a lot of people say, you know, are worried about this question.",
            "Where does the prior come from?",
            "I'll try to talk about that and then how do we do these integrals?",
            "This is the practical question, OK?",
            "So here is one way at that you could come."
        ],
        [
            "At the Bayesian framework, and this is if you want to think about artificial intelligence problems.",
            "So imagine you have a robot and in order for this robot to behave intelligently, it needs to be able to represent its beliefs about some propositions in the world.",
            "Like you know, where is the you know where is this charging location?",
            "Or is this sensor malfunctioning or something like that?",
            "Is it about to get shot whatever?",
            "Now.",
            "Uh.",
            "The question is now not a question about statistics at all.",
            "It's a question about how does the robot represent beliefs and we want a framework that we can use to represent the strength of these beliefs numerically in the brain of the robot.",
            "And we want to know the mathematical rules for manipulating these beliefs.",
            "So it's basically like you know the rules of logic right?",
            "But extended to."
        ],
        [
            "You capture strength of belief.",
            "So let's let's start with trying to describe what we mean a little bit.",
            "So let's say B of X represents the strength of belief or possibility in some Proposition X, and we're going to say it ranges between zero and one where B of X = 0 means that the robot believes X is definitely not true.",
            "That doesn't have to mean that X is definitely not true.",
            "That's just a statement about the robots belief B of X = 1 means that robot believes that X is definitely true, and we're going to use B of X given Y.",
            "To represent the strength of belief that X is true, given that we know that Y is true.",
            "OK.",
            "So now we want to be able to manipulate these beliefs an if.",
            "If you accept certain axioms and these go all the way back to Cox in 1946, and I've been sort of developed overtime by Jaynes, and there was a really nice review, more recently by Fat Horn.",
            "So it goes along these lines if we're going to say strengths of beliefs or degrees of possibility are represented by real numbers, and we want some qualitative correspondence to common sense in terms of how we manipulate these things, and we want some sense of consistency so that if if the conclusion can be reason is several ways, for example, then each way should lead to the same answer.",
            "It doesn't matter which order you apply the operations in, you still get the same answer.",
            "And the robot is not allowed to just arbitrarily ignore relevant information and equivalent states of knowledge have to be represented with equivalent plausibility statements.",
            "Then it turns out these belief functions must satisfy the rules of probability theory.",
            "In the sum rule, the product rule, an Bayes rule that follows.",
            "So here's a kind of interesting sideways approach to arriving at.",
            "Probability theory it's a language for representing beliefs of rational agents, OK?",
            "It extends logic to strengths of beliefs.",
            "Questions about that."
        ],
        [
            "So here is a different way of approaching the problem that motivates probability theory.",
            "This is kind of a gambling or economics approach.",
            "It comes in the form of the Dutch book theorem.",
            "Which I think it definitely even talked about.",
            "Two is sort of one of the founders of Bayesian statistics, so assume you're willing to accept bets with odds proportional to the strength of your belief.",
            "So now the way we're going to assess the strength of your beliefs beliefs is that we're going to make you gamble like you know, is it.",
            "Is it Real Madrid and Barcelona they're playing tonight.",
            "No real.",
            "Oh darn, OK. Alright, next week so you could have beliefs about who you think is going to win, right?",
            "And you might be willing to bet on the basis of those beliefs.",
            "So if you believe that REL is going to win with strength of belief, .9 with that means is that you'd be willing to accept 9 to 1 odds or better.",
            "OK.",
            "So here's the interesting fact.",
            "If you have a set of different beliefs, not one belief, but a set of different beliefs.",
            "Then, unless these different beliefs are coherent with each other in that they satisfy the rules of probability theory.",
            "OK, the most trivial one would be my belief in X and my belief and not X should sum to one for example, but things along those lines.",
            "Then, unless your belief satisfy the rules of probability theory, there exists some set of simultaneous bets called a Dutch book which you are willing to accept.",
            "For which you're guaranteed to lose money no matter what the outcome, OK?",
            "This isn't about losing money, on average.",
            "This is about a sure loss, and the only way to guard from that is for your beliefs to be coherent with the rules of probability theory.",
            "OK, so this is kind of an economics or Yep.",
            "By confidence values are ranges.",
            "In our.",
            "In this setting, no.",
            "In the sense that you know if you if you have a range over something like between .7 and .8.",
            "Then there is no clear way of betting on that in this framework.",
            "So, so you need to basically.",
            "You need to commit, and actually in a betting scenario it's much more.",
            "Intuitive to to exact to figure out what somebody's beliefs are.",
            "OK, because you know that if you ask someone what are your beliefs, they might not know and then you say OK, I'll give you 10 to 1 odds and they'll say no.",
            "No, you say 9 to 1 eight to one whatever and then at some point they're willing to accept the Beth Ann.",
            "That's one way of figuring out what that person's beliefs are, but of course, as the model of humans is quite flawed because humans are not coherent.",
            "For example, you know lotteries would never work if humans were totally coherent, right?",
            "Maybe I don't know.",
            "In general, humans are not very coherent, and so it's not necessarily.",
            "It's a model of what humans should be.",
            "We want our robots.",
            "We don't want our robots to be humans.",
            "We want to robots to be rational.",
            "OK, they should be more rational than humans.",
            "Um?",
            "Anyway, so this is just a model of economic rationality, which is a bit idealized.",
            "Of course.",
            "Yeah, just having megabits could also base your belief.",
            "Sorry.",
            "Do.",
            "It could affect your belief you mean.",
            "That's true.",
            "Yeah, that's true.",
            "So I think in in the psychology literature it's certainly the case that once you've made the bet, you believe that thing much, much more than before you made the bet, which is not rational, because the bet actually gave you no information.",
            "Right, OK?",
            "So this is a Dutch book theorem argument for an economic argument for the use of probability theory to represent beliefs.",
            "So let's talk about some of the theoretical properties of."
        ],
        [
            "Of Bayesian methods, so if you're worried about, let's say, let's come back to the realm of statistics a little bit, imagine the following scenario.",
            "You have a data set DN of N data points and it was generated by some model with some true parameter Theta star OK. Now, of course from a base in point of view, I'm perfectly happy to accept that there is a true parameter out there.",
            "Bayesianism doesn't mean I believe the parameters random.",
            "OK, that's just a really bad way of very incorrect way of describing things.",
            "I can believe there is a true parameter out there, for example, that parameter could be the number of people in this room that I'm trying to infer.",
            "There's some true number of people in this room, OK, but I'm uncertain about that, so I'm going to represent my uncertainty by probability distribution.",
            "So consider the following statistics kind of scenario where I have a data set DN consisting of N data points.",
            "It was generated from some true parameter Theta star.",
            "Then, under some regularity conditions, and that's careful.",
            "The footnote says careful with the regularity conditions, these are just sketches of the theoretical results.",
            "As long as I put to be more formal here, I put some positive probability mass around Theta star.",
            "OK.",
            "The in this I've written it as the density of Theta star is greater than zero, but what I really mean is not just the density of data stars greater than zero, but there is some probability mass around Theta star apriori.",
            "Then the limit as the amount of data goes to Infinity of the posterior is a Delta function centered around Theta star.",
            "So what does this mean?",
            "This means that.",
            "As long as I put some probability mass around all reasonable Theta stars in the limit, my posterior will converge to a Delta function.",
            "So what are the regularity?",
            "What are the conditions that we need to worry about?",
            "I won't go into details, but one of the conditions is, for example, identify ability of my model.",
            "I need to be able to make sure there are multiple different thetas that are all equivalent to each other, for example, OK. All the thetas could be uniquely identified, so these are similar to the regularity conditions that you need for consistency of classical estimators as well.",
            "So any questions about that.",
            "Yeah.",
            "This data is IID.",
            "So.",
            "I think you can generalize this to the.",
            "More general case of non IID data.",
            "But then the conditions become more complicated.",
            "So now this is nice.",
            "This says essentially OK, so a lot of people worry about how to choose the prior.",
            "This says if you choose your prior so that you put probability mass near all reasonable thetas.",
            "There are many such priors, right?",
            "There isn't one right prior.",
            "There's no such thing, but if you choose a prior that puts some probability mass near most reasonable thetas, then don't worry.",
            "You know?",
            "In the limit, you'll just find, find out what the right data is.",
            "Yeah, is there any risk of like having two simple model and then you will over certain?",
            "Well, if you have two simpler models, let me give you a concrete example.",
            "Let's take polynomials.",
            "If I put my whole.",
            "Prior mass on linear polynomials and my data actually came from a quadratic.",
            "Then it doesn't matter how much data I observe, I put all my mass on linear polynomials.",
            "I'll learn the best possible linear polynomial there is.",
            "And that's exactly what this second point says.",
            "So in the UN realisable case where the data was generated by some P star of X which cannot be modeled by any Theta.",
            "For example, a quadratic shape that can be modeled by any linear.",
            "Then the posterior will converge to some Theta hat.",
            "Which minimizes the KL divergences between P star of X&P and the likelihood P of X given Theta.",
            "OK, so it'll find the nearest model in KL sense to P star OK.",
            "Which in fact is the maximum likelihood model.",
            "So my posterior will converge around the maximum likelihood parameter setting.",
            "Subject to all these regularity conditions in the finite dimensional case.",
            "In the limit, OK.",
            "So at least this is giving you some links between stuff that happens from the Bayesian posterior and stuff that would happen in the classical statistics setting.",
            "OK, questions about that.",
            "So here is what happens, this is."
        ],
        [
            "Asymptotically consensus, this is what happens when you have multiple Bayesians OK.",
            "So imagine you have two different Bayesians with different priors, P1 of Theta and P2 of Theta.",
            "But they observe the same data.",
            "Now here is the key.",
            "Assume both Bayesians agree on the set of possible and impossible values of Theta.",
            "In other words, the set Theta.",
            "That P1 assigns positive probability to.",
            "Is the same as the set data that P2 assigns positive probability to OK?",
            "Everybody happy with this, so this is basically P1 and P2 have the same universe of status that they're considering, but they just spread their mass out in different ways.",
            "Over that universe, then in the limit, the posteriors P1 of Theta, given DN and P2 of Theta.",
            "Given the N will converge to each other.",
            "This is why it's called asymptotically consensus.",
            "Given the same data, and assuming that their priors are not mutually.",
            "Incoherent with each other.",
            "All Bayesians converge to the same answer OK.",
            "So this is comforting as well.",
            "Don't worry so much about the prior, just worry about putting probability mass over all the reasonable parameter values than you can think of.",
            "You will converge into the same thing.",
            "Of course you can re analyze different priors if you want.",
            "And in finite, in the finite data case, of course the answers will be different, but in the limit they'll converge under those same regularity conditions as before.",
            "Yep.",
            "As effective versions of this is how it goes like this?",
            "Absolutely there is a lot of more theory.",
            "There's asymptotic normality results, yeah?",
            "And convergence rates.",
            "There's a lot of work on rates which I'm not going to talk about, but that's a more model specific.",
            "Under a lot of conditions you can get you know optimal rates.",
            "You can also find conditions where your rates are not optimal there is this is this is something I'm going to get to in my last slides of this talk.",
            "I won't talk about the formal results, but this is kind of the nice interface between Bayesian modeling and frequentists are classical analysis of Bayesian methods, so I see Bayesian methods is a way of developing models.",
            "But I can use classical frequentist tools to analyze the properties of my Bayesian procedure.",
            "For example, to prove convergence and things like that and rates, Yep.",
            "Any other reason apart home computation time?",
            "Why you should never define a prior 0 then?",
            "To put zero mass somewhere, you say, OK, well in.",
            "I think.",
            "It depends on how you parameterize your space.",
            "I'll be talking and Peter are bands as well.",
            "We'll be talking quite a lot about Bayesian nonparametrics, where you have an infinite dimensional parameter space, and then it's even hard to know exactly where you're putting 0 mass and non zero mass on, but in simple cases then you do want to spread your support out just in case.",
            "OK.",
            "So here is a quiz question by the way.",
            "Do you know, let's take a prior Anna likelihood OK?",
            "Take the log prior plus the log likelihood and optimize that with respect to the parameters.",
            "What's that called?",
            "Map maximum a posteriori right.",
            "So is that a Bayesian procedure?",
            "OK, not according to what I've said.",
            "I said all we need to do is a summary in the product rule.",
            "Nowhere in there did it say footnote do map.",
            "OK.",
            "There is no.",
            "There is no.",
            "There is no formal Bayesian justification for doing map.",
            "Other than as a very crude approximation to doing the integrals, Yep.",
            "Divergences like Jeffries, Type R and you exclude the Republicans points and you will be flying to fight smaller sets E around the separated points and open.",
            "You can move them by doing updates, so it might be.",
            "Yeah, I mean that might come under the regularity conditions for that sort of situation.",
            "So basically the.",
            "For.",
            "For these to hold, I think that.",
            "It's not just about the.",
            "The set of zeros, but in some cases if you have Delta masses in your priors of some kind or points at which you're diverging, then you might.",
            "Fall afoul of the regularity conditions as well.",
            "OK.",
            "So."
        ],
        [
            "Let's move on.",
            "So let's talk about model selection.",
            "So consider this case.",
            "This sort of running example that we had of polynomials, so I have these eight data points an I can model them with the linear sorry, constant linear, quadratic, cubic up to 7th order polynomial.",
            "Clearly with eight data points I can perfectly interpolate.",
            "The data with the 7th order polynomial.",
            "What I've shown you in blue is a maximum likelihood fit.",
            "To these different polynomials of the same data, now we're all very familiar with this model selection idea, where we want to, on the one hand, avoid overfitting.",
            "This definitely looks like overfitting.",
            "Imagine what we're doing here is between these two data points, where extract interpolating this completely ridiculous.",
            "Ridiculous form of the function that just seems like overfitting to me.",
            "It's impossible to prove, but seems like overfitting.",
            "Here we seem to be under fitting.",
            "There could be some structure in the data, but we fit only a constant, so we want to do model selection or model comparison, both avoiding underfitting and overfitting."
        ],
        [
            "So here is the.",
            "Use of the marginal likelihood to do this.",
            "So here is how we could use just the same things that I talked about.",
            "The sum rule in the product rule now in the form of Bayesian model comparison to be able to avoid both models that are too simple and models that are too complicated.",
            "So imagine we have two model classes, M&M Prime.",
            "Now if we want to compare them clearly we can give different priors to Eminem prime, a priority we might prefer the simpler model or the more complicated model.",
            "Let's say that's what this term is, but.",
            "What I'm going to focus on is this term here, which is that marginal likelihood, also called the model evidence or the integrated likelihood, and this basically is the integral over all possible parameter values of the likelihood.",
            "Times the prior integrated with respect to the parameters.",
            "Now clearly the maximum likelihood is going to get is going to be higher at higher for the more complicated models because we're optimizing this thing over.",
            "More and more parameters and we can fit the data better and better, but the marginal likelihood doesn't necessarily go up as we get more complexity in our models.",
            "In fact, typically what happens is that the marginal likelihood will either go up and come down as the model complexity varies, or it will go up and sort of gently plateau, depending on how I define my nested class of models.",
            "How do we understand that this is at the core of Bayesian Occam's Razor?",
            "This sort of automatic preference for simplicity that comes from doing averaging.",
            "It doesn't come from anything else.",
            "It comes from following the rules of probability theory.",
            "So in this case, the marginal likelihood is doing an averaging over the parameters and it's encoding.",
            "This notion of simplicity in the following way, so we can interpret the marginal likelihood.",
            "As three different sort of equivalent ways.",
            "Is the probability that randomly selected parameters from the prior would have generated data set D?",
            "So one way of computing this integral?",
            "Is it's a very bad way.",
            "It's a very inefficient way, but it's conceptually correct way is randomly sample parameters from the prior.",
            "Evaluate the likelihood and average.",
            "OK, so clearly a model this more complicated like the 7th order polynomial.",
            "If I randomly sample 7th order polynomials, they'll be all over the place, and so the probability that they assign to my particular data set could be incredibly low.",
            "Here's a different interpretation.",
            "Is the probability of the data under the model averaging over all possible parameter values?",
            "That's just a verbal way of writing down this statement, and again, if you think about it a little bit, then there's no reason to think that more complicated model will give higher probability to the data if you happen to like information theory, you can interpret this as log of one over.",
            "This marginal likelihood is the number of bits of surprise at observing data D. Under Model M. So you know, for a simpler model.",
            "A complicated data set will be surprising.",
            "But also for a complicated model, A particularly simple data set could be surprising, OK?",
            "So we can kind of formalize this or sort of visualize this in this diagram, where what I've drawn here is on this axis, I've strung together all possible datasets of some particular size and this is just the.",
            "Probability of the data given the model class for three different models, and essentially what's going on is a simple model like the constant or the linear polynomial is is concentrating its probability mass on some particular datasets.",
            "A more complicated model like that 7th order polynomial is spreading its probability mass over more possible datasets.",
            "But the rules say that all of these probability distributions have to integrate to one.",
            "So if a complicated mass chooses to spread its bets over more models, necessarily it has to put less mass on the simple models.",
            "OK. Of the simple datasets, sorry.",
            "So the game is all is a fair game.",
            "All models come in with with, you know, one euro of probability mass that they are allowed to spread over different possible datasets.",
            "They come into the casino of modeling with their one euro.",
            "They each spread it differently.",
            "The real world rolls its dice.",
            "A data set appears and the winner could be either a simple model or a complicated model depending on how they spread their probability mass.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I've been losing my voice over the last few days, so.",
                    "label": 0
                },
                {
                    "sent": "Hopefully by the end of the two hours I'll still have some voice, but the magic microphones will help out that way.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to be talking about Bayesian modeling and first of all, I just want to mention I put the slides online just right now it's mlg.nj.com.ac.uk/zubin talks, lect, onebase.pdf.",
                    "label": 0
                },
                {
                    "sent": "So if you all download it now, then nobody will be able to use the Internet, probably for the rest of the day, OK?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be talking about Bayesian modeling an I'm I know that you've had quite a lot of advanced materials so far, but.",
                    "label": 0
                },
                {
                    "sent": "So especially following from yesterday talking about Kingman's coalescent and things like that, it's going to seem very basic.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to say, but I think it's still very important to start from the basics and try to really understand what's going on.",
                    "label": 0
                },
                {
                    "sent": "Why do you know Bayesian machine learning?",
                    "label": 0
                },
                {
                    "sent": "People do things the way that they do.",
                    "label": 0
                },
                {
                    "sent": "What's the philosophy behind it?",
                    "label": 0
                },
                {
                    "sent": "What is the formal theoretical foundations behind it and?",
                    "label": 0
                },
                {
                    "sent": "How does it relate to the rest of machine learning?",
                    "label": 0
                },
                {
                    "sent": "So in particular, I'm talking about Bayesian modeling and I want to focus on these two words.",
                    "label": 0
                },
                {
                    "sent": "Modeling and Bayesian for at the beginning and try to just very clearly describe to you what I mean by these things.",
                    "label": 0
                },
                {
                    "sent": "And then I'll talk about more specifics and more and more technical material as I go on through.",
                    "label": 0
                },
                {
                    "sent": "I think I have maybe 6 hours of lectures.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This usually happens.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just as motivation, it's a really exciting time to be doing machine learning or modeling.",
                    "label": 0
                },
                {
                    "sent": "We're in the middle of an information revolution.",
                    "label": 1
                },
                {
                    "sent": "There's huge amounts of data everywhere affecting society.",
                    "label": 0
                },
                {
                    "sent": "The way Sciences are done, the way business is done so you know, all of us will hopefully have nice jobs because there's a lot of demand for what we are trying to do.",
                    "label": 0
                },
                {
                    "sent": "But of course with huge amounts of data, we really need tools for dealing with that data, and it's not.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to focus on the largeness.",
                    "label": 0
                },
                {
                    "sent": "Quantity of tools of the data.",
                    "label": 0
                },
                {
                    "sent": "I'm going to focus on the interesting things that we can do with that data, so we need tools for modeling, searching, visualizing, and understanding large datasets.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And, uh, our modeling tools need to, I feel, be able to do certain things.",
                    "label": 0
                },
                {
                    "sent": "The modeling tools should be able to capture some form of uncertainty about our model structure.",
                    "label": 1
                },
                {
                    "sent": "They should be able to capture uncertainty in the in the data.",
                    "label": 0
                },
                {
                    "sent": "The noise processes in the data data, especially stuff that you get nowadays on the web, isn't very clean experimental data.",
                    "label": 0
                },
                {
                    "sent": "It's incredibly messy.",
                    "label": 0
                },
                {
                    "sent": "We want tools that are automated that adapt that are robust, that scale well, etc.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the framework I'm going to be talking about is probabilistic modeling framework, so let me focus on the word model.",
                    "label": 0
                },
                {
                    "sent": "What's a model to me?",
                    "label": 0
                },
                {
                    "sent": "Well, to me, a model is a description of possible data one could observe from a system.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So funny way of describing a model, but I've come to sort of like this way of thinking about models.",
                    "label": 0
                },
                {
                    "sent": "So model is a way of describing possible data you could observe.",
                    "label": 0
                },
                {
                    "sent": "In any kind of domain you're trying to model.",
                    "label": 0
                },
                {
                    "sent": "And if it's not that, then I don't understand what a model could be OK if a model can't predict data, then there's no way of falsifying a model.",
                    "label": 0
                },
                {
                    "sent": "There's no way of telling whether model is good or bad, because ultimately we have to evaluate models with respect to our real world data.",
                    "label": 0
                },
                {
                    "sent": "Of course, models serve other purposes.",
                    "label": 0
                },
                {
                    "sent": "Models might also be much more interpretable or understandable than the raw data, and those are kind of side benefits of models, but ultimately their descriptions of possible data one could observe.",
                    "label": 0
                },
                {
                    "sent": "Now the framework we're going to take is to use the mathematics of probability theory to express all the forms of uncertainty and noise that we have in our model.",
                    "label": 1
                },
                {
                    "sent": "If we do that, the nice thing is then we can use the same rules of probability theory.",
                    "label": 0
                },
                {
                    "sent": "What used to be called?",
                    "label": 0
                },
                {
                    "sent": "You know, over 100 years ago, inverse probability or Bayes rule, but it's basically the application of the rules of probability theory to allow us to infer unknown quantities, adapt our models, learn parameters, learn from data, etc, make predictions.",
                    "label": 1
                },
                {
                    "sent": "Etc.",
                    "label": 0
                },
                {
                    "sent": "Now I want to.",
                    "label": 0
                },
                {
                    "sent": "I want to draw an analogy for you.",
                    "label": 0
                },
                {
                    "sent": "You know a few 100 years ago, Newton and Leibnitz, and people like that developed calculus.",
                    "label": 0
                },
                {
                    "sent": "Calculus was a mathematical language for talking about rates of change.",
                    "label": 0
                },
                {
                    "sent": "Probability theory is a mathematical language for talking about uncertainty.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's basically our way of expressing uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the probabilistic modeling framework that I'm going to take, and you know you've seen this stuff to death, but basically.",
                    "label": 0
                },
                {
                    "sent": "You know that's probably not base as Neil said.",
                    "label": 0
                },
                {
                    "sent": "I think in his talk there's some arguments.",
                    "label": 0
                },
                {
                    "sent": "It's the only image of Bayes that anybody claims could be of base.",
                    "label": 0
                },
                {
                    "sent": "But there's some dispute about it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The famous Bayes rules just a basic rule of probability theory.",
                    "label": 0
                },
                {
                    "sent": "But when you use terms like hypothesis and data, you can write it down in this form that says it's a way of turning uncertainty about some set of hypothesis before observing the data.",
                    "label": 0
                },
                {
                    "sent": "That's the prior is there it?",
                    "label": 0
                },
                {
                    "sent": "Does anybody have pointer?",
                    "label": 0
                },
                {
                    "sent": "OK, this looks suspiciously like a pointer.",
                    "label": 0
                },
                {
                    "sent": "OK, this looks like Swiss issues like pointer that doesn't work, sorry.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Wow, I'm gonna I'm gonna be in pain.",
                    "label": 0
                },
                {
                    "sent": "OK yeah alright get stronger overtime.",
                    "label": 0
                },
                {
                    "sent": "OK, here's a mouse that's easier.",
                    "label": 0
                },
                {
                    "sent": "OK, so the Bayes rule tells us how to turn our uncertainty about a bunch of hypothesis before observing the data.",
                    "label": 0
                },
                {
                    "sent": "That's the prior into our uncertainty about the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "After observing the data and the key ingredient that we need to be able to do this transformation is this likelihood.",
                    "label": 0
                },
                {
                    "sent": "Which basically says that every hypothesis has to be able to give probabilities to data.",
                    "label": 0
                },
                {
                    "sent": "So hypothesis is only well formulated if it can actually assign a probability to your observed data.",
                    "label": 0
                },
                {
                    "sent": "Let me just elaborate on this point.",
                    "label": 0
                },
                {
                    "sent": "Let's say my hypothesis was that the relationship between X&Y is a linear regression.",
                    "label": 0
                },
                {
                    "sent": "That's not a well formulated hypothesis.",
                    "label": 0
                },
                {
                    "sent": "OK, because I can't, I can't assign probabilities data if I say it's a linear regression, let's say with Gaussian noise, that's still not a well formulated hypothesis because I haven't told you what range the parameters can take.",
                    "label": 0
                },
                {
                    "sent": "OK, a linear regression that can take parameter ranges between, let's say, slopes between minus one and one is a completely different model than a linear regression that can take slopes between minus 100 and 100.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I tediously and laboriously write down all of my assumptions, let's say it's a linear regression model, my soaps are going to arrange my parameters are going to range between minus 100 and 100.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give a uniform distribution to those parameters.",
                    "label": 0
                },
                {
                    "sent": "My nose is going to be Gaussian with some variance Sigma squared, that Sigma squared is going to come from an exponential distribution, etc.",
                    "label": 0
                },
                {
                    "sent": "I write down all of those assumptions at some point Bingo, I have a model.",
                    "label": 0
                },
                {
                    "sent": "That actually could predict data, and only at that point am I going to call that a model.",
                    "label": 0
                },
                {
                    "sent": "OK before that, it's an incomplete model.",
                    "label": 0
                },
                {
                    "sent": "So if you ask, for example, an you some of you will already be thinking where does the prior come from.",
                    "label": 0
                },
                {
                    "sent": "It's the same as, where does the likelihood come from?",
                    "label": 0
                },
                {
                    "sent": "It's a part of the model.",
                    "label": 0
                },
                {
                    "sent": "If you don't specify as a Bayesian if you don't specify all the parts, it's like building a car without all the components.",
                    "label": 0
                },
                {
                    "sent": "It won't drive until you put all the components on it OK.",
                    "label": 0
                },
                {
                    "sent": "So Bayes rule tells us how to do inference about hypothesis from data learning.",
                    "label": 1
                },
                {
                    "sent": "Prediction can be seen as forms of inference.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will elaborate on this a lot more, so let me contrast 2 views of machine learning.",
                    "label": 0
                },
                {
                    "sent": "The view that I'm going to be kind of mostly talking about is this goal of machine learning as learning models of data.",
                    "label": 0
                },
                {
                    "sent": "So we define a space of possible models.",
                    "label": 1
                },
                {
                    "sent": "We learn the parameters and structure of these models from data.",
                    "label": 1
                },
                {
                    "sent": "We make predictions and decisions.",
                    "label": 0
                },
                {
                    "sent": "The alternative, and I would say perhaps dominant view of machine learning is.",
                    "label": 0
                },
                {
                    "sent": "That machine learning is a toolbox of methods for processing data that's maybe not a very nice way of putting it, but I think that's essentially how a lot of practitioners out there in the real world end up using machine learning is they download.",
                    "label": 0
                },
                {
                    "sent": "They have some data there like I need to do some clustering, they download a tool for doing clustering.",
                    "label": 0
                },
                {
                    "sent": "They run it on their algorithm, they process their data in some way, and they're going to.",
                    "label": 0
                },
                {
                    "sent": "Then they say, OK, what do I do now with the clusters, OK?",
                    "label": 1
                },
                {
                    "sent": "You feed the data into many possible one of many possible methods.",
                    "label": 0
                },
                {
                    "sent": "You choose methods.",
                    "label": 0
                },
                {
                    "sent": "Of course you know people are very clever.",
                    "label": 0
                },
                {
                    "sent": "There are good and bad methods and you choose methods that have excellent theoretical properties or excellent empirical properties, and that's a good way of getting a lot of stuff done.",
                    "label": 0
                },
                {
                    "sent": "You can make predictions, decisions that way as well.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's see if this works OK, so here is my plan for for today for at least these first 2 hours I'm going to talk about some of the foundations I'm going to talk about intractability and I'm going to give a brief overview of approximation tools.",
                    "label": 0
                },
                {
                    "sent": "Now, again, this is all a bit backwards.",
                    "label": 0
                },
                {
                    "sent": "You've heard a lot about Hamiltonian Monte Carlo, right?",
                    "label": 0
                },
                {
                    "sent": "But I'm going to just give you the overview and maybe you can slot some of what you've learned into the bigger picture.",
                    "label": 0
                },
                {
                    "sent": "Unless you probably already had that in your head.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about some advanced topics and some limitations and discussions because I think it's also very important to place the Bayesian framework alongside the Classical an optimization based frameworks and say what are the advantages and disadvantages.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in more detail.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the detailed outline of what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "I'm I'm going to skip parts of this.",
                    "label": 0
                },
                {
                    "sent": "This is actually vaguely based on a nice email tutorial I gave a bunch of years ago, but it's evolved overtime and I've edited some things out basically overtime.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to be on the same page, I'm going to introduce some very Canonical machine learning problems and just show them to you in, you know, unified notate.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is linear classification.",
                    "label": 1
                },
                {
                    "sent": "I have some end data points points in some plane and their class label plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "I want to learn a linear boundary.",
                    "label": 0
                },
                {
                    "sent": "I write down a model which in this case is a hard classifier, but written as a likelihood function.",
                    "label": 0
                },
                {
                    "sent": "That's fine, OK, I can write a likelihood function.",
                    "label": 0
                },
                {
                    "sent": "This deterministic in this particular case, it's a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "We have some parameters Theta and.",
                    "label": 0
                },
                {
                    "sent": "Our goal.",
                    "label": 0
                },
                {
                    "sent": "Is going to be to infer these parameters data from the data so as to be able to generalize to predict future data OK?",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nope.",
                    "label": 0
                },
                {
                    "sent": "So here is polynomial regression.",
                    "label": 1
                },
                {
                    "sent": "Again, we have a bunch of data points.",
                    "label": 0
                },
                {
                    "sent": "Pairs of X is and wise, our model is going to be this polynomial with some noise and I have to specify some.",
                    "label": 0
                },
                {
                    "sent": "Distribution for the noise.",
                    "label": 0
                },
                {
                    "sent": "So let's make that Gaussian.",
                    "label": 0
                },
                {
                    "sent": "In the simplest case, and we have some parameters, our goal is going to be done for the parameters data from the data so as to predict future outputs.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and here is clustering.",
                    "label": 0
                },
                {
                    "sent": "With Gaussian mixture models.",
                    "label": 0
                },
                {
                    "sent": "Or you could think of it also is a density estimation problem.",
                    "label": 0
                },
                {
                    "sent": "We have a bunch of data point these blue dots.",
                    "label": 0
                },
                {
                    "sent": "We have a model which is a.",
                    "label": 0
                },
                {
                    "sent": "In this case, the finite mixture model with M components.",
                    "label": 0
                },
                {
                    "sent": "Each of these, let's say, is Gaussian with each component is a Gaussian with some mean and some covariance matrix denoted as these... And you have some mixing coefficients for the relative sizes or masses of these Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Now you can write down your parameters and again our goal is going to be to infer the parameters data from the data, so it's to predict either.",
                    "label": 0
                },
                {
                    "sent": "For example, the density of new data points, or to infer whether two points belong to the same cluster or not OK.",
                    "label": 1
                },
                {
                    "sent": "So any questions about that?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the Bayesian framework to machine learning says just basically apply the rules of probability theory.",
                    "label": 1
                },
                {
                    "sent": "OK, that's it.",
                    "label": 0
                },
                {
                    "sent": "That's essentially all we need to do.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, you know there's a couple of more caveats here.",
                    "label": 0
                },
                {
                    "sent": "First of all, write down everything you know using the language of probability theory on certainty as probabilities.",
                    "label": 0
                },
                {
                    "sent": "Then use the rules of probability theory on your data.",
                    "label": 0
                },
                {
                    "sent": "OK, so the good news is.",
                    "label": 0
                },
                {
                    "sent": "You don't have to memorize a lot of stuff, we only have to know 2 rules.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the sum rule in the product rule.",
                    "label": 1
                },
                {
                    "sent": "Some rule says the probability of X can be written as the sum or integral if Y is continuous of the joint probability of X&Y.",
                    "label": 0
                },
                {
                    "sent": "The product rule says that the joint probability of X&Y can be decomposed into the probability of X times the probability of Y given X or the other way round, OK.",
                    "label": 0
                },
                {
                    "sent": "So it's great for people like me.",
                    "label": 0
                },
                {
                    "sent": "I can't memorize a lot of things if I'm if I'm.",
                    "label": 0
                },
                {
                    "sent": "In the middle of a derivation and I'm confused what to do.",
                    "label": 0
                },
                {
                    "sent": "I just sit back and say, alright, I just have to follow the summer on the product rule, OK?",
                    "label": 0
                },
                {
                    "sent": "It's true you can do incredible things with the sum rule in the product rule, belief propagation.",
                    "label": 0
                },
                {
                    "sent": "How many people here have studied belief propagation?",
                    "label": 0
                },
                {
                    "sent": "OK, it's just the sum rule in the product rule.",
                    "label": 0
                },
                {
                    "sent": "OK, it's just done that on a graph.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know the same with Kalman filtering.",
                    "label": 0
                },
                {
                    "sent": "I took a two semester class.",
                    "label": 0
                },
                {
                    "sent": "On Kalman filtering.",
                    "label": 0
                },
                {
                    "sent": "If they had just told me it's the summer on the product rule for Gaussians, it would have saved me a lot of time.",
                    "label": 0
                },
                {
                    "sent": "OK, so so from this summer in the product rule we get Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "Obviously by just applying these two rules here.",
                    "label": 0
                },
                {
                    "sent": "And I just replace the symbols X&Y with Theta and D to represent data being the parameters of our model and D being the data OK.",
                    "label": 0
                },
                {
                    "sent": "So that's the prior.",
                    "label": 0
                },
                {
                    "sent": "This is the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "This is called the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to it later and this is the posterior over the parameters given the data.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "If we want to do prediction, let's say what does prediction mean?",
                    "label": 0
                },
                {
                    "sent": "Prediction means.",
                    "label": 0
                },
                {
                    "sent": "Assume I'm in some particular model.",
                    "label": 0
                },
                {
                    "sent": "I've observed some data D. And I want to be able to predict new data points X.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what is the probability of a new data point X given the observed data D and our current model assumption?",
                    "label": 0
                },
                {
                    "sent": "Well, find out I apply the same rule in the product rule.",
                    "label": 0
                },
                {
                    "sent": "Basically I can write that as a sum or integral over all possible parameters Theta of the product of.",
                    "label": 0
                },
                {
                    "sent": "The predictions for that particular parameter times the posterior over the parameters given the data.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "This just looks like boring math, but it actually is very, very intuitive and very nice with this.",
                    "label": 0
                },
                {
                    "sent": "Basically says is if I want to know the predictions of my model.",
                    "label": 0
                },
                {
                    "sent": "What I do is I take the predictions for any particular parameter and I average those predictions with respect to my posterior distribution over the parameters.",
                    "label": 0
                },
                {
                    "sent": "So here it is.",
                    "label": 0
                },
                {
                    "sent": "This is like some rule product rule giving you the kind of idea of ensemble learning methods, which is that you're averaging your predictions over multiple models.",
                    "label": 0
                },
                {
                    "sent": "OK, I have more to say about that.",
                    "label": 0
                },
                {
                    "sent": "It's more subtle if you push that to the end, but essentially this is what is going on here.",
                    "label": 0
                },
                {
                    "sent": "Averaging the key idea in Bayesian methods is if you don't know something average over it, OK?",
                    "label": 1
                },
                {
                    "sent": "Now a model comparison again also follows from this.",
                    "label": 0
                },
                {
                    "sent": "If I wanted the know, the probability of a model given the data, I could apply these rules.",
                    "label": 0
                },
                {
                    "sent": "The key quantity here is what's called the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "This term here and that is the integral of over the parameters of the likelihood times the prior.",
                    "label": 0
                },
                {
                    "sent": "Integrate with respect to the parameters.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's.",
                    "label": 0
                },
                {
                    "sent": "Just step back.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "OK really.",
                    "label": 0
                },
                {
                    "sent": "You know the rest of the stuff is.",
                    "label": 0
                },
                {
                    "sent": "Well, what's left, OK?",
                    "label": 0
                },
                {
                    "sent": "So do you have any questions?",
                    "label": 0
                },
                {
                    "sent": "Should we stop there?",
                    "label": 0
                },
                {
                    "sent": "It's not good weather to go to the beach I guess, so maybe we could stay a little longer.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Maybe I meant I'm going to answer.",
                    "label": 0
                },
                {
                    "sent": "I do have more material.",
                    "label": 0
                },
                {
                    "sent": "But I want to see if I could pre empt some of that.",
                    "label": 0
                },
                {
                    "sent": "You're all sold.",
                    "label": 0
                },
                {
                    "sent": "Has it been a long week?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I'm not going on until somebody asked the question.",
                    "label": 0
                },
                {
                    "sent": "Who's going to be the brave one?",
                    "label": 0
                },
                {
                    "sent": "The previous slide prompt questions.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's not a question.",
                    "label": 0
                },
                {
                    "sent": "Request no.",
                    "label": 0
                },
                {
                    "sent": "OK question excellent.",
                    "label": 0
                },
                {
                    "sent": "Mining and machine learning.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "Science is made of people, and people tend to form communities.",
                    "label": 0
                },
                {
                    "sent": "So I I came into this field from a particular community.",
                    "label": 0
                },
                {
                    "sent": "I used to go for example to the NIPS conference, because when I started my PhD, the really exciting thing to me was neural networks.",
                    "label": 0
                },
                {
                    "sent": "I'm afraid to admit.",
                    "label": 0
                },
                {
                    "sent": "And you know our community people Mike Jordan and Jeff Hinton.",
                    "label": 0
                },
                {
                    "sent": "And you know I'm Neil Lawrence and myself.",
                    "label": 0
                },
                {
                    "sent": "You know we we were, you know we can't.",
                    "label": 0
                },
                {
                    "sent": "We came in.",
                    "label": 0
                },
                {
                    "sent": "I think if interested in or were you interested in all networks and I wanted to know how to break.",
                    "label": 0
                },
                {
                    "sent": "I went to the extent of actually getting a PhD in neuroscience 'cause I was interested in how the brain works.",
                    "label": 0
                },
                {
                    "sent": "I don't care how the brain works anymore.",
                    "label": 0
                },
                {
                    "sent": "This is much more fun to do machine learning.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so there was a community that came into the field through that route.",
                    "label": 0
                },
                {
                    "sent": "There was a community that came into the field from databases where they were had lots of databases and they wanted to do interesting operations on those databases.",
                    "label": 0
                },
                {
                    "sent": "There are more computer science in their thinking, so they're more the data mining community community there is.",
                    "label": 0
                },
                {
                    "sent": "The uncertainty in artificial intelligence community which came at it from graphical models.",
                    "label": 0
                },
                {
                    "sent": "There is the ICL community which is machine.",
                    "label": 0
                },
                {
                    "sent": "AI people who wanted to do machine learning with rule based machine learning originally.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that.",
                    "label": 0
                },
                {
                    "sent": "Well, and then there's the statisticians.",
                    "label": 0
                },
                {
                    "sent": "Let's not forget them, OK?",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is that all of these ships were on a collision course.",
                    "label": 0
                },
                {
                    "sent": "OK, so now they're doing much more similar things, but when you go into a data mining conference, the flavor of things that you get is much more historically based on the idea of big databases.",
                    "label": 0
                },
                {
                    "sent": "Large scale but simple forms of machine learning.",
                    "label": 1
                },
                {
                    "sent": "OK. Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's one question I can move on.",
                    "label": 0
                },
                {
                    "sent": "I don't need two questions.",
                    "label": 0
                },
                {
                    "sent": "Go ahead for model comparison.",
                    "label": 1
                },
                {
                    "sent": "You assume that you can integrate the models, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, what does it mean exactly?",
                    "label": 0
                },
                {
                    "sent": "How do you define integrating out?",
                    "label": 0
                },
                {
                    "sent": "OK, this is a very good question.",
                    "label": 0
                },
                {
                    "sent": "I'll come to this in a minute in model comparison here.",
                    "label": 0
                },
                {
                    "sent": "At least if I want to compare two models, would I need to be able to do is integrate out the parameters for those two models?",
                    "label": 0
                },
                {
                    "sent": "OK, but if I have a larger space of models, then one of the tricky things about Bayesian inference is that I need to be able to somehow enumerate.",
                    "label": 0
                },
                {
                    "sent": "Either implicitly or explicitly, all the models I'm willing to consider.",
                    "label": 0
                },
                {
                    "sent": "This is sometimes called the closed world hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Basically, you know.",
                    "label": 0
                },
                {
                    "sent": "I have to somehow specify a generative process over models now that could be very sophisticated.",
                    "label": 0
                },
                {
                    "sent": "For example, you could have a grammar over models that would generate more and more complicated models as long as you're the probability mass that you associate to all possible models in your in your closed world or universe of models sums to one or at most one.",
                    "label": 0
                },
                {
                    "sent": "So it's a very good question actually.",
                    "label": 0
                },
                {
                    "sent": "Alright, yeah, another question.",
                    "label": 0
                },
                {
                    "sent": "Likelihood of three times the next thing you know, the probability of data so.",
                    "label": 0
                },
                {
                    "sent": "Can I interchange likelihood and probability?",
                    "label": 0
                },
                {
                    "sent": "OK, I haven't.",
                    "label": 1
                },
                {
                    "sent": "I use the symbol P uniformly on this page OK, which means there are probabilities.",
                    "label": 0
                },
                {
                    "sent": "This is the probability of the data given Theta.",
                    "label": 0
                },
                {
                    "sent": "A lot of people, confusingly say the likelihood of the data because it you know they use the word likelihood colloquially for probability.",
                    "label": 0
                },
                {
                    "sent": "Formally, if you look at any statistics textbook, this is the likelihood of Theta, which is.",
                    "label": 0
                },
                {
                    "sent": "It's a function of Theta.",
                    "label": 0
                },
                {
                    "sent": "That's not a probability function, OK?",
                    "label": 0
                },
                {
                    "sent": "But you know, Alternatively, for a Bayesian, the likelihood of Theta is just the probability of the data given Theta.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Prior likelihood of the time to say prior probability is it.",
                    "label": 1
                },
                {
                    "sent": "Yeah it's it's this is this is a probability over theater.",
                    "label": 0
                },
                {
                    "sent": "Low likelihood is not a probability over Theta.",
                    "label": 0
                },
                {
                    "sent": "The likelihood is not a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, let me move on.",
                    "label": 0
                },
                {
                    "sent": "OK so that's it, but it's not to.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so here are the questions I want to address.",
                    "label": 0
                },
                {
                    "sent": "So first of all you know what are the?",
                    "label": 0
                },
                {
                    "sent": "What are the foundations of bayesianism?",
                    "label": 0
                },
                {
                    "sent": "Why are some people using this basic methodology?",
                    "label": 0
                },
                {
                    "sent": "I actually think it's weird.",
                    "label": 0
                },
                {
                    "sent": "I don't like the term Bayesian, it makes it sound different somehow.",
                    "label": 0
                },
                {
                    "sent": "Like it's some choice.",
                    "label": 0
                },
                {
                    "sent": "You know, we don't say why are we all calculation Ists right?",
                    "label": 0
                },
                {
                    "sent": "Because we use calculus.",
                    "label": 0
                },
                {
                    "sent": "You know, I, I think that this basically you know this comes down to why are we using probability theory to answer questions from probability theory.",
                    "label": 0
                },
                {
                    "sent": "Well, you know what else would we use right?",
                    "label": 0
                },
                {
                    "sent": "Of course there is the question.",
                    "label": 0
                },
                {
                    "sent": "Are there other ways of doing data analysis and modeling?",
                    "label": 0
                },
                {
                    "sent": "And that's certainly true, right?",
                    "label": 0
                },
                {
                    "sent": "There are many other ways of doing data analysis and machine learning.",
                    "label": 0
                },
                {
                    "sent": "So a lot of people say, you know, are worried about this question.",
                    "label": 0
                },
                {
                    "sent": "Where does the prior come from?",
                    "label": 1
                },
                {
                    "sent": "I'll try to talk about that and then how do we do these integrals?",
                    "label": 0
                },
                {
                    "sent": "This is the practical question, OK?",
                    "label": 0
                },
                {
                    "sent": "So here is one way at that you could come.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the Bayesian framework, and this is if you want to think about artificial intelligence problems.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have a robot and in order for this robot to behave intelligently, it needs to be able to represent its beliefs about some propositions in the world.",
                    "label": 1
                },
                {
                    "sent": "Like you know, where is the you know where is this charging location?",
                    "label": 0
                },
                {
                    "sent": "Or is this sensor malfunctioning or something like that?",
                    "label": 0
                },
                {
                    "sent": "Is it about to get shot whatever?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "The question is now not a question about statistics at all.",
                    "label": 1
                },
                {
                    "sent": "It's a question about how does the robot represent beliefs and we want a framework that we can use to represent the strength of these beliefs numerically in the brain of the robot.",
                    "label": 1
                },
                {
                    "sent": "And we want to know the mathematical rules for manipulating these beliefs.",
                    "label": 0
                },
                {
                    "sent": "So it's basically like you know the rules of logic right?",
                    "label": 0
                },
                {
                    "sent": "But extended to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You capture strength of belief.",
                    "label": 0
                },
                {
                    "sent": "So let's let's start with trying to describe what we mean a little bit.",
                    "label": 0
                },
                {
                    "sent": "So let's say B of X represents the strength of belief or possibility in some Proposition X, and we're going to say it ranges between zero and one where B of X = 0 means that the robot believes X is definitely not true.",
                    "label": 0
                },
                {
                    "sent": "That doesn't have to mean that X is definitely not true.",
                    "label": 1
                },
                {
                    "sent": "That's just a statement about the robots belief B of X = 1 means that robot believes that X is definitely true, and we're going to use B of X given Y.",
                    "label": 0
                },
                {
                    "sent": "To represent the strength of belief that X is true, given that we know that Y is true.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now we want to be able to manipulate these beliefs an if.",
                    "label": 0
                },
                {
                    "sent": "If you accept certain axioms and these go all the way back to Cox in 1946, and I've been sort of developed overtime by Jaynes, and there was a really nice review, more recently by Fat Horn.",
                    "label": 1
                },
                {
                    "sent": "So it goes along these lines if we're going to say strengths of beliefs or degrees of possibility are represented by real numbers, and we want some qualitative correspondence to common sense in terms of how we manipulate these things, and we want some sense of consistency so that if if the conclusion can be reason is several ways, for example, then each way should lead to the same answer.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter which order you apply the operations in, you still get the same answer.",
                    "label": 1
                },
                {
                    "sent": "And the robot is not allowed to just arbitrarily ignore relevant information and equivalent states of knowledge have to be represented with equivalent plausibility statements.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out these belief functions must satisfy the rules of probability theory.",
                    "label": 0
                },
                {
                    "sent": "In the sum rule, the product rule, an Bayes rule that follows.",
                    "label": 0
                },
                {
                    "sent": "So here's a kind of interesting sideways approach to arriving at.",
                    "label": 0
                },
                {
                    "sent": "Probability theory it's a language for representing beliefs of rational agents, OK?",
                    "label": 0
                },
                {
                    "sent": "It extends logic to strengths of beliefs.",
                    "label": 0
                },
                {
                    "sent": "Questions about that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a different way of approaching the problem that motivates probability theory.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a gambling or economics approach.",
                    "label": 0
                },
                {
                    "sent": "It comes in the form of the Dutch book theorem.",
                    "label": 1
                },
                {
                    "sent": "Which I think it definitely even talked about.",
                    "label": 0
                },
                {
                    "sent": "Two is sort of one of the founders of Bayesian statistics, so assume you're willing to accept bets with odds proportional to the strength of your belief.",
                    "label": 1
                },
                {
                    "sent": "So now the way we're going to assess the strength of your beliefs beliefs is that we're going to make you gamble like you know, is it.",
                    "label": 0
                },
                {
                    "sent": "Is it Real Madrid and Barcelona they're playing tonight.",
                    "label": 0
                },
                {
                    "sent": "No real.",
                    "label": 0
                },
                {
                    "sent": "Oh darn, OK. Alright, next week so you could have beliefs about who you think is going to win, right?",
                    "label": 0
                },
                {
                    "sent": "And you might be willing to bet on the basis of those beliefs.",
                    "label": 0
                },
                {
                    "sent": "So if you believe that REL is going to win with strength of belief, .9 with that means is that you'd be willing to accept 9 to 1 odds or better.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here's the interesting fact.",
                    "label": 0
                },
                {
                    "sent": "If you have a set of different beliefs, not one belief, but a set of different beliefs.",
                    "label": 0
                },
                {
                    "sent": "Then, unless these different beliefs are coherent with each other in that they satisfy the rules of probability theory.",
                    "label": 0
                },
                {
                    "sent": "OK, the most trivial one would be my belief in X and my belief and not X should sum to one for example, but things along those lines.",
                    "label": 0
                },
                {
                    "sent": "Then, unless your belief satisfy the rules of probability theory, there exists some set of simultaneous bets called a Dutch book which you are willing to accept.",
                    "label": 1
                },
                {
                    "sent": "For which you're guaranteed to lose money no matter what the outcome, OK?",
                    "label": 0
                },
                {
                    "sent": "This isn't about losing money, on average.",
                    "label": 0
                },
                {
                    "sent": "This is about a sure loss, and the only way to guard from that is for your beliefs to be coherent with the rules of probability theory.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of an economics or Yep.",
                    "label": 0
                },
                {
                    "sent": "By confidence values are ranges.",
                    "label": 0
                },
                {
                    "sent": "In our.",
                    "label": 0
                },
                {
                    "sent": "In this setting, no.",
                    "label": 0
                },
                {
                    "sent": "In the sense that you know if you if you have a range over something like between .7 and .8.",
                    "label": 0
                },
                {
                    "sent": "Then there is no clear way of betting on that in this framework.",
                    "label": 0
                },
                {
                    "sent": "So, so you need to basically.",
                    "label": 0
                },
                {
                    "sent": "You need to commit, and actually in a betting scenario it's much more.",
                    "label": 0
                },
                {
                    "sent": "Intuitive to to exact to figure out what somebody's beliefs are.",
                    "label": 0
                },
                {
                    "sent": "OK, because you know that if you ask someone what are your beliefs, they might not know and then you say OK, I'll give you 10 to 1 odds and they'll say no.",
                    "label": 0
                },
                {
                    "sent": "No, you say 9 to 1 eight to one whatever and then at some point they're willing to accept the Beth Ann.",
                    "label": 0
                },
                {
                    "sent": "That's one way of figuring out what that person's beliefs are, but of course, as the model of humans is quite flawed because humans are not coherent.",
                    "label": 0
                },
                {
                    "sent": "For example, you know lotteries would never work if humans were totally coherent, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe I don't know.",
                    "label": 0
                },
                {
                    "sent": "In general, humans are not very coherent, and so it's not necessarily.",
                    "label": 0
                },
                {
                    "sent": "It's a model of what humans should be.",
                    "label": 0
                },
                {
                    "sent": "We want our robots.",
                    "label": 0
                },
                {
                    "sent": "We don't want our robots to be humans.",
                    "label": 0
                },
                {
                    "sent": "We want to robots to be rational.",
                    "label": 0
                },
                {
                    "sent": "OK, they should be more rational than humans.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Anyway, so this is just a model of economic rationality, which is a bit idealized.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Yeah, just having megabits could also base your belief.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "It could affect your belief you mean.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "So I think in in the psychology literature it's certainly the case that once you've made the bet, you believe that thing much, much more than before you made the bet, which is not rational, because the bet actually gave you no information.",
                    "label": 0
                },
                {
                    "sent": "Right, OK?",
                    "label": 0
                },
                {
                    "sent": "So this is a Dutch book theorem argument for an economic argument for the use of probability theory to represent beliefs.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about some of the theoretical properties of.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of Bayesian methods, so if you're worried about, let's say, let's come back to the realm of statistics a little bit, imagine the following scenario.",
                    "label": 0
                },
                {
                    "sent": "You have a data set DN of N data points and it was generated by some model with some true parameter Theta star OK. Now, of course from a base in point of view, I'm perfectly happy to accept that there is a true parameter out there.",
                    "label": 0
                },
                {
                    "sent": "Bayesianism doesn't mean I believe the parameters random.",
                    "label": 0
                },
                {
                    "sent": "OK, that's just a really bad way of very incorrect way of describing things.",
                    "label": 0
                },
                {
                    "sent": "I can believe there is a true parameter out there, for example, that parameter could be the number of people in this room that I'm trying to infer.",
                    "label": 0
                },
                {
                    "sent": "There's some true number of people in this room, OK, but I'm uncertain about that, so I'm going to represent my uncertainty by probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So consider the following statistics kind of scenario where I have a data set DN consisting of N data points.",
                    "label": 1
                },
                {
                    "sent": "It was generated from some true parameter Theta star.",
                    "label": 0
                },
                {
                    "sent": "Then, under some regularity conditions, and that's careful.",
                    "label": 0
                },
                {
                    "sent": "The footnote says careful with the regularity conditions, these are just sketches of the theoretical results.",
                    "label": 1
                },
                {
                    "sent": "As long as I put to be more formal here, I put some positive probability mass around Theta star.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The in this I've written it as the density of Theta star is greater than zero, but what I really mean is not just the density of data stars greater than zero, but there is some probability mass around Theta star apriori.",
                    "label": 0
                },
                {
                    "sent": "Then the limit as the amount of data goes to Infinity of the posterior is a Delta function centered around Theta star.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "This means that.",
                    "label": 0
                },
                {
                    "sent": "As long as I put some probability mass around all reasonable Theta stars in the limit, my posterior will converge to a Delta function.",
                    "label": 0
                },
                {
                    "sent": "So what are the regularity?",
                    "label": 0
                },
                {
                    "sent": "What are the conditions that we need to worry about?",
                    "label": 0
                },
                {
                    "sent": "I won't go into details, but one of the conditions is, for example, identify ability of my model.",
                    "label": 0
                },
                {
                    "sent": "I need to be able to make sure there are multiple different thetas that are all equivalent to each other, for example, OK. All the thetas could be uniquely identified, so these are similar to the regularity conditions that you need for consistency of classical estimators as well.",
                    "label": 0
                },
                {
                    "sent": "So any questions about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This data is IID.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think you can generalize this to the.",
                    "label": 0
                },
                {
                    "sent": "More general case of non IID data.",
                    "label": 0
                },
                {
                    "sent": "But then the conditions become more complicated.",
                    "label": 0
                },
                {
                    "sent": "So now this is nice.",
                    "label": 0
                },
                {
                    "sent": "This says essentially OK, so a lot of people worry about how to choose the prior.",
                    "label": 0
                },
                {
                    "sent": "This says if you choose your prior so that you put probability mass near all reasonable thetas.",
                    "label": 0
                },
                {
                    "sent": "There are many such priors, right?",
                    "label": 0
                },
                {
                    "sent": "There isn't one right prior.",
                    "label": 0
                },
                {
                    "sent": "There's no such thing, but if you choose a prior that puts some probability mass near most reasonable thetas, then don't worry.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "In the limit, you'll just find, find out what the right data is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, is there any risk of like having two simple model and then you will over certain?",
                    "label": 0
                },
                {
                    "sent": "Well, if you have two simpler models, let me give you a concrete example.",
                    "label": 0
                },
                {
                    "sent": "Let's take polynomials.",
                    "label": 0
                },
                {
                    "sent": "If I put my whole.",
                    "label": 0
                },
                {
                    "sent": "Prior mass on linear polynomials and my data actually came from a quadratic.",
                    "label": 0
                },
                {
                    "sent": "Then it doesn't matter how much data I observe, I put all my mass on linear polynomials.",
                    "label": 0
                },
                {
                    "sent": "I'll learn the best possible linear polynomial there is.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what this second point says.",
                    "label": 0
                },
                {
                    "sent": "So in the UN realisable case where the data was generated by some P star of X which cannot be modeled by any Theta.",
                    "label": 0
                },
                {
                    "sent": "For example, a quadratic shape that can be modeled by any linear.",
                    "label": 0
                },
                {
                    "sent": "Then the posterior will converge to some Theta hat.",
                    "label": 0
                },
                {
                    "sent": "Which minimizes the KL divergences between P star of X&P and the likelihood P of X given Theta.",
                    "label": 0
                },
                {
                    "sent": "OK, so it'll find the nearest model in KL sense to P star OK.",
                    "label": 0
                },
                {
                    "sent": "Which in fact is the maximum likelihood model.",
                    "label": 0
                },
                {
                    "sent": "So my posterior will converge around the maximum likelihood parameter setting.",
                    "label": 0
                },
                {
                    "sent": "Subject to all these regularity conditions in the finite dimensional case.",
                    "label": 0
                },
                {
                    "sent": "In the limit, OK.",
                    "label": 0
                },
                {
                    "sent": "So at least this is giving you some links between stuff that happens from the Bayesian posterior and stuff that would happen in the classical statistics setting.",
                    "label": 0
                },
                {
                    "sent": "OK, questions about that.",
                    "label": 0
                },
                {
                    "sent": "So here is what happens, this is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Asymptotically consensus, this is what happens when you have multiple Bayesians OK.",
                    "label": 0
                },
                {
                    "sent": "So imagine you have two different Bayesians with different priors, P1 of Theta and P2 of Theta.",
                    "label": 0
                },
                {
                    "sent": "But they observe the same data.",
                    "label": 0
                },
                {
                    "sent": "Now here is the key.",
                    "label": 0
                },
                {
                    "sent": "Assume both Bayesians agree on the set of possible and impossible values of Theta.",
                    "label": 1
                },
                {
                    "sent": "In other words, the set Theta.",
                    "label": 0
                },
                {
                    "sent": "That P1 assigns positive probability to.",
                    "label": 0
                },
                {
                    "sent": "Is the same as the set data that P2 assigns positive probability to OK?",
                    "label": 0
                },
                {
                    "sent": "Everybody happy with this, so this is basically P1 and P2 have the same universe of status that they're considering, but they just spread their mass out in different ways.",
                    "label": 0
                },
                {
                    "sent": "Over that universe, then in the limit, the posteriors P1 of Theta, given DN and P2 of Theta.",
                    "label": 0
                },
                {
                    "sent": "Given the N will converge to each other.",
                    "label": 0
                },
                {
                    "sent": "This is why it's called asymptotically consensus.",
                    "label": 0
                },
                {
                    "sent": "Given the same data, and assuming that their priors are not mutually.",
                    "label": 0
                },
                {
                    "sent": "Incoherent with each other.",
                    "label": 0
                },
                {
                    "sent": "All Bayesians converge to the same answer OK.",
                    "label": 0
                },
                {
                    "sent": "So this is comforting as well.",
                    "label": 0
                },
                {
                    "sent": "Don't worry so much about the prior, just worry about putting probability mass over all the reasonable parameter values than you can think of.",
                    "label": 0
                },
                {
                    "sent": "You will converge into the same thing.",
                    "label": 0
                },
                {
                    "sent": "Of course you can re analyze different priors if you want.",
                    "label": 0
                },
                {
                    "sent": "And in finite, in the finite data case, of course the answers will be different, but in the limit they'll converge under those same regularity conditions as before.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "As effective versions of this is how it goes like this?",
                    "label": 0
                },
                {
                    "sent": "Absolutely there is a lot of more theory.",
                    "label": 0
                },
                {
                    "sent": "There's asymptotic normality results, yeah?",
                    "label": 0
                },
                {
                    "sent": "And convergence rates.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work on rates which I'm not going to talk about, but that's a more model specific.",
                    "label": 0
                },
                {
                    "sent": "Under a lot of conditions you can get you know optimal rates.",
                    "label": 0
                },
                {
                    "sent": "You can also find conditions where your rates are not optimal there is this is this is something I'm going to get to in my last slides of this talk.",
                    "label": 0
                },
                {
                    "sent": "I won't talk about the formal results, but this is kind of the nice interface between Bayesian modeling and frequentists are classical analysis of Bayesian methods, so I see Bayesian methods is a way of developing models.",
                    "label": 0
                },
                {
                    "sent": "But I can use classical frequentist tools to analyze the properties of my Bayesian procedure.",
                    "label": 0
                },
                {
                    "sent": "For example, to prove convergence and things like that and rates, Yep.",
                    "label": 0
                },
                {
                    "sent": "Any other reason apart home computation time?",
                    "label": 0
                },
                {
                    "sent": "Why you should never define a prior 0 then?",
                    "label": 0
                },
                {
                    "sent": "To put zero mass somewhere, you say, OK, well in.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "It depends on how you parameterize your space.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking and Peter are bands as well.",
                    "label": 0
                },
                {
                    "sent": "We'll be talking quite a lot about Bayesian nonparametrics, where you have an infinite dimensional parameter space, and then it's even hard to know exactly where you're putting 0 mass and non zero mass on, but in simple cases then you do want to spread your support out just in case.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here is a quiz question by the way.",
                    "label": 0
                },
                {
                    "sent": "Do you know, let's take a prior Anna likelihood OK?",
                    "label": 0
                },
                {
                    "sent": "Take the log prior plus the log likelihood and optimize that with respect to the parameters.",
                    "label": 0
                },
                {
                    "sent": "What's that called?",
                    "label": 0
                },
                {
                    "sent": "Map maximum a posteriori right.",
                    "label": 0
                },
                {
                    "sent": "So is that a Bayesian procedure?",
                    "label": 0
                },
                {
                    "sent": "OK, not according to what I've said.",
                    "label": 0
                },
                {
                    "sent": "I said all we need to do is a summary in the product rule.",
                    "label": 0
                },
                {
                    "sent": "Nowhere in there did it say footnote do map.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There is no.",
                    "label": 0
                },
                {
                    "sent": "There is no.",
                    "label": 0
                },
                {
                    "sent": "There is no formal Bayesian justification for doing map.",
                    "label": 0
                },
                {
                    "sent": "Other than as a very crude approximation to doing the integrals, Yep.",
                    "label": 0
                },
                {
                    "sent": "Divergences like Jeffries, Type R and you exclude the Republicans points and you will be flying to fight smaller sets E around the separated points and open.",
                    "label": 0
                },
                {
                    "sent": "You can move them by doing updates, so it might be.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean that might come under the regularity conditions for that sort of situation.",
                    "label": 0
                },
                {
                    "sent": "So basically the.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "For these to hold, I think that.",
                    "label": 0
                },
                {
                    "sent": "It's not just about the.",
                    "label": 0
                },
                {
                    "sent": "The set of zeros, but in some cases if you have Delta masses in your priors of some kind or points at which you're diverging, then you might.",
                    "label": 0
                },
                {
                    "sent": "Fall afoul of the regularity conditions as well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's move on.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about model selection.",
                    "label": 0
                },
                {
                    "sent": "So consider this case.",
                    "label": 0
                },
                {
                    "sent": "This sort of running example that we had of polynomials, so I have these eight data points an I can model them with the linear sorry, constant linear, quadratic, cubic up to 7th order polynomial.",
                    "label": 0
                },
                {
                    "sent": "Clearly with eight data points I can perfectly interpolate.",
                    "label": 0
                },
                {
                    "sent": "The data with the 7th order polynomial.",
                    "label": 0
                },
                {
                    "sent": "What I've shown you in blue is a maximum likelihood fit.",
                    "label": 0
                },
                {
                    "sent": "To these different polynomials of the same data, now we're all very familiar with this model selection idea, where we want to, on the one hand, avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "This definitely looks like overfitting.",
                    "label": 0
                },
                {
                    "sent": "Imagine what we're doing here is between these two data points, where extract interpolating this completely ridiculous.",
                    "label": 0
                },
                {
                    "sent": "Ridiculous form of the function that just seems like overfitting to me.",
                    "label": 0
                },
                {
                    "sent": "It's impossible to prove, but seems like overfitting.",
                    "label": 0
                },
                {
                    "sent": "Here we seem to be under fitting.",
                    "label": 0
                },
                {
                    "sent": "There could be some structure in the data, but we fit only a constant, so we want to do model selection or model comparison, both avoiding underfitting and overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the.",
                    "label": 0
                },
                {
                    "sent": "Use of the marginal likelihood to do this.",
                    "label": 1
                },
                {
                    "sent": "So here is how we could use just the same things that I talked about.",
                    "label": 0
                },
                {
                    "sent": "The sum rule in the product rule now in the form of Bayesian model comparison to be able to avoid both models that are too simple and models that are too complicated.",
                    "label": 0
                },
                {
                    "sent": "So imagine we have two model classes, M&M Prime.",
                    "label": 0
                },
                {
                    "sent": "Now if we want to compare them clearly we can give different priors to Eminem prime, a priority we might prefer the simpler model or the more complicated model.",
                    "label": 0
                },
                {
                    "sent": "Let's say that's what this term is, but.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to focus on is this term here, which is that marginal likelihood, also called the model evidence or the integrated likelihood, and this basically is the integral over all possible parameter values of the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Times the prior integrated with respect to the parameters.",
                    "label": 0
                },
                {
                    "sent": "Now clearly the maximum likelihood is going to get is going to be higher at higher for the more complicated models because we're optimizing this thing over.",
                    "label": 0
                },
                {
                    "sent": "More and more parameters and we can fit the data better and better, but the marginal likelihood doesn't necessarily go up as we get more complexity in our models.",
                    "label": 0
                },
                {
                    "sent": "In fact, typically what happens is that the marginal likelihood will either go up and come down as the model complexity varies, or it will go up and sort of gently plateau, depending on how I define my nested class of models.",
                    "label": 0
                },
                {
                    "sent": "How do we understand that this is at the core of Bayesian Occam's Razor?",
                    "label": 0
                },
                {
                    "sent": "This sort of automatic preference for simplicity that comes from doing averaging.",
                    "label": 0
                },
                {
                    "sent": "It doesn't come from anything else.",
                    "label": 0
                },
                {
                    "sent": "It comes from following the rules of probability theory.",
                    "label": 0
                },
                {
                    "sent": "So in this case, the marginal likelihood is doing an averaging over the parameters and it's encoding.",
                    "label": 0
                },
                {
                    "sent": "This notion of simplicity in the following way, so we can interpret the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "As three different sort of equivalent ways.",
                    "label": 0
                },
                {
                    "sent": "Is the probability that randomly selected parameters from the prior would have generated data set D?",
                    "label": 1
                },
                {
                    "sent": "So one way of computing this integral?",
                    "label": 0
                },
                {
                    "sent": "Is it's a very bad way.",
                    "label": 0
                },
                {
                    "sent": "It's a very inefficient way, but it's conceptually correct way is randomly sample parameters from the prior.",
                    "label": 0
                },
                {
                    "sent": "Evaluate the likelihood and average.",
                    "label": 0
                },
                {
                    "sent": "OK, so clearly a model this more complicated like the 7th order polynomial.",
                    "label": 0
                },
                {
                    "sent": "If I randomly sample 7th order polynomials, they'll be all over the place, and so the probability that they assign to my particular data set could be incredibly low.",
                    "label": 0
                },
                {
                    "sent": "Here's a different interpretation.",
                    "label": 0
                },
                {
                    "sent": "Is the probability of the data under the model averaging over all possible parameter values?",
                    "label": 1
                },
                {
                    "sent": "That's just a verbal way of writing down this statement, and again, if you think about it a little bit, then there's no reason to think that more complicated model will give higher probability to the data if you happen to like information theory, you can interpret this as log of one over.",
                    "label": 1
                },
                {
                    "sent": "This marginal likelihood is the number of bits of surprise at observing data D. Under Model M. So you know, for a simpler model.",
                    "label": 0
                },
                {
                    "sent": "A complicated data set will be surprising.",
                    "label": 0
                },
                {
                    "sent": "But also for a complicated model, A particularly simple data set could be surprising, OK?",
                    "label": 0
                },
                {
                    "sent": "So we can kind of formalize this or sort of visualize this in this diagram, where what I've drawn here is on this axis, I've strung together all possible datasets of some particular size and this is just the.",
                    "label": 0
                },
                {
                    "sent": "Probability of the data given the model class for three different models, and essentially what's going on is a simple model like the constant or the linear polynomial is is concentrating its probability mass on some particular datasets.",
                    "label": 0
                },
                {
                    "sent": "A more complicated model like that 7th order polynomial is spreading its probability mass over more possible datasets.",
                    "label": 0
                },
                {
                    "sent": "But the rules say that all of these probability distributions have to integrate to one.",
                    "label": 0
                },
                {
                    "sent": "So if a complicated mass chooses to spread its bets over more models, necessarily it has to put less mass on the simple models.",
                    "label": 0
                },
                {
                    "sent": "OK. Of the simple datasets, sorry.",
                    "label": 0
                },
                {
                    "sent": "So the game is all is a fair game.",
                    "label": 0
                },
                {
                    "sent": "All models come in with with, you know, one euro of probability mass that they are allowed to spread over different possible datasets.",
                    "label": 0
                },
                {
                    "sent": "They come into the casino of modeling with their one euro.",
                    "label": 0
                },
                {
                    "sent": "They each spread it differently.",
                    "label": 0
                },
                {
                    "sent": "The real world rolls its dice.",
                    "label": 0
                },
                {
                    "sent": "A data set appears and the winner could be either a simple model or a complicated model depending on how they spread their probability mass.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}