{
    "id": "y4byuecqp7nstplkopqi7ubotejjhool",
    "title": "Deep Learning:Theoretical Motivations",
    "info": {
        "author": [
            "Yoshua Bengio, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_bengio_theoretical_motivations/",
    "segmentation": [
        [
            "So I'm going to.",
            "Not introduce myself.",
            "And get started with the second lecture.",
            "Which will go a bit more into networks, but staying at a fairly conceptual level.",
            "You'll see more about algorithms like, especially the optimization part this afternoon with Leon Bottou and tomorrow with Hugo Larochelle.",
            "But today it be more about the.",
            "Theoretical motivations for deep learning."
        ],
        [
            "So I guess you're here because you've heard about deep learning, and many of you have already played with these algorithms, and what were they about basically?",
            "Deep learning is about learning representations.",
            "And in particular, learning multiple levels of representations and the idea is that these multiple levels of representation correspond to multiple levels of abstraction an presumably, as I'll try to convince you if we're able to do a good job of capturing these high level abstractions, we can generalize well.",
            "And this is of course correlated with the amazing results that have been obtained in computer vision, object recognition, object detection and speech recognition, and more recently, fairly impressive progress in natural language as well."
        ],
        [
            "So as Pascal was saying, the the evolution of.",
            "Of systems that can be used to to answer questions and that we want to be more intelligent can be traced to this idea of incorporating learning and then incorporating.",
            "Representation, learning and then multiple levels of representation.",
            "So in the old days of AI and symbolic machine, not machine is symbolic.",
            "AI basically the knowledge about the world that we wanted to put in the machine.",
            "Was provided by the humans, the experts.",
            "That's why they called these expert systems.",
            "An innocence we built a function from input to output using hand design program that had two parts, the facts and the logic that allows to combine those facts to to answer questions.",
            "Then came classical machine learning which tried to solve similar problems.",
            "Then the classical AI problems.",
            "Not trying to do for you, but just doing something much simpler like pattern recognition.",
            "But it relied on hand design features like Pascal was telling you about.",
            "In fact, if anybody I'm sure there's many of you here who have played with machine learning trying machine learning, especially in industrial setting, and most of the time that you guys have spent this probably to design the input of the machine learning system.",
            "And then we use some kind of generic say classifier to produce the output, so that's that's what machine learning has been for many years and continues to be in many applications very crucial part to do this hand design of input features.",
            "You networks having there as long and the sort of standard 1 hidden layer or two hidden layer in your nuts, you can think of as in addition to having a linear classifier on top, having this layer of features that are learned automatically at the same time as the classifier and deep learning is just.",
            "Going further with this idea and saying, oh we can have multiple levels of features composed on top of each other in order to come up with.",
            "Our final solution.",
            "OK, So what I'll try to tell you about in the rest of this lecture is why this kind of approach may be working so well and and I don't claim that I have all the answers.",
            "There are many reasons why some of these systems are working, and some of them I won't talk about like some of the priors that come into convolutional Nets that will be discussed by other lecturers, but I'll talk about some of the."
        ],
        [
            "Basic general ingredients that I think explain part of the success of these deep learning systems."
        ],
        [
            "Before I get there, let me just step back a little bit and tell you about how we might build really complicated systems for AI.",
            "And how this is related to?",
            "How this question is related to famous result from machine learning called No Free Lunch Theorem.",
            "So what I think are the basic ingredients for successful machine learning, especially towards the eye are the following.",
            "You need lots and lots of data.",
            "That's number one and Pascal mentioned it an why do we need lots and lots of data?",
            "You have to understand why we're trying to build a system that's going to take decisions and in order to take good decisions, which is what.",
            "Basically intelligence is about.",
            "You need a lot of knowledge.",
            "Right, so an AI system or something that's solving a complicated task needs a lot of knowledge about the task that is trying to solve.",
            "Where is that knowledge going to come from?",
            "Well, it either comes from humans who put it in or from data and the machine learning part is about extracting knowledge from data.",
            "That's what machine learning is about, and if we want to learn a complicated task like understanding language or images, because these tasks are fairly complicated, we're going to need a lot of data unless we already know how to solve that problem.",
            "And then we don't need that, but but typically we don't know enough and that's why we're going to be using machine learning now as sort of a culinary to this.",
            "It's not enough to have lots of data, you need to have a family of functions or model that's rich enough to represent that complicated function to represent all that knowledge, right?",
            "So the knowledge comes in the form implicitly in the beta, but then we have to translate that knowledge into something that we can use for taking decision.",
            "That's our model.",
            "Our neural net.",
            "And of course we have to store that knowledge somewhere that it starts from the data.",
            "It goes into a model, so the model has to.",
            "Be big enough, flexible enough, they can represent the kind of complicated function you need for your solving your task, right?",
            "So so for sure we need these two things now.",
            "Some people thought that might be enough.",
            "I don't think so.",
            "And a lot of what will be telling you about later is about the third ingredient, which you can basically think about as additional prior knowledge.",
            "And in particular, I'll talk about the curse of dimensionality and Ann and how it's it's a challenge and how we might get around it using some of the players that come with most deep learning algorithms.",
            "Alright, so this is going to be the subject of much of my presentation today.",
            "The third part, right?",
            "So given one and two data, an powerful flexible family of functions.",
            "Is not enough.",
            "We're going to need something else which is fairly general knowledge about the world.",
            "But that's going to be necessary to get good generalization.",
            "Classical nonparametric statistics works with one and two and a very weak prior, which is called a smoothness prior, which I'll explain a bit later.",
            "But it's not enough."
        ],
        [
            "Alright, so I guess I already said much of this in order to get the knowledge we need data and we need to learn.",
            "We need learning algorithms and learning algorithms involve two things.",
            "Prior, so in other words, some kind of preference over the set of functions baskal talked about choosing a parametric family function, but you could be a bit more so saying well, there's some functions I prefer more than others, like if we use regularization technique, it says some of those functions.",
            "I prefer some I prefer less, but you know, maybe I'll end up choosing these.",
            "And you need some optimization techniques, because choosing a function in an infinite set is, you know, potentially intractable, but some some approximation or some search method will be required now to get the machine learning to work.",
            "Pascal told you about generalization.",
            "This is really central.",
            "Of course in machine learning, but one thing he started talking a bit about, which I'll come back to is a kind of geometric interpretation of what generalization means.",
            "So for people who know a little bit about probability, the way I like to think about it is generalization.",
            "The game of Generalization is a guessing game.",
            "And it amounts to guessing which configuration of the variables of interests are good or likely.",
            "So you can think of it like you know, guessing where probability mass should go should be concentrated.",
            "I'll come back to that notion in my slides.",
            "And this this kind of geometric view of generalization is interesting because it also connects with another important idea in machine learning is statistiques, which is the curse of dimensionality and again I'll try to explain this.",
            "But basically what happens is we're dealing with these high dimensional random variables like pixels like sequence of characters like acoustic signals and because there are so many dimensions.",
            "And each dimension can take many values, maybe even just two already is going to be a lot because you know.",
            "1002 two power thousand.",
            "We shared a number of images of 1000 pixels with binary pixels is already huge.",
            "Is already much too large to be able to enumerate and we're not going to have enough data to consider all of those configurations.",
            "So for most configurations we can have zero data that tell us what the right answer should be.",
            "That's the problem right?",
            "In the 2D simple 2D case that Pascal showed, it looked like generalization is easy, but in a high dimensional space generalization looks almost.",
            "Impossible.",
            "But yet we have algorithms that you know, do a decent job.",
            "So how can we potentially deal with this curse of dimensionality?",
            "And one of the big ideas that I've been trying to sell in many of my papers for the last almost the last decade is this idea that.",
            "In order to defeat the curse of dimensionality we want to.",
            "Figure out how the data was generated to understand the explanatory factors that explain that that that that are the causes of what we observing.",
            "If we could do that, of course it's a tall order.",
            "Then we would basically be making sense of the data and we'd be able to make very good predictions on your example.",
            "That's that's of course very ambitious program, and I think that what we currently doing in deep learning is a step in that direction.",
            "We have by no means solve this problem.",
            "This is what science is trying to do, right?",
            "We're trying to make sense of the world by doing experiments and building theories that explain what we observe.",
            "So machine learning is basically kind of automated science.",
            "You can think of it like this, but on a scale where we're looking at lots and lots of data.",
            "Alright, so let's go a little bit deeper into these questions."
        ],
        [
            "But first let me go back to this nation notion of the curse of dimensionality and why the classical nonparametric methods like nearest neighbors that Pascal talked about.",
            "Don't cut it why they?",
            "They are really hurt by the curse of dimensionality, right?"
        ],
        [
            "So remember I said the curse of dimensionality arises because we have many configurations of the random variables.",
            "So if you have only one random variable, say A1 dimensional thing and and we're trying to learn from it like to predict which values are likely or what, why to associate to a given value of X where so?",
            "These are the different values of X we could we could be in the X is we could break the input into say 10 different values and then we can count how many times each value comes, or for each bin we could we could average what value of Y comes up.",
            "And that would be a simple nonparametric predictor which is basically on a histogram.",
            "And that would work well actually, if if there's only one viable and it's sufficient to break it into 10 different values, 'cause we would need to accumulate what you know.",
            "Maybe 100 examples an we would probably covering enough each of the cells in this discretization to to give a decent predictor an we could maybe extend this to two dimensions.",
            "So instead of having one verbal, we have two variables.",
            "Now we have 10 by 10 configurations of interests, but if we have three variables Now, we have 10 times Thames Time Thames.",
            "Configurations of interest and, well, it's not so bad.",
            "Actually it's only 1000 configurations, but you can see that it's growing fast and if we only had 100 examples that were maybe enough here, it's not going to be enough here.",
            "There may be a lot of configurations that we don't see.",
            "Maybe some of them get a lot of examples and most of them get 0 examples.",
            "And if you have not three variables but a million variables, well, we're totally dead.",
            "Even with 100 rebels were already dead so.",
            "There are just too many configurations.",
            "How could we possibly generalize to new configurations?",
            "We have never seen?",
            "That's the game of machine learning.",
            "How do we say something about configurations we've never seen?",
            "So the classical solution to this?",
            "Well, the classical approach to nonparametric statistics is to say, rely on smoothness.",
            "So in the 1D case, when we discretize the spacing OK, we're going to say that all the values between 2.5 and three are going to be considered equivalent, and we're going to be able to lump all those examples or average in some neighborhood, like with nearest neighbors.",
            "Basically what we're saying is we're seeing the function we want to learn is smooth enough that.",
            "I don't care about the exact difference between the value at 2.5 and the value of three because the function changes very smoothly.",
            "An in low dimension that works, but in high dimensions.",
            "What happens is that if I want to look at the little neighborhood of values that I'm going to average around neighborhood of examples, I'm going to average around.",
            "I'm going to end up with either.",
            "An empty ball with no example in it or a ball that contains almost all of the examples, and it's going to be useless.",
            "So it's something funny that happens in high dimension that you you you can't just average locally and get something meaningful.",
            "OK, so so there is this notion of dimensionality which.",
            "Which comes up.",
            "But actually if you dig deeper mathematically, turns out that it's not."
        ],
        [
            "Really, the number of dimensions.",
            "It's the number of variations of the function you want to learn.",
            "Remember I said that these these statistical methods rely on smoothness of the underlying function and smoothness.",
            "Basically, you can think of it how many Upson Downs are there?",
            "If it's, if it's linear, there's just it's very, very smooth.",
            "If it goes up and down just a few times, it's less smooth, but it's still very smooth.",
            "If it goes up and down a million times, well then it's it's getting to be complicated function actually.",
            "The kinds of functions we want to learn may be very very non smooth.",
            "So if you're trying to do computer vision.",
            "Or is trying to do natural language or speech at the functions we actually want to learn are very complex.",
            "The number of Upson Downs or the number of different regions that we actually want to distinguish?",
            "Could be very large, so a few years ago we showed that the.",
            "Issue with the curse of dimensionality really is not dimensionality, it's it's the number of ups and downs.",
            "So in particular, many nonparametric.",
            "Statistical methods rely on the Gaussian kernel or some kind of similar looking Gaussian shape which is used to average in some neighborhood.",
            "And with these kinds of methods we showed that the number of examples you need to get good generalization has to do is is proportional to the number of apps and downs of the function that you're trying to capture.",
            "So if it has 2K zero crossings, then you need order of K examples, and that's a problem, because if you have many dimensions then the number of apps and downs could be exponential in the number of dimensions, but it doesn't have to be in high dimension.",
            "You can have a 1 dimensional function that's incredibly complicated.",
            "And it would still be hard to learn, even though it's 1 dimensional, OK?"
        ],
        [
            "So the geometric view now.",
            "That I I'll I introduced earlier, let me try to visualize a little bit.",
            "So remember Pascal told you about the training data as the empirical distribution and he showed these plots.",
            "These these these examples are circles, so I'm going to do the same thing.",
            "Now.",
            "Let's say you're trying to model the distribution of X&Y here.",
            "Or maybe you're trying to predict Y.",
            "Given XI don't care, the point is, you're trying to find which configurations of X&Y here in that plane.",
            "Are likely so that you can answer questions like you know.",
            "Is this configuration likely or is this configuration likely?",
            "Or if I know X, which value of, why should I use right?",
            "These are the kinds of questions you're using machine learning for.",
            "Now the problem is we know the answer at these points.",
            "We know these points are likely, which means we can answer about about those points, but what about everywhere else in this 2D plane?",
            "So the smoothness assumption says the function we're trying to learn is smooth, meaning that if if we know its value at some point X, we know that in some neighborhood the value should be close.",
            "So if we know that the value the probability function should be high here, we could guess that it should be high in some neighborhood, right?",
            "So we could just draw a little balls or little kernel, little Gaussian kernels around each example.",
            "An and of course that's what many nonparametric statistical methods do.",
            "In this 2D case, it might actually work reasonably well, but of course we're trying to think about what happens in high dimensions.",
            "And what's going to happen in high dimensions is that these balls are either going to be so large that they cover everything, or they're going to leave a lot of holes in places where there should be a high probability.",
            "So it's not going to be good enough in order to really workout this data set to make our solutions really generalize, what do we need here?",
            "We need to discover something a bit smarter about that data that there is some structure, right?",
            "Where is that structure?",
            "Well, here we could guess the structure is that there's a 1 dimensional manifold near which we have a high probability concentration.",
            "In other words, that the points near that manifold have a high probability of actually happening in the data.",
            "So this is something we'll come back to a lot later in the week with with other representation learning algorithms.",
            "The reason is connected to the notion of representation.",
            "Is that?",
            "If you think about it.",
            "Is this low dimensional representation of the?",
            "There is a low dimensional representation of the data, which is the position on that curve, right?",
            "So we started here with a 2 dimensional input and I'm saying there's a concentration of probability in some region, so these are the configurations that actually happen more often.",
            "If you could discover that representation, then that I'm sorry that concentration.",
            "Where is it that concentrates?",
            "We basically solve our problem, but there's a way to think about this, which is if we could discover a new representation which is a 1 dimensional representation or or maybe just a 2 dimensional representation, but we changed the axis right?",
            "So we can have one axis which is going to be the position on the manifold, right?",
            "And we can have another access which is the direction orthogonal.",
            "So if you're here.",
            "You are some distance away from the manifold.",
            "That's one axis, and then you are at some position on the manifold, so it's a change of representation and you can see that under that representation many things become easy.",
            "For example, of course, you can get the density very easily because on one of the dimensions you know maybe maybe this is the position zero according to that dimension, and so everywhere at position 0 has a high probability and everything else has a low probability.",
            "So that's that.",
            "Makes the density problem very easy, but it also allows you to do things like interpolate between examples more meaningfully, and maybe predict things more easily.",
            "Now you can have kind of what we've taken this manifold which was.",
            "Nonlinear and maybe in reality we're going to have manifolds which are very, very complicated, and by changing representation we essentially flatten the manifold, right?",
            "So in the new representation space where one dimension is the position here and the other is orthogonal to it.",
            "It's like if we made the whole thing Euclidean again.",
            "And then we can do linear things in that space in order to make predictions and interpolate and do all kinds of fun things right?",
            "So so we turned a complicated distribution, which was.",
            "You know this this nonlinear manifold into something that's flat and Euclidean.",
            "Now we can do things like PCA and and density estimation very easily.",
            "Alright."
        ],
        [
            "So me.",
            "Go back to this curse of dimensionality problem.",
            "What I've been trying to say is that if we only use smoothness, we're not going to be able to defeat the curse of dimensionality and smoothness has been the ingredient in most statistical, nonparametric, statistical methods so.",
            "We want to be nonparametric in the sense that we want to be able to have families of functions that grow in flexibility as we get more data and you will.",
            "Nets are nonparametric in that sense, right?",
            "So there, parametric in the sense that if we fix the number of units, we have a fixed set of parameters.",
            "But actually, that's not the way we use neural Nets.",
            "The way we use neural Nets is we allow ourselves to change the number of parameters by changing the number of hidden units.",
            "Depending on how much data you have using a validation set, as Pascal was telling you at the end.",
            "So we have.",
            "The ability to let our model grow with a number of examples, but.",
            "We need to go beyond this this smoothness prior.",
            "It turns out that with deep learning there are at least two priors that week that are basically coming in for free.",
            "And I'm going to now explain them.",
            "So one of them is basically coming under the name of distributed representations and the other is basically about depth.",
            "And what I'll be telling you about is that these suppliers have a thing in common, which is that there are ways to exploit composition at compositionality and put it into our our model.",
            "In other words, we were going to say that there is something about the world out there, the way that the world is generated are we coming back to this notion of understanding how the world was generated?",
            "And making hypothesis about how it was generated.",
            "But here we're going to make it very, very simple assumption that the way the data we're observing came up is by some composition of pieces, and the composition can be either parallel or sequential, or both.",
            "So the parallel case is going to be the idea behind distributed presentations, and that's what I'm going to be spending.",
            "A lot of time in next few slides.",
            "And basically it's the same idea of you know, feature learning, but I'm going to explain how it's making assumptions about the world.",
            "And the second is the depth.",
            "So in other words, we're going to have multiple levels of feature learning an, whereas here it's going to be kind of parallel composition.",
            "We choose this feature and that feature or that feature to be active, and we have a subset of features that are active to represent our input.",
            "A pattern of activation of features.",
            "In the case here, we're going to be.",
            "Not taking these functions in parallel but in sequence, so we're going to have.",
            "Functions that are obtained by composing other functions that are obtained by composing other functions, and we get that with deep architectures, and we get that with recurrent Nets and.",
            "So we have these two things.",
            "As you'll see, we will basically have added an additional prior or additional kind of intuitive notion of how the data come came about, which is assuming some form of compositionality explains the things we are observing in the world.",
            "Any questions up to now?",
            "Yes.",
            "Yes.",
            "Yes, OK, so so there are different definitions of nonparametric.",
            "But the one that I like is 1 where we basically say nonparametric.",
            "Really means that we don't have a fixed parameter vector.",
            "Depending on the data we get.",
            "We can choose a family of functions that's more or less flexible.",
            "So a linear classifier is parametric because I can't increase the flexibility even if I get 10 times more data.",
            "I'm stuck with my linear model.",
            "On the other hand, with the neural network, if I get more data, I'm going to choose more hidden units and maybe more layers as well.",
            "So the family of functions that I'm allowed to choose from is not fixed apriori.",
            "It's actually growing depending on how many examples I get, and that's what nonparametric really means.",
            "It's not about having no parameters, it's about not having a fixed parameter vector, but allowing yourself to choose the number of parameters that you need.",
            "Depending on the richness of your data.",
            "Is that answering your question?",
            "Absolutely.",
            "Yes, if you exactly So what you have is a non parametric family which uses as a building block a parametric family.",
            "Metric.",
            "In fact, in many state of the art.",
            "Say in computer vision neural Nets.",
            "There are many more weights, many more parameters than examples.",
            "So we're talking about like millions of examples an hundreds of millions of parameters.",
            "Typically, it's a very common situation, all right.",
            "Any other question?"
        ],
        [
            "So yeah, before I get there, I show you this picture to illustrate.",
            "That comes from Hong, likely, who will be speaking shortly, maybe in a couple of days.",
            "Anne Anne Anne drawing at Stanford.",
            "A few years ago, which illustrates one aspect of of.",
            "Multiple levels of representation, which is that these representations can capture.",
            "Kind of hierarchy of parts, so one way to get a composition of functions.",
            "Is to have functions that detect.",
            "Features like edges or parts of a face.",
            "Anne compose them to get higher level objects or parts right?",
            "So here what you see is that using these stacks of RBM's they were able to have the first level units that detect edges so they liked oriented contrasts at some locations and the next day they had units that preferred things like a nose or and I, and then the next level they had.",
            "Units that preferred actually full faces in different configurations, so it's it's a it's not exactly what is going on.",
            "There's much more to it, but it gives a glimpse of what multiple levels of representation could be apart space hierarchy, but there's there's more to that."
        ],
        [
            "As I'll try to argue an exponential advantage over methods, I don't use this this.",
            "This prior this structure."
        ],
        [
            "OK, so the methods that don't use a distributor representation are things like classical clustering, N grams for language models, nearest neighbor methods, kernel, many kinds of kernel, SVM's, most nonparametric density estimation or regression using local kernels.",
            "And so that's a lot of algorithms, and there's many more or also decision trees.",
            "For those of you who know about this in trees.",
            "There's a very simple way to understand what these algorithms do at a very high level.",
            "What they do is you take your input space.",
            "So we have the input space and again I'm doing a 2D thing, but you have to think this is in 1000 D or a million D. We have the input space and we're going to break it into regions.",
            "We're going to partition the input space and for many of these, algorithm is actually a hard partition like this.",
            "For others is something a bit more subtle where you have some some kind of soft partition.",
            "Where are you allowed to know?",
            "Kind of smooth interpolation between nearby regions, but but basically behind this you have a notion of regions.",
            "And the important point is that we're going to have a different set of parameters for each region.",
            "We're going to be able to tune what the answer should be in each region and where those regions are by using the data.",
            "For example, if you're doing clustering, maybe then we have some examples or some centers that correspond to the centers of these regions.",
            "And if you're doing one years neighbors, maybe every example is used to define a region.",
            "And in those two cases you actually get is some kind of ranalli partition of the input.",
            "So.",
            "You can think of how, which is the function we're going to compute.",
            "Here.",
            "As related to how many regions there are 'cause I'm able to change the output in in each region depending on say how many points come here, or whether the labels are that are associated to each region.",
            "To be the examples of following each region so so there's there's, there's a notion of complexity that's tide to how many regions I have.",
            "The complexity in the sense of how rich, how flexible is my function.",
            "So if I'm doing nearest neighbor, only have 3 examples.",
            "My function is going to be very simple.",
            "It's going to be some very simple partition if I have a million examples I can actually represent something very complicated, right?",
            "Now.",
            "We can think in terms of sort of learning theory.",
            "Ask the question how are we going to generalize?",
            "And that's going to depend on the relationship between the number of examples I need and the kind of.",
            "The kind of complexity I get, right?",
            "So what do I mean here?",
            "If I want to have a richer function?",
            "I'm gonna need more regions right?",
            "And to be able to define those regions, I'm going to need more.",
            "Examples, so the there's a in for these kinds of models, there's a linear relationship between a number of distinguishable regions and the number of parameters, or equivalently, there's a linear relationship between a number of distinguishable regions and the number of training examples.",
            "I want more regions anymore, more data, right?",
            "It says, and the relationship is linear.",
            "And you might think that's the only way that is possible, unless I introduce some extra constraints or prior.",
            "If I don't know nothing else.",
            "That's all I can do right if I want to be a bit finer about what's going on this region, I need more examples.",
            "I can split this region in two and have a different answer in these two places.",
            "OK."
        ],
        [
            "But there's another option, and that other option is basically what's going on with neural Nets an and in general with distributed representations so.",
            "This is maybe one of the most important slides of my whole lecture, so please pay attention.",
            "So we are also going to define a kind of partitioning of the input space.",
            "But we're going to do it in a way that's incredibly efficient.",
            "We're going to do it in such a way that we're going to have an exponential number of regions at the price of a linear number of parameters.",
            "So we'll be able to say something, a function about a function that looks very complicated.",
            "In the sense that it has many ups and downs, many different kinds of values, many regions I can distinguish, and yet I don't need that many examples to learn it, right?",
            "That's the magic part that Pascal was talking about, but it's no magic as well.",
            "See, there's a reason why this is happening, and it means something about.",
            "An assumption we're making regarding the world that generated the those data.",
            "It may be that the these kinds of models like we have in your nuts.",
            "Are not applicable everywhere to any random function.",
            "In fact, they're not.",
            "They work well in this universe in this on this planet.",
            "Right, so let me go in a bit more detail.",
            "How are we going to do that?",
            "So.",
            "The way we get our regions, I'm going to consider a very simple case.",
            "You can have more complicated cases, but I'm going to do a very simple case to illustrate.",
            "We're going to have just one hidden layer of units, like here.",
            "This is the two dimensional input.",
            "And let's say we have these three units and again, I'm going to make these units as simple as can be, so.",
            "Linear classifiers with A10 answer binary classifier, so each hidden unit says one.",
            "Or zero, depending on which side you are on the hyperplane.",
            "So you know the weights of these units and the bias together define a hyperplane in input space on one side of the hyperplane.",
            "The unit says one on the other side it says zero OK.",
            "So each unit defines a hyperplane and now we have some configuration here.",
            "Corresponding to saying on which side of each hyperplane we are.",
            "So if we have 3 units.",
            "We could think we have 8 configurations.",
            "Actually there are only seven that exists.",
            "Because, well, because we're in 2D and but that's a detail.",
            "So what's going on here?",
            "We have more regions.",
            "Then the number of.",
            "Well.",
            "If we extend this to higher dimension, we're going to have a number of regions that grows exponentially with the number of features, and so with the number of parameters.",
            "Because the number of letters is the number of features times the number of inputs.",
            "Whereas the number of regions is essentially 22, the number of features.",
            "Now if you actually have less dimensions than features is some correction for that, but it's still nearly exponential.",
            "In fact, it's exponential in the input size.",
            "OK, so.",
            "You can see that there you know all the configurations of these of these bits that can occur.",
            "Some of them can't, but.",
            "But essentially, if.",
            "If we have the input size large enough, we can have an exponential number of regions.",
            "OK, so compared to the situation we had previously.",
            "We had a number of regions that was linear in the number of parameters or linear in number of examples that we need.",
            "Here we have a number of regions that can potentially grow exponentially with the number of parameters.",
            "An end with a number of examples.",
            "So how is that possible?",
            "You know there must be a trick, right?",
            "Is is it magical or there's a cheat?",
            "Well, every prior is a kind of cheat.",
            "It works for some things, but it doesn't work for everything.",
            "So you see that you know those regions.",
            "I have these seven regions here.",
            "I can't really choose their locations and also let's say I put linear classifier on top of this.",
            "I can't choose the answers I'll get in dependently for each region.",
            "So.",
            "There are some constraints about the kinds of functions that can represent.",
            "Not every function or not.",
            "Every partition here is is feasible, whereas in the other case I could do any partition, right?",
            "It was a very very flexible model.",
            "So this is going to work under some conditions, so let's try to figure out what those conditions are.",
            "Let's keep them for this.",
            "I'm going to give you an example.",
            "Imagine that now the input is an image of a person.",
            "And that my 3 features are binary detectors, so they're going to have to be nonlinear rather than here.",
            "They can be binary detectors that tell me if the person is.",
            "Is a tall or short?",
            "Maybe there's another one that tells me if the person is male or female, and maybe there's another one that tells me if the person wears glasses or not.",
            "OK, so now you see that.",
            "With these kinds of features, it makes sense that we can learn about each of those features.",
            "Almost independently of each other, right?",
            "I can I can train detector.",
            "I could potentially imagine training a detector or having machine discovering what you wearing glasses means visually.",
            "And I don't need to see all the configurations of the other features for learning that.",
            "'cause detecting you know I glass detector is going to work should work independently of how tall the person is, whether you know the person is male or female.",
            "I mean that might be some little interactions.",
            "Some females have funny glasses maybe.",
            "But wow, from from my point of view of course.",
            "So there might be some interactions, but mostly you can learn about each of these features independently of the other, and so you can see that.",
            "And you can have more than three where you could have a thousand of these features.",
            "You can see that the number of examples I might need to learn those features doesn't need to grow as two to the thousand.",
            "It just grows like 1000.",
            "The more features I put in, you know, each time I add a feature I don't need to double the number of examples I see I just need to have enough examples of that feature being present or not present.",
            "And so so we get what we want, right?",
            "But but it works because.",
            "We're assuming that the world is arising out of the interactions of those features that some say are one summer, 0 for a particular example, and for any particular example, we just pick a pattern of these.",
            "And we can pretty much learn independently.",
            "Each of these features.",
            "If we were given, you know enough the appropriate kind of data that covers their variations, we should be able to learn about each of these features more less independently.",
            "So that's that's what gives us an exponential advantage, because now we don't need.",
            "The number of regions to corresponding number examples.",
            "We can have many regions which are all the configurations of those features, and yet the number of examples is reasonable.",
            "It grows only linearly with the number of features.",
            "Do you have some questions about this?",
            "Yes.",
            "Well, I gave you a real world example of an image.",
            "With.",
            "That's the input, and the features are binary features which tell you something about the image.",
            "Different aspects of it may be different parts, so I talked about glasses about the size of the height of a person and you can.",
            "You can imagine you know many more features, but the point is, if you imagine having you know somehow data that characterizes your distribution and that covers maybe the two different values of each feature.",
            "To keep things simple at their binary, they don't have to be.",
            "The.",
            "You don't need to see the configurations of all of the other features to really learn about each one, and that's.",
            "That's different from from the situation with nondescript representations where somehow I need to see all the configurations of my pixels to be able to generalize here.",
            "The thing that's going to make me able to generalize is that.",
            "I don't need to see all the configurations of those features to be able to make a meaningful statement right.",
            "So for example.",
            "It could be that I've never seen 000 here, but I've seen the first feature being zero alone.",
            "I've seen the second feature being zero in some other configuration, and I think that I've seen the third feature being zero and maybe they just sort of add up to come up with with some weights to come up with an answer about something you care about that maybe only depends on some subset of them, or some linear combination of them so.",
            "So that's what allows you to generalize that you are able to.",
            "To say something about configurations of these features that you have never seen because individually they keep their meaning like this.",
            "The meaning of tile person.",
            "This is not tall person is something that remains meaningful whatever the other elements of the visual scene are.",
            "And you don't need to see all the configurations of all the other variables in order to be able to make sense of that feature, yes.",
            "Well, they don't have to be exactly independent, but if they were in some in some in the right chosen way of formalizing it, yes, that would be really helpful in practice.",
            "Of course, they won't be exactly independence.",
            "I gave him my example.",
            "Maybe you know the female glasses in the glasses are not exactly the same, so you actually need to see.",
            "Both of these.",
            "But there are many things about glasses that don't depend about whether you know how the style is is just the shape and where they are, and these things could be learned independently of the other configurations.",
            "Yes.",
            "Maybe I don't understand your question, but yes, there's, there's some math that tells things about how many regions we get depending on number of features and number of inputs, and maybe that's not the question you are asking.",
            "Yes.",
            "Right?",
            "Maximum over what set?",
            "I don't understand what the maximum corresponds to maximum in what sense.",
            "Oh, I see what you mean, yes, so so we have a NIPS paper in 2014.",
            "Which studies fairly deeply these questions.",
            "And.",
            "Regarding the number of regions, what happens is that.",
            "If you're not lucky, you might have.",
            "And our coincidences that make some of those hyperplanes do nothing, because you know, they kind of align in a funny way.",
            "But the chances of this happening is is like measure.",
            "0.",
            "It's very unlikely.",
            "So for most parameters configurations you're going to get the maximum number of regions.",
            "Yes, and we can count that we so we have.",
            "Approximate counting arguments that give us bounds on the number of regions you can get.",
            "Yes.",
            "Yes.",
            "Yes, but I haven't talked about depth yet, so I'm only talking about the notion of distributor presentation.",
            "So one layer I'm going to tell you what happens now if you combine multiple levels, but but even if you think about just one level of representation, you already get an exponential advantage here.",
            "So the number of examples you need to get a certain level of normalization might be exponentially smaller than if you use.",
            "If you don't use this prior.",
            "So if you just use the smoothness prior, might be.",
            "In practice we never see.",
            "Such an advantage, right?",
            "So if you compare like the best neural Nets for computer vision, ANAN the sort of state of the art using, say, pure kernel machines, we don't observe an exponential ratio of the number of examples needed for getting a particular transition, or we we get a big advantage, but it's not exponential.",
            "Uh huh.",
            "Right, so there's the question of what do we put on top of these features?",
            "So you need to have enough flexibility to exploit this representation.",
            "But the good news is if you need to do what I was trying to explain about these manifolds.",
            "If this representation is really good, what it really is doing is it's unfolding the manifold.",
            "It's like a new coordinate system where things are flat.",
            "And if you have really flatten the manifold, then basically linear classifiers can do a good job.",
            "Alright, yes.",
            "Discrete model is optimized on some of the parameters that we learn and that can help in this way.",
            "Yeah, I'm not sure to understand your question.",
            "But yeah, there are situations where you do want to have the kind of flexibility that the classical nonparametric methods bring.",
            "And so there so there's certainly room to think about how to combine them in places where you need, for example, a lot of memory to store instances, right?",
            "So neural Nets are really good at.",
            "Discovering representations that are captured.",
            "Semantic aspects.",
            "Like what is it that makes a cat be a cat?",
            "What are the features that makes a cat be account but?",
            "It's not very good at learning things by heart if I want to remember about you know my cat, your cat, and his cat, and maybe 1000 other cats.",
            "I might want to use some kind of memory based nonparametric type of model.",
            "So yes, there are certainly applications where you want to combine both.",
            "But the generalization comes from these representations.",
            "Yes.",
            "Well.",
            "I this is, I think, something we don't fully understand yet.",
            "The question you're asking about.",
            "First of all, the adversarial examples problem occurs for many machine learning is not really specific to neural Nets.",
            "2nd, 2nd, but you're right.",
            "You're right that having kind of RBF units kind of limits this problem, but but then, if you have a model that doesn't suffer from, you know this.",
            "These weird examples could actually happen, even though very rarely, or somebody could make them up.",
            "That's certainly something we have to worry about, but then not getting any good realization is not good either.",
            "You first have to have good realization and then if there are issues we need to fix them.",
            "So there's already some work showing that you can train with these adversarial examples and make the system a lot.",
            "Much, much more robust to them, but this is very, very sort of recent work, and we need more investigation in that direction.",
            "Yes.",
            "Creating like some clusters, like smart, but that's what classical non distributed presentations do.",
            "They take essentially all of the regions that they can given the data that they have.",
            "But they don't get any generalization out of this.",
            "In other words, if I give you a location in input space which you have no data, you're not able to say anything about it, whereas.",
            "With this approach, we are able to say something meaningful about configurations we've never seen before, and that is the essence of visualization.",
            "OK, I should probably move on.",
            "Yeah, yeah.",
            "Right?",
            "Yes, except that the pixels are except the pixels.",
            "That's right, except that the pixels are not a good representation.",
            "Because you can't learn about the distribution of each pixel.",
            "Almost independently of the different the other pixels, they're very strongly tide together.",
            "Whereas here I can learn about, you know what the relationship between the input and an each of these features almost independently.",
            "So in a sense.",
            "We're kind of factorizing.",
            "The distribution saying the way it comes about is that we have these sort of independent factors that explain what we see and these actually are the kinds of things we talk about in natural language, natural language.",
            "Those meant, I think, to explain those high level factors that that you know from one person to another person that explain the data.",
            "So, so the things we talk about that we put labels on tend to be explanatory factors.",
            "So whether the person is male or female or has glasses, or is short or has long hair, these are things we've put names on because they help us making sense of the world.",
            "There are high level factors, but not any.",
            "Yes, yes.",
            "That's right, that's right exactly.",
            "OK, I should probably move on."
        ],
        [
            "So this idea of this representation was really at the heart of the renewal of neural Nets that happened in the early 80s.",
            "That was called Connectionism or the connectionists approach, and two of the major players of this renewal or Jeff Hinton and David Rumelhart, and a bunch of others, of course.",
            "And they were kind of.",
            "Trying to present this way of thinking in contrast with the classical AI approach, which is based on the notion of symbols.",
            "Lucia symbol is something that's very natural when you think about symbolic processing of things like language or logic and rules where each concept.",
            "Is is associated with just a pure?",
            "Entity a symbol is just something that's either present or not present.",
            "And there's there's nothing intrinsic about those symbols that you know.",
            "Define some kind of relationship between them, whereas if we think about concepts as patterns of features, patterns of activations of neurons in the brain.",
            "Then the concept of cat and the concept of dog.",
            "Can be related in the sense that they would share some features.",
            "So here I've tried to illustrate that by saying OK, so I have some some symbolic representation here, which is my my input or it could be an output where there's a one.",
            "What's called 1 hot representation where I have a unit that says it's a cap and the other units are saying other things like dog and so on.",
            "And it's either one or the other.",
            "But I also have a distributor representation, so a bunch of features that every concept can have an an the features for CAT and the features for dog.",
            "So this is now the representation for dog and the representation for Cat somehow are close to each other because they share a lot of attributes there.",
            "Both pets and mammals and so on.",
            "And so if we if we.",
            "We are able to measure kind of distance between those symbols, whereas between symbols there is nothing to say.",
            "They're all different from each other in equal ways and this is something that's really important for for deep learning and for neural Nets to understand this notion of.",
            "Representation of concepts of of what may seem like symbolic concept to start with."
        ],
        [
            "This is something I built on with what's called a neural language model, starting with NIPS paper in 2000, of which actually Pascal, Anne and others are coauthors.",
            "We built models of natural language sequences of words.",
            "Using this idea of.",
            "I distributed representation for words, so the way we did this is we train the neural net that has input a sequence of words and tries to predict the next word.",
            "You could try to predict all kinds of things, but in that paper we just predict.",
            "The probability for the next word being any one of the words in the vocabulary.",
            "So we have a large.",
            "Output layer with one unit per word in the vocabulary an.",
            "We have also very large inputs where you have one hot representation for each word in the input window, so it's not a bag of words representation like Pascal said.",
            "Instead, it's just one hot vector for each position, and the crucial point here is that the first layer of that neural net is just a mapping that's learned that projects these symbols these integers.",
            "Into a district representation which is just a vector of numbers, and we use the same mapping for all the words independent of their position.",
            "Of course, each word in the vocabulary gets a different mapping and these mappings."
        ],
        [
            "Are things that we can visualize by two dimensional projections usually known in your projections, and we see clusters of world of words coming up that are semantically similar.",
            "So if we zoom in and zoom in again, we see things like, you know, countries having their vectors, their word vectors, their their district representations, being close to each other.",
            "And this is not something that was built in.",
            "It just discovers these representations that have these properties.",
            "Or maybe verbs come close to each other.",
            "Or conjugations of two become close to each other."
        ],
        [
            "And even more exciting is that these vectors somehow have dimensions that are learned that is completely discovered from scratch.",
            "That or are meaningful and whose meaning is.",
            "Preserved across space so you can do some kind of arithmetic with those vectors, simple arithmetic and an.",
            "It will allow you to do things like reasoning by analogy.",
            "So for example, if I take the vector for friends and I subtract the vector for Paris, I now get a new vector.",
            "Which is pretty much aligned with the vector I get by subtracting the representation from for Italy from the representation from Rome.",
            "So so these two different vectors are almost the same.",
            "They represent the meaning of the capital of a state.",
            "And you can do the same game with many pairs of words.",
            "So if you take King minus Queen, you get a vector which is almost the same as man minus woman.",
            "So why is that happening?",
            "This is initially we were very surprised about this and it seems almost amazing.",
            "But it makes a lot of sense.",
            "So if you think about a pattern of activation corresponding to King and the pattern of activation corresponding to Queen.",
            "They should be almost the same.",
            "In most dimensions, except for what has to do with, you know, being male or female.",
            "An and if we're lucky, maybe there is a input dimension that corresponds, maybe a bit that says this is the male version of the concept, or it's the female version of the concept.",
            "And of course, the same bit would be present in men and women and men and women would have all of the same attributes except for that bit.",
            "So now when we take the difference between those two vectors, what remains is just a bit and we take the difference between those two vectors.",
            "Again we get the same bit.",
            "Now, in practice it's not going to be a particular axis.",
            "Aligned direction is going to be a vector that's not at 1 hot vector, but it's the same concept.",
            "Right, so we are learning these representations that are.",
            "Because they are used to try to capture the distribution of the data.",
            "I tend to.",
            "Discover meaningful aspects of the world that are like the underlying factors we may think about that can generate the data an because that happens, we get amazing kinds of generalizations, like in this example.",
            "It's not just these kinds of toy examples, these kinds of models, of course are used in practice for solving many problems."
        ],
        [
            "Right, I should move on."
        ],
        [
            "You talk about depth.",
            "So I said that.",
            "Having a district representation is gives you an exponential advantage at least.",
            "Up to an exponential advantage, but there's another source of exponential advantage which is.",
            "Can be composed can be obtained on top of what you get with this representation which is having multiple levels of representation, so that's deep."
        ],
        [
            "So to explain this.",
            "I first need to tell you a little bit about some of the history here about representations.",
            "There is a lot of misunderstanding about what depth brings.",
            "That I'm going to try to dispel here, but before I get there.",
            "One reason why depth was not studied that much before.",
            "Century.",
            "Is that many people thought that you don't need?",
            "A deep neural net, a shallow neural net with a single hidden layer, in theory is sufficient to represent any function, and it's not just neural Nets.",
            "You can have two layers of logic gates, for example insufficient to represent any Boolean function, or you know RBF network with one layer of RBF units is sufficient to approximate any function.",
            "Assuming you have enough of these units in a few minutes, you can.",
            "You can approximate any function with a required degree of accuracy, and we call this property universal approximation property.",
            "But what this doesn't?",
            "These results don't tell us is how many units will I need?",
            "So it may be that with a deeper neural net.",
            "We could represent the same function that we could also represent with a shallow one, but more cheaply with less units an in fact.",
            "There's now a series of results showing that this is true, and that there exists families of functions and fairly large ones for which you can get an exponential advantage.",
            "So the number of units or the number of parameters you need.",
            "Can be exponentially larger if you have only a shallow network.",
            "Then if you allow yourself to have a network that's deep enough now how deep?",
            "We don't know it depends on the function is trying to represent.",
            "So presumably if you're trying to represent a function that's.",
            "Very deep, in other words, that the true function needs many levels of composition to really be represented properly.",
            "Then your neural net needs more layers, but we don't know ahead of time how many layers you need.",
            "One thing I want to mention is that.",
            "Actually.",
            "Death is not necessary to have a flexible family of function, so people think that if we have deeper networks, that means that they represent somehow a corresponds to higher capacity.",
            "But actually, that's not necessarily the case.",
            "You can still roughly think about the number of free parameters representing your capacity so deeper doesn't mean you can represent more functions, it's just that the set of functions that you can represent.",
            "Is.",
            "It has a particular characteristic it has this characteristic that is obtained through composition of many operations, and if the functions you want to learn have this characteristic, this property, then you're much better off approximating these functions with a deep neural net.",
            "There is.",
            "There are some other analogies that help you understand depth.",
            "So first of all."
        ],
        [
            "This notion of reuse, so in a deep neural net, we're reusing the lower layer features to produce the higher level features.",
            "When we write a computer program, we don't usually write, you know main program.",
            "With you know one line after the other.",
            "Typically what we do is we have sub."
        ],
        [
            "Routines.",
            "That we reuse so you can think of what the hidden units are doing as.",
            "Kind of subroutines for the bigger program, which is what you have at the final layer, so that's one way to think about it.",
            "Another way to think about it is that actually when you run a program, it's a sequence of operations.",
            "So if you have a program that has 10 lines, you can think of it like at each line.",
            "Let's say it's Python code at each line.",
            "You're doing some computation and the result of that computation is changing the state of the machine.",
            "To provide an input for the next line.",
            "Right, so the input at each step of the computation.",
            "Is the state of the machine an and the output is a new state of the machine?",
            "So that's the Turing machine, right?",
            "So the number of steps that a particular Turing machine executes actually corresponds to the depth of computation.",
            "And so, although in principle you can represent.",
            "Any function by having to to program with two steps?",
            "It's not going to be a very interesting program.",
            "It's not going to be, it's going to be able to compute anything.",
            "But it's not going to be able to compute interesting functions efficiently, so how would you have a program in two steps in two lines that can output any function?",
            "With some precision, how could you do that?",
            "How could you have a two step program?",
            "That can represent any function.",
            "It's a Riddle.",
            "Look up table, thanks.",
            "That's what a SVM kernel SVM does.",
            "It's a look up table.",
            "A shallow neural net is kind of a look up table as well.",
            "So look up table first line you know get the content of some at some address and you look up table.",
            "Second line take the result and you know do something with.",
            "It may be multiplied by some number and an output.",
            "OK, so so we need deeper programs."
        ],
        [
            "There's another analogy that I sometimes use that helps to understand this notion of reuse.",
            "So most people being trained in the bit of mathematics understand polynomials.",
            "So let's think how you could represent polynomials.",
            "You could represent polynomials by running out the polynomial as a sum of products.",
            "Right, that's the usual.",
            "Form.",
            "But you could also write it as a graph.",
            "Of computations where each node performance in addition or multiplication, so this is what I've chosen here.",
            "And now if you do it this way, you can have kind of deep computation where you could represent a polynomial that has an exponentially large number of in terms of these terms.",
            "If you do the summer products, even though the number of computations I'm going to, the number of additions and multiplications could be small, so why would that happen?",
            "Well, because I can reuse some operations like.",
            "Let's say I have this node that multiply X 2X3.",
            "Because it's it's value is going to be used in both of these two nodes and then combine later.",
            "This product X 2X3 comes up in the polynomial associated with that later node in many places, like in four places.",
            "And so even though.",
            "I. I have, you know, maybe an exponential number of of.",
            "Of these products coming up later, I needed to do this product X 2 * X Three only once.",
            "Right, so this is a. I guess an illustration of that same idea, and there's some theory you know that's connected to that.",
            "Anne."
        ],
        [
            "I mentioned this result from our last NIPS, which basically shows.",
            "Very large family functions.",
            "That for large families of functions, the result I was telling you about with an exponential gain both for shallow neural net and for deep neural net.",
            "So these are rectifier networks or piecewise linear networks, so that includes things like Max out.",
            "And it turns out that these types of neural Nets are very common Now, so this is a fairly large family of functions.",
            "And this paper is about counting number of regions.",
            "Exactly what I was telling you about before, but doing it in a very formal way.",
            "And so it basically shows that with a single hidden layer you get an exponential number of regions compared to.",
            "Exponential in the number of units, right or inputs?",
            "There's depends on both.",
            "And if you have more layers then you can get another exponential gain.",
            "OK, so so I'll let you look at these if you are more into the math for these things.",
            "The last last bit of sort of motivating theory I want to talk about.",
            "Anne."
        ],
        [
            "It's not.",
            "It's not about an advantage, but about a.",
            "Belief that seems to be wrong about a supposed disadvantage of neural Nets, so one of the reasons why neural Nets were discarded in the late 90s and early 2000 was that the.",
            "Optimization problem for neural Nets is non convex.",
            "Meaning that there could be lots of local minima.",
            "In fact, since the late 80s early 90s, we know from theory that.",
            "There are an exponential number of local minima in neural Nets.",
            "And this this fact, combined with the success of kernel machines in the mid 90s, late 90s.",
            "So sort of play role in kind of greatly reducing the interest of many researchers in your nuts.",
            "But this was relying on the idea that if we have many local minima because the optimization function is not convex, then we're in trouble.",
            "An for sure, we're not going to get any guarantees about finding the optimal solution, but Furthermore we might be stuck in very poor solutions.",
            "Well."
        ],
        [
            "Something changed very recently, so over the last year.",
            "We now have evidence both at the Terry Cloth vertical level and empirical level, that this issue of non convexity is actually maybe not an issue at all.",
            "Or at least it changes the picture we have of the optimization problem with neural Nets.",
            "So if you're interested in the optimization aspects of neural Nets or deep learning, I encourage you to look at these papers so there is.",
            "We presented this for the first time.",
            "Then one year ago, roughly in the spring, then we had a NIPS paper, and then the group at NYU with Unnatural Manske and Yellow Curb presented a different kind of perspective on the same problem, which essentially comes down to the same kind of conclusions.",
            "So let me try to explain a little bit what these papers say."
        ],
        [
            "They tell us about saddle points.",
            "So let me explain a little bit the picture of the optimization problem in high dimensions versus low dimensions.",
            "So if we are in low dimensions, which is the kinds of things we mentale visualize.",
            "You're going to have a lot of local minimum.",
            "Right, if I do, if I draw a random 1 dimensional curve, it's going to have lots of ups and downs, and here's a random 2 dimensional curve, right so?",
            "A random curve in two dimension typically has lots of local minima and with that kind of picture we think that local minima are going to be a big problem.",
            "But it turns out that in high dimensions.",
            "Local minima are not really the critical points that are most prevalent in most of the places of interest.",
            "So let me try to explain what I mean.",
            "Basically, when we optimize our neural Nets or any high dimensional functions.",
            "For most of our trajectory as we optimize.",
            "The critical points.",
            "In other words, the places where the derivative is zero that we're going to come close to are going to be saddle points.",
            "So what is the saddle point?",
            "They're not going to be local minima, so we not going to be trapped in local minima.",
            "We might have trouble escaping saddle points, but we can always escape saddle point.",
            "So what is the saddle point?",
            "So there's a picture here.",
            "In in, again in 2D, but in general.",
            "So what is the critical point is where the derivative is 0, so the function that is flat in some place.",
            "And then well.",
            "It has to go up or down in different directions, so here you see it's going up in these directions, but it's going down in this directions.",
            "And that's a saddle point where there are directions where it's going up, and directions where it's going down.",
            "A local minimum or global minimum has all directions going up.",
            "A local Maxima has all the directions going down right?",
            "You can't.",
            "You can't go higher locally and same thing for the local minimum.",
            "So.",
            "So what's going on here is that intuitively?",
            "In a very high dimensional space, so the space of parameters here.",
            "If you think about a saddle point.",
            "I.",
            "The the number of directions where it would have to go up to become a local minimum would be with the other dimensions, right?",
            "So if if somehow there's something like a kind of randomness in how these functions are constructed and you independently choose for each direction, whether at the critical point you're going up or down, well, you can see that it's going to be exponentially unlikely that.",
            "All the directions go up, except if you are near the bottom.",
            "Of your lens game.",
            "In other words, you're near the global minimum, 'cause of course, when when you have a minimum that's near the global minimum, you can't go further down, so all the directions have to go up.",
            "So what you that's kind of the intuition, and So what happens, really, is that you have local minima, but they are very close to the global minimum in terms of their objective function.",
            "And there is some 30 crew results from statistical physics annand matrix theory that.",
            "Suggest the following that for some families of functions that are fairly large like those.",
            "That you get by Gaussian processes.",
            "Anne.",
            "There is a tight relationship.",
            "Concentration of probability between what's called the index of the critical points and the objective function and training error.",
            "What is the index?",
            "The index is the number of is a fraction of directions that.",
            "Now going down so you have say at a minimum, all of the directions are going up, so you have 0% of the directions that are going down, and that's a local minimum.",
            "So when the index is equal to 0 you have local minimum.",
            "When the index is equal to 1, you have a local maximum.",
            "Anything in between the fraction of directions that are going down.",
            "Is something in between you have a saddle point so local minima are kind of special cases of saddle points where the index is 0.",
            "OK, and So what this theory says is that the if you think of the joint distribution of index an objective function.",
            "Distribution because we imagine a large distribution over functions over that we're trying to optimize, there's going to be a concentration of probability on some curve.",
            "Meaning that.",
            "For a particular training objective.",
            "Most of the critical points are going to be subtle points with a particular proportion of particular index.",
            "And we can, we can do empirical experiments to verify that we.",
            "So that's what we did, and we find that indeed you have a kind of tight.",
            "Relationship between index an objective function.",
            "OK.",
            "Yes.",
            "Right, we don't so these.",
            "You can always imagine constructing a an objective function.",
            "That is hard that has bad local minima in the middle of, you know high error.",
            "So that's you know you can.",
            "You can build by hand very easily.",
            "You can place the local minima in bad places.",
            "So the question is whether these families of random functions that can be studied theoretically like this are representative of what we have with neural Nets and we don't know the answer to that.",
            "But we've done some experiments like this experiment is on real neural Nets where what we do is we launch.",
            "A bunch of experiments where we train an along the way you know every once in awhile we look using very expensive methods for the nearby critical points, and then we use like Newton's method to find the index.",
            "Like how many directions does it go up, compute the eigenvalues of that has seen at that location.",
            "So it's it's only an empirical validation, and we probably would like to have more such experiments to verify that that statement is true, so that's why I'm saying we have no proof that this result applies to the kinds of neural Nets we want to optimize, but we have some evidence that the behavior we're getting corresponds with the theory suggests.",
            "And of course the theory makes assumptions which we don't know are true.",
            "Yes.",
            "Anything else?",
            "Maybe, maybe not.",
            "So, even though we may not get stuck in local minima, we might get stuck in regions that are or near regions that are flat or some other complicated shape.",
            "Of the objective function which makes.",
            "Say stochastic gradient look like it's stuck.",
            "In theory if you wait long enough.",
            "Sgt will escape any any someone 'cause there's no local minimal SG will escape.",
            "But it you know it might take so long that you won't see it, and we have lots of observations.",
            "So let me show you in the net."
        ],
        [
            "Slide so we have lots of observations where we see things and people have played with you all night, sometimes see this where you see the error go down and then it seems to plateau and maybe if you're lucky something happens and then it goes down and then it plateaus.",
            "But let's say we had stopped the experiment here.",
            "We might think that this is the best we can get.",
            "Actually, something else is going on.",
            "It looks like we are approaching the region of a saddle point.",
            "The gradients are not approaching zero.",
            "We don't understand why, but but there's no progress of the objective function.",
            "This kind of bouncing around not finding the escape route.",
            "My intuition is not this is such a high dimensional space that there's only a few dimensions where it's going down and somehow it's not finding them.",
            "Maybe because of curvature problems or all the things but.",
            "So what I'm saying is, yes, it's worthwhile to continue exploring optimization methods beyond simple gradient descent.",
            "But maybe we need to take into account the fact that what we are finding is not local minima.",
            "It might be something else.",
            "It might be something classical like differences, curvature or something more subtle.",
            "Yeah, so we can measure those.",
            "Yeah yeah, that's right.",
            "An in fact what you find is that as the as you go lower down.",
            "You see here in this picture it spends more and more time on these plateaus, right?",
            "They get harder and harder, presumably because there are less directions going down.",
            "Yes.",
            "I don't.",
            "Stopping criterion is how much do you want to spend in time, right?",
            "So I remember.",
            "People are getting lazy these days.",
            "They'd like to have the experiments finished in two days or something, or.",
            "When I was a PC.",
            "We were willing to wait like for weeks.",
            "And that was not enough.",
            "I mean, in retrospect, if we had known.",
            "We should have waited for like 2 years and then.",
            "We might actually have accelerated the progress of research in neural Nets because.",
            "People discarded these things in part because we didn't train them long enough to get the kinds of results we are able to get now with GPU's and much faster hardware, yes.",
            "Um?",
            "I believe it's an old 1/4 task with either letters or M lists, so it's it's that's in the NIPS 2014 paper.",
            "First author is yondo fast, so you can find all that information there.",
            "A yes or.",
            "It's very expensive to actually compute that.",
            "So it's not something practical, right?",
            "So we can do.",
            "Long experiments where we measure these things, but you know."
        ],
        [
            "Alright, I'm going to."
        ],
        [
            "2."
        ],
        [
            "Just use the last few minutes quickly to tell you about other priors that come in too many deep learning approaches, especially some that use unsupervised learning.",
            "But you know a lot of that will have to wait for either my lecture at the end of the summer school, or other lectures that will talk about these concepts."
        ],
        [
            "But to get things to get you motivated.",
            "One interesting observation is that humans.",
            "And especially specially striking with children.",
            "Are able to learn a new task like.",
            "The category of and a new object from very few examples, sometimes just a handful, sometimes one example.",
            "If you think I did from a statistical learning POV, it doesn't make any sense, right?",
            "We can't possibly have meaningful generalization even using a deep net with a single example.",
            "The only way this can happen is because the child is using accumulated knowledge from previous learning.",
            "And that knowledge could be, you know, used to build representations.",
            "Such that in the new representation space, things are very very easy, like a single, you know.",
            "Prototype gives you buys.",
            "You essentially know the characterization of this new category.",
            "One other way to think about it, which is even more interesting, is that if you've built a mental model of the world that basically understands how the world works.",
            "And now you give me a new category, say.",
            "Here's an example of blah blah blah because you understand how the world works.",
            "It's obvious what blah blah blah means, right?",
            "And now you can generalize to two other instances.",
            "So you know an example of blah blah blah could be, oh, I'm going to teach you a new game.",
            "Here's an example of how we play it.",
            "We do that all the time.",
            "So.",
            "So for this to work, you need more.",
            "Set statistical assumptions.",
            "That basically the way the world works now in for this.",
            "For these new examples is the same as the way the world worked for the other tasks that I saw before.",
            "Or maybe the unsupervised learning I did from all of the examples I've seen before.",
            "So we need to introduce more priors beyond the the ones that we talked about with this representations in depth, for example.",
            "We we would like to use the following prior that the underlying explanations for the X is that we see now in the context of having to predict the Y are the same same as those we know there are behind the data we've seen before, from which maybe we've learned by unsupervised learning to characterize the distribution of the past data.",
            "So if the same factors explain.",
            "X.",
            "When we see it alone and the relationship between X&Y in supervised learning, then we're in business, then we can hope to actually get very fast generalization.",
            "This is called transfer learning."
        ],
        [
            "Well between task or between a supervised learning.",
            "OK, so so I guess many of you I've heard about the idea of semi supervised learning.",
            "In purely supervised learning, we just using the labeled examples an here maybe is what we do, but if somebody gives us these other unlabeled examples, we would probably choose something like this linear classifier.",
            "So we need to discover something about the input distribution in order to do that.",
            "Other exam."
        ],
        [
            "You have sharing that's very very common is multi task learning, so the most common cases say we have images as input and there are different questions you'd like to ask about these images and it may be that the lower levels of deep net can extract features that are useful for many of these tasks.",
            "If there is this kind of sharing of underlying factors.",
            "That is applicable to many of these tasks.",
            "In other words, there is some underlying factors and that explain the variations in X and explain some of the tasks and different tasks.",
            "Basically, maybe look at a different subset of these factors in different ways.",
            "Then we can gain a lot by by training our models with this in mind.",
            "For example, we could use the same neural net except for the upper layers that are task specific, so that's the most common setup.",
            "You could also have some kind of multimo."
        ],
        [
            "Model approaches where you say OK, I'm going to have representations for different types, so this is actually what is used in Google Image search that was built by my brother Sammy at Google and by collaborators where.",
            "We're going to imagine that there is a representation for images and the representation for queries that go in the same space.",
            "And by learning these two.",
            "Representations together we can generalize in pretty amazing ways."
        ],
        [
            "More generally, you can imagine that you have, say different modalities like X&Y.",
            "We learn representation for each modality as well as the kind of translation between them.",
            "That tells us how to go from one to the other.",
            "If we write, try to learn this directly in X&Y, it's hard, but if we do it in the right space things become very easy.",
            "That's the hope another."
        ],
        [
            "Similar cases well like speaker adaptation or write adaptation or other kinds of cases where the sharing is near the input, not the output.",
            "Maybe we want to do speech recognition and we want to have a different kind of transformation.",
            "Maybe depends on depending on the noise depending on the person who's speaking so we can share the upper levels here.",
            "So there are many places where you could share.",
            "Alright."
        ],
        [
            "You see, there is one idea that would like to end with which is connected to many things I've been saying and to some work done by some of the leaders in kernel machines that I've been working on causality recently.",
            "Burnout shop cough and his group.",
            "That I think give kind of intuition as to why.",
            "Wise learning could be helpful.",
            "The idea is the following.",
            "Um?",
            "Imagine we have our input sacks an the labels why they were trying to predict and now think about the related the causal relationship that could exist between these images and labels.",
            "Well, there are two basic possibilities.",
            "One is that X is a cause of Y or X is an effect of Y, right?",
            "So which is the cause?",
            "Which is the effect?",
            "Of course it could be complicated more complicated than that, but let's imagine just these two cases.",
            "Well, it turns out that if.",
            "If Y is one of the causes of X, then doing unsupervised learning is going to be very, very helpful.",
            "But if it's the other way around, that why is an effect of X&X is the cause then that provides learning is useless for the purpose of semi supervised learning, right?",
            "And so.",
            "It turns out that in the in most of the eye tasks, the wise are things like words that we like to talk about the world and the X is are things we measure in the world and our brain is trying to make sense of the world.",
            "So we created these labels.",
            "These concepts that we talk about that are our best ways of characterizing the things we observe, an that probably means essentially a way to talk about the causes of what we observe.",
            "And so it works.",
            "It works because in the real world the things we want to predict.",
            "Are the things that humans like to predict those things that human talk about, and then it means that there is a right kind of causal relationship between the X is in the wise.",
            "The effect of that causal relationship is that if you look at P of Y given X, then the right way of modeling it, that will the one that will have the greater statistical power is basically involves capturing P of X given YNP of Y. Anne Anne because P of X through Bayes rule is just the sum over at over Y of these things, the right P of X.",
            "The one that will work the best is is the one that.",
            "Really involves introducing the wise as an explanation, so let me give you a very simple.",
            "Very simple example of that.",
            "If I can get this to work and yes.",
            "And yes, so imagine that the world.",
            "Is very very simple.",
            "It's just a mixture of two Gaussians, but so this is X.",
            "And this is P of X.",
            "And and this is y = 0 and this is y = 1.",
            "But you don't know.",
            "However you do see the data you see the X and you see that there are two natural classes.",
            "And because the data are really was generated with, why is one of the causes the way you think the data was generated?",
            "Here is somebody first picked the Y and then given that why you have a Gaussian distribution?",
            "That's the right way of modeling that data.",
            "Now, of course, in the real world, the relationship between X&Y is much more complicated, but you know it tells us that even if we don't observe the Y, if we model it the proper way, we're going to see emerge as a natural random variable that explains it.",
            "These these these labels, if it turns out that these labels are the kinds of things we want to predict at the end of the day, then we're really in business, so that's what's going on alright.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Not introduce myself.",
                    "label": 0
                },
                {
                    "sent": "And get started with the second lecture.",
                    "label": 0
                },
                {
                    "sent": "Which will go a bit more into networks, but staying at a fairly conceptual level.",
                    "label": 0
                },
                {
                    "sent": "You'll see more about algorithms like, especially the optimization part this afternoon with Leon Bottou and tomorrow with Hugo Larochelle.",
                    "label": 0
                },
                {
                    "sent": "But today it be more about the.",
                    "label": 0
                },
                {
                    "sent": "Theoretical motivations for deep learning.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I guess you're here because you've heard about deep learning, and many of you have already played with these algorithms, and what were they about basically?",
                    "label": 0
                },
                {
                    "sent": "Deep learning is about learning representations.",
                    "label": 1
                },
                {
                    "sent": "And in particular, learning multiple levels of representations and the idea is that these multiple levels of representation correspond to multiple levels of abstraction an presumably, as I'll try to convince you if we're able to do a good job of capturing these high level abstractions, we can generalize well.",
                    "label": 0
                },
                {
                    "sent": "And this is of course correlated with the amazing results that have been obtained in computer vision, object recognition, object detection and speech recognition, and more recently, fairly impressive progress in natural language as well.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as Pascal was saying, the the evolution of.",
                    "label": 0
                },
                {
                    "sent": "Of systems that can be used to to answer questions and that we want to be more intelligent can be traced to this idea of incorporating learning and then incorporating.",
                    "label": 0
                },
                {
                    "sent": "Representation, learning and then multiple levels of representation.",
                    "label": 1
                },
                {
                    "sent": "So in the old days of AI and symbolic machine, not machine is symbolic.",
                    "label": 0
                },
                {
                    "sent": "AI basically the knowledge about the world that we wanted to put in the machine.",
                    "label": 0
                },
                {
                    "sent": "Was provided by the humans, the experts.",
                    "label": 0
                },
                {
                    "sent": "That's why they called these expert systems.",
                    "label": 0
                },
                {
                    "sent": "An innocence we built a function from input to output using hand design program that had two parts, the facts and the logic that allows to combine those facts to to answer questions.",
                    "label": 0
                },
                {
                    "sent": "Then came classical machine learning which tried to solve similar problems.",
                    "label": 0
                },
                {
                    "sent": "Then the classical AI problems.",
                    "label": 0
                },
                {
                    "sent": "Not trying to do for you, but just doing something much simpler like pattern recognition.",
                    "label": 0
                },
                {
                    "sent": "But it relied on hand design features like Pascal was telling you about.",
                    "label": 0
                },
                {
                    "sent": "In fact, if anybody I'm sure there's many of you here who have played with machine learning trying machine learning, especially in industrial setting, and most of the time that you guys have spent this probably to design the input of the machine learning system.",
                    "label": 0
                },
                {
                    "sent": "And then we use some kind of generic say classifier to produce the output, so that's that's what machine learning has been for many years and continues to be in many applications very crucial part to do this hand design of input features.",
                    "label": 1
                },
                {
                    "sent": "You networks having there as long and the sort of standard 1 hidden layer or two hidden layer in your nuts, you can think of as in addition to having a linear classifier on top, having this layer of features that are learned automatically at the same time as the classifier and deep learning is just.",
                    "label": 0
                },
                {
                    "sent": "Going further with this idea and saying, oh we can have multiple levels of features composed on top of each other in order to come up with.",
                    "label": 0
                },
                {
                    "sent": "Our final solution.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'll try to tell you about in the rest of this lecture is why this kind of approach may be working so well and and I don't claim that I have all the answers.",
                    "label": 0
                },
                {
                    "sent": "There are many reasons why some of these systems are working, and some of them I won't talk about like some of the priors that come into convolutional Nets that will be discussed by other lecturers, but I'll talk about some of the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basic general ingredients that I think explain part of the success of these deep learning systems.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before I get there, let me just step back a little bit and tell you about how we might build really complicated systems for AI.",
                    "label": 0
                },
                {
                    "sent": "And how this is related to?",
                    "label": 0
                },
                {
                    "sent": "How this question is related to famous result from machine learning called No Free Lunch Theorem.",
                    "label": 1
                },
                {
                    "sent": "So what I think are the basic ingredients for successful machine learning, especially towards the eye are the following.",
                    "label": 0
                },
                {
                    "sent": "You need lots and lots of data.",
                    "label": 0
                },
                {
                    "sent": "That's number one and Pascal mentioned it an why do we need lots and lots of data?",
                    "label": 0
                },
                {
                    "sent": "You have to understand why we're trying to build a system that's going to take decisions and in order to take good decisions, which is what.",
                    "label": 0
                },
                {
                    "sent": "Basically intelligence is about.",
                    "label": 0
                },
                {
                    "sent": "You need a lot of knowledge.",
                    "label": 0
                },
                {
                    "sent": "Right, so an AI system or something that's solving a complicated task needs a lot of knowledge about the task that is trying to solve.",
                    "label": 0
                },
                {
                    "sent": "Where is that knowledge going to come from?",
                    "label": 0
                },
                {
                    "sent": "Well, it either comes from humans who put it in or from data and the machine learning part is about extracting knowledge from data.",
                    "label": 0
                },
                {
                    "sent": "That's what machine learning is about, and if we want to learn a complicated task like understanding language or images, because these tasks are fairly complicated, we're going to need a lot of data unless we already know how to solve that problem.",
                    "label": 0
                },
                {
                    "sent": "And then we don't need that, but but typically we don't know enough and that's why we're going to be using machine learning now as sort of a culinary to this.",
                    "label": 0
                },
                {
                    "sent": "It's not enough to have lots of data, you need to have a family of functions or model that's rich enough to represent that complicated function to represent all that knowledge, right?",
                    "label": 0
                },
                {
                    "sent": "So the knowledge comes in the form implicitly in the beta, but then we have to translate that knowledge into something that we can use for taking decision.",
                    "label": 0
                },
                {
                    "sent": "That's our model.",
                    "label": 0
                },
                {
                    "sent": "Our neural net.",
                    "label": 0
                },
                {
                    "sent": "And of course we have to store that knowledge somewhere that it starts from the data.",
                    "label": 0
                },
                {
                    "sent": "It goes into a model, so the model has to.",
                    "label": 0
                },
                {
                    "sent": "Be big enough, flexible enough, they can represent the kind of complicated function you need for your solving your task, right?",
                    "label": 0
                },
                {
                    "sent": "So so for sure we need these two things now.",
                    "label": 0
                },
                {
                    "sent": "Some people thought that might be enough.",
                    "label": 0
                },
                {
                    "sent": "I don't think so.",
                    "label": 0
                },
                {
                    "sent": "And a lot of what will be telling you about later is about the third ingredient, which you can basically think about as additional prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "And in particular, I'll talk about the curse of dimensionality and Ann and how it's it's a challenge and how we might get around it using some of the players that come with most deep learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is going to be the subject of much of my presentation today.",
                    "label": 0
                },
                {
                    "sent": "The third part, right?",
                    "label": 0
                },
                {
                    "sent": "So given one and two data, an powerful flexible family of functions.",
                    "label": 0
                },
                {
                    "sent": "Is not enough.",
                    "label": 0
                },
                {
                    "sent": "We're going to need something else which is fairly general knowledge about the world.",
                    "label": 0
                },
                {
                    "sent": "But that's going to be necessary to get good generalization.",
                    "label": 0
                },
                {
                    "sent": "Classical nonparametric statistics works with one and two and a very weak prior, which is called a smoothness prior, which I'll explain a bit later.",
                    "label": 0
                },
                {
                    "sent": "But it's not enough.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so I guess I already said much of this in order to get the knowledge we need data and we need to learn.",
                    "label": 0
                },
                {
                    "sent": "We need learning algorithms and learning algorithms involve two things.",
                    "label": 0
                },
                {
                    "sent": "Prior, so in other words, some kind of preference over the set of functions baskal talked about choosing a parametric family function, but you could be a bit more so saying well, there's some functions I prefer more than others, like if we use regularization technique, it says some of those functions.",
                    "label": 0
                },
                {
                    "sent": "I prefer some I prefer less, but you know, maybe I'll end up choosing these.",
                    "label": 0
                },
                {
                    "sent": "And you need some optimization techniques, because choosing a function in an infinite set is, you know, potentially intractable, but some some approximation or some search method will be required now to get the machine learning to work.",
                    "label": 0
                },
                {
                    "sent": "Pascal told you about generalization.",
                    "label": 0
                },
                {
                    "sent": "This is really central.",
                    "label": 0
                },
                {
                    "sent": "Of course in machine learning, but one thing he started talking a bit about, which I'll come back to is a kind of geometric interpretation of what generalization means.",
                    "label": 0
                },
                {
                    "sent": "So for people who know a little bit about probability, the way I like to think about it is generalization.",
                    "label": 0
                },
                {
                    "sent": "The game of Generalization is a guessing game.",
                    "label": 0
                },
                {
                    "sent": "And it amounts to guessing which configuration of the variables of interests are good or likely.",
                    "label": 1
                },
                {
                    "sent": "So you can think of it like you know, guessing where probability mass should go should be concentrated.",
                    "label": 1
                },
                {
                    "sent": "I'll come back to that notion in my slides.",
                    "label": 0
                },
                {
                    "sent": "And this this kind of geometric view of generalization is interesting because it also connects with another important idea in machine learning is statistiques, which is the curse of dimensionality and again I'll try to explain this.",
                    "label": 0
                },
                {
                    "sent": "But basically what happens is we're dealing with these high dimensional random variables like pixels like sequence of characters like acoustic signals and because there are so many dimensions.",
                    "label": 0
                },
                {
                    "sent": "And each dimension can take many values, maybe even just two already is going to be a lot because you know.",
                    "label": 0
                },
                {
                    "sent": "1002 two power thousand.",
                    "label": 0
                },
                {
                    "sent": "We shared a number of images of 1000 pixels with binary pixels is already huge.",
                    "label": 0
                },
                {
                    "sent": "Is already much too large to be able to enumerate and we're not going to have enough data to consider all of those configurations.",
                    "label": 0
                },
                {
                    "sent": "So for most configurations we can have zero data that tell us what the right answer should be.",
                    "label": 0
                },
                {
                    "sent": "That's the problem right?",
                    "label": 0
                },
                {
                    "sent": "In the 2D simple 2D case that Pascal showed, it looked like generalization is easy, but in a high dimensional space generalization looks almost.",
                    "label": 0
                },
                {
                    "sent": "Impossible.",
                    "label": 0
                },
                {
                    "sent": "But yet we have algorithms that you know, do a decent job.",
                    "label": 0
                },
                {
                    "sent": "So how can we potentially deal with this curse of dimensionality?",
                    "label": 0
                },
                {
                    "sent": "And one of the big ideas that I've been trying to sell in many of my papers for the last almost the last decade is this idea that.",
                    "label": 0
                },
                {
                    "sent": "In order to defeat the curse of dimensionality we want to.",
                    "label": 1
                },
                {
                    "sent": "Figure out how the data was generated to understand the explanatory factors that explain that that that that are the causes of what we observing.",
                    "label": 0
                },
                {
                    "sent": "If we could do that, of course it's a tall order.",
                    "label": 0
                },
                {
                    "sent": "Then we would basically be making sense of the data and we'd be able to make very good predictions on your example.",
                    "label": 1
                },
                {
                    "sent": "That's that's of course very ambitious program, and I think that what we currently doing in deep learning is a step in that direction.",
                    "label": 0
                },
                {
                    "sent": "We have by no means solve this problem.",
                    "label": 0
                },
                {
                    "sent": "This is what science is trying to do, right?",
                    "label": 0
                },
                {
                    "sent": "We're trying to make sense of the world by doing experiments and building theories that explain what we observe.",
                    "label": 0
                },
                {
                    "sent": "So machine learning is basically kind of automated science.",
                    "label": 0
                },
                {
                    "sent": "You can think of it like this, but on a scale where we're looking at lots and lots of data.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's go a little bit deeper into these questions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But first let me go back to this nation notion of the curse of dimensionality and why the classical nonparametric methods like nearest neighbors that Pascal talked about.",
                    "label": 0
                },
                {
                    "sent": "Don't cut it why they?",
                    "label": 0
                },
                {
                    "sent": "They are really hurt by the curse of dimensionality, right?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So remember I said the curse of dimensionality arises because we have many configurations of the random variables.",
                    "label": 1
                },
                {
                    "sent": "So if you have only one random variable, say A1 dimensional thing and and we're trying to learn from it like to predict which values are likely or what, why to associate to a given value of X where so?",
                    "label": 0
                },
                {
                    "sent": "These are the different values of X we could we could be in the X is we could break the input into say 10 different values and then we can count how many times each value comes, or for each bin we could we could average what value of Y comes up.",
                    "label": 0
                },
                {
                    "sent": "And that would be a simple nonparametric predictor which is basically on a histogram.",
                    "label": 0
                },
                {
                    "sent": "And that would work well actually, if if there's only one viable and it's sufficient to break it into 10 different values, 'cause we would need to accumulate what you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe 100 examples an we would probably covering enough each of the cells in this discretization to to give a decent predictor an we could maybe extend this to two dimensions.",
                    "label": 0
                },
                {
                    "sent": "So instead of having one verbal, we have two variables.",
                    "label": 0
                },
                {
                    "sent": "Now we have 10 by 10 configurations of interests, but if we have three variables Now, we have 10 times Thames Time Thames.",
                    "label": 0
                },
                {
                    "sent": "Configurations of interest and, well, it's not so bad.",
                    "label": 0
                },
                {
                    "sent": "Actually it's only 1000 configurations, but you can see that it's growing fast and if we only had 100 examples that were maybe enough here, it's not going to be enough here.",
                    "label": 0
                },
                {
                    "sent": "There may be a lot of configurations that we don't see.",
                    "label": 0
                },
                {
                    "sent": "Maybe some of them get a lot of examples and most of them get 0 examples.",
                    "label": 0
                },
                {
                    "sent": "And if you have not three variables but a million variables, well, we're totally dead.",
                    "label": 0
                },
                {
                    "sent": "Even with 100 rebels were already dead so.",
                    "label": 0
                },
                {
                    "sent": "There are just too many configurations.",
                    "label": 0
                },
                {
                    "sent": "How could we possibly generalize to new configurations?",
                    "label": 0
                },
                {
                    "sent": "We have never seen?",
                    "label": 0
                },
                {
                    "sent": "That's the game of machine learning.",
                    "label": 0
                },
                {
                    "sent": "How do we say something about configurations we've never seen?",
                    "label": 0
                },
                {
                    "sent": "So the classical solution to this?",
                    "label": 0
                },
                {
                    "sent": "Well, the classical approach to nonparametric statistics is to say, rely on smoothness.",
                    "label": 0
                },
                {
                    "sent": "So in the 1D case, when we discretize the spacing OK, we're going to say that all the values between 2.5 and three are going to be considered equivalent, and we're going to be able to lump all those examples or average in some neighborhood, like with nearest neighbors.",
                    "label": 1
                },
                {
                    "sent": "Basically what we're saying is we're seeing the function we want to learn is smooth enough that.",
                    "label": 0
                },
                {
                    "sent": "I don't care about the exact difference between the value at 2.5 and the value of three because the function changes very smoothly.",
                    "label": 0
                },
                {
                    "sent": "An in low dimension that works, but in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "What happens is that if I want to look at the little neighborhood of values that I'm going to average around neighborhood of examples, I'm going to average around.",
                    "label": 0
                },
                {
                    "sent": "I'm going to end up with either.",
                    "label": 0
                },
                {
                    "sent": "An empty ball with no example in it or a ball that contains almost all of the examples, and it's going to be useless.",
                    "label": 0
                },
                {
                    "sent": "So it's something funny that happens in high dimension that you you you can't just average locally and get something meaningful.",
                    "label": 0
                },
                {
                    "sent": "OK, so so there is this notion of dimensionality which.",
                    "label": 0
                },
                {
                    "sent": "Which comes up.",
                    "label": 0
                },
                {
                    "sent": "But actually if you dig deeper mathematically, turns out that it's not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really, the number of dimensions.",
                    "label": 0
                },
                {
                    "sent": "It's the number of variations of the function you want to learn.",
                    "label": 1
                },
                {
                    "sent": "Remember I said that these these statistical methods rely on smoothness of the underlying function and smoothness.",
                    "label": 0
                },
                {
                    "sent": "Basically, you can think of it how many Upson Downs are there?",
                    "label": 0
                },
                {
                    "sent": "If it's, if it's linear, there's just it's very, very smooth.",
                    "label": 0
                },
                {
                    "sent": "If it goes up and down just a few times, it's less smooth, but it's still very smooth.",
                    "label": 0
                },
                {
                    "sent": "If it goes up and down a million times, well then it's it's getting to be complicated function actually.",
                    "label": 1
                },
                {
                    "sent": "The kinds of functions we want to learn may be very very non smooth.",
                    "label": 0
                },
                {
                    "sent": "So if you're trying to do computer vision.",
                    "label": 0
                },
                {
                    "sent": "Or is trying to do natural language or speech at the functions we actually want to learn are very complex.",
                    "label": 0
                },
                {
                    "sent": "The number of Upson Downs or the number of different regions that we actually want to distinguish?",
                    "label": 1
                },
                {
                    "sent": "Could be very large, so a few years ago we showed that the.",
                    "label": 0
                },
                {
                    "sent": "Issue with the curse of dimensionality really is not dimensionality, it's it's the number of ups and downs.",
                    "label": 0
                },
                {
                    "sent": "So in particular, many nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Statistical methods rely on the Gaussian kernel or some kind of similar looking Gaussian shape which is used to average in some neighborhood.",
                    "label": 0
                },
                {
                    "sent": "And with these kinds of methods we showed that the number of examples you need to get good generalization has to do is is proportional to the number of apps and downs of the function that you're trying to capture.",
                    "label": 0
                },
                {
                    "sent": "So if it has 2K zero crossings, then you need order of K examples, and that's a problem, because if you have many dimensions then the number of apps and downs could be exponential in the number of dimensions, but it doesn't have to be in high dimension.",
                    "label": 0
                },
                {
                    "sent": "You can have a 1 dimensional function that's incredibly complicated.",
                    "label": 0
                },
                {
                    "sent": "And it would still be hard to learn, even though it's 1 dimensional, OK?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the geometric view now.",
                    "label": 0
                },
                {
                    "sent": "That I I'll I introduced earlier, let me try to visualize a little bit.",
                    "label": 0
                },
                {
                    "sent": "So remember Pascal told you about the training data as the empirical distribution and he showed these plots.",
                    "label": 0
                },
                {
                    "sent": "These these these examples are circles, so I'm going to do the same thing.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Let's say you're trying to model the distribution of X&Y here.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you're trying to predict Y.",
                    "label": 0
                },
                {
                    "sent": "Given XI don't care, the point is, you're trying to find which configurations of X&Y here in that plane.",
                    "label": 0
                },
                {
                    "sent": "Are likely so that you can answer questions like you know.",
                    "label": 0
                },
                {
                    "sent": "Is this configuration likely or is this configuration likely?",
                    "label": 0
                },
                {
                    "sent": "Or if I know X, which value of, why should I use right?",
                    "label": 0
                },
                {
                    "sent": "These are the kinds of questions you're using machine learning for.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is we know the answer at these points.",
                    "label": 0
                },
                {
                    "sent": "We know these points are likely, which means we can answer about about those points, but what about everywhere else in this 2D plane?",
                    "label": 0
                },
                {
                    "sent": "So the smoothness assumption says the function we're trying to learn is smooth, meaning that if if we know its value at some point X, we know that in some neighborhood the value should be close.",
                    "label": 0
                },
                {
                    "sent": "So if we know that the value the probability function should be high here, we could guess that it should be high in some neighborhood, right?",
                    "label": 0
                },
                {
                    "sent": "So we could just draw a little balls or little kernel, little Gaussian kernels around each example.",
                    "label": 0
                },
                {
                    "sent": "An and of course that's what many nonparametric statistical methods do.",
                    "label": 0
                },
                {
                    "sent": "In this 2D case, it might actually work reasonably well, but of course we're trying to think about what happens in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "And what's going to happen in high dimensions is that these balls are either going to be so large that they cover everything, or they're going to leave a lot of holes in places where there should be a high probability.",
                    "label": 0
                },
                {
                    "sent": "So it's not going to be good enough in order to really workout this data set to make our solutions really generalize, what do we need here?",
                    "label": 0
                },
                {
                    "sent": "We need to discover something a bit smarter about that data that there is some structure, right?",
                    "label": 1
                },
                {
                    "sent": "Where is that structure?",
                    "label": 1
                },
                {
                    "sent": "Well, here we could guess the structure is that there's a 1 dimensional manifold near which we have a high probability concentration.",
                    "label": 0
                },
                {
                    "sent": "In other words, that the points near that manifold have a high probability of actually happening in the data.",
                    "label": 0
                },
                {
                    "sent": "So this is something we'll come back to a lot later in the week with with other representation learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "The reason is connected to the notion of representation.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "If you think about it.",
                    "label": 0
                },
                {
                    "sent": "Is this low dimensional representation of the?",
                    "label": 0
                },
                {
                    "sent": "There is a low dimensional representation of the data, which is the position on that curve, right?",
                    "label": 0
                },
                {
                    "sent": "So we started here with a 2 dimensional input and I'm saying there's a concentration of probability in some region, so these are the configurations that actually happen more often.",
                    "label": 0
                },
                {
                    "sent": "If you could discover that representation, then that I'm sorry that concentration.",
                    "label": 0
                },
                {
                    "sent": "Where is it that concentrates?",
                    "label": 0
                },
                {
                    "sent": "We basically solve our problem, but there's a way to think about this, which is if we could discover a new representation which is a 1 dimensional representation or or maybe just a 2 dimensional representation, but we changed the axis right?",
                    "label": 0
                },
                {
                    "sent": "So we can have one axis which is going to be the position on the manifold, right?",
                    "label": 0
                },
                {
                    "sent": "And we can have another access which is the direction orthogonal.",
                    "label": 0
                },
                {
                    "sent": "So if you're here.",
                    "label": 0
                },
                {
                    "sent": "You are some distance away from the manifold.",
                    "label": 0
                },
                {
                    "sent": "That's one axis, and then you are at some position on the manifold, so it's a change of representation and you can see that under that representation many things become easy.",
                    "label": 0
                },
                {
                    "sent": "For example, of course, you can get the density very easily because on one of the dimensions you know maybe maybe this is the position zero according to that dimension, and so everywhere at position 0 has a high probability and everything else has a low probability.",
                    "label": 0
                },
                {
                    "sent": "So that's that.",
                    "label": 0
                },
                {
                    "sent": "Makes the density problem very easy, but it also allows you to do things like interpolate between examples more meaningfully, and maybe predict things more easily.",
                    "label": 0
                },
                {
                    "sent": "Now you can have kind of what we've taken this manifold which was.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear and maybe in reality we're going to have manifolds which are very, very complicated, and by changing representation we essentially flatten the manifold, right?",
                    "label": 0
                },
                {
                    "sent": "So in the new representation space where one dimension is the position here and the other is orthogonal to it.",
                    "label": 0
                },
                {
                    "sent": "It's like if we made the whole thing Euclidean again.",
                    "label": 0
                },
                {
                    "sent": "And then we can do linear things in that space in order to make predictions and interpolate and do all kinds of fun things right?",
                    "label": 0
                },
                {
                    "sent": "So so we turned a complicated distribution, which was.",
                    "label": 0
                },
                {
                    "sent": "You know this this nonlinear manifold into something that's flat and Euclidean.",
                    "label": 0
                },
                {
                    "sent": "Now we can do things like PCA and and density estimation very easily.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So me.",
                    "label": 0
                },
                {
                    "sent": "Go back to this curse of dimensionality problem.",
                    "label": 1
                },
                {
                    "sent": "What I've been trying to say is that if we only use smoothness, we're not going to be able to defeat the curse of dimensionality and smoothness has been the ingredient in most statistical, nonparametric, statistical methods so.",
                    "label": 0
                },
                {
                    "sent": "We want to be nonparametric in the sense that we want to be able to have families of functions that grow in flexibility as we get more data and you will.",
                    "label": 0
                },
                {
                    "sent": "Nets are nonparametric in that sense, right?",
                    "label": 0
                },
                {
                    "sent": "So there, parametric in the sense that if we fix the number of units, we have a fixed set of parameters.",
                    "label": 0
                },
                {
                    "sent": "But actually, that's not the way we use neural Nets.",
                    "label": 0
                },
                {
                    "sent": "The way we use neural Nets is we allow ourselves to change the number of parameters by changing the number of hidden units.",
                    "label": 0
                },
                {
                    "sent": "Depending on how much data you have using a validation set, as Pascal was telling you at the end.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "The ability to let our model grow with a number of examples, but.",
                    "label": 1
                },
                {
                    "sent": "We need to go beyond this this smoothness prior.",
                    "label": 0
                },
                {
                    "sent": "It turns out that with deep learning there are at least two priors that week that are basically coming in for free.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to now explain them.",
                    "label": 0
                },
                {
                    "sent": "So one of them is basically coming under the name of distributed representations and the other is basically about depth.",
                    "label": 0
                },
                {
                    "sent": "And what I'll be telling you about is that these suppliers have a thing in common, which is that there are ways to exploit composition at compositionality and put it into our our model.",
                    "label": 0
                },
                {
                    "sent": "In other words, we were going to say that there is something about the world out there, the way that the world is generated are we coming back to this notion of understanding how the world was generated?",
                    "label": 0
                },
                {
                    "sent": "And making hypothesis about how it was generated.",
                    "label": 0
                },
                {
                    "sent": "But here we're going to make it very, very simple assumption that the way the data we're observing came up is by some composition of pieces, and the composition can be either parallel or sequential, or both.",
                    "label": 0
                },
                {
                    "sent": "So the parallel case is going to be the idea behind distributed presentations, and that's what I'm going to be spending.",
                    "label": 0
                },
                {
                    "sent": "A lot of time in next few slides.",
                    "label": 0
                },
                {
                    "sent": "And basically it's the same idea of you know, feature learning, but I'm going to explain how it's making assumptions about the world.",
                    "label": 0
                },
                {
                    "sent": "And the second is the depth.",
                    "label": 0
                },
                {
                    "sent": "So in other words, we're going to have multiple levels of feature learning an, whereas here it's going to be kind of parallel composition.",
                    "label": 1
                },
                {
                    "sent": "We choose this feature and that feature or that feature to be active, and we have a subset of features that are active to represent our input.",
                    "label": 0
                },
                {
                    "sent": "A pattern of activation of features.",
                    "label": 0
                },
                {
                    "sent": "In the case here, we're going to be.",
                    "label": 0
                },
                {
                    "sent": "Not taking these functions in parallel but in sequence, so we're going to have.",
                    "label": 0
                },
                {
                    "sent": "Functions that are obtained by composing other functions that are obtained by composing other functions, and we get that with deep architectures, and we get that with recurrent Nets and.",
                    "label": 0
                },
                {
                    "sent": "So we have these two things.",
                    "label": 0
                },
                {
                    "sent": "As you'll see, we will basically have added an additional prior or additional kind of intuitive notion of how the data come came about, which is assuming some form of compositionality explains the things we are observing in the world.",
                    "label": 0
                },
                {
                    "sent": "Any questions up to now?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, so so there are different definitions of nonparametric.",
                    "label": 0
                },
                {
                    "sent": "But the one that I like is 1 where we basically say nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Really means that we don't have a fixed parameter vector.",
                    "label": 0
                },
                {
                    "sent": "Depending on the data we get.",
                    "label": 0
                },
                {
                    "sent": "We can choose a family of functions that's more or less flexible.",
                    "label": 0
                },
                {
                    "sent": "So a linear classifier is parametric because I can't increase the flexibility even if I get 10 times more data.",
                    "label": 0
                },
                {
                    "sent": "I'm stuck with my linear model.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, with the neural network, if I get more data, I'm going to choose more hidden units and maybe more layers as well.",
                    "label": 0
                },
                {
                    "sent": "So the family of functions that I'm allowed to choose from is not fixed apriori.",
                    "label": 0
                },
                {
                    "sent": "It's actually growing depending on how many examples I get, and that's what nonparametric really means.",
                    "label": 0
                },
                {
                    "sent": "It's not about having no parameters, it's about not having a fixed parameter vector, but allowing yourself to choose the number of parameters that you need.",
                    "label": 0
                },
                {
                    "sent": "Depending on the richness of your data.",
                    "label": 0
                },
                {
                    "sent": "Is that answering your question?",
                    "label": 0
                },
                {
                    "sent": "Absolutely.",
                    "label": 0
                },
                {
                    "sent": "Yes, if you exactly So what you have is a non parametric family which uses as a building block a parametric family.",
                    "label": 0
                },
                {
                    "sent": "Metric.",
                    "label": 0
                },
                {
                    "sent": "In fact, in many state of the art.",
                    "label": 0
                },
                {
                    "sent": "Say in computer vision neural Nets.",
                    "label": 0
                },
                {
                    "sent": "There are many more weights, many more parameters than examples.",
                    "label": 0
                },
                {
                    "sent": "So we're talking about like millions of examples an hundreds of millions of parameters.",
                    "label": 0
                },
                {
                    "sent": "Typically, it's a very common situation, all right.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yeah, before I get there, I show you this picture to illustrate.",
                    "label": 0
                },
                {
                    "sent": "That comes from Hong, likely, who will be speaking shortly, maybe in a couple of days.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne Anne drawing at Stanford.",
                    "label": 0
                },
                {
                    "sent": "A few years ago, which illustrates one aspect of of.",
                    "label": 0
                },
                {
                    "sent": "Multiple levels of representation, which is that these representations can capture.",
                    "label": 1
                },
                {
                    "sent": "Kind of hierarchy of parts, so one way to get a composition of functions.",
                    "label": 0
                },
                {
                    "sent": "Is to have functions that detect.",
                    "label": 0
                },
                {
                    "sent": "Features like edges or parts of a face.",
                    "label": 0
                },
                {
                    "sent": "Anne compose them to get higher level objects or parts right?",
                    "label": 0
                },
                {
                    "sent": "So here what you see is that using these stacks of RBM's they were able to have the first level units that detect edges so they liked oriented contrasts at some locations and the next day they had units that preferred things like a nose or and I, and then the next level they had.",
                    "label": 0
                },
                {
                    "sent": "Units that preferred actually full faces in different configurations, so it's it's a it's not exactly what is going on.",
                    "label": 0
                },
                {
                    "sent": "There's much more to it, but it gives a glimpse of what multiple levels of representation could be apart space hierarchy, but there's there's more to that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I'll try to argue an exponential advantage over methods, I don't use this this.",
                    "label": 0
                },
                {
                    "sent": "This prior this structure.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the methods that don't use a distributor representation are things like classical clustering, N grams for language models, nearest neighbor methods, kernel, many kinds of kernel, SVM's, most nonparametric density estimation or regression using local kernels.",
                    "label": 0
                },
                {
                    "sent": "And so that's a lot of algorithms, and there's many more or also decision trees.",
                    "label": 1
                },
                {
                    "sent": "For those of you who know about this in trees.",
                    "label": 0
                },
                {
                    "sent": "There's a very simple way to understand what these algorithms do at a very high level.",
                    "label": 0
                },
                {
                    "sent": "What they do is you take your input space.",
                    "label": 0
                },
                {
                    "sent": "So we have the input space and again I'm doing a 2D thing, but you have to think this is in 1000 D or a million D. We have the input space and we're going to break it into regions.",
                    "label": 0
                },
                {
                    "sent": "We're going to partition the input space and for many of these, algorithm is actually a hard partition like this.",
                    "label": 0
                },
                {
                    "sent": "For others is something a bit more subtle where you have some some kind of soft partition.",
                    "label": 0
                },
                {
                    "sent": "Where are you allowed to know?",
                    "label": 0
                },
                {
                    "sent": "Kind of smooth interpolation between nearby regions, but but basically behind this you have a notion of regions.",
                    "label": 0
                },
                {
                    "sent": "And the important point is that we're going to have a different set of parameters for each region.",
                    "label": 1
                },
                {
                    "sent": "We're going to be able to tune what the answer should be in each region and where those regions are by using the data.",
                    "label": 0
                },
                {
                    "sent": "For example, if you're doing clustering, maybe then we have some examples or some centers that correspond to the centers of these regions.",
                    "label": 0
                },
                {
                    "sent": "And if you're doing one years neighbors, maybe every example is used to define a region.",
                    "label": 0
                },
                {
                    "sent": "And in those two cases you actually get is some kind of ranalli partition of the input.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You can think of how, which is the function we're going to compute.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "As related to how many regions there are 'cause I'm able to change the output in in each region depending on say how many points come here, or whether the labels are that are associated to each region.",
                    "label": 0
                },
                {
                    "sent": "To be the examples of following each region so so there's there's, there's a notion of complexity that's tide to how many regions I have.",
                    "label": 0
                },
                {
                    "sent": "The complexity in the sense of how rich, how flexible is my function.",
                    "label": 0
                },
                {
                    "sent": "So if I'm doing nearest neighbor, only have 3 examples.",
                    "label": 0
                },
                {
                    "sent": "My function is going to be very simple.",
                    "label": 0
                },
                {
                    "sent": "It's going to be some very simple partition if I have a million examples I can actually represent something very complicated, right?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "We can think in terms of sort of learning theory.",
                    "label": 0
                },
                {
                    "sent": "Ask the question how are we going to generalize?",
                    "label": 0
                },
                {
                    "sent": "And that's going to depend on the relationship between the number of examples I need and the kind of.",
                    "label": 0
                },
                {
                    "sent": "The kind of complexity I get, right?",
                    "label": 0
                },
                {
                    "sent": "So what do I mean here?",
                    "label": 0
                },
                {
                    "sent": "If I want to have a richer function?",
                    "label": 0
                },
                {
                    "sent": "I'm gonna need more regions right?",
                    "label": 0
                },
                {
                    "sent": "And to be able to define those regions, I'm going to need more.",
                    "label": 0
                },
                {
                    "sent": "Examples, so the there's a in for these kinds of models, there's a linear relationship between a number of distinguishable regions and the number of parameters, or equivalently, there's a linear relationship between a number of distinguishable regions and the number of training examples.",
                    "label": 0
                },
                {
                    "sent": "I want more regions anymore, more data, right?",
                    "label": 1
                },
                {
                    "sent": "It says, and the relationship is linear.",
                    "label": 0
                },
                {
                    "sent": "And you might think that's the only way that is possible, unless I introduce some extra constraints or prior.",
                    "label": 0
                },
                {
                    "sent": "If I don't know nothing else.",
                    "label": 0
                },
                {
                    "sent": "That's all I can do right if I want to be a bit finer about what's going on this region, I need more examples.",
                    "label": 0
                },
                {
                    "sent": "I can split this region in two and have a different answer in these two places.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's another option, and that other option is basically what's going on with neural Nets an and in general with distributed representations so.",
                    "label": 0
                },
                {
                    "sent": "This is maybe one of the most important slides of my whole lecture, so please pay attention.",
                    "label": 0
                },
                {
                    "sent": "So we are also going to define a kind of partitioning of the input space.",
                    "label": 0
                },
                {
                    "sent": "But we're going to do it in a way that's incredibly efficient.",
                    "label": 0
                },
                {
                    "sent": "We're going to do it in such a way that we're going to have an exponential number of regions at the price of a linear number of parameters.",
                    "label": 0
                },
                {
                    "sent": "So we'll be able to say something, a function about a function that looks very complicated.",
                    "label": 0
                },
                {
                    "sent": "In the sense that it has many ups and downs, many different kinds of values, many regions I can distinguish, and yet I don't need that many examples to learn it, right?",
                    "label": 0
                },
                {
                    "sent": "That's the magic part that Pascal was talking about, but it's no magic as well.",
                    "label": 0
                },
                {
                    "sent": "See, there's a reason why this is happening, and it means something about.",
                    "label": 0
                },
                {
                    "sent": "An assumption we're making regarding the world that generated the those data.",
                    "label": 0
                },
                {
                    "sent": "It may be that the these kinds of models like we have in your nuts.",
                    "label": 0
                },
                {
                    "sent": "Are not applicable everywhere to any random function.",
                    "label": 0
                },
                {
                    "sent": "In fact, they're not.",
                    "label": 0
                },
                {
                    "sent": "They work well in this universe in this on this planet.",
                    "label": 0
                },
                {
                    "sent": "Right, so let me go in a bit more detail.",
                    "label": 0
                },
                {
                    "sent": "How are we going to do that?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The way we get our regions, I'm going to consider a very simple case.",
                    "label": 0
                },
                {
                    "sent": "You can have more complicated cases, but I'm going to do a very simple case to illustrate.",
                    "label": 0
                },
                {
                    "sent": "We're going to have just one hidden layer of units, like here.",
                    "label": 0
                },
                {
                    "sent": "This is the two dimensional input.",
                    "label": 0
                },
                {
                    "sent": "And let's say we have these three units and again, I'm going to make these units as simple as can be, so.",
                    "label": 0
                },
                {
                    "sent": "Linear classifiers with A10 answer binary classifier, so each hidden unit says one.",
                    "label": 0
                },
                {
                    "sent": "Or zero, depending on which side you are on the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "So you know the weights of these units and the bias together define a hyperplane in input space on one side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "The unit says one on the other side it says zero OK.",
                    "label": 0
                },
                {
                    "sent": "So each unit defines a hyperplane and now we have some configuration here.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to saying on which side of each hyperplane we are.",
                    "label": 0
                },
                {
                    "sent": "So if we have 3 units.",
                    "label": 0
                },
                {
                    "sent": "We could think we have 8 configurations.",
                    "label": 0
                },
                {
                    "sent": "Actually there are only seven that exists.",
                    "label": 0
                },
                {
                    "sent": "Because, well, because we're in 2D and but that's a detail.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here?",
                    "label": 0
                },
                {
                    "sent": "We have more regions.",
                    "label": 0
                },
                {
                    "sent": "Then the number of.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "If we extend this to higher dimension, we're going to have a number of regions that grows exponentially with the number of features, and so with the number of parameters.",
                    "label": 0
                },
                {
                    "sent": "Because the number of letters is the number of features times the number of inputs.",
                    "label": 0
                },
                {
                    "sent": "Whereas the number of regions is essentially 22, the number of features.",
                    "label": 0
                },
                {
                    "sent": "Now if you actually have less dimensions than features is some correction for that, but it's still nearly exponential.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's exponential in the input size.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "You can see that there you know all the configurations of these of these bits that can occur.",
                    "label": 0
                },
                {
                    "sent": "Some of them can't, but.",
                    "label": 0
                },
                {
                    "sent": "But essentially, if.",
                    "label": 0
                },
                {
                    "sent": "If we have the input size large enough, we can have an exponential number of regions.",
                    "label": 0
                },
                {
                    "sent": "OK, so compared to the situation we had previously.",
                    "label": 0
                },
                {
                    "sent": "We had a number of regions that was linear in the number of parameters or linear in number of examples that we need.",
                    "label": 0
                },
                {
                    "sent": "Here we have a number of regions that can potentially grow exponentially with the number of parameters.",
                    "label": 0
                },
                {
                    "sent": "An end with a number of examples.",
                    "label": 0
                },
                {
                    "sent": "So how is that possible?",
                    "label": 0
                },
                {
                    "sent": "You know there must be a trick, right?",
                    "label": 0
                },
                {
                    "sent": "Is is it magical or there's a cheat?",
                    "label": 0
                },
                {
                    "sent": "Well, every prior is a kind of cheat.",
                    "label": 0
                },
                {
                    "sent": "It works for some things, but it doesn't work for everything.",
                    "label": 0
                },
                {
                    "sent": "So you see that you know those regions.",
                    "label": 0
                },
                {
                    "sent": "I have these seven regions here.",
                    "label": 0
                },
                {
                    "sent": "I can't really choose their locations and also let's say I put linear classifier on top of this.",
                    "label": 0
                },
                {
                    "sent": "I can't choose the answers I'll get in dependently for each region.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There are some constraints about the kinds of functions that can represent.",
                    "label": 0
                },
                {
                    "sent": "Not every function or not.",
                    "label": 0
                },
                {
                    "sent": "Every partition here is is feasible, whereas in the other case I could do any partition, right?",
                    "label": 0
                },
                {
                    "sent": "It was a very very flexible model.",
                    "label": 0
                },
                {
                    "sent": "So this is going to work under some conditions, so let's try to figure out what those conditions are.",
                    "label": 0
                },
                {
                    "sent": "Let's keep them for this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you an example.",
                    "label": 0
                },
                {
                    "sent": "Imagine that now the input is an image of a person.",
                    "label": 0
                },
                {
                    "sent": "And that my 3 features are binary detectors, so they're going to have to be nonlinear rather than here.",
                    "label": 0
                },
                {
                    "sent": "They can be binary detectors that tell me if the person is.",
                    "label": 0
                },
                {
                    "sent": "Is a tall or short?",
                    "label": 0
                },
                {
                    "sent": "Maybe there's another one that tells me if the person is male or female, and maybe there's another one that tells me if the person wears glasses or not.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you see that.",
                    "label": 0
                },
                {
                    "sent": "With these kinds of features, it makes sense that we can learn about each of those features.",
                    "label": 0
                },
                {
                    "sent": "Almost independently of each other, right?",
                    "label": 0
                },
                {
                    "sent": "I can I can train detector.",
                    "label": 0
                },
                {
                    "sent": "I could potentially imagine training a detector or having machine discovering what you wearing glasses means visually.",
                    "label": 0
                },
                {
                    "sent": "And I don't need to see all the configurations of the other features for learning that.",
                    "label": 0
                },
                {
                    "sent": "'cause detecting you know I glass detector is going to work should work independently of how tall the person is, whether you know the person is male or female.",
                    "label": 0
                },
                {
                    "sent": "I mean that might be some little interactions.",
                    "label": 0
                },
                {
                    "sent": "Some females have funny glasses maybe.",
                    "label": 0
                },
                {
                    "sent": "But wow, from from my point of view of course.",
                    "label": 0
                },
                {
                    "sent": "So there might be some interactions, but mostly you can learn about each of these features independently of the other, and so you can see that.",
                    "label": 0
                },
                {
                    "sent": "And you can have more than three where you could have a thousand of these features.",
                    "label": 0
                },
                {
                    "sent": "You can see that the number of examples I might need to learn those features doesn't need to grow as two to the thousand.",
                    "label": 0
                },
                {
                    "sent": "It just grows like 1000.",
                    "label": 0
                },
                {
                    "sent": "The more features I put in, you know, each time I add a feature I don't need to double the number of examples I see I just need to have enough examples of that feature being present or not present.",
                    "label": 0
                },
                {
                    "sent": "And so so we get what we want, right?",
                    "label": 0
                },
                {
                    "sent": "But but it works because.",
                    "label": 0
                },
                {
                    "sent": "We're assuming that the world is arising out of the interactions of those features that some say are one summer, 0 for a particular example, and for any particular example, we just pick a pattern of these.",
                    "label": 0
                },
                {
                    "sent": "And we can pretty much learn independently.",
                    "label": 0
                },
                {
                    "sent": "Each of these features.",
                    "label": 0
                },
                {
                    "sent": "If we were given, you know enough the appropriate kind of data that covers their variations, we should be able to learn about each of these features more less independently.",
                    "label": 0
                },
                {
                    "sent": "So that's that's what gives us an exponential advantage, because now we don't need.",
                    "label": 0
                },
                {
                    "sent": "The number of regions to corresponding number examples.",
                    "label": 0
                },
                {
                    "sent": "We can have many regions which are all the configurations of those features, and yet the number of examples is reasonable.",
                    "label": 0
                },
                {
                    "sent": "It grows only linearly with the number of features.",
                    "label": 0
                },
                {
                    "sent": "Do you have some questions about this?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Well, I gave you a real world example of an image.",
                    "label": 0
                },
                {
                    "sent": "With.",
                    "label": 0
                },
                {
                    "sent": "That's the input, and the features are binary features which tell you something about the image.",
                    "label": 0
                },
                {
                    "sent": "Different aspects of it may be different parts, so I talked about glasses about the size of the height of a person and you can.",
                    "label": 0
                },
                {
                    "sent": "You can imagine you know many more features, but the point is, if you imagine having you know somehow data that characterizes your distribution and that covers maybe the two different values of each feature.",
                    "label": 0
                },
                {
                    "sent": "To keep things simple at their binary, they don't have to be.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "You don't need to see the configurations of all of the other features to really learn about each one, and that's.",
                    "label": 0
                },
                {
                    "sent": "That's different from from the situation with nondescript representations where somehow I need to see all the configurations of my pixels to be able to generalize here.",
                    "label": 0
                },
                {
                    "sent": "The thing that's going to make me able to generalize is that.",
                    "label": 0
                },
                {
                    "sent": "I don't need to see all the configurations of those features to be able to make a meaningful statement right.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "It could be that I've never seen 000 here, but I've seen the first feature being zero alone.",
                    "label": 0
                },
                {
                    "sent": "I've seen the second feature being zero in some other configuration, and I think that I've seen the third feature being zero and maybe they just sort of add up to come up with with some weights to come up with an answer about something you care about that maybe only depends on some subset of them, or some linear combination of them so.",
                    "label": 0
                },
                {
                    "sent": "So that's what allows you to generalize that you are able to.",
                    "label": 0
                },
                {
                    "sent": "To say something about configurations of these features that you have never seen because individually they keep their meaning like this.",
                    "label": 0
                },
                {
                    "sent": "The meaning of tile person.",
                    "label": 0
                },
                {
                    "sent": "This is not tall person is something that remains meaningful whatever the other elements of the visual scene are.",
                    "label": 0
                },
                {
                    "sent": "And you don't need to see all the configurations of all the other variables in order to be able to make sense of that feature, yes.",
                    "label": 0
                },
                {
                    "sent": "Well, they don't have to be exactly independent, but if they were in some in some in the right chosen way of formalizing it, yes, that would be really helpful in practice.",
                    "label": 0
                },
                {
                    "sent": "Of course, they won't be exactly independence.",
                    "label": 0
                },
                {
                    "sent": "I gave him my example.",
                    "label": 0
                },
                {
                    "sent": "Maybe you know the female glasses in the glasses are not exactly the same, so you actually need to see.",
                    "label": 0
                },
                {
                    "sent": "Both of these.",
                    "label": 0
                },
                {
                    "sent": "But there are many things about glasses that don't depend about whether you know how the style is is just the shape and where they are, and these things could be learned independently of the other configurations.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Maybe I don't understand your question, but yes, there's, there's some math that tells things about how many regions we get depending on number of features and number of inputs, and maybe that's not the question you are asking.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Maximum over what set?",
                    "label": 0
                },
                {
                    "sent": "I don't understand what the maximum corresponds to maximum in what sense.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see what you mean, yes, so so we have a NIPS paper in 2014.",
                    "label": 0
                },
                {
                    "sent": "Which studies fairly deeply these questions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Regarding the number of regions, what happens is that.",
                    "label": 0
                },
                {
                    "sent": "If you're not lucky, you might have.",
                    "label": 0
                },
                {
                    "sent": "And our coincidences that make some of those hyperplanes do nothing, because you know, they kind of align in a funny way.",
                    "label": 0
                },
                {
                    "sent": "But the chances of this happening is is like measure.",
                    "label": 0
                },
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "It's very unlikely.",
                    "label": 0
                },
                {
                    "sent": "So for most parameters configurations you're going to get the maximum number of regions.",
                    "label": 0
                },
                {
                    "sent": "Yes, and we can count that we so we have.",
                    "label": 0
                },
                {
                    "sent": "Approximate counting arguments that give us bounds on the number of regions you can get.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, but I haven't talked about depth yet, so I'm only talking about the notion of distributor presentation.",
                    "label": 0
                },
                {
                    "sent": "So one layer I'm going to tell you what happens now if you combine multiple levels, but but even if you think about just one level of representation, you already get an exponential advantage here.",
                    "label": 0
                },
                {
                    "sent": "So the number of examples you need to get a certain level of normalization might be exponentially smaller than if you use.",
                    "label": 0
                },
                {
                    "sent": "If you don't use this prior.",
                    "label": 0
                },
                {
                    "sent": "So if you just use the smoothness prior, might be.",
                    "label": 0
                },
                {
                    "sent": "In practice we never see.",
                    "label": 0
                },
                {
                    "sent": "Such an advantage, right?",
                    "label": 0
                },
                {
                    "sent": "So if you compare like the best neural Nets for computer vision, ANAN the sort of state of the art using, say, pure kernel machines, we don't observe an exponential ratio of the number of examples needed for getting a particular transition, or we we get a big advantage, but it's not exponential.",
                    "label": 0
                },
                {
                    "sent": "Uh huh.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's the question of what do we put on top of these features?",
                    "label": 0
                },
                {
                    "sent": "So you need to have enough flexibility to exploit this representation.",
                    "label": 0
                },
                {
                    "sent": "But the good news is if you need to do what I was trying to explain about these manifolds.",
                    "label": 0
                },
                {
                    "sent": "If this representation is really good, what it really is doing is it's unfolding the manifold.",
                    "label": 0
                },
                {
                    "sent": "It's like a new coordinate system where things are flat.",
                    "label": 0
                },
                {
                    "sent": "And if you have really flatten the manifold, then basically linear classifiers can do a good job.",
                    "label": 0
                },
                {
                    "sent": "Alright, yes.",
                    "label": 0
                },
                {
                    "sent": "Discrete model is optimized on some of the parameters that we learn and that can help in this way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm not sure to understand your question.",
                    "label": 0
                },
                {
                    "sent": "But yeah, there are situations where you do want to have the kind of flexibility that the classical nonparametric methods bring.",
                    "label": 0
                },
                {
                    "sent": "And so there so there's certainly room to think about how to combine them in places where you need, for example, a lot of memory to store instances, right?",
                    "label": 0
                },
                {
                    "sent": "So neural Nets are really good at.",
                    "label": 0
                },
                {
                    "sent": "Discovering representations that are captured.",
                    "label": 0
                },
                {
                    "sent": "Semantic aspects.",
                    "label": 0
                },
                {
                    "sent": "Like what is it that makes a cat be a cat?",
                    "label": 0
                },
                {
                    "sent": "What are the features that makes a cat be account but?",
                    "label": 0
                },
                {
                    "sent": "It's not very good at learning things by heart if I want to remember about you know my cat, your cat, and his cat, and maybe 1000 other cats.",
                    "label": 0
                },
                {
                    "sent": "I might want to use some kind of memory based nonparametric type of model.",
                    "label": 0
                },
                {
                    "sent": "So yes, there are certainly applications where you want to combine both.",
                    "label": 0
                },
                {
                    "sent": "But the generalization comes from these representations.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I this is, I think, something we don't fully understand yet.",
                    "label": 0
                },
                {
                    "sent": "The question you're asking about.",
                    "label": 0
                },
                {
                    "sent": "First of all, the adversarial examples problem occurs for many machine learning is not really specific to neural Nets.",
                    "label": 0
                },
                {
                    "sent": "2nd, 2nd, but you're right.",
                    "label": 0
                },
                {
                    "sent": "You're right that having kind of RBF units kind of limits this problem, but but then, if you have a model that doesn't suffer from, you know this.",
                    "label": 0
                },
                {
                    "sent": "These weird examples could actually happen, even though very rarely, or somebody could make them up.",
                    "label": 0
                },
                {
                    "sent": "That's certainly something we have to worry about, but then not getting any good realization is not good either.",
                    "label": 0
                },
                {
                    "sent": "You first have to have good realization and then if there are issues we need to fix them.",
                    "label": 0
                },
                {
                    "sent": "So there's already some work showing that you can train with these adversarial examples and make the system a lot.",
                    "label": 0
                },
                {
                    "sent": "Much, much more robust to them, but this is very, very sort of recent work, and we need more investigation in that direction.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Creating like some clusters, like smart, but that's what classical non distributed presentations do.",
                    "label": 0
                },
                {
                    "sent": "They take essentially all of the regions that they can given the data that they have.",
                    "label": 0
                },
                {
                    "sent": "But they don't get any generalization out of this.",
                    "label": 0
                },
                {
                    "sent": "In other words, if I give you a location in input space which you have no data, you're not able to say anything about it, whereas.",
                    "label": 0
                },
                {
                    "sent": "With this approach, we are able to say something meaningful about configurations we've never seen before, and that is the essence of visualization.",
                    "label": 0
                },
                {
                    "sent": "OK, I should probably move on.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yes, except that the pixels are except the pixels.",
                    "label": 0
                },
                {
                    "sent": "That's right, except that the pixels are not a good representation.",
                    "label": 0
                },
                {
                    "sent": "Because you can't learn about the distribution of each pixel.",
                    "label": 0
                },
                {
                    "sent": "Almost independently of the different the other pixels, they're very strongly tide together.",
                    "label": 0
                },
                {
                    "sent": "Whereas here I can learn about, you know what the relationship between the input and an each of these features almost independently.",
                    "label": 0
                },
                {
                    "sent": "So in a sense.",
                    "label": 0
                },
                {
                    "sent": "We're kind of factorizing.",
                    "label": 0
                },
                {
                    "sent": "The distribution saying the way it comes about is that we have these sort of independent factors that explain what we see and these actually are the kinds of things we talk about in natural language, natural language.",
                    "label": 0
                },
                {
                    "sent": "Those meant, I think, to explain those high level factors that that you know from one person to another person that explain the data.",
                    "label": 0
                },
                {
                    "sent": "So, so the things we talk about that we put labels on tend to be explanatory factors.",
                    "label": 0
                },
                {
                    "sent": "So whether the person is male or female or has glasses, or is short or has long hair, these are things we've put names on because they help us making sense of the world.",
                    "label": 0
                },
                {
                    "sent": "There are high level factors, but not any.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right exactly.",
                    "label": 0
                },
                {
                    "sent": "OK, I should probably move on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this idea of this representation was really at the heart of the renewal of neural Nets that happened in the early 80s.",
                    "label": 0
                },
                {
                    "sent": "That was called Connectionism or the connectionists approach, and two of the major players of this renewal or Jeff Hinton and David Rumelhart, and a bunch of others, of course.",
                    "label": 1
                },
                {
                    "sent": "And they were kind of.",
                    "label": 0
                },
                {
                    "sent": "Trying to present this way of thinking in contrast with the classical AI approach, which is based on the notion of symbols.",
                    "label": 0
                },
                {
                    "sent": "Lucia symbol is something that's very natural when you think about symbolic processing of things like language or logic and rules where each concept.",
                    "label": 0
                },
                {
                    "sent": "Is is associated with just a pure?",
                    "label": 0
                },
                {
                    "sent": "Entity a symbol is just something that's either present or not present.",
                    "label": 0
                },
                {
                    "sent": "And there's there's nothing intrinsic about those symbols that you know.",
                    "label": 0
                },
                {
                    "sent": "Define some kind of relationship between them, whereas if we think about concepts as patterns of features, patterns of activations of neurons in the brain.",
                    "label": 0
                },
                {
                    "sent": "Then the concept of cat and the concept of dog.",
                    "label": 0
                },
                {
                    "sent": "Can be related in the sense that they would share some features.",
                    "label": 0
                },
                {
                    "sent": "So here I've tried to illustrate that by saying OK, so I have some some symbolic representation here, which is my my input or it could be an output where there's a one.",
                    "label": 0
                },
                {
                    "sent": "What's called 1 hot representation where I have a unit that says it's a cap and the other units are saying other things like dog and so on.",
                    "label": 0
                },
                {
                    "sent": "And it's either one or the other.",
                    "label": 0
                },
                {
                    "sent": "But I also have a distributor representation, so a bunch of features that every concept can have an an the features for CAT and the features for dog.",
                    "label": 0
                },
                {
                    "sent": "So this is now the representation for dog and the representation for Cat somehow are close to each other because they share a lot of attributes there.",
                    "label": 0
                },
                {
                    "sent": "Both pets and mammals and so on.",
                    "label": 0
                },
                {
                    "sent": "And so if we if we.",
                    "label": 0
                },
                {
                    "sent": "We are able to measure kind of distance between those symbols, whereas between symbols there is nothing to say.",
                    "label": 0
                },
                {
                    "sent": "They're all different from each other in equal ways and this is something that's really important for for deep learning and for neural Nets to understand this notion of.",
                    "label": 1
                },
                {
                    "sent": "Representation of concepts of of what may seem like symbolic concept to start with.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is something I built on with what's called a neural language model, starting with NIPS paper in 2000, of which actually Pascal, Anne and others are coauthors.",
                    "label": 0
                },
                {
                    "sent": "We built models of natural language sequences of words.",
                    "label": 0
                },
                {
                    "sent": "Using this idea of.",
                    "label": 0
                },
                {
                    "sent": "I distributed representation for words, so the way we did this is we train the neural net that has input a sequence of words and tries to predict the next word.",
                    "label": 0
                },
                {
                    "sent": "You could try to predict all kinds of things, but in that paper we just predict.",
                    "label": 0
                },
                {
                    "sent": "The probability for the next word being any one of the words in the vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So we have a large.",
                    "label": 0
                },
                {
                    "sent": "Output layer with one unit per word in the vocabulary an.",
                    "label": 0
                },
                {
                    "sent": "We have also very large inputs where you have one hot representation for each word in the input window, so it's not a bag of words representation like Pascal said.",
                    "label": 0
                },
                {
                    "sent": "Instead, it's just one hot vector for each position, and the crucial point here is that the first layer of that neural net is just a mapping that's learned that projects these symbols these integers.",
                    "label": 0
                },
                {
                    "sent": "Into a district representation which is just a vector of numbers, and we use the same mapping for all the words independent of their position.",
                    "label": 0
                },
                {
                    "sent": "Of course, each word in the vocabulary gets a different mapping and these mappings.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are things that we can visualize by two dimensional projections usually known in your projections, and we see clusters of world of words coming up that are semantically similar.",
                    "label": 0
                },
                {
                    "sent": "So if we zoom in and zoom in again, we see things like, you know, countries having their vectors, their word vectors, their their district representations, being close to each other.",
                    "label": 0
                },
                {
                    "sent": "And this is not something that was built in.",
                    "label": 0
                },
                {
                    "sent": "It just discovers these representations that have these properties.",
                    "label": 0
                },
                {
                    "sent": "Or maybe verbs come close to each other.",
                    "label": 0
                },
                {
                    "sent": "Or conjugations of two become close to each other.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And even more exciting is that these vectors somehow have dimensions that are learned that is completely discovered from scratch.",
                    "label": 0
                },
                {
                    "sent": "That or are meaningful and whose meaning is.",
                    "label": 0
                },
                {
                    "sent": "Preserved across space so you can do some kind of arithmetic with those vectors, simple arithmetic and an.",
                    "label": 0
                },
                {
                    "sent": "It will allow you to do things like reasoning by analogy.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I take the vector for friends and I subtract the vector for Paris, I now get a new vector.",
                    "label": 0
                },
                {
                    "sent": "Which is pretty much aligned with the vector I get by subtracting the representation from for Italy from the representation from Rome.",
                    "label": 0
                },
                {
                    "sent": "So so these two different vectors are almost the same.",
                    "label": 0
                },
                {
                    "sent": "They represent the meaning of the capital of a state.",
                    "label": 0
                },
                {
                    "sent": "And you can do the same game with many pairs of words.",
                    "label": 0
                },
                {
                    "sent": "So if you take King minus Queen, you get a vector which is almost the same as man minus woman.",
                    "label": 0
                },
                {
                    "sent": "So why is that happening?",
                    "label": 0
                },
                {
                    "sent": "This is initially we were very surprised about this and it seems almost amazing.",
                    "label": 0
                },
                {
                    "sent": "But it makes a lot of sense.",
                    "label": 0
                },
                {
                    "sent": "So if you think about a pattern of activation corresponding to King and the pattern of activation corresponding to Queen.",
                    "label": 0
                },
                {
                    "sent": "They should be almost the same.",
                    "label": 0
                },
                {
                    "sent": "In most dimensions, except for what has to do with, you know, being male or female.",
                    "label": 0
                },
                {
                    "sent": "An and if we're lucky, maybe there is a input dimension that corresponds, maybe a bit that says this is the male version of the concept, or it's the female version of the concept.",
                    "label": 0
                },
                {
                    "sent": "And of course, the same bit would be present in men and women and men and women would have all of the same attributes except for that bit.",
                    "label": 0
                },
                {
                    "sent": "So now when we take the difference between those two vectors, what remains is just a bit and we take the difference between those two vectors.",
                    "label": 0
                },
                {
                    "sent": "Again we get the same bit.",
                    "label": 0
                },
                {
                    "sent": "Now, in practice it's not going to be a particular axis.",
                    "label": 0
                },
                {
                    "sent": "Aligned direction is going to be a vector that's not at 1 hot vector, but it's the same concept.",
                    "label": 0
                },
                {
                    "sent": "Right, so we are learning these representations that are.",
                    "label": 0
                },
                {
                    "sent": "Because they are used to try to capture the distribution of the data.",
                    "label": 0
                },
                {
                    "sent": "I tend to.",
                    "label": 0
                },
                {
                    "sent": "Discover meaningful aspects of the world that are like the underlying factors we may think about that can generate the data an because that happens, we get amazing kinds of generalizations, like in this example.",
                    "label": 0
                },
                {
                    "sent": "It's not just these kinds of toy examples, these kinds of models, of course are used in practice for solving many problems.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, I should move on.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You talk about depth.",
                    "label": 0
                },
                {
                    "sent": "So I said that.",
                    "label": 0
                },
                {
                    "sent": "Having a district representation is gives you an exponential advantage at least.",
                    "label": 0
                },
                {
                    "sent": "Up to an exponential advantage, but there's another source of exponential advantage which is.",
                    "label": 0
                },
                {
                    "sent": "Can be composed can be obtained on top of what you get with this representation which is having multiple levels of representation, so that's deep.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to explain this.",
                    "label": 0
                },
                {
                    "sent": "I first need to tell you a little bit about some of the history here about representations.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of misunderstanding about what depth brings.",
                    "label": 0
                },
                {
                    "sent": "That I'm going to try to dispel here, but before I get there.",
                    "label": 0
                },
                {
                    "sent": "One reason why depth was not studied that much before.",
                    "label": 0
                },
                {
                    "sent": "Century.",
                    "label": 0
                },
                {
                    "sent": "Is that many people thought that you don't need?",
                    "label": 0
                },
                {
                    "sent": "A deep neural net, a shallow neural net with a single hidden layer, in theory is sufficient to represent any function, and it's not just neural Nets.",
                    "label": 0
                },
                {
                    "sent": "You can have two layers of logic gates, for example insufficient to represent any Boolean function, or you know RBF network with one layer of RBF units is sufficient to approximate any function.",
                    "label": 1
                },
                {
                    "sent": "Assuming you have enough of these units in a few minutes, you can.",
                    "label": 0
                },
                {
                    "sent": "You can approximate any function with a required degree of accuracy, and we call this property universal approximation property.",
                    "label": 0
                },
                {
                    "sent": "But what this doesn't?",
                    "label": 0
                },
                {
                    "sent": "These results don't tell us is how many units will I need?",
                    "label": 0
                },
                {
                    "sent": "So it may be that with a deeper neural net.",
                    "label": 0
                },
                {
                    "sent": "We could represent the same function that we could also represent with a shallow one, but more cheaply with less units an in fact.",
                    "label": 0
                },
                {
                    "sent": "There's now a series of results showing that this is true, and that there exists families of functions and fairly large ones for which you can get an exponential advantage.",
                    "label": 0
                },
                {
                    "sent": "So the number of units or the number of parameters you need.",
                    "label": 1
                },
                {
                    "sent": "Can be exponentially larger if you have only a shallow network.",
                    "label": 0
                },
                {
                    "sent": "Then if you allow yourself to have a network that's deep enough now how deep?",
                    "label": 0
                },
                {
                    "sent": "We don't know it depends on the function is trying to represent.",
                    "label": 0
                },
                {
                    "sent": "So presumably if you're trying to represent a function that's.",
                    "label": 0
                },
                {
                    "sent": "Very deep, in other words, that the true function needs many levels of composition to really be represented properly.",
                    "label": 0
                },
                {
                    "sent": "Then your neural net needs more layers, but we don't know ahead of time how many layers you need.",
                    "label": 0
                },
                {
                    "sent": "One thing I want to mention is that.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Death is not necessary to have a flexible family of function, so people think that if we have deeper networks, that means that they represent somehow a corresponds to higher capacity.",
                    "label": 0
                },
                {
                    "sent": "But actually, that's not necessarily the case.",
                    "label": 0
                },
                {
                    "sent": "You can still roughly think about the number of free parameters representing your capacity so deeper doesn't mean you can represent more functions, it's just that the set of functions that you can represent.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "It has a particular characteristic it has this characteristic that is obtained through composition of many operations, and if the functions you want to learn have this characteristic, this property, then you're much better off approximating these functions with a deep neural net.",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 0
                },
                {
                    "sent": "There are some other analogies that help you understand depth.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This notion of reuse, so in a deep neural net, we're reusing the lower layer features to produce the higher level features.",
                    "label": 0
                },
                {
                    "sent": "When we write a computer program, we don't usually write, you know main program.",
                    "label": 1
                },
                {
                    "sent": "With you know one line after the other.",
                    "label": 0
                },
                {
                    "sent": "Typically what we do is we have sub.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Routines.",
                    "label": 0
                },
                {
                    "sent": "That we reuse so you can think of what the hidden units are doing as.",
                    "label": 0
                },
                {
                    "sent": "Kind of subroutines for the bigger program, which is what you have at the final layer, so that's one way to think about it.",
                    "label": 0
                },
                {
                    "sent": "Another way to think about it is that actually when you run a program, it's a sequence of operations.",
                    "label": 0
                },
                {
                    "sent": "So if you have a program that has 10 lines, you can think of it like at each line.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's Python code at each line.",
                    "label": 0
                },
                {
                    "sent": "You're doing some computation and the result of that computation is changing the state of the machine.",
                    "label": 0
                },
                {
                    "sent": "To provide an input for the next line.",
                    "label": 0
                },
                {
                    "sent": "Right, so the input at each step of the computation.",
                    "label": 0
                },
                {
                    "sent": "Is the state of the machine an and the output is a new state of the machine?",
                    "label": 0
                },
                {
                    "sent": "So that's the Turing machine, right?",
                    "label": 0
                },
                {
                    "sent": "So the number of steps that a particular Turing machine executes actually corresponds to the depth of computation.",
                    "label": 0
                },
                {
                    "sent": "And so, although in principle you can represent.",
                    "label": 0
                },
                {
                    "sent": "Any function by having to to program with two steps?",
                    "label": 0
                },
                {
                    "sent": "It's not going to be a very interesting program.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be, it's going to be able to compute anything.",
                    "label": 0
                },
                {
                    "sent": "But it's not going to be able to compute interesting functions efficiently, so how would you have a program in two steps in two lines that can output any function?",
                    "label": 0
                },
                {
                    "sent": "With some precision, how could you do that?",
                    "label": 0
                },
                {
                    "sent": "How could you have a two step program?",
                    "label": 0
                },
                {
                    "sent": "That can represent any function.",
                    "label": 0
                },
                {
                    "sent": "It's a Riddle.",
                    "label": 0
                },
                {
                    "sent": "Look up table, thanks.",
                    "label": 0
                },
                {
                    "sent": "That's what a SVM kernel SVM does.",
                    "label": 0
                },
                {
                    "sent": "It's a look up table.",
                    "label": 0
                },
                {
                    "sent": "A shallow neural net is kind of a look up table as well.",
                    "label": 0
                },
                {
                    "sent": "So look up table first line you know get the content of some at some address and you look up table.",
                    "label": 0
                },
                {
                    "sent": "Second line take the result and you know do something with.",
                    "label": 0
                },
                {
                    "sent": "It may be multiplied by some number and an output.",
                    "label": 0
                },
                {
                    "sent": "OK, so so we need deeper programs.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's another analogy that I sometimes use that helps to understand this notion of reuse.",
                    "label": 0
                },
                {
                    "sent": "So most people being trained in the bit of mathematics understand polynomials.",
                    "label": 0
                },
                {
                    "sent": "So let's think how you could represent polynomials.",
                    "label": 0
                },
                {
                    "sent": "You could represent polynomials by running out the polynomial as a sum of products.",
                    "label": 0
                },
                {
                    "sent": "Right, that's the usual.",
                    "label": 0
                },
                {
                    "sent": "Form.",
                    "label": 0
                },
                {
                    "sent": "But you could also write it as a graph.",
                    "label": 0
                },
                {
                    "sent": "Of computations where each node performance in addition or multiplication, so this is what I've chosen here.",
                    "label": 0
                },
                {
                    "sent": "And now if you do it this way, you can have kind of deep computation where you could represent a polynomial that has an exponentially large number of in terms of these terms.",
                    "label": 0
                },
                {
                    "sent": "If you do the summer products, even though the number of computations I'm going to, the number of additions and multiplications could be small, so why would that happen?",
                    "label": 0
                },
                {
                    "sent": "Well, because I can reuse some operations like.",
                    "label": 0
                },
                {
                    "sent": "Let's say I have this node that multiply X 2X3.",
                    "label": 0
                },
                {
                    "sent": "Because it's it's value is going to be used in both of these two nodes and then combine later.",
                    "label": 0
                },
                {
                    "sent": "This product X 2X3 comes up in the polynomial associated with that later node in many places, like in four places.",
                    "label": 0
                },
                {
                    "sent": "And so even though.",
                    "label": 0
                },
                {
                    "sent": "I. I have, you know, maybe an exponential number of of.",
                    "label": 0
                },
                {
                    "sent": "Of these products coming up later, I needed to do this product X 2 * X Three only once.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is a. I guess an illustration of that same idea, and there's some theory you know that's connected to that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mentioned this result from our last NIPS, which basically shows.",
                    "label": 0
                },
                {
                    "sent": "Very large family functions.",
                    "label": 0
                },
                {
                    "sent": "That for large families of functions, the result I was telling you about with an exponential gain both for shallow neural net and for deep neural net.",
                    "label": 0
                },
                {
                    "sent": "So these are rectifier networks or piecewise linear networks, so that includes things like Max out.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that these types of neural Nets are very common Now, so this is a fairly large family of functions.",
                    "label": 0
                },
                {
                    "sent": "And this paper is about counting number of regions.",
                    "label": 0
                },
                {
                    "sent": "Exactly what I was telling you about before, but doing it in a very formal way.",
                    "label": 0
                },
                {
                    "sent": "And so it basically shows that with a single hidden layer you get an exponential number of regions compared to.",
                    "label": 0
                },
                {
                    "sent": "Exponential in the number of units, right or inputs?",
                    "label": 0
                },
                {
                    "sent": "There's depends on both.",
                    "label": 0
                },
                {
                    "sent": "And if you have more layers then you can get another exponential gain.",
                    "label": 0
                },
                {
                    "sent": "OK, so so I'll let you look at these if you are more into the math for these things.",
                    "label": 0
                },
                {
                    "sent": "The last last bit of sort of motivating theory I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not about an advantage, but about a.",
                    "label": 0
                },
                {
                    "sent": "Belief that seems to be wrong about a supposed disadvantage of neural Nets, so one of the reasons why neural Nets were discarded in the late 90s and early 2000 was that the.",
                    "label": 0
                },
                {
                    "sent": "Optimization problem for neural Nets is non convex.",
                    "label": 0
                },
                {
                    "sent": "Meaning that there could be lots of local minima.",
                    "label": 0
                },
                {
                    "sent": "In fact, since the late 80s early 90s, we know from theory that.",
                    "label": 0
                },
                {
                    "sent": "There are an exponential number of local minima in neural Nets.",
                    "label": 0
                },
                {
                    "sent": "And this this fact, combined with the success of kernel machines in the mid 90s, late 90s.",
                    "label": 0
                },
                {
                    "sent": "So sort of play role in kind of greatly reducing the interest of many researchers in your nuts.",
                    "label": 0
                },
                {
                    "sent": "But this was relying on the idea that if we have many local minima because the optimization function is not convex, then we're in trouble.",
                    "label": 0
                },
                {
                    "sent": "An for sure, we're not going to get any guarantees about finding the optimal solution, but Furthermore we might be stuck in very poor solutions.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something changed very recently, so over the last year.",
                    "label": 0
                },
                {
                    "sent": "We now have evidence both at the Terry Cloth vertical level and empirical level, that this issue of non convexity is actually maybe not an issue at all.",
                    "label": 0
                },
                {
                    "sent": "Or at least it changes the picture we have of the optimization problem with neural Nets.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in the optimization aspects of neural Nets or deep learning, I encourage you to look at these papers so there is.",
                    "label": 0
                },
                {
                    "sent": "We presented this for the first time.",
                    "label": 0
                },
                {
                    "sent": "Then one year ago, roughly in the spring, then we had a NIPS paper, and then the group at NYU with Unnatural Manske and Yellow Curb presented a different kind of perspective on the same problem, which essentially comes down to the same kind of conclusions.",
                    "label": 0
                },
                {
                    "sent": "So let me try to explain a little bit what these papers say.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They tell us about saddle points.",
                    "label": 1
                },
                {
                    "sent": "So let me explain a little bit the picture of the optimization problem in high dimensions versus low dimensions.",
                    "label": 0
                },
                {
                    "sent": "So if we are in low dimensions, which is the kinds of things we mentale visualize.",
                    "label": 0
                },
                {
                    "sent": "You're going to have a lot of local minimum.",
                    "label": 0
                },
                {
                    "sent": "Right, if I do, if I draw a random 1 dimensional curve, it's going to have lots of ups and downs, and here's a random 2 dimensional curve, right so?",
                    "label": 0
                },
                {
                    "sent": "A random curve in two dimension typically has lots of local minima and with that kind of picture we think that local minima are going to be a big problem.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "Local minima are not really the critical points that are most prevalent in most of the places of interest.",
                    "label": 0
                },
                {
                    "sent": "So let me try to explain what I mean.",
                    "label": 0
                },
                {
                    "sent": "Basically, when we optimize our neural Nets or any high dimensional functions.",
                    "label": 0
                },
                {
                    "sent": "For most of our trajectory as we optimize.",
                    "label": 0
                },
                {
                    "sent": "The critical points.",
                    "label": 0
                },
                {
                    "sent": "In other words, the places where the derivative is zero that we're going to come close to are going to be saddle points.",
                    "label": 0
                },
                {
                    "sent": "So what is the saddle point?",
                    "label": 0
                },
                {
                    "sent": "They're not going to be local minima, so we not going to be trapped in local minima.",
                    "label": 0
                },
                {
                    "sent": "We might have trouble escaping saddle points, but we can always escape saddle point.",
                    "label": 0
                },
                {
                    "sent": "So what is the saddle point?",
                    "label": 0
                },
                {
                    "sent": "So there's a picture here.",
                    "label": 0
                },
                {
                    "sent": "In in, again in 2D, but in general.",
                    "label": 0
                },
                {
                    "sent": "So what is the critical point is where the derivative is 0, so the function that is flat in some place.",
                    "label": 0
                },
                {
                    "sent": "And then well.",
                    "label": 0
                },
                {
                    "sent": "It has to go up or down in different directions, so here you see it's going up in these directions, but it's going down in this directions.",
                    "label": 0
                },
                {
                    "sent": "And that's a saddle point where there are directions where it's going up, and directions where it's going down.",
                    "label": 0
                },
                {
                    "sent": "A local minimum or global minimum has all directions going up.",
                    "label": 0
                },
                {
                    "sent": "A local Maxima has all the directions going down right?",
                    "label": 0
                },
                {
                    "sent": "You can't.",
                    "label": 0
                },
                {
                    "sent": "You can't go higher locally and same thing for the local minimum.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So what's going on here is that intuitively?",
                    "label": 0
                },
                {
                    "sent": "In a very high dimensional space, so the space of parameters here.",
                    "label": 0
                },
                {
                    "sent": "If you think about a saddle point.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "The the number of directions where it would have to go up to become a local minimum would be with the other dimensions, right?",
                    "label": 0
                },
                {
                    "sent": "So if if somehow there's something like a kind of randomness in how these functions are constructed and you independently choose for each direction, whether at the critical point you're going up or down, well, you can see that it's going to be exponentially unlikely that.",
                    "label": 0
                },
                {
                    "sent": "All the directions go up, except if you are near the bottom.",
                    "label": 0
                },
                {
                    "sent": "Of your lens game.",
                    "label": 0
                },
                {
                    "sent": "In other words, you're near the global minimum, 'cause of course, when when you have a minimum that's near the global minimum, you can't go further down, so all the directions have to go up.",
                    "label": 0
                },
                {
                    "sent": "So what you that's kind of the intuition, and So what happens, really, is that you have local minima, but they are very close to the global minimum in terms of their objective function.",
                    "label": 1
                },
                {
                    "sent": "And there is some 30 crew results from statistical physics annand matrix theory that.",
                    "label": 0
                },
                {
                    "sent": "Suggest the following that for some families of functions that are fairly large like those.",
                    "label": 0
                },
                {
                    "sent": "That you get by Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "There is a tight relationship.",
                    "label": 0
                },
                {
                    "sent": "Concentration of probability between what's called the index of the critical points and the objective function and training error.",
                    "label": 0
                },
                {
                    "sent": "What is the index?",
                    "label": 0
                },
                {
                    "sent": "The index is the number of is a fraction of directions that.",
                    "label": 0
                },
                {
                    "sent": "Now going down so you have say at a minimum, all of the directions are going up, so you have 0% of the directions that are going down, and that's a local minimum.",
                    "label": 0
                },
                {
                    "sent": "So when the index is equal to 0 you have local minimum.",
                    "label": 0
                },
                {
                    "sent": "When the index is equal to 1, you have a local maximum.",
                    "label": 0
                },
                {
                    "sent": "Anything in between the fraction of directions that are going down.",
                    "label": 1
                },
                {
                    "sent": "Is something in between you have a saddle point so local minima are kind of special cases of saddle points where the index is 0.",
                    "label": 0
                },
                {
                    "sent": "OK, and So what this theory says is that the if you think of the joint distribution of index an objective function.",
                    "label": 0
                },
                {
                    "sent": "Distribution because we imagine a large distribution over functions over that we're trying to optimize, there's going to be a concentration of probability on some curve.",
                    "label": 0
                },
                {
                    "sent": "Meaning that.",
                    "label": 0
                },
                {
                    "sent": "For a particular training objective.",
                    "label": 0
                },
                {
                    "sent": "Most of the critical points are going to be subtle points with a particular proportion of particular index.",
                    "label": 0
                },
                {
                    "sent": "And we can, we can do empirical experiments to verify that we.",
                    "label": 0
                },
                {
                    "sent": "So that's what we did, and we find that indeed you have a kind of tight.",
                    "label": 0
                },
                {
                    "sent": "Relationship between index an objective function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right, we don't so these.",
                    "label": 0
                },
                {
                    "sent": "You can always imagine constructing a an objective function.",
                    "label": 0
                },
                {
                    "sent": "That is hard that has bad local minima in the middle of, you know high error.",
                    "label": 0
                },
                {
                    "sent": "So that's you know you can.",
                    "label": 0
                },
                {
                    "sent": "You can build by hand very easily.",
                    "label": 0
                },
                {
                    "sent": "You can place the local minima in bad places.",
                    "label": 0
                },
                {
                    "sent": "So the question is whether these families of random functions that can be studied theoretically like this are representative of what we have with neural Nets and we don't know the answer to that.",
                    "label": 0
                },
                {
                    "sent": "But we've done some experiments like this experiment is on real neural Nets where what we do is we launch.",
                    "label": 0
                },
                {
                    "sent": "A bunch of experiments where we train an along the way you know every once in awhile we look using very expensive methods for the nearby critical points, and then we use like Newton's method to find the index.",
                    "label": 0
                },
                {
                    "sent": "Like how many directions does it go up, compute the eigenvalues of that has seen at that location.",
                    "label": 0
                },
                {
                    "sent": "So it's it's only an empirical validation, and we probably would like to have more such experiments to verify that that statement is true, so that's why I'm saying we have no proof that this result applies to the kinds of neural Nets we want to optimize, but we have some evidence that the behavior we're getting corresponds with the theory suggests.",
                    "label": 0
                },
                {
                    "sent": "And of course the theory makes assumptions which we don't know are true.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Anything else?",
                    "label": 0
                },
                {
                    "sent": "Maybe, maybe not.",
                    "label": 0
                },
                {
                    "sent": "So, even though we may not get stuck in local minima, we might get stuck in regions that are or near regions that are flat or some other complicated shape.",
                    "label": 0
                },
                {
                    "sent": "Of the objective function which makes.",
                    "label": 0
                },
                {
                    "sent": "Say stochastic gradient look like it's stuck.",
                    "label": 0
                },
                {
                    "sent": "In theory if you wait long enough.",
                    "label": 0
                },
                {
                    "sent": "Sgt will escape any any someone 'cause there's no local minimal SG will escape.",
                    "label": 0
                },
                {
                    "sent": "But it you know it might take so long that you won't see it, and we have lots of observations.",
                    "label": 0
                },
                {
                    "sent": "So let me show you in the net.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slide so we have lots of observations where we see things and people have played with you all night, sometimes see this where you see the error go down and then it seems to plateau and maybe if you're lucky something happens and then it goes down and then it plateaus.",
                    "label": 0
                },
                {
                    "sent": "But let's say we had stopped the experiment here.",
                    "label": 0
                },
                {
                    "sent": "We might think that this is the best we can get.",
                    "label": 0
                },
                {
                    "sent": "Actually, something else is going on.",
                    "label": 0
                },
                {
                    "sent": "It looks like we are approaching the region of a saddle point.",
                    "label": 1
                },
                {
                    "sent": "The gradients are not approaching zero.",
                    "label": 0
                },
                {
                    "sent": "We don't understand why, but but there's no progress of the objective function.",
                    "label": 0
                },
                {
                    "sent": "This kind of bouncing around not finding the escape route.",
                    "label": 0
                },
                {
                    "sent": "My intuition is not this is such a high dimensional space that there's only a few dimensions where it's going down and somehow it's not finding them.",
                    "label": 0
                },
                {
                    "sent": "Maybe because of curvature problems or all the things but.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying is, yes, it's worthwhile to continue exploring optimization methods beyond simple gradient descent.",
                    "label": 0
                },
                {
                    "sent": "But maybe we need to take into account the fact that what we are finding is not local minima.",
                    "label": 0
                },
                {
                    "sent": "It might be something else.",
                    "label": 0
                },
                {
                    "sent": "It might be something classical like differences, curvature or something more subtle.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we can measure those.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "An in fact what you find is that as the as you go lower down.",
                    "label": 0
                },
                {
                    "sent": "You see here in this picture it spends more and more time on these plateaus, right?",
                    "label": 0
                },
                {
                    "sent": "They get harder and harder, presumably because there are less directions going down.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I don't.",
                    "label": 0
                },
                {
                    "sent": "Stopping criterion is how much do you want to spend in time, right?",
                    "label": 0
                },
                {
                    "sent": "So I remember.",
                    "label": 0
                },
                {
                    "sent": "People are getting lazy these days.",
                    "label": 0
                },
                {
                    "sent": "They'd like to have the experiments finished in two days or something, or.",
                    "label": 0
                },
                {
                    "sent": "When I was a PC.",
                    "label": 0
                },
                {
                    "sent": "We were willing to wait like for weeks.",
                    "label": 0
                },
                {
                    "sent": "And that was not enough.",
                    "label": 0
                },
                {
                    "sent": "I mean, in retrospect, if we had known.",
                    "label": 0
                },
                {
                    "sent": "We should have waited for like 2 years and then.",
                    "label": 0
                },
                {
                    "sent": "We might actually have accelerated the progress of research in neural Nets because.",
                    "label": 0
                },
                {
                    "sent": "People discarded these things in part because we didn't train them long enough to get the kinds of results we are able to get now with GPU's and much faster hardware, yes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I believe it's an old 1/4 task with either letters or M lists, so it's it's that's in the NIPS 2014 paper.",
                    "label": 0
                },
                {
                    "sent": "First author is yondo fast, so you can find all that information there.",
                    "label": 0
                },
                {
                    "sent": "A yes or.",
                    "label": 0
                },
                {
                    "sent": "It's very expensive to actually compute that.",
                    "label": 0
                },
                {
                    "sent": "So it's not something practical, right?",
                    "label": 0
                },
                {
                    "sent": "So we can do.",
                    "label": 0
                },
                {
                    "sent": "Long experiments where we measure these things, but you know.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just use the last few minutes quickly to tell you about other priors that come in too many deep learning approaches, especially some that use unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "But you know a lot of that will have to wait for either my lecture at the end of the summer school, or other lectures that will talk about these concepts.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But to get things to get you motivated.",
                    "label": 0
                },
                {
                    "sent": "One interesting observation is that humans.",
                    "label": 0
                },
                {
                    "sent": "And especially specially striking with children.",
                    "label": 0
                },
                {
                    "sent": "Are able to learn a new task like.",
                    "label": 0
                },
                {
                    "sent": "The category of and a new object from very few examples, sometimes just a handful, sometimes one example.",
                    "label": 1
                },
                {
                    "sent": "If you think I did from a statistical learning POV, it doesn't make any sense, right?",
                    "label": 0
                },
                {
                    "sent": "We can't possibly have meaningful generalization even using a deep net with a single example.",
                    "label": 0
                },
                {
                    "sent": "The only way this can happen is because the child is using accumulated knowledge from previous learning.",
                    "label": 1
                },
                {
                    "sent": "And that knowledge could be, you know, used to build representations.",
                    "label": 0
                },
                {
                    "sent": "Such that in the new representation space, things are very very easy, like a single, you know.",
                    "label": 0
                },
                {
                    "sent": "Prototype gives you buys.",
                    "label": 0
                },
                {
                    "sent": "You essentially know the characterization of this new category.",
                    "label": 0
                },
                {
                    "sent": "One other way to think about it, which is even more interesting, is that if you've built a mental model of the world that basically understands how the world works.",
                    "label": 0
                },
                {
                    "sent": "And now you give me a new category, say.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of blah blah blah because you understand how the world works.",
                    "label": 0
                },
                {
                    "sent": "It's obvious what blah blah blah means, right?",
                    "label": 0
                },
                {
                    "sent": "And now you can generalize to two other instances.",
                    "label": 0
                },
                {
                    "sent": "So you know an example of blah blah blah could be, oh, I'm going to teach you a new game.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of how we play it.",
                    "label": 0
                },
                {
                    "sent": "We do that all the time.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So for this to work, you need more.",
                    "label": 0
                },
                {
                    "sent": "Set statistical assumptions.",
                    "label": 0
                },
                {
                    "sent": "That basically the way the world works now in for this.",
                    "label": 0
                },
                {
                    "sent": "For these new examples is the same as the way the world worked for the other tasks that I saw before.",
                    "label": 0
                },
                {
                    "sent": "Or maybe the unsupervised learning I did from all of the examples I've seen before.",
                    "label": 0
                },
                {
                    "sent": "So we need to introduce more priors beyond the the ones that we talked about with this representations in depth, for example.",
                    "label": 0
                },
                {
                    "sent": "We we would like to use the following prior that the underlying explanations for the X is that we see now in the context of having to predict the Y are the same same as those we know there are behind the data we've seen before, from which maybe we've learned by unsupervised learning to characterize the distribution of the past data.",
                    "label": 0
                },
                {
                    "sent": "So if the same factors explain.",
                    "label": 0
                },
                {
                    "sent": "X.",
                    "label": 0
                },
                {
                    "sent": "When we see it alone and the relationship between X&Y in supervised learning, then we're in business, then we can hope to actually get very fast generalization.",
                    "label": 0
                },
                {
                    "sent": "This is called transfer learning.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well between task or between a supervised learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so so I guess many of you I've heard about the idea of semi supervised learning.",
                    "label": 1
                },
                {
                    "sent": "In purely supervised learning, we just using the labeled examples an here maybe is what we do, but if somebody gives us these other unlabeled examples, we would probably choose something like this linear classifier.",
                    "label": 0
                },
                {
                    "sent": "So we need to discover something about the input distribution in order to do that.",
                    "label": 0
                },
                {
                    "sent": "Other exam.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have sharing that's very very common is multi task learning, so the most common cases say we have images as input and there are different questions you'd like to ask about these images and it may be that the lower levels of deep net can extract features that are useful for many of these tasks.",
                    "label": 0
                },
                {
                    "sent": "If there is this kind of sharing of underlying factors.",
                    "label": 0
                },
                {
                    "sent": "That is applicable to many of these tasks.",
                    "label": 0
                },
                {
                    "sent": "In other words, there is some underlying factors and that explain the variations in X and explain some of the tasks and different tasks.",
                    "label": 0
                },
                {
                    "sent": "Basically, maybe look at a different subset of these factors in different ways.",
                    "label": 0
                },
                {
                    "sent": "Then we can gain a lot by by training our models with this in mind.",
                    "label": 0
                },
                {
                    "sent": "For example, we could use the same neural net except for the upper layers that are task specific, so that's the most common setup.",
                    "label": 0
                },
                {
                    "sent": "You could also have some kind of multimo.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model approaches where you say OK, I'm going to have representations for different types, so this is actually what is used in Google Image search that was built by my brother Sammy at Google and by collaborators where.",
                    "label": 0
                },
                {
                    "sent": "We're going to imagine that there is a representation for images and the representation for queries that go in the same space.",
                    "label": 1
                },
                {
                    "sent": "And by learning these two.",
                    "label": 0
                },
                {
                    "sent": "Representations together we can generalize in pretty amazing ways.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More generally, you can imagine that you have, say different modalities like X&Y.",
                    "label": 0
                },
                {
                    "sent": "We learn representation for each modality as well as the kind of translation between them.",
                    "label": 0
                },
                {
                    "sent": "That tells us how to go from one to the other.",
                    "label": 0
                },
                {
                    "sent": "If we write, try to learn this directly in X&Y, it's hard, but if we do it in the right space things become very easy.",
                    "label": 0
                },
                {
                    "sent": "That's the hope another.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar cases well like speaker adaptation or write adaptation or other kinds of cases where the sharing is near the input, not the output.",
                    "label": 0
                },
                {
                    "sent": "Maybe we want to do speech recognition and we want to have a different kind of transformation.",
                    "label": 0
                },
                {
                    "sent": "Maybe depends on depending on the noise depending on the person who's speaking so we can share the upper levels here.",
                    "label": 0
                },
                {
                    "sent": "So there are many places where you could share.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You see, there is one idea that would like to end with which is connected to many things I've been saying and to some work done by some of the leaders in kernel machines that I've been working on causality recently.",
                    "label": 0
                },
                {
                    "sent": "Burnout shop cough and his group.",
                    "label": 0
                },
                {
                    "sent": "That I think give kind of intuition as to why.",
                    "label": 0
                },
                {
                    "sent": "Wise learning could be helpful.",
                    "label": 0
                },
                {
                    "sent": "The idea is the following.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Imagine we have our input sacks an the labels why they were trying to predict and now think about the related the causal relationship that could exist between these images and labels.",
                    "label": 0
                },
                {
                    "sent": "Well, there are two basic possibilities.",
                    "label": 0
                },
                {
                    "sent": "One is that X is a cause of Y or X is an effect of Y, right?",
                    "label": 0
                },
                {
                    "sent": "So which is the cause?",
                    "label": 0
                },
                {
                    "sent": "Which is the effect?",
                    "label": 0
                },
                {
                    "sent": "Of course it could be complicated more complicated than that, but let's imagine just these two cases.",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out that if.",
                    "label": 0
                },
                {
                    "sent": "If Y is one of the causes of X, then doing unsupervised learning is going to be very, very helpful.",
                    "label": 1
                },
                {
                    "sent": "But if it's the other way around, that why is an effect of X&X is the cause then that provides learning is useless for the purpose of semi supervised learning, right?",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "It turns out that in the in most of the eye tasks, the wise are things like words that we like to talk about the world and the X is are things we measure in the world and our brain is trying to make sense of the world.",
                    "label": 0
                },
                {
                    "sent": "So we created these labels.",
                    "label": 0
                },
                {
                    "sent": "These concepts that we talk about that are our best ways of characterizing the things we observe, an that probably means essentially a way to talk about the causes of what we observe.",
                    "label": 0
                },
                {
                    "sent": "And so it works.",
                    "label": 0
                },
                {
                    "sent": "It works because in the real world the things we want to predict.",
                    "label": 0
                },
                {
                    "sent": "Are the things that humans like to predict those things that human talk about, and then it means that there is a right kind of causal relationship between the X is in the wise.",
                    "label": 0
                },
                {
                    "sent": "The effect of that causal relationship is that if you look at P of Y given X, then the right way of modeling it, that will the one that will have the greater statistical power is basically involves capturing P of X given YNP of Y. Anne Anne because P of X through Bayes rule is just the sum over at over Y of these things, the right P of X.",
                    "label": 0
                },
                {
                    "sent": "The one that will work the best is is the one that.",
                    "label": 0
                },
                {
                    "sent": "Really involves introducing the wise as an explanation, so let me give you a very simple.",
                    "label": 0
                },
                {
                    "sent": "Very simple example of that.",
                    "label": 0
                },
                {
                    "sent": "If I can get this to work and yes.",
                    "label": 0
                },
                {
                    "sent": "And yes, so imagine that the world.",
                    "label": 0
                },
                {
                    "sent": "Is very very simple.",
                    "label": 0
                },
                {
                    "sent": "It's just a mixture of two Gaussians, but so this is X.",
                    "label": 0
                },
                {
                    "sent": "And this is P of X.",
                    "label": 0
                },
                {
                    "sent": "And and this is y = 0 and this is y = 1.",
                    "label": 0
                },
                {
                    "sent": "But you don't know.",
                    "label": 0
                },
                {
                    "sent": "However you do see the data you see the X and you see that there are two natural classes.",
                    "label": 0
                },
                {
                    "sent": "And because the data are really was generated with, why is one of the causes the way you think the data was generated?",
                    "label": 0
                },
                {
                    "sent": "Here is somebody first picked the Y and then given that why you have a Gaussian distribution?",
                    "label": 0
                },
                {
                    "sent": "That's the right way of modeling that data.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, in the real world, the relationship between X&Y is much more complicated, but you know it tells us that even if we don't observe the Y, if we model it the proper way, we're going to see emerge as a natural random variable that explains it.",
                    "label": 0
                },
                {
                    "sent": "These these these labels, if it turns out that these labels are the kinds of things we want to predict at the end of the day, then we're really in business, so that's what's going on alright.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}