{
    "id": "uh6vxvovir7lj6jt7tzinju6ribry25r",
    "title": "Unsupervised learning of probabilistic context-free grammar using iterative biclustering",
    "info": {
        "author": [
            "Kewei Tu, Iowa State University"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/icgi08_tu_ulpcg/",
    "segmentation": [
        [
            "OK, so the final talk of this session is entitled unsupervised learning of probabilistic context free grammars using iterative biclustering as being presented by Karen too."
        ],
        [
            "OK, so.",
            "In this presentation I will talk about unsupervised algorithm for running probabilistic context free grammar.",
            "And we use greedy search trying to maximize the posterior of the grammar given the training corpus.",
            "And this is quite similar to some of the previous work and the main difference is that we use iterative distribution of biclustering to do the search.",
            "And we got a computer too experimental result."
        ],
        [
            "It's.",
            "Set an outline first.",
            "I will give an introduction of the background and problem definition and then some discussion over the CFG representation.",
            "And the algorithm will then be discussed and then I will present the."
        ],
        [
            "Experimental results.",
            "So as we know, our PFG is widely used in areas like natural language processing and by medics, so it is important to learn the CFG from data.",
            "But in most cases we don't have labeled corpus, we only have a raw corpus, so we have to use.",
            "We have to use unsupervised learning."
        ],
        [
            "And unsupervised learning of PCF means that we have a corpus of correct sentences and we want to learn the grammar, the underlying grammar.",
            "From this corpus alone."
        ],
        [
            "So as we know, context free grammar contains 4 parts, then terminals, terminals and grammar rules and start symbol.",
            "And appears the FG is just CFG plus probabilities on grammar rules."
        ],
        [
            "And, uh.",
            "Any PC can be converted into a probabilistic Chomsky normal form, which contains two types of rules."
        ],
        [
            "And we go one step further.",
            "We represent a problem probabilistic CNF grammar.",
            "Into an end or form, and we do this just to make it easier to explain our algorithm.",
            "So in this and or form we have two kinds of symbols, so.",
            "And symbols and or symbols.",
            "And and symbol can produce a sequence of two or symbols and or symbol can produce a set of.",
            "Either end symbols or terminals.",
            "Soul."
        ],
        [
            "Let's see a simple example.",
            "So on the left we have.",
            "A CNF grammar and on the right we have the corresponding and awful, and the conversion is actually quite straightforward.",
            "So or.",
            "Each non terminals in the Chomsky normal form we have.",
            "And or symbol in this and or form.",
            "And for each combination of two nonterminals.",
            "We have an.",
            "And symbol in Oracle.",
            "So."
        ],
        [
            "And in this and all form.",
            "The grammar can be divided into 2 parts.",
            "The first part is a set of standard rules, so I started rule is just a rule with the status symbol on the left hand side.",
            "And the second part is a set of an or groups.",
            "So.",
            "Each end of group contains exactly 1 end symbol and two or symbols such that this end symbol can produce the sequence of the two or symbols.",
            "And there is a bijection between.",
            "And symbol and the end of groups.",
            "So in other words, for each end symbol we have one in our group.",
            "On the other hand, for all symbols and all symbol may appear in multiple groups."
        ],
        [
            "Let's see our examples so.",
            "In this and or form we have one or group which is highlighted by this blue box.",
            "So again, we do this just to simplify the explination of our algorithm."
        ],
        [
            "And now we can go into the."
        ],
        [
            "Details of the algorithm.",
            "So we start from only terminals.",
            "And we repeat the two steps to add.",
            "Grammar rules into this grammar.",
            "So the first step is.",
            "Try to learn a new and or group by doing by clustering.",
            "And second step is try to.",
            "So we learn a new end symbol in the first step.",
            "Then we try to find some rules that can attach this new end symbol to existing or symbols.",
            "I will discuss these two steps later, so this second second step is an additional step.",
            "Try to find rules that can be found in the first step alone and we repeat the two steps to grow the grammar until no further rule can be found.",
            "Then we go into the post processing.",
            "To add as a starter rules.",
            "Anne, as we mentioned before.",
            "Any CNF grammar in this and or form can be divided into 2 parts, so either a set of end or Group A set of animal groups and a self starter rules.",
            "So these steps in principle are capable of constructing any CNF grammar."
        ],
        [
            "And from a patient perspective of you, we actually try to find the rules that yield the greatest increase of the posterior of the grammar given the training corpus.",
            "So in other words, we.",
            "Try to.",
            "We do local search with the posterior as the objective function.",
            "And to a defined this posterior we use appriver that favors simple small grammars.",
            "To avoid overfitting."
        ],
        [
            "So now let's look at the first step, which try to.",
            "Learn a new endo group."
        ],
        [
            "By doing biclustering.",
            "So here is the intuition.",
            "So we construct the table.",
            "So each row and each column.",
            "Represent a symbol.",
            "That appeared in the corpus.",
            "And the cell at row X and column Y is the number of times the pair XY appears in the corpus.",
            "So.",
            "For example, is 24 mean that?",
            "This pair the circle.",
            "Appears 24 times in the corpus.",
            "And by the way, this table is just a part of the whole table.",
            "And also I only show the all the nonzero cells."
        ],
        [
            "And we can see that.",
            "Oh, another group actually corresponds to a bike last.",
            "Which has a submatrix in this table.",
            "So let's see this example.",
            "If we have this and or group, well, the end symbol represents noun phrase.",
            "Which produce.",
            "The Terminator and known and the Terminator could be either Lee or a Anon could be circle, triangle or square.",
            "And it corresponds to this by cluster.",
            "So the set of rows.",
            "Correspond to the first or symbol, a set of columns corresponds to the second or symbol.",
            "Note that the road and column don't have to be adjacent.",
            "And I put some ideas and just to make this bicluster more clear."
        ],
        [
            "And we can prove that this bicluster should be multiplicatively coherent, which means that for any two rows and any two columns in this bicluster.",
            "We have this equation.",
            "So to make it more clear, see this example.",
            "So if we take the two rows of DNA and two columns of circle and triangle, we will see that this equation holds.",
            "And the reason for that is.",
            "Remember that in this and or group we have two or symbols.",
            "An each or symbol.",
            "We have a multinomial distribution defined which specifies the probability of each symbol being chosen.",
            "And we have two or symbols, so we have two multinomial distributions which are independent.",
            "So.",
            "We can either prove this."
        ],
        [
            "An further we define an expression context matrix of this bicluster, so each row.",
            "Represents a single player in this podcast.",
            "And each column represents the context in which the single parent appear in the corpus, so.",
            "For example, this one.",
            "Means that we have one sentence in the corpus which is something covers a circle.",
            "And we can prove that this expression context matrix is also multiplicatively coherent.",
            "So for any two rows and column we have that equation.",
            "And the reason is, well, the grammar is context free.",
            "So the context and expression should be independent.",
            "And then it's easy.",
            "It's easy to prove this coherence."
        ],
        [
            "So based on these properties, we.",
            "Help this intuitive approach that if we can find in this table about cluster that is multiplicatively coherent.",
            "And also it had a multiplicative coherent expression context matrix.",
            "Then we can learn and and or group.",
            "From this podcast and add it into the grammar.",
            "This is the intuition."
        ],
        [
            "And this intuition can be justified by some probabilistic analysis.",
            "So we ask the question.",
            "How is the likelihood changed at the result of adding?",
            "Of learning and end of rope from a bicluster.",
            "So the likelihood of the corpus given the grammar.",
            "And here is what we get.",
            "So this.",
            "LGBT means the likelihood gain of learning from a bicluster BC.",
            "And this PR is rule the probability of the grammar rules.",
            "So this actually is the likelihood gain.",
            "Given the optimal.",
            "Grammar drop of probabilities and in this formula we have two parts.",
            "And we can see that the first part actually measures the multiplicative coherence of the bike last.",
            "So in other words.",
            "If this bicluster is coherent, then we this part will have large value.",
            "And the second part actually measures the multiplicative coherence of the expression context matrix.",
            "So.",
            "This means if we want to maximize the likelihood gain.",
            "We'd like to.",
            "See that bicluster is coherent and expression.",
            "Context matrix it also coherent, so this somehow verified our intuition.",
            "But as we know, if we only maximize the likelihood, it's very likely to."
        ],
        [
            "Overfit.",
            "So we also consider we also use a prior to prevent overfitting.",
            "So this problem is 2 to the power of the negative description length of the grammar.",
            "Which favors small and compact grammar."
        ],
        [
            "So overall we try to find in this table by cluster that.",
            "Leads to the maximum posterior gain.",
            "And then we learned a new and or group from this back last.",
            "An app that we reduce corpus using the new rules.",
            "So for example, if we learn the bicluster which is shown in our example, we should replace all the appearance of the circle by the end symbol that we learned.",
            "And then we update the table and becausw we.",
            "We add new end symbol and you end symbol into grammar.",
            "We have to add a new role and a new column to table so the table is updated."
        ],
        [
            "So is there the 1st?"
        ],
        [
            "Yep, and now we can go into the second step.",
            "Which attach the attach the end symbol that we learned in the first step on the existing or so for that new end symbol, learning the first step.",
            "There might be some more symbols in the target in the learn grammar.",
            "Such that let's say, oh is such an old symbol.",
            "Then all produce N getting attached grammar.",
            "And we want to learn this rule.",
            "But this kind of rule can't be learned by the first step alone.",
            "And this is the cause when we learn.",
            "The OR symbol.",
            "Oh we haven't.",
            "We haven't learned.",
            "The end simple end.",
            "And when we learn the.",
            "And symbol N we only learn this rule and produces AB.",
            "So in this step we want to learn such kind of rules.",
            "And notice that recursion is learned in this step, cause by the first step alone we actually.",
            "Establish a partial order among the symbols.",
            "So it is in this step that we can add rules that to form cycles which recursion."
        ],
        [
            "Can a pure I will only give a brief introduction of this second step.",
            "So again, intuition first, remember that the OR symbol in the grammar is learned by.",
            "By learning from a back last.",
            "So this all symbol corresponds to either a set of the set of rows or the set of columns of the bicluster.",
            "So add a new rule or produce an equivalent to add a adding a new role or a new column to the bicluster so we get an expanded by cluster.",
            "And if this rule all produces an is in the target grammar.",
            "Then we should expect that the expanded bicluster.",
            "It multiplicatively coherent, and it has a multiplicatively coherent expression context matrix.",
            "Based on the same reason as in the first step.",
            "So the intuitive approach would be if we can find another symbol.",
            "Such that the expanded by class has the these two properties.",
            "Then we can add a new, add a new rule, or produce an Instagram."
        ],
        [
            "Then again, we can justify this intuition by some probabilistic analysis, so the likelihood gain.",
            "Of adding such a new rule is approximately the likelihood gain of learning from.",
            "From an approximation of the expanded bicluster.",
            "So in other words, if we want to.",
            "Maximize.",
            "The likelihood gain.",
            "We would like to see an expanded bicluster which is coherent and has coherent expression context matrix that we discussed in the first step.",
            "And again, to prevent overfitting, we have to also consider prior.",
            "So this."
        ],
        [
            "Get the second step.",
            "We try to find or symbols that lead to large posterior gain as defined in the previous slide and if we can find such a.",
            "Or symbol.",
            "We add a new rule of produce and to the grammar, and we do a maximum reduction of the corpus and we update the table T."
        ],
        [
            "So we repeat the first 2 steps too.",
            "Add and or groups into the grammar.",
            "Until that until we can no longer learn any new rule.",
            "So which means we can no longer find any biclustering the table.",
            "Then we go into the post processing to add a start rules.",
            "So in post processing, we go over the training corpus to see for each sentence if it has been fully reduced into a single symbol.",
            "Next then we just add a start rule as produces X.",
            "On the other hand, if we find a sentence that hasn't been fully reduced, we have a few options.",
            "So we can either.",
            "Ignore it.",
            "Or we can try to parse it again, trying to fully reduce it, or we can add some ad hoc grammar rules in Telegram.",
            "And after that we return the grammar."
        ],
        [
            "So now I will present some experimental results.",
            "We we evaluate the weak generative capacity of the learn grammar.",
            "So we use the measurements of precision, recall and F measure.",
            "I mean, we compare the language of the touch Graham, who is the language of the learn grammar.",
            "And we use a set of.",
            "Artificial English like context free grammars."
        ],
        [
            "And here is the result.",
            "This part is.",
            "The results of our algorithm and we compare it with compared with two other methods.",
            "Me Renee Diaz.",
            "European precision and RN recall and F as F score and the number in the princesses.",
            "The standard deviation.",
            "And we can see that our algorithm outperforms me.",
            "Renee Diaz and also.",
            "It has lower standard deviations, which means it might be more stable."
        ],
        [
            "So in summary.",
            "I presented an answer file the CFG learning algorithm.",
            "Which.",
            "Learn new grammar rules by doing iterative by clustering on table of simple pairs.",
            "Anne.",
            "Each step in each step, the algorithm actually tries to.",
            "Maximize the increase of the posterior of the grammar given the training corpus and we got competitive experimental results."
        ],
        [
            "Some work in progress.",
            "We are.",
            "Trying to find some alternative strategies so instead of greedy search.",
            "Trying to some trying to find some other strategies to optimize the objective function.",
            "And also we are working on the evaluation.",
            "And adaptation of our algorithm to some real world applications like natural language grammars.",
            "With respect to both weak and strong generative path."
        ],
        [
            "So that's all, thank you.",
            "I'm still trying to get my head around the idea of this multiplicative coherency.",
            "So what happens if you have words that are ambiguous saying for speech?",
            "If you want to call it so first level up if they're ambiguous in that, that means that they're actually not.",
            "They don't have to be multiple collectively.",
            "Yeah, in that case you will find well.",
            "Maybe to buy glasses which is overlapping.",
            "So the overlapped part is an ambiguous part, and.",
            "What?",
            "Algorithm now.",
            "Combined account account in two parts right so?",
            "Well.",
            "The formulation in our algorithm doesn't consider the bigger it, so if there are significant ambiguity in the corpus then it may not work well so.",
            "I'm really curious how.",
            "Yeah yeah, yeah.",
            "I think as possible to improve the bicluster algorithm so it can try to find some.",
            "I mean try to find the case of overlapping.",
            "So it's possible, but it's totally complex.",
            "What do you do if you don't find a good oral?",
            "Let go.",
            "Connect into I mean.",
            "To the first step again.",
            "I mean as possible.",
            "It's impossible to find the same handle in a laser.",
            "The same Andrew.",
            "Ha."
        ],
        [
            "So.",
            "What do you mean by finding the same animal?",
            "Possible.",
            "No.",
            "Well.",
            "We are not looking in the step.",
            "We're not looking at the end rule that we found in the first step.",
            "We actually try to.",
            "Find some additional rules.",
            "We chat in the form of this.",
            "This will produce an.",
            "So the end is the end symbol that is learning the first step and all.",
            "Is some or symbol that is learned in previous.",
            "Loops.",
            "So we are not looking at the end rule that they learn.",
            "Instead we try to find some additional rules that can be learned in first step.",
            "Does that answer question?",
            "And then you try to write.",
            "So you you already learned some morals.",
            "Now you've learned a new Android.",
            "You fit into an already existing order.",
            "But what if it does not exist?",
            "Let's see an example.",
            "I think I have a slide so."
        ],
        [
            "So.",
            "This is the end of group.",
            "OK, so let's say in the first step we learn a new end symbol and.",
            "And then.",
            "In the target grammar, we have this rule.",
            "Or to produce it and produce end.",
            "And this rule, as I mentioned in the previous slide, can be learned by the first step.",
            "So in this second step we are trying to find this rule.",
            "Well, we got up to.",
            "So it seems to me that the from a statistical point of view, this criterion you're trying to optimize is.",
            "Map estimator, so it's it's a combination of the likelihood, an end of prior, so I'm not sure that the the competitive approach that you've been comparing with are also optimizing these criteria.",
            "No they are.",
            "I don't know the last one, but they mean.",
            "So I think it might be worth to actually compare with, I think approach.",
            "Which are after optimizing the same criterion.",
            "I'm thinking you're talking about the work of stalking.",
            "Just like 40 years ago.",
            "And where he had actually tackled the problem by Asian running of PCG, which is exactly what you would like your attacking, so right.",
            "Yeah.",
            "OK, I think that's all we have time for, so let's thank the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the final talk of this session is entitled unsupervised learning of probabilistic context free grammars using iterative biclustering as being presented by Karen too.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "In this presentation I will talk about unsupervised algorithm for running probabilistic context free grammar.",
                    "label": 0
                },
                {
                    "sent": "And we use greedy search trying to maximize the posterior of the grammar given the training corpus.",
                    "label": 1
                },
                {
                    "sent": "And this is quite similar to some of the previous work and the main difference is that we use iterative distribution of biclustering to do the search.",
                    "label": 0
                },
                {
                    "sent": "And we got a computer too experimental result.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Set an outline first.",
                    "label": 0
                },
                {
                    "sent": "I will give an introduction of the background and problem definition and then some discussion over the CFG representation.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm will then be discussed and then I will present the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experimental results.",
                    "label": 0
                },
                {
                    "sent": "So as we know, our PFG is widely used in areas like natural language processing and by medics, so it is important to learn the CFG from data.",
                    "label": 1
                },
                {
                    "sent": "But in most cases we don't have labeled corpus, we only have a raw corpus, so we have to use.",
                    "label": 1
                },
                {
                    "sent": "We have to use unsupervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And unsupervised learning of PCF means that we have a corpus of correct sentences and we want to learn the grammar, the underlying grammar.",
                    "label": 0
                },
                {
                    "sent": "From this corpus alone.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as we know, context free grammar contains 4 parts, then terminals, terminals and grammar rules and start symbol.",
                    "label": 0
                },
                {
                    "sent": "And appears the FG is just CFG plus probabilities on grammar rules.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And, uh.",
                    "label": 0
                },
                {
                    "sent": "Any PC can be converted into a probabilistic Chomsky normal form, which contains two types of rules.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we go one step further.",
                    "label": 0
                },
                {
                    "sent": "We represent a problem probabilistic CNF grammar.",
                    "label": 0
                },
                {
                    "sent": "Into an end or form, and we do this just to make it easier to explain our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So in this and or form we have two kinds of symbols, so.",
                    "label": 0
                },
                {
                    "sent": "And symbols and or symbols.",
                    "label": 0
                },
                {
                    "sent": "And and symbol can produce a sequence of two or symbols and or symbol can produce a set of.",
                    "label": 0
                },
                {
                    "sent": "Either end symbols or terminals.",
                    "label": 0
                },
                {
                    "sent": "Soul.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see a simple example.",
                    "label": 0
                },
                {
                    "sent": "So on the left we have.",
                    "label": 0
                },
                {
                    "sent": "A CNF grammar and on the right we have the corresponding and awful, and the conversion is actually quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "So or.",
                    "label": 0
                },
                {
                    "sent": "Each non terminals in the Chomsky normal form we have.",
                    "label": 0
                },
                {
                    "sent": "And or symbol in this and or form.",
                    "label": 0
                },
                {
                    "sent": "And for each combination of two nonterminals.",
                    "label": 0
                },
                {
                    "sent": "We have an.",
                    "label": 0
                },
                {
                    "sent": "And symbol in Oracle.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this and all form.",
                    "label": 0
                },
                {
                    "sent": "The grammar can be divided into 2 parts.",
                    "label": 1
                },
                {
                    "sent": "The first part is a set of standard rules, so I started rule is just a rule with the status symbol on the left hand side.",
                    "label": 1
                },
                {
                    "sent": "And the second part is a set of an or groups.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Each end of group contains exactly 1 end symbol and two or symbols such that this end symbol can produce the sequence of the two or symbols.",
                    "label": 0
                },
                {
                    "sent": "And there is a bijection between.",
                    "label": 0
                },
                {
                    "sent": "And symbol and the end of groups.",
                    "label": 0
                },
                {
                    "sent": "So in other words, for each end symbol we have one in our group.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, for all symbols and all symbol may appear in multiple groups.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see our examples so.",
                    "label": 0
                },
                {
                    "sent": "In this and or form we have one or group which is highlighted by this blue box.",
                    "label": 0
                },
                {
                    "sent": "So again, we do this just to simplify the explination of our algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we can go into the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Details of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we start from only terminals.",
                    "label": 0
                },
                {
                    "sent": "And we repeat the two steps to add.",
                    "label": 1
                },
                {
                    "sent": "Grammar rules into this grammar.",
                    "label": 0
                },
                {
                    "sent": "So the first step is.",
                    "label": 0
                },
                {
                    "sent": "Try to learn a new and or group by doing by clustering.",
                    "label": 1
                },
                {
                    "sent": "And second step is try to.",
                    "label": 0
                },
                {
                    "sent": "So we learn a new end symbol in the first step.",
                    "label": 0
                },
                {
                    "sent": "Then we try to find some rules that can attach this new end symbol to existing or symbols.",
                    "label": 0
                },
                {
                    "sent": "I will discuss these two steps later, so this second second step is an additional step.",
                    "label": 0
                },
                {
                    "sent": "Try to find rules that can be found in the first step alone and we repeat the two steps to grow the grammar until no further rule can be found.",
                    "label": 0
                },
                {
                    "sent": "Then we go into the post processing.",
                    "label": 0
                },
                {
                    "sent": "To add as a starter rules.",
                    "label": 0
                },
                {
                    "sent": "Anne, as we mentioned before.",
                    "label": 0
                },
                {
                    "sent": "Any CNF grammar in this and or form can be divided into 2 parts, so either a set of end or Group A set of animal groups and a self starter rules.",
                    "label": 1
                },
                {
                    "sent": "So these steps in principle are capable of constructing any CNF grammar.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And from a patient perspective of you, we actually try to find the rules that yield the greatest increase of the posterior of the grammar given the training corpus.",
                    "label": 1
                },
                {
                    "sent": "So in other words, we.",
                    "label": 0
                },
                {
                    "sent": "Try to.",
                    "label": 1
                },
                {
                    "sent": "We do local search with the posterior as the objective function.",
                    "label": 0
                },
                {
                    "sent": "And to a defined this posterior we use appriver that favors simple small grammars.",
                    "label": 0
                },
                {
                    "sent": "To avoid overfitting.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's look at the first step, which try to.",
                    "label": 0
                },
                {
                    "sent": "Learn a new endo group.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By doing biclustering.",
                    "label": 0
                },
                {
                    "sent": "So here is the intuition.",
                    "label": 0
                },
                {
                    "sent": "So we construct the table.",
                    "label": 0
                },
                {
                    "sent": "So each row and each column.",
                    "label": 0
                },
                {
                    "sent": "Represent a symbol.",
                    "label": 0
                },
                {
                    "sent": "That appeared in the corpus.",
                    "label": 0
                },
                {
                    "sent": "And the cell at row X and column Y is the number of times the pair XY appears in the corpus.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For example, is 24 mean that?",
                    "label": 0
                },
                {
                    "sent": "This pair the circle.",
                    "label": 0
                },
                {
                    "sent": "Appears 24 times in the corpus.",
                    "label": 0
                },
                {
                    "sent": "And by the way, this table is just a part of the whole table.",
                    "label": 0
                },
                {
                    "sent": "And also I only show the all the nonzero cells.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can see that.",
                    "label": 0
                },
                {
                    "sent": "Oh, another group actually corresponds to a bike last.",
                    "label": 1
                },
                {
                    "sent": "Which has a submatrix in this table.",
                    "label": 0
                },
                {
                    "sent": "So let's see this example.",
                    "label": 0
                },
                {
                    "sent": "If we have this and or group, well, the end symbol represents noun phrase.",
                    "label": 0
                },
                {
                    "sent": "Which produce.",
                    "label": 0
                },
                {
                    "sent": "The Terminator and known and the Terminator could be either Lee or a Anon could be circle, triangle or square.",
                    "label": 0
                },
                {
                    "sent": "And it corresponds to this by cluster.",
                    "label": 0
                },
                {
                    "sent": "So the set of rows.",
                    "label": 0
                },
                {
                    "sent": "Correspond to the first or symbol, a set of columns corresponds to the second or symbol.",
                    "label": 0
                },
                {
                    "sent": "Note that the road and column don't have to be adjacent.",
                    "label": 0
                },
                {
                    "sent": "And I put some ideas and just to make this bicluster more clear.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can prove that this bicluster should be multiplicatively coherent, which means that for any two rows and any two columns in this bicluster.",
                    "label": 1
                },
                {
                    "sent": "We have this equation.",
                    "label": 0
                },
                {
                    "sent": "So to make it more clear, see this example.",
                    "label": 0
                },
                {
                    "sent": "So if we take the two rows of DNA and two columns of circle and triangle, we will see that this equation holds.",
                    "label": 0
                },
                {
                    "sent": "And the reason for that is.",
                    "label": 0
                },
                {
                    "sent": "Remember that in this and or group we have two or symbols.",
                    "label": 0
                },
                {
                    "sent": "An each or symbol.",
                    "label": 0
                },
                {
                    "sent": "We have a multinomial distribution defined which specifies the probability of each symbol being chosen.",
                    "label": 0
                },
                {
                    "sent": "And we have two or symbols, so we have two multinomial distributions which are independent.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can either prove this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An further we define an expression context matrix of this bicluster, so each row.",
                    "label": 1
                },
                {
                    "sent": "Represents a single player in this podcast.",
                    "label": 0
                },
                {
                    "sent": "And each column represents the context in which the single parent appear in the corpus, so.",
                    "label": 1
                },
                {
                    "sent": "For example, this one.",
                    "label": 0
                },
                {
                    "sent": "Means that we have one sentence in the corpus which is something covers a circle.",
                    "label": 1
                },
                {
                    "sent": "And we can prove that this expression context matrix is also multiplicatively coherent.",
                    "label": 0
                },
                {
                    "sent": "So for any two rows and column we have that equation.",
                    "label": 0
                },
                {
                    "sent": "And the reason is, well, the grammar is context free.",
                    "label": 0
                },
                {
                    "sent": "So the context and expression should be independent.",
                    "label": 0
                },
                {
                    "sent": "And then it's easy.",
                    "label": 0
                },
                {
                    "sent": "It's easy to prove this coherence.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So based on these properties, we.",
                    "label": 0
                },
                {
                    "sent": "Help this intuitive approach that if we can find in this table about cluster that is multiplicatively coherent.",
                    "label": 1
                },
                {
                    "sent": "And also it had a multiplicative coherent expression context matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we can learn and and or group.",
                    "label": 0
                },
                {
                    "sent": "From this podcast and add it into the grammar.",
                    "label": 0
                },
                {
                    "sent": "This is the intuition.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this intuition can be justified by some probabilistic analysis.",
                    "label": 0
                },
                {
                    "sent": "So we ask the question.",
                    "label": 0
                },
                {
                    "sent": "How is the likelihood changed at the result of adding?",
                    "label": 1
                },
                {
                    "sent": "Of learning and end of rope from a bicluster.",
                    "label": 0
                },
                {
                    "sent": "So the likelihood of the corpus given the grammar.",
                    "label": 0
                },
                {
                    "sent": "And here is what we get.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "LGBT means the likelihood gain of learning from a bicluster BC.",
                    "label": 0
                },
                {
                    "sent": "And this PR is rule the probability of the grammar rules.",
                    "label": 0
                },
                {
                    "sent": "So this actually is the likelihood gain.",
                    "label": 0
                },
                {
                    "sent": "Given the optimal.",
                    "label": 1
                },
                {
                    "sent": "Grammar drop of probabilities and in this formula we have two parts.",
                    "label": 0
                },
                {
                    "sent": "And we can see that the first part actually measures the multiplicative coherence of the bike last.",
                    "label": 0
                },
                {
                    "sent": "So in other words.",
                    "label": 1
                },
                {
                    "sent": "If this bicluster is coherent, then we this part will have large value.",
                    "label": 0
                },
                {
                    "sent": "And the second part actually measures the multiplicative coherence of the expression context matrix.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This means if we want to maximize the likelihood gain.",
                    "label": 0
                },
                {
                    "sent": "We'd like to.",
                    "label": 0
                },
                {
                    "sent": "See that bicluster is coherent and expression.",
                    "label": 0
                },
                {
                    "sent": "Context matrix it also coherent, so this somehow verified our intuition.",
                    "label": 0
                },
                {
                    "sent": "But as we know, if we only maximize the likelihood, it's very likely to.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overfit.",
                    "label": 0
                },
                {
                    "sent": "So we also consider we also use a prior to prevent overfitting.",
                    "label": 1
                },
                {
                    "sent": "So this problem is 2 to the power of the negative description length of the grammar.",
                    "label": 0
                },
                {
                    "sent": "Which favors small and compact grammar.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So overall we try to find in this table by cluster that.",
                    "label": 0
                },
                {
                    "sent": "Leads to the maximum posterior gain.",
                    "label": 1
                },
                {
                    "sent": "And then we learned a new and or group from this back last.",
                    "label": 0
                },
                {
                    "sent": "An app that we reduce corpus using the new rules.",
                    "label": 1
                },
                {
                    "sent": "So for example, if we learn the bicluster which is shown in our example, we should replace all the appearance of the circle by the end symbol that we learned.",
                    "label": 1
                },
                {
                    "sent": "And then we update the table and becausw we.",
                    "label": 0
                },
                {
                    "sent": "We add new end symbol and you end symbol into grammar.",
                    "label": 0
                },
                {
                    "sent": "We have to add a new role and a new column to table so the table is updated.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So is there the 1st?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yep, and now we can go into the second step.",
                    "label": 0
                },
                {
                    "sent": "Which attach the attach the end symbol that we learned in the first step on the existing or so for that new end symbol, learning the first step.",
                    "label": 0
                },
                {
                    "sent": "There might be some more symbols in the target in the learn grammar.",
                    "label": 1
                },
                {
                    "sent": "Such that let's say, oh is such an old symbol.",
                    "label": 0
                },
                {
                    "sent": "Then all produce N getting attached grammar.",
                    "label": 0
                },
                {
                    "sent": "And we want to learn this rule.",
                    "label": 0
                },
                {
                    "sent": "But this kind of rule can't be learned by the first step alone.",
                    "label": 0
                },
                {
                    "sent": "And this is the cause when we learn.",
                    "label": 0
                },
                {
                    "sent": "The OR symbol.",
                    "label": 0
                },
                {
                    "sent": "Oh we haven't.",
                    "label": 0
                },
                {
                    "sent": "We haven't learned.",
                    "label": 0
                },
                {
                    "sent": "The end simple end.",
                    "label": 0
                },
                {
                    "sent": "And when we learn the.",
                    "label": 1
                },
                {
                    "sent": "And symbol N we only learn this rule and produces AB.",
                    "label": 0
                },
                {
                    "sent": "So in this step we want to learn such kind of rules.",
                    "label": 0
                },
                {
                    "sent": "And notice that recursion is learned in this step, cause by the first step alone we actually.",
                    "label": 1
                },
                {
                    "sent": "Establish a partial order among the symbols.",
                    "label": 0
                },
                {
                    "sent": "So it is in this step that we can add rules that to form cycles which recursion.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can a pure I will only give a brief introduction of this second step.",
                    "label": 0
                },
                {
                    "sent": "So again, intuition first, remember that the OR symbol in the grammar is learned by.",
                    "label": 1
                },
                {
                    "sent": "By learning from a back last.",
                    "label": 0
                },
                {
                    "sent": "So this all symbol corresponds to either a set of the set of rows or the set of columns of the bicluster.",
                    "label": 0
                },
                {
                    "sent": "So add a new rule or produce an equivalent to add a adding a new role or a new column to the bicluster so we get an expanded by cluster.",
                    "label": 1
                },
                {
                    "sent": "And if this rule all produces an is in the target grammar.",
                    "label": 1
                },
                {
                    "sent": "Then we should expect that the expanded bicluster.",
                    "label": 0
                },
                {
                    "sent": "It multiplicatively coherent, and it has a multiplicatively coherent expression context matrix.",
                    "label": 0
                },
                {
                    "sent": "Based on the same reason as in the first step.",
                    "label": 0
                },
                {
                    "sent": "So the intuitive approach would be if we can find another symbol.",
                    "label": 0
                },
                {
                    "sent": "Such that the expanded by class has the these two properties.",
                    "label": 0
                },
                {
                    "sent": "Then we can add a new, add a new rule, or produce an Instagram.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then again, we can justify this intuition by some probabilistic analysis, so the likelihood gain.",
                    "label": 0
                },
                {
                    "sent": "Of adding such a new rule is approximately the likelihood gain of learning from.",
                    "label": 0
                },
                {
                    "sent": "From an approximation of the expanded bicluster.",
                    "label": 1
                },
                {
                    "sent": "So in other words, if we want to.",
                    "label": 0
                },
                {
                    "sent": "Maximize.",
                    "label": 0
                },
                {
                    "sent": "The likelihood gain.",
                    "label": 0
                },
                {
                    "sent": "We would like to see an expanded bicluster which is coherent and has coherent expression context matrix that we discussed in the first step.",
                    "label": 1
                },
                {
                    "sent": "And again, to prevent overfitting, we have to also consider prior.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get the second step.",
                    "label": 0
                },
                {
                    "sent": "We try to find or symbols that lead to large posterior gain as defined in the previous slide and if we can find such a.",
                    "label": 1
                },
                {
                    "sent": "Or symbol.",
                    "label": 1
                },
                {
                    "sent": "We add a new rule of produce and to the grammar, and we do a maximum reduction of the corpus and we update the table T.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we repeat the first 2 steps too.",
                    "label": 0
                },
                {
                    "sent": "Add and or groups into the grammar.",
                    "label": 0
                },
                {
                    "sent": "Until that until we can no longer learn any new rule.",
                    "label": 0
                },
                {
                    "sent": "So which means we can no longer find any biclustering the table.",
                    "label": 0
                },
                {
                    "sent": "Then we go into the post processing to add a start rules.",
                    "label": 0
                },
                {
                    "sent": "So in post processing, we go over the training corpus to see for each sentence if it has been fully reduced into a single symbol.",
                    "label": 1
                },
                {
                    "sent": "Next then we just add a start rule as produces X.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, if we find a sentence that hasn't been fully reduced, we have a few options.",
                    "label": 0
                },
                {
                    "sent": "So we can either.",
                    "label": 0
                },
                {
                    "sent": "Ignore it.",
                    "label": 0
                },
                {
                    "sent": "Or we can try to parse it again, trying to fully reduce it, or we can add some ad hoc grammar rules in Telegram.",
                    "label": 1
                },
                {
                    "sent": "And after that we return the grammar.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I will present some experimental results.",
                    "label": 0
                },
                {
                    "sent": "We we evaluate the weak generative capacity of the learn grammar.",
                    "label": 1
                },
                {
                    "sent": "So we use the measurements of precision, recall and F measure.",
                    "label": 0
                },
                {
                    "sent": "I mean, we compare the language of the touch Graham, who is the language of the learn grammar.",
                    "label": 0
                },
                {
                    "sent": "And we use a set of.",
                    "label": 0
                },
                {
                    "sent": "Artificial English like context free grammars.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is the result.",
                    "label": 0
                },
                {
                    "sent": "This part is.",
                    "label": 0
                },
                {
                    "sent": "The results of our algorithm and we compare it with compared with two other methods.",
                    "label": 0
                },
                {
                    "sent": "Me Renee Diaz.",
                    "label": 0
                },
                {
                    "sent": "European precision and RN recall and F as F score and the number in the princesses.",
                    "label": 1
                },
                {
                    "sent": "The standard deviation.",
                    "label": 0
                },
                {
                    "sent": "And we can see that our algorithm outperforms me.",
                    "label": 0
                },
                {
                    "sent": "Renee Diaz and also.",
                    "label": 0
                },
                {
                    "sent": "It has lower standard deviations, which means it might be more stable.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary.",
                    "label": 0
                },
                {
                    "sent": "I presented an answer file the CFG learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Learn new grammar rules by doing iterative by clustering on table of simple pairs.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "Each step in each step, the algorithm actually tries to.",
                    "label": 0
                },
                {
                    "sent": "Maximize the increase of the posterior of the grammar given the training corpus and we got competitive experimental results.",
                    "label": 1
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some work in progress.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                },
                {
                    "sent": "Trying to find some alternative strategies so instead of greedy search.",
                    "label": 0
                },
                {
                    "sent": "Trying to some trying to find some other strategies to optimize the objective function.",
                    "label": 0
                },
                {
                    "sent": "And also we are working on the evaluation.",
                    "label": 0
                },
                {
                    "sent": "And adaptation of our algorithm to some real world applications like natural language grammars.",
                    "label": 1
                },
                {
                    "sent": "With respect to both weak and strong generative path.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's all, thank you.",
                    "label": 0
                },
                {
                    "sent": "I'm still trying to get my head around the idea of this multiplicative coherency.",
                    "label": 0
                },
                {
                    "sent": "So what happens if you have words that are ambiguous saying for speech?",
                    "label": 0
                },
                {
                    "sent": "If you want to call it so first level up if they're ambiguous in that, that means that they're actually not.",
                    "label": 0
                },
                {
                    "sent": "They don't have to be multiple collectively.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in that case you will find well.",
                    "label": 0
                },
                {
                    "sent": "Maybe to buy glasses which is overlapping.",
                    "label": 0
                },
                {
                    "sent": "So the overlapped part is an ambiguous part, and.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Algorithm now.",
                    "label": 0
                },
                {
                    "sent": "Combined account account in two parts right so?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "The formulation in our algorithm doesn't consider the bigger it, so if there are significant ambiguity in the corpus then it may not work well so.",
                    "label": 0
                },
                {
                    "sent": "I'm really curious how.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "I think as possible to improve the bicluster algorithm so it can try to find some.",
                    "label": 0
                },
                {
                    "sent": "I mean try to find the case of overlapping.",
                    "label": 0
                },
                {
                    "sent": "So it's possible, but it's totally complex.",
                    "label": 0
                },
                {
                    "sent": "What do you do if you don't find a good oral?",
                    "label": 0
                },
                {
                    "sent": "Let go.",
                    "label": 0
                },
                {
                    "sent": "Connect into I mean.",
                    "label": 0
                },
                {
                    "sent": "To the first step again.",
                    "label": 0
                },
                {
                    "sent": "I mean as possible.",
                    "label": 0
                },
                {
                    "sent": "It's impossible to find the same handle in a laser.",
                    "label": 0
                },
                {
                    "sent": "The same Andrew.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What do you mean by finding the same animal?",
                    "label": 0
                },
                {
                    "sent": "Possible.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "We are not looking in the step.",
                    "label": 0
                },
                {
                    "sent": "We're not looking at the end rule that we found in the first step.",
                    "label": 0
                },
                {
                    "sent": "We actually try to.",
                    "label": 0
                },
                {
                    "sent": "Find some additional rules.",
                    "label": 0
                },
                {
                    "sent": "We chat in the form of this.",
                    "label": 0
                },
                {
                    "sent": "This will produce an.",
                    "label": 0
                },
                {
                    "sent": "So the end is the end symbol that is learning the first step and all.",
                    "label": 0
                },
                {
                    "sent": "Is some or symbol that is learned in previous.",
                    "label": 0
                },
                {
                    "sent": "Loops.",
                    "label": 0
                },
                {
                    "sent": "So we are not looking at the end rule that they learn.",
                    "label": 0
                },
                {
                    "sent": "Instead we try to find some additional rules that can be learned in first step.",
                    "label": 0
                },
                {
                    "sent": "Does that answer question?",
                    "label": 0
                },
                {
                    "sent": "And then you try to write.",
                    "label": 0
                },
                {
                    "sent": "So you you already learned some morals.",
                    "label": 0
                },
                {
                    "sent": "Now you've learned a new Android.",
                    "label": 0
                },
                {
                    "sent": "You fit into an already existing order.",
                    "label": 0
                },
                {
                    "sent": "But what if it does not exist?",
                    "label": 0
                },
                {
                    "sent": "Let's see an example.",
                    "label": 0
                },
                {
                    "sent": "I think I have a slide so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the end of group.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's say in the first step we learn a new end symbol and.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "In the target grammar, we have this rule.",
                    "label": 0
                },
                {
                    "sent": "Or to produce it and produce end.",
                    "label": 0
                },
                {
                    "sent": "And this rule, as I mentioned in the previous slide, can be learned by the first step.",
                    "label": 0
                },
                {
                    "sent": "So in this second step we are trying to find this rule.",
                    "label": 0
                },
                {
                    "sent": "Well, we got up to.",
                    "label": 0
                },
                {
                    "sent": "So it seems to me that the from a statistical point of view, this criterion you're trying to optimize is.",
                    "label": 0
                },
                {
                    "sent": "Map estimator, so it's it's a combination of the likelihood, an end of prior, so I'm not sure that the the competitive approach that you've been comparing with are also optimizing these criteria.",
                    "label": 0
                },
                {
                    "sent": "No they are.",
                    "label": 0
                },
                {
                    "sent": "I don't know the last one, but they mean.",
                    "label": 0
                },
                {
                    "sent": "So I think it might be worth to actually compare with, I think approach.",
                    "label": 0
                },
                {
                    "sent": "Which are after optimizing the same criterion.",
                    "label": 0
                },
                {
                    "sent": "I'm thinking you're talking about the work of stalking.",
                    "label": 0
                },
                {
                    "sent": "Just like 40 years ago.",
                    "label": 0
                },
                {
                    "sent": "And where he had actually tackled the problem by Asian running of PCG, which is exactly what you would like your attacking, so right.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, I think that's all we have time for, so let's thank the speaker.",
                    "label": 0
                }
            ]
        }
    }
}