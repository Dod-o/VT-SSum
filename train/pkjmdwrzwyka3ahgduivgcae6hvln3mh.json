{
    "id": "pkjmdwrzwyka3ahgduivgcae6hvln3mh",
    "title": "Topic Models with Power-Law Using Pitman-Yor Process",
    "info": {
        "author": [
            "Issei Sato, University of Tokyo"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining->Applications"
        ]
    },
    "url": "http://videolectures.net/kdd2010_sato_tmplupyp/",
    "segmentation": [
        [
            "I'm easy, subtle from University of Tokyo.",
            "Joint work with Hiroshi Nakagawa.",
            "Today I will talk about topic models with power role using Pitman process.",
            "This is today's contents.",
            "So let's start with.",
            "Reviewing"
        ],
        [
            "Rate indicial allocation well known topic model?",
            "Writing this allocation.",
            "Assume that each document.",
            "Yes, mixture of latent topics.",
            "So.",
            "Our rating topics is are generated from topic distribution margin.",
            "Normal distribution whose parameters for Boeing.",
            "Three distribution and earlier also models were distribution too, according to each topics.",
            "By using multinomial distribution.",
            "And everybody always generates each word from topic."
        ],
        [
            "Our model is LDA prospeed Mayer Memorizer what memorizer our model like LDA.",
            "Our model generates award from topic.",
            "However our model.",
            "Often choose a word that has already.",
            "Appeared in the document.",
            "That is, we consider not only topics but also words that we previously use.",
            "This means memorizer.",
            "This memorizing so this memorizer property capture the."
        ],
        [
            "Overall phenomenon overall distribution in a document.",
            "So let's go back to the LDA.",
            "This is a graphical model for LDA.",
            "We focus on the topic assignments.",
            "Of LDA.",
            "LDA assigns topic for each word in a document.",
            "So.",
            "We illustrate this situation with restaurant representation."
        ],
        [
            "We we represent a document."
        ],
        [
            "At the restaurant.",
            "This is an empty document.",
            "And we're going to generate words by using LDA in LDA first generate."
        ],
        [
            "Topic from topic distribution.",
            "And.",
            "Generate word from."
        ],
        [
            "Topic specific word distribution.",
            "In this case, topic 3.",
            "The generated words indicates our dish served at the table and indicate word type in this document."
        ],
        [
            "Finally, customer ardyss customer."
        ],
        [
            "Sit at the table this customers."
        ],
        [
            "Arrival indicates a token of the."
        ],
        [
            "Words in a document.",
            "And in this case, this term appears in a document.",
            "In the same way our generates topic and generate world.",
            "In"
        ],
        [
            "LDA generates words from topic.",
            "So topic distribution and water distribution.",
            "You can see."
        ],
        [
            "Early assumes talk topic for each word, which indicates that a customer always sit at the table around.",
            "In our representation, this representation seems strange to someone.",
            "However, this representation clarify the difference between LDA and our models.",
            "So the basic idea of our model is to."
        ],
        [
            "Introduced the seating arrangement of customers into LDA.",
            "So let's consider.",
            "Let us consider the.",
            "So what kind of pricing model is appropriate for this seating arrangement?"
        ],
        [
            "We use.",
            "The Pitman process for the seating arrangement of customers in your model.",
            "And we propose Pete minor topic."
        ],
        [
            "Model.",
            "So the Pitman top are extremely are sorry.",
            "Peter Mayer Process has a stochastic process.",
            "Called Chinese restaurant process, the Chinese restaurant process is the process for establishing customer seating arrangement in.",
            "Restaurant.",
            "Suppose that there are three customers.",
            "At the first table and a lot of customers are sitting at the second table and the customer.",
            "Sit at the South table around so impede my process in its restaurant representation.",
            "Our new customers sit at the table in proportion to the number of customers holding already sitting there.",
            "And also.",
            "Sit at a new table with some probability.",
            "Depending on the number of tables.",
            "In this case, the number of tables is 3.",
            "And if our customers sit at the new table.",
            "This is generated from base distribution.",
            "Or our best measure?",
            "So I am Pittman process.",
            "The number of customers seated at the table follows the Power Distribution.",
            "The Pitman process has concentration parameter gamma and discount parameter D that control the power property in.",
            "Distribution, and Moreover the number of existing table places.",
            "Emphasis on the new table generation that induces the long tail phenomenon in a distribution.",
            "In this way, discrete probability distribution is."
        ],
        [
            "Generated from Pete my process.",
            "This is a example of power of property over documents, so this is a statistics about number of occurrences of words in five about 500 words document example.",
            "In order for bus.",
            "NW in so this indicates the number of occurrences of words.",
            "And this indicates our PONW indicates the empirical probability.",
            "So, so this point indicates that.",
            "This document has many."
        ],
        [
            "Frequency 1 words and this document or sorry.",
            "And so this document has large number of real world types and small number of common world types.",
            "So this behavior is characteristics of.",
            "Power property.",
            "Now by using the.",
            "Pete my process.",
            "We propose Pittman Topic Pittman topic model in our model.",
            "Our customers seat at the table in proportion to the number of customers already sitting there.",
            "This selection mechanism indicates the memorizer for history words and also sit at the new table.",
            "If a customer sits at a new table, a topic is generated from topic distribution and what is generated from.",
            "Topic specific word distribution and served at the table.",
            "Note that LDA are always are.",
            "In LDA, our customer always sits at a new table.",
            "That is so LDA.",
            "So do not have.",
            "Our memories are property.",
            "This is a graphical model of."
        ],
        [
            "Update My own topic model compared to LDA."
        ],
        [
            "So this, whatever is our table, actually a word subset the table and.",
            "X indicates the table index.",
            "So."
        ],
        [
            "Once again, note that LDA as assigns topic for each word.",
            "In our model, Peeterman topic model assigns topic for each table.",
            "That is, the number of topics in LDA is equal to the number of words.",
            "In contrast, the number of topics in Pitman topic model is equal to the number of tables not worth.",
            "So.",
            "If we always set.",
            "Exxon you index our model is equal to LDA.",
            "In other words, the table seating of LDA is limited to the situation that one table has one customers.",
            "And so it can be said that our model.",
            "Relax.",
            "Is this limitation of LDA by introducing these variables we estimate all parameters and latent variables by using Gibbs sampler."
        ],
        [
            "This is a predictive distribution of.",
            "Pitman topic model.",
            "There are predictive distribution is used for calculating the publicity in experiments.",
            "So.",
            "Ah.",
            "OK.",
            "This predictive distribution consists of two parts.",
            "One part is memorizer parts and other is LDA like prediction parts.",
            "So this predictive distribution is regarded as a linear interpolation of memorizer and.",
            "Earlier like prediction.",
            "Or if so, if what we has always already appeared in this document, this term is effective, in other words.",
            "If this term.",
            "It is the first appearance in this documents.",
            "This time is there.",
            "And so our our model is similar to every year for.",
            "Predicting this term so.",
            "If so, when word, word, word is so when are words has short so.",
            "If our.",
            "If our documents.",
            "Has a small number of word types.",
            "In this case the number of table is small and so the memorization part.",
            "Strongly affect this prediction.",
            "In contrast, when a document has large number of word types, in this case the number of table is right.",
            "So.",
            "Earlier I prediction strongly affect this prediction.",
            "In this way our predictive distribution.",
            "Our deal with our flexibly there.",
            "Document properties."
        ],
        [
            "We also propose more generalized model impact my on topic model.",
            "Our topic specific word distribution is generated from digital distribution as well as LDA.",
            "So we assume that.",
            "This word distribution is also.",
            "Generated from Pitman process by using higher copy to my process.",
            "So this is a higher carbonation modeling for Pete Meyer topic model and we call higher carpet amount of IT model."
        ],
        [
            "So let's talk about about.",
            "X."
        ],
        [
            "Payment.",
            "We compared our model with LDA in.",
            "Are three sets of tickets later.",
            "Reuters, Associated Press and Wall Street Journal we we only show that the results of Wall Street Journal Becausw we we don't have.",
            "We don't have time to show all data set and we spread our.",
            "We spread the words into.",
            "In a document into training sets and take his test sets.",
            "And so we use the.",
            "Public see as comparison metrics.",
            "The public still indicates the prediction prediction.",
            "The performance for new words in document modeling.",
            "Lower publicity indicate the better performance.",
            "We run each to each model 5 times with different landing initialization.",
            "As you can see.",
            "The our models outperforms LDA.",
            "Big address the number of topics.",
            "The most prominent feature of our model is to achieve lower public safety in small number of topics.",
            "This means the.",
            "Our memorizer is very useful in.",
            "The number of the small number of topics.",
            "And higher carpet.",
            "My model slightly outperforms Pittman topic model.",
            "The the result of other corpus has the same property.",
            "As this results."
        ],
        [
            "So in conclusion.",
            "We propose Peatman topic model.",
            "This model is LDA Prospekt Mira memorizer.",
            "Our model.",
            "Achieve low public.",
            "See in the small number of topics.",
            "For future works, our Mother is a basic topic model, so can be applicable to other.",
            "Data mining fields.",
            "For example, market data.",
            "R. The user indicates a document and item indicates awards, so we can we can flexibly deal with the users that always buy new items and that always buy the same items.",
            "That's all, thanks."
        ],
        [
            "Any questions?",
            "Rear certain.",
            "Could you repeat the question or the comment?",
            "Yeah.",
            "Public public safety doesn't directly so indicates.",
            "Performance for classification in the public City indicator prediction.",
            "Humans for our price modeling so.",
            "It.",
            "Yeah, there in the classification task, so our model.",
            "Yeah it is fit.",
            "My feature works so so every eight hour model by using the classification task.",
            "I have a question on the scalability.",
            "When you do this future work on other data, how do you expect that complexity to resolve?",
            "Like what kind of data is, how larger data set?",
            "Do you plan to try?",
            "Yes, it's good question.",
            "So our model our the running time is our model is so.",
            "Is a.",
            "Has out of our model has a long running time.",
            "Compared to LDA but our model.",
            "Are there are we estimate our model by using give Sombra so?",
            "Yeah, so we can use incremental gives some pro for our model.",
            "It's a.",
            "It leads to the scalability of our model."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm easy, subtle from University of Tokyo.",
                    "label": 1
                },
                {
                    "sent": "Joint work with Hiroshi Nakagawa.",
                    "label": 0
                },
                {
                    "sent": "Today I will talk about topic models with power role using Pitman process.",
                    "label": 0
                },
                {
                    "sent": "This is today's contents.",
                    "label": 0
                },
                {
                    "sent": "So let's start with.",
                    "label": 0
                },
                {
                    "sent": "Reviewing",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rate indicial allocation well known topic model?",
                    "label": 1
                },
                {
                    "sent": "Writing this allocation.",
                    "label": 0
                },
                {
                    "sent": "Assume that each document.",
                    "label": 0
                },
                {
                    "sent": "Yes, mixture of latent topics.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our rating topics is are generated from topic distribution margin.",
                    "label": 0
                },
                {
                    "sent": "Normal distribution whose parameters for Boeing.",
                    "label": 0
                },
                {
                    "sent": "Three distribution and earlier also models were distribution too, according to each topics.",
                    "label": 0
                },
                {
                    "sent": "By using multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "And everybody always generates each word from topic.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our model is LDA prospeed Mayer Memorizer what memorizer our model like LDA.",
                    "label": 0
                },
                {
                    "sent": "Our model generates award from topic.",
                    "label": 0
                },
                {
                    "sent": "However our model.",
                    "label": 0
                },
                {
                    "sent": "Often choose a word that has already.",
                    "label": 0
                },
                {
                    "sent": "Appeared in the document.",
                    "label": 0
                },
                {
                    "sent": "That is, we consider not only topics but also words that we previously use.",
                    "label": 0
                },
                {
                    "sent": "This means memorizer.",
                    "label": 0
                },
                {
                    "sent": "This memorizing so this memorizer property capture the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overall phenomenon overall distribution in a document.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to the LDA.",
                    "label": 0
                },
                {
                    "sent": "This is a graphical model for LDA.",
                    "label": 0
                },
                {
                    "sent": "We focus on the topic assignments.",
                    "label": 0
                },
                {
                    "sent": "Of LDA.",
                    "label": 0
                },
                {
                    "sent": "LDA assigns topic for each word in a document.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We illustrate this situation with restaurant representation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We we represent a document.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the restaurant.",
                    "label": 0
                },
                {
                    "sent": "This is an empty document.",
                    "label": 0
                },
                {
                    "sent": "And we're going to generate words by using LDA in LDA first generate.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Topic from topic distribution.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Generate word from.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Topic specific word distribution.",
                    "label": 0
                },
                {
                    "sent": "In this case, topic 3.",
                    "label": 1
                },
                {
                    "sent": "The generated words indicates our dish served at the table and indicate word type in this document.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, customer ardyss customer.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sit at the table this customers.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arrival indicates a token of the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words in a document.",
                    "label": 0
                },
                {
                    "sent": "And in this case, this term appears in a document.",
                    "label": 0
                },
                {
                    "sent": "In the same way our generates topic and generate world.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "LDA generates words from topic.",
                    "label": 0
                },
                {
                    "sent": "So topic distribution and water distribution.",
                    "label": 1
                },
                {
                    "sent": "You can see.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Early assumes talk topic for each word, which indicates that a customer always sit at the table around.",
                    "label": 0
                },
                {
                    "sent": "In our representation, this representation seems strange to someone.",
                    "label": 0
                },
                {
                    "sent": "However, this representation clarify the difference between LDA and our models.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea of our model is to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introduced the seating arrangement of customers into LDA.",
                    "label": 0
                },
                {
                    "sent": "So let's consider.",
                    "label": 0
                },
                {
                    "sent": "Let us consider the.",
                    "label": 0
                },
                {
                    "sent": "So what kind of pricing model is appropriate for this seating arrangement?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use.",
                    "label": 0
                },
                {
                    "sent": "The Pitman process for the seating arrangement of customers in your model.",
                    "label": 0
                },
                {
                    "sent": "And we propose Pete minor topic.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "So the Pitman top are extremely are sorry.",
                    "label": 0
                },
                {
                    "sent": "Peter Mayer Process has a stochastic process.",
                    "label": 0
                },
                {
                    "sent": "Called Chinese restaurant process, the Chinese restaurant process is the process for establishing customer seating arrangement in.",
                    "label": 1
                },
                {
                    "sent": "Restaurant.",
                    "label": 0
                },
                {
                    "sent": "Suppose that there are three customers.",
                    "label": 0
                },
                {
                    "sent": "At the first table and a lot of customers are sitting at the second table and the customer.",
                    "label": 0
                },
                {
                    "sent": "Sit at the South table around so impede my process in its restaurant representation.",
                    "label": 0
                },
                {
                    "sent": "Our new customers sit at the table in proportion to the number of customers holding already sitting there.",
                    "label": 0
                },
                {
                    "sent": "And also.",
                    "label": 0
                },
                {
                    "sent": "Sit at a new table with some probability.",
                    "label": 0
                },
                {
                    "sent": "Depending on the number of tables.",
                    "label": 0
                },
                {
                    "sent": "In this case, the number of tables is 3.",
                    "label": 0
                },
                {
                    "sent": "And if our customers sit at the new table.",
                    "label": 0
                },
                {
                    "sent": "This is generated from base distribution.",
                    "label": 0
                },
                {
                    "sent": "Or our best measure?",
                    "label": 0
                },
                {
                    "sent": "So I am Pittman process.",
                    "label": 0
                },
                {
                    "sent": "The number of customers seated at the table follows the Power Distribution.",
                    "label": 0
                },
                {
                    "sent": "The Pitman process has concentration parameter gamma and discount parameter D that control the power property in.",
                    "label": 0
                },
                {
                    "sent": "Distribution, and Moreover the number of existing table places.",
                    "label": 0
                },
                {
                    "sent": "Emphasis on the new table generation that induces the long tail phenomenon in a distribution.",
                    "label": 0
                },
                {
                    "sent": "In this way, discrete probability distribution is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generated from Pete my process.",
                    "label": 0
                },
                {
                    "sent": "This is a example of power of property over documents, so this is a statistics about number of occurrences of words in five about 500 words document example.",
                    "label": 0
                },
                {
                    "sent": "In order for bus.",
                    "label": 0
                },
                {
                    "sent": "NW in so this indicates the number of occurrences of words.",
                    "label": 0
                },
                {
                    "sent": "And this indicates our PONW indicates the empirical probability.",
                    "label": 0
                },
                {
                    "sent": "So, so this point indicates that.",
                    "label": 0
                },
                {
                    "sent": "This document has many.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Frequency 1 words and this document or sorry.",
                    "label": 0
                },
                {
                    "sent": "And so this document has large number of real world types and small number of common world types.",
                    "label": 0
                },
                {
                    "sent": "So this behavior is characteristics of.",
                    "label": 0
                },
                {
                    "sent": "Power property.",
                    "label": 0
                },
                {
                    "sent": "Now by using the.",
                    "label": 0
                },
                {
                    "sent": "Pete my process.",
                    "label": 0
                },
                {
                    "sent": "We propose Pittman Topic Pittman topic model in our model.",
                    "label": 0
                },
                {
                    "sent": "Our customers seat at the table in proportion to the number of customers already sitting there.",
                    "label": 0
                },
                {
                    "sent": "This selection mechanism indicates the memorizer for history words and also sit at the new table.",
                    "label": 0
                },
                {
                    "sent": "If a customer sits at a new table, a topic is generated from topic distribution and what is generated from.",
                    "label": 0
                },
                {
                    "sent": "Topic specific word distribution and served at the table.",
                    "label": 0
                },
                {
                    "sent": "Note that LDA are always are.",
                    "label": 0
                },
                {
                    "sent": "In LDA, our customer always sits at a new table.",
                    "label": 0
                },
                {
                    "sent": "That is so LDA.",
                    "label": 0
                },
                {
                    "sent": "So do not have.",
                    "label": 0
                },
                {
                    "sent": "Our memories are property.",
                    "label": 0
                },
                {
                    "sent": "This is a graphical model of.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Update My own topic model compared to LDA.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this, whatever is our table, actually a word subset the table and.",
                    "label": 0
                },
                {
                    "sent": "X indicates the table index.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once again, note that LDA as assigns topic for each word.",
                    "label": 0
                },
                {
                    "sent": "In our model, Peeterman topic model assigns topic for each table.",
                    "label": 0
                },
                {
                    "sent": "That is, the number of topics in LDA is equal to the number of words.",
                    "label": 0
                },
                {
                    "sent": "In contrast, the number of topics in Pitman topic model is equal to the number of tables not worth.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we always set.",
                    "label": 0
                },
                {
                    "sent": "Exxon you index our model is equal to LDA.",
                    "label": 0
                },
                {
                    "sent": "In other words, the table seating of LDA is limited to the situation that one table has one customers.",
                    "label": 0
                },
                {
                    "sent": "And so it can be said that our model.",
                    "label": 0
                },
                {
                    "sent": "Relax.",
                    "label": 0
                },
                {
                    "sent": "Is this limitation of LDA by introducing these variables we estimate all parameters and latent variables by using Gibbs sampler.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a predictive distribution of.",
                    "label": 0
                },
                {
                    "sent": "Pitman topic model.",
                    "label": 0
                },
                {
                    "sent": "There are predictive distribution is used for calculating the publicity in experiments.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This predictive distribution consists of two parts.",
                    "label": 0
                },
                {
                    "sent": "One part is memorizer parts and other is LDA like prediction parts.",
                    "label": 0
                },
                {
                    "sent": "So this predictive distribution is regarded as a linear interpolation of memorizer and.",
                    "label": 0
                },
                {
                    "sent": "Earlier like prediction.",
                    "label": 0
                },
                {
                    "sent": "Or if so, if what we has always already appeared in this document, this term is effective, in other words.",
                    "label": 0
                },
                {
                    "sent": "If this term.",
                    "label": 0
                },
                {
                    "sent": "It is the first appearance in this documents.",
                    "label": 0
                },
                {
                    "sent": "This time is there.",
                    "label": 0
                },
                {
                    "sent": "And so our our model is similar to every year for.",
                    "label": 0
                },
                {
                    "sent": "Predicting this term so.",
                    "label": 0
                },
                {
                    "sent": "If so, when word, word, word is so when are words has short so.",
                    "label": 0
                },
                {
                    "sent": "If our.",
                    "label": 0
                },
                {
                    "sent": "If our documents.",
                    "label": 0
                },
                {
                    "sent": "Has a small number of word types.",
                    "label": 0
                },
                {
                    "sent": "In this case the number of table is small and so the memorization part.",
                    "label": 0
                },
                {
                    "sent": "Strongly affect this prediction.",
                    "label": 0
                },
                {
                    "sent": "In contrast, when a document has large number of word types, in this case the number of table is right.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Earlier I prediction strongly affect this prediction.",
                    "label": 0
                },
                {
                    "sent": "In this way our predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "Our deal with our flexibly there.",
                    "label": 0
                },
                {
                    "sent": "Document properties.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also propose more generalized model impact my on topic model.",
                    "label": 0
                },
                {
                    "sent": "Our topic specific word distribution is generated from digital distribution as well as LDA.",
                    "label": 0
                },
                {
                    "sent": "So we assume that.",
                    "label": 0
                },
                {
                    "sent": "This word distribution is also.",
                    "label": 0
                },
                {
                    "sent": "Generated from Pitman process by using higher copy to my process.",
                    "label": 0
                },
                {
                    "sent": "So this is a higher carbonation modeling for Pete Meyer topic model and we call higher carpet amount of IT model.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's talk about about.",
                    "label": 0
                },
                {
                    "sent": "X.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Payment.",
                    "label": 0
                },
                {
                    "sent": "We compared our model with LDA in.",
                    "label": 0
                },
                {
                    "sent": "Are three sets of tickets later.",
                    "label": 0
                },
                {
                    "sent": "Reuters, Associated Press and Wall Street Journal we we only show that the results of Wall Street Journal Becausw we we don't have.",
                    "label": 0
                },
                {
                    "sent": "We don't have time to show all data set and we spread our.",
                    "label": 0
                },
                {
                    "sent": "We spread the words into.",
                    "label": 0
                },
                {
                    "sent": "In a document into training sets and take his test sets.",
                    "label": 0
                },
                {
                    "sent": "And so we use the.",
                    "label": 0
                },
                {
                    "sent": "Public see as comparison metrics.",
                    "label": 0
                },
                {
                    "sent": "The public still indicates the prediction prediction.",
                    "label": 0
                },
                {
                    "sent": "The performance for new words in document modeling.",
                    "label": 0
                },
                {
                    "sent": "Lower publicity indicate the better performance.",
                    "label": 0
                },
                {
                    "sent": "We run each to each model 5 times with different landing initialization.",
                    "label": 0
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                },
                {
                    "sent": "The our models outperforms LDA.",
                    "label": 0
                },
                {
                    "sent": "Big address the number of topics.",
                    "label": 0
                },
                {
                    "sent": "The most prominent feature of our model is to achieve lower public safety in small number of topics.",
                    "label": 0
                },
                {
                    "sent": "This means the.",
                    "label": 0
                },
                {
                    "sent": "Our memorizer is very useful in.",
                    "label": 0
                },
                {
                    "sent": "The number of the small number of topics.",
                    "label": 0
                },
                {
                    "sent": "And higher carpet.",
                    "label": 0
                },
                {
                    "sent": "My model slightly outperforms Pittman topic model.",
                    "label": 1
                },
                {
                    "sent": "The the result of other corpus has the same property.",
                    "label": 0
                },
                {
                    "sent": "As this results.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion.",
                    "label": 0
                },
                {
                    "sent": "We propose Peatman topic model.",
                    "label": 1
                },
                {
                    "sent": "This model is LDA Prospekt Mira memorizer.",
                    "label": 0
                },
                {
                    "sent": "Our model.",
                    "label": 0
                },
                {
                    "sent": "Achieve low public.",
                    "label": 0
                },
                {
                    "sent": "See in the small number of topics.",
                    "label": 0
                },
                {
                    "sent": "For future works, our Mother is a basic topic model, so can be applicable to other.",
                    "label": 0
                },
                {
                    "sent": "Data mining fields.",
                    "label": 0
                },
                {
                    "sent": "For example, market data.",
                    "label": 0
                },
                {
                    "sent": "R. The user indicates a document and item indicates awards, so we can we can flexibly deal with the users that always buy new items and that always buy the same items.",
                    "label": 0
                },
                {
                    "sent": "That's all, thanks.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Rear certain.",
                    "label": 0
                },
                {
                    "sent": "Could you repeat the question or the comment?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Public public safety doesn't directly so indicates.",
                    "label": 0
                },
                {
                    "sent": "Performance for classification in the public City indicator prediction.",
                    "label": 0
                },
                {
                    "sent": "Humans for our price modeling so.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there in the classification task, so our model.",
                    "label": 0
                },
                {
                    "sent": "Yeah it is fit.",
                    "label": 0
                },
                {
                    "sent": "My feature works so so every eight hour model by using the classification task.",
                    "label": 0
                },
                {
                    "sent": "I have a question on the scalability.",
                    "label": 0
                },
                {
                    "sent": "When you do this future work on other data, how do you expect that complexity to resolve?",
                    "label": 0
                },
                {
                    "sent": "Like what kind of data is, how larger data set?",
                    "label": 0
                },
                {
                    "sent": "Do you plan to try?",
                    "label": 0
                },
                {
                    "sent": "Yes, it's good question.",
                    "label": 0
                },
                {
                    "sent": "So our model our the running time is our model is so.",
                    "label": 0
                },
                {
                    "sent": "Is a.",
                    "label": 0
                },
                {
                    "sent": "Has out of our model has a long running time.",
                    "label": 0
                },
                {
                    "sent": "Compared to LDA but our model.",
                    "label": 0
                },
                {
                    "sent": "Are there are we estimate our model by using give Sombra so?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we can use incremental gives some pro for our model.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It leads to the scalability of our model.",
                    "label": 0
                }
            ]
        }
    }
}