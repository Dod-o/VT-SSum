{
    "id": "txzvpuyv5mpmvapvyopha3o2uwg4fbsa",
    "title": "Distribution-Dependent PAC-Bayes Priors",
    "info": {
        "author": [
            "Guy Lever, Department of Computer Science, University College London"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_lever_ddpb/",
    "segmentation": [
        [
            "So this is joint work with Francois Laviolette and John Shaw Taylor.",
            "And in overview then we study this."
        ],
        [
            "Idea that the PAC Bayes prior can be informed by the data generating distribution, so this is cortonese localization.",
            "An we study this in two settings really.",
            "So first of all, with Boltzmann distributions as our prior and posterior and in this setting we prove what seems to be quite sharp risk analysis for this setting and then we go on to investigate the possibility of controlling function class complexity in that setting.",
            "And we show that it's really possible to kind of encode some quite sophisticated assumptions about how we might expect a good hypothesis to interact with the kind of intrinsic geometry defined by the data.",
            "Going to extend this localization idea to Gaussian processes, so this may be a bit more practical, more practical setting, and really the hallmark of this these ideas is that we obtain really quite a significant reduction in the size of the KL term of the PAC Bayes."
        ],
        [
            "And so just some really quick preliminaries.",
            "We have, as usual, some unknown distribution D over our labeled input space.",
            "We have a sample from that distribution, and we consider, as usual, some class of hypothesis H mapping our input space to some classification space and impact base.",
            "Then we have.",
            "Prior P which is not informed by the data sample in a posterior Q distributions over H and just recalling here are a typical PAC Bayes bounds, so this is this is Segers bound, which bands a deviation between the empirical risk in the true risk of our Gibbs classifier and as in France, was talk earlier this size M is just some order root M term.",
            "So for any distributions Q&P all we need to do to obtain an analysis like this is to bound the KL term.",
            "And that's typically the dominant term in this kind of analysis, and it can be quite large.",
            "So if we have."
        ],
        [
            "Some non informative prior right, just maybe some Gaussian as we've been discussing today over.",
            "Over a hypothesis class, then in particular, kind of priors going to assign a large weight to some bad classifiers, right?",
            "So if we choose in Q2 to contain just what we think are good classifiers, then the overlap between Q&P can be weaken the divergents large.",
            "So it should say the KL actually should be large.",
            "And I think the key observation of localization is that P can be informed by the data generating distribution and in particular contain just hypothesis is true risk is small as in this equation and working with a prior like that.",
            "I mean probably that is unknown, right?",
            "But the point is, as long as we can find a Q such that we can evaluate or upper bound, the KL between Q&P, then we have a valid PAC Bayes bound for that posterior."
        ],
        [
            "And so kind of specializing that too.",
            "Our interpretation of that then, so we consider just distributions from the extra class of exponential families.",
            "So here FP&FQ or just some energy functions if you like.",
            "I'm recording what we've just said.",
            "All we need to do to obtain a risk analysis for a posterior prior like this is to bound the KL, and in this fairly general setting what we have is this simple lemma which basically says that the KL divergent's can be upper bounded by the deviation between the energy functions.",
            "In fact, it's the difference in the expected.",
            "It's the difference between expected deviation in the prior and the expected deviation in the posterior, and this is quite sharp, so the the inequality only actually arises from just one exploitation of the convexity of the negative log function.",
            "Apart from, that's just simple algebra, it's quite simple.",
            "So straight away you can see that if we choose F, Q2 estimate FP from the sample, then what we want here is some kind of concentration result for our posterior energy function, and we can actually replace the expectations just with the Supreme and then we just want some kind of uniform convergence result.",
            "But the kind of lucky thing is that this quantity on the right hand side of this inequality is exactly the kind of quantity that PAC Bayes bounds provider bound for.",
            "So if we imagine doing that and getting an upper bound for this with some pipe based theory will actually have.",
            "This quantity upper bound by some term in the KL and we can just take that over to the other side.",
            "So using this lemma is kind of recursive in that sense.",
            "And I mean already, you can see that for kind of natural choices of your energy functions, then we're going to expect this KL term to decay, and we're expecting at least one over order one over root M convergence."
        ],
        [
            "So specializing to the first case then.",
            "So we take these Boltzmann distributions where our risk features as the energy function.",
            "So in the posterior we have the empirical risk and in the prior the true risk.",
            "So recalling what we've just said and using the previous lemma or that we need to do in this case is get some concentration result for the empirical risk.",
            "And well, I mean every pack standard pacbase bound provides that, so we can actually just use Segers bound to obtain straight away this upper bound on the KL and plug that back into Segars bound itself to obtain a version of Segers bound for for this case.",
            "So this is just a risk bound for.",
            "This choice of prior and past area.",
            "And already you can kind of see the advantage over typical methods, right?",
            "So we've actually done here is upper bound the KL and I mean it seems to be quite small, right?",
            "There's.",
            "And in particular, it's decaying with the sample size, and maybe it seems quite strange that there's maybe something funny about this bound in that it doesn't seem that there's any kind of explicit.",
            "Dependence on any notion of function class complexity in that band.",
            "It seems to suggest it will given any hypothesis class I can apply this method, I get this bound that I can learn any function class you know equally well.",
            "And I mean obviously there's something a bit wrong with that.",
            "So really the kind of question is where is the notion of?"
        ],
        [
            "Function class complexity in that bound, and I think the idea is that it's kind of encoded in this gamma parameter and this gamma parameter came back.",
            "One slide was is basically the this inverse temperature parameter of our Boltzmann distributions, so that's really controlling the variance for Boltzmann distributions there, and so the idea is that if H is very rich and contains, then lots of bad classifiers, then we must really choose gamma to be very large.",
            "Control the empirical risk of the Gibbs classifier, but I have a really rich function class and I choose gamma small.",
            "I have a large variance Boltzmann distribution and all these bad functions are getting some opportunity to feature in that Gibbs classifier, and so really that controls the empirical rister.",
            "So maybe there's some kind of knew notion of function class complexity needed here to kind of understand what's going on there, but it's quite subtle 'cause it's kind of dependent on the function class, the algorithm, the data generating distribution.",
            "And I mean even simple things like what's the relationship with the VC dimension, that kind of thing which are natural questions we haven't really worked out yet."
        ],
        [
            "But we can develop that idea by going on and attempting to kind of control the function class capacity in some way.",
            "So here we have the same Boltzmann distributions, but we've added basically regularization term.",
            "So this FP and FQ now punish complicated hypothesis in some way.",
            "So far function class has some norm seats in our KHS or something we can just pick the norm in that function class as the regularizer and the first thing to notice here really is that if P = F Q, we just get the same bound, right that recording.",
            "Again, all we need to do to obtain this analysis is get concentration of this posterior energy function to the prior energy function.",
            "And with F P = F Q, there's no other convergence to worry about, right?",
            "So we get the same bound, but the point is, maybe this enables learning with a smaller gamma basically."
        ],
        [
            "And we go on to basically do a different kind of regularization.",
            "So what I want to do here is regularize with respect to the way that we kind of expect hypothesis.",
            "We're all the way that hypothesis interact with the kind of true geometry of the data, generating distribution and to kind of motivate that idea, there's been a recent theme of machine learning.",
            "That data defines its own geometry in some way, so it can.",
            "Occupy some kind of submanifold of the representation space or be a collection of clusters or something like that and that really defines it's kind of its own geometry and it can be quite different to the geometry of kind of inherited from the ambient space, essentially so in this picture.",
            "For example, if we imagine just some metric in the kind of ambient space, then points A&B will always be very close in this metric, but with respect to the manifold geometry defined by the data there.",
            "Very different.",
            "So from a learning theory perspective, if I'm kind of trying to learn on the on data like this under something like a large margin assumption, then it can fail and in situations like that, if I encode that large margin assumption with respect to the wrong kind of geometry so.",
            "The kind of contention really of this part of the work is that.",
            "You want to encode those assumptions with respect to the right geometry, right?",
            "And so the kind of key results in this."
        ],
        [
            "Um?",
            "Spite of machine learning is that we can actually capture this geometry so we can learn it from random samples.",
            "So this is like this is kind of the key idea in graph theoretical methods in machine learning.",
            "Is that given a sample of points from my data generating distribution, I can form a graph on that sample and I can form a smoothness functional like this on that graph.",
            "So the W here are basically edge weights of this graph which measures similarity between two data points.",
            "And so this smoothness functional basically measures how smooth the hypothesis is viewed as a function on that graph, and the key result rarely hear of Heine ET al.",
            "And Johnny was involved in this work is that this converges to a really natural measure of smoothness with respect to that data generating distribution.",
            "So that's really what we want.",
            "We want to measure how smooth hypothesis are as they kind of move over the geometry defined by the data.",
            "So using this kind of method, we can capture that, and that's not really possible without passing to some kind of empirical notion of the geometry of things."
        ],
        [
            "So.",
            "We can just plug those ideas basically into our setting.",
            "So how again the same Boltzmann distributions?",
            "But here we have.",
            "The regularizer in the posterior is this smoothness on a graph formed on the sample.",
            "And in the prior, it's basically the expectation of that oversamples, so again to get a bound to get a pack Bayes risk analysis, all we need to do is obtain some concentration result for this smoothness functional and I should have said earlier that now we can consider like samples with some unlabeled points as well.",
            "So this unlabeled set can be empty, but.",
            "Can obviously be helpful to learn the geometry to have extra unlabeled data.",
            "So essentially this smoothness functional is a U statistic 2nd order U statistic.",
            "So what we actually need there is a PAC Bayes concentration result for the process.",
            "So you process is just a family of you statistics index by some function class.",
            "So."
        ],
        [
            "That's what we provide and we do that quite generally then.",
            "So for any second order use statistic.",
            "We tried this pipe Bayes bound.",
            "Basically it looks like any other pacbase bound, but we have some boundedness condition on what's called the kernel of EU statistics.",
            "That's this F function.",
            "And just a note about the proof of this.",
            "So this basically users this really quite general recipe bound for generating PAC Bayes bounds of Schierman ET al that fronts were introduced earlier and it basically reduces obtaining a PAC Bayes bound to evaluating or upper bounding moment generating function, and in this case we can do that with hostings decomposition of you statistics into martingales.",
            "So there's a kind of Canonical decomposition, and once you've done that, you can use Hovding's lemma recursively, which is just exactly as it's done in the proof of Mcdiarmid's inequality.",
            "So basically that's all we need."
        ],
        [
            "To obtain our PAC Bayes bound for this setting right?",
            "So it's really quite simple still and this is again, this is just seegars bound specialized to this case.",
            "I've just plugged in my bound that I've just obtained on the KL Divergent.",
            "So just recalled prior and posterior at the top.",
            "So what I've done here is basically just upper bound, the KL term and again it's seems quite small.",
            "So A&B are both order one over root M terms, so there's nothing.",
            "There's no big terms featuring in this.",
            "There's no kind of complicated terms at all.",
            "So B is just some bounded condition bounding or function class now and W is the maximum edge weight on our graph and this can be really easily one apart from that.",
            "Apart from that, the only unknown thing again is this gamma.",
            "This inverse temperature parameter.",
            "So I just wanted to kind of emphasize actually that controlling function class complexity in that way with respect to how hypothesis really interact with this geometry.",
            "It's quite unusual that we can obtain an analysis like that so easily I think, and I think that's really kind of testament to how flexible PAC Bayes is, and this localization idea that you can just encode this in your prior so easily if you try to do this in classical methods, it's really quite difficult."
        ],
        [
            "And just finally, I just basically wanted to extend those ideas to.",
            "The Gaussian process setting.",
            "So there's a duality between Gaussian processes and RK chess methods.",
            "So I find the arc HSV a bit easier to motivate things from, so just kind of motivated from that angle, so we just have some kernel on our input space.",
            "And we form the typical arc HS which functions which are identified as functions on our input space by the normal inner product formula.",
            "What we do is we try and fit some Gaussian distributions on this arc ahs.",
            "So our posterior distribution is basically has its mean.",
            "This mu S which is the.",
            "Minimizer, some regularised empirical risk, basically.",
            "So L this risk is the risk with respect to some Alpha Lipschitz convex loss function, so it's just some regularised empirical risk minimizer.",
            "And in the prior we just take the expectation over that.",
            "Oversamples and basically the point is that predicting with our Gibbs classifier drawn from this posterior distribution is a Gaussian process on our original space, whose mean is the mean and whose covariance is given by the kernel and the temperature parameter again.",
            "So.",
            "Along the lines of everything that."
        ],
        [
            "Just done as well.",
            "All that we need to do to get a risk bound for this case is to basically bound the KL between those two distributions.",
            "Those two Gaussians, and so that's just the difference between the means.",
            "So the distance between the means in feature space.",
            "So in this arc HS and it turns out you can bound this basically using a kind of method of bounded differences.",
            "So what we do is we basically consider consider 2 sample.",
            "So our sample in its perturbation.",
            "And this is just obtained by resampling one point and by a kind of stability argument.",
            "So this is the idea of algorithmic stability in machine learning you can upper bound the.",
            "Distance in feature space between your empirical risk minimizer on S and on its perturbation, and then you have what looks like a kind of bounded differences condition.",
            "So it looks like the kind of condition you have in the arms inequality, for example, but you're in a high dimensional RKHS.",
            "But it turns out basically there's a version of Azuma's inequality for arbitrarily high dimensional Hilbert spaces, and so you can just apply that to get a bound on the KL, and that's all you need.",
            "So this just plugs into.",
            "I see."
        ],
        [
            "Eagles bound again.",
            "So I've just recalled everything.",
            "At the top, and this is our version of Seegars bound for this case, so again, all I've done is evaluate the KL here and there are no extra.",
            "Big terms worry bout Alpha is just how Lipschitz my loss function is.",
            "Capper is an upper bound on the kernel, so this can be easily one, and then Lambda is just how much I regularize here.",
            "The only unknown thing again is this this inverse temperature this gamma parameter.",
            "I think that's it.",
            "So."
        ],
        [
            "So some conclusion.",
            "So we basically developed what seemed like.",
            "So sharp risk analysis for this localization idea with Boltzmann priors and posteriors, and had a look at kind of seeing what happens if we try and control function class complexity by adding regularization terms.",
            "And we showed you can actually do this quite easily.",
            "With quite sophisticated way of controlling capacity.",
            "And then we extended those ideas to Gaussian processors and again got looks like quite sharp sharp risk bounds basically.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with Francois Laviolette and John Shaw Taylor.",
                    "label": 0
                },
                {
                    "sent": "And in overview then we study this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Idea that the PAC Bayes prior can be informed by the data generating distribution, so this is cortonese localization.",
                    "label": 0
                },
                {
                    "sent": "An we study this in two settings really.",
                    "label": 0
                },
                {
                    "sent": "So first of all, with Boltzmann distributions as our prior and posterior and in this setting we prove what seems to be quite sharp risk analysis for this setting and then we go on to investigate the possibility of controlling function class complexity in that setting.",
                    "label": 1
                },
                {
                    "sent": "And we show that it's really possible to kind of encode some quite sophisticated assumptions about how we might expect a good hypothesis to interact with the kind of intrinsic geometry defined by the data.",
                    "label": 1
                },
                {
                    "sent": "Going to extend this localization idea to Gaussian processes, so this may be a bit more practical, more practical setting, and really the hallmark of this these ideas is that we obtain really quite a significant reduction in the size of the KL term of the PAC Bayes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so just some really quick preliminaries.",
                    "label": 0
                },
                {
                    "sent": "We have, as usual, some unknown distribution D over our labeled input space.",
                    "label": 0
                },
                {
                    "sent": "We have a sample from that distribution, and we consider, as usual, some class of hypothesis H mapping our input space to some classification space and impact base.",
                    "label": 0
                },
                {
                    "sent": "Then we have.",
                    "label": 0
                },
                {
                    "sent": "Prior P which is not informed by the data sample in a posterior Q distributions over H and just recalling here are a typical PAC Bayes bounds, so this is this is Segers bound, which bands a deviation between the empirical risk in the true risk of our Gibbs classifier and as in France, was talk earlier this size M is just some order root M term.",
                    "label": 1
                },
                {
                    "sent": "So for any distributions Q&P all we need to do to obtain an analysis like this is to bound the KL term.",
                    "label": 0
                },
                {
                    "sent": "And that's typically the dominant term in this kind of analysis, and it can be quite large.",
                    "label": 0
                },
                {
                    "sent": "So if we have.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some non informative prior right, just maybe some Gaussian as we've been discussing today over.",
                    "label": 0
                },
                {
                    "sent": "Over a hypothesis class, then in particular, kind of priors going to assign a large weight to some bad classifiers, right?",
                    "label": 0
                },
                {
                    "sent": "So if we choose in Q2 to contain just what we think are good classifiers, then the overlap between Q&P can be weaken the divergents large.",
                    "label": 0
                },
                {
                    "sent": "So it should say the KL actually should be large.",
                    "label": 0
                },
                {
                    "sent": "And I think the key observation of localization is that P can be informed by the data generating distribution and in particular contain just hypothesis is true risk is small as in this equation and working with a prior like that.",
                    "label": 1
                },
                {
                    "sent": "I mean probably that is unknown, right?",
                    "label": 0
                },
                {
                    "sent": "But the point is, as long as we can find a Q such that we can evaluate or upper bound, the KL between Q&P, then we have a valid PAC Bayes bound for that posterior.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so kind of specializing that too.",
                    "label": 0
                },
                {
                    "sent": "Our interpretation of that then, so we consider just distributions from the extra class of exponential families.",
                    "label": 0
                },
                {
                    "sent": "So here FP&FQ or just some energy functions if you like.",
                    "label": 0
                },
                {
                    "sent": "I'm recording what we've just said.",
                    "label": 0
                },
                {
                    "sent": "All we need to do to obtain a risk analysis for a posterior prior like this is to bound the KL, and in this fairly general setting what we have is this simple lemma which basically says that the KL divergent's can be upper bounded by the deviation between the energy functions.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's the difference in the expected.",
                    "label": 0
                },
                {
                    "sent": "It's the difference between expected deviation in the prior and the expected deviation in the posterior, and this is quite sharp, so the the inequality only actually arises from just one exploitation of the convexity of the negative log function.",
                    "label": 0
                },
                {
                    "sent": "Apart from, that's just simple algebra, it's quite simple.",
                    "label": 0
                },
                {
                    "sent": "So straight away you can see that if we choose F, Q2 estimate FP from the sample, then what we want here is some kind of concentration result for our posterior energy function, and we can actually replace the expectations just with the Supreme and then we just want some kind of uniform convergence result.",
                    "label": 1
                },
                {
                    "sent": "But the kind of lucky thing is that this quantity on the right hand side of this inequality is exactly the kind of quantity that PAC Bayes bounds provider bound for.",
                    "label": 0
                },
                {
                    "sent": "So if we imagine doing that and getting an upper bound for this with some pipe based theory will actually have.",
                    "label": 0
                },
                {
                    "sent": "This quantity upper bound by some term in the KL and we can just take that over to the other side.",
                    "label": 0
                },
                {
                    "sent": "So using this lemma is kind of recursive in that sense.",
                    "label": 0
                },
                {
                    "sent": "And I mean already, you can see that for kind of natural choices of your energy functions, then we're going to expect this KL term to decay, and we're expecting at least one over order one over root M convergence.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So specializing to the first case then.",
                    "label": 0
                },
                {
                    "sent": "So we take these Boltzmann distributions where our risk features as the energy function.",
                    "label": 0
                },
                {
                    "sent": "So in the posterior we have the empirical risk and in the prior the true risk.",
                    "label": 0
                },
                {
                    "sent": "So recalling what we've just said and using the previous lemma or that we need to do in this case is get some concentration result for the empirical risk.",
                    "label": 0
                },
                {
                    "sent": "And well, I mean every pack standard pacbase bound provides that, so we can actually just use Segers bound to obtain straight away this upper bound on the KL and plug that back into Segars bound itself to obtain a version of Segers bound for for this case.",
                    "label": 0
                },
                {
                    "sent": "So this is just a risk bound for.",
                    "label": 1
                },
                {
                    "sent": "This choice of prior and past area.",
                    "label": 0
                },
                {
                    "sent": "And already you can kind of see the advantage over typical methods, right?",
                    "label": 0
                },
                {
                    "sent": "So we've actually done here is upper bound the KL and I mean it seems to be quite small, right?",
                    "label": 0
                },
                {
                    "sent": "There's.",
                    "label": 0
                },
                {
                    "sent": "And in particular, it's decaying with the sample size, and maybe it seems quite strange that there's maybe something funny about this bound in that it doesn't seem that there's any kind of explicit.",
                    "label": 0
                },
                {
                    "sent": "Dependence on any notion of function class complexity in that band.",
                    "label": 0
                },
                {
                    "sent": "It seems to suggest it will given any hypothesis class I can apply this method, I get this bound that I can learn any function class you know equally well.",
                    "label": 0
                },
                {
                    "sent": "And I mean obviously there's something a bit wrong with that.",
                    "label": 0
                },
                {
                    "sent": "So really the kind of question is where is the notion of?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function class complexity in that bound, and I think the idea is that it's kind of encoded in this gamma parameter and this gamma parameter came back.",
                    "label": 0
                },
                {
                    "sent": "One slide was is basically the this inverse temperature parameter of our Boltzmann distributions, so that's really controlling the variance for Boltzmann distributions there, and so the idea is that if H is very rich and contains, then lots of bad classifiers, then we must really choose gamma to be very large.",
                    "label": 1
                },
                {
                    "sent": "Control the empirical risk of the Gibbs classifier, but I have a really rich function class and I choose gamma small.",
                    "label": 0
                },
                {
                    "sent": "I have a large variance Boltzmann distribution and all these bad functions are getting some opportunity to feature in that Gibbs classifier, and so really that controls the empirical rister.",
                    "label": 0
                },
                {
                    "sent": "So maybe there's some kind of knew notion of function class complexity needed here to kind of understand what's going on there, but it's quite subtle 'cause it's kind of dependent on the function class, the algorithm, the data generating distribution.",
                    "label": 1
                },
                {
                    "sent": "And I mean even simple things like what's the relationship with the VC dimension, that kind of thing which are natural questions we haven't really worked out yet.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can develop that idea by going on and attempting to kind of control the function class capacity in some way.",
                    "label": 0
                },
                {
                    "sent": "So here we have the same Boltzmann distributions, but we've added basically regularization term.",
                    "label": 0
                },
                {
                    "sent": "So this FP and FQ now punish complicated hypothesis in some way.",
                    "label": 0
                },
                {
                    "sent": "So far function class has some norm seats in our KHS or something we can just pick the norm in that function class as the regularizer and the first thing to notice here really is that if P = F Q, we just get the same bound, right that recording.",
                    "label": 0
                },
                {
                    "sent": "Again, all we need to do to obtain this analysis is get concentration of this posterior energy function to the prior energy function.",
                    "label": 0
                },
                {
                    "sent": "And with F P = F Q, there's no other convergence to worry about, right?",
                    "label": 0
                },
                {
                    "sent": "So we get the same bound, but the point is, maybe this enables learning with a smaller gamma basically.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we go on to basically do a different kind of regularization.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do here is regularize with respect to the way that we kind of expect hypothesis.",
                    "label": 0
                },
                {
                    "sent": "We're all the way that hypothesis interact with the kind of true geometry of the data, generating distribution and to kind of motivate that idea, there's been a recent theme of machine learning.",
                    "label": 0
                },
                {
                    "sent": "That data defines its own geometry in some way, so it can.",
                    "label": 0
                },
                {
                    "sent": "Occupy some kind of submanifold of the representation space or be a collection of clusters or something like that and that really defines it's kind of its own geometry and it can be quite different to the geometry of kind of inherited from the ambient space, essentially so in this picture.",
                    "label": 1
                },
                {
                    "sent": "For example, if we imagine just some metric in the kind of ambient space, then points A&B will always be very close in this metric, but with respect to the manifold geometry defined by the data there.",
                    "label": 0
                },
                {
                    "sent": "Very different.",
                    "label": 0
                },
                {
                    "sent": "So from a learning theory perspective, if I'm kind of trying to learn on the on data like this under something like a large margin assumption, then it can fail and in situations like that, if I encode that large margin assumption with respect to the wrong kind of geometry so.",
                    "label": 0
                },
                {
                    "sent": "The kind of contention really of this part of the work is that.",
                    "label": 0
                },
                {
                    "sent": "You want to encode those assumptions with respect to the right geometry, right?",
                    "label": 0
                },
                {
                    "sent": "And so the kind of key results in this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Spite of machine learning is that we can actually capture this geometry so we can learn it from random samples.",
                    "label": 0
                },
                {
                    "sent": "So this is like this is kind of the key idea in graph theoretical methods in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Is that given a sample of points from my data generating distribution, I can form a graph on that sample and I can form a smoothness functional like this on that graph.",
                    "label": 0
                },
                {
                    "sent": "So the W here are basically edge weights of this graph which measures similarity between two data points.",
                    "label": 0
                },
                {
                    "sent": "And so this smoothness functional basically measures how smooth the hypothesis is viewed as a function on that graph, and the key result rarely hear of Heine ET al.",
                    "label": 0
                },
                {
                    "sent": "And Johnny was involved in this work is that this converges to a really natural measure of smoothness with respect to that data generating distribution.",
                    "label": 0
                },
                {
                    "sent": "So that's really what we want.",
                    "label": 0
                },
                {
                    "sent": "We want to measure how smooth hypothesis are as they kind of move over the geometry defined by the data.",
                    "label": 0
                },
                {
                    "sent": "So using this kind of method, we can capture that, and that's not really possible without passing to some kind of empirical notion of the geometry of things.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can just plug those ideas basically into our setting.",
                    "label": 0
                },
                {
                    "sent": "So how again the same Boltzmann distributions?",
                    "label": 0
                },
                {
                    "sent": "But here we have.",
                    "label": 0
                },
                {
                    "sent": "The regularizer in the posterior is this smoothness on a graph formed on the sample.",
                    "label": 1
                },
                {
                    "sent": "And in the prior, it's basically the expectation of that oversamples, so again to get a bound to get a pack Bayes risk analysis, all we need to do is obtain some concentration result for this smoothness functional and I should have said earlier that now we can consider like samples with some unlabeled points as well.",
                    "label": 0
                },
                {
                    "sent": "So this unlabeled set can be empty, but.",
                    "label": 0
                },
                {
                    "sent": "Can obviously be helpful to learn the geometry to have extra unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So essentially this smoothness functional is a U statistic 2nd order U statistic.",
                    "label": 1
                },
                {
                    "sent": "So what we actually need there is a PAC Bayes concentration result for the process.",
                    "label": 0
                },
                {
                    "sent": "So you process is just a family of you statistics index by some function class.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what we provide and we do that quite generally then.",
                    "label": 0
                },
                {
                    "sent": "So for any second order use statistic.",
                    "label": 0
                },
                {
                    "sent": "We tried this pipe Bayes bound.",
                    "label": 0
                },
                {
                    "sent": "Basically it looks like any other pacbase bound, but we have some boundedness condition on what's called the kernel of EU statistics.",
                    "label": 0
                },
                {
                    "sent": "That's this F function.",
                    "label": 0
                },
                {
                    "sent": "And just a note about the proof of this.",
                    "label": 0
                },
                {
                    "sent": "So this basically users this really quite general recipe bound for generating PAC Bayes bounds of Schierman ET al that fronts were introduced earlier and it basically reduces obtaining a PAC Bayes bound to evaluating or upper bounding moment generating function, and in this case we can do that with hostings decomposition of you statistics into martingales.",
                    "label": 0
                },
                {
                    "sent": "So there's a kind of Canonical decomposition, and once you've done that, you can use Hovding's lemma recursively, which is just exactly as it's done in the proof of Mcdiarmid's inequality.",
                    "label": 0
                },
                {
                    "sent": "So basically that's all we need.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To obtain our PAC Bayes bound for this setting right?",
                    "label": 1
                },
                {
                    "sent": "So it's really quite simple still and this is again, this is just seegars bound specialized to this case.",
                    "label": 0
                },
                {
                    "sent": "I've just plugged in my bound that I've just obtained on the KL Divergent.",
                    "label": 0
                },
                {
                    "sent": "So just recalled prior and posterior at the top.",
                    "label": 0
                },
                {
                    "sent": "So what I've done here is basically just upper bound, the KL term and again it's seems quite small.",
                    "label": 0
                },
                {
                    "sent": "So A&B are both order one over root M terms, so there's nothing.",
                    "label": 0
                },
                {
                    "sent": "There's no big terms featuring in this.",
                    "label": 1
                },
                {
                    "sent": "There's no kind of complicated terms at all.",
                    "label": 0
                },
                {
                    "sent": "So B is just some bounded condition bounding or function class now and W is the maximum edge weight on our graph and this can be really easily one apart from that.",
                    "label": 0
                },
                {
                    "sent": "Apart from that, the only unknown thing again is this gamma.",
                    "label": 0
                },
                {
                    "sent": "This inverse temperature parameter.",
                    "label": 0
                },
                {
                    "sent": "So I just wanted to kind of emphasize actually that controlling function class complexity in that way with respect to how hypothesis really interact with this geometry.",
                    "label": 1
                },
                {
                    "sent": "It's quite unusual that we can obtain an analysis like that so easily I think, and I think that's really kind of testament to how flexible PAC Bayes is, and this localization idea that you can just encode this in your prior so easily if you try to do this in classical methods, it's really quite difficult.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just finally, I just basically wanted to extend those ideas to.",
                    "label": 0
                },
                {
                    "sent": "The Gaussian process setting.",
                    "label": 0
                },
                {
                    "sent": "So there's a duality between Gaussian processes and RK chess methods.",
                    "label": 0
                },
                {
                    "sent": "So I find the arc HSV a bit easier to motivate things from, so just kind of motivated from that angle, so we just have some kernel on our input space.",
                    "label": 0
                },
                {
                    "sent": "And we form the typical arc HS which functions which are identified as functions on our input space by the normal inner product formula.",
                    "label": 0
                },
                {
                    "sent": "What we do is we try and fit some Gaussian distributions on this arc ahs.",
                    "label": 0
                },
                {
                    "sent": "So our posterior distribution is basically has its mean.",
                    "label": 0
                },
                {
                    "sent": "This mu S which is the.",
                    "label": 0
                },
                {
                    "sent": "Minimizer, some regularised empirical risk, basically.",
                    "label": 0
                },
                {
                    "sent": "So L this risk is the risk with respect to some Alpha Lipschitz convex loss function, so it's just some regularised empirical risk minimizer.",
                    "label": 0
                },
                {
                    "sent": "And in the prior we just take the expectation over that.",
                    "label": 0
                },
                {
                    "sent": "Oversamples and basically the point is that predicting with our Gibbs classifier drawn from this posterior distribution is a Gaussian process on our original space, whose mean is the mean and whose covariance is given by the kernel and the temperature parameter again.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Along the lines of everything that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just done as well.",
                    "label": 0
                },
                {
                    "sent": "All that we need to do to get a risk bound for this case is to basically bound the KL between those two distributions.",
                    "label": 0
                },
                {
                    "sent": "Those two Gaussians, and so that's just the difference between the means.",
                    "label": 0
                },
                {
                    "sent": "So the distance between the means in feature space.",
                    "label": 0
                },
                {
                    "sent": "So in this arc HS and it turns out you can bound this basically using a kind of method of bounded differences.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we basically consider consider 2 sample.",
                    "label": 0
                },
                {
                    "sent": "So our sample in its perturbation.",
                    "label": 0
                },
                {
                    "sent": "And this is just obtained by resampling one point and by a kind of stability argument.",
                    "label": 0
                },
                {
                    "sent": "So this is the idea of algorithmic stability in machine learning you can upper bound the.",
                    "label": 0
                },
                {
                    "sent": "Distance in feature space between your empirical risk minimizer on S and on its perturbation, and then you have what looks like a kind of bounded differences condition.",
                    "label": 0
                },
                {
                    "sent": "So it looks like the kind of condition you have in the arms inequality, for example, but you're in a high dimensional RKHS.",
                    "label": 0
                },
                {
                    "sent": "But it turns out basically there's a version of Azuma's inequality for arbitrarily high dimensional Hilbert spaces, and so you can just apply that to get a bound on the KL, and that's all you need.",
                    "label": 1
                },
                {
                    "sent": "So this just plugs into.",
                    "label": 0
                },
                {
                    "sent": "I see.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eagles bound again.",
                    "label": 0
                },
                {
                    "sent": "So I've just recalled everything.",
                    "label": 0
                },
                {
                    "sent": "At the top, and this is our version of Seegars bound for this case, so again, all I've done is evaluate the KL here and there are no extra.",
                    "label": 0
                },
                {
                    "sent": "Big terms worry bout Alpha is just how Lipschitz my loss function is.",
                    "label": 0
                },
                {
                    "sent": "Capper is an upper bound on the kernel, so this can be easily one, and then Lambda is just how much I regularize here.",
                    "label": 0
                },
                {
                    "sent": "The only unknown thing again is this this inverse temperature this gamma parameter.",
                    "label": 0
                },
                {
                    "sent": "I think that's it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some conclusion.",
                    "label": 0
                },
                {
                    "sent": "So we basically developed what seemed like.",
                    "label": 0
                },
                {
                    "sent": "So sharp risk analysis for this localization idea with Boltzmann priors and posteriors, and had a look at kind of seeing what happens if we try and control function class complexity by adding regularization terms.",
                    "label": 1
                },
                {
                    "sent": "And we showed you can actually do this quite easily.",
                    "label": 0
                },
                {
                    "sent": "With quite sophisticated way of controlling capacity.",
                    "label": 0
                },
                {
                    "sent": "And then we extended those ideas to Gaussian processors and again got looks like quite sharp sharp risk bounds basically.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}