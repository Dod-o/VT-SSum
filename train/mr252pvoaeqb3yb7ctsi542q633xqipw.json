{
    "id": "mr252pvoaeqb3yb7ctsi542q633xqipw",
    "title": "KL Control Theory and Decision Making under Uncertainty",
    "info": {
        "author": [
            "Bert Kappen, Department of Medical Physics and Biophysics, Radboud University Nijmegen"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_kappen_klctdu/",
    "segmentation": [
        [
            "OK, well thank you very much.",
            "I want to thank first of all.",
            "Boss called for helping out this this workshop.",
            "Making it financially possible.",
            "I'm going to talk about Cal control theory, which is basically another name for the linear Bellman equation and decision-making under certainty I wanted to."
        ],
        [
            "Review some some of the results because some people may be new to this, so I thought putting some of that and I wanted to present some new results.",
            "So to make sure, and that's all in 15 minutes, so bear with me.",
            "So the motivation for my original work is that optimal optimal solution of control problem is noise dependent and everybody sort of knows that if there's noise, you have a different approach then if there's no noise and computation is intractable and I think the great news of these linear Bellman equations that"
        ],
        [
            "Even handle on both these issues, and so in my work in 2005 I studied a continuous state in time Gaussian noise, arbitrary rewards in dynamics, additive control, kind of situation, and formulated a log transform which linearized Bellman equation and that ideas goes very actually back to the Schrodinger equation.",
            "If you're familiar with the physics where the shorting equations are linear equation and if you do the log transform you get the Hamilton.",
            "You get the Hamilton Jacobi.",
            "Equation actually, that idea also goes back somehow to Fleming, but nothing much seemed to have happened with that idea at the time in the 70s.",
            "So the idea is that the cost optimal cost to go becomes as you can say, can say is a free energy, which is the cost to go optimal cost ago, not any cost to go optimal.",
            "Costco is a log over some of the trajectories of exponent of something you could call the action, which is the cost of a path which runs from zero to time to T and this could be a continuous or discrete time.",
            "Formulation and it's divided by the noise.",
            "And so some if you have some intuition, is statistical mechanics, then you will know that this noise will control very much a typical behavior of that system, and having a high noise, you get very different solution in low noise.",
            "So that already gives a glimpse of this idea of that you get for different noise, different optimal control solutions.",
            "So there are these phase transitions in this system.",
            "But today I will not have time to go into that, but discussed in that paper.",
            "And the second thing is, is that once you have this formulation, you can think of all kinds of graphical model implementation that goes on to compute your optimal approximate inference, optional control solution, and you can make use of all kind of approximate inference schemes to do that.",
            "So Mo.",
            "Developed this this wonderful idea using this scale notion, which sort of I believe gives gives a handle today's and put it in a more general framework.",
            "At least allow some sort of generalizations, which I think is very interesting, and so we're able today talked a lot about the continuous case I'm going to talk a lot about discrete case today, so we're switching."
        ],
        [
            "Switching roles.",
            "What I want to talk about as an application after I give a little bit of an introduction is opponent modeling.",
            "Which I think is very exciting.",
            "It's about how you can use this kind of idea in a distributed setting where you have multiple agents and where the optimal behavior of the successful behavior of the agent depends on some notion of what the other agents in the environment are doing.",
            "And this is of course relevant for dialogue maintenance.",
            "Foreman machine interfaces for team play.",
            "And this can be either a cooperative antagonist and we'll see how we can handle that in this."
        ],
        [
            "This framework"
        ],
        [
            "So I want to tell you a little bit about the notion of approximately infinite, because it's sort of key in this whole thinking, and the idea is extremely simple, but maybe just to bear with me if I have a probability distribution, I can always write it in this way, and I can equivalently define this probability distribution through a KL, which this scale and just say, well, the function, the probability distribution that minimizes the scale is exactly this probability distribution.",
            "So I can define a probability distribution through a KL and the optimizing.",
            "Probability distribution has a KL which is equal to minus the log partition.",
            "Some of that distribution and that is the starting point for all kind of approximate inference methods where you either approximate.",
            "Now this KL and get get a solution of an approximate Cal like belief propagation or you restrict the minimization to attract local class where you get something like the mean field."
        ],
        [
            "Solutions.",
            "OK, so with that in mind.",
            "We're going to go to the KL control theory, which is, as already pointed out, we have some trajectories which follow a dynamics which we get for free.",
            "There's no cost attached.",
            "We have some rewards and we want to minimize a quantified P which is close to Q, but also is sort of optimizing the reward.",
            "So we get a sum of two terms once they all turn and wanted to state dependent cost term."
        ],
        [
            "And since we have so you can visualize it in this way, so the free dynamics, maybe some sampling that you do in the state space.",
            "So this is time.",
            "This is space.",
            "There is some reward.",
            "He defined it intermediate time.",
            "So this might maybe maybe samples from Q and samples from PR.",
            "The successful ones that actually avoid the obstacles and go go away from the computation is about computing this."
        ],
        [
            "So since the since we want to minimize this, this this control cost and I gave you on the 1st slide.",
            "Is this approximate inference slide?",
            "You can immediately write down the optimal solution, it's just the minimization of this scale where the SSP is proportional to this interaction term and we have to normalize.",
            "Soapy becomes that term with the normalization.",
            "And so once we have that, we can plug it back into the control into the cost and then forget the optimal cost, which then becomes minus the log of the partition sum.",
            "And the partition some.",
            "What is it?",
            "It's a sum of all states of this interaction term, and so it's this quantity.",
            "So with three lines we have derived this whole control this whole path and goal control idea where we have established that the optimal control is the path integral.",
            "In the sense that this is some of it, so the optimal cost to go, sorry is the path integral which is sum over paths weighted by the free, the free dynamics and waited by the exponentiated rewards and.",
            "And there we are.",
            "Now we sold it."
        ],
        [
            "So if we want to move with this dynamics with this control, then we have to compute the marginal what where to be at the first time step, given that this area is times that we are at 0.",
            "So if the computer is marginal in this graphical model and that is nothing else than doing a backwards message, passing in this in this probability distribution and I've written out for you, so you get the first term times the message coming from the future and the message from the future is computed recursively as in the standard way in an inference problem."
        ],
        [
            "So to give a Birds Eye overview, normally you have some dynamics, you do some dynamic programming, some dynamic programming development equation, approximate.",
            "Get your optimal policy here we have said we'll get a restricted class control problems which are identical to a probability distribution and we do approximately inference to get to the optimal control."
        ],
        [
            "So we have applied this to several cases in the past.",
            "Just want to flush it here.",
            "This is a Angel coordination problem where each of the agents has its own dynamics and they have to coordinate their activities to be at the end time in some common state.",
            "And you can use belief propagation to do this hard inference problem.",
            "We have a lot of agents and what happens is that these agents then have to move to some approximate to some average goal location that compute from this.",
            "From this inference problem, and then the solution is back propagated to the zero time which defines the control actions and you can compare here the solution.",
            "As a function of the number of agents for the using something, an exact method like the junction tree and compare it with the belief propagation and you get spectacular."
        ],
        [
            "Here's another thing we did, some blockworld some stacking problem, but let me just go on if you."
        ],
        [
            "Full time.",
            "So in the agent case, I want to discuss the case that we have that control the free dynamics of these agents did couple one dynamics per agent, but the rewards are coupled so we get done for each agent.",
            "Has maybe its own KL term and its own reward term which depends on the probability distribution of the other agent and in the.",
            "In this approach I presume that the other agent is governed by some dynamics which I assume.",
            "To know for myself and then later I'm going to see what to do with that.",
            "So once I assume that for Agent one I assume for agent two and each of them computer solutions.",
            "Um?"
        ],
        [
            "And so I want to discuss this very funny case, which is a cooperative game.",
            "Uh.",
            "We assume that the problem is symmetric, which means that what is stated here and then Agent one assumes some initial opponent model about Agent two, which says some dynamics and then it computes optimal control against it.",
            "Using this scale formulation.",
            "And once you've done that, say well agent two is equal to me, agent two could have done the same.",
            "Therefore agent two could have reached the same dynamics that I just computed.",
            "So let's be my improve his opponent model and therefore I can now compute again anew.",
            "Optimal control against that model, and I can do this recursively, and so this sort of thinking that I think that you think that I think that you think you get a nested belief updating where where the where people call."
        ],
        [
            "Is the level of sophistication, so in other words, since the setup is symmetric, I only have to consider one agent, and at iteration K of this of this sophistication recursion, I compute my new PK plus one.",
            "Given a PK which I computed in the previous iteration depending on my cost and so I get an iteration of my new optimal control solution as a function of my previous optimal control solution, and we can iterate this and try to get."
        ],
        [
            "Fixed point solution here.",
            "So I applied this to the case of wanting games.",
            "Well known stag hunt, so the game is simply described by either get a hair for yourself or get a stack together and our two Nash equilibrium.",
            "And it is a model for human cooperation and Animal Corporation."
        ],
        [
            "And so if you apply it in this case, the probability we apply to the case that was only there's no time horizon for the time being.",
            "We assume Q or trivial to be one, and then we can express the probability over X, which is a bit as just by its mean value in using this formula where X is plus or minus one, and then this equation for PK plus one in terms of PK becomes just an expression in terms of MK plus one in terms of MK.",
            "And it has these very well known familiar form.",
            "Where these constants, Alpha and beta, are just simple linear combinations of the entries of the game matrix.",
            "So and it looks."
        ],
        [
            "This you start with some initial belief about what you think the opponent is going to do, and then you're going to compute your new solution and it's going to.",
            "It's going to converge to some fixed point solution depending on where you where you start.",
            "So if you have some game matrix this there may be multiple solutions.",
            "In this case two solutions, and if you have some other game matrix, samosa, Mother Alpha Beta actually that may be unique solution and so it's in the case that there are two solution.",
            "The solution depends on the initial condition."
        ],
        [
            "So here you see the full space of all the stack hunts here is beta.",
            "Sorry for the small print, here is Alpha and all the games that are defined by Alpha and beta in this quote.",
            "In this in this area are stack hunts and the blue line indicates the region in which the stack hunts.",
            "Using this dynamic has multiple solutions and this the rest remaining.",
            "There is a unique solution and here you see this solutions depicted here you see.",
            "In the case that there is only one solution where across only one time and he is such an example where they cross two times.",
            "Now what are you going to ask?",
            "What are the other things there are other games?",
            "For instance, the games below, here, or prisoner dilemma games and they always have only one.",
            "I want one fixed point you see in the picture there."
        ],
        [
            "So you can reason that that.",
            "That you get multiple solutions depending on the initial condition.",
            "So let's now see how this happens, how this plays out if we do this dynamically where we have a large Verizon time, so we get the same equations PK plus one has to be solved as a function of PK, and this is the solution that you get and so now we have to compute this expected reward which depends on PK.",
            "And this is the reward for me to be in state or futures XT to one, and it's an expectation of all the future states of the opponent.",
            "And so I have to do this some over.",
            "Overall the future state opponent and I can actually do that by some forward."
        ],
        [
            "Forward message passing, so the picture is sort of like this.",
            "This is time zero.",
            "This is the end time I want to compute my dynamics, which is the P1 and I assume about the opponents on P2.",
            "So first of all I have to do with forward given P2.",
            "I can compute this forward sampling and therefore get the expectation for all the future rewards.",
            "And once I have that I can use these beta messages to do the backward propagation to get actually the optimal control solution.",
            "So there's the.",
            "That's the Alpha."
        ],
        [
            "And that you can use that.",
            "And this is the result as a function of the depth of recursion of sophistication going from order two 200.",
            "And you see the solution that develops here as a function of the state of player one against the state of player.",
            "2 Brown means, so the color is the expected cost to go, and Brown means we go for her hair and blue means you go for a stack.",
            "So you see that in some areas you see Corporation developing.",
            "Between these two players as a function of this sophistication, so it means that if you are doing this recursive reasoning, you have better chance to cooperate then if you don't, but you see also some funny things happening that, for instance, here that where I'm at the location, sorry I should have said that this location 14 this is the location of the of the stack, and this is so here and the location of the hair is here around around 5:00, so if I'm if I'm a disposition, I'm on top of the stack, but.",
            "If the other guy is on top of the hair, I'm still going to defect because I think he is not going to come to me to go together with me.",
            "Finding finding a stack.",
            "But you see the same happening in the order in the order.",
            "In the reverse situation, so I'm.",
            "On on top of the.",
            "Stack of of the hair position the rabbit and the the other guy is on top of the stack, but nevertheless I'm not going to go for the STACK solution because I know that he thinks that I think that he think etc.",
            "And therefore we know that we've got a defect.",
            "He's not going to.",
            "He's not going to go for that solution, so that's why it's going to affect an I'm going to effect.",
            "So you get this very complex interaction between these two players in this way.",
            "So this was the.",
            "My talk basically.",
            "I presented some."
        ],
        [
            "Work for passenger goals for non linear quadratic Gaussian problems, and I think it's exciting how to how this work relates the inference in control problems, and I was very excited to see very many examples.",
            "Also this morning, how it?",
            "How do examples of like forward sampling?",
            "That you.",
            "Already in working in various works here, so I think it's very nice to connect in this way.",
            "I think the main supporters of this method is that you can.",
            "That gives rise to very many, possibly a very efficient approximation schemes that can really apply these control problems to larger areas using predominantly particle filtering and MCMC methods.",
            "There's also posed on some robots are over there using these approaches.",
            "Furthermore, using deterministic approaches such as belief propagation or.",
            "Or cluster variation method for to get approximations.",
            "I think the main research issues are the partial observability issue, which has not been worked out and it's something that we should look into and also this is all model based, so the reinforcement learning issue is still very much an open problem."
        ],
        [
            "Regarding the this nested belief, recursion is just a very simple example of nontrivial metal agent reasoning.",
            "There is an extension to moving targets on the poster on the wall where you get much more complex behavior that I've shown here at the talk here.",
            "I think the main issues are two extended work to Antagoniste Nonsymmetric case and also on learning based on actual play.",
            "So with that I would like to.",
            "Thank you.",
            "Yeah.",
            "Steakhouse.",
            "2.",
            "Like a repeated fictitious play.",
            "Or any kind of convergence guarantee for these kind of recursions or beliefs?",
            "Yeah, so these these converge.",
            "2.",
            "Not in 2K in the two player case.",
            "Definitely convert in the end player game.",
            "I'm not sure they will converge because it depends there.",
            "So in a two player game is definitely convergent and play again, you have to think about how to set up the game.",
            "I'm not sure.",
            "Yeah it could be.",
            "There could be cycles actually.",
            "Traditional influence diagram.",
            "When you do message passing message.",
            "Edit utility component.",
            "So here there's just police, right?",
            "So what's the connection with standard infrastructure?",
            "So, um.",
            "In the discrete state case."
        ],
        [
            "Francis, you get a graphical model like here, which runs which is rolled out overtime.",
            "And you could just get you have to compute the marginal at the first time slice, because that marginal is going to tell you what is the probability for exit time is 1 given at exit time zero, which is where you are currently.",
            "So you have to compute that marginal.",
            "So think of it as a smoothing problem.",
            "Is in the time series problem, so just that yeah.",
            "Is that your question?",
            "Or did you take it offline so I'm so the utility?",
            "So are you talking bout the observations?",
            "How to multiply anymore?",
            "Utility functions is somewhere in there.",
            "Didn't see where so utility function.",
            "Messages around."
        ],
        [
            "The utility's are in here, right?",
            "This is the reward.",
            "So if they are, if they if they are some.",
            "If this is a sum over costs or rewards per time, then it's just multiplying it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, well thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I want to thank first of all.",
                    "label": 0
                },
                {
                    "sent": "Boss called for helping out this this workshop.",
                    "label": 0
                },
                {
                    "sent": "Making it financially possible.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about Cal control theory, which is basically another name for the linear Bellman equation and decision-making under certainty I wanted to.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Review some some of the results because some people may be new to this, so I thought putting some of that and I wanted to present some new results.",
                    "label": 0
                },
                {
                    "sent": "So to make sure, and that's all in 15 minutes, so bear with me.",
                    "label": 0
                },
                {
                    "sent": "So the motivation for my original work is that optimal optimal solution of control problem is noise dependent and everybody sort of knows that if there's noise, you have a different approach then if there's no noise and computation is intractable and I think the great news of these linear Bellman equations that",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Even handle on both these issues, and so in my work in 2005 I studied a continuous state in time Gaussian noise, arbitrary rewards in dynamics, additive control, kind of situation, and formulated a log transform which linearized Bellman equation and that ideas goes very actually back to the Schrodinger equation.",
                    "label": 1
                },
                {
                    "sent": "If you're familiar with the physics where the shorting equations are linear equation and if you do the log transform you get the Hamilton.",
                    "label": 0
                },
                {
                    "sent": "You get the Hamilton Jacobi.",
                    "label": 0
                },
                {
                    "sent": "Equation actually, that idea also goes back somehow to Fleming, but nothing much seemed to have happened with that idea at the time in the 70s.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that the cost optimal cost to go becomes as you can say, can say is a free energy, which is the cost to go optimal cost ago, not any cost to go optimal.",
                    "label": 0
                },
                {
                    "sent": "Costco is a log over some of the trajectories of exponent of something you could call the action, which is the cost of a path which runs from zero to time to T and this could be a continuous or discrete time.",
                    "label": 0
                },
                {
                    "sent": "Formulation and it's divided by the noise.",
                    "label": 0
                },
                {
                    "sent": "And so some if you have some intuition, is statistical mechanics, then you will know that this noise will control very much a typical behavior of that system, and having a high noise, you get very different solution in low noise.",
                    "label": 0
                },
                {
                    "sent": "So that already gives a glimpse of this idea of that you get for different noise, different optimal control solutions.",
                    "label": 0
                },
                {
                    "sent": "So there are these phase transitions in this system.",
                    "label": 0
                },
                {
                    "sent": "But today I will not have time to go into that, but discussed in that paper.",
                    "label": 0
                },
                {
                    "sent": "And the second thing is, is that once you have this formulation, you can think of all kinds of graphical model implementation that goes on to compute your optimal approximate inference, optional control solution, and you can make use of all kind of approximate inference schemes to do that.",
                    "label": 0
                },
                {
                    "sent": "So Mo.",
                    "label": 0
                },
                {
                    "sent": "Developed this this wonderful idea using this scale notion, which sort of I believe gives gives a handle today's and put it in a more general framework.",
                    "label": 0
                },
                {
                    "sent": "At least allow some sort of generalizations, which I think is very interesting, and so we're able today talked a lot about the continuous case I'm going to talk a lot about discrete case today, so we're switching.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Switching roles.",
                    "label": 0
                },
                {
                    "sent": "What I want to talk about as an application after I give a little bit of an introduction is opponent modeling.",
                    "label": 1
                },
                {
                    "sent": "Which I think is very exciting.",
                    "label": 0
                },
                {
                    "sent": "It's about how you can use this kind of idea in a distributed setting where you have multiple agents and where the optimal behavior of the successful behavior of the agent depends on some notion of what the other agents in the environment are doing.",
                    "label": 0
                },
                {
                    "sent": "And this is of course relevant for dialogue maintenance.",
                    "label": 1
                },
                {
                    "sent": "Foreman machine interfaces for team play.",
                    "label": 1
                },
                {
                    "sent": "And this can be either a cooperative antagonist and we'll see how we can handle that in this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This framework",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to tell you a little bit about the notion of approximately infinite, because it's sort of key in this whole thinking, and the idea is extremely simple, but maybe just to bear with me if I have a probability distribution, I can always write it in this way, and I can equivalently define this probability distribution through a KL, which this scale and just say, well, the function, the probability distribution that minimizes the scale is exactly this probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So I can define a probability distribution through a KL and the optimizing.",
                    "label": 0
                },
                {
                    "sent": "Probability distribution has a KL which is equal to minus the log partition.",
                    "label": 0
                },
                {
                    "sent": "Some of that distribution and that is the starting point for all kind of approximate inference methods where you either approximate.",
                    "label": 0
                },
                {
                    "sent": "Now this KL and get get a solution of an approximate Cal like belief propagation or you restrict the minimization to attract local class where you get something like the mean field.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solutions.",
                    "label": 0
                },
                {
                    "sent": "OK, so with that in mind.",
                    "label": 0
                },
                {
                    "sent": "We're going to go to the KL control theory, which is, as already pointed out, we have some trajectories which follow a dynamics which we get for free.",
                    "label": 0
                },
                {
                    "sent": "There's no cost attached.",
                    "label": 0
                },
                {
                    "sent": "We have some rewards and we want to minimize a quantified P which is close to Q, but also is sort of optimizing the reward.",
                    "label": 0
                },
                {
                    "sent": "So we get a sum of two terms once they all turn and wanted to state dependent cost term.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And since we have so you can visualize it in this way, so the free dynamics, maybe some sampling that you do in the state space.",
                    "label": 0
                },
                {
                    "sent": "So this is time.",
                    "label": 0
                },
                {
                    "sent": "This is space.",
                    "label": 0
                },
                {
                    "sent": "There is some reward.",
                    "label": 0
                },
                {
                    "sent": "He defined it intermediate time.",
                    "label": 0
                },
                {
                    "sent": "So this might maybe maybe samples from Q and samples from PR.",
                    "label": 0
                },
                {
                    "sent": "The successful ones that actually avoid the obstacles and go go away from the computation is about computing this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So since the since we want to minimize this, this this control cost and I gave you on the 1st slide.",
                    "label": 1
                },
                {
                    "sent": "Is this approximate inference slide?",
                    "label": 0
                },
                {
                    "sent": "You can immediately write down the optimal solution, it's just the minimization of this scale where the SSP is proportional to this interaction term and we have to normalize.",
                    "label": 1
                },
                {
                    "sent": "Soapy becomes that term with the normalization.",
                    "label": 0
                },
                {
                    "sent": "And so once we have that, we can plug it back into the control into the cost and then forget the optimal cost, which then becomes minus the log of the partition sum.",
                    "label": 0
                },
                {
                    "sent": "And the partition some.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "It's a sum of all states of this interaction term, and so it's this quantity.",
                    "label": 1
                },
                {
                    "sent": "So with three lines we have derived this whole control this whole path and goal control idea where we have established that the optimal control is the path integral.",
                    "label": 0
                },
                {
                    "sent": "In the sense that this is some of it, so the optimal cost to go, sorry is the path integral which is sum over paths weighted by the free, the free dynamics and waited by the exponentiated rewards and.",
                    "label": 0
                },
                {
                    "sent": "And there we are.",
                    "label": 0
                },
                {
                    "sent": "Now we sold it.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we want to move with this dynamics with this control, then we have to compute the marginal what where to be at the first time step, given that this area is times that we are at 0.",
                    "label": 0
                },
                {
                    "sent": "So if the computer is marginal in this graphical model and that is nothing else than doing a backwards message, passing in this in this probability distribution and I've written out for you, so you get the first term times the message coming from the future and the message from the future is computed recursively as in the standard way in an inference problem.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to give a Birds Eye overview, normally you have some dynamics, you do some dynamic programming, some dynamic programming development equation, approximate.",
                    "label": 0
                },
                {
                    "sent": "Get your optimal policy here we have said we'll get a restricted class control problems which are identical to a probability distribution and we do approximately inference to get to the optimal control.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have applied this to several cases in the past.",
                    "label": 0
                },
                {
                    "sent": "Just want to flush it here.",
                    "label": 0
                },
                {
                    "sent": "This is a Angel coordination problem where each of the agents has its own dynamics and they have to coordinate their activities to be at the end time in some common state.",
                    "label": 0
                },
                {
                    "sent": "And you can use belief propagation to do this hard inference problem.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of agents and what happens is that these agents then have to move to some approximate to some average goal location that compute from this.",
                    "label": 0
                },
                {
                    "sent": "From this inference problem, and then the solution is back propagated to the zero time which defines the control actions and you can compare here the solution.",
                    "label": 0
                },
                {
                    "sent": "As a function of the number of agents for the using something, an exact method like the junction tree and compare it with the belief propagation and you get spectacular.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another thing we did, some blockworld some stacking problem, but let me just go on if you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Full time.",
                    "label": 0
                },
                {
                    "sent": "So in the agent case, I want to discuss the case that we have that control the free dynamics of these agents did couple one dynamics per agent, but the rewards are coupled so we get done for each agent.",
                    "label": 1
                },
                {
                    "sent": "Has maybe its own KL term and its own reward term which depends on the probability distribution of the other agent and in the.",
                    "label": 0
                },
                {
                    "sent": "In this approach I presume that the other agent is governed by some dynamics which I assume.",
                    "label": 0
                },
                {
                    "sent": "To know for myself and then later I'm going to see what to do with that.",
                    "label": 0
                },
                {
                    "sent": "So once I assume that for Agent one I assume for agent two and each of them computer solutions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I want to discuss this very funny case, which is a cooperative game.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "We assume that the problem is symmetric, which means that what is stated here and then Agent one assumes some initial opponent model about Agent two, which says some dynamics and then it computes optimal control against it.",
                    "label": 1
                },
                {
                    "sent": "Using this scale formulation.",
                    "label": 1
                },
                {
                    "sent": "And once you've done that, say well agent two is equal to me, agent two could have done the same.",
                    "label": 1
                },
                {
                    "sent": "Therefore agent two could have reached the same dynamics that I just computed.",
                    "label": 0
                },
                {
                    "sent": "So let's be my improve his opponent model and therefore I can now compute again anew.",
                    "label": 0
                },
                {
                    "sent": "Optimal control against that model, and I can do this recursively, and so this sort of thinking that I think that you think that I think that you think you get a nested belief updating where where the where people call.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the level of sophistication, so in other words, since the setup is symmetric, I only have to consider one agent, and at iteration K of this of this sophistication recursion, I compute my new PK plus one.",
                    "label": 0
                },
                {
                    "sent": "Given a PK which I computed in the previous iteration depending on my cost and so I get an iteration of my new optimal control solution as a function of my previous optimal control solution, and we can iterate this and try to get.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fixed point solution here.",
                    "label": 0
                },
                {
                    "sent": "So I applied this to the case of wanting games.",
                    "label": 0
                },
                {
                    "sent": "Well known stag hunt, so the game is simply described by either get a hair for yourself or get a stack together and our two Nash equilibrium.",
                    "label": 1
                },
                {
                    "sent": "And it is a model for human cooperation and Animal Corporation.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so if you apply it in this case, the probability we apply to the case that was only there's no time horizon for the time being.",
                    "label": 0
                },
                {
                    "sent": "We assume Q or trivial to be one, and then we can express the probability over X, which is a bit as just by its mean value in using this formula where X is plus or minus one, and then this equation for PK plus one in terms of PK becomes just an expression in terms of MK plus one in terms of MK.",
                    "label": 1
                },
                {
                    "sent": "And it has these very well known familiar form.",
                    "label": 1
                },
                {
                    "sent": "Where these constants, Alpha and beta, are just simple linear combinations of the entries of the game matrix.",
                    "label": 0
                },
                {
                    "sent": "So and it looks.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This you start with some initial belief about what you think the opponent is going to do, and then you're going to compute your new solution and it's going to.",
                    "label": 0
                },
                {
                    "sent": "It's going to converge to some fixed point solution depending on where you where you start.",
                    "label": 0
                },
                {
                    "sent": "So if you have some game matrix this there may be multiple solutions.",
                    "label": 0
                },
                {
                    "sent": "In this case two solutions, and if you have some other game matrix, samosa, Mother Alpha Beta actually that may be unique solution and so it's in the case that there are two solution.",
                    "label": 1
                },
                {
                    "sent": "The solution depends on the initial condition.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here you see the full space of all the stack hunts here is beta.",
                    "label": 0
                },
                {
                    "sent": "Sorry for the small print, here is Alpha and all the games that are defined by Alpha and beta in this quote.",
                    "label": 0
                },
                {
                    "sent": "In this in this area are stack hunts and the blue line indicates the region in which the stack hunts.",
                    "label": 0
                },
                {
                    "sent": "Using this dynamic has multiple solutions and this the rest remaining.",
                    "label": 0
                },
                {
                    "sent": "There is a unique solution and here you see this solutions depicted here you see.",
                    "label": 0
                },
                {
                    "sent": "In the case that there is only one solution where across only one time and he is such an example where they cross two times.",
                    "label": 0
                },
                {
                    "sent": "Now what are you going to ask?",
                    "label": 0
                },
                {
                    "sent": "What are the other things there are other games?",
                    "label": 0
                },
                {
                    "sent": "For instance, the games below, here, or prisoner dilemma games and they always have only one.",
                    "label": 0
                },
                {
                    "sent": "I want one fixed point you see in the picture there.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can reason that that.",
                    "label": 0
                },
                {
                    "sent": "That you get multiple solutions depending on the initial condition.",
                    "label": 0
                },
                {
                    "sent": "So let's now see how this happens, how this plays out if we do this dynamically where we have a large Verizon time, so we get the same equations PK plus one has to be solved as a function of PK, and this is the solution that you get and so now we have to compute this expected reward which depends on PK.",
                    "label": 0
                },
                {
                    "sent": "And this is the reward for me to be in state or futures XT to one, and it's an expectation of all the future states of the opponent.",
                    "label": 0
                },
                {
                    "sent": "And so I have to do this some over.",
                    "label": 0
                },
                {
                    "sent": "Overall the future state opponent and I can actually do that by some forward.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Forward message passing, so the picture is sort of like this.",
                    "label": 0
                },
                {
                    "sent": "This is time zero.",
                    "label": 0
                },
                {
                    "sent": "This is the end time I want to compute my dynamics, which is the P1 and I assume about the opponents on P2.",
                    "label": 0
                },
                {
                    "sent": "So first of all I have to do with forward given P2.",
                    "label": 0
                },
                {
                    "sent": "I can compute this forward sampling and therefore get the expectation for all the future rewards.",
                    "label": 0
                },
                {
                    "sent": "And once I have that I can use these beta messages to do the backward propagation to get actually the optimal control solution.",
                    "label": 0
                },
                {
                    "sent": "So there's the.",
                    "label": 0
                },
                {
                    "sent": "That's the Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that you can use that.",
                    "label": 0
                },
                {
                    "sent": "And this is the result as a function of the depth of recursion of sophistication going from order two 200.",
                    "label": 0
                },
                {
                    "sent": "And you see the solution that develops here as a function of the state of player one against the state of player.",
                    "label": 0
                },
                {
                    "sent": "2 Brown means, so the color is the expected cost to go, and Brown means we go for her hair and blue means you go for a stack.",
                    "label": 0
                },
                {
                    "sent": "So you see that in some areas you see Corporation developing.",
                    "label": 0
                },
                {
                    "sent": "Between these two players as a function of this sophistication, so it means that if you are doing this recursive reasoning, you have better chance to cooperate then if you don't, but you see also some funny things happening that, for instance, here that where I'm at the location, sorry I should have said that this location 14 this is the location of the of the stack, and this is so here and the location of the hair is here around around 5:00, so if I'm if I'm a disposition, I'm on top of the stack, but.",
                    "label": 0
                },
                {
                    "sent": "If the other guy is on top of the hair, I'm still going to defect because I think he is not going to come to me to go together with me.",
                    "label": 0
                },
                {
                    "sent": "Finding finding a stack.",
                    "label": 0
                },
                {
                    "sent": "But you see the same happening in the order in the order.",
                    "label": 0
                },
                {
                    "sent": "In the reverse situation, so I'm.",
                    "label": 0
                },
                {
                    "sent": "On on top of the.",
                    "label": 0
                },
                {
                    "sent": "Stack of of the hair position the rabbit and the the other guy is on top of the stack, but nevertheless I'm not going to go for the STACK solution because I know that he thinks that I think that he think etc.",
                    "label": 0
                },
                {
                    "sent": "And therefore we know that we've got a defect.",
                    "label": 0
                },
                {
                    "sent": "He's not going to.",
                    "label": 0
                },
                {
                    "sent": "He's not going to go for that solution, so that's why it's going to affect an I'm going to effect.",
                    "label": 0
                },
                {
                    "sent": "So you get this very complex interaction between these two players in this way.",
                    "label": 0
                },
                {
                    "sent": "So this was the.",
                    "label": 0
                },
                {
                    "sent": "My talk basically.",
                    "label": 0
                },
                {
                    "sent": "I presented some.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work for passenger goals for non linear quadratic Gaussian problems, and I think it's exciting how to how this work relates the inference in control problems, and I was very excited to see very many examples.",
                    "label": 0
                },
                {
                    "sent": "Also this morning, how it?",
                    "label": 0
                },
                {
                    "sent": "How do examples of like forward sampling?",
                    "label": 0
                },
                {
                    "sent": "That you.",
                    "label": 0
                },
                {
                    "sent": "Already in working in various works here, so I think it's very nice to connect in this way.",
                    "label": 0
                },
                {
                    "sent": "I think the main supporters of this method is that you can.",
                    "label": 0
                },
                {
                    "sent": "That gives rise to very many, possibly a very efficient approximation schemes that can really apply these control problems to larger areas using predominantly particle filtering and MCMC methods.",
                    "label": 0
                },
                {
                    "sent": "There's also posed on some robots are over there using these approaches.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, using deterministic approaches such as belief propagation or.",
                    "label": 0
                },
                {
                    "sent": "Or cluster variation method for to get approximations.",
                    "label": 0
                },
                {
                    "sent": "I think the main research issues are the partial observability issue, which has not been worked out and it's something that we should look into and also this is all model based, so the reinforcement learning issue is still very much an open problem.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regarding the this nested belief, recursion is just a very simple example of nontrivial metal agent reasoning.",
                    "label": 0
                },
                {
                    "sent": "There is an extension to moving targets on the poster on the wall where you get much more complex behavior that I've shown here at the talk here.",
                    "label": 0
                },
                {
                    "sent": "I think the main issues are two extended work to Antagoniste Nonsymmetric case and also on learning based on actual play.",
                    "label": 1
                },
                {
                    "sent": "So with that I would like to.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Steakhouse.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Like a repeated fictitious play.",
                    "label": 0
                },
                {
                    "sent": "Or any kind of convergence guarantee for these kind of recursions or beliefs?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so these these converge.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Not in 2K in the two player case.",
                    "label": 0
                },
                {
                    "sent": "Definitely convert in the end player game.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure they will converge because it depends there.",
                    "label": 0
                },
                {
                    "sent": "So in a two player game is definitely convergent and play again, you have to think about how to set up the game.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah it could be.",
                    "label": 0
                },
                {
                    "sent": "There could be cycles actually.",
                    "label": 0
                },
                {
                    "sent": "Traditional influence diagram.",
                    "label": 0
                },
                {
                    "sent": "When you do message passing message.",
                    "label": 0
                },
                {
                    "sent": "Edit utility component.",
                    "label": 0
                },
                {
                    "sent": "So here there's just police, right?",
                    "label": 0
                },
                {
                    "sent": "So what's the connection with standard infrastructure?",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "In the discrete state case.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Francis, you get a graphical model like here, which runs which is rolled out overtime.",
                    "label": 0
                },
                {
                    "sent": "And you could just get you have to compute the marginal at the first time slice, because that marginal is going to tell you what is the probability for exit time is 1 given at exit time zero, which is where you are currently.",
                    "label": 0
                },
                {
                    "sent": "So you have to compute that marginal.",
                    "label": 0
                },
                {
                    "sent": "So think of it as a smoothing problem.",
                    "label": 0
                },
                {
                    "sent": "Is in the time series problem, so just that yeah.",
                    "label": 0
                },
                {
                    "sent": "Is that your question?",
                    "label": 0
                },
                {
                    "sent": "Or did you take it offline so I'm so the utility?",
                    "label": 0
                },
                {
                    "sent": "So are you talking bout the observations?",
                    "label": 0
                },
                {
                    "sent": "How to multiply anymore?",
                    "label": 0
                },
                {
                    "sent": "Utility functions is somewhere in there.",
                    "label": 0
                },
                {
                    "sent": "Didn't see where so utility function.",
                    "label": 0
                },
                {
                    "sent": "Messages around.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The utility's are in here, right?",
                    "label": 0
                },
                {
                    "sent": "This is the reward.",
                    "label": 0
                },
                {
                    "sent": "So if they are, if they if they are some.",
                    "label": 0
                },
                {
                    "sent": "If this is a sum over costs or rewards per time, then it's just multiplying it.",
                    "label": 0
                }
            ]
        }
    }
}