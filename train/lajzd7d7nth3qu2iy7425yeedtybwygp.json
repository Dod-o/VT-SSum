{
    "id": "lajzd7d7nth3qu2iy7425yeedtybwygp",
    "title": "Characterization of Linkage Based Clustering",
    "info": {
        "author": [
            "David Loker, University of Waterloo"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_loker_clb/",
    "segmentation": [
        [
            "Then we talking about characterization of linkage based algorithms.",
            "This is joint work with Margaret Ackerman and should have been David and we're all from the University of Waterloo."
        ],
        [
            "OK, so just to motivate this talk a little bit so there are a wide variety of clustering algorithms.",
            "As we've seen today, in many many different clustering algorithms, and they often produce very different clusterings even on the same data set.",
            "I'm sure.",
            "I'm sure, as I'm sure you all know, so the question is the reason why we're looking at this is how should a user, so someone who's using these are trying to decide how to cluster their data.",
            "How should they decide what algorithm to use for their particular application?",
            "This is a question where were."
        ],
        [
            "Interested in and so our approach.",
            "Going over now is basically to identify properties that distinguish.",
            "Between the results are different clustering paradigms so.",
            "These properties are just any properties.",
            "These properties we want.",
            "We want a couple of things out of them.",
            "We want them to be first intuitive and user friendly.",
            "Right and also we want them to be useful for clustering or for classifying clustering algorithms.",
            "So the two things we want to properties and then after we have these properties we want so that clustering users can utilize prior knowledge that they have about their specific task to determine which properties that make sense for their particular application.",
            "And then after they've done this, they can use these properties.",
            "To then sort out what clustering algorithm they want to use well clustering, satisfy the properties that are interested in that they can then choose one.",
            "That's specific to their task with their application.",
            "That's our approach to solving this problem."
        ],
        [
            "So some previous work clients Berg proposed a few different properties, lost something in last talk that he said that are called axioms and that was presented in NIPS in 2002 of various clustering functions, and then Reza.",
            "Or was it?",
            "And then David provided a set of properties that characterize single linkage in UI 2009."
        ],
        [
            "So our contributions and what I'm going to go over in this talk is first we're going to propose a couple of properties that are going to uniquely identify linkage based clustering algorithms, so I'm going to define what I mean by that in a minute.",
            "And also we construct a taxonomy of clustering algorithms.",
            "So common clustering algorithms based on those properties and a couple others as well.",
            "And I'll go over that that taxonomy at the very end.",
            "So here's the outline."
        ],
        [
            "We're going to define more.",
            "First, we're going to informal, but they're going later on, give a formal definition of what it is to be a linkage based clustering algorithm that we're going to go over some or three.",
            "In particular clustering properties, so these are what we believe are intuitive clustering properties that are useful for the users.",
            "Then I'm going to present or state the main result and provide a brief sketch of how we prove this result.",
            "And finally, I'm going over the taxonomy of common clustering algorithms using clustering properties and then to conclusions."
        ],
        [
            "So first let's go over the formal set up, and we've seen this a few times, so we have a finite domain set X and the distance function D, which Maps 2 points in X to a real number.",
            "As we've seen that many times, I'm saying a clustering function basically takes as input X&D and a positive integer K, and basically outputs AK partition of your data set X.",
            "So this K partition is what we call the clustering, and we'll refer to it as clustering for its remainder this talk."
        ],
        [
            "So then the linkage based algorithm.",
            "This is more of Indian formal definition.",
            "So basically you start with your clustering of singletons.",
            "We saw this again in the last time.",
            "And then you merge the closest pair of clusters.",
            "And you repeat this until indicate clusters remain.",
            "In this case, you're what you're looking for in K partition.",
            "So the question is, what do we mean exactly?",
            "By the closest pair of clusters.",
            "So an informal definition of between cluster distance.",
            "So basically between cluster distance distances and an extension of the between point distance that applies to subsets of your domain X.",
            "So in particular, looking at the distance between subsets A and subset B subset of X your domain and you want to know the distance between them as an extension of the point distance over your entire domain set.",
            "So the definition here of between clustered in distance is the how you define it.",
            "How you actually calculate it is what distinguishes between the various language based algorithms.",
            "So single linkage complete linkage average linkage.",
            "They all have various variations in their definition of the between cluster distance in terms of the data points in there."
        ],
        [
            "OK. Now I'm going to go over a few properties starting."
        ],
        [
            "Hierarchical clustering, so this is slightly different than the previous talk, and then I'm not combining hierarchical clustering with linkage based.",
            "I'm separating the two of them into two distinct entities I guess.",
            "So first refinement in order to define this, the clustering, see, we say, is a refinement of clustering C prime.",
            "If every cluster in C prime can be made up of union of clusters and see.",
            "So basically you can get from C to C prime by merging clusters.",
            "So then a clustering function.",
            "We say is hierarchical if for every K prime and KKK prime, the K prime clustering is a refinement of the K cluster, so that's for all can keep trying.",
            "OK, so I mean, this is, we've seen hierarchical clustering, so this is fairly intuitive and everyone should be fairly familiar with that so."
        ],
        [
            "I'm going to get onto locality, which is.",
            "Something that we've we're introducing here, so I'm going to go over an example first to show you what I mean, and then I'll get to the exact definition after.",
            "So let's say we have a data set here.",
            "And let's say that this is our for clustering that comes out of our clustering function.",
            "Whatever clustering function that maybe.",
            "And let's say that I want to consider a subset of these clusters, C. The ones on the right.",
            "Let's say this.",
            "Now if I consider then the data points of C by themselves outside of everything, ignoring everything else.",
            "So I'm only taking the points in C and only taking the distance as it applies to those points.",
            "And then I want to run my clustering on these points, given that given that distance.",
            "Just by itself, and I asked for the number of clusters equal to the number of questions that I had in the left.",
            "Clustering function and would be local and this is an example of get to the general if every time you did this you got the same clustering, see.",
            "So we say it's local.",
            "If you see on the left and you run it only on those data points you run across the same clustering function.",
            "Again, only on those data points you end up with the same clustering as you originally had.",
            "Of that data set.",
            "So then the formal definition.",
            "We say that a clustering function F is local.",
            "If for any subset are sub clusters of your clustering.",
            "You can restrict your attention to those data points.",
            "And ask for the number of clusters.",
            "That you got are.",
            "Actually clusters equal to the size of your set C and you end up with the same clustering.",
            "So that's what locality is.",
            "OK, so that's how we define locality."
        ],
        [
            "So which paradigms then just deviate for a second?",
            "Which paradigm satisfy locality?",
            "So many clustering algorithms actually our local K means K median single linkage, average linkage and completely nature are all examples of local clustering algorithms and notably the spectral clustering algorithms ratio cut and normalized cut are not local.",
            "OK.",
            "So none."
        ],
        [
            "To another property outer consistency.",
            "This is based on clients burghs consistency axiom.",
            "So again, I'm going to define it using an example first, so we have a data set and a distance function.",
            "And this is, say, my three clustering my three partition of this data set.",
            "With that my clustering function.",
            "And now let's consider stretching the distances between those clusters while leaving all the distances within each cluster.",
            "The within cluster distances we leave them the same, so we don't change them at all, so this is what we're seeing that are consistent.",
            "If when we run our clustering function Now, we end up with the same clustering as we did originally.",
            "OK.",
            "So we say that are consistent.",
            "We just stretched the distances between the clusters.",
            "So basically it your distance function D prime is equal to original distance function, except you've increased the between cluster distances and then re run your algorithm and you get the same result.",
            "OK, so.",
            "All these, are they all the different clustering."
        ],
        [
            "That satisfy our consistency.",
            "So all the ones that we looked at impact in this in this paper, satisfied after consistency doesn't mean that they all will most like a divisive algorithm, will not but K means K median single linkage, average linkage completely, and the spectral clustering algorithms ratio cut and normalized cut all satisfied.",
            "Outer consistency.",
            "OK.",
            "The metric.",
            "No, it doesn't need to know."
        ],
        [
            "OK, so now I'm going to present our main result's theorem so.",
            "Basically what we're saying in this in this result is that if you have a clustering function that is out are consistent.",
            "We see that clustering function is linkage based if and only if.",
            "It is hierarchical and local.",
            "So in particular, it's a.",
            "It defines linkage based, so if its outer consistent hierarchical and locals linkage base and vice versa.",
            "So there are two directions then to this and I'm going to go over easy one first."
        ],
        [
            "So one of these directions, any linkage function based clustering function is hierarchical and local.",
            "You don't even need the outer consistency part of things to get this to hold.",
            "If all hierarchical follows straight from the definition of look what linkage based is and local you can do just using.",
            "A contradiction example.",
            "So we're going to go."
        ],
        [
            "Into more of a sketch about the interesting direction, the proof from first I'm going to talk about it.",
            "So if F is that are consistent, local an hierarchical.",
            "Then F is linkage base.",
            "So to prove this direction, we first have to formalize linkage based clustering a little more by formally defining what we mean by between cluster distance."
        ],
        [
            "So I'm going over what we expect from between cluster distance.",
            "First, probably obviously we expected to extend the pointwise distance, so we want it to depend on the underlying distances between the points.",
            "2nd, we expect that the distance between subsets A&B are independent.",
            "Of the data outside of the of these two clusters.",
            "Of these two subsets.",
            "So just to illustrate if you have these data points.",
            "And this was your clustering.",
            "And this was A and B status.",
            "Arbitrarily then the distances between A&B is not depend whatsoever on the other clusters of the other data points, so distances between them.",
            "And finally, we usually assume no ties.",
            "So if you have distance between A&B equal the distance between C&D if and only if a B = C D. OK.",
            "So that's what we would expect from between cluster distance.",
            "So now we can."
        ],
        [
            "Go over sketch of the proof.",
            "So to recall, we're talking about if F is outer consistent, local, an hierarchical, then F is linkage based.",
            "So our goal then.",
            "Is given D / X.",
            "We want to find a between cluster distance D bar.",
            "Let's say that Maps.",
            "Pairs of subsets to a real number.",
            "Such that the following algorithm outputs the same clustering as F. OK, so first you start with the clustering singletons.",
            "You merge two clusters that minimize the bar and you repeat until only K clusters remain and that will give you the same clustering as your original clustering functionality.",
            "That's our goal.",
            "So given any app we wanted, we want to construct this.",
            "How many questions are there?"
        ],
        [
            "So to continue, I say we have four subsets of X, ABC and D. We'll say that a B is less than CD, so we're defining operator less than operator ABES less than CD.",
            "If there exists a data set X prime as a subset of X.",
            "Such that when we apply the clustering function, so after we apply it, your original clustering function 2X prime A&B are merged before CMD, so less than operator can think of.",
            "We're just trying to make to try to say when a enbies merge before CMD one does it merge.",
            "So there just has to exist some ex prime such this occurs.",
            "And what we have to do here is we have to prove that the less than operator is an ordering by proving that is both antisymmetric and transitive.",
            "And after we do that, we use the ordering to construct the bar.",
            "And we can do that because we have a finite space.",
            "Basically, we're trying to construct the between cluster distances.",
            "That's going to give us the same output as the original algorithm in a linkage based setting.",
            "So that's the sketch of the proof, and I'm going to go over the taxonomy of common clustering algorithms using our properties and go over a few points that are in there.",
            "So here's the big chart of of the different."
        ],
        [
            "Clustering properties and clustering algorithms and where they hold and don't hold a check mark holds X does not.",
            "So I didn't talk about inner consistency, it's the other half of consistency.",
            "Essentially, you're allowed to.",
            "Change the within cluster distance is making them smaller, but you're not changing the between cluster distances.",
            "And order invariants so.",
            "You can change the distance function as long as you prefer preserve.",
            "The order the relative ordering of the distances between points.",
            "So then single linkage satisfies all of them."
        ],
        [
            "But in particular, all the linkage based algorithms, obviously by our results, obviously is going to be local ever consistent in hierarchical we showed, and in particular anyone that satisfies these three is also going to be language based."
        ],
        [
            "One interesting one.",
            "Average linkage is not order invariant, so you if you change any of the distances and you don't even violate the relative ordering, it's still not going to.",
            "It might not give you the same clustering."
        ],
        [
            "Inner consistency, average linkage incomplete linkage failed.",
            "So the stability thing you were talking about earlier.",
            "Actually, they're not in are consistent, so it's very unlikely that they can't do that.",
            "They're not going to be stable."
        ],
        [
            "And also spectral clustering as I went over earlier don't satisfy locality."
        ],
        [
            "And in conclusion, just to recap.",
            "So we introduced new properties locality, in particular of clustering algorithms, and we define hierarchical clustering in a way that's detached from linkage based algorithms themselves.",
            "We showed that our consistent clustering algorithm is linkage based if and only if it is hierarchical and local.",
            "So we characterize linkage based algorithms and we classified common clustering algorithms using these properties in a couple others.",
            "Is it?",
            "Thank you very much.",
            "Question.",
            "Counterexample to show that completely kitsch is not.",
            "I have it in my email.",
            "I can't, I found the example about few months ago.",
            "I can't remember the exact sample, but basically it had.",
            "Only five vertices, I think in basically you just.",
            "The distances were all very close to one another, so that when you ended up shrinking one of them, you made such a huge difference.",
            "Anyway, I can email it to you for sure.",
            "Yeah, I can't escape me right now, exactly.",
            "And then it ended up applying to average languages, while I found it for complete linkage in it.",
            "Which language filters, yeah?",
            "Example of looking at property in an application where you say application.",
            "I actually went.",
            "Anything?",
            "So I think that would be the domain experts that would choose your properties, right?",
            "And then they would go and look at the algorithm to satisfy the properties that they want.",
            "So earlier we heard from the invited Speaker from Yahoo and he was saying that you know he clusters and he looks at then at some subset of the points and then clusters that because it's easier to do well.",
            "He's kind of implicitly assuming that he has a hierarchical structure to his data.",
            "In order to make some kind of meaning from that right?",
            "So he knows that that's what he wants.",
            "And he might also know that he wants locality like these are things so.",
            "I really can't say what the person is going to put.",
            "I'm leaving them to make that decision for themselves.",
            "Distance.",
            "So this past distances is a explained.",
            "You have to pause and this the past.",
            "This is between them is the loudest jump.",
            "The minimal largest sample from overpasses you can think of situations in which you say this is really what I care about.",
            "If I want to cluster species according to their DNA and I want to say how likely it is that these two species came from the same region and I look at the edit distance, then it may be related to what is the largest jump that they have to make in creating a chain that transfers this DNA to this day.",
            "So in such a.",
            "Application it may be natural to assume that the past this is what you care about.",
            "This is one of the properties that we.",
            "Questions.",
            "So I had one I think you said for this to work, your input space is finite, right?",
            "Yes, so your proof works for that.",
            "Do you know that it doesn't work if it's not?",
            "But they can't that it can't.",
            "You can't have the claim that you have.",
            "Now the.",
            "He's talking about the case where your domain says is infinite, yes?",
            "The problem is, well, maybe I'm misunderstanding here, right?",
            "So when you say domain set, is that like the input space from which things are drawn?",
            "Or you just have a finite number of samples?",
            "Samples.",
            "Examples your point.",
            "Can you say anything about the generalization?",
            "Statement.",
            "Characterized.",
            "Practicing the linkage seems to be pretty, so that would be probably well.",
            "I mean a property with single language does bad in right, in which case of practitioner was trying to decide whether it's you single language would have to say well is my data noisy?",
            "And if it is then single linkage was maybe if that's one of their top key priorities.",
            "That data is really noisy than single language is something he wants to clear.",
            "Yeah, so this is linkage based in general, not just single linkage, so maybe the average is better for you in that case, right?",
            "But maybe Joes implicitly asking, you know this is some more abstracted property that captures this noise internal problem.",
            "Potentially.",
            "Yes, I don't know for sure.",
            "That would be another Rd your table.",
            "Yeah exactly.",
            "Thank you very much, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we talking about characterization of linkage based algorithms.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Margaret Ackerman and should have been David and we're all from the University of Waterloo.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to motivate this talk a little bit so there are a wide variety of clustering algorithms.",
                    "label": 1
                },
                {
                    "sent": "As we've seen today, in many many different clustering algorithms, and they often produce very different clusterings even on the same data set.",
                    "label": 0
                },
                {
                    "sent": "I'm sure.",
                    "label": 0
                },
                {
                    "sent": "I'm sure, as I'm sure you all know, so the question is the reason why we're looking at this is how should a user, so someone who's using these are trying to decide how to cluster their data.",
                    "label": 1
                },
                {
                    "sent": "How should they decide what algorithm to use for their particular application?",
                    "label": 0
                },
                {
                    "sent": "This is a question where were.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interested in and so our approach.",
                    "label": 0
                },
                {
                    "sent": "Going over now is basically to identify properties that distinguish.",
                    "label": 1
                },
                {
                    "sent": "Between the results are different clustering paradigms so.",
                    "label": 1
                },
                {
                    "sent": "These properties are just any properties.",
                    "label": 0
                },
                {
                    "sent": "These properties we want.",
                    "label": 0
                },
                {
                    "sent": "We want a couple of things out of them.",
                    "label": 0
                },
                {
                    "sent": "We want them to be first intuitive and user friendly.",
                    "label": 1
                },
                {
                    "sent": "Right and also we want them to be useful for clustering or for classifying clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "So the two things we want to properties and then after we have these properties we want so that clustering users can utilize prior knowledge that they have about their specific task to determine which properties that make sense for their particular application.",
                    "label": 1
                },
                {
                    "sent": "And then after they've done this, they can use these properties.",
                    "label": 0
                },
                {
                    "sent": "To then sort out what clustering algorithm they want to use well clustering, satisfy the properties that are interested in that they can then choose one.",
                    "label": 0
                },
                {
                    "sent": "That's specific to their task with their application.",
                    "label": 0
                },
                {
                    "sent": "That's our approach to solving this problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some previous work clients Berg proposed a few different properties, lost something in last talk that he said that are called axioms and that was presented in NIPS in 2002 of various clustering functions, and then Reza.",
                    "label": 0
                },
                {
                    "sent": "Or was it?",
                    "label": 0
                },
                {
                    "sent": "And then David provided a set of properties that characterize single linkage in UI 2009.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our contributions and what I'm going to go over in this talk is first we're going to propose a couple of properties that are going to uniquely identify linkage based clustering algorithms, so I'm going to define what I mean by that in a minute.",
                    "label": 0
                },
                {
                    "sent": "And also we construct a taxonomy of clustering algorithms.",
                    "label": 1
                },
                {
                    "sent": "So common clustering algorithms based on those properties and a couple others as well.",
                    "label": 0
                },
                {
                    "sent": "And I'll go over that that taxonomy at the very end.",
                    "label": 0
                },
                {
                    "sent": "So here's the outline.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to define more.",
                    "label": 0
                },
                {
                    "sent": "First, we're going to informal, but they're going later on, give a formal definition of what it is to be a linkage based clustering algorithm that we're going to go over some or three.",
                    "label": 0
                },
                {
                    "sent": "In particular clustering properties, so these are what we believe are intuitive clustering properties that are useful for the users.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to present or state the main result and provide a brief sketch of how we prove this result.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'm going over the taxonomy of common clustering algorithms using clustering properties and then to conclusions.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first let's go over the formal set up, and we've seen this a few times, so we have a finite domain set X and the distance function D, which Maps 2 points in X to a real number.",
                    "label": 1
                },
                {
                    "sent": "As we've seen that many times, I'm saying a clustering function basically takes as input X&D and a positive integer K, and basically outputs AK partition of your data set X.",
                    "label": 0
                },
                {
                    "sent": "So this K partition is what we call the clustering, and we'll refer to it as clustering for its remainder this talk.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then the linkage based algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is more of Indian formal definition.",
                    "label": 0
                },
                {
                    "sent": "So basically you start with your clustering of singletons.",
                    "label": 1
                },
                {
                    "sent": "We saw this again in the last time.",
                    "label": 0
                },
                {
                    "sent": "And then you merge the closest pair of clusters.",
                    "label": 1
                },
                {
                    "sent": "And you repeat this until indicate clusters remain.",
                    "label": 0
                },
                {
                    "sent": "In this case, you're what you're looking for in K partition.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what do we mean exactly?",
                    "label": 0
                },
                {
                    "sent": "By the closest pair of clusters.",
                    "label": 1
                },
                {
                    "sent": "So an informal definition of between cluster distance.",
                    "label": 1
                },
                {
                    "sent": "So basically between cluster distance distances and an extension of the between point distance that applies to subsets of your domain X.",
                    "label": 1
                },
                {
                    "sent": "So in particular, looking at the distance between subsets A and subset B subset of X your domain and you want to know the distance between them as an extension of the point distance over your entire domain set.",
                    "label": 0
                },
                {
                    "sent": "So the definition here of between clustered in distance is the how you define it.",
                    "label": 0
                },
                {
                    "sent": "How you actually calculate it is what distinguishes between the various language based algorithms.",
                    "label": 0
                },
                {
                    "sent": "So single linkage complete linkage average linkage.",
                    "label": 0
                },
                {
                    "sent": "They all have various variations in their definition of the between cluster distance in terms of the data points in there.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Now I'm going to go over a few properties starting.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hierarchical clustering, so this is slightly different than the previous talk, and then I'm not combining hierarchical clustering with linkage based.",
                    "label": 0
                },
                {
                    "sent": "I'm separating the two of them into two distinct entities I guess.",
                    "label": 0
                },
                {
                    "sent": "So first refinement in order to define this, the clustering, see, we say, is a refinement of clustering C prime.",
                    "label": 1
                },
                {
                    "sent": "If every cluster in C prime can be made up of union of clusters and see.",
                    "label": 0
                },
                {
                    "sent": "So basically you can get from C to C prime by merging clusters.",
                    "label": 0
                },
                {
                    "sent": "So then a clustering function.",
                    "label": 0
                },
                {
                    "sent": "We say is hierarchical if for every K prime and KKK prime, the K prime clustering is a refinement of the K cluster, so that's for all can keep trying.",
                    "label": 1
                },
                {
                    "sent": "OK, so I mean, this is, we've seen hierarchical clustering, so this is fairly intuitive and everyone should be fairly familiar with that so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to get onto locality, which is.",
                    "label": 0
                },
                {
                    "sent": "Something that we've we're introducing here, so I'm going to go over an example first to show you what I mean, and then I'll get to the exact definition after.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have a data set here.",
                    "label": 0
                },
                {
                    "sent": "And let's say that this is our for clustering that comes out of our clustering function.",
                    "label": 0
                },
                {
                    "sent": "Whatever clustering function that maybe.",
                    "label": 0
                },
                {
                    "sent": "And let's say that I want to consider a subset of these clusters, C. The ones on the right.",
                    "label": 0
                },
                {
                    "sent": "Let's say this.",
                    "label": 0
                },
                {
                    "sent": "Now if I consider then the data points of C by themselves outside of everything, ignoring everything else.",
                    "label": 0
                },
                {
                    "sent": "So I'm only taking the points in C and only taking the distance as it applies to those points.",
                    "label": 0
                },
                {
                    "sent": "And then I want to run my clustering on these points, given that given that distance.",
                    "label": 0
                },
                {
                    "sent": "Just by itself, and I asked for the number of clusters equal to the number of questions that I had in the left.",
                    "label": 0
                },
                {
                    "sent": "Clustering function and would be local and this is an example of get to the general if every time you did this you got the same clustering, see.",
                    "label": 0
                },
                {
                    "sent": "So we say it's local.",
                    "label": 0
                },
                {
                    "sent": "If you see on the left and you run it only on those data points you run across the same clustering function.",
                    "label": 0
                },
                {
                    "sent": "Again, only on those data points you end up with the same clustering as you originally had.",
                    "label": 0
                },
                {
                    "sent": "Of that data set.",
                    "label": 0
                },
                {
                    "sent": "So then the formal definition.",
                    "label": 0
                },
                {
                    "sent": "We say that a clustering function F is local.",
                    "label": 1
                },
                {
                    "sent": "If for any subset are sub clusters of your clustering.",
                    "label": 0
                },
                {
                    "sent": "You can restrict your attention to those data points.",
                    "label": 0
                },
                {
                    "sent": "And ask for the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "That you got are.",
                    "label": 0
                },
                {
                    "sent": "Actually clusters equal to the size of your set C and you end up with the same clustering.",
                    "label": 0
                },
                {
                    "sent": "So that's what locality is.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how we define locality.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So which paradigms then just deviate for a second?",
                    "label": 1
                },
                {
                    "sent": "Which paradigm satisfy locality?",
                    "label": 0
                },
                {
                    "sent": "So many clustering algorithms actually our local K means K median single linkage, average linkage and completely nature are all examples of local clustering algorithms and notably the spectral clustering algorithms ratio cut and normalized cut are not local.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So none.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To another property outer consistency.",
                    "label": 1
                },
                {
                    "sent": "This is based on clients burghs consistency axiom.",
                    "label": 0
                },
                {
                    "sent": "So again, I'm going to define it using an example first, so we have a data set and a distance function.",
                    "label": 0
                },
                {
                    "sent": "And this is, say, my three clustering my three partition of this data set.",
                    "label": 0
                },
                {
                    "sent": "With that my clustering function.",
                    "label": 0
                },
                {
                    "sent": "And now let's consider stretching the distances between those clusters while leaving all the distances within each cluster.",
                    "label": 0
                },
                {
                    "sent": "The within cluster distances we leave them the same, so we don't change them at all, so this is what we're seeing that are consistent.",
                    "label": 0
                },
                {
                    "sent": "If when we run our clustering function Now, we end up with the same clustering as we did originally.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we say that are consistent.",
                    "label": 1
                },
                {
                    "sent": "We just stretched the distances between the clusters.",
                    "label": 0
                },
                {
                    "sent": "So basically it your distance function D prime is equal to original distance function, except you've increased the between cluster distances and then re run your algorithm and you get the same result.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "All these, are they all the different clustering.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That satisfy our consistency.",
                    "label": 0
                },
                {
                    "sent": "So all the ones that we looked at impact in this in this paper, satisfied after consistency doesn't mean that they all will most like a divisive algorithm, will not but K means K median single linkage, average linkage completely, and the spectral clustering algorithms ratio cut and normalized cut all satisfied.",
                    "label": 1
                },
                {
                    "sent": "Outer consistency.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The metric.",
                    "label": 0
                },
                {
                    "sent": "No, it doesn't need to know.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm going to present our main result's theorem so.",
                    "label": 0
                },
                {
                    "sent": "Basically what we're saying in this in this result is that if you have a clustering function that is out are consistent.",
                    "label": 0
                },
                {
                    "sent": "We see that clustering function is linkage based if and only if.",
                    "label": 1
                },
                {
                    "sent": "It is hierarchical and local.",
                    "label": 1
                },
                {
                    "sent": "So in particular, it's a.",
                    "label": 0
                },
                {
                    "sent": "It defines linkage based, so if its outer consistent hierarchical and locals linkage base and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So there are two directions then to this and I'm going to go over easy one first.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of these directions, any linkage function based clustering function is hierarchical and local.",
                    "label": 1
                },
                {
                    "sent": "You don't even need the outer consistency part of things to get this to hold.",
                    "label": 0
                },
                {
                    "sent": "If all hierarchical follows straight from the definition of look what linkage based is and local you can do just using.",
                    "label": 0
                },
                {
                    "sent": "A contradiction example.",
                    "label": 0
                },
                {
                    "sent": "So we're going to go.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into more of a sketch about the interesting direction, the proof from first I'm going to talk about it.",
                    "label": 0
                },
                {
                    "sent": "So if F is that are consistent, local an hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Then F is linkage base.",
                    "label": 1
                },
                {
                    "sent": "So to prove this direction, we first have to formalize linkage based clustering a little more by formally defining what we mean by between cluster distance.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going over what we expect from between cluster distance.",
                    "label": 1
                },
                {
                    "sent": "First, probably obviously we expected to extend the pointwise distance, so we want it to depend on the underlying distances between the points.",
                    "label": 1
                },
                {
                    "sent": "2nd, we expect that the distance between subsets A&B are independent.",
                    "label": 0
                },
                {
                    "sent": "Of the data outside of the of these two clusters.",
                    "label": 1
                },
                {
                    "sent": "Of these two subsets.",
                    "label": 1
                },
                {
                    "sent": "So just to illustrate if you have these data points.",
                    "label": 0
                },
                {
                    "sent": "And this was your clustering.",
                    "label": 0
                },
                {
                    "sent": "And this was A and B status.",
                    "label": 0
                },
                {
                    "sent": "Arbitrarily then the distances between A&B is not depend whatsoever on the other clusters of the other data points, so distances between them.",
                    "label": 0
                },
                {
                    "sent": "And finally, we usually assume no ties.",
                    "label": 0
                },
                {
                    "sent": "So if you have distance between A&B equal the distance between C&D if and only if a B = C D. OK.",
                    "label": 0
                },
                {
                    "sent": "So that's what we would expect from between cluster distance.",
                    "label": 0
                },
                {
                    "sent": "So now we can.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go over sketch of the proof.",
                    "label": 0
                },
                {
                    "sent": "So to recall, we're talking about if F is outer consistent, local, an hierarchical, then F is linkage based.",
                    "label": 1
                },
                {
                    "sent": "So our goal then.",
                    "label": 0
                },
                {
                    "sent": "Is given D / X.",
                    "label": 0
                },
                {
                    "sent": "We want to find a between cluster distance D bar.",
                    "label": 0
                },
                {
                    "sent": "Let's say that Maps.",
                    "label": 0
                },
                {
                    "sent": "Pairs of subsets to a real number.",
                    "label": 0
                },
                {
                    "sent": "Such that the following algorithm outputs the same clustering as F. OK, so first you start with the clustering singletons.",
                    "label": 1
                },
                {
                    "sent": "You merge two clusters that minimize the bar and you repeat until only K clusters remain and that will give you the same clustering as your original clustering functionality.",
                    "label": 0
                },
                {
                    "sent": "That's our goal.",
                    "label": 0
                },
                {
                    "sent": "So given any app we wanted, we want to construct this.",
                    "label": 0
                },
                {
                    "sent": "How many questions are there?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to continue, I say we have four subsets of X, ABC and D. We'll say that a B is less than CD, so we're defining operator less than operator ABES less than CD.",
                    "label": 0
                },
                {
                    "sent": "If there exists a data set X prime as a subset of X.",
                    "label": 1
                },
                {
                    "sent": "Such that when we apply the clustering function, so after we apply it, your original clustering function 2X prime A&B are merged before CMD, so less than operator can think of.",
                    "label": 0
                },
                {
                    "sent": "We're just trying to make to try to say when a enbies merge before CMD one does it merge.",
                    "label": 0
                },
                {
                    "sent": "So there just has to exist some ex prime such this occurs.",
                    "label": 0
                },
                {
                    "sent": "And what we have to do here is we have to prove that the less than operator is an ordering by proving that is both antisymmetric and transitive.",
                    "label": 1
                },
                {
                    "sent": "And after we do that, we use the ordering to construct the bar.",
                    "label": 0
                },
                {
                    "sent": "And we can do that because we have a finite space.",
                    "label": 0
                },
                {
                    "sent": "Basically, we're trying to construct the between cluster distances.",
                    "label": 0
                },
                {
                    "sent": "That's going to give us the same output as the original algorithm in a linkage based setting.",
                    "label": 0
                },
                {
                    "sent": "So that's the sketch of the proof, and I'm going to go over the taxonomy of common clustering algorithms using our properties and go over a few points that are in there.",
                    "label": 0
                },
                {
                    "sent": "So here's the big chart of of the different.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clustering properties and clustering algorithms and where they hold and don't hold a check mark holds X does not.",
                    "label": 1
                },
                {
                    "sent": "So I didn't talk about inner consistency, it's the other half of consistency.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you're allowed to.",
                    "label": 0
                },
                {
                    "sent": "Change the within cluster distance is making them smaller, but you're not changing the between cluster distances.",
                    "label": 0
                },
                {
                    "sent": "And order invariants so.",
                    "label": 0
                },
                {
                    "sent": "You can change the distance function as long as you prefer preserve.",
                    "label": 0
                },
                {
                    "sent": "The order the relative ordering of the distances between points.",
                    "label": 0
                },
                {
                    "sent": "So then single linkage satisfies all of them.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in particular, all the linkage based algorithms, obviously by our results, obviously is going to be local ever consistent in hierarchical we showed, and in particular anyone that satisfies these three is also going to be language based.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One interesting one.",
                    "label": 0
                },
                {
                    "sent": "Average linkage is not order invariant, so you if you change any of the distances and you don't even violate the relative ordering, it's still not going to.",
                    "label": 0
                },
                {
                    "sent": "It might not give you the same clustering.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inner consistency, average linkage incomplete linkage failed.",
                    "label": 1
                },
                {
                    "sent": "So the stability thing you were talking about earlier.",
                    "label": 0
                },
                {
                    "sent": "Actually, they're not in are consistent, so it's very unlikely that they can't do that.",
                    "label": 0
                },
                {
                    "sent": "They're not going to be stable.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also spectral clustering as I went over earlier don't satisfy locality.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in conclusion, just to recap.",
                    "label": 0
                },
                {
                    "sent": "So we introduced new properties locality, in particular of clustering algorithms, and we define hierarchical clustering in a way that's detached from linkage based algorithms themselves.",
                    "label": 0
                },
                {
                    "sent": "We showed that our consistent clustering algorithm is linkage based if and only if it is hierarchical and local.",
                    "label": 1
                },
                {
                    "sent": "So we characterize linkage based algorithms and we classified common clustering algorithms using these properties in a couple others.",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Counterexample to show that completely kitsch is not.",
                    "label": 0
                },
                {
                    "sent": "I have it in my email.",
                    "label": 0
                },
                {
                    "sent": "I can't, I found the example about few months ago.",
                    "label": 0
                },
                {
                    "sent": "I can't remember the exact sample, but basically it had.",
                    "label": 0
                },
                {
                    "sent": "Only five vertices, I think in basically you just.",
                    "label": 0
                },
                {
                    "sent": "The distances were all very close to one another, so that when you ended up shrinking one of them, you made such a huge difference.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I can email it to you for sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I can't escape me right now, exactly.",
                    "label": 0
                },
                {
                    "sent": "And then it ended up applying to average languages, while I found it for complete linkage in it.",
                    "label": 0
                },
                {
                    "sent": "Which language filters, yeah?",
                    "label": 0
                },
                {
                    "sent": "Example of looking at property in an application where you say application.",
                    "label": 0
                },
                {
                    "sent": "I actually went.",
                    "label": 0
                },
                {
                    "sent": "Anything?",
                    "label": 0
                },
                {
                    "sent": "So I think that would be the domain experts that would choose your properties, right?",
                    "label": 0
                },
                {
                    "sent": "And then they would go and look at the algorithm to satisfy the properties that they want.",
                    "label": 0
                },
                {
                    "sent": "So earlier we heard from the invited Speaker from Yahoo and he was saying that you know he clusters and he looks at then at some subset of the points and then clusters that because it's easier to do well.",
                    "label": 0
                },
                {
                    "sent": "He's kind of implicitly assuming that he has a hierarchical structure to his data.",
                    "label": 0
                },
                {
                    "sent": "In order to make some kind of meaning from that right?",
                    "label": 0
                },
                {
                    "sent": "So he knows that that's what he wants.",
                    "label": 0
                },
                {
                    "sent": "And he might also know that he wants locality like these are things so.",
                    "label": 0
                },
                {
                    "sent": "I really can't say what the person is going to put.",
                    "label": 0
                },
                {
                    "sent": "I'm leaving them to make that decision for themselves.",
                    "label": 0
                },
                {
                    "sent": "Distance.",
                    "label": 0
                },
                {
                    "sent": "So this past distances is a explained.",
                    "label": 0
                },
                {
                    "sent": "You have to pause and this the past.",
                    "label": 0
                },
                {
                    "sent": "This is between them is the loudest jump.",
                    "label": 0
                },
                {
                    "sent": "The minimal largest sample from overpasses you can think of situations in which you say this is really what I care about.",
                    "label": 0
                },
                {
                    "sent": "If I want to cluster species according to their DNA and I want to say how likely it is that these two species came from the same region and I look at the edit distance, then it may be related to what is the largest jump that they have to make in creating a chain that transfers this DNA to this day.",
                    "label": 0
                },
                {
                    "sent": "So in such a.",
                    "label": 0
                },
                {
                    "sent": "Application it may be natural to assume that the past this is what you care about.",
                    "label": 0
                },
                {
                    "sent": "This is one of the properties that we.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "So I had one I think you said for this to work, your input space is finite, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, so your proof works for that.",
                    "label": 0
                },
                {
                    "sent": "Do you know that it doesn't work if it's not?",
                    "label": 0
                },
                {
                    "sent": "But they can't that it can't.",
                    "label": 0
                },
                {
                    "sent": "You can't have the claim that you have.",
                    "label": 0
                },
                {
                    "sent": "Now the.",
                    "label": 0
                },
                {
                    "sent": "He's talking about the case where your domain says is infinite, yes?",
                    "label": 0
                },
                {
                    "sent": "The problem is, well, maybe I'm misunderstanding here, right?",
                    "label": 0
                },
                {
                    "sent": "So when you say domain set, is that like the input space from which things are drawn?",
                    "label": 0
                },
                {
                    "sent": "Or you just have a finite number of samples?",
                    "label": 0
                },
                {
                    "sent": "Samples.",
                    "label": 0
                },
                {
                    "sent": "Examples your point.",
                    "label": 0
                },
                {
                    "sent": "Can you say anything about the generalization?",
                    "label": 0
                },
                {
                    "sent": "Statement.",
                    "label": 0
                },
                {
                    "sent": "Characterized.",
                    "label": 0
                },
                {
                    "sent": "Practicing the linkage seems to be pretty, so that would be probably well.",
                    "label": 0
                },
                {
                    "sent": "I mean a property with single language does bad in right, in which case of practitioner was trying to decide whether it's you single language would have to say well is my data noisy?",
                    "label": 0
                },
                {
                    "sent": "And if it is then single linkage was maybe if that's one of their top key priorities.",
                    "label": 0
                },
                {
                    "sent": "That data is really noisy than single language is something he wants to clear.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is linkage based in general, not just single linkage, so maybe the average is better for you in that case, right?",
                    "label": 0
                },
                {
                    "sent": "But maybe Joes implicitly asking, you know this is some more abstracted property that captures this noise internal problem.",
                    "label": 0
                },
                {
                    "sent": "Potentially.",
                    "label": 0
                },
                {
                    "sent": "Yes, I don't know for sure.",
                    "label": 0
                },
                {
                    "sent": "That would be another Rd your table.",
                    "label": 0
                },
                {
                    "sent": "Yeah exactly.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much, thank you.",
                    "label": 0
                }
            ]
        }
    }
}