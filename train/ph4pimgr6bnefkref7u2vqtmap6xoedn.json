{
    "id": "ph4pimgr6bnefkref7u2vqtmap6xoedn",
    "title": "Text Classification",
    "info": {
        "author": [
            "William Cohen, Carnegie Mellon University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "September 2006",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/mlas06_cohen_tc/",
    "segmentation": [
        [
            "Alright, so this is a tutorial introduction to text classification and a lot of what we're going to be talking about has to do with text and classification of text is sort of one of the fundamental things, so this is kind of an ambitious tutorial or kind of a lot of slides, and we'll see how many of these are actually kind of get through.",
            "Ever want to feel comfortable breaking in and asking questions?",
            "There's no point sitting here.",
            "It's pretty small crowd.",
            "There's no point sitting here if if something is.",
            "Not clear you should.",
            "You should certainly raise your hand and and."
        ],
        [
            "And ask me, OK?",
            "So basically there are going to be 2 parts of this tutorial, so I'm going to start out kind of slow.",
            "I'm going to talk about what is text classification?",
            "What are the applications of it, and then I'll talk about some of the really basic techniques and methods for text classification.",
            "Will start out with Naive Bayes, which is very simple practical generative model.",
            "And then I'm going to talk about some other models, some discriminative models for learning how to classify text, which are also very effective and very practical and fast.",
            "Alright, so then I'm going to move to some more advanced topics, so I'm going to talk about other types of classification task, which I guess are less well understood from a research perspective.",
            "And that's basically the plan."
        ],
        [
            "So.",
            "Let's start out with the definition of what text classification is.",
            "OK, so when you're classifying objects, you have an input which is the object you want to classify, and the output is a predicted class.",
            "You know.",
            "So maybe good or bad, high or low from some label from some fixed set that you've determined in advance.",
            "So the experience that you're learning from here is a set of examples with their associated correct classification, right?",
            "And what you're learning is a function.",
            "That Maps you from.",
            "From an input X, which in this case is a document 2."
        ],
        [
            "And output Y, which is the class you want to associate with that.",
            "So there are lots and lots of examples of this, so one that's been pretty well studied is classifying news stories.",
            "So maybe you want to classify the stories that pop up in some news aggregator, like Google News and put them into categories like World, US business and so on.",
            "Another topic which has been investigated a lot.",
            "This classification of technical articles in biomedicine.",
            "There's an enormous number of biomedical articles that are published.",
            "There's a database of 14 million of these that you can look at, and one way that people make these more accessible to scientists is to classify them according to content area in a very large classification scheme classification scheme.",
            "Sort of like the Dewey Decimal System or Library of Congress system.",
            "So what you basically you'd be adding terms to abstracts in this large set.",
            "And the terms would talk about things like conscious sedation EO 3250.",
            "So if anyone's suffering conscious sedation right now, it could just be lunch or it could be EO 3250.",
            "Any other categories instances of this particular problem, like classifying movie reviews as favorable or unfavorable, classifying jokes is funny, not funny.",
            "So there's lots of things that problems that satisfy this basic interface.",
            "A document in a class."
        ],
        [
            "Doubt now, technically these have different characteristics we're going to start by talking about one of the best study of these things, which is classification of news articles by the topic that's associated with them.",
            "And there's a very well studied benchmark that contains several thousand Newswire stories are classified in about 90 categories.",
            "One of the categories is exemplified here.",
            "There are things about wheat or grain shipments, so here's an example article.",
            "Not very interesting categories you get here are grain and wheat, so there are 93 binary classes that get associated with these articles and this all came out of a project that Reuters had many years ago where they wanted to automate indexing system for their."
        ],
        [
            "Articles, so let's start out with how do you?",
            "Automate this process so the first thing you have to do is you have to choose a representation for the object you're going to.",
            "You're going to classify.",
            "So what we have here is we have a long string, right?",
            "That's the original representation of a document.",
            "How do you actually want to represent it?",
            "For classification, well, the simplest useful reference."
        ],
        [
            "Notation is basically to represent it as a list of words.",
            "So in English language, particularly when you're looking at, you know the central topics about that.",
            "A document is about looking at the words gives you a pretty good idea so.",
            "You start out by taking the individual words in here, and there might be a few refinements.",
            "So for example, very common words like Andonov tend to not give you a lot of information about the topic of a document.",
            "And it's often the case that things like you know registration versus registrations, oilseed versus oilseeds, things like plurals and other morphological variants can sort of be easily compressed, so these are these are two things that you almost always do when you're looking at topical text classification.",
            "There are some other things I want to talk about in a second cliffs and multi."
        ],
        [
            "Occurrences into one, but for now let's ignore that.",
            "Alright, let's just consider a document as a list of words, right?",
            "So here's a very simple way that you can build a classifier.",
            "So the idea is for every class Y. Alright, so we've got for example grain.",
            "We're going to consider this a binary classification.",
            "I want to build a probabilistic model of what the documents X look like, given that the classes grain going to build a model of the documents in this category.",
            "OK, so.",
            "What's the probability that I get this particular list of things given that the category is wheat?",
            "Or what's the probability that I get this particular list of documents given that the category is?",
            "Non wheat.",
            "OK so I'm going to build this for the probabilistic model and then to classify if I want to classify a document in these binary categories of wheat or non wheat.",
            "I want to find the class we torn on which is most likely to generate X according to this probabilistic model.",
            "OK, so basically want to find out the most.",
            "The most probable why the why?",
            "That has the highest score for the probability of X given Y and also we also want to consider the prior probability of why?",
            "So if a category is very unlikely to occur, then we want to somehow penalize it, so this is a very natural thing to do.",
            "Alright, so how do we do this?",
            "Well, we have to build this probabilistic model.",
            "How do you?",
            "How do you describe the probability of generating this particul?"
        ],
        [
            "List of words given a category.",
            "Alright, so the simplest useful process again is the file.",
            "When you pick some word according to some probability of generating a single word, and then you repeat that same process for the next word in the next word in the next word.",
            "So you're ignoring the order, so each word is generated independently of all the other words.",
            "So we're learning a lot of the structure of this document alright, and what you end up with is if you do this, if you make this rather naive assumption that the order of the words doesn't matter.",
            "Is the probability of this list of words is basically the product of the probabilities of each word given?",
            "Why so now?",
            "How do I model this?",
            "Um?"
        ],
        [
            "Again.",
            "The simplest.",
            "Useful thing to do.",
            "Is to estimate this probability by just looking at the counts in the data, so you can look at the number of times you saw that word in the category devoted by the total number of times you saw any word in the category, right?",
            "So if a word like oilseed occurs very often in the green category, then that would have a higher weight than a word like maybe.",
            "Stocks, which maybe when the curve at frequently in that category.",
            "Alright.",
            "So this has a little bit of a problem, because if you run into a word that you've never seen, and your training data, it will get a score of 0, which means that the probability of that class is going to be 0, and the probability of the complement of the class will also be 0.",
            "'cause you just never seen that word.",
            "So very common thing to do is to put a little bit of smoothing in here, so we put on a, wait an extra value here on the top and on the bottom.",
            "And a typical example is M might be one and P might be 1/2, let's say.",
            "So this is the.",
            "This is the formula we're going to use for estimating the probability of a word given a class.",
            "So this is called the multinomial distribution.",
            "So it's a fancy term for very simple idea, and this particular use of M&P is called a Dirichlet prior."
        ],
        [
            "Again, a fancy name for very simple thing.",
            "So if we use these counts of plus one and plus .5 which notice that means that basically the probabilities go to 1/2 if I've never seen a word before.",
            "OK, so that basically means that the score for a particular class given a list of words is going to be.",
            "I especially if we're going to work in log space, which is.",
            "In probabilistic models like this, almost always more convenient becausw the actual values get very close to 0 very quickly.",
            "Then we can end up where we end up with by scoring everything by this particular formula, right?",
            "Which you can see is very simple and very easy to implement, and the only things you really need to keep track of is the number of times each word appears in, the number of words that are in each category.",
            "So this is a very very simple algorithm, and it turns out to work quite well.",
            "We got our little Dursley prior here, OK?",
            "And we only need the other nice thing about this is what I'm scoring a particular document, but I'm looking at the score for a class wheat or non wheat.",
            "I only need to look at the accounts of the words that actually appear in the document.",
            "Alright, so accumulating counts for many many different words.",
            "Alright, I'm looking at a large collection, say 20,000 news articles.",
            "I have counts for thousands of words, but each of these counts are going to be stored in a very sparse way, right?",
            "So we only need to worry bout counts for the words that actually occur.",
            "Or in the classes that they actually occur in the total number of words for each class, so I can actually do this quite efficiently, even if there are a lot of classes in that there are a lot."
        ],
        [
            "Words.",
            "So this is a pretty common technique.",
            "It's quite likely that one of you.",
            "It's quite likely each of you have actually used one of these things.",
            "If ever used any learning system that has an adaptive spam filter, you know Mail reader where you can sort of classify things as junk, and it remembers that.",
            "Most of these work using some variation of 90s naive Bayes.",
            "So first paper that was published on this was like in the late 90s, nineteen 98.",
            "Some guys at Microsoft and there were some other papers just around the same time that talked about using Naive Bayes for spam filtering, and it works fairly well, or at least it did work fairly well until people started getting the idea of of.",
            "Changing the words that were in the message by using variants that look a little bit different so you know instead of saying you know free, they might say Fr 3 three or something like."
        ],
        [
            "That so.",
            "That was in 1998, so five years later, there was basically a huge slew of email filters that you could download.",
            "A lot of these are based on Naive Bayes.",
            "You can sort of tell sometimes from the names, so a lot of people were playing with these things."
        ],
        [
            "This is a one thing which was in the SourceForge repository was one of the top downloaded things for quite awhile, so this is kind of cool, so we've got a machine learning program based on research ideas from a few years ago.",
            "That is."
        ],
        [
            "Is already sort of in wide use, so.",
            "Naive Bayes has got some really nice things.",
            "It's very fast.",
            "It's very easy to implement.",
            "It's very well understood formally, in the sense that there's this nice probabilistic model which you can use to justify the algorithm.",
            "OK, and people have been using it for a long, long time, so it's fairly well understood experimentally.",
            "Now the bad things about Naive Bayes is it usually if you think about the problem or if you look at alternative algorithms.",
            "You can often find other albums that work a little bit better, so you're paying for a little bit a little bit for this simplicity in.",
            "Ease of implementation.",
            "Another problem is 1.",
            "Would really like these probabilities to be good estimates, so if someone if naive Bayes tells you all the probability that sweet is .7, you'd really like to be able to believe that probability.",
            "So you could pass it along and use it in some other system in later stage of the processing.",
            "But in fact those don't tend to be accurate just because assumptions made in the model are so simple.",
            "So in practice the probabilities tend to."
        ],
        [
            "Very, very close to zero or one, so they're not not very useful in general.",
            "So that's the first step.",
            "That's the warmup.",
            "Now we're going to talk about some other learning methods.",
            "Which, like naive Bayes are relatively fast, simple to implement and fairly practical."
        ],
        [
            "For for text classification methods, so let me go back to this question of how we represent text.",
            "Alright, so I talked about a couple of things like removing stopwords and stemming words.",
            "Another very common thing you're going to do is to collapse the multiple occurrences of words into into some single figure.",
            "Alright, so just to motivate that recall in the Naive Bayes algorithm we looked at a list of words and each word was generated independently.",
            "Which basically meant the contribution to the score was insensitive to the position you get the same contribution to get the same score.",
            "If you took the words an reverse them or shuffle them randomly.",
            "So since the order doesn't matter, maybe we don't need to save the order, so rather than saving the words a list, we can just say the words or the stems that appear in the document and then the number of times they appear.",
            "So that's basically all the information that's in this document.",
            "Um, modulo the fact that you know here there's an order of the words in here."
        ],
        [
            "It's not OK, right?",
            "So this is a somewhat simpler way of representing the document, and if you look at this particular.",
            "Picture of the document where essentially what I've done is I've I've put in bold and made a little bit larger.",
            "The words that are most frequent, and if you kind of close your eyes, or maybe if you're in the back of the room, you don't even need to do that.",
            "You can sort of guess black.",
            "It probably tell what that document is about without looking at all the other words without knowing what the order is, right?",
            "So it sort of seems like a reasonable thing to do for topical classification.",
            "It sort of seems reasonable.",
            "You should be able to look at this information and make a categorization decision.",
            "OK, so technically.",
            "It's nice to think about this.",
            "Representation, so internally what you would represent in the computer is basically some sort of hashtable, which gives you the word in the frequency of each word.",
            "Alright, but mathematically it's nice to think about this as a very long sparse vector.",
            "OK, where the dimensions of the vector are the entire vocabulary, not just the words that appear in the document, but they all 300,000 words that are used in English OK, and.",
            "Each the ice component of the vector is the frequency of the ice word.",
            "Alright, so conceptually you can think of this as a very long vector where most of the entries are zero.",
            "So mathematically it's much more convenient.",
            "So as I go through the rest of these algorithms, that's the that's the mathematical representation I'm going to use, but under the hood inside the computer, what we're really going to be looking at is this sort of word frequency."
        ],
        [
            "Hyper representation OK, so a little bit of history now.",
            "OK, the first really serious experimental look at text classification was David Lewises thesis at the University of Massachusetts back in 1992.",
            "OK, so as computer science goes, Anna's machine learning goes.",
            "1992 is not that long ago.",
            "So just.",
            "For the purpose of comparison, first machine learning paper talking about classification of.",
            "Of you know, vector data that I know of that's widely cited.",
            "I'm sure there are other ones that go that earlier than that was Fisher's 1936 paper that talked about the linear discriminate, so that was in 1936, so David Lewis did this text categorization stuff, and it was 50 years later.",
            "So why 50 years later?",
            "Why didn't people look at text classification before before that?",
            "Given that there are so many different applications of it?",
            "Well, one thing is just we had to get computers that were ready to handle that.",
            "Rob, right, so in a typical text classification problem, and here's one that you know I had a while back with your own singer.",
            "So we have 300,000 documents.",
            "They're all pretty short, but still there were 67,000 words and it turned out to do.",
            "We do a little bit better if we used 4 grams, so we had like 3 million 4 grams.",
            "So we're looking at 3300 thousand documents and 3 million features.",
            "OK, so computationally there's there's a cost and storing these things and and.",
            "Are working with them even with efficient methods like Naive Bayes, but there's also a conceptual problem so.",
            "In Fishers data he was classifying Flowers based on four measurements.",
            "OK, so every example had four numbers and there was a belief that as you increase the number of dimensions, the problem got intrinsically harder.",
            "OK, so here we're talking about learning with millions and millions of features.",
            "OK, and typically more features and we have examples.",
            "So how on Earth can network so from a theoretical point of view, it seems like it shouldn't work OK. And actually, even in 1992 it wasn't clear why it ought to work.",
            "But since then we sort of come to understood why text classification can work and does work, and they're basically several different pieces to the puzzle.",
            "So for efficiency.",
            "You don't need to represent.",
            "You know this matrix of 300,000 documents and 3 million features by representing.",
            "With the matrix that's 319 thousand rows and 3 million columns, where every value is explicitly represented.",
            "So by using sparse matrices you can represent this stuff inside a computer without too much work.",
            "So for efficiency you need to use sparse representations, But the other important observation was if you're using simple classifiers that have wide margins, then you can theoretically showed that the curse of dimensionality is not going to."
        ],
        [
            "Back and bite you.",
            "So I'm just going to do a little bit of the intuitions behind these observations.",
            "So what's the idea here?",
            "So we've got some examples, is just in 2 dimensional space, right?",
            "But I've got some positive examples and some negative examples down here, so this line right here is put in the middle of these things.",
            "OK. And of course there are lots of different lines you can draw between these.",
            "I'm just drawing a few here, right?",
            "So what's special about this particular line?",
            "Will?",
            "Nice thing about this particular line is, there's nothing even close to it.",
            "Alright, so it's far away from all the data points.",
            "Alright, so the width of this blue buffer area is called the margin for that particular hyperplane.",
            "OK, so the number of features in learning does matter.",
            "OK, but it doesn't matter if the margin is wide and the examples are somehow close to the origin."
        ],
        [
            "Alright, so let's make that a little bit more concrete.",
            "OK, so here's a very, very simple algorithm.",
            "Again, this is perhaps simpler than naive Bayes.",
            "OK, so I'm just going to assume I've got two classes.",
            "I'll call the positive one and negative one OK. And I'm going to start with a hypothesis, which is just the all zero vector, and every time I get an example which remember is just a sparse vector, I'm going to look at the inner product of the example.",
            "With this all zero vector.",
            "OK, and then I'll look at the sign of that so the sign will either be positive or negative.",
            "Or I guess zero.",
            "The first time around.",
            "OK if that's the right sign then I will basically just incremental counter.",
            "So everyone of these V sub case that I have every hypothesis is going to have a counter which will reflect the number of times it made a correct prediction, right?",
            "If it's not right then I'm just going to take the existing example vector and I'm going to add the example that it got wrong or.",
            "If it was positive in the example should have been negative, it's going to subtract that example.",
            "Alright, so we're going to throw away the old old hypothesis we're going to increment, create a new hypothesis just by adding in the example, or subtracting it, and then I'll start with a new counter for this new hypothesis.",
            "OK, so this is a very, very simple algorithm.",
            "If you think about it, kind of makes sense, right?",
            "So if you around when you either too high or too low, so let's move things in the right direction, and we're going to move them with the example alright.",
            "And when we're done, we're going to classify by taking all these visas.",
            "Kesan we're going to vote them, and the number of votes each piece of K is going to get is the number of times it was correct.",
            "OK, so you know hypothesis that persists for a long time, gets lots of things right?",
            "It's going to have a high weight.",
            "They have an example of just sort of transient.",
            "It will get a pretty low weight.",
            "Alright, So what can you say about this?",
            "Alright, so it turns out that you can fairly easily say alright.",
            "If every example is close to the origin, So what does that mean?",
            "There's some number R the radius so that the norm of the vector is less than R. OK, it's just the two norm of the vector, so it's within distance R. The origin.",
            "OK, alright, and there's some classifier you OK, so think of one of these V sub K, so if you can't classify these things at all into space then there's nothing we can say about.",
            "But there is something OK, alright, that's not too large.",
            "In fact I want to say the norm is less than or equal to 1.",
            "OK, and Furthermore it gets every example right?",
            "OK and not only does it get every example right?",
            "OK, if we look at this.",
            "This number here.",
            "So this is the inner product of you and X. Alright, so the prediction would be the sign of that.",
            "OK if we multiply that by the actual value, which is either plus one or minus one.",
            "OK, so if this was plus I multiplied by plus then I should get a positive number.",
            "If it's negative and multiply it by minus one, I'll get a positive number.",
            "OK right?",
            "So basically saying it's greater than zero is saying that prediction was correct, saying it's greater than some number.",
            "Delta means that it was correct and there's a margin of Delta.",
            "OK, so if there is some you that's not too large and gets the right answer for every example, OK, no words.",
            "If there is a situation like this, picture here where there's a wide margin.",
            "OK, then this particular algorithm is going to make a relatively small number of mistakes.",
            "OK, in the process of covering these examples.",
            "OK, so the number of mistakes depends on the radius and the inverse of the margin.",
            "OK, so since it's making very relatively few mistakes, it basically means it's going to get most of these predictions correct in this online process.",
            "Alright.",
            "OK, so let's just kind of take these mathematical things and translate them into ordinary English for text.",
            "OK, so suppose you have text.",
            "Let's forget the frequencies for a second.",
            "Let's assume you just have binary feature zero or one.",
            "OK, so if that document has every document has no more than 100 words, then obviously it's two norm is less than 100.",
            "OK, it's less than distance 100 from the origin, so immediately we get our radius by just saying the documents are short.",
            "OK, and as I pointed out before, this thing right here basically means the margin is at least Delta.",
            "OK, so this is a really nice result, and for anyone that like wants to think about these margins and really understand them, I recommend you look at this paper."
        ],
        [
            "And look at the proof.",
            "This is the whole proof.",
            "You could actually look at this in the slide when you go home.",
            "I'm not going to step through it right, but it's something that you can really put all in your head at once, and it's worth doing because it really gives you intuition as to why."
        ],
        [
            "By this curse of dimensionality doesn't hurt you here.",
            "OK, So what are the lessons of this?",
            "Well, the voted Perceptrons are very simple algorithm.",
            "Actually.",
            "Later on I'll show you some experiments that show you that in fact it's pretty competitive in a lot of situations alright.",
            "But basically what it shows you is that you can make few mistakes incrementally learning.",
            "OK, alright, provided there's some you that small and has a large margin.",
            "OK, so here's an idea.",
            "Alright, this is kind of seems like a neat idea, but it seems like kind of a silly way of.",
            "Coming up with that approximating this number, you why don't I just look for that directly?",
            "Alright, well, I just like to open my books on optimization and see if I can figure out how to get that you correct directly, and in fact, that's what you do when you're building a support vector machine.",
            "OK, so with support vector machine you minimize the norm of you subject to some fixed margin, or Alternatively the other way around you maximize the margin relative to some bound on the norm.",
            "OK, so their math optimization."
        ],
        [
            "Methods that let you do this.",
            "I won't go into the details, but they tend to work pretty well.",
            "So again, some nomenclature here.",
            "So again, this is a very simple idea, but there's some fancy term, so when you're looking at that picture, I should have like copied it here, but the picture we've got that big broad blue stripe with the hyperplane in the middle.",
            "OK, the X is the examples that actually touch that margin are called support vectors.",
            "That's why it's called a support vector machine.",
            "And it turns out you can write this this you can rewrite this formula.",
            "The sign of some linear vector.",
            "You an inner product of you and X you can rewrite that.",
            "As a weighted sum of the inner product of the support vector at X, so another way of thinking about this support vector is basically a some sort of fancy weighted nearest neighbor.",
            "OK, so you're looking at the distance the inner product between XI and X is a sort of distance between these two and you're waiting the different examples differently.",
            "And finally you're taking the sign of all that.",
            "OK, so this is another way of thinking about the support vector machine, and one thing that's nice about this formulation is that these.",
            "Inner product's right here can replace with more general class of distance functions called kernels, so that gives you some additional flexibility in how the algorithm behaves, 'cause you can change these distance functions and in fact support vector machines are one of the standard techniques for using for handling topical text classification techniques.",
            "These are sort of the.",
            "You know the brand X method that you always try and beat in a research paper."
        ],
        [
            "These are sort of the baseline competitive methods here again, or some kind of old results, but these were from a very influential paper back in 1998, so this is naive Bayes and these are on 10 of the categories from this Reuters data that I showed you.",
            "OK, and there's a bunch of different variants of support vector machine based on different distance functions, But the simplest distance function is basically just an inner product.",
            "OK, and you know the micro averaged.",
            "Accuracy here, which is some way of measuring the average over these ten categories is 84.2 OK, whereas for Naive Bayes is 72.0, so you can sort of see we're getting a little bit of extra performance for the extra work we're doing in this optimization.",
            "OK, so.",
            "In this picture there are a couple of other algorithms here which also do pretty well, so there's roquillo here.",
            "There's K nearest neighbor.",
            "I won't talk about C 4.5, but I will talk briefly about these other two alright, so these other two algorithms are simpler algorithms that are based on a particular representation for documents, and the representation is just a variant of the."
        ],
        [
            "So this vector that I showed you OK so.",
            "We started out with a vector where each component was the frequency of a word.",
            "OK, so it's a number like 12345, right so?",
            "The idea in this representation is to change these numbers to represent some sort of statistical weight on the word.",
            "Some measure of the importance of that word.",
            "So a formula which is a rather ad hoc looking formula but seems to work quite well, is to wait words higher if they're infrequent.",
            "OK, so if their words that appear infrequently in the document, so it only appears in you know 10 documents in the entire collection, then it should have a high weight, whereas it's a word that appears many times in the document, will have a lower weight.",
            "Alright, and this is, you know, by extension things that appear frequently enough are stop words.",
            "You can just throw them away alright, the other thing you want to do is you want to increase the weight of words that are frequent, and it turns out taking those things and pass him through a log seems to work better than just using the frequency directly.",
            "So this is just a formula for changing the weights of these documents, so kind of common thing to do is to use these weights and then we scale all the vectors so they have length one.",
            "Alright, so this is called the TF IDF representation.",
            "It is a very simple idea."
        ],
        [
            "And it turns out to be a very surprisingly useful thing in a lot of different situations.",
            "So this is an old trick from information retrieval.",
            "To give you some examples, here's another very, very simple algorithm.",
            "So given an example of vector, you want to classify, just find the K closest things, vectors that are nearest to it.",
            "In other words, have the closest largest inner product.",
            "And then we're going to predict by basically looking at a weighted sum of these vectors.",
            "OK, we're going to find the class, so that if I look at all the things that have the class and some of the distances, it'll be sorry the inner product is really similarity.",
            "Some of the similarities that will be the highest score.",
            "OK, so it's close to a lot of positive things and far away from the negative.",
            "Things will give it positive weight.",
            "Otherwise we'll give it negative way.",
            "Alright, so that's K nearest neighbor in TF IDF space.",
            "Alright?",
            "Another algorithm, maybe even a little simpler is will take all the positive examples and will add 'em up.",
            "And we'll take all the negative examples and will subtract him off, and we'll put a couple of weights in here.",
            "Alpha beta equals one.",
            "It's not a bad idea.",
            "And then we'll just take that as our classifier.",
            "So if you like, we're looking at things that are close to the center of the positive examples and far."
        ],
        [
            "From the center of the negative examples.",
            "Alright, so these two algorithms are roquillo an K nearest neighbor.",
            "In TF IDF space, and you see those are actually doing quite well also, so there there are micro average performances up close to 80 or a little above.",
            "So these are some other competitive algorithms.",
            "One of the nice things about the K nearest neighbor algorithm.",
            "Well actually 2 nice things about the K nearest neighbor algorithm, so getting the neighbors the close neighbors of a document sounds like it might be a hard thing to do OK, but in fact that's exactly what an IR search engine is designed to do.",
            "So traditional ranked retrieval engines basically try and find things that are most similar in this TF IDF space.",
            "So finding these neighbors if you have a search engine lying around, just a question of making one call, the other thing that's nice about this is when you find these nearest neighbors.",
            "And add up these similarities.",
            "You end up with scores for every class, and it doesn't matter how many classes you have, you'll still get scores for every class.",
            "So this is a very efficient thing to do if you have, say, 100 classes or."
        ],
        [
            "2000 classes solar.",
            "Here's a one last thing which shows it TF IDF is sort of a surprisingly useful thing to do.",
            "So a couple of years ago, I think in 2000 two or three Jason running at MIT.",
            "Did the paperwork evaluate naive Bayes with TF IDF weighting on all the vectors, which from our probabilistic POV doesn't really make any sense whatsoever, right?",
            "And the other little trick is he used it on the complement of the data rather than original class.",
            "But it's kind of a little detail.",
            "So this is multinomial naive Bayes and this is his TF IDF weighted complement naive Bayes.",
            "And you can sort of see a fairly large performance improvement on several different problems here, and in fact what you end up with something that's quite compare."
        ],
        [
            "With support vector machines.",
            "OK.",
            "So we are.",
            "I'm not going to talk briefly about some other fast discriminative methods.",
            "In fact, probably going to switch through this very quickly because I want to spend a little bit of time on some of the other things in Part 2 and we got off to a little bit of a late start.",
            "So earlier I talked about this Perceptron algorithm, which is has this very simple formula pattern.",
            "You get a new example, you predict the value, and then if it was wrong, you do something to update the model.",
            "So for Perceptron you basically just added a new example, but there are lots of other ways you could imagine doing update that will make the current hypothesis alittle bit closer to the right.",
            "Move it a little bit closer to the right place so there are a lot of other algorithms that have this basic online behavior.",
            "Alright, so Windows and probably the best known one aside from Perceptron.",
            "So in practice these algorithms aren't usually used online and said you iterate over the data several times, you have something that's pretty fast but not quite as fast as say, naive Bayes, which.",
            "As I showed you few slides back, you can estimate all parameters for just one pass over the data.",
            "So we looked at comparing a number of different algorithms that were of this particular type of mistake driven online learner, including one slightly more complicated one little bit more complicated.",
            "In Perceptron, where the update rules look like this, which we called modified balanced window."
        ],
        [
            "OK.",
            "So I hear some results for this, So what we're looking at here is performance of several different algorithms.",
            "This one is voted Perceptron.",
            "This is modified balance when I want it voted version and then SVM and naive Bayes are kind of standard techniques, so we can see that this modified balance well, even though only makes one Passover the data strictly linear time, that's pretty well on the sparse high dimensional text classification problems.",
            "It doesn't do as well on these lower dimensional problems, which we have a set of here.",
            "OK, but works very well on text classification tasks.",
            "So if we're comparing it to SVM, we can see that it actually does quite well."
        ],
        [
            "Now I'll go ahead and skip past this bit, 'cause I'd like to talk about some of these other topics.",
            "Alright, so we've talked a bit about.",
            "Some basic, well understood algorithms for doing text classification.",
            "We've seen you can do it quite efficiently with some."
        ],
        [
            "Simple methods, let's look at some other types of problems.",
            "Some problems that are different from the types of topical classification technique that I talked about with the news stories, right?",
            "So what do I mean by topical classification?",
            "Well, the easiest way to describe it is by example, since you're all good learning systems, right?",
            "So I had this long list of different types of problems, so classifying things as spam.",
            "Another is topical.",
            "Classifying email to a technical staff as like Mac or Windows.",
            "Or you know.",
            "Internet stack or whatever is our is another category.",
            "Adding mesh terms is also topical classification, so we're trying to find the central topic of the message.",
            "But there are other cases where you might not be interested in.",
            "You know the central topic of the message, so here's one that we're going to look at."
        ],
        [
            "Classifying reviews as favorable or unfavorable.",
            "OK, so I'm going to go back to this message.",
            "This paper here by a guy named Peter Turney, from ACL in 2002.",
            "So here he was looking at reviews from several different areas like autos, banks, movies, travel destinations and the reviews are categorized as favorable or unfavorable, right?",
            "And we want to classify we want to learn how to do that.",
            "OK, so what's the method he used?",
            "Well, there are two intuitions behind what he did.",
            "OK, one was that if you say have a word.",
            "Like, say, unpredictable.",
            "Probably whether that word is positive or negative.",
            "If it indicates a favorable or unfavorable opinion depends on the context, right?",
            "So if your airline.",
            "Has unpredictable departure times, that's not so good.",
            "If a movie has done predictable plot, maybe that's like a good thing.",
            "OK, so the idea was to use not words, but two word phrases that contain an adverb or an adjective as the sort of central part, and the way things were classified was using this clever technique.",
            "So he looked at the semantic orientation as the pointwise mutual information between a phrase in the word excellent and appointment is mutual information between a phrase in the word poor.",
            "So does it come up a lot with the word excellent or does it come up a lot with the word poor?",
            "OK, so point my pointwise mutual information basically is this little formula of the probability of seeing these two words together alright versus the probability of seeing this word times the probability of seeing that word.",
            "So if the words tend to Co occur, a lot will be high, otherwise it will be well and the nice thing about this is you can compute this by just using a search engine, so there's no labeled data that you need.",
            "There's no, this isn't.",
            "Like sort of a traditional learning algorithm, you just.",
            "Go through the document and look for these phrases and then you look at each of these."
        ],
        [
            "Phrases and you compute their score, and then we're going to look at the average of these scores as the classification of a document.",
            "Alright, so it's a simple idea.",
            "Here's a document was a review of online banks and these are different phrases like low fees, inconveniently located and so on, right?",
            "And the overall score was."
        ],
        [
            ".32 so favorable, right?",
            "And here are some results, so I won't go into these in great detail, but on average the performance is about 74%, which is not wonderful, but it's a lot better than just guessing the majority class, which was 59%.",
            "So on average you're doing fairly well using this very simple learning technique, which is just based on using a search engine to get some idea as to how.",
            "Strongly favorable.",
            "Each semantically correlated phrases and notice this is not a topical classification task.",
            "Right topics would be maybe taking these reviews and trying to figure out if they're about Cancun or Pearl Harbor or the Matrix, right?",
            "This is basically looking at another orthogonal dimension.",
            "Whether the reviews are favorable or not."
        ],
        [
            "Favorable OK so.",
            "Another, much more widely cited paper on the same topic from around the same time was by boat pangen, Lillian Lee and some other students whose name I've forgotten right now.",
            "Uh.",
            "Also in 2002 OK and hear the the data set was a little bit different, so these were 700 movie reviews.",
            "They chose movie reviews because peer turning.",
            "Notice that movie reviews were the hardest ones to predict the sentiment for OK and they looked at not this fancy semantic orientation method, but just off the shelf machine learning albums like Naive Bayes, Linear SVM's, an accent which perhaps will tell you about.",
            "OK, and the interesting and they also looked at a bunch of different variations, so Turney looked at say, adjectives, so he thought those were very important.",
            "They looked at just the adjectives, or they looked at by grams or unigrams plus bigrams so they were looking at sort of these variations, which included some of the information that Ernie had and kind of the short story is the off the shelf methods using unigrams worked as well as anything else really, so this is the off the shelf SVM with unigrams.",
            "Question, I mean this sort of maybe tangential, but.",
            "So so some of these algorithms in a certain task you're getting 80%.",
            "Is that enough to convince the user that's useful?",
            "That depends a lot on the task.",
            "It depends a lot on the time.",
            "I guess a lot on the user, right?",
            "So there are a lot of user interface issues involved in.",
            "Getting the data or if the user has active work in order to make this data available right.",
            "If the user is required to actually sort of market messages like spam or not saying right, there's a lot of effort questions about whether you can get the user to do that kind of showing some immediate benefit fairly quickly.",
            "So how is this something we told you that already in a certain elements, so they didn't do it and?",
            "I can't, I can't think of one of the top of my head.",
            "Maybe things like sort of in the UI literature, not yeah.",
            "Between 60% and 75% of users hardly win.",
            "See the difference?",
            "Intuitively, just by using it for a short time, yeah, so yeah.",
            "Also some issues because a lot of these things are hard for even a human to look at right.",
            "They'll look at them and they'll say, well, there's a bit of confusion.",
            "Is this positive or negative movie reviews?",
            "They're being clever, yeah?",
            "Alright."
        ],
        [
            "So this was the 1st paper that Pang and Lee did 2nd paper they did was following up on turning this idea of focusing on particular parts of the review that they thought would have some clarity, some sentiment, some favorable and unfavorable aspects.",
            "So the idea was the following.",
            "What they're going to do is they're going to 1st take the review and pull out all the subjective sentence is so subjective means it's not talking bout fax, it's talking bout opinions alright?",
            "So once we have this smaller subset of you that just has the subjective sentences.",
            "Then we're going to classify it alright."
        ],
        [
            "This two step process, so let's talk about these things.",
            "So to get this subjectivity detector, what they did was they took sentences from Rotten Tomatoes, which is all about reviews and is very subjective and they also took some plot summaries, not plot reviews, plot summaries which tend to be much more factual.",
            "Alright, so those were the positive and negative examples, so it's kind of noisy training data for what the sentence extractors subjectivity estimator will be like and they just built used machine learning to do that.",
            "Now the second idea they had was to try and force nearby sentences to have similar subjectivity.",
            "So I'm going to take a minute or two to talk about that idea, 'cause it's a really kind of."
        ],
        [
            "Clever and important idea.",
            "So let's look at one of these reviews.",
            "So I just want to just pull off the web.",
            "Yesterday when I was putting these slides together so it's about fearless.",
            "This new movie with Jet Li.",
            "And if you look about."
        ],
        [
            "Through this OK, so these red phrases I've marked, or ones that I'm considering to be objective.",
            "So it's his last turn as a martial arts movie star.",
            "He's 42.",
            "He's an ex wushu champion and wushu is a general Chinese term for martial arts.",
            "OK, so this is a lot of facts.",
            "OK, and that's pretty much what the first paragraph is about.",
            "It's telling you facts about the movie, then it goes well.",
            "It's a highly fictionalized, highly fictionalized terms.",
            "Most dramatic sequence, and then they sort of step away, which is why I say.",
            "Some of these things are difficult to do.",
            "If I had to classify this sentence is subjective or objective, well, it's objectively telling you that part of this was based on fact, but it's talking about it in highly fictionalized terms.",
            "It's dramatic sequence that's rather subjective here.",
            "It tells you that this was choreographed by Uping, right, who did Crouching Tiger in the mattress matrix, but he's the Bob Fosse of Kung Fu moves alright, so these green things are subjective.",
            "These red things are much less subjective, objective, OK?",
            "And the key point here is that you sort of tend to get some clustering.",
            "Here we get a lot of red stuff.",
            "Here we get a lot of green stuff here."
        ],
        [
            "So can you take advantage of that?",
            "So the way they decided to take advantage of that was by basically using a min cut on a, particularly on a particular graph."
        ],
        [
            "They constructed, so let's zoom into this for a second so they could take a graph.",
            "OK has one node for the class subjective and one for the class non subjective OK and then every sentence has a vertex OK. And if the sentence has high weight, I you have a strong prediction that subject if you put a strong link here.",
            "If there's a strong probability it's non subjective, you put a strong link here in a weak link here.",
            "OK so these links are the predicted classes.",
            "Alright, but then there are other links in this graph which measure proximity.",
            "Alright?",
            "So if we are notion of proximity to say, things that are in the same paragraph or things that are within three sentence three sentence window, then those would be the edges in the graph."
        ],
        [
            "OK, so this is a graph that gets constructed, so we take this graph and now we're going to figure out what's the minimum cut.",
            "In other words, what are what is the minimum weight set of edges that separate S from T?",
            "OK, So what does that mean?",
            "So if I cut one of these edges alright, so I cut this edge here but not this edge here, right?",
            "That is separating us from T on one path and it's choosing a class for that V3 OK?",
            "And analogously here I'm picking a class for V1.",
            "Alright, and if I decide that I'm going to keep this edge here and discard this edge here so these things are both in proximity, all three of these things are together, right?",
            "But I've decided to keep this one and not this one, alright, so choosing the maximal I'm sorry the minimal weight cuts is choosing in some sense the most consistent, consistent set of labels for that graph."
        ],
        [
            "OK, and here's some results that they used.",
            "Using this subjectivity extractor.",
            "So taking say, the first few sentences or the last N sentence is is not nearly as good as taking the most subjective sentences.",
            "OK, I guess actually taking the last 10 sentences is not too bad.",
            "And you can do a little bit better than taking the Fall River."
        ],
        [
            "With much less of the material.",
            "OK, so this is an example of something called collective classification.",
            "And I'm not.",
            "I'm very quickly fly through just one more example of collective classification.",
            "So you get a sense for what it's like."
        ],
        [
            "Another problem that I studied recently with classifying emails into what we called X.",
            "So these X are descriptions of what the what the.",
            "What the user is motivation was for sending the message.",
            "OK, so you might have things like.",
            "Well, I'm trying to make a request or I'm trying to commit to a particular object or trying to deliver some document that was asked for.",
            "An announcer basically sort of.",
            "The subject of that, so the nouns would be things like you know that you deliver data information about a meeting."
        ],
        [
            "And so on.",
            "Alright.",
            "So, um, the idea was to try and predict X in a context that is.",
            "One of these graphs, like context, so if we look at a thread of email messages.",
            "OK, so here's a typical thread.",
            "There's lots of information about the particular acts that would be in a message, and of course East matches might have several, right?",
            "I mean, message might request one thing and propose another thing and make a commitment to yet another thing.",
            "Alright, so there may be several different types of acts in a message so that it can be information about the acts in a message in the surrounding messages, the parents and the child children, OK?",
            "But of course they don't tend to be exactly the same, so proximity here doesn't mean that things tend to have the same label, just means the labels of 1 tend to influence."
        ],
        [
            "Also the other.",
            "And you can sort of see this if you look at.",
            "These sorts of probabilistic connections, so if we have a request then you're quite likely to get a delivery in the next message.",
            "OK, or if I get a proposal, then I'm fairly likely to see a commit message.",
            "OK, so there are some patterns in the sequel."
        ],
        [
            "Facts that you tend to see and you can sort of verify this so we actually did these experiments where we looked at predicting the class of an act using just the words.",
            "OK, so bag of words, representation.",
            "That's what these blue lines.",
            "I'm sorry that's the yellow lines, which are content and the blue lines are using as features instead.",
            "The set of the set of categories for the surrounding messages.",
            "OK, so you can sort of see that each of these gives you some fairly reasonable accuracy.",
            "This is Kappa, which basically means that zero is random guessing, except in this one case for delivering data.",
            "That's the one case where you don't get very."
        ],
        [
            "Much information from the context that seems to happen, kind of asynchronously.",
            "Alright, so now the final question is how do you use this information, right?",
            "So to use these features, these context features the parent and the child classes right?",
            "We need to classify the parent and the child right?",
            "And of course the child to classify that we need to get the parents, which means we have to look at the parents of the first child, which is back to X again.",
            "So there's some circle."
        ],
        [
            "Clarity and the technique we used with something called dependency networks, which is a lot like pseudo likelihood and just to give you an idea what the algorithm is, what you do is you build a local classifier which predicts the class four of each email act given the true labels of the surrounding classes.",
            "So you assume initially that you know the classes alright.",
            "Then we'll initialize this with some guess as to what the right classes are.",
            "In this case, using a content only classifier, and then we iterate OK and there's this.",
            "Outer loop, which is basically sort of an annealing schedule where you have confidence, but at the end of the day, we're basically looking at.",
            "The confidence from the local classifier OK, and if we're confident in the class, we're going to update the email act OK, Alright, and we do this by cycling over every class.",
            "Alright, so the idea here is that gradually we get more and more correct email classes, correct predictions and that will give us more useful information for the local classifier, which will give us more correct predictions and so on, right?",
            "So you can show using like MCMC."
        ],
        [
            "This sort of technique should converge under pretty broad conditions, and in fact you do tend to see an improvement in most of the cases, so this is delivered the case where the context didn't help very much and you can sort of see the accuracy as each iteration as you go through these iterations doesn't really change too much, it goes up and down, ends up around the same spot for these guys.",
            "We got a little bit of an improvement."
        ],
        [
            "So I'm going to go ahead and I think skip this last section, which was also about email acts.",
            "Just so I can.",
            "Just say a few."
        ],
        [
            "Words and conclusion.",
            "OK, so one of the great things about text classification is there are lots and lots and lots of places where it can be used.",
            "There lots of applications of it.",
            "OK, so from our research point of view, there's lots of prior work to build on from practical point of view.",
            "Again, there's lots of prior work that you can build on.",
            "And topical text classification is the best understood of these particular types of applications.",
            "So if you're looking at topical classification problems or lots of techniques that we sort of know off the shelf should work OK.",
            "So for a lot of cases, however, the classes aren't topic, so the hot ones now seem to be things like sentiment detection, estimating the subjectivity of sentence is looking for opinions, detection of user intent OK, and the bottom line is we really don't know yet what methods and what.",
            "Representations for text work here and the final thing which I talked about in terms of this min cut algorithm and dependency network.",
            "Is there a lot of cases where in fact you don't want to classify each individual document completely independently where the documents are in some contexts where these classification decisions might interact.",
            "OK, so there are lots of cases like this and there are now emerging these methods for doing this collective classification, which helps when they're strong dependencies between messages of.",
            "Documents of one class and things that are sort of nearby in the document space.",
            "Alright, so I'm going to stop here.",
            "And I'll come up, come up and maybe while he's plugging in, we can have a minute or two for questions.",
            "If people have any more incidental questions.",
            "Yes no.",
            "Alright."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is a tutorial introduction to text classification and a lot of what we're going to be talking about has to do with text and classification of text is sort of one of the fundamental things, so this is kind of an ambitious tutorial or kind of a lot of slides, and we'll see how many of these are actually kind of get through.",
                    "label": 0
                },
                {
                    "sent": "Ever want to feel comfortable breaking in and asking questions?",
                    "label": 0
                },
                {
                    "sent": "There's no point sitting here.",
                    "label": 0
                },
                {
                    "sent": "It's pretty small crowd.",
                    "label": 0
                },
                {
                    "sent": "There's no point sitting here if if something is.",
                    "label": 0
                },
                {
                    "sent": "Not clear you should.",
                    "label": 0
                },
                {
                    "sent": "You should certainly raise your hand and and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And ask me, OK?",
                    "label": 0
                },
                {
                    "sent": "So basically there are going to be 2 parts of this tutorial, so I'm going to start out kind of slow.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about what is text classification?",
                    "label": 1
                },
                {
                    "sent": "What are the applications of it, and then I'll talk about some of the really basic techniques and methods for text classification.",
                    "label": 0
                },
                {
                    "sent": "Will start out with Naive Bayes, which is very simple practical generative model.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to talk about some other models, some discriminative models for learning how to classify text, which are also very effective and very practical and fast.",
                    "label": 0
                },
                {
                    "sent": "Alright, so then I'm going to move to some more advanced topics, so I'm going to talk about other types of classification task, which I guess are less well understood from a research perspective.",
                    "label": 0
                },
                {
                    "sent": "And that's basically the plan.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's start out with the definition of what text classification is.",
                    "label": 1
                },
                {
                    "sent": "OK, so when you're classifying objects, you have an input which is the object you want to classify, and the output is a predicted class.",
                    "label": 1
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "So maybe good or bad, high or low from some label from some fixed set that you've determined in advance.",
                    "label": 1
                },
                {
                    "sent": "So the experience that you're learning from here is a set of examples with their associated correct classification, right?",
                    "label": 0
                },
                {
                    "sent": "And what you're learning is a function.",
                    "label": 0
                },
                {
                    "sent": "That Maps you from.",
                    "label": 0
                },
                {
                    "sent": "From an input X, which in this case is a document 2.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And output Y, which is the class you want to associate with that.",
                    "label": 0
                },
                {
                    "sent": "So there are lots and lots of examples of this, so one that's been pretty well studied is classifying news stories.",
                    "label": 0
                },
                {
                    "sent": "So maybe you want to classify the stories that pop up in some news aggregator, like Google News and put them into categories like World, US business and so on.",
                    "label": 0
                },
                {
                    "sent": "Another topic which has been investigated a lot.",
                    "label": 0
                },
                {
                    "sent": "This classification of technical articles in biomedicine.",
                    "label": 0
                },
                {
                    "sent": "There's an enormous number of biomedical articles that are published.",
                    "label": 0
                },
                {
                    "sent": "There's a database of 14 million of these that you can look at, and one way that people make these more accessible to scientists is to classify them according to content area in a very large classification scheme classification scheme.",
                    "label": 0
                },
                {
                    "sent": "Sort of like the Dewey Decimal System or Library of Congress system.",
                    "label": 0
                },
                {
                    "sent": "So what you basically you'd be adding terms to abstracts in this large set.",
                    "label": 0
                },
                {
                    "sent": "And the terms would talk about things like conscious sedation EO 3250.",
                    "label": 0
                },
                {
                    "sent": "So if anyone's suffering conscious sedation right now, it could just be lunch or it could be EO 3250.",
                    "label": 0
                },
                {
                    "sent": "Any other categories instances of this particular problem, like classifying movie reviews as favorable or unfavorable, classifying jokes is funny, not funny.",
                    "label": 0
                },
                {
                    "sent": "So there's lots of things that problems that satisfy this basic interface.",
                    "label": 0
                },
                {
                    "sent": "A document in a class.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doubt now, technically these have different characteristics we're going to start by talking about one of the best study of these things, which is classification of news articles by the topic that's associated with them.",
                    "label": 0
                },
                {
                    "sent": "And there's a very well studied benchmark that contains several thousand Newswire stories are classified in about 90 categories.",
                    "label": 0
                },
                {
                    "sent": "One of the categories is exemplified here.",
                    "label": 0
                },
                {
                    "sent": "There are things about wheat or grain shipments, so here's an example article.",
                    "label": 0
                },
                {
                    "sent": "Not very interesting categories you get here are grain and wheat, so there are 93 binary classes that get associated with these articles and this all came out of a project that Reuters had many years ago where they wanted to automate indexing system for their.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Articles, so let's start out with how do you?",
                    "label": 0
                },
                {
                    "sent": "Automate this process so the first thing you have to do is you have to choose a representation for the object you're going to.",
                    "label": 0
                },
                {
                    "sent": "You're going to classify.",
                    "label": 0
                },
                {
                    "sent": "So what we have here is we have a long string, right?",
                    "label": 0
                },
                {
                    "sent": "That's the original representation of a document.",
                    "label": 0
                },
                {
                    "sent": "How do you actually want to represent it?",
                    "label": 0
                },
                {
                    "sent": "For classification, well, the simplest useful reference.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notation is basically to represent it as a list of words.",
                    "label": 0
                },
                {
                    "sent": "So in English language, particularly when you're looking at, you know the central topics about that.",
                    "label": 0
                },
                {
                    "sent": "A document is about looking at the words gives you a pretty good idea so.",
                    "label": 0
                },
                {
                    "sent": "You start out by taking the individual words in here, and there might be a few refinements.",
                    "label": 0
                },
                {
                    "sent": "So for example, very common words like Andonov tend to not give you a lot of information about the topic of a document.",
                    "label": 0
                },
                {
                    "sent": "And it's often the case that things like you know registration versus registrations, oilseed versus oilseeds, things like plurals and other morphological variants can sort of be easily compressed, so these are these are two things that you almost always do when you're looking at topical text classification.",
                    "label": 0
                },
                {
                    "sent": "There are some other things I want to talk about in a second cliffs and multi.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Occurrences into one, but for now let's ignore that.",
                    "label": 0
                },
                {
                    "sent": "Alright, let's just consider a document as a list of words, right?",
                    "label": 1
                },
                {
                    "sent": "So here's a very simple way that you can build a classifier.",
                    "label": 0
                },
                {
                    "sent": "So the idea is for every class Y. Alright, so we've got for example grain.",
                    "label": 0
                },
                {
                    "sent": "We're going to consider this a binary classification.",
                    "label": 1
                },
                {
                    "sent": "I want to build a probabilistic model of what the documents X look like, given that the classes grain going to build a model of the documents in this category.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What's the probability that I get this particular list of things given that the category is wheat?",
                    "label": 0
                },
                {
                    "sent": "Or what's the probability that I get this particular list of documents given that the category is?",
                    "label": 0
                },
                {
                    "sent": "Non wheat.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm going to build this for the probabilistic model and then to classify if I want to classify a document in these binary categories of wheat or non wheat.",
                    "label": 0
                },
                {
                    "sent": "I want to find the class we torn on which is most likely to generate X according to this probabilistic model.",
                    "label": 1
                },
                {
                    "sent": "OK, so basically want to find out the most.",
                    "label": 0
                },
                {
                    "sent": "The most probable why the why?",
                    "label": 0
                },
                {
                    "sent": "That has the highest score for the probability of X given Y and also we also want to consider the prior probability of why?",
                    "label": 0
                },
                {
                    "sent": "So if a category is very unlikely to occur, then we want to somehow penalize it, so this is a very natural thing to do.",
                    "label": 0
                },
                {
                    "sent": "Alright, so how do we do this?",
                    "label": 0
                },
                {
                    "sent": "Well, we have to build this probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "How do you?",
                    "label": 0
                },
                {
                    "sent": "How do you describe the probability of generating this particul?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "List of words given a category.",
                    "label": 1
                },
                {
                    "sent": "Alright, so the simplest useful process again is the file.",
                    "label": 1
                },
                {
                    "sent": "When you pick some word according to some probability of generating a single word, and then you repeat that same process for the next word in the next word in the next word.",
                    "label": 0
                },
                {
                    "sent": "So you're ignoring the order, so each word is generated independently of all the other words.",
                    "label": 1
                },
                {
                    "sent": "So we're learning a lot of the structure of this document alright, and what you end up with is if you do this, if you make this rather naive assumption that the order of the words doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Is the probability of this list of words is basically the product of the probabilities of each word given?",
                    "label": 0
                },
                {
                    "sent": "Why so now?",
                    "label": 0
                },
                {
                    "sent": "How do I model this?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "The simplest.",
                    "label": 0
                },
                {
                    "sent": "Useful thing to do.",
                    "label": 0
                },
                {
                    "sent": "Is to estimate this probability by just looking at the counts in the data, so you can look at the number of times you saw that word in the category devoted by the total number of times you saw any word in the category, right?",
                    "label": 1
                },
                {
                    "sent": "So if a word like oilseed occurs very often in the green category, then that would have a higher weight than a word like maybe.",
                    "label": 0
                },
                {
                    "sent": "Stocks, which maybe when the curve at frequently in that category.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So this has a little bit of a problem, because if you run into a word that you've never seen, and your training data, it will get a score of 0, which means that the probability of that class is going to be 0, and the probability of the complement of the class will also be 0.",
                    "label": 0
                },
                {
                    "sent": "'cause you just never seen that word.",
                    "label": 0
                },
                {
                    "sent": "So very common thing to do is to put a little bit of smoothing in here, so we put on a, wait an extra value here on the top and on the bottom.",
                    "label": 0
                },
                {
                    "sent": "And a typical example is M might be one and P might be 1/2, let's say.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the formula we're going to use for estimating the probability of a word given a class.",
                    "label": 0
                },
                {
                    "sent": "So this is called the multinomial distribution.",
                    "label": 0
                },
                {
                    "sent": "So it's a fancy term for very simple idea, and this particular use of M&P is called a Dirichlet prior.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, a fancy name for very simple thing.",
                    "label": 0
                },
                {
                    "sent": "So if we use these counts of plus one and plus .5 which notice that means that basically the probabilities go to 1/2 if I've never seen a word before.",
                    "label": 0
                },
                {
                    "sent": "OK, so that basically means that the score for a particular class given a list of words is going to be.",
                    "label": 0
                },
                {
                    "sent": "I especially if we're going to work in log space, which is.",
                    "label": 0
                },
                {
                    "sent": "In probabilistic models like this, almost always more convenient becausw the actual values get very close to 0 very quickly.",
                    "label": 0
                },
                {
                    "sent": "Then we can end up where we end up with by scoring everything by this particular formula, right?",
                    "label": 0
                },
                {
                    "sent": "Which you can see is very simple and very easy to implement, and the only things you really need to keep track of is the number of times each word appears in, the number of words that are in each category.",
                    "label": 0
                },
                {
                    "sent": "So this is a very very simple algorithm, and it turns out to work quite well.",
                    "label": 0
                },
                {
                    "sent": "We got our little Dursley prior here, OK?",
                    "label": 0
                },
                {
                    "sent": "And we only need the other nice thing about this is what I'm scoring a particular document, but I'm looking at the score for a class wheat or non wheat.",
                    "label": 0
                },
                {
                    "sent": "I only need to look at the accounts of the words that actually appear in the document.",
                    "label": 1
                },
                {
                    "sent": "Alright, so accumulating counts for many many different words.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'm looking at a large collection, say 20,000 news articles.",
                    "label": 0
                },
                {
                    "sent": "I have counts for thousands of words, but each of these counts are going to be stored in a very sparse way, right?",
                    "label": 0
                },
                {
                    "sent": "So we only need to worry bout counts for the words that actually occur.",
                    "label": 1
                },
                {
                    "sent": "Or in the classes that they actually occur in the total number of words for each class, so I can actually do this quite efficiently, even if there are a lot of classes in that there are a lot.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words.",
                    "label": 0
                },
                {
                    "sent": "So this is a pretty common technique.",
                    "label": 0
                },
                {
                    "sent": "It's quite likely that one of you.",
                    "label": 0
                },
                {
                    "sent": "It's quite likely each of you have actually used one of these things.",
                    "label": 0
                },
                {
                    "sent": "If ever used any learning system that has an adaptive spam filter, you know Mail reader where you can sort of classify things as junk, and it remembers that.",
                    "label": 0
                },
                {
                    "sent": "Most of these work using some variation of 90s naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "So first paper that was published on this was like in the late 90s, nineteen 98.",
                    "label": 0
                },
                {
                    "sent": "Some guys at Microsoft and there were some other papers just around the same time that talked about using Naive Bayes for spam filtering, and it works fairly well, or at least it did work fairly well until people started getting the idea of of.",
                    "label": 0
                },
                {
                    "sent": "Changing the words that were in the message by using variants that look a little bit different so you know instead of saying you know free, they might say Fr 3 three or something like.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That so.",
                    "label": 0
                },
                {
                    "sent": "That was in 1998, so five years later, there was basically a huge slew of email filters that you could download.",
                    "label": 0
                },
                {
                    "sent": "A lot of these are based on Naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "You can sort of tell sometimes from the names, so a lot of people were playing with these things.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a one thing which was in the SourceForge repository was one of the top downloaded things for quite awhile, so this is kind of cool, so we've got a machine learning program based on research ideas from a few years ago.",
                    "label": 0
                },
                {
                    "sent": "That is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is already sort of in wide use, so.",
                    "label": 0
                },
                {
                    "sent": "Naive Bayes has got some really nice things.",
                    "label": 1
                },
                {
                    "sent": "It's very fast.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to implement.",
                    "label": 0
                },
                {
                    "sent": "It's very well understood formally, in the sense that there's this nice probabilistic model which you can use to justify the algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, and people have been using it for a long, long time, so it's fairly well understood experimentally.",
                    "label": 0
                },
                {
                    "sent": "Now the bad things about Naive Bayes is it usually if you think about the problem or if you look at alternative algorithms.",
                    "label": 0
                },
                {
                    "sent": "You can often find other albums that work a little bit better, so you're paying for a little bit a little bit for this simplicity in.",
                    "label": 0
                },
                {
                    "sent": "Ease of implementation.",
                    "label": 0
                },
                {
                    "sent": "Another problem is 1.",
                    "label": 0
                },
                {
                    "sent": "Would really like these probabilities to be good estimates, so if someone if naive Bayes tells you all the probability that sweet is .7, you'd really like to be able to believe that probability.",
                    "label": 0
                },
                {
                    "sent": "So you could pass it along and use it in some other system in later stage of the processing.",
                    "label": 0
                },
                {
                    "sent": "But in fact those don't tend to be accurate just because assumptions made in the model are so simple.",
                    "label": 0
                },
                {
                    "sent": "So in practice the probabilities tend to.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very, very close to zero or one, so they're not not very useful in general.",
                    "label": 0
                },
                {
                    "sent": "So that's the first step.",
                    "label": 0
                },
                {
                    "sent": "That's the warmup.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to talk about some other learning methods.",
                    "label": 0
                },
                {
                    "sent": "Which, like naive Bayes are relatively fast, simple to implement and fairly practical.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For for text classification methods, so let me go back to this question of how we represent text.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I talked about a couple of things like removing stopwords and stemming words.",
                    "label": 0
                },
                {
                    "sent": "Another very common thing you're going to do is to collapse the multiple occurrences of words into into some single figure.",
                    "label": 0
                },
                {
                    "sent": "Alright, so just to motivate that recall in the Naive Bayes algorithm we looked at a list of words and each word was generated independently.",
                    "label": 0
                },
                {
                    "sent": "Which basically meant the contribution to the score was insensitive to the position you get the same contribution to get the same score.",
                    "label": 0
                },
                {
                    "sent": "If you took the words an reverse them or shuffle them randomly.",
                    "label": 0
                },
                {
                    "sent": "So since the order doesn't matter, maybe we don't need to save the order, so rather than saving the words a list, we can just say the words or the stems that appear in the document and then the number of times they appear.",
                    "label": 0
                },
                {
                    "sent": "So that's basically all the information that's in this document.",
                    "label": 0
                },
                {
                    "sent": "Um, modulo the fact that you know here there's an order of the words in here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's not OK, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a somewhat simpler way of representing the document, and if you look at this particular.",
                    "label": 0
                },
                {
                    "sent": "Picture of the document where essentially what I've done is I've I've put in bold and made a little bit larger.",
                    "label": 0
                },
                {
                    "sent": "The words that are most frequent, and if you kind of close your eyes, or maybe if you're in the back of the room, you don't even need to do that.",
                    "label": 0
                },
                {
                    "sent": "You can sort of guess black.",
                    "label": 0
                },
                {
                    "sent": "It probably tell what that document is about without looking at all the other words without knowing what the order is, right?",
                    "label": 1
                },
                {
                    "sent": "So it sort of seems like a reasonable thing to do for topical classification.",
                    "label": 0
                },
                {
                    "sent": "It sort of seems reasonable.",
                    "label": 0
                },
                {
                    "sent": "You should be able to look at this information and make a categorization decision.",
                    "label": 0
                },
                {
                    "sent": "OK, so technically.",
                    "label": 0
                },
                {
                    "sent": "It's nice to think about this.",
                    "label": 0
                },
                {
                    "sent": "Representation, so internally what you would represent in the computer is basically some sort of hashtable, which gives you the word in the frequency of each word.",
                    "label": 1
                },
                {
                    "sent": "Alright, but mathematically it's nice to think about this as a very long sparse vector.",
                    "label": 1
                },
                {
                    "sent": "OK, where the dimensions of the vector are the entire vocabulary, not just the words that appear in the document, but they all 300,000 words that are used in English OK, and.",
                    "label": 0
                },
                {
                    "sent": "Each the ice component of the vector is the frequency of the ice word.",
                    "label": 1
                },
                {
                    "sent": "Alright, so conceptually you can think of this as a very long vector where most of the entries are zero.",
                    "label": 0
                },
                {
                    "sent": "So mathematically it's much more convenient.",
                    "label": 0
                },
                {
                    "sent": "So as I go through the rest of these algorithms, that's the that's the mathematical representation I'm going to use, but under the hood inside the computer, what we're really going to be looking at is this sort of word frequency.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hyper representation OK, so a little bit of history now.",
                    "label": 0
                },
                {
                    "sent": "OK, the first really serious experimental look at text classification was David Lewises thesis at the University of Massachusetts back in 1992.",
                    "label": 1
                },
                {
                    "sent": "OK, so as computer science goes, Anna's machine learning goes.",
                    "label": 0
                },
                {
                    "sent": "1992 is not that long ago.",
                    "label": 0
                },
                {
                    "sent": "So just.",
                    "label": 0
                },
                {
                    "sent": "For the purpose of comparison, first machine learning paper talking about classification of.",
                    "label": 0
                },
                {
                    "sent": "Of you know, vector data that I know of that's widely cited.",
                    "label": 0
                },
                {
                    "sent": "I'm sure there are other ones that go that earlier than that was Fisher's 1936 paper that talked about the linear discriminate, so that was in 1936, so David Lewis did this text categorization stuff, and it was 50 years later.",
                    "label": 0
                },
                {
                    "sent": "So why 50 years later?",
                    "label": 1
                },
                {
                    "sent": "Why didn't people look at text classification before before that?",
                    "label": 0
                },
                {
                    "sent": "Given that there are so many different applications of it?",
                    "label": 0
                },
                {
                    "sent": "Well, one thing is just we had to get computers that were ready to handle that.",
                    "label": 0
                },
                {
                    "sent": "Rob, right, so in a typical text classification problem, and here's one that you know I had a while back with your own singer.",
                    "label": 0
                },
                {
                    "sent": "So we have 300,000 documents.",
                    "label": 0
                },
                {
                    "sent": "They're all pretty short, but still there were 67,000 words and it turned out to do.",
                    "label": 0
                },
                {
                    "sent": "We do a little bit better if we used 4 grams, so we had like 3 million 4 grams.",
                    "label": 0
                },
                {
                    "sent": "So we're looking at 3300 thousand documents and 3 million features.",
                    "label": 0
                },
                {
                    "sent": "OK, so computationally there's there's a cost and storing these things and and.",
                    "label": 0
                },
                {
                    "sent": "Are working with them even with efficient methods like Naive Bayes, but there's also a conceptual problem so.",
                    "label": 0
                },
                {
                    "sent": "In Fishers data he was classifying Flowers based on four measurements.",
                    "label": 0
                },
                {
                    "sent": "OK, so every example had four numbers and there was a belief that as you increase the number of dimensions, the problem got intrinsically harder.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we're talking about learning with millions and millions of features.",
                    "label": 0
                },
                {
                    "sent": "OK, and typically more features and we have examples.",
                    "label": 0
                },
                {
                    "sent": "So how on Earth can network so from a theoretical point of view, it seems like it shouldn't work OK. And actually, even in 1992 it wasn't clear why it ought to work.",
                    "label": 0
                },
                {
                    "sent": "But since then we sort of come to understood why text classification can work and does work, and they're basically several different pieces to the puzzle.",
                    "label": 0
                },
                {
                    "sent": "So for efficiency.",
                    "label": 0
                },
                {
                    "sent": "You don't need to represent.",
                    "label": 0
                },
                {
                    "sent": "You know this matrix of 300,000 documents and 3 million features by representing.",
                    "label": 0
                },
                {
                    "sent": "With the matrix that's 319 thousand rows and 3 million columns, where every value is explicitly represented.",
                    "label": 0
                },
                {
                    "sent": "So by using sparse matrices you can represent this stuff inside a computer without too much work.",
                    "label": 0
                },
                {
                    "sent": "So for efficiency you need to use sparse representations, But the other important observation was if you're using simple classifiers that have wide margins, then you can theoretically showed that the curse of dimensionality is not going to.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Back and bite you.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to do a little bit of the intuitions behind these observations.",
                    "label": 0
                },
                {
                    "sent": "So what's the idea here?",
                    "label": 0
                },
                {
                    "sent": "So we've got some examples, is just in 2 dimensional space, right?",
                    "label": 0
                },
                {
                    "sent": "But I've got some positive examples and some negative examples down here, so this line right here is put in the middle of these things.",
                    "label": 0
                },
                {
                    "sent": "OK. And of course there are lots of different lines you can draw between these.",
                    "label": 0
                },
                {
                    "sent": "I'm just drawing a few here, right?",
                    "label": 0
                },
                {
                    "sent": "So what's special about this particular line?",
                    "label": 0
                },
                {
                    "sent": "Will?",
                    "label": 0
                },
                {
                    "sent": "Nice thing about this particular line is, there's nothing even close to it.",
                    "label": 0
                },
                {
                    "sent": "Alright, so it's far away from all the data points.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the width of this blue buffer area is called the margin for that particular hyperplane.",
                    "label": 0
                },
                {
                    "sent": "OK, so the number of features in learning does matter.",
                    "label": 1
                },
                {
                    "sent": "OK, but it doesn't matter if the margin is wide and the examples are somehow close to the origin.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so let's make that a little bit more concrete.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a very, very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Again, this is perhaps simpler than naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm just going to assume I've got two classes.",
                    "label": 0
                },
                {
                    "sent": "I'll call the positive one and negative one OK. And I'm going to start with a hypothesis, which is just the all zero vector, and every time I get an example which remember is just a sparse vector, I'm going to look at the inner product of the example.",
                    "label": 0
                },
                {
                    "sent": "With this all zero vector.",
                    "label": 0
                },
                {
                    "sent": "OK, and then I'll look at the sign of that so the sign will either be positive or negative.",
                    "label": 0
                },
                {
                    "sent": "Or I guess zero.",
                    "label": 0
                },
                {
                    "sent": "The first time around.",
                    "label": 0
                },
                {
                    "sent": "OK if that's the right sign then I will basically just incremental counter.",
                    "label": 0
                },
                {
                    "sent": "So everyone of these V sub case that I have every hypothesis is going to have a counter which will reflect the number of times it made a correct prediction, right?",
                    "label": 0
                },
                {
                    "sent": "If it's not right then I'm just going to take the existing example vector and I'm going to add the example that it got wrong or.",
                    "label": 0
                },
                {
                    "sent": "If it was positive in the example should have been negative, it's going to subtract that example.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we're going to throw away the old old hypothesis we're going to increment, create a new hypothesis just by adding in the example, or subtracting it, and then I'll start with a new counter for this new hypothesis.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a very, very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, kind of makes sense, right?",
                    "label": 0
                },
                {
                    "sent": "So if you around when you either too high or too low, so let's move things in the right direction, and we're going to move them with the example alright.",
                    "label": 0
                },
                {
                    "sent": "And when we're done, we're going to classify by taking all these visas.",
                    "label": 1
                },
                {
                    "sent": "Kesan we're going to vote them, and the number of votes each piece of K is going to get is the number of times it was correct.",
                    "label": 0
                },
                {
                    "sent": "OK, so you know hypothesis that persists for a long time, gets lots of things right?",
                    "label": 0
                },
                {
                    "sent": "It's going to have a high weight.",
                    "label": 0
                },
                {
                    "sent": "They have an example of just sort of transient.",
                    "label": 0
                },
                {
                    "sent": "It will get a pretty low weight.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what can you say about this?",
                    "label": 0
                },
                {
                    "sent": "Alright, so it turns out that you can fairly easily say alright.",
                    "label": 0
                },
                {
                    "sent": "If every example is close to the origin, So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "There's some number R the radius so that the norm of the vector is less than R. OK, it's just the two norm of the vector, so it's within distance R. The origin.",
                    "label": 0
                },
                {
                    "sent": "OK, alright, and there's some classifier you OK, so think of one of these V sub K, so if you can't classify these things at all into space then there's nothing we can say about.",
                    "label": 0
                },
                {
                    "sent": "But there is something OK, alright, that's not too large.",
                    "label": 0
                },
                {
                    "sent": "In fact I want to say the norm is less than or equal to 1.",
                    "label": 0
                },
                {
                    "sent": "OK, and Furthermore it gets every example right?",
                    "label": 0
                },
                {
                    "sent": "OK and not only does it get every example right?",
                    "label": 0
                },
                {
                    "sent": "OK, if we look at this.",
                    "label": 0
                },
                {
                    "sent": "This number here.",
                    "label": 0
                },
                {
                    "sent": "So this is the inner product of you and X. Alright, so the prediction would be the sign of that.",
                    "label": 0
                },
                {
                    "sent": "OK if we multiply that by the actual value, which is either plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "OK, so if this was plus I multiplied by plus then I should get a positive number.",
                    "label": 0
                },
                {
                    "sent": "If it's negative and multiply it by minus one, I'll get a positive number.",
                    "label": 0
                },
                {
                    "sent": "OK right?",
                    "label": 0
                },
                {
                    "sent": "So basically saying it's greater than zero is saying that prediction was correct, saying it's greater than some number.",
                    "label": 0
                },
                {
                    "sent": "Delta means that it was correct and there's a margin of Delta.",
                    "label": 0
                },
                {
                    "sent": "OK, so if there is some you that's not too large and gets the right answer for every example, OK, no words.",
                    "label": 1
                },
                {
                    "sent": "If there is a situation like this, picture here where there's a wide margin.",
                    "label": 0
                },
                {
                    "sent": "OK, then this particular algorithm is going to make a relatively small number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "OK, in the process of covering these examples.",
                    "label": 0
                },
                {
                    "sent": "OK, so the number of mistakes depends on the radius and the inverse of the margin.",
                    "label": 0
                },
                {
                    "sent": "OK, so since it's making very relatively few mistakes, it basically means it's going to get most of these predictions correct in this online process.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's just kind of take these mathematical things and translate them into ordinary English for text.",
                    "label": 0
                },
                {
                    "sent": "OK, so suppose you have text.",
                    "label": 0
                },
                {
                    "sent": "Let's forget the frequencies for a second.",
                    "label": 0
                },
                {
                    "sent": "Let's assume you just have binary feature zero or one.",
                    "label": 0
                },
                {
                    "sent": "OK, so if that document has every document has no more than 100 words, then obviously it's two norm is less than 100.",
                    "label": 0
                },
                {
                    "sent": "OK, it's less than distance 100 from the origin, so immediately we get our radius by just saying the documents are short.",
                    "label": 0
                },
                {
                    "sent": "OK, and as I pointed out before, this thing right here basically means the margin is at least Delta.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is a really nice result, and for anyone that like wants to think about these margins and really understand them, I recommend you look at this paper.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And look at the proof.",
                    "label": 0
                },
                {
                    "sent": "This is the whole proof.",
                    "label": 0
                },
                {
                    "sent": "You could actually look at this in the slide when you go home.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to step through it right, but it's something that you can really put all in your head at once, and it's worth doing because it really gives you intuition as to why.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By this curse of dimensionality doesn't hurt you here.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are the lessons of this?",
                    "label": 1
                },
                {
                    "sent": "Well, the voted Perceptrons are very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "Later on I'll show you some experiments that show you that in fact it's pretty competitive in a lot of situations alright.",
                    "label": 0
                },
                {
                    "sent": "But basically what it shows you is that you can make few mistakes incrementally learning.",
                    "label": 1
                },
                {
                    "sent": "OK, alright, provided there's some you that small and has a large margin.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's an idea.",
                    "label": 0
                },
                {
                    "sent": "Alright, this is kind of seems like a neat idea, but it seems like kind of a silly way of.",
                    "label": 0
                },
                {
                    "sent": "Coming up with that approximating this number, you why don't I just look for that directly?",
                    "label": 0
                },
                {
                    "sent": "Alright, well, I just like to open my books on optimization and see if I can figure out how to get that you correct directly, and in fact, that's what you do when you're building a support vector machine.",
                    "label": 1
                },
                {
                    "sent": "OK, so with support vector machine you minimize the norm of you subject to some fixed margin, or Alternatively the other way around you maximize the margin relative to some bound on the norm.",
                    "label": 0
                },
                {
                    "sent": "OK, so their math optimization.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Methods that let you do this.",
                    "label": 0
                },
                {
                    "sent": "I won't go into the details, but they tend to work pretty well.",
                    "label": 0
                },
                {
                    "sent": "So again, some nomenclature here.",
                    "label": 0
                },
                {
                    "sent": "So again, this is a very simple idea, but there's some fancy term, so when you're looking at that picture, I should have like copied it here, but the picture we've got that big broad blue stripe with the hyperplane in the middle.",
                    "label": 0
                },
                {
                    "sent": "OK, the X is the examples that actually touch that margin are called support vectors.",
                    "label": 1
                },
                {
                    "sent": "That's why it's called a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "And it turns out you can write this this you can rewrite this formula.",
                    "label": 1
                },
                {
                    "sent": "The sign of some linear vector.",
                    "label": 0
                },
                {
                    "sent": "You an inner product of you and X you can rewrite that.",
                    "label": 0
                },
                {
                    "sent": "As a weighted sum of the inner product of the support vector at X, so another way of thinking about this support vector is basically a some sort of fancy weighted nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "OK, so you're looking at the distance the inner product between XI and X is a sort of distance between these two and you're waiting the different examples differently.",
                    "label": 0
                },
                {
                    "sent": "And finally you're taking the sign of all that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is another way of thinking about the support vector machine, and one thing that's nice about this formulation is that these.",
                    "label": 0
                },
                {
                    "sent": "Inner product's right here can replace with more general class of distance functions called kernels, so that gives you some additional flexibility in how the algorithm behaves, 'cause you can change these distance functions and in fact support vector machines are one of the standard techniques for using for handling topical text classification techniques.",
                    "label": 1
                },
                {
                    "sent": "These are sort of the.",
                    "label": 0
                },
                {
                    "sent": "You know the brand X method that you always try and beat in a research paper.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are sort of the baseline competitive methods here again, or some kind of old results, but these were from a very influential paper back in 1998, so this is naive Bayes and these are on 10 of the categories from this Reuters data that I showed you.",
                    "label": 0
                },
                {
                    "sent": "OK, and there's a bunch of different variants of support vector machine based on different distance functions, But the simplest distance function is basically just an inner product.",
                    "label": 1
                },
                {
                    "sent": "OK, and you know the micro averaged.",
                    "label": 0
                },
                {
                    "sent": "Accuracy here, which is some way of measuring the average over these ten categories is 84.2 OK, whereas for Naive Bayes is 72.0, so you can sort of see we're getting a little bit of extra performance for the extra work we're doing in this optimization.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "In this picture there are a couple of other algorithms here which also do pretty well, so there's roquillo here.",
                    "label": 0
                },
                {
                    "sent": "There's K nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "I won't talk about C 4.5, but I will talk briefly about these other two alright, so these other two algorithms are simpler algorithms that are based on a particular representation for documents, and the representation is just a variant of the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this vector that I showed you OK so.",
                    "label": 0
                },
                {
                    "sent": "We started out with a vector where each component was the frequency of a word.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a number like 12345, right so?",
                    "label": 0
                },
                {
                    "sent": "The idea in this representation is to change these numbers to represent some sort of statistical weight on the word.",
                    "label": 0
                },
                {
                    "sent": "Some measure of the importance of that word.",
                    "label": 0
                },
                {
                    "sent": "So a formula which is a rather ad hoc looking formula but seems to work quite well, is to wait words higher if they're infrequent.",
                    "label": 0
                },
                {
                    "sent": "OK, so if their words that appear infrequently in the document, so it only appears in you know 10 documents in the entire collection, then it should have a high weight, whereas it's a word that appears many times in the document, will have a lower weight.",
                    "label": 0
                },
                {
                    "sent": "Alright, and this is, you know, by extension things that appear frequently enough are stop words.",
                    "label": 0
                },
                {
                    "sent": "You can just throw them away alright, the other thing you want to do is you want to increase the weight of words that are frequent, and it turns out taking those things and pass him through a log seems to work better than just using the frequency directly.",
                    "label": 0
                },
                {
                    "sent": "So this is just a formula for changing the weights of these documents, so kind of common thing to do is to use these weights and then we scale all the vectors so they have length one.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is called the TF IDF representation.",
                    "label": 0
                },
                {
                    "sent": "It is a very simple idea.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it turns out to be a very surprisingly useful thing in a lot of different situations.",
                    "label": 0
                },
                {
                    "sent": "So this is an old trick from information retrieval.",
                    "label": 1
                },
                {
                    "sent": "To give you some examples, here's another very, very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "So given an example of vector, you want to classify, just find the K closest things, vectors that are nearest to it.",
                    "label": 0
                },
                {
                    "sent": "In other words, have the closest largest inner product.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to predict by basically looking at a weighted sum of these vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, we're going to find the class, so that if I look at all the things that have the class and some of the distances, it'll be sorry the inner product is really similarity.",
                    "label": 0
                },
                {
                    "sent": "Some of the similarities that will be the highest score.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's close to a lot of positive things and far away from the negative.",
                    "label": 0
                },
                {
                    "sent": "Things will give it positive weight.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we'll give it negative way.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's K nearest neighbor in TF IDF space.",
                    "label": 0
                },
                {
                    "sent": "Alright?",
                    "label": 0
                },
                {
                    "sent": "Another algorithm, maybe even a little simpler is will take all the positive examples and will add 'em up.",
                    "label": 0
                },
                {
                    "sent": "And we'll take all the negative examples and will subtract him off, and we'll put a couple of weights in here.",
                    "label": 0
                },
                {
                    "sent": "Alpha beta equals one.",
                    "label": 0
                },
                {
                    "sent": "It's not a bad idea.",
                    "label": 0
                },
                {
                    "sent": "And then we'll just take that as our classifier.",
                    "label": 0
                },
                {
                    "sent": "So if you like, we're looking at things that are close to the center of the positive examples and far.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the center of the negative examples.",
                    "label": 0
                },
                {
                    "sent": "Alright, so these two algorithms are roquillo an K nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "In TF IDF space, and you see those are actually doing quite well also, so there there are micro average performances up close to 80 or a little above.",
                    "label": 0
                },
                {
                    "sent": "So these are some other competitive algorithms.",
                    "label": 0
                },
                {
                    "sent": "One of the nice things about the K nearest neighbor algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well actually 2 nice things about the K nearest neighbor algorithm, so getting the neighbors the close neighbors of a document sounds like it might be a hard thing to do OK, but in fact that's exactly what an IR search engine is designed to do.",
                    "label": 0
                },
                {
                    "sent": "So traditional ranked retrieval engines basically try and find things that are most similar in this TF IDF space.",
                    "label": 0
                },
                {
                    "sent": "So finding these neighbors if you have a search engine lying around, just a question of making one call, the other thing that's nice about this is when you find these nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "And add up these similarities.",
                    "label": 0
                },
                {
                    "sent": "You end up with scores for every class, and it doesn't matter how many classes you have, you'll still get scores for every class.",
                    "label": 0
                },
                {
                    "sent": "So this is a very efficient thing to do if you have, say, 100 classes or.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2000 classes solar.",
                    "label": 0
                },
                {
                    "sent": "Here's a one last thing which shows it TF IDF is sort of a surprisingly useful thing to do.",
                    "label": 0
                },
                {
                    "sent": "So a couple of years ago, I think in 2000 two or three Jason running at MIT.",
                    "label": 0
                },
                {
                    "sent": "Did the paperwork evaluate naive Bayes with TF IDF weighting on all the vectors, which from our probabilistic POV doesn't really make any sense whatsoever, right?",
                    "label": 0
                },
                {
                    "sent": "And the other little trick is he used it on the complement of the data rather than original class.",
                    "label": 0
                },
                {
                    "sent": "But it's kind of a little detail.",
                    "label": 0
                },
                {
                    "sent": "So this is multinomial naive Bayes and this is his TF IDF weighted complement naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "And you can sort of see a fairly large performance improvement on several different problems here, and in fact what you end up with something that's quite compare.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With support vector machines.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we are.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk briefly about some other fast discriminative methods.",
                    "label": 1
                },
                {
                    "sent": "In fact, probably going to switch through this very quickly because I want to spend a little bit of time on some of the other things in Part 2 and we got off to a little bit of a late start.",
                    "label": 0
                },
                {
                    "sent": "So earlier I talked about this Perceptron algorithm, which is has this very simple formula pattern.",
                    "label": 0
                },
                {
                    "sent": "You get a new example, you predict the value, and then if it was wrong, you do something to update the model.",
                    "label": 0
                },
                {
                    "sent": "So for Perceptron you basically just added a new example, but there are lots of other ways you could imagine doing update that will make the current hypothesis alittle bit closer to the right.",
                    "label": 0
                },
                {
                    "sent": "Move it a little bit closer to the right place so there are a lot of other algorithms that have this basic online behavior.",
                    "label": 0
                },
                {
                    "sent": "Alright, so Windows and probably the best known one aside from Perceptron.",
                    "label": 0
                },
                {
                    "sent": "So in practice these algorithms aren't usually used online and said you iterate over the data several times, you have something that's pretty fast but not quite as fast as say, naive Bayes, which.",
                    "label": 1
                },
                {
                    "sent": "As I showed you few slides back, you can estimate all parameters for just one pass over the data.",
                    "label": 0
                },
                {
                    "sent": "So we looked at comparing a number of different algorithms that were of this particular type of mistake driven online learner, including one slightly more complicated one little bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "In Perceptron, where the update rules look like this, which we called modified balanced window.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I hear some results for this, So what we're looking at here is performance of several different algorithms.",
                    "label": 0
                },
                {
                    "sent": "This one is voted Perceptron.",
                    "label": 0
                },
                {
                    "sent": "This is modified balance when I want it voted version and then SVM and naive Bayes are kind of standard techniques, so we can see that this modified balance well, even though only makes one Passover the data strictly linear time, that's pretty well on the sparse high dimensional text classification problems.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do as well on these lower dimensional problems, which we have a set of here.",
                    "label": 1
                },
                {
                    "sent": "OK, but works very well on text classification tasks.",
                    "label": 0
                },
                {
                    "sent": "So if we're comparing it to SVM, we can see that it actually does quite well.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'll go ahead and skip past this bit, 'cause I'd like to talk about some of these other topics.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we've talked a bit about.",
                    "label": 0
                },
                {
                    "sent": "Some basic, well understood algorithms for doing text classification.",
                    "label": 1
                },
                {
                    "sent": "We've seen you can do it quite efficiently with some.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple methods, let's look at some other types of problems.",
                    "label": 0
                },
                {
                    "sent": "Some problems that are different from the types of topical classification technique that I talked about with the news stories, right?",
                    "label": 1
                },
                {
                    "sent": "So what do I mean by topical classification?",
                    "label": 1
                },
                {
                    "sent": "Well, the easiest way to describe it is by example, since you're all good learning systems, right?",
                    "label": 0
                },
                {
                    "sent": "So I had this long list of different types of problems, so classifying things as spam.",
                    "label": 0
                },
                {
                    "sent": "Another is topical.",
                    "label": 0
                },
                {
                    "sent": "Classifying email to a technical staff as like Mac or Windows.",
                    "label": 1
                },
                {
                    "sent": "Or you know.",
                    "label": 0
                },
                {
                    "sent": "Internet stack or whatever is our is another category.",
                    "label": 1
                },
                {
                    "sent": "Adding mesh terms is also topical classification, so we're trying to find the central topic of the message.",
                    "label": 0
                },
                {
                    "sent": "But there are other cases where you might not be interested in.",
                    "label": 0
                },
                {
                    "sent": "You know the central topic of the message, so here's one that we're going to look at.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classifying reviews as favorable or unfavorable.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'm going to go back to this message.",
                    "label": 0
                },
                {
                    "sent": "This paper here by a guy named Peter Turney, from ACL in 2002.",
                    "label": 0
                },
                {
                    "sent": "So here he was looking at reviews from several different areas like autos, banks, movies, travel destinations and the reviews are categorized as favorable or unfavorable, right?",
                    "label": 1
                },
                {
                    "sent": "And we want to classify we want to learn how to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the method he used?",
                    "label": 0
                },
                {
                    "sent": "Well, there are two intuitions behind what he did.",
                    "label": 0
                },
                {
                    "sent": "OK, one was that if you say have a word.",
                    "label": 0
                },
                {
                    "sent": "Like, say, unpredictable.",
                    "label": 0
                },
                {
                    "sent": "Probably whether that word is positive or negative.",
                    "label": 0
                },
                {
                    "sent": "If it indicates a favorable or unfavorable opinion depends on the context, right?",
                    "label": 0
                },
                {
                    "sent": "So if your airline.",
                    "label": 0
                },
                {
                    "sent": "Has unpredictable departure times, that's not so good.",
                    "label": 0
                },
                {
                    "sent": "If a movie has done predictable plot, maybe that's like a good thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea was to use not words, but two word phrases that contain an adverb or an adjective as the sort of central part, and the way things were classified was using this clever technique.",
                    "label": 0
                },
                {
                    "sent": "So he looked at the semantic orientation as the pointwise mutual information between a phrase in the word excellent and appointment is mutual information between a phrase in the word poor.",
                    "label": 0
                },
                {
                    "sent": "So does it come up a lot with the word excellent or does it come up a lot with the word poor?",
                    "label": 0
                },
                {
                    "sent": "OK, so point my pointwise mutual information basically is this little formula of the probability of seeing these two words together alright versus the probability of seeing this word times the probability of seeing that word.",
                    "label": 0
                },
                {
                    "sent": "So if the words tend to Co occur, a lot will be high, otherwise it will be well and the nice thing about this is you can compute this by just using a search engine, so there's no labeled data that you need.",
                    "label": 0
                },
                {
                    "sent": "There's no, this isn't.",
                    "label": 0
                },
                {
                    "sent": "Like sort of a traditional learning algorithm, you just.",
                    "label": 0
                },
                {
                    "sent": "Go through the document and look for these phrases and then you look at each of these.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Phrases and you compute their score, and then we're going to look at the average of these scores as the classification of a document.",
                    "label": 0
                },
                {
                    "sent": "Alright, so it's a simple idea.",
                    "label": 0
                },
                {
                    "sent": "Here's a document was a review of online banks and these are different phrases like low fees, inconveniently located and so on, right?",
                    "label": 0
                },
                {
                    "sent": "And the overall score was.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": ".32 so favorable, right?",
                    "label": 0
                },
                {
                    "sent": "And here are some results, so I won't go into these in great detail, but on average the performance is about 74%, which is not wonderful, but it's a lot better than just guessing the majority class, which was 59%.",
                    "label": 0
                },
                {
                    "sent": "So on average you're doing fairly well using this very simple learning technique, which is just based on using a search engine to get some idea as to how.",
                    "label": 0
                },
                {
                    "sent": "Strongly favorable.",
                    "label": 0
                },
                {
                    "sent": "Each semantically correlated phrases and notice this is not a topical classification task.",
                    "label": 0
                },
                {
                    "sent": "Right topics would be maybe taking these reviews and trying to figure out if they're about Cancun or Pearl Harbor or the Matrix, right?",
                    "label": 0
                },
                {
                    "sent": "This is basically looking at another orthogonal dimension.",
                    "label": 0
                },
                {
                    "sent": "Whether the reviews are favorable or not.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Favorable OK so.",
                    "label": 0
                },
                {
                    "sent": "Another, much more widely cited paper on the same topic from around the same time was by boat pangen, Lillian Lee and some other students whose name I've forgotten right now.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Also in 2002 OK and hear the the data set was a little bit different, so these were 700 movie reviews.",
                    "label": 0
                },
                {
                    "sent": "They chose movie reviews because peer turning.",
                    "label": 0
                },
                {
                    "sent": "Notice that movie reviews were the hardest ones to predict the sentiment for OK and they looked at not this fancy semantic orientation method, but just off the shelf machine learning albums like Naive Bayes, Linear SVM's, an accent which perhaps will tell you about.",
                    "label": 1
                },
                {
                    "sent": "OK, and the interesting and they also looked at a bunch of different variations, so Turney looked at say, adjectives, so he thought those were very important.",
                    "label": 0
                },
                {
                    "sent": "They looked at just the adjectives, or they looked at by grams or unigrams plus bigrams so they were looking at sort of these variations, which included some of the information that Ernie had and kind of the short story is the off the shelf methods using unigrams worked as well as anything else really, so this is the off the shelf SVM with unigrams.",
                    "label": 0
                },
                {
                    "sent": "Question, I mean this sort of maybe tangential, but.",
                    "label": 0
                },
                {
                    "sent": "So so some of these algorithms in a certain task you're getting 80%.",
                    "label": 0
                },
                {
                    "sent": "Is that enough to convince the user that's useful?",
                    "label": 0
                },
                {
                    "sent": "That depends a lot on the task.",
                    "label": 0
                },
                {
                    "sent": "It depends a lot on the time.",
                    "label": 0
                },
                {
                    "sent": "I guess a lot on the user, right?",
                    "label": 0
                },
                {
                    "sent": "So there are a lot of user interface issues involved in.",
                    "label": 0
                },
                {
                    "sent": "Getting the data or if the user has active work in order to make this data available right.",
                    "label": 0
                },
                {
                    "sent": "If the user is required to actually sort of market messages like spam or not saying right, there's a lot of effort questions about whether you can get the user to do that kind of showing some immediate benefit fairly quickly.",
                    "label": 0
                },
                {
                    "sent": "So how is this something we told you that already in a certain elements, so they didn't do it and?",
                    "label": 0
                },
                {
                    "sent": "I can't, I can't think of one of the top of my head.",
                    "label": 0
                },
                {
                    "sent": "Maybe things like sort of in the UI literature, not yeah.",
                    "label": 0
                },
                {
                    "sent": "Between 60% and 75% of users hardly win.",
                    "label": 0
                },
                {
                    "sent": "See the difference?",
                    "label": 0
                },
                {
                    "sent": "Intuitively, just by using it for a short time, yeah, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Also some issues because a lot of these things are hard for even a human to look at right.",
                    "label": 0
                },
                {
                    "sent": "They'll look at them and they'll say, well, there's a bit of confusion.",
                    "label": 0
                },
                {
                    "sent": "Is this positive or negative movie reviews?",
                    "label": 0
                },
                {
                    "sent": "They're being clever, yeah?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was the 1st paper that Pang and Lee did 2nd paper they did was following up on turning this idea of focusing on particular parts of the review that they thought would have some clarity, some sentiment, some favorable and unfavorable aspects.",
                    "label": 0
                },
                {
                    "sent": "So the idea was the following.",
                    "label": 0
                },
                {
                    "sent": "What they're going to do is they're going to 1st take the review and pull out all the subjective sentence is so subjective means it's not talking bout fax, it's talking bout opinions alright?",
                    "label": 0
                },
                {
                    "sent": "So once we have this smaller subset of you that just has the subjective sentences.",
                    "label": 1
                },
                {
                    "sent": "Then we're going to classify it alright.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This two step process, so let's talk about these things.",
                    "label": 0
                },
                {
                    "sent": "So to get this subjectivity detector, what they did was they took sentences from Rotten Tomatoes, which is all about reviews and is very subjective and they also took some plot summaries, not plot reviews, plot summaries which tend to be much more factual.",
                    "label": 0
                },
                {
                    "sent": "Alright, so those were the positive and negative examples, so it's kind of noisy training data for what the sentence extractors subjectivity estimator will be like and they just built used machine learning to do that.",
                    "label": 0
                },
                {
                    "sent": "Now the second idea they had was to try and force nearby sentences to have similar subjectivity.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to take a minute or two to talk about that idea, 'cause it's a really kind of.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clever and important idea.",
                    "label": 0
                },
                {
                    "sent": "So let's look at one of these reviews.",
                    "label": 0
                },
                {
                    "sent": "So I just want to just pull off the web.",
                    "label": 0
                },
                {
                    "sent": "Yesterday when I was putting these slides together so it's about fearless.",
                    "label": 0
                },
                {
                    "sent": "This new movie with Jet Li.",
                    "label": 0
                },
                {
                    "sent": "And if you look about.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Through this OK, so these red phrases I've marked, or ones that I'm considering to be objective.",
                    "label": 0
                },
                {
                    "sent": "So it's his last turn as a martial arts movie star.",
                    "label": 0
                },
                {
                    "sent": "He's 42.",
                    "label": 0
                },
                {
                    "sent": "He's an ex wushu champion and wushu is a general Chinese term for martial arts.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a lot of facts.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's pretty much what the first paragraph is about.",
                    "label": 0
                },
                {
                    "sent": "It's telling you facts about the movie, then it goes well.",
                    "label": 0
                },
                {
                    "sent": "It's a highly fictionalized, highly fictionalized terms.",
                    "label": 0
                },
                {
                    "sent": "Most dramatic sequence, and then they sort of step away, which is why I say.",
                    "label": 0
                },
                {
                    "sent": "Some of these things are difficult to do.",
                    "label": 0
                },
                {
                    "sent": "If I had to classify this sentence is subjective or objective, well, it's objectively telling you that part of this was based on fact, but it's talking about it in highly fictionalized terms.",
                    "label": 0
                },
                {
                    "sent": "It's dramatic sequence that's rather subjective here.",
                    "label": 0
                },
                {
                    "sent": "It tells you that this was choreographed by Uping, right, who did Crouching Tiger in the mattress matrix, but he's the Bob Fosse of Kung Fu moves alright, so these green things are subjective.",
                    "label": 1
                },
                {
                    "sent": "These red things are much less subjective, objective, OK?",
                    "label": 0
                },
                {
                    "sent": "And the key point here is that you sort of tend to get some clustering.",
                    "label": 0
                },
                {
                    "sent": "Here we get a lot of red stuff.",
                    "label": 0
                },
                {
                    "sent": "Here we get a lot of green stuff here.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So can you take advantage of that?",
                    "label": 0
                },
                {
                    "sent": "So the way they decided to take advantage of that was by basically using a min cut on a, particularly on a particular graph.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They constructed, so let's zoom into this for a second so they could take a graph.",
                    "label": 0
                },
                {
                    "sent": "OK has one node for the class subjective and one for the class non subjective OK and then every sentence has a vertex OK. And if the sentence has high weight, I you have a strong prediction that subject if you put a strong link here.",
                    "label": 0
                },
                {
                    "sent": "If there's a strong probability it's non subjective, you put a strong link here in a weak link here.",
                    "label": 1
                },
                {
                    "sent": "OK so these links are the predicted classes.",
                    "label": 0
                },
                {
                    "sent": "Alright, but then there are other links in this graph which measure proximity.",
                    "label": 0
                },
                {
                    "sent": "Alright?",
                    "label": 0
                },
                {
                    "sent": "So if we are notion of proximity to say, things that are in the same paragraph or things that are within three sentence three sentence window, then those would be the edges in the graph.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is a graph that gets constructed, so we take this graph and now we're going to figure out what's the minimum cut.",
                    "label": 0
                },
                {
                    "sent": "In other words, what are what is the minimum weight set of edges that separate S from T?",
                    "label": 0
                },
                {
                    "sent": "OK, So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "So if I cut one of these edges alright, so I cut this edge here but not this edge here, right?",
                    "label": 0
                },
                {
                    "sent": "That is separating us from T on one path and it's choosing a class for that V3 OK?",
                    "label": 0
                },
                {
                    "sent": "And analogously here I'm picking a class for V1.",
                    "label": 1
                },
                {
                    "sent": "Alright, and if I decide that I'm going to keep this edge here and discard this edge here so these things are both in proximity, all three of these things are together, right?",
                    "label": 0
                },
                {
                    "sent": "But I've decided to keep this one and not this one, alright, so choosing the maximal I'm sorry the minimal weight cuts is choosing in some sense the most consistent, consistent set of labels for that graph.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and here's some results that they used.",
                    "label": 0
                },
                {
                    "sent": "Using this subjectivity extractor.",
                    "label": 0
                },
                {
                    "sent": "So taking say, the first few sentences or the last N sentence is is not nearly as good as taking the most subjective sentences.",
                    "label": 0
                },
                {
                    "sent": "OK, I guess actually taking the last 10 sentences is not too bad.",
                    "label": 0
                },
                {
                    "sent": "And you can do a little bit better than taking the Fall River.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With much less of the material.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an example of something called collective classification.",
                    "label": 0
                },
                {
                    "sent": "And I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm very quickly fly through just one more example of collective classification.",
                    "label": 0
                },
                {
                    "sent": "So you get a sense for what it's like.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another problem that I studied recently with classifying emails into what we called X.",
                    "label": 0
                },
                {
                    "sent": "So these X are descriptions of what the what the.",
                    "label": 0
                },
                {
                    "sent": "What the user is motivation was for sending the message.",
                    "label": 0
                },
                {
                    "sent": "OK, so you might have things like.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm trying to make a request or I'm trying to commit to a particular object or trying to deliver some document that was asked for.",
                    "label": 0
                },
                {
                    "sent": "An announcer basically sort of.",
                    "label": 0
                },
                {
                    "sent": "The subject of that, so the nouns would be things like you know that you deliver data information about a meeting.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So, um, the idea was to try and predict X in a context that is.",
                    "label": 0
                },
                {
                    "sent": "One of these graphs, like context, so if we look at a thread of email messages.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a typical thread.",
                    "label": 0
                },
                {
                    "sent": "There's lots of information about the particular acts that would be in a message, and of course East matches might have several, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, message might request one thing and propose another thing and make a commitment to yet another thing.",
                    "label": 0
                },
                {
                    "sent": "Alright, so there may be several different types of acts in a message so that it can be information about the acts in a message in the surrounding messages, the parents and the child children, OK?",
                    "label": 1
                },
                {
                    "sent": "But of course they don't tend to be exactly the same, so proximity here doesn't mean that things tend to have the same label, just means the labels of 1 tend to influence.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also the other.",
                    "label": 0
                },
                {
                    "sent": "And you can sort of see this if you look at.",
                    "label": 0
                },
                {
                    "sent": "These sorts of probabilistic connections, so if we have a request then you're quite likely to get a delivery in the next message.",
                    "label": 0
                },
                {
                    "sent": "OK, or if I get a proposal, then I'm fairly likely to see a commit message.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are some patterns in the sequel.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Facts that you tend to see and you can sort of verify this so we actually did these experiments where we looked at predicting the class of an act using just the words.",
                    "label": 0
                },
                {
                    "sent": "OK, so bag of words, representation.",
                    "label": 0
                },
                {
                    "sent": "That's what these blue lines.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry that's the yellow lines, which are content and the blue lines are using as features instead.",
                    "label": 0
                },
                {
                    "sent": "The set of the set of categories for the surrounding messages.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can sort of see that each of these gives you some fairly reasonable accuracy.",
                    "label": 0
                },
                {
                    "sent": "This is Kappa, which basically means that zero is random guessing, except in this one case for delivering data.",
                    "label": 0
                },
                {
                    "sent": "That's the one case where you don't get very.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much information from the context that seems to happen, kind of asynchronously.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now the final question is how do you use this information, right?",
                    "label": 0
                },
                {
                    "sent": "So to use these features, these context features the parent and the child classes right?",
                    "label": 1
                },
                {
                    "sent": "We need to classify the parent and the child right?",
                    "label": 1
                },
                {
                    "sent": "And of course the child to classify that we need to get the parents, which means we have to look at the parents of the first child, which is back to X again.",
                    "label": 0
                },
                {
                    "sent": "So there's some circle.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clarity and the technique we used with something called dependency networks, which is a lot like pseudo likelihood and just to give you an idea what the algorithm is, what you do is you build a local classifier which predicts the class four of each email act given the true labels of the surrounding classes.",
                    "label": 0
                },
                {
                    "sent": "So you assume initially that you know the classes alright.",
                    "label": 0
                },
                {
                    "sent": "Then we'll initialize this with some guess as to what the right classes are.",
                    "label": 0
                },
                {
                    "sent": "In this case, using a content only classifier, and then we iterate OK and there's this.",
                    "label": 0
                },
                {
                    "sent": "Outer loop, which is basically sort of an annealing schedule where you have confidence, but at the end of the day, we're basically looking at.",
                    "label": 0
                },
                {
                    "sent": "The confidence from the local classifier OK, and if we're confident in the class, we're going to update the email act OK, Alright, and we do this by cycling over every class.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the idea here is that gradually we get more and more correct email classes, correct predictions and that will give us more useful information for the local classifier, which will give us more correct predictions and so on, right?",
                    "label": 0
                },
                {
                    "sent": "So you can show using like MCMC.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This sort of technique should converge under pretty broad conditions, and in fact you do tend to see an improvement in most of the cases, so this is delivered the case where the context didn't help very much and you can sort of see the accuracy as each iteration as you go through these iterations doesn't really change too much, it goes up and down, ends up around the same spot for these guys.",
                    "label": 0
                },
                {
                    "sent": "We got a little bit of an improvement.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to go ahead and I think skip this last section, which was also about email acts.",
                    "label": 0
                },
                {
                    "sent": "Just so I can.",
                    "label": 0
                },
                {
                    "sent": "Just say a few.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Words and conclusion.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of the great things about text classification is there are lots and lots and lots of places where it can be used.",
                    "label": 1
                },
                {
                    "sent": "There lots of applications of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so from our research point of view, there's lots of prior work to build on from practical point of view.",
                    "label": 1
                },
                {
                    "sent": "Again, there's lots of prior work that you can build on.",
                    "label": 0
                },
                {
                    "sent": "And topical text classification is the best understood of these particular types of applications.",
                    "label": 1
                },
                {
                    "sent": "So if you're looking at topical classification problems or lots of techniques that we sort of know off the shelf should work OK.",
                    "label": 0
                },
                {
                    "sent": "So for a lot of cases, however, the classes aren't topic, so the hot ones now seem to be things like sentiment detection, estimating the subjectivity of sentence is looking for opinions, detection of user intent OK, and the bottom line is we really don't know yet what methods and what.",
                    "label": 1
                },
                {
                    "sent": "Representations for text work here and the final thing which I talked about in terms of this min cut algorithm and dependency network.",
                    "label": 0
                },
                {
                    "sent": "Is there a lot of cases where in fact you don't want to classify each individual document completely independently where the documents are in some contexts where these classification decisions might interact.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are lots of cases like this and there are now emerging these methods for doing this collective classification, which helps when they're strong dependencies between messages of.",
                    "label": 0
                },
                {
                    "sent": "Documents of one class and things that are sort of nearby in the document space.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to stop here.",
                    "label": 0
                },
                {
                    "sent": "And I'll come up, come up and maybe while he's plugging in, we can have a minute or two for questions.",
                    "label": 0
                },
                {
                    "sent": "If people have any more incidental questions.",
                    "label": 0
                },
                {
                    "sent": "Yes no.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        }
    }
}