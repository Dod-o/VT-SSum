{
    "id": "wh32pq2c3qqzity7port6shk45oi7gb7",
    "title": "A Non-Parametric Approach to Dynamic Programming",
    "info": {
        "author": [
            "Oliver B Kroemer, Technische Universit\u00e4t Darmstadt"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nips2011_kroemer_programming/",
    "segmentation": [
        [
            "Today I'm gonna be talking about a nonparametric approach to dynamic programming.",
            "As already said, this is a joint work with my supervisor, John Peters, and we'll both associated with the technical investigate down steps as well as the next step is to for intelligence."
        ],
        [
            "Steps of the work out loud.",
            "The classical years has mainly been on learning and episodic scenarios specifically.",
            "Robot learning, as seen here.",
            "So here we have a robot arm playing table tennis.",
            "And recently we've been also moving further into actually developing methods for achieving these learn skills together, and today I'm going to be talking about one of the theoretical results that is come out of these endeavors.",
            "Those of you who visit nips quite often should remember this video from a couple of years ago by again, scuba."
        ],
        [
            "So to begin with, I'm going to give a introduction background on reinforcement learning in general."
        ],
        [
            "Before going on to my method, so in reinforcement learning, we're really interested in looking at the interaction between agent and its environment at a certain time step T."
        ],
        [
            "Were they just went by itself and in the state S?",
            "And given the state they just got to choose."
        ],
        [
            "Action which we get by A and the way that the agent chooses it is by its policy Pi of a given S. Now this policy can either be sarcastic or."
        ],
        [
            "Terministic given this taken action, the environment is going to determine what the next state for the agent is, which we denote by as dash, and which thing is passed back to the agent.",
            "Along with this transition probability, PMS dash given AS the environment also has a."
        ],
        [
            "Reward.",
            "The reward is also function at the state in action."
        ],
        [
            "The idea is that the agent ultimately wants to determine what policy in the usage to maximize the accumulation of rewards as it moves around this cycle."
        ],
        [
            "Now important tool for reinforcement learning is value functions.",
            "Value function is given by the equation shown here and what it means is."
        ],
        [
            "Actually, an indication of the amount of rewards that an agent can expect to receive, given that it's in State S S = 0."
        ],
        [
            "We also put a discount factor as we have an infinite horizon.",
            "This discount factor is between zero and one.",
            "It simply means that we have given more weight to the rewards that come sooner time rather than farther."
        ],
        [
            "Nature.",
            "And obviously our value is also dependent on what policy we choose.",
            "As this one to determine which States and actions we come across and select."
        ],
        [
            "Are the reason why value function is so important is because it indicates what a good state is.",
            "If you have a high value, that's a good state to be in.",
            "And if we want to have a new policy Pi which is going to have a higher will give us more rewards and we can actually find a second group policy by greedily selecting an action that will give us immediately high rewarded and leaders to state with a high value."
        ],
        [
            "Now the value function can also be written in the, which is as the famous velvet equation shown here.",
            "So again, this is an expectation."
        ],
        [
            "However, now we've splits our future, all our rewards into two parts.",
            "We have the immediate reward given by the reward function, as well as all future rewards encapsulated in the value of S dash.",
            "So the value of the next state."
        ],
        [
            "The important thing about validation is that if the value is there for a system for certain system, the true value is invariant underneath this recursion.",
            "So the VS.",
            "Dash and Vos are going to have exactly the same form.",
            "It also be noted that every system has a unique value function and that one always exists."
        ],
        [
            "Now in this talk I'm going to be focusing on actually computing the value function for continuous systems.",
            "And I'm generally assuming, though, that the policy is constant crap."
        ],
        [
            "So let's first of all look at enforcement learning in general and the kind of approaches that we have out there."
        ],
        [
            "So the first type of approach three is a policy search.",
            "In this case, we take data with the agent, takes data from series of rollouts, and accumulates rewards, and using this data it directly determines new policies to choose."
        ],
        [
            "The second type of approach is known as value function methods.",
            "These methods take the data in order to approximate the value function of S and then use this value function to determine improve policies."
        ],
        [
            "The third approach, and the one that only focusing on today is dynamic programming and dynamic programming.",
            "The agent collects data.",
            "Would you then uses to model this system using.",
            "This model is then determine what the value function is and use that value function to color improve policy now as the goal is to today is to learn a value function, I'm going to 1st go look into the."
        ],
        [
            "Value function methods and give an example of how they approach the problem."
        ],
        [
            "So obviously they tried to approach the value directly using a function approximation, and these methods can again be slipping too."
        ],
        [
            "Three subcategories, the first one is Monte Carlo methods.",
            "These methods require the agent to perform long trajectories over and determine actually how much reward they accumulate over these trajectories.",
            "It, and then the function approximation affectively comes to supervised learning."
        ],
        [
            "However, there's quite a lot of variance of these methods."
        ],
        [
            "The second approach, which you just heard about, also is a temporal difference learning.",
            "When you think function approximation, however temporal difference learning."
        ],
        [
            "Please advise solutions and is fairly dependent on the faces features 1 uses.",
            "The third."
        ],
        [
            "Approaches residual gradient methods for these methods.",
            "One performs gradient descent on the residual."
        ],
        [
            "And these will generally lead to a BI solution, unless one can actually double sample states which is not always possible.",
            "And also these methods can be quite slow.",
            "So these are the function value function methods."
        ],
        [
            "Now we're going to look at dynamic programming, so once we actually try to use a model and then determine the value for that model."
        ],
        [
            "Value dynamic programming is really only been solved for two types of system, so the first one is the discrete state system.",
            "So here we have a two state system where some of you might recognize.",
            "I have the models actually defined by a transition table, so it's little you're telling table with one element for each state action and next state pairing.",
            "Sorry for drive thru today.",
            "And we also have a reward table which has one element for each state in action."
        ],
        [
            "For this kind of system, we know that our value function also has a bad tabular form as shown here.",
            "So which has one element for each state?",
            "So the problem with using this kind of approach or can."
        ],
        [
            "Your system is obviously that we would need to discretize our state space in."
        ],
        [
            "Action Space, A continuous situation, continues.",
            "Model is actually the linear quadratic system, in which case the system is represented by a set of linear equations shown in blue.",
            "And also the blue line shown here gets the shift from the current status to the next state.",
            "For this case, we assume that there is a quadratic reward function in both."
        ],
        [
            "Stating the action and in fact our value function also has a quadratic reward as shown here in the equation and in the current state the problem."
        ],
        [
            "This approach is that it will will require us to linearize our systems, which is annoying if we have more than your SIS."
        ],
        [
            "So that should give you a general background.",
            "And now we're going to look at the proposed nonparametric dynamic programming approach.",
            "And this approach uses 3 steps.",
            "The first step is to actually model the system in a flexible nonparametric manner.",
            "The second step is to determine what the form of the value function is for this kind of system.",
            "And the third step is to actually determine evaluate policy and determine the parameters for a value function."
        ],
        [
            "So to begin with, we're going to assume that our knowledge of the system comes from this set of N samples.",
            "For each sample we have, this takes an action, and the next state, as well as a reward."
        ],
        [
            "And what we're going to do is we're going to model the joint distribution POSAS Dash.",
            "So this is the probability of being in a state performing an action and transition into the next state.",
            "This joint distribution is important because it contains both our transition probability of the system as well as our policy.",
            "A given S."
        ],
        [
            "Now, the way that we represent this is in a male parametric manner is using kernel density estimation, so this is shown here where we place a kernel function on each of our samples and we decompose the kernel into three parts, one for the for the next dates.",
            "When for the action of, one for the current state.",
            "What this implies is that a single sample cannot define covariances between, for instance, the states in the next state.",
            "But multiple samples can and the good thing like using current density estimates is that there is a wide range of proofs or whatever kernel density estimates that do actually converge to the true distribution given sufficient number of samples."
        ],
        [
            "For the reward function, given that we have a kernel density estimate model the we can expect.",
            "A another Watson reward function as shown here and you should notice that we actually have the same kernel functions in our reward function as we do in our kernel density estimate."
        ],
        [
            "Now, given this model, we actually want to determine what is the form of our valued function and as member we we do this by determining what function form will be invariant underneath the Bellman equation."
        ],
        [
            "So if we take our common equation and we rewrite it, we can get it in this form shown here.",
            "And as you can see, the actions have actually been integrated at this point as part of being in all policy."
        ],
        [
            "And also notice that this version up here is actually constant, is independent of S and.",
            "And this is actually thought of as the values for our transitions.",
            "So what this should tell?"
        ],
        [
            "Is that if we use again another IO Watson form for our value function?",
            "If we put that form into the VS dash, we will still get a natural Watson form for the VMS, which means that this is actually invariant underneath the Bellman equation and therefore represents the true form of the value function for this."
        ],
        [
            "So.",
            "So here we see it again, this time with the status and again once you notice about the kernel functions are the same ones as used for our kernel density estimate.",
            "And in fact, this value function is completely defined by."
        ],
        [
            "Our model.",
            "So for policy evaluation we need to compute the paper parameters.",
            "Are we do this using the computation show down here?",
            "So either is a vector of the theater parameters?",
            "Or is the vector sample rewards and Lambda is a stochastic or a transition matrix with elements IJ given by this function on the right?",
            "Now, this does require a matrix inversion.",
            "However, it is only a sparse matrix version, which can be done relatively efficiently."
        ],
        [
            "So here we see a summary of the.",
            "Algorithm again, so as inputs we take samples where we have the states.",
            "The next states in their wards.",
            "I've left out the actions here as we don't exceed them explicitly.",
            "And we need kernel functions.",
            "Biocide, although generally these will have the same form as there for the current state in the next state.",
            "So using this model we can actually represent fairly complicated systems.",
            "Given enough samples.",
            "The computations are as shown on the previous slide.",
            "We can compute Lambda beta and then those things to tell us the value function as shown at the bottom.",
            "And these value functions are will actually have a similar basis.",
            "Then as the transitions of our system that allowed the observed transition transition services."
        ],
        [
            "So now we're going to go look into a straightforward illustre Tori numerical value."
        ],
        [
            "Patient in particular, we're going to be looking at this system where we have a sinusoidal transition function, so this doesn't actually mean that the agent is going to go in a sinusoid, but rather that if it's going to move according to the plot on the left.",
            "So for current given state is going to transition to the corresponding next state and see it's a fairly nonlinear dynamics.",
            "And on the right we have effectively cosine, which tells us the current reward for the current stage on top of the transition of the sinusoidal part of the transition.",
            "We also have some noise as indicated by the dashed lines.",
            "To begin with, we want to actually find out what is the true form of the value function for this system, and we can do that using Monte Carlo methods.",
            "So we start off by taking 500,000 samples of this trajectory."
        ],
        [
            "And we get this value function and as you can see it's symmetrical, which is what we expect.",
            "Now we're going to take only 100 samples and see what we get using the nonparametric dynamic programming."
        ],
        [
            "Approach and we get this.",
            "So here you can see we've already gotten the rough shape of the value function.",
            "There is still some offset and we have a mean squared error over the state state space of 3.8.",
            "Three now."
        ],
        [
            "Increases to 200 samples.",
            "We already get a much better fit.",
            "I'm still a little bit rough around the edges, but our mean squared error has gone down to 0.02.",
            "Ah."
        ],
        [
            "Anything close to 300 we get even closer and yeah."
        ],
        [
            "So as you can see.",
            "We can quite accurately predict the true value function using only a limited number of samples, and these are some quite promising initial results.",
            "So here we also see the model that was learned for the 300, so we don't explicitly need to compute this, but for illustration it's always nice to see it.",
            "So on the left we have the kernel density estimate and on the right we have the approximation of the reward function.",
            "Obviously, in the future we hope to implement this on a real robot, and to do that we will need some policy improvement in."
        ],
        [
            "As well.",
            "So that brings me to the conclusion of my talk, so I presented Ultrametric approach to dynamic programming.",
            "The main philosophy behind this approach was to try to approximate a value, a system and then determine the value function for that system, rather than to approximate the value function directly.",
            "By using a non parametric approach we can model the system in a very flexible manner.",
            "And afterwards we can precisely compute the value function for this model.",
            "Well, the benefits of MPP is that it ensures that the basis functions actually match the observed dynamics.",
            "And as you saw, we already have some positive results and hopefully in the future we will have some real robot evaluations.",
            "I'd also like to point out that in the paper there's a second contribution, which is a common framework for deriving not only nonparametric dynamic programming, but these squares, temporal difference learning and kernelized temporal difference.",
            "Learning, such as Gaussian process, temporal difference, learning.",
            "Thank you very much and I'll look for your questions this time.",
            "I was wondering about if you could tell us a few words about was the relationship between this work and the work by or more light and Glenn kernel based reinforcement learning.",
            "So with kernel based reinforcement learning you also have this normalized kernel form, right?",
            "Yes, so I think the best way of describing the relationship that is actually."
        ],
        [
            "If we look here so.",
            "We have these sizes, size and the size so the distribution over the next steps.",
            "If you change that into a Delta function then you should actually derive the same algorithm as this, so this is trying to sort of including more general approach to it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'm gonna be talking about a nonparametric approach to dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "As already said, this is a joint work with my supervisor, John Peters, and we'll both associated with the technical investigate down steps as well as the next step is to for intelligence.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Steps of the work out loud.",
                    "label": 0
                },
                {
                    "sent": "The classical years has mainly been on learning and episodic scenarios specifically.",
                    "label": 1
                },
                {
                    "sent": "Robot learning, as seen here.",
                    "label": 0
                },
                {
                    "sent": "So here we have a robot arm playing table tennis.",
                    "label": 0
                },
                {
                    "sent": "And recently we've been also moving further into actually developing methods for achieving these learn skills together, and today I'm going to be talking about one of the theoretical results that is come out of these endeavors.",
                    "label": 0
                },
                {
                    "sent": "Those of you who visit nips quite often should remember this video from a couple of years ago by again, scuba.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to begin with, I'm going to give a introduction background on reinforcement learning in general.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before going on to my method, so in reinforcement learning, we're really interested in looking at the interaction between agent and its environment at a certain time step T.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Were they just went by itself and in the state S?",
                    "label": 0
                },
                {
                    "sent": "And given the state they just got to choose.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action which we get by A and the way that the agent chooses it is by its policy Pi of a given S. Now this policy can either be sarcastic or.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Terministic given this taken action, the environment is going to determine what the next state for the agent is, which we denote by as dash, and which thing is passed back to the agent.",
                    "label": 0
                },
                {
                    "sent": "Along with this transition probability, PMS dash given AS the environment also has a.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reward.",
                    "label": 0
                },
                {
                    "sent": "The reward is also function at the state in action.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea is that the agent ultimately wants to determine what policy in the usage to maximize the accumulation of rewards as it moves around this cycle.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now important tool for reinforcement learning is value functions.",
                    "label": 0
                },
                {
                    "sent": "Value function is given by the equation shown here and what it means is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, an indication of the amount of rewards that an agent can expect to receive, given that it's in State S S = 0.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also put a discount factor as we have an infinite horizon.",
                    "label": 0
                },
                {
                    "sent": "This discount factor is between zero and one.",
                    "label": 1
                },
                {
                    "sent": "It simply means that we have given more weight to the rewards that come sooner time rather than farther.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nature.",
                    "label": 0
                },
                {
                    "sent": "And obviously our value is also dependent on what policy we choose.",
                    "label": 0
                },
                {
                    "sent": "As this one to determine which States and actions we come across and select.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are the reason why value function is so important is because it indicates what a good state is.",
                    "label": 0
                },
                {
                    "sent": "If you have a high value, that's a good state to be in.",
                    "label": 1
                },
                {
                    "sent": "And if we want to have a new policy Pi which is going to have a higher will give us more rewards and we can actually find a second group policy by greedily selecting an action that will give us immediately high rewarded and leaders to state with a high value.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the value function can also be written in the, which is as the famous velvet equation shown here.",
                    "label": 0
                },
                {
                    "sent": "So again, this is an expectation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, now we've splits our future, all our rewards into two parts.",
                    "label": 0
                },
                {
                    "sent": "We have the immediate reward given by the reward function, as well as all future rewards encapsulated in the value of S dash.",
                    "label": 0
                },
                {
                    "sent": "So the value of the next state.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The important thing about validation is that if the value is there for a system for certain system, the true value is invariant underneath this recursion.",
                    "label": 0
                },
                {
                    "sent": "So the VS.",
                    "label": 0
                },
                {
                    "sent": "Dash and Vos are going to have exactly the same form.",
                    "label": 0
                },
                {
                    "sent": "It also be noted that every system has a unique value function and that one always exists.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now in this talk I'm going to be focusing on actually computing the value function for continuous systems.",
                    "label": 0
                },
                {
                    "sent": "And I'm generally assuming, though, that the policy is constant crap.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's first of all look at enforcement learning in general and the kind of approaches that we have out there.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first type of approach three is a policy search.",
                    "label": 0
                },
                {
                    "sent": "In this case, we take data with the agent, takes data from series of rollouts, and accumulates rewards, and using this data it directly determines new policies to choose.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second type of approach is known as value function methods.",
                    "label": 0
                },
                {
                    "sent": "These methods take the data in order to approximate the value function of S and then use this value function to determine improve policies.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The third approach, and the one that only focusing on today is dynamic programming and dynamic programming.",
                    "label": 1
                },
                {
                    "sent": "The agent collects data.",
                    "label": 0
                },
                {
                    "sent": "Would you then uses to model this system using.",
                    "label": 0
                },
                {
                    "sent": "This model is then determine what the value function is and use that value function to color improve policy now as the goal is to today is to learn a value function, I'm going to 1st go look into the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Value function methods and give an example of how they approach the problem.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So obviously they tried to approach the value directly using a function approximation, and these methods can again be slipping too.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Three subcategories, the first one is Monte Carlo methods.",
                    "label": 1
                },
                {
                    "sent": "These methods require the agent to perform long trajectories over and determine actually how much reward they accumulate over these trajectories.",
                    "label": 0
                },
                {
                    "sent": "It, and then the function approximation affectively comes to supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, there's quite a lot of variance of these methods.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second approach, which you just heard about, also is a temporal difference learning.",
                    "label": 0
                },
                {
                    "sent": "When you think function approximation, however temporal difference learning.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please advise solutions and is fairly dependent on the faces features 1 uses.",
                    "label": 0
                },
                {
                    "sent": "The third.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approaches residual gradient methods for these methods.",
                    "label": 0
                },
                {
                    "sent": "One performs gradient descent on the residual.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these will generally lead to a BI solution, unless one can actually double sample states which is not always possible.",
                    "label": 0
                },
                {
                    "sent": "And also these methods can be quite slow.",
                    "label": 0
                },
                {
                    "sent": "So these are the function value function methods.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're going to look at dynamic programming, so once we actually try to use a model and then determine the value for that model.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Value dynamic programming is really only been solved for two types of system, so the first one is the discrete state system.",
                    "label": 1
                },
                {
                    "sent": "So here we have a two state system where some of you might recognize.",
                    "label": 0
                },
                {
                    "sent": "I have the models actually defined by a transition table, so it's little you're telling table with one element for each state action and next state pairing.",
                    "label": 0
                },
                {
                    "sent": "Sorry for drive thru today.",
                    "label": 1
                },
                {
                    "sent": "And we also have a reward table which has one element for each state in action.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this kind of system, we know that our value function also has a bad tabular form as shown here.",
                    "label": 0
                },
                {
                    "sent": "So which has one element for each state?",
                    "label": 0
                },
                {
                    "sent": "So the problem with using this kind of approach or can.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your system is obviously that we would need to discretize our state space in.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action Space, A continuous situation, continues.",
                    "label": 0
                },
                {
                    "sent": "Model is actually the linear quadratic system, in which case the system is represented by a set of linear equations shown in blue.",
                    "label": 0
                },
                {
                    "sent": "And also the blue line shown here gets the shift from the current status to the next state.",
                    "label": 0
                },
                {
                    "sent": "For this case, we assume that there is a quadratic reward function in both.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stating the action and in fact our value function also has a quadratic reward as shown here in the equation and in the current state the problem.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This approach is that it will will require us to linearize our systems, which is annoying if we have more than your SIS.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that should give you a general background.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to look at the proposed nonparametric dynamic programming approach.",
                    "label": 1
                },
                {
                    "sent": "And this approach uses 3 steps.",
                    "label": 0
                },
                {
                    "sent": "The first step is to actually model the system in a flexible nonparametric manner.",
                    "label": 0
                },
                {
                    "sent": "The second step is to determine what the form of the value function is for this kind of system.",
                    "label": 1
                },
                {
                    "sent": "And the third step is to actually determine evaluate policy and determine the parameters for a value function.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to begin with, we're going to assume that our knowledge of the system comes from this set of N samples.",
                    "label": 0
                },
                {
                    "sent": "For each sample we have, this takes an action, and the next state, as well as a reward.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we're going to do is we're going to model the joint distribution POSAS Dash.",
                    "label": 1
                },
                {
                    "sent": "So this is the probability of being in a state performing an action and transition into the next state.",
                    "label": 0
                },
                {
                    "sent": "This joint distribution is important because it contains both our transition probability of the system as well as our policy.",
                    "label": 0
                },
                {
                    "sent": "A given S.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, the way that we represent this is in a male parametric manner is using kernel density estimation, so this is shown here where we place a kernel function on each of our samples and we decompose the kernel into three parts, one for the for the next dates.",
                    "label": 0
                },
                {
                    "sent": "When for the action of, one for the current state.",
                    "label": 0
                },
                {
                    "sent": "What this implies is that a single sample cannot define covariances between, for instance, the states in the next state.",
                    "label": 0
                },
                {
                    "sent": "But multiple samples can and the good thing like using current density estimates is that there is a wide range of proofs or whatever kernel density estimates that do actually converge to the true distribution given sufficient number of samples.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the reward function, given that we have a kernel density estimate model the we can expect.",
                    "label": 0
                },
                {
                    "sent": "A another Watson reward function as shown here and you should notice that we actually have the same kernel functions in our reward function as we do in our kernel density estimate.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, given this model, we actually want to determine what is the form of our valued function and as member we we do this by determining what function form will be invariant underneath the Bellman equation.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we take our common equation and we rewrite it, we can get it in this form shown here.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, the actions have actually been integrated at this point as part of being in all policy.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also notice that this version up here is actually constant, is independent of S and.",
                    "label": 0
                },
                {
                    "sent": "And this is actually thought of as the values for our transitions.",
                    "label": 0
                },
                {
                    "sent": "So what this should tell?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that if we use again another IO Watson form for our value function?",
                    "label": 0
                },
                {
                    "sent": "If we put that form into the VS dash, we will still get a natural Watson form for the VMS, which means that this is actually invariant underneath the Bellman equation and therefore represents the true form of the value function for this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here we see it again, this time with the status and again once you notice about the kernel functions are the same ones as used for our kernel density estimate.",
                    "label": 0
                },
                {
                    "sent": "And in fact, this value function is completely defined by.",
                    "label": 1
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our model.",
                    "label": 0
                },
                {
                    "sent": "So for policy evaluation we need to compute the paper parameters.",
                    "label": 1
                },
                {
                    "sent": "Are we do this using the computation show down here?",
                    "label": 0
                },
                {
                    "sent": "So either is a vector of the theater parameters?",
                    "label": 0
                },
                {
                    "sent": "Or is the vector sample rewards and Lambda is a stochastic or a transition matrix with elements IJ given by this function on the right?",
                    "label": 0
                },
                {
                    "sent": "Now, this does require a matrix inversion.",
                    "label": 0
                },
                {
                    "sent": "However, it is only a sparse matrix version, which can be done relatively efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we see a summary of the.",
                    "label": 0
                },
                {
                    "sent": "Algorithm again, so as inputs we take samples where we have the states.",
                    "label": 0
                },
                {
                    "sent": "The next states in their wards.",
                    "label": 0
                },
                {
                    "sent": "I've left out the actions here as we don't exceed them explicitly.",
                    "label": 0
                },
                {
                    "sent": "And we need kernel functions.",
                    "label": 0
                },
                {
                    "sent": "Biocide, although generally these will have the same form as there for the current state in the next state.",
                    "label": 0
                },
                {
                    "sent": "So using this model we can actually represent fairly complicated systems.",
                    "label": 0
                },
                {
                    "sent": "Given enough samples.",
                    "label": 0
                },
                {
                    "sent": "The computations are as shown on the previous slide.",
                    "label": 0
                },
                {
                    "sent": "We can compute Lambda beta and then those things to tell us the value function as shown at the bottom.",
                    "label": 0
                },
                {
                    "sent": "And these value functions are will actually have a similar basis.",
                    "label": 0
                },
                {
                    "sent": "Then as the transitions of our system that allowed the observed transition transition services.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're going to go look into a straightforward illustre Tori numerical value.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient in particular, we're going to be looking at this system where we have a sinusoidal transition function, so this doesn't actually mean that the agent is going to go in a sinusoid, but rather that if it's going to move according to the plot on the left.",
                    "label": 0
                },
                {
                    "sent": "So for current given state is going to transition to the corresponding next state and see it's a fairly nonlinear dynamics.",
                    "label": 0
                },
                {
                    "sent": "And on the right we have effectively cosine, which tells us the current reward for the current stage on top of the transition of the sinusoidal part of the transition.",
                    "label": 0
                },
                {
                    "sent": "We also have some noise as indicated by the dashed lines.",
                    "label": 0
                },
                {
                    "sent": "To begin with, we want to actually find out what is the true form of the value function for this system, and we can do that using Monte Carlo methods.",
                    "label": 0
                },
                {
                    "sent": "So we start off by taking 500,000 samples of this trajectory.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we get this value function and as you can see it's symmetrical, which is what we expect.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to take only 100 samples and see what we get using the nonparametric dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approach and we get this.",
                    "label": 0
                },
                {
                    "sent": "So here you can see we've already gotten the rough shape of the value function.",
                    "label": 0
                },
                {
                    "sent": "There is still some offset and we have a mean squared error over the state state space of 3.8.",
                    "label": 0
                },
                {
                    "sent": "Three now.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Increases to 200 samples.",
                    "label": 0
                },
                {
                    "sent": "We already get a much better fit.",
                    "label": 0
                },
                {
                    "sent": "I'm still a little bit rough around the edges, but our mean squared error has gone down to 0.02.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anything close to 300 we get even closer and yeah.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as you can see.",
                    "label": 0
                },
                {
                    "sent": "We can quite accurately predict the true value function using only a limited number of samples, and these are some quite promising initial results.",
                    "label": 1
                },
                {
                    "sent": "So here we also see the model that was learned for the 300, so we don't explicitly need to compute this, but for illustration it's always nice to see it.",
                    "label": 0
                },
                {
                    "sent": "So on the left we have the kernel density estimate and on the right we have the approximation of the reward function.",
                    "label": 0
                },
                {
                    "sent": "Obviously, in the future we hope to implement this on a real robot, and to do that we will need some policy improvement in.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "So that brings me to the conclusion of my talk, so I presented Ultrametric approach to dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "The main philosophy behind this approach was to try to approximate a value, a system and then determine the value function for that system, rather than to approximate the value function directly.",
                    "label": 0
                },
                {
                    "sent": "By using a non parametric approach we can model the system in a very flexible manner.",
                    "label": 1
                },
                {
                    "sent": "And afterwards we can precisely compute the value function for this model.",
                    "label": 1
                },
                {
                    "sent": "Well, the benefits of MPP is that it ensures that the basis functions actually match the observed dynamics.",
                    "label": 0
                },
                {
                    "sent": "And as you saw, we already have some positive results and hopefully in the future we will have some real robot evaluations.",
                    "label": 0
                },
                {
                    "sent": "I'd also like to point out that in the paper there's a second contribution, which is a common framework for deriving not only nonparametric dynamic programming, but these squares, temporal difference learning and kernelized temporal difference.",
                    "label": 1
                },
                {
                    "sent": "Learning, such as Gaussian process, temporal difference, learning.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much and I'll look for your questions this time.",
                    "label": 0
                },
                {
                    "sent": "I was wondering about if you could tell us a few words about was the relationship between this work and the work by or more light and Glenn kernel based reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So with kernel based reinforcement learning you also have this normalized kernel form, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, so I think the best way of describing the relationship that is actually.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look here so.",
                    "label": 0
                },
                {
                    "sent": "We have these sizes, size and the size so the distribution over the next steps.",
                    "label": 0
                },
                {
                    "sent": "If you change that into a Delta function then you should actually derive the same algorithm as this, so this is trying to sort of including more general approach to it.",
                    "label": 0
                }
            ]
        }
    }
}