{
    "id": "jzdaunrjajyzdx3li5g5iptgj4uz2jqw",
    "title": "Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems",
    "info": {
        "author": [
            "Yash Deshpande, Department of Electrical Engineering, Stanford University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_deshpande_submatrix_problems/",
    "segmentation": [
        [
            "So OK, you."
        ],
        [
            "Your data is a graph that is generated in one of two ways under the null hypothesis is a purely random graph where in between each vertex the edge exists with probability 1/2.",
            "On the other hand, the alternative hypothesis is that there exists a clique of size K that is forcibly induced and every other edge exists with probability 1/2 as in the null hypothesis.",
            "The detection problem I will talk about is very simple.",
            "You're given this graph and you just want to distinguish between these two hypothesis.",
            "It's zero in each one.",
            "OK, now."
        ],
        [
            "Well, I'm sure everybody can see here which one is the click, but your data is never ordered and it's never colored."
        ],
        [
            "But there is one more way of looking at this."
        ],
        [
            "Which is in terms of the adjacency matrix.",
            "So if I encode the absence of an edge by a -- 1 and the presence of an edge by a plus one, then under 8 zero the adjacency matrix is basically a symmetric matrix that's full of random signs.",
            "On the other hand, under H1 there exists a principal submatrix indexed by Q, and this contains all plus one second.",
            "The detection problem is just the same."
        ],
        [
            "OK, so here is an outline for the talk.",
            "I'll say a little bit about why we are interested in this problem.",
            "This I'll talk about spectral methods, which are the algorithm of choice and set up the sum of squares relaxation.",
            "In this setting I'll state the main result and then finally I'll say a little bit about the proof."
        ],
        [
            "OK, so the most interesting aspect I think about this problem is that it's a prototypical example of a statistical inference problem where really computational constraints are the key.",
            "The statistically the problem is very simple and easy to understand, but competent computationally is rather more complicated, and you know this phenomenon is related to similar phenomena in other in other inference problems, and in fact, in some cases they are also related, as we will see."
        ],
        [
            "OK, so statistically, what is?",
            "What is the threshold is that basically if K is a log arhythmic in case of order log in or bigger than you can use a brute force search algorithm just to identify the click and then this automatically distinguishes between the hypothesis.",
            "This is follows really for from a very simple second moment calculation and originally for the case of random graphs I think was done by agreement.",
            "But there is all sorts of generalizations for for more interesting models as well."
        ],
        [
            "A computationally speaking really the best algorithm that we know is roughly the spectral method, and this is based on on the following observation, which is that if you take the matrix A and split it into its expectation plus a deviation, the expectation is a rank one matrix you are trying to estimate a rank one matrix in noise.",
            "It's very natural that you just want to do PCA or just estimate the rank one matrix using the principle eigenvector of A and the efficiency of this method really just depends on the signal to noise ratio.",
            "Here the signal to noise ratio is quantified by the ratio of spectral norms.",
            "The spectrum of the signal is easy to compute, is proportional to the size of Q or K, and the spectral norm of the noise.",
            "We know from very standard results in random matrix theory that says that it is about order square root of N. So basically if if K is bigger than square root of N, the principal eigenvector of a has something to do with with the indicator on the click, and then you can clean up this estimator a little bit.",
            "So that is all."
        ],
        [
            "So a connection with sparse PCA and that follows from a very similar observation, which is that not only is the signal rank one, but, but indeed it is also sparse, so there is a bit more structure that hopefully you can exploit, and indeed this has been used."
        ],
        [
            "Introductions by variety of."
        ],
        [
            "People, a lot of whom are actually in the audience.",
            "So."
        ],
        [
            "Number is really the picture is is twofold.",
            "Statistically, brute force algorithm achieves the information theoretic threshold of K about Logan, where is computationally really the best known algorithm that we know requires something of order square root of N. And it's a spectral based algorithm."
        ],
        [
            "OK, so this is naturally led to a search for a variety of computational lower bounds for this problem, and there is early work by Jerome on the Metropolis algorithm that is also work by Vitaly Feldman and coauthors on the notion of algorithm called statistical algorithms, but most closest to our work is work by Feigen crowd camera.",
            "They analyze the lower Shriver hierarchy of semidefinite programs, which attempts to solve combinatorial optimization problems, and what they prove is that the detection problem is hard.",
            "When K is smaller than OK, there is a constant which is, I think 1 / sqrt 2 and then there is sqrt N / 2 to the R. So it's a fairly sharp result for the Arthur."
        ],
        [
            "Found in this semidefinite program.",
            "Really the question is.",
            "Is this regime hard and for what algorithms can we hope to prove that this is hard?",
            "Because really speaking, we don't know how to do average case reductions too well."
        ],
        [
            "So for this talk, I'll restrict to talking about the sum of squares recipe or the sum of squares algorithms.",
            "This is really a very powerful idea for polynomial optimization and has gathered a lot of interest since the work of parrillo and lesser.",
            "It really is A is a recipe for constructing outer approximations to a convex set and and not only are these approximations so they are nested approximation, so so the next in the sequence is better than the one prior.",
            "So it subsumes all previous relaxation schemes, the one that I mentioned over Shriver relaxation scheme.",
            "But also there is one do to share alien items that have been studied in CS literature.",
            "Surprisingly, it also underlies a lot of approximation algorithms, and this was a fact discovered somehow post facto you.",
            "For instance governments.",
            "Williamson Max Cut is very famous example of this.",
            "So really speaking, the sum of squares is a concrete class of algorithms that seems to capture a lot of clever ideas, so it makes sense to try and prove lower bounds in this.",
            "In this proof system or in this set of algorithms."
        ],
        [
            "So what is the?",
            "What is the optimization?",
            "So the standard polynomial optimization for this is very simple.",
            "I'll attach to every vertex in the graph 01 Boolean variable which just indicates membership in the click.",
            "So one is I'm in the click and zero is.",
            "I'm not and I want to maximize the sum of XI, which is the size subject to the Boolean constraint and subject to the second constraint that if a pair of vertices is not connected then it must be that one of the end points is not in the click.",
            "And the key idea of the sum of squares recipe is that I don't want to optimize over points on this feasible set.",
            "I'll optimize over distribution supported on these points, and Moreover I will actually.",
            "More importantly, I will optimize.",
            "I will represent these probability distributions using just a small number of moments."
        ],
        [
            "So OK to set up the relaxation.",
            "I need a little bit of notation so this is a bit of a dense slide, but embraces the set of 1st and integers and choose D is the set of subsets of size at most subsets of size, exactly D for some integer D. And choose less equals D is the set of subsets of size at most D. And the decision variable is something that takes a subset of size at mostly and spits me a number between zero and one.",
            "And the interpretation that I want to keep in the back of the mind that this number is a probability or confidence or belief that this set actually is a part of the click is completely contained."
        ],
        [
            "So this is this is the.",
            "This is the optimization you want to maximize the mass on the singletons.",
            "This is basically the size subject to a couple of constraints.",
            "The first is just normalization.",
            "You want probability distributions to integrate to one.",
            "The second is the 2nd is OK.",
            "Constraints imposed by the relaxation, the third constraint comes from the second identity, which is that if a subset of vertices is not connected in the graph, then it must be that I do not assign positive probability to this subset.",
            "And the 4th constraint is really the most important one, which is that X induces a PSD form or there is a certain moment matrix that you can write given the variable X.",
            "And this must be a PSD matrix.",
            "Now what is this matrix?",
            "It's a matrix on subsets of size D, / 2 and the empty corresponding to S1S2 is just X applied to S1 union S. Pretty simple."
        ],
        [
            "OK, so I'll define the following test, which is that given given a graph and an integer D in order end to the deep time the the algorithm goes and computes the value of the optimal SOS program on the graph G for the integrity and tests.",
            "Whether this is bigger than K or not.",
            "If it is bigger than K, it rejects the null hypothesis and says the alternative is true.",
            "Otherwise it accepts the novel."
        ],
        [
            "OK, so the rationale for this test is kind of very natural.",
            "Why I want to do this test?",
            "Well, imagine that instead of the SOS program I had something that give an article that give me the value of the true optimum or they are optimum for the polynomial optimization program.",
            "This is, of course hard to do.",
            "Or is this?",
            "We do not know how to do, but this is just the natural aesthetic session for for that optimal test."
        ],
        [
            "So OK, this is the main result.",
            "Our main result is a hardness for Ford equals four or the first some non trivial level of the SOS relaxation.",
            "And what we say is that the equal to four level of the hierarchy fails to distinguish that or fails to solve the detection problem when K is smaller than N to the one third and by the twiddle her.",
            "I mean that I'm ignoring log factors because I've heard it once said that water a few logs between friends.",
            "Independently there is.",
            "There is work by make a partition in viktorsson.",
            "They prove that for generality, the regime case smaller than into the 1 / T again ignoring log factors is difficult for for the same class of problems just to compare the results.",
            "This is more general, of course, because it handles larger DD bigger than four, but for the value equals 4, which is the case that we consider the result is a bit weaker."
        ],
        [
            "OK, so actually in reality both of these results back a lot of improvement, right?",
            "Conjecture I think should be that that the for any constant DK smaller than N to the 1/2 times, perhaps a constant or a log arhythmic factor that depends on T should be the right should be the right threshold for this kind of algorithms.",
            "Previously Mecca Viktorsson had had a paper that claimed to do this and set up a lot of machinery for this.",
            "It's a beautiful paper, but unfortunately you know their proof is wrong and for very subtle reasons."
        ],
        [
            "OK, so I'll say a little bit about the proof strategy.",
            "So what is the error rate here?",
            "The error rate is just the fraction of times that I make a false positive plus the fraction of times that I make a false negative.",
            "Or by definition, because I'm doing a relaxation, the true solution is always a solution to the relaxation, so I never make a false negative.",
            "On the other hand, so and the fraction of times I make a false positive is the fraction of times that basically under the null again at the value of the program is large.",
            "So.",
            "To prove a lower bound on the error on detection, it suffices to show that this value is large under the null hypothesis, and to do that I will just construct a feasible point.",
            "And compute the value of that feasible point this autumn.",
            "And since the point is feasible automatically, we're talking about a maximization problem.",
            "So the value is larger than the value of the feasible point."
        ],
        [
            "OK, So what is?",
            "What is this witness for?",
            "D equals for script G of S is is just the indicator that the subset of vertices S induces a click.",
            "I said X of the empty set to be one.",
            "This is just the normalization as usual for every every other set.",
            "I'll set it to be some multiplier Alpha S times the script GFS.",
            "Now the script your face is dictated by by the linear constraint.",
            "It has to be there and I'll make the simplest possible assumption on Alpha S, which is that it is only a function of the size.",
            "So Alpha is just depends on whether it is a set of size 123 or 4.",
            "OK, so again this satisfies the linear constraints automatically and the only thing that I need to worry about is that the moment matrix is PSD.",
            "So let's look at this moment matrix."
        ],
        [
            "It's kind of big.",
            "So it's a matrix that's roughly about 10 ^2 * 10 squared, and then a reality of the proof that this matrix with high probability is positive semidefinite.",
            "But I'll try and concentrate."
        ],
        [
            "Just on two portions, one is the principal submatrix index by the singletons and the 2nd is the principle submatrix index by the pairs and.",
            "This kind of illustrates the problem fairly well."
        ],
        [
            "So I'll just compare the two on the on the right and the left on the left we have the one on the Singleton, so this is an increase in matrix.",
            "On the right we have the submatrix on the pairs.",
            "This is an N choose 2 * N choose two matrix so it's much bigger than your original 1."
        ],
        [
            "The one on the left really is is simple matrix.",
            "It's just the adjacency matrix.",
            "It has all independent entries, where is the one on the right is really a function of the one on the left and hence has highly dependent entries.",
            "This is part of the problem.",
            "We know a lot about the adjacency matrix of graphs.",
            "Arguably we know all we need to know about it, since since the work of Wigner in the 1950s, so there is a wealth of tools from from random matrix theory that we can apply to this problem.",
            "But for the Matrix on the right there is.",
            "No, such no such wealth of information that we can draw, but there is."
        ],
        [
            "This is a toy example of the matrix on the right.",
            "It's kind of part of it, but not really the whole thing, so it's a matrix that's defined on pairs.",
            "I'm not sure we can read the labels, but basically MIJKL is the product of the random variables achy, ijl, AJK, and AGL?",
            "So this is all four of the edges in the graph above.",
            "And the question boils down to how can we control the spectral norm of this matrix?",
            "How do we control the operator now?",
            "Again, this is the tool that we use."
        ],
        [
            "Also.",
            "Does upon the work of it really goes back to Wigner and was used.",
            "In in in random graph since the 1980s, perhaps even earlier for and in the in, the point is basically very simple to compute the operator norm or to control the operator norm, it suffices to compute large trace power.",
            "OK, for the case of for the Wigner matrix on the left.",
            "This is kind of easy.",
            "It amounts to doing.",
            "Combinatorics on cycles morally so you have a cycle of length 2R and every vertex in the cycle depends on every vertex in the cycle corresponds to a row or a column of the matrix, and every edge corresponds to an entry.",
            "On the other hand, for the matrix that we really care about, this is a little more complicated.",
            "So."
        ],
        [
            "For R = 2, There is a cycle of length four and again in the.",
            "In the toy example, really of two similar combinatorics on on.",
            "Really, these ribbons and more complicated examples of these ribbons."
        ],
        [
            "So I'll conclude by saying that I talked about just a very specific model for the hearing click problem, But the results are subject to a fair amount of generalization I think, and to some extent we do that in the paper.",
            "We have some tools that allow us to bound spectral norms of felling very nonstandard examples of random matrices and hopefully this can be applied elsewhere, perhaps in similar proofs or or in other settings, and I'll end with the open problem, which I think is very interesting, which is that can you prove that K smaller than square root of N is hard for SD or even just SOS 4?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your data is a graph that is generated in one of two ways under the null hypothesis is a purely random graph where in between each vertex the edge exists with probability 1/2.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the alternative hypothesis is that there exists a clique of size K that is forcibly induced and every other edge exists with probability 1/2 as in the null hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The detection problem I will talk about is very simple.",
                    "label": 1
                },
                {
                    "sent": "You're given this graph and you just want to distinguish between these two hypothesis.",
                    "label": 0
                },
                {
                    "sent": "It's zero in each one.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, I'm sure everybody can see here which one is the click, but your data is never ordered and it's never colored.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there is one more way of looking at this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is in terms of the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So if I encode the absence of an edge by a -- 1 and the presence of an edge by a plus one, then under 8 zero the adjacency matrix is basically a symmetric matrix that's full of random signs.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, under H1 there exists a principal submatrix indexed by Q, and this contains all plus one second.",
                    "label": 0
                },
                {
                    "sent": "The detection problem is just the same.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here is an outline for the talk.",
                    "label": 0
                },
                {
                    "sent": "I'll say a little bit about why we are interested in this problem.",
                    "label": 0
                },
                {
                    "sent": "This I'll talk about spectral methods, which are the algorithm of choice and set up the sum of squares relaxation.",
                    "label": 0
                },
                {
                    "sent": "In this setting I'll state the main result and then finally I'll say a little bit about the proof.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the most interesting aspect I think about this problem is that it's a prototypical example of a statistical inference problem where really computational constraints are the key.",
                    "label": 0
                },
                {
                    "sent": "The statistically the problem is very simple and easy to understand, but competent computationally is rather more complicated, and you know this phenomenon is related to similar phenomena in other in other inference problems, and in fact, in some cases they are also related, as we will see.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so statistically, what is?",
                    "label": 0
                },
                {
                    "sent": "What is the threshold is that basically if K is a log arhythmic in case of order log in or bigger than you can use a brute force search algorithm just to identify the click and then this automatically distinguishes between the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "This is follows really for from a very simple second moment calculation and originally for the case of random graphs I think was done by agreement.",
                    "label": 0
                },
                {
                    "sent": "But there is all sorts of generalizations for for more interesting models as well.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A computationally speaking really the best algorithm that we know is roughly the spectral method, and this is based on on the following observation, which is that if you take the matrix A and split it into its expectation plus a deviation, the expectation is a rank one matrix you are trying to estimate a rank one matrix in noise.",
                    "label": 0
                },
                {
                    "sent": "It's very natural that you just want to do PCA or just estimate the rank one matrix using the principle eigenvector of A and the efficiency of this method really just depends on the signal to noise ratio.",
                    "label": 0
                },
                {
                    "sent": "Here the signal to noise ratio is quantified by the ratio of spectral norms.",
                    "label": 0
                },
                {
                    "sent": "The spectrum of the signal is easy to compute, is proportional to the size of Q or K, and the spectral norm of the noise.",
                    "label": 0
                },
                {
                    "sent": "We know from very standard results in random matrix theory that says that it is about order square root of N. So basically if if K is bigger than square root of N, the principal eigenvector of a has something to do with with the indicator on the click, and then you can clean up this estimator a little bit.",
                    "label": 0
                },
                {
                    "sent": "So that is all.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a connection with sparse PCA and that follows from a very similar observation, which is that not only is the signal rank one, but, but indeed it is also sparse, so there is a bit more structure that hopefully you can exploit, and indeed this has been used.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Introductions by variety of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "People, a lot of whom are actually in the audience.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Number is really the picture is is twofold.",
                    "label": 0
                },
                {
                    "sent": "Statistically, brute force algorithm achieves the information theoretic threshold of K about Logan, where is computationally really the best known algorithm that we know requires something of order square root of N. And it's a spectral based algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is naturally led to a search for a variety of computational lower bounds for this problem, and there is early work by Jerome on the Metropolis algorithm that is also work by Vitaly Feldman and coauthors on the notion of algorithm called statistical algorithms, but most closest to our work is work by Feigen crowd camera.",
                    "label": 0
                },
                {
                    "sent": "They analyze the lower Shriver hierarchy of semidefinite programs, which attempts to solve combinatorial optimization problems, and what they prove is that the detection problem is hard.",
                    "label": 0
                },
                {
                    "sent": "When K is smaller than OK, there is a constant which is, I think 1 / sqrt 2 and then there is sqrt N / 2 to the R. So it's a fairly sharp result for the Arthur.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Found in this semidefinite program.",
                    "label": 0
                },
                {
                    "sent": "Really the question is.",
                    "label": 0
                },
                {
                    "sent": "Is this regime hard and for what algorithms can we hope to prove that this is hard?",
                    "label": 0
                },
                {
                    "sent": "Because really speaking, we don't know how to do average case reductions too well.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for this talk, I'll restrict to talking about the sum of squares recipe or the sum of squares algorithms.",
                    "label": 1
                },
                {
                    "sent": "This is really a very powerful idea for polynomial optimization and has gathered a lot of interest since the work of parrillo and lesser.",
                    "label": 1
                },
                {
                    "sent": "It really is A is a recipe for constructing outer approximations to a convex set and and not only are these approximations so they are nested approximation, so so the next in the sequence is better than the one prior.",
                    "label": 0
                },
                {
                    "sent": "So it subsumes all previous relaxation schemes, the one that I mentioned over Shriver relaxation scheme.",
                    "label": 0
                },
                {
                    "sent": "But also there is one do to share alien items that have been studied in CS literature.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly, it also underlies a lot of approximation algorithms, and this was a fact discovered somehow post facto you.",
                    "label": 0
                },
                {
                    "sent": "For instance governments.",
                    "label": 0
                },
                {
                    "sent": "Williamson Max Cut is very famous example of this.",
                    "label": 0
                },
                {
                    "sent": "So really speaking, the sum of squares is a concrete class of algorithms that seems to capture a lot of clever ideas, so it makes sense to try and prove lower bounds in this.",
                    "label": 0
                },
                {
                    "sent": "In this proof system or in this set of algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the?",
                    "label": 0
                },
                {
                    "sent": "What is the optimization?",
                    "label": 0
                },
                {
                    "sent": "So the standard polynomial optimization for this is very simple.",
                    "label": 1
                },
                {
                    "sent": "I'll attach to every vertex in the graph 01 Boolean variable which just indicates membership in the click.",
                    "label": 0
                },
                {
                    "sent": "So one is I'm in the click and zero is.",
                    "label": 0
                },
                {
                    "sent": "I'm not and I want to maximize the sum of XI, which is the size subject to the Boolean constraint and subject to the second constraint that if a pair of vertices is not connected then it must be that one of the end points is not in the click.",
                    "label": 0
                },
                {
                    "sent": "And the key idea of the sum of squares recipe is that I don't want to optimize over points on this feasible set.",
                    "label": 1
                },
                {
                    "sent": "I'll optimize over distribution supported on these points, and Moreover I will actually.",
                    "label": 0
                },
                {
                    "sent": "More importantly, I will optimize.",
                    "label": 0
                },
                {
                    "sent": "I will represent these probability distributions using just a small number of moments.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK to set up the relaxation.",
                    "label": 0
                },
                {
                    "sent": "I need a little bit of notation so this is a bit of a dense slide, but embraces the set of 1st and integers and choose D is the set of subsets of size at most subsets of size, exactly D for some integer D. And choose less equals D is the set of subsets of size at most D. And the decision variable is something that takes a subset of size at mostly and spits me a number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And the interpretation that I want to keep in the back of the mind that this number is a probability or confidence or belief that this set actually is a part of the click is completely contained.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the optimization you want to maximize the mass on the singletons.",
                    "label": 0
                },
                {
                    "sent": "This is basically the size subject to a couple of constraints.",
                    "label": 0
                },
                {
                    "sent": "The first is just normalization.",
                    "label": 0
                },
                {
                    "sent": "You want probability distributions to integrate to one.",
                    "label": 0
                },
                {
                    "sent": "The second is the 2nd is OK.",
                    "label": 0
                },
                {
                    "sent": "Constraints imposed by the relaxation, the third constraint comes from the second identity, which is that if a subset of vertices is not connected in the graph, then it must be that I do not assign positive probability to this subset.",
                    "label": 0
                },
                {
                    "sent": "And the 4th constraint is really the most important one, which is that X induces a PSD form or there is a certain moment matrix that you can write given the variable X.",
                    "label": 0
                },
                {
                    "sent": "And this must be a PSD matrix.",
                    "label": 0
                },
                {
                    "sent": "Now what is this matrix?",
                    "label": 0
                },
                {
                    "sent": "It's a matrix on subsets of size D, / 2 and the empty corresponding to S1S2 is just X applied to S1 union S. Pretty simple.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'll define the following test, which is that given given a graph and an integer D in order end to the deep time the the algorithm goes and computes the value of the optimal SOS program on the graph G for the integrity and tests.",
                    "label": 0
                },
                {
                    "sent": "Whether this is bigger than K or not.",
                    "label": 0
                },
                {
                    "sent": "If it is bigger than K, it rejects the null hypothesis and says the alternative is true.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it accepts the novel.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the rationale for this test is kind of very natural.",
                    "label": 0
                },
                {
                    "sent": "Why I want to do this test?",
                    "label": 0
                },
                {
                    "sent": "Well, imagine that instead of the SOS program I had something that give an article that give me the value of the true optimum or they are optimum for the polynomial optimization program.",
                    "label": 0
                },
                {
                    "sent": "This is, of course hard to do.",
                    "label": 0
                },
                {
                    "sent": "Or is this?",
                    "label": 0
                },
                {
                    "sent": "We do not know how to do, but this is just the natural aesthetic session for for that optimal test.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, this is the main result.",
                    "label": 0
                },
                {
                    "sent": "Our main result is a hardness for Ford equals four or the first some non trivial level of the SOS relaxation.",
                    "label": 1
                },
                {
                    "sent": "And what we say is that the equal to four level of the hierarchy fails to distinguish that or fails to solve the detection problem when K is smaller than N to the one third and by the twiddle her.",
                    "label": 0
                },
                {
                    "sent": "I mean that I'm ignoring log factors because I've heard it once said that water a few logs between friends.",
                    "label": 0
                },
                {
                    "sent": "Independently there is.",
                    "label": 0
                },
                {
                    "sent": "There is work by make a partition in viktorsson.",
                    "label": 0
                },
                {
                    "sent": "They prove that for generality, the regime case smaller than into the 1 / T again ignoring log factors is difficult for for the same class of problems just to compare the results.",
                    "label": 0
                },
                {
                    "sent": "This is more general, of course, because it handles larger DD bigger than four, but for the value equals 4, which is the case that we consider the result is a bit weaker.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so actually in reality both of these results back a lot of improvement, right?",
                    "label": 0
                },
                {
                    "sent": "Conjecture I think should be that that the for any constant DK smaller than N to the 1/2 times, perhaps a constant or a log arhythmic factor that depends on T should be the right should be the right threshold for this kind of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Previously Mecca Viktorsson had had a paper that claimed to do this and set up a lot of machinery for this.",
                    "label": 0
                },
                {
                    "sent": "It's a beautiful paper, but unfortunately you know their proof is wrong and for very subtle reasons.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'll say a little bit about the proof strategy.",
                    "label": 0
                },
                {
                    "sent": "So what is the error rate here?",
                    "label": 0
                },
                {
                    "sent": "The error rate is just the fraction of times that I make a false positive plus the fraction of times that I make a false negative.",
                    "label": 0
                },
                {
                    "sent": "Or by definition, because I'm doing a relaxation, the true solution is always a solution to the relaxation, so I never make a false negative.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, so and the fraction of times I make a false positive is the fraction of times that basically under the null again at the value of the program is large.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To prove a lower bound on the error on detection, it suffices to show that this value is large under the null hypothesis, and to do that I will just construct a feasible point.",
                    "label": 0
                },
                {
                    "sent": "And compute the value of that feasible point this autumn.",
                    "label": 0
                },
                {
                    "sent": "And since the point is feasible automatically, we're talking about a maximization problem.",
                    "label": 0
                },
                {
                    "sent": "So the value is larger than the value of the feasible point.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what is?",
                    "label": 0
                },
                {
                    "sent": "What is this witness for?",
                    "label": 0
                },
                {
                    "sent": "D equals for script G of S is is just the indicator that the subset of vertices S induces a click.",
                    "label": 0
                },
                {
                    "sent": "I said X of the empty set to be one.",
                    "label": 0
                },
                {
                    "sent": "This is just the normalization as usual for every every other set.",
                    "label": 0
                },
                {
                    "sent": "I'll set it to be some multiplier Alpha S times the script GFS.",
                    "label": 0
                },
                {
                    "sent": "Now the script your face is dictated by by the linear constraint.",
                    "label": 0
                },
                {
                    "sent": "It has to be there and I'll make the simplest possible assumption on Alpha S, which is that it is only a function of the size.",
                    "label": 0
                },
                {
                    "sent": "So Alpha is just depends on whether it is a set of size 123 or 4.",
                    "label": 0
                },
                {
                    "sent": "OK, so again this satisfies the linear constraints automatically and the only thing that I need to worry about is that the moment matrix is PSD.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this moment matrix.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's kind of big.",
                    "label": 0
                },
                {
                    "sent": "So it's a matrix that's roughly about 10 ^2 * 10 squared, and then a reality of the proof that this matrix with high probability is positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "But I'll try and concentrate.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just on two portions, one is the principal submatrix index by the singletons and the 2nd is the principle submatrix index by the pairs and.",
                    "label": 0
                },
                {
                    "sent": "This kind of illustrates the problem fairly well.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll just compare the two on the on the right and the left on the left we have the one on the Singleton, so this is an increase in matrix.",
                    "label": 0
                },
                {
                    "sent": "On the right we have the submatrix on the pairs.",
                    "label": 0
                },
                {
                    "sent": "This is an N choose 2 * N choose two matrix so it's much bigger than your original 1.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The one on the left really is is simple matrix.",
                    "label": 0
                },
                {
                    "sent": "It's just the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "It has all independent entries, where is the one on the right is really a function of the one on the left and hence has highly dependent entries.",
                    "label": 0
                },
                {
                    "sent": "This is part of the problem.",
                    "label": 0
                },
                {
                    "sent": "We know a lot about the adjacency matrix of graphs.",
                    "label": 0
                },
                {
                    "sent": "Arguably we know all we need to know about it, since since the work of Wigner in the 1950s, so there is a wealth of tools from from random matrix theory that we can apply to this problem.",
                    "label": 0
                },
                {
                    "sent": "But for the Matrix on the right there is.",
                    "label": 0
                },
                {
                    "sent": "No, such no such wealth of information that we can draw, but there is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a toy example of the matrix on the right.",
                    "label": 0
                },
                {
                    "sent": "It's kind of part of it, but not really the whole thing, so it's a matrix that's defined on pairs.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure we can read the labels, but basically MIJKL is the product of the random variables achy, ijl, AJK, and AGL?",
                    "label": 0
                },
                {
                    "sent": "So this is all four of the edges in the graph above.",
                    "label": 0
                },
                {
                    "sent": "And the question boils down to how can we control the spectral norm of this matrix?",
                    "label": 0
                },
                {
                    "sent": "How do we control the operator now?",
                    "label": 0
                },
                {
                    "sent": "Again, this is the tool that we use.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "Does upon the work of it really goes back to Wigner and was used.",
                    "label": 0
                },
                {
                    "sent": "In in in random graph since the 1980s, perhaps even earlier for and in the in, the point is basically very simple to compute the operator norm or to control the operator norm, it suffices to compute large trace power.",
                    "label": 0
                },
                {
                    "sent": "OK, for the case of for the Wigner matrix on the left.",
                    "label": 0
                },
                {
                    "sent": "This is kind of easy.",
                    "label": 0
                },
                {
                    "sent": "It amounts to doing.",
                    "label": 0
                },
                {
                    "sent": "Combinatorics on cycles morally so you have a cycle of length 2R and every vertex in the cycle depends on every vertex in the cycle corresponds to a row or a column of the matrix, and every edge corresponds to an entry.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, for the matrix that we really care about, this is a little more complicated.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For R = 2, There is a cycle of length four and again in the.",
                    "label": 1
                },
                {
                    "sent": "In the toy example, really of two similar combinatorics on on.",
                    "label": 0
                },
                {
                    "sent": "Really, these ribbons and more complicated examples of these ribbons.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll conclude by saying that I talked about just a very specific model for the hearing click problem, But the results are subject to a fair amount of generalization I think, and to some extent we do that in the paper.",
                    "label": 0
                },
                {
                    "sent": "We have some tools that allow us to bound spectral norms of felling very nonstandard examples of random matrices and hopefully this can be applied elsewhere, perhaps in similar proofs or or in other settings, and I'll end with the open problem, which I think is very interesting, which is that can you prove that K smaller than square root of N is hard for SD or even just SOS 4?",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}