{
    "id": "tesedc5ayylapenxfye3skwvrg7fdtkq",
    "title": "Spectral Clustering and Embedding with Hidden Markov Models",
    "info": {
        "author": [
            "Tony Jebara, Columbia University"
        ],
        "published": "Jan. 29, 2008",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/ecml07_jebara_sce/",
    "segmentation": [
        [
            "OK, I've been enjoying the conference so far, including this session Ann.",
            "I'm going to be talking about some initially applied work we started doing in this space and it's led to some interesting theoretical results, so I'll be showing mostly applied results.",
            "We have a paper coming up and nips where we've seen some connections to semiparametric density estimation.",
            "Start off some intuition and then got some math out of it.",
            "So what I'll be talking bout is spectral clustering and embedding with hidden Markov."
        ],
        [
            "Models.",
            "And the basic goal is going to be combining 2 themes, an unsupervised learning which is parametric learning and nonparametric learning.",
            "And these two counter that around for awhile in density estimation.",
            "People know about nonparametric learning as kernel density estimation.",
            "And I'll be comparing that to parametric learning.",
            "The standard maximum likelihood approaches for the exponential family and showing some intermediate, which is semiparametric likelihoods, and we'll talk about that but only briefly, 'cause that's a nips, but for this talk I'll be looking at clustering where we're going to mix between parametric types of knowledge and non parametric types of knowledge together because they both have complementary advantages and come up with something like a semi parametric clustering, which is this idea.",
            "Combining hidden Markov models with spectral clustering and so in contrast to fully parametric EM types of clustering or fully nonparametric vanilla spectral clustering.",
            "And they will bridge these two things together.",
            "The parametric and nonparametric are through these kernels, which are kernels between probability models.",
            "Parametric probability models, typically kernels are non parametric types of things, but now will be using them within with parametric probability models.",
            "Like the hidden Markov model, and then I'll show some spectral clustering results and then embedding or multidimensional scaling with external between hidden Markov models.",
            "So so think about it as a way of defining kernel between."
        ],
        [
            "Hmm's.",
            "OK, so.",
            "Parametric methods.",
            "There's many different applications of parametric thinking.",
            "Density estimation is a standard one.",
            "People usually think of parametric maximum likelihood as exponential family models, where you have sufficient statistics and that's summarizes the whole data set.",
            "So you don't need to keep the density around or the samples around, you can just compute means of sufficient statistics and what's nice about parametric methods is the number of parameters does not grow with the data.",
            "So if I see a million data points or 100 data points, the parameter size is the same, and Meanwhile nonparametric techniques they might have parameters.",
            "But the key concept is that the number of parameters grows with the size of the data set.",
            "And so things like density estimator estimators that are nonparametric like parzen estimators or L1 smoothing or infinite mixture models, the amount of modeling is going to get more complicated as we see more and more data.",
            "And similarly for the clustering, the clusters might get more complicated in your space if you do spectral clustering, which is a non parametric method, whereas an M. Let's say you're doing 2 Gaussian mixture modeling.",
            "The description of each cluster does not get more complicated as you see more samples even though there aren't sufficient autistics.",
            "Percent when you run them.",
            "Similarly visualization there's nonparametric approaches like multidimensional scaling, locally linear embedding and so on.",
            "But one disadvantage about nonparametric techniques is you don't have the same richness of mixtures.",
            "Bayesian networks, hidden Markov models, all that modeling that is usually associated with parametric methods.",
            "Instead, you have to work with kernels and distance metrics and graphs on data, and it might be awkward if you do have some of this modeling type of knowledge."
        ],
        [
            "Stairs.",
            "So here's a quick preview of what I mean by going to semi parametric types of density estimations.",
            "The most general density estimator is you're given samples and you want to learn a probability probability density function over all these samples so that P is really almost two general right?",
            "It's going to just give you.",
            "A probability value over all observations of N data points.",
            "OK, so the first step is to assume that these samples are at least independent, independently distributed and so that probability function really factorizes into a product of N. And probabilities over each individual data point.",
            "That's the ID assumption that's made in the nonparametric approaches, which are more general.",
            "Parametric approaches.",
            "Go step further and say that not only are these is this big P distribution factorized, all the marginals are.",
            "These are the same, they are identical and we drop that subscript an off of the Theta and say they're all the same probability model.",
            "So one thing we've been looking at his waist to combine the two, an one approach is to do semi parametric.",
            "Density estimation, where we say in addition to the ID assumption, we're going to have these kernels between all the probability models that tell them try to be similar to each other.",
            "OK, so don't force everyone to be exactly the same like a parametric approach, just put push the models together through a kernel an that kernel is a probability kernel, which just basically integrates one distribution and another over the sample space, and that tells you how similar they are.",
            "They integrate nicely on top of each other, going to get a lot of weight, and they're happy."
        ],
        [
            "And they're similar.",
            "OK, so this is a rough picture of what we're talking about.",
            "Nonparametric density estimation?",
            "Just put a single.",
            "Gaussian on each model on each point gets its own model, and they don't communicate at all.",
            "Parametric says there's a single.",
            "Gaussian for the whole data set.",
            "Now we're talking bout Semiparametric where we're also going to put some glue between the models to pull them together and make them similar to each other.",
            "And we're going to get something like this where we're pulling models together with this integral.",
            "Again, this kernel called the probability product kernel product of both probability integrate."
        ],
        [
            "Over the sample space.",
            "We've introduced this awhile ago.",
            "It's a natural way of computing similarity between two distributions.",
            "You just have two distributions.",
            "Let's say here's one, and here's another, and we integrate them again using this formula over the sample space, and you could take the distributions that power of beta, which just changes the kernel a little bit.",
            "We typically use data equals one, or beta equals 1/2.",
            "This is a non negative quantity 'cause it's integral to distributions, and if yous beta equals 1/2, that integral is always equal to 1.",
            "If P = P prime.",
            "With the two distributions are the same.",
            "OK, so measures the overlap."
        ],
        [
            "Of the two distributions, what's nice is this can be computed close form for any distributions in the exponential family, so if I'm trying to pull Gaussians together or multinomial together, or place on distributions together, the math always looks like this for the distribution.",
            "That's the natural form for an exponential family.",
            "Some linear function of the data times some linear function of the model.",
            "Alright, that's the standard exponential family formed.",
            "The kernel in that situation when you do this integral is just this simple formula involving the cumulant generating function or partition function.",
            "So the math is really simple, you have to worry about solving these integrals for the exponential family case.",
            "And for the Gaussian case with just Gaussian means we get back RBF kernels.",
            "So if you have a Gaussian on each point and you integrate their overlap, it's like an RBF.",
            "But now we can talk about other types of distributions.",
            "Have general covariance Gaussian?"
        ],
        [
            "Hands or multinomial's or hidden Markov models.",
            "So what's the overlap between 2 hidden Markov models?",
            "So hidden Markov model has this type of distribution?",
            "It's basically summing out over all these hidden configurations on the backbone of the chain.",
            "And the number of possible configurations in this Markov model is the size of the hidden states based on the power of T. The length of this chain.",
            "And we've got the standard Markov assumption, which is that the states transition with this transition table and you only look at the previous state.",
            "There's only one arrow you don't have to go backwards in time more than once that.",
            "OK, so this is vanilla hmm modeling and we can go and compute the kernel between two Hmm's by just integrating the density of 1 HMM model times the density of the other HMM models and integrating over the whole sample space.",
            "All possible sequences of length T. OK, but that integral is going to look really nasty because we have to sum over all the hidden configurations.",
            "All as possibilities for one.",
            "Hmm and all you possibilities for the other hmm, and then do these integrals over their output space or emissions.",
            "So for two Hmm's, one with the state space of size S, let's say 3 state HMM and a four state hmm with sysdate space you.",
            "That's after the power of T times due to the power of T. So that's a nightmare, but."
        ],
        [
            "We're not going to suggest you do that.",
            "Instead, there's an efficient way of computing.",
            "This kernel, without going through this brute force cross product using a, basically a forward backward type of algorithm.",
            "And you can do this with other types of Bayesian networks wherever there's a nice junction tree algorithm as well, so that big summation over S and you all the possible states sequences is replaced by basically realizing we can compute a sub kernel over only the emissions.",
            "Of each hmm cross, the emissions of the other, hmm.",
            "And then what I'm basically showing is that this is.",
            "Now looking like a big junction tree where were instead of solving over all the all the possible hidden states, we're going to basically stack one HMM on top of the other.",
            "This way look at their common emissions and group these into large cliques so this state goes with this date.",
            "That's it goes in this day and we get this.",
            "Chain OK and we do four backward on this chain, but now the size of these cliques is.",
            "S by U cross.",
            "S by you.",
            "OK, so actually this is wrong.",
            "This is TS cross U yeah so.",
            "That should be squared because these are clean for this size, so there's a squared missing over here, but that's much better than the exponential brute force calculation."
        ],
        [
            "So the point is, we can compute this kernel between two Hmm's and here's the pseudocode for it.",
            "You have two Gaussian Hmm's for each, Gaussian for each state of hmm, one and each Gaussian of each state of hmm.",
            "Two, we compute a scalar value, which is how similar are these Gaussians?",
            "So this is for a 3 state HMM cross, the four state hmm a 3 by 4 table which compares all their Gaussians pairwise.",
            "And that's the formula is saying how similar is is my Gaussian number one to your Gaussian #3.",
            "And that scale or just gets plugged into a table.",
            "And then here's simple pseudocode that runs down the chain and computes the kernel.",
            "And you can basically see we have to do T steps and some over the states over here this way.",
            "And so that's very fast and we get this kernel which tells you."
        ],
        [
            "Similar to Hmm's are so how do we use this with clustering?",
            "So unlike the semi parametric density estimation, now we're going to see parametric clustering.",
            "If we had fully parametric clustering, we'd just do EM, and we'd say there's two.",
            "Types of classes ASI negative and ASI positive.",
            "We don't know which sequence this came from.",
            "Was this a sequence of running or sequence of walking?",
            "Let's say C + C minus?",
            "So that's what M is going to maximize.",
            "Its going to maximize this log likelihood over this unknown labeling.",
            "Nonparametric clustering like spectral clustering or Max cut.",
            "Basically tries to find a labeling so that you maximize some kind of cut along all the pairs of edges between your data which are computed with the kernel.",
            "So we're going to be proposing is semi parametric clustering where we're going to fit the data using maximum likelihood so that each model is estimated from.",
            "Its own sequence.",
            "But then when we go out and compute the spectral clustering, we use the kernel between the probability models.",
            "OK, so we estimate this.",
            "Then we estimate that.",
            "So why not do these jointly?",
            "Well, that's in the NIPS paper.",
            "We're not going to do that for now, we're just going to find the best Hmm's for each sequence and then do the spectral clustering using the probability product kernel on Hmm's.",
            "How am I doing on time?",
            "What's?",
            "7 minutes 7 minutes OK great."
        ],
        [
            "OK, so parametric em clustering.",
            "I'm not going to spend too much time on this, but basically you want to maximize that incomplete log likelihood you're given, let's say two models, one per class.",
            "We compute their responsibilities.",
            "So how much this data point likes to be with model one versus how much?",
            "I'd like to be with Model 2 and then we maximize the expected complete likelihood so the Gaussians gobble up their data points.",
            "If the X is are sequences, then we do clustering with two HMM models and the better HMM steals that sequence away from the weaker HMM.",
            "They gobble up sequences and do this conquer and divide, and that's the aloneness Claire off paper where they clustered motion capture data of people running and walking around using mixtures of HMM this way.",
            "So the step mom step are basically the same as what they are for mixtures of Gaussians, it's just that now you've got no.",
            "Not only is the class unknown, the hidden states are also unknown, so we have to sum over that downstairs as well in the expected."
        ],
        [
            "Fleet likelihood.",
            "So yeah, it works great if you have a mixture of two Gaussians and if your assumption was correct to begin with, this parametric assumption.",
            "But if the data wasn't actually a mixture of two Gaussians, it was actually a mixture of some stringy distribution and some other one.",
            "Then it's not going to work very well and the same thing will happen with Hmm's.",
            "So if you're watching two people walked one person walking, the other person running, if there's some drift in the running sequences and some drift in The Walking sequences, just like this red and black drift.",
            "You're not going to cluster the people together properly.",
            "You're not going to say.",
            "Oh, here's the runner.",
            "And here's the Walker.",
            "Just in the same way that you're not going to cluster things properly with Para."
        ],
        [
            "Metric EM.",
            "And so we'd like to use nonparametric spectral clustering, which is agnostic about the shape of the clusters, so I don't know what the shape is.",
            "I'm just going to use spectral clustering and the most popular one these days is this stabilized clustering, buying Wison Jordan and we find the top eigenvectors of the normalized Laplacian.",
            "Which takes your kernel matrix.",
            "Just normalize it when you compute it between all pairs and typically use the RBF affinity to get the kernels, so instead, so that gives you these types of nice agnostic clusterings, but instead of the RBF affinity, let's use the probability product kernel as a surrogate because we might have each one of these points might be a time series.",
            "We can't put a Gaussian on it, we have to put in HMM on it.",
            "There's other competitor kernels for time series, like the Yen Yang Kernel, but they don't really make these types of parametric assumptions on the data set.",
            "For example, I might tell you each one of these is generated by a two state hidden Markov model.",
            "Each data point is a hidden Markov model you want to exploit.",
            "That parametric assumption about each point, but you don't know the shape of the clouds.",
            "The points form, so you want to put parametric knowledge at a small scale, but not at the large."
        ],
        [
            "Scale your data.",
            "So here's what we mean by kind of a Walker Runner runner.",
            "We took somebody walking and change the angle of their walk by rotating this 3D motion capture data.",
            "So here's the Walker and it forms the outer ring, and the runner is the inner ring.",
            "And they're just time series sequences.",
            "But The Walking in the running is slowly rotating, so if you do them on this, it's not going to work very well.",
            "But spectral clustering will actually pull out the perfect clustering if you model each persons are two state HMM.",
            "That's a great model, because running and walking have these two phases.",
            "You're basically you're."
        ],
        [
            "Put forward your right foot forward, so here's the pseudocode for each time series you fit in.",
            "Hmm, we compute this kernel between all pairs and then we do spectral clustering on this weight weighted graph.",
            "So here's the kernel again, and then we just want to cut this into different classes.",
            "And it's a nonparametric spectral clustering.",
            "For embedding, we use multidimensional scaling or Eli or SDE.",
            "Anyone these popular packages which also needs a pairwise kernel between the things you're trying to visual."
        ],
        [
            "So here's pseudocode again.",
            "What I just said before, and this is the details of the spectral clustering algorithm.",
            "So three of these lines are carbon copies of England Jordan.",
            "I'm not going to spend too much."
        ],
        [
            "Time on that.",
            "And here are the results.",
            "So the first one was just looking at walking and running and rotating the angles.",
            "So if you rotate only a little bit, The Walking in the running, you don't go too far with it.",
            "All the clustering techniques work just as well.",
            "Mm-hmm works well.",
            "K means on HMM source well and this time series kernel by Eden Yang works well.",
            "But as you start rotating the data.",
            "Then you start confusing which cluster should go which point should go with which cluster.",
            "But the spectral technique keeps working very well.",
            "OK, it always separates The Walking from the running, whereas the other techniques slowly start to degenerate and are putting, walking and running into the same cluster, so they should keep them separate and you can see that starts to get really bad when you go a full 360 degrees because a walk.",
            "If you turn it around enough, will actually look more similar to the run.",
            "Using these types of."
        ],
        [
            "Strings.",
            "We also said let's try to classify different types of movements from the motion capture data set.",
            "Here each data point is 123 dimensional time series of XYZ.",
            "Points on the person moving overtime, and so we use two state Gaussian emission HMMS and spectral clustering on RPK kernel.",
            "And here's different recognition of different activities running versus walking versus jogging.",
            "Jumping versus jumping forward.",
            "And again you see the spectral clustering with that parametric kernel saying this is really like a hidden Markov model at the local scale is doing really well compared to the other techniques."
        ],
        [
            "We also looked at Arabic characters, Arabic writing, and said let's try to.",
            "So there's like 20 or 30 examples of each character.",
            "This is a time series of XY, showing how to draw this Arabic character 20 or 30 examples for class and we tried to just throw a different time series together and see if we can recover the two classes of digits.",
            "So again K means M Yang.",
            "For all these different pairs of digits are getting confused.",
            "You get really reliable spectral clustering's here again.",
            "Again, just the two states."
        ],
        [
            "Gaussian emission hmm.",
            "Some sign language finally.",
            "Making these movements with your hands in 3D hot versus cold, eat versus drink, and so on.",
            "Again, you get better performance.",
            "And even when you change the number of states but over here you start seeing that as you increase the number of states of the HMM, you're starting to overfit because you're fitting one big hmm to single time series.",
            "So be careful at that level, right?",
            "Very complicated.",
            "Hmm for one data point is going to overfit."
        ],
        [
            "But we have a work around for that.",
            "We've also done this clustering of network traces to model.",
            "Basically."
        ],
        [
            "Servers, let's not focus on that one.",
            "Other feature of this is it's a really fast algorithm, so you fit each hmm with its own sequence, and then you spectral cluster so that M run for that single HMM on that single sequence is really fast.",
            "Then we do spectral clustering.",
            "That's cubic time, and it turns out that that's overall much faster than doing M where you have to retrain the Hmm's over and over again onto onto the current clustering of sequences.",
            "So you're getting a tenfold speedup because the Hmm's are aren't clustered with them, and also trying to figure out the hidden states with them all jointly."
        ],
        [
            "Which slows down the convergence.",
            "And here is the embedding results.",
            "Here's the Yin Yang kernel working quite as well, but here's The Walking in the running example.",
            "We showed an embedding actually pulls out nice circular ring, so it really looks like this in some higher dimensional space.",
            "It looks like these two concentric rings.",
            "OK, that's why the spectral clustering does well.",
            "But this is the visualization multidimensional."
        ],
        [
            "Failing visualization of the data and here is the Arabic in sign language visualizations.",
            "This is the Arabic you see the three clusters for the different classes very nicely embedded with this spectral clustering kernel.",
            "Well, spectral clustering of the probability product kernel.",
            "And here's the sign language with three different words.",
            "Somebody signing again, nicely embedded in that."
        ],
        [
            "Space.",
            "So to wrap up, we're looking at 70 parametric techniques that explore the waters between fully parametric and fully nonparametric.",
            "Ann, what's nice about this is you avoid making assumptions assumptions about the shape of your distribution, but you still want to make assumptions about the data and the structure that each data might have.",
            "Maybe the data has some Markov properties or or might fit into some known exponential family distribution, and so this has led to a new semiparametric likelihood criterion which mixes between ID and ID and encourages the models agree while they fit the data.",
            "And it's given us a new clustering criterion here, which in this first version is just fit each data with a maximum likelihood estimator an then computer kernel between all these density estimates and then do spectral clustering or some nonparametric learner on top of that an now we're looking at the semi parametric density estimation to iteratively tweak these Hmm's with these little weights pulling together so that you don't just do individual one.",
            "Hmm for one datum.",
            "At a time, this will avoid this overfitting problem we saw earlier.",
            "We try to fit a four state hmm to a single time series, and if there's time I'll show a sneak peak of some new applications, but I don't know if there is.",
            "Maybe we can do that offline.",
            "There's some nice other applications with GPS data we're using this technique for.",
            "OK, thank you.",
            "Question.",
            "Models.",
            "I'm not sure that we needed.",
            "You're awfully quiet.",
            "It's a convex function, which is the main reason for missing person, so that's a good question.",
            "Do we really need a kernel that's convex or that satisfies Mercer's condition between all these pairs of probability models?",
            "So I think you could use other surrogates.",
            "It turns out and we have some paper, some arguments that this is going to produce something that's consistent that satisfies marginalization properties, and that's kind of a unique maximum, so the convexity or concavity of that kernel is nice because maximum likelihood is convex.",
            "Or concave however you want to write it whenever you start somewhere, you find that unique global optimum.",
            "So if you tie these models together with something that's not convex, you'll get stuck as you pull them close to each other while you try to maximize.",
            "So it was important for getting to the unique global Max.",
            "Production data as well.",
            "In my coding properties then you lose this metric.",
            "It feels as though I see, so if there is an asymmetry in the relationships between the models, then this kernel is inappropriate.",
            "You don't want to use asymmetric type of overlap.",
            "Yes, I agree.",
            "So you might want to use an asymmetric KL divergences, for example.",
            "Or there might be some other other techniques there.",
            "I mean like the legal trends between the types used in Beaverton.",
            "Comparing the quarter so another type of divergance so you could use many different types of divergences.",
            "This is this is actually the bad Acharya affinity, which is a classical version of all these divergent measures.",
            "So top so has a summary of many that possible divergences from Jensen, Shannon, etc.",
            "This one had some nice properties.",
            "Also it's really important about this divergance as you can compute it efficiently for hidden Markov models.",
            "the Cal Divergent between 2 hidden Markov models is very difficult.",
            "To calculate exactly, and that's true for many of the other divergences.",
            "This one is completely efficient and takes order TM squared with M is the number of states, so that's an important feature.",
            "For Hmm's at least.",
            "You mentioned that on that $1 set with the sign language you have overfitting problems.",
            "When you increase the number of states.",
            "But when you look at the OEM results you actually get increasing accuracy with when you increase the number of states to you, so that's a great point.",
            "So you get better results with them when you give it more States and you get worse results with this technique when you give it more states.",
            "The reason why you're using this technique?",
            "To fit to one single time Series A large hmm.",
            "And so the overfitting starts to kick in there, whereas M might be underfitting and then you give it more States and then the clusters become a little more refined, so those Gaussians become more detailed.",
            "And that's because EM is trying to fit half the data to one HMM and the other half of the sequence of the to the other.",
            "So it's more resilient to that overfitting.",
            "But there's a.",
            "There's a mismatch with the shape.",
            "And the shape assumption might be really bad.",
            "So yeah, so there is this tradeoff, but we're looking at fixing that by maximizing not just the.",
            "Each hmm to each sequence we're trying to do this larger semiparametric likelihood which is in the next next conference.",
            "Anymore questions.",
            "Did not understand.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I've been enjoying the conference so far, including this session Ann.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be talking about some initially applied work we started doing in this space and it's led to some interesting theoretical results, so I'll be showing mostly applied results.",
                    "label": 0
                },
                {
                    "sent": "We have a paper coming up and nips where we've seen some connections to semiparametric density estimation.",
                    "label": 0
                },
                {
                    "sent": "Start off some intuition and then got some math out of it.",
                    "label": 0
                },
                {
                    "sent": "So what I'll be talking bout is spectral clustering and embedding with hidden Markov.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "And the basic goal is going to be combining 2 themes, an unsupervised learning which is parametric learning and nonparametric learning.",
                    "label": 0
                },
                {
                    "sent": "And these two counter that around for awhile in density estimation.",
                    "label": 0
                },
                {
                    "sent": "People know about nonparametric learning as kernel density estimation.",
                    "label": 0
                },
                {
                    "sent": "And I'll be comparing that to parametric learning.",
                    "label": 0
                },
                {
                    "sent": "The standard maximum likelihood approaches for the exponential family and showing some intermediate, which is semiparametric likelihoods, and we'll talk about that but only briefly, 'cause that's a nips, but for this talk I'll be looking at clustering where we're going to mix between parametric types of knowledge and non parametric types of knowledge together because they both have complementary advantages and come up with something like a semi parametric clustering, which is this idea.",
                    "label": 0
                },
                {
                    "sent": "Combining hidden Markov models with spectral clustering and so in contrast to fully parametric EM types of clustering or fully nonparametric vanilla spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "And they will bridge these two things together.",
                    "label": 0
                },
                {
                    "sent": "The parametric and nonparametric are through these kernels, which are kernels between probability models.",
                    "label": 0
                },
                {
                    "sent": "Parametric probability models, typically kernels are non parametric types of things, but now will be using them within with parametric probability models.",
                    "label": 0
                },
                {
                    "sent": "Like the hidden Markov model, and then I'll show some spectral clustering results and then embedding or multidimensional scaling with external between hidden Markov models.",
                    "label": 1
                },
                {
                    "sent": "So so think about it as a way of defining kernel between.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hmm's.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Parametric methods.",
                    "label": 0
                },
                {
                    "sent": "There's many different applications of parametric thinking.",
                    "label": 0
                },
                {
                    "sent": "Density estimation is a standard one.",
                    "label": 0
                },
                {
                    "sent": "People usually think of parametric maximum likelihood as exponential family models, where you have sufficient statistics and that's summarizes the whole data set.",
                    "label": 0
                },
                {
                    "sent": "So you don't need to keep the density around or the samples around, you can just compute means of sufficient statistics and what's nice about parametric methods is the number of parameters does not grow with the data.",
                    "label": 0
                },
                {
                    "sent": "So if I see a million data points or 100 data points, the parameter size is the same, and Meanwhile nonparametric techniques they might have parameters.",
                    "label": 0
                },
                {
                    "sent": "But the key concept is that the number of parameters grows with the size of the data set.",
                    "label": 0
                },
                {
                    "sent": "And so things like density estimator estimators that are nonparametric like parzen estimators or L1 smoothing or infinite mixture models, the amount of modeling is going to get more complicated as we see more and more data.",
                    "label": 0
                },
                {
                    "sent": "And similarly for the clustering, the clusters might get more complicated in your space if you do spectral clustering, which is a non parametric method, whereas an M. Let's say you're doing 2 Gaussian mixture modeling.",
                    "label": 0
                },
                {
                    "sent": "The description of each cluster does not get more complicated as you see more samples even though there aren't sufficient autistics.",
                    "label": 0
                },
                {
                    "sent": "Percent when you run them.",
                    "label": 0
                },
                {
                    "sent": "Similarly visualization there's nonparametric approaches like multidimensional scaling, locally linear embedding and so on.",
                    "label": 0
                },
                {
                    "sent": "But one disadvantage about nonparametric techniques is you don't have the same richness of mixtures.",
                    "label": 0
                },
                {
                    "sent": "Bayesian networks, hidden Markov models, all that modeling that is usually associated with parametric methods.",
                    "label": 1
                },
                {
                    "sent": "Instead, you have to work with kernels and distance metrics and graphs on data, and it might be awkward if you do have some of this modeling type of knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stairs.",
                    "label": 0
                },
                {
                    "sent": "So here's a quick preview of what I mean by going to semi parametric types of density estimations.",
                    "label": 0
                },
                {
                    "sent": "The most general density estimator is you're given samples and you want to learn a probability probability density function over all these samples so that P is really almost two general right?",
                    "label": 0
                },
                {
                    "sent": "It's going to just give you.",
                    "label": 0
                },
                {
                    "sent": "A probability value over all observations of N data points.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first step is to assume that these samples are at least independent, independently distributed and so that probability function really factorizes into a product of N. And probabilities over each individual data point.",
                    "label": 0
                },
                {
                    "sent": "That's the ID assumption that's made in the nonparametric approaches, which are more general.",
                    "label": 0
                },
                {
                    "sent": "Parametric approaches.",
                    "label": 0
                },
                {
                    "sent": "Go step further and say that not only are these is this big P distribution factorized, all the marginals are.",
                    "label": 0
                },
                {
                    "sent": "These are the same, they are identical and we drop that subscript an off of the Theta and say they're all the same probability model.",
                    "label": 0
                },
                {
                    "sent": "So one thing we've been looking at his waist to combine the two, an one approach is to do semi parametric.",
                    "label": 0
                },
                {
                    "sent": "Density estimation, where we say in addition to the ID assumption, we're going to have these kernels between all the probability models that tell them try to be similar to each other.",
                    "label": 0
                },
                {
                    "sent": "OK, so don't force everyone to be exactly the same like a parametric approach, just put push the models together through a kernel an that kernel is a probability kernel, which just basically integrates one distribution and another over the sample space, and that tells you how similar they are.",
                    "label": 0
                },
                {
                    "sent": "They integrate nicely on top of each other, going to get a lot of weight, and they're happy.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they're similar.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a rough picture of what we're talking about.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric density estimation?",
                    "label": 0
                },
                {
                    "sent": "Just put a single.",
                    "label": 0
                },
                {
                    "sent": "Gaussian on each model on each point gets its own model, and they don't communicate at all.",
                    "label": 0
                },
                {
                    "sent": "Parametric says there's a single.",
                    "label": 0
                },
                {
                    "sent": "Gaussian for the whole data set.",
                    "label": 0
                },
                {
                    "sent": "Now we're talking bout Semiparametric where we're also going to put some glue between the models to pull them together and make them similar to each other.",
                    "label": 0
                },
                {
                    "sent": "And we're going to get something like this where we're pulling models together with this integral.",
                    "label": 0
                },
                {
                    "sent": "Again, this kernel called the probability product kernel product of both probability integrate.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the sample space.",
                    "label": 0
                },
                {
                    "sent": "We've introduced this awhile ago.",
                    "label": 0
                },
                {
                    "sent": "It's a natural way of computing similarity between two distributions.",
                    "label": 0
                },
                {
                    "sent": "You just have two distributions.",
                    "label": 0
                },
                {
                    "sent": "Let's say here's one, and here's another, and we integrate them again using this formula over the sample space, and you could take the distributions that power of beta, which just changes the kernel a little bit.",
                    "label": 0
                },
                {
                    "sent": "We typically use data equals one, or beta equals 1/2.",
                    "label": 0
                },
                {
                    "sent": "This is a non negative quantity 'cause it's integral to distributions, and if yous beta equals 1/2, that integral is always equal to 1.",
                    "label": 0
                },
                {
                    "sent": "If P = P prime.",
                    "label": 0
                },
                {
                    "sent": "With the two distributions are the same.",
                    "label": 0
                },
                {
                    "sent": "OK, so measures the overlap.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the two distributions, what's nice is this can be computed close form for any distributions in the exponential family, so if I'm trying to pull Gaussians together or multinomial together, or place on distributions together, the math always looks like this for the distribution.",
                    "label": 0
                },
                {
                    "sent": "That's the natural form for an exponential family.",
                    "label": 0
                },
                {
                    "sent": "Some linear function of the data times some linear function of the model.",
                    "label": 0
                },
                {
                    "sent": "Alright, that's the standard exponential family formed.",
                    "label": 0
                },
                {
                    "sent": "The kernel in that situation when you do this integral is just this simple formula involving the cumulant generating function or partition function.",
                    "label": 0
                },
                {
                    "sent": "So the math is really simple, you have to worry about solving these integrals for the exponential family case.",
                    "label": 0
                },
                {
                    "sent": "And for the Gaussian case with just Gaussian means we get back RBF kernels.",
                    "label": 0
                },
                {
                    "sent": "So if you have a Gaussian on each point and you integrate their overlap, it's like an RBF.",
                    "label": 0
                },
                {
                    "sent": "But now we can talk about other types of distributions.",
                    "label": 0
                },
                {
                    "sent": "Have general covariance Gaussian?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hands or multinomial's or hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "So what's the overlap between 2 hidden Markov models?",
                    "label": 0
                },
                {
                    "sent": "So hidden Markov model has this type of distribution?",
                    "label": 0
                },
                {
                    "sent": "It's basically summing out over all these hidden configurations on the backbone of the chain.",
                    "label": 0
                },
                {
                    "sent": "And the number of possible configurations in this Markov model is the size of the hidden states based on the power of T. The length of this chain.",
                    "label": 0
                },
                {
                    "sent": "And we've got the standard Markov assumption, which is that the states transition with this transition table and you only look at the previous state.",
                    "label": 0
                },
                {
                    "sent": "There's only one arrow you don't have to go backwards in time more than once that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is vanilla hmm modeling and we can go and compute the kernel between two Hmm's by just integrating the density of 1 HMM model times the density of the other HMM models and integrating over the whole sample space.",
                    "label": 0
                },
                {
                    "sent": "All possible sequences of length T. OK, but that integral is going to look really nasty because we have to sum over all the hidden configurations.",
                    "label": 0
                },
                {
                    "sent": "All as possibilities for one.",
                    "label": 0
                },
                {
                    "sent": "Hmm and all you possibilities for the other hmm, and then do these integrals over their output space or emissions.",
                    "label": 0
                },
                {
                    "sent": "So for two Hmm's, one with the state space of size S, let's say 3 state HMM and a four state hmm with sysdate space you.",
                    "label": 0
                },
                {
                    "sent": "That's after the power of T times due to the power of T. So that's a nightmare, but.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're not going to suggest you do that.",
                    "label": 0
                },
                {
                    "sent": "Instead, there's an efficient way of computing.",
                    "label": 0
                },
                {
                    "sent": "This kernel, without going through this brute force cross product using a, basically a forward backward type of algorithm.",
                    "label": 0
                },
                {
                    "sent": "And you can do this with other types of Bayesian networks wherever there's a nice junction tree algorithm as well, so that big summation over S and you all the possible states sequences is replaced by basically realizing we can compute a sub kernel over only the emissions.",
                    "label": 0
                },
                {
                    "sent": "Of each hmm cross, the emissions of the other, hmm.",
                    "label": 0
                },
                {
                    "sent": "And then what I'm basically showing is that this is.",
                    "label": 0
                },
                {
                    "sent": "Now looking like a big junction tree where were instead of solving over all the all the possible hidden states, we're going to basically stack one HMM on top of the other.",
                    "label": 0
                },
                {
                    "sent": "This way look at their common emissions and group these into large cliques so this state goes with this date.",
                    "label": 0
                },
                {
                    "sent": "That's it goes in this day and we get this.",
                    "label": 0
                },
                {
                    "sent": "Chain OK and we do four backward on this chain, but now the size of these cliques is.",
                    "label": 0
                },
                {
                    "sent": "S by U cross.",
                    "label": 0
                },
                {
                    "sent": "S by you.",
                    "label": 0
                },
                {
                    "sent": "OK, so actually this is wrong.",
                    "label": 0
                },
                {
                    "sent": "This is TS cross U yeah so.",
                    "label": 0
                },
                {
                    "sent": "That should be squared because these are clean for this size, so there's a squared missing over here, but that's much better than the exponential brute force calculation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the point is, we can compute this kernel between two Hmm's and here's the pseudocode for it.",
                    "label": 0
                },
                {
                    "sent": "You have two Gaussian Hmm's for each, Gaussian for each state of hmm, one and each Gaussian of each state of hmm.",
                    "label": 1
                },
                {
                    "sent": "Two, we compute a scalar value, which is how similar are these Gaussians?",
                    "label": 0
                },
                {
                    "sent": "So this is for a 3 state HMM cross, the four state hmm a 3 by 4 table which compares all their Gaussians pairwise.",
                    "label": 0
                },
                {
                    "sent": "And that's the formula is saying how similar is is my Gaussian number one to your Gaussian #3.",
                    "label": 0
                },
                {
                    "sent": "And that scale or just gets plugged into a table.",
                    "label": 0
                },
                {
                    "sent": "And then here's simple pseudocode that runs down the chain and computes the kernel.",
                    "label": 1
                },
                {
                    "sent": "And you can basically see we have to do T steps and some over the states over here this way.",
                    "label": 0
                },
                {
                    "sent": "And so that's very fast and we get this kernel which tells you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar to Hmm's are so how do we use this with clustering?",
                    "label": 0
                },
                {
                    "sent": "So unlike the semi parametric density estimation, now we're going to see parametric clustering.",
                    "label": 0
                },
                {
                    "sent": "If we had fully parametric clustering, we'd just do EM, and we'd say there's two.",
                    "label": 0
                },
                {
                    "sent": "Types of classes ASI negative and ASI positive.",
                    "label": 0
                },
                {
                    "sent": "We don't know which sequence this came from.",
                    "label": 0
                },
                {
                    "sent": "Was this a sequence of running or sequence of walking?",
                    "label": 0
                },
                {
                    "sent": "Let's say C + C minus?",
                    "label": 0
                },
                {
                    "sent": "So that's what M is going to maximize.",
                    "label": 0
                },
                {
                    "sent": "Its going to maximize this log likelihood over this unknown labeling.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric clustering like spectral clustering or Max cut.",
                    "label": 0
                },
                {
                    "sent": "Basically tries to find a labeling so that you maximize some kind of cut along all the pairs of edges between your data which are computed with the kernel.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be proposing is semi parametric clustering where we're going to fit the data using maximum likelihood so that each model is estimated from.",
                    "label": 0
                },
                {
                    "sent": "Its own sequence.",
                    "label": 0
                },
                {
                    "sent": "But then when we go out and compute the spectral clustering, we use the kernel between the probability models.",
                    "label": 0
                },
                {
                    "sent": "OK, so we estimate this.",
                    "label": 0
                },
                {
                    "sent": "Then we estimate that.",
                    "label": 0
                },
                {
                    "sent": "So why not do these jointly?",
                    "label": 0
                },
                {
                    "sent": "Well, that's in the NIPS paper.",
                    "label": 0
                },
                {
                    "sent": "We're not going to do that for now, we're just going to find the best Hmm's for each sequence and then do the spectral clustering using the probability product kernel on Hmm's.",
                    "label": 0
                },
                {
                    "sent": "How am I doing on time?",
                    "label": 0
                },
                {
                    "sent": "What's?",
                    "label": 0
                },
                {
                    "sent": "7 minutes 7 minutes OK great.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so parametric em clustering.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to spend too much time on this, but basically you want to maximize that incomplete log likelihood you're given, let's say two models, one per class.",
                    "label": 1
                },
                {
                    "sent": "We compute their responsibilities.",
                    "label": 0
                },
                {
                    "sent": "So how much this data point likes to be with model one versus how much?",
                    "label": 0
                },
                {
                    "sent": "I'd like to be with Model 2 and then we maximize the expected complete likelihood so the Gaussians gobble up their data points.",
                    "label": 1
                },
                {
                    "sent": "If the X is are sequences, then we do clustering with two HMM models and the better HMM steals that sequence away from the weaker HMM.",
                    "label": 0
                },
                {
                    "sent": "They gobble up sequences and do this conquer and divide, and that's the aloneness Claire off paper where they clustered motion capture data of people running and walking around using mixtures of HMM this way.",
                    "label": 0
                },
                {
                    "sent": "So the step mom step are basically the same as what they are for mixtures of Gaussians, it's just that now you've got no.",
                    "label": 0
                },
                {
                    "sent": "Not only is the class unknown, the hidden states are also unknown, so we have to sum over that downstairs as well in the expected.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fleet likelihood.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it works great if you have a mixture of two Gaussians and if your assumption was correct to begin with, this parametric assumption.",
                    "label": 1
                },
                {
                    "sent": "But if the data wasn't actually a mixture of two Gaussians, it was actually a mixture of some stringy distribution and some other one.",
                    "label": 0
                },
                {
                    "sent": "Then it's not going to work very well and the same thing will happen with Hmm's.",
                    "label": 0
                },
                {
                    "sent": "So if you're watching two people walked one person walking, the other person running, if there's some drift in the running sequences and some drift in The Walking sequences, just like this red and black drift.",
                    "label": 0
                },
                {
                    "sent": "You're not going to cluster the people together properly.",
                    "label": 0
                },
                {
                    "sent": "You're not going to say.",
                    "label": 0
                },
                {
                    "sent": "Oh, here's the runner.",
                    "label": 0
                },
                {
                    "sent": "And here's the Walker.",
                    "label": 0
                },
                {
                    "sent": "Just in the same way that you're not going to cluster things properly with Para.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Metric EM.",
                    "label": 0
                },
                {
                    "sent": "And so we'd like to use nonparametric spectral clustering, which is agnostic about the shape of the clusters, so I don't know what the shape is.",
                    "label": 1
                },
                {
                    "sent": "I'm just going to use spectral clustering and the most popular one these days is this stabilized clustering, buying Wison Jordan and we find the top eigenvectors of the normalized Laplacian.",
                    "label": 1
                },
                {
                    "sent": "Which takes your kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Just normalize it when you compute it between all pairs and typically use the RBF affinity to get the kernels, so instead, so that gives you these types of nice agnostic clusterings, but instead of the RBF affinity, let's use the probability product kernel as a surrogate because we might have each one of these points might be a time series.",
                    "label": 1
                },
                {
                    "sent": "We can't put a Gaussian on it, we have to put in HMM on it.",
                    "label": 0
                },
                {
                    "sent": "There's other competitor kernels for time series, like the Yen Yang Kernel, but they don't really make these types of parametric assumptions on the data set.",
                    "label": 0
                },
                {
                    "sent": "For example, I might tell you each one of these is generated by a two state hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Each data point is a hidden Markov model you want to exploit.",
                    "label": 0
                },
                {
                    "sent": "That parametric assumption about each point, but you don't know the shape of the clouds.",
                    "label": 0
                },
                {
                    "sent": "The points form, so you want to put parametric knowledge at a small scale, but not at the large.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scale your data.",
                    "label": 0
                },
                {
                    "sent": "So here's what we mean by kind of a Walker Runner runner.",
                    "label": 0
                },
                {
                    "sent": "We took somebody walking and change the angle of their walk by rotating this 3D motion capture data.",
                    "label": 1
                },
                {
                    "sent": "So here's the Walker and it forms the outer ring, and the runner is the inner ring.",
                    "label": 0
                },
                {
                    "sent": "And they're just time series sequences.",
                    "label": 0
                },
                {
                    "sent": "But The Walking in the running is slowly rotating, so if you do them on this, it's not going to work very well.",
                    "label": 0
                },
                {
                    "sent": "But spectral clustering will actually pull out the perfect clustering if you model each persons are two state HMM.",
                    "label": 0
                },
                {
                    "sent": "That's a great model, because running and walking have these two phases.",
                    "label": 0
                },
                {
                    "sent": "You're basically you're.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Put forward your right foot forward, so here's the pseudocode for each time series you fit in.",
                    "label": 1
                },
                {
                    "sent": "Hmm, we compute this kernel between all pairs and then we do spectral clustering on this weight weighted graph.",
                    "label": 1
                },
                {
                    "sent": "So here's the kernel again, and then we just want to cut this into different classes.",
                    "label": 0
                },
                {
                    "sent": "And it's a nonparametric spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "For embedding, we use multidimensional scaling or Eli or SDE.",
                    "label": 0
                },
                {
                    "sent": "Anyone these popular packages which also needs a pairwise kernel between the things you're trying to visual.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's pseudocode again.",
                    "label": 0
                },
                {
                    "sent": "What I just said before, and this is the details of the spectral clustering algorithm.",
                    "label": 1
                },
                {
                    "sent": "So three of these lines are carbon copies of England Jordan.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to spend too much.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Time on that.",
                    "label": 0
                },
                {
                    "sent": "And here are the results.",
                    "label": 0
                },
                {
                    "sent": "So the first one was just looking at walking and running and rotating the angles.",
                    "label": 0
                },
                {
                    "sent": "So if you rotate only a little bit, The Walking in the running, you don't go too far with it.",
                    "label": 0
                },
                {
                    "sent": "All the clustering techniques work just as well.",
                    "label": 0
                },
                {
                    "sent": "Mm-hmm works well.",
                    "label": 0
                },
                {
                    "sent": "K means on HMM source well and this time series kernel by Eden Yang works well.",
                    "label": 1
                },
                {
                    "sent": "But as you start rotating the data.",
                    "label": 0
                },
                {
                    "sent": "Then you start confusing which cluster should go which point should go with which cluster.",
                    "label": 0
                },
                {
                    "sent": "But the spectral technique keeps working very well.",
                    "label": 0
                },
                {
                    "sent": "OK, it always separates The Walking from the running, whereas the other techniques slowly start to degenerate and are putting, walking and running into the same cluster, so they should keep them separate and you can see that starts to get really bad when you go a full 360 degrees because a walk.",
                    "label": 0
                },
                {
                    "sent": "If you turn it around enough, will actually look more similar to the run.",
                    "label": 0
                },
                {
                    "sent": "Using these types of.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Strings.",
                    "label": 0
                },
                {
                    "sent": "We also said let's try to classify different types of movements from the motion capture data set.",
                    "label": 0
                },
                {
                    "sent": "Here each data point is 123 dimensional time series of XYZ.",
                    "label": 1
                },
                {
                    "sent": "Points on the person moving overtime, and so we use two state Gaussian emission HMMS and spectral clustering on RPK kernel.",
                    "label": 0
                },
                {
                    "sent": "And here's different recognition of different activities running versus walking versus jogging.",
                    "label": 0
                },
                {
                    "sent": "Jumping versus jumping forward.",
                    "label": 0
                },
                {
                    "sent": "And again you see the spectral clustering with that parametric kernel saying this is really like a hidden Markov model at the local scale is doing really well compared to the other techniques.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also looked at Arabic characters, Arabic writing, and said let's try to.",
                    "label": 1
                },
                {
                    "sent": "So there's like 20 or 30 examples of each character.",
                    "label": 0
                },
                {
                    "sent": "This is a time series of XY, showing how to draw this Arabic character 20 or 30 examples for class and we tried to just throw a different time series together and see if we can recover the two classes of digits.",
                    "label": 1
                },
                {
                    "sent": "So again K means M Yang.",
                    "label": 0
                },
                {
                    "sent": "For all these different pairs of digits are getting confused.",
                    "label": 0
                },
                {
                    "sent": "You get really reliable spectral clustering's here again.",
                    "label": 0
                },
                {
                    "sent": "Again, just the two states.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gaussian emission hmm.",
                    "label": 0
                },
                {
                    "sent": "Some sign language finally.",
                    "label": 0
                },
                {
                    "sent": "Making these movements with your hands in 3D hot versus cold, eat versus drink, and so on.",
                    "label": 0
                },
                {
                    "sent": "Again, you get better performance.",
                    "label": 0
                },
                {
                    "sent": "And even when you change the number of states but over here you start seeing that as you increase the number of states of the HMM, you're starting to overfit because you're fitting one big hmm to single time series.",
                    "label": 0
                },
                {
                    "sent": "So be careful at that level, right?",
                    "label": 0
                },
                {
                    "sent": "Very complicated.",
                    "label": 0
                },
                {
                    "sent": "Hmm for one data point is going to overfit.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we have a work around for that.",
                    "label": 0
                },
                {
                    "sent": "We've also done this clustering of network traces to model.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Servers, let's not focus on that one.",
                    "label": 0
                },
                {
                    "sent": "Other feature of this is it's a really fast algorithm, so you fit each hmm with its own sequence, and then you spectral cluster so that M run for that single HMM on that single sequence is really fast.",
                    "label": 0
                },
                {
                    "sent": "Then we do spectral clustering.",
                    "label": 1
                },
                {
                    "sent": "That's cubic time, and it turns out that that's overall much faster than doing M where you have to retrain the Hmm's over and over again onto onto the current clustering of sequences.",
                    "label": 0
                },
                {
                    "sent": "So you're getting a tenfold speedup because the Hmm's are aren't clustered with them, and also trying to figure out the hidden states with them all jointly.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which slows down the convergence.",
                    "label": 0
                },
                {
                    "sent": "And here is the embedding results.",
                    "label": 0
                },
                {
                    "sent": "Here's the Yin Yang kernel working quite as well, but here's The Walking in the running example.",
                    "label": 0
                },
                {
                    "sent": "We showed an embedding actually pulls out nice circular ring, so it really looks like this in some higher dimensional space.",
                    "label": 0
                },
                {
                    "sent": "It looks like these two concentric rings.",
                    "label": 0
                },
                {
                    "sent": "OK, that's why the spectral clustering does well.",
                    "label": 0
                },
                {
                    "sent": "But this is the visualization multidimensional.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Failing visualization of the data and here is the Arabic in sign language visualizations.",
                    "label": 0
                },
                {
                    "sent": "This is the Arabic you see the three clusters for the different classes very nicely embedded with this spectral clustering kernel.",
                    "label": 0
                },
                {
                    "sent": "Well, spectral clustering of the probability product kernel.",
                    "label": 0
                },
                {
                    "sent": "And here's the sign language with three different words.",
                    "label": 0
                },
                {
                    "sent": "Somebody signing again, nicely embedded in that.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "So to wrap up, we're looking at 70 parametric techniques that explore the waters between fully parametric and fully nonparametric.",
                    "label": 0
                },
                {
                    "sent": "Ann, what's nice about this is you avoid making assumptions assumptions about the shape of your distribution, but you still want to make assumptions about the data and the structure that each data might have.",
                    "label": 0
                },
                {
                    "sent": "Maybe the data has some Markov properties or or might fit into some known exponential family distribution, and so this has led to a new semiparametric likelihood criterion which mixes between ID and ID and encourages the models agree while they fit the data.",
                    "label": 0
                },
                {
                    "sent": "And it's given us a new clustering criterion here, which in this first version is just fit each data with a maximum likelihood estimator an then computer kernel between all these density estimates and then do spectral clustering or some nonparametric learner on top of that an now we're looking at the semi parametric density estimation to iteratively tweak these Hmm's with these little weights pulling together so that you don't just do individual one.",
                    "label": 1
                },
                {
                    "sent": "Hmm for one datum.",
                    "label": 0
                },
                {
                    "sent": "At a time, this will avoid this overfitting problem we saw earlier.",
                    "label": 1
                },
                {
                    "sent": "We try to fit a four state hmm to a single time series, and if there's time I'll show a sneak peak of some new applications, but I don't know if there is.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can do that offline.",
                    "label": 0
                },
                {
                    "sent": "There's some nice other applications with GPS data we're using this technique for.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure that we needed.",
                    "label": 0
                },
                {
                    "sent": "You're awfully quiet.",
                    "label": 0
                },
                {
                    "sent": "It's a convex function, which is the main reason for missing person, so that's a good question.",
                    "label": 0
                },
                {
                    "sent": "Do we really need a kernel that's convex or that satisfies Mercer's condition between all these pairs of probability models?",
                    "label": 0
                },
                {
                    "sent": "So I think you could use other surrogates.",
                    "label": 0
                },
                {
                    "sent": "It turns out and we have some paper, some arguments that this is going to produce something that's consistent that satisfies marginalization properties, and that's kind of a unique maximum, so the convexity or concavity of that kernel is nice because maximum likelihood is convex.",
                    "label": 0
                },
                {
                    "sent": "Or concave however you want to write it whenever you start somewhere, you find that unique global optimum.",
                    "label": 0
                },
                {
                    "sent": "So if you tie these models together with something that's not convex, you'll get stuck as you pull them close to each other while you try to maximize.",
                    "label": 0
                },
                {
                    "sent": "So it was important for getting to the unique global Max.",
                    "label": 0
                },
                {
                    "sent": "Production data as well.",
                    "label": 0
                },
                {
                    "sent": "In my coding properties then you lose this metric.",
                    "label": 0
                },
                {
                    "sent": "It feels as though I see, so if there is an asymmetry in the relationships between the models, then this kernel is inappropriate.",
                    "label": 0
                },
                {
                    "sent": "You don't want to use asymmetric type of overlap.",
                    "label": 0
                },
                {
                    "sent": "Yes, I agree.",
                    "label": 0
                },
                {
                    "sent": "So you might want to use an asymmetric KL divergences, for example.",
                    "label": 0
                },
                {
                    "sent": "Or there might be some other other techniques there.",
                    "label": 0
                },
                {
                    "sent": "I mean like the legal trends between the types used in Beaverton.",
                    "label": 0
                },
                {
                    "sent": "Comparing the quarter so another type of divergance so you could use many different types of divergences.",
                    "label": 0
                },
                {
                    "sent": "This is this is actually the bad Acharya affinity, which is a classical version of all these divergent measures.",
                    "label": 0
                },
                {
                    "sent": "So top so has a summary of many that possible divergences from Jensen, Shannon, etc.",
                    "label": 0
                },
                {
                    "sent": "This one had some nice properties.",
                    "label": 0
                },
                {
                    "sent": "Also it's really important about this divergance as you can compute it efficiently for hidden Markov models.",
                    "label": 0
                },
                {
                    "sent": "the Cal Divergent between 2 hidden Markov models is very difficult.",
                    "label": 0
                },
                {
                    "sent": "To calculate exactly, and that's true for many of the other divergences.",
                    "label": 0
                },
                {
                    "sent": "This one is completely efficient and takes order TM squared with M is the number of states, so that's an important feature.",
                    "label": 0
                },
                {
                    "sent": "For Hmm's at least.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that on that $1 set with the sign language you have overfitting problems.",
                    "label": 0
                },
                {
                    "sent": "When you increase the number of states.",
                    "label": 0
                },
                {
                    "sent": "But when you look at the OEM results you actually get increasing accuracy with when you increase the number of states to you, so that's a great point.",
                    "label": 0
                },
                {
                    "sent": "So you get better results with them when you give it more States and you get worse results with this technique when you give it more states.",
                    "label": 0
                },
                {
                    "sent": "The reason why you're using this technique?",
                    "label": 0
                },
                {
                    "sent": "To fit to one single time Series A large hmm.",
                    "label": 0
                },
                {
                    "sent": "And so the overfitting starts to kick in there, whereas M might be underfitting and then you give it more States and then the clusters become a little more refined, so those Gaussians become more detailed.",
                    "label": 0
                },
                {
                    "sent": "And that's because EM is trying to fit half the data to one HMM and the other half of the sequence of the to the other.",
                    "label": 0
                },
                {
                    "sent": "So it's more resilient to that overfitting.",
                    "label": 0
                },
                {
                    "sent": "But there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a mismatch with the shape.",
                    "label": 0
                },
                {
                    "sent": "And the shape assumption might be really bad.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so there is this tradeoff, but we're looking at fixing that by maximizing not just the.",
                    "label": 0
                },
                {
                    "sent": "Each hmm to each sequence we're trying to do this larger semiparametric likelihood which is in the next next conference.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Did not understand.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}