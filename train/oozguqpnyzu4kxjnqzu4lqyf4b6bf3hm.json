{
    "id": "oozguqpnyzu4kxjnqzu4lqyf4b6bf3hm",
    "title": "GPU programming for Deep Learning",
    "info": {
        "author": [
            "Ryan Olson, NVIDIA Corporation",
            "Julie Bernauer, Laboratoire d'Informatique, \u00c9cole Polytechnique"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_bernauer_olson_deep_learning/",
    "segmentation": [
        [
            "So thank you very much for coming having us here.",
            "I also see much more much more.",
            "I mean, many girls in the audience.",
            "An last year was not so many girls, so keep it up.",
            "So glad not to be the only one with a crazy laptop an so today we're going to do something which is in between a talk Anna hands online so."
        ],
        [
            "I'm going to start with a general talk about where we are, what we do for deep learning.",
            "For those of you that I'm not as familiar with NVIDIA, I'm going to mention the libraries and the platform, and I'm going to also show you the new product, but I'm going to try to make that pop short because some of you have very good access to that information and the idea is to move to a second session where we're going to go and do an introduction to CUDA for GPU programming.",
            "So in the last days you can toss you've seen.",
            "Piano, you've seen a lot of things.",
            "I'm pretty sure a lot of you have seen GPU usage.",
            "And what I'm trying to show you here is what happens underneath the scenes.",
            "Anne Anne.",
            "Why those frameworks interact like that with the GPU.",
            "What we're trying to do to make it easier for you guys to interact with the GPU at any level, right?",
            "And then in the second part, we're going to end Zone Lab on Amazon, so most of you have a laptop will give you credential an will give you a chance to try 3 different labs.",
            "So voluntarily I open three tabs.",
            "I'll be mostly answering questions with Ryan on the first one, which is an introduction to GPU programming.",
            "But if you are more familiar with GPU programming, you might want to try more advanced lab.",
            "So I open two more advanced lab.",
            "We will however, focus on the questions for the first lab.",
            "OK for everybody."
        ],
        [
            "OK so GPU computing."
        ],
        [
            "I usually start with that slight and I like it very much.",
            "It reason Atede yesterday very much when James Dean presented what they were doing.",
            "GPU computing.",
            "Is it your genius computing the GPU has to be attached to something and that something can be very different thing.",
            "It can be a power CPU, it can be a non CPU, can be an X86 CPU and that means in terms of programming we're going to have to take care of it.",
            "We're going to have to be aware of what is underneath this year, right?",
            "So I'm going to try to show that later.",
            "So power is mostly service right now.",
            "It's mostly embedded right now, and each statistiques is what we usually got laptops and servers.",
            "But we're going to see later on that changes the what we do in terms of software we're trying to be generated, so we're trying to have everything works mostly everywhere.",
            "But you know, there are some consideration.",
            "We can't really scape."
        ],
        [
            "CUDA, so it's called GPU programming and there are several ways to program a GPU.",
            "NVIDIA will try, so we develop CUDA sometime ago and the idea was to have a framework to program NVIDIA GPU's that makes it simple for everybody.",
            "Anne here the purpose is just to show that the syntax is very close to the C syntax.",
            "OK, so that's what we're going to do later, so I'm not going to spend a lot of time on this here, but the idea is just to be able to program the GPU and to do parallel programming without having to explicitly write the loops and everything.",
            "OK, so that's an example of a sum of two vectors are raising T an the GPU friendly version and CUDA an for most of you should be natural OK?"
        ],
        [
            "So you can interact with the GPU in CUDA or any direct GPU programming language that you might not want to go so low level.",
            "So you might want to use a library library instead.",
            "And so there are several drop-in acceleration libraries for application that we're going to see that we see for GPU's.",
            "So here I've made like a Panorama of what we can do, so some of you might know stressed for salting or this kind of duration.",
            "I'm going to present.",
            "CU DNN Ann mentioned today Q blasen crosspost very quickly, but the idea is again you have the GPU.",
            "You can interact directly with the GPU.",
            "You can program the GPU or you can decide not to be fully aware of the GPU and interact with the GPU to libraries."
        ],
        [
            "So did John that Angie PS why I also GPU so good at deep learning?",
            "Why are we here and why do you care?",
            "So."
        ],
        [
            "So the reason why most of people care is was very well actually tackled yesterday afternoon.",
            "I'm very thankful to Jeff Dean because you made my job much more much easier working on the GPU makes everything so much faster that eat a little bit of research development by allowing quicker prototyping.",
            "And this is a very famous example of the Google Data Center three years ago, where for the first experiment they needed 1000 CPU servers they needed.",
            "16,000 cores and there was 2000 CPUs, so most of us we cannot handle that in our small University data center or in our local resource.",
            "An one year afterwards, the same people that so people at Stanford did exactly the same experiment with just three servers.",
            "So that's the kind of speedup we're talking about, and that makes all the prototyping.",
            "There was not feasible before feasible that way.",
            "Interestingly, also people usually say Oh yeah, but GPU consume a lot of power.",
            "I don't have that kind of power.",
            "If you look at the ratio here for the same type of computation, would like better to look at the.",
            "Instruction per Watt or like put that power conversation relatively to what you can achieve with it.",
            "If you have any questions at anytime feel free to you know.",
            "Raise your hand."
        ],
        [
            "So very quickly in recent improvements Imagenet most of you are aware about the pedestrian detection.",
            "I mean, you might have heard that NVIDIA is very well versed in that emotive right, not to like several companies around.",
            "An so that's an example of pretty strange detection.",
            "So in black it's regular traditional techniques.",
            "An in red you have all the techniques that did the DNN based, and using a GPU's and specially for everything which is automotive related or pedestrian detection.",
            "Right now we only see GPU based workloads.",
            "An all this like for the last part of this pen like was in less than a year of the time frame, right?",
            "So that was very very quick and that quick prototyping scheme we were talking about that could be was achievable with this."
        ],
        [
            "Cody and so.",
            "Specially for deep learning at NVIDIA we develop that library that is called Cody and the purpose is to give building blocks to accelerat most of the deep learning neural network.",
            "Instructions and it started with convolutions.",
            "It started in 2014, was the first version of Cody and I heard this morning something like, yeah, it depends on which you DNN version you have, so your speeds are going to depend on where you are, so I'm glad because that is what that slide is trying to show too.",
            "So right now two years afterward, we're CU.",
            "DNN version 5.",
            "An combining software and hardware, we managed to be five times faster than we were two years ago, so again it was possible two years ago can be twice faster, so it allows for better prototyping, but also production."
        ],
        [
            "Was it libraries that can be used in machine learning?",
            "Because usually you know, we see people, not everybody is doing LCM, not everybody is doing convolution only.",
            "And sometimes people might be interested by different type of linear algebra operations.",
            "So I'm mentioning two other libraries.",
            "This one is called Q glass.",
            "It's a drop in for the linear algebra class library.",
            "So for those of you that are interested is the subject of the optional end Zone lab number one.",
            "I'm not going to give more detail here, it's just GPU's are very good.",
            "Linear algebra operation an we provide different way of doing that.",
            "Something for the second line."
        ],
        [
            "Free.",
            "It's accelerating, sparse operation is called Kuzbass.",
            "Sensing provides a lot of like metrics operation for Spark metrics is that our GPU accelerated again, it's drop in.",
            "You need to do that operation.",
            "You call that function.",
            "You get a result, so it's not you to take care of what the GPU is going to do."
        ],
        [
            "Yesterday also was mentioned something very very interesting.",
            "We see more and more multi GPU workload and we're going to see in an exception that we see also more and more of multi GPU hardware.",
            "An it's it's difficult programming multiple GPU's an having efficient scaling is very very difficult.",
            "But if one manages to do that.",
            "Then it makes everything faster.",
            "You can work easier and more data at the same time.",
            "You could do that parallelism, but you can also work on bigger models if you want to model properties and so to try to make that easier for everybody.",
            "We developed that library called Nickel and CCL an it's a collective library.",
            "So what this means is it's a research library of accelerated routine to provide the classical operation people use when doing parallel computing like scatter, gather, reduce.",
            "The pattern is developed after the MPI collective, so somebody mentioned MPI yesterday were using that kind of scheme to make sure people know your correction or find the operation they expect an it handles every internode communication in an optimal way, so you don't have to worry about how you imagine you have a box with a GPU's.",
            "An you could put four GP on one CPU HP on the same CPU, and so on and so forth, and it ends up being a nightmare of having to figure out where your data pattern here is and where you communication pattern.",
            "Here is the purpose of this is just to make it.",
            "Easier for everybody and actually not having to worry of where your GPU's are.",
            "So when you're using that library, having a GPU's or four GPU on single CPU is not going to change anything, or at least it's going to be minimal.",
            "For most of the cases."
        ],
        [
            "Again, the purpose is to make it easy for everybody so it looks very much like the classical C approach.",
            "So this is just a small example of a single threaded or reduce operation here.",
            "And that's how you called that library.",
            "You include that library and you just called and or reduce and you get the data from all the GPU you had and we make sure that operation is tune from multi GPU platform."
        ],
        [
            "Question.",
            "OK, I'm moving on.",
            "I'm moving on fast quickly on purpose so that we can get to the end zone, but please don't be disturbed by that and feel free to stop me platform."
        ],
        [
            "So you might have heard that last week if I remember correctly, we launched a new product which is the Titan Express cow, so you should have received an email from a raffle.",
            "The titanics by scale doesn't fit in my purse, so the raffle is such that we gather the email information and we're going to send the titanics Pascal to the winner.",
            "So yeah, I don't have the classical titanics Pascal here today.",
            "As I said, it doesn't fit in my bag, so please don't forget to register for the raffle.",
            "Some of you did.",
            "And so, as you might have seen, it's very interesting because.",
            "On the NVIDIA blog you can see the detail of that tub and the first thing we say.",
            "It's 11th teraflops.",
            "FP 32.",
            "OK, but the second thing we say is that called Conduent 8.",
            "And if you remember the talks from yesterday, it shows that that called somehow is made for deep learning development, right?",
            "We're not looking at different GPUs having different set of instructions that can do different type of deep learning operations, and so that's a code that people can use for developing into 8 deep learning workloads.",
            "So it has 3500 for CUDA cores.",
            "I'm sorry I missed a word and plug gig of GDDR 5X RAM.",
            "So that's the brand new product.",
            "Don't forget to enter the raffle."
        ],
        [
            "And yes, it doesn't fit in your bag.",
            "This one is even bigger.",
            "It doesn't fit in your back either.",
            "It might not even fit in your car, but we hope it's going to fit in your data center.",
            "This is the DGX 1.",
            "So it was mentioned yesterday that not only multi GPU workloads where I'm becoming more important but also the communications between the GPU's were becoming more important.",
            "And this one is very important for us, because that's the first product for which we do the full server that is dedicated to deep learning.",
            "Anet uses a new type of interconnect between the GPU's, which has like a higher bandwidth than the PCI E. Best that people were using before but not only have we done that, we also make sure we took care about the IO.",
            "Indie planning people don't only care about compute, they only care about pushing the data to the GPU and sometimes it can be a bottleneck.",
            "Some benchmark I've done like only on synthetic data, but in daily life we are very conscious that if you work on images you need to bring those images to the GPU.",
            "And this can be a bottleneck, so we make sure that this box can handle it in the best way possible.",
            "So it uses a Tesla P-100 hybrid Commission.",
            "That's the way the GPU's organized.",
            "We can discuss that a bit more, and so thanks to Ryan, we brought a few 100 so Ryan Devine come and do the presentation.",
            "And, well, so we'll have the P. 100 so that you can look and touch it.",
            "Don't put it in your backpack.",
            "It's not functional.",
            "And on top of it, it's like you might not have the baseball to put it on so.",
            "Right, OK, so the P-100 is.",
            "This is actually a really exciting new GPU that we've announced an I'm the pass around the room, so you can all get your hands on it."
        ],
        [
            "Yep.",
            "And so the first thing you're going to notice is a wait.",
            "So not only is there some super interesting like hardware design going on this chip, but there's also some really interesting mechanical engineering we can talk about that after, but there's some really cool stuff that's just going on the baseboard.",
            "Three things I'm going to talk about just to point out from architectural point of view is the chip, the new GPU, and the new architecture.",
            "The memory, which is completely new and the envy link, which is I would say probably the biggest and most exciting change of this whole design.",
            "So with Pascal, this is a completely new architecture.",
            "You're not going to be surprised.",
            "I'm going to tell you it's faster.",
            "Kind of a no brainer, but under the covers the SM design in the microarchitecture is also completely new or not completely new, but newer in the sense that we have a lot more SMS.",
            "Each SM is maybe a little less powerful, but we keep all the same register and shared memory units per SM.",
            "So now you have less cores per SM with the same amount of registers and shared memory, which is makes actually programming for those SME's a little bit easier.",
            "Moving on to the high bandwidth memory so most people as you pass around, you're going to notice the units going around the outside and most people will say.",
            "I bet that's the memory and you'd be wrong.",
            "So on most GPU's memory is outside the die but with high bandwidth or HBM 2, we moved the memory onto the chip.",
            "So this is a technology that we introduced called a chip on wafer on substrate so the GPU upsell on to that."
        ],
        [
            "Yeah, so the GPU is a chip that sits on a piece of Silicon with an interposer and we stack the memory rate on the die.",
            "So the benefits from that is a vast reduction in the amount of energy per memory operation.",
            "So not only can we run, we greatly increase the memory bandwidth up to 720 gigabytes a second, but we've also greatly reduced the power per memory OP, and so that's a huge increase over.",
            "I think we had like 200 and 32180 on the previous Maxwell generation, so there's a huge step up and finally envy link.",
            "So on the back there's two plastic pins in there that you can pop off and you can look at the actual pins.",
            "One of those connectors for the Envy link and the other is for the PCI E and for the power and this is attached directly on to the baseboard and envy link is.",
            "Trace onto the onto the baseboard Anne.",
            "Really, this is kind of the most unique architectural change that we've enabled with the P-100 because.",
            "It basically is the multicore SMP for GPU's.",
            "Now you have you can directly address memory for directly attached GPU's and then you can take and take advantage of nickel for the communication between.",
            "So if you have can make if your algorithms can make use of aggregate distributed memory you now have 16 'cause we have 16 gigs of HBM 2 * 8 in the DGX one.",
            "So you have 128 gigabytes of.",
            "Directly accessible memory, so that can greatly increase the models that you can put in there, and you have very fast access to it.",
            "And with that, I think I'll turn it back to Julie."
        ],
        [
            "Thank you, So what you can see it's it's brand new and it's getting us very excited.",
            "We just hope it's going to get you as excited as we are.",
            "An so this is a great for us.",
            "This is a great knew.",
            "Hello.",
            "How do I improvement that?",
            "We need to be able to program that hardware and we need we want everybody to be able to program that all there.",
            "So Nickel was taking care of the multi GPU communication.",
            "Now we see a lot of people are looking at inference right?",
            "And this is the whole scheme on how we see the world in production.",
            "Where in an image.",
            "Let's imagine an image workload.",
            "People would take the image, put them in a repository of meant maybe welcome those images.",
            "And work with you know Cafe Torciano.",
            "So right 9 digits.",
            "We have caffeine torch where we provide a web UI to help people develop their home.",
            "Deep learning workload without knowing anything about frameworks and then we train, we test.",
            "We set up a model an we want to do inference and when you want to do inference you can be interested in doing inference for very different applications.",
            "OK, so this is the focus.",
            "We have an inference.",
            "OK so you have a drone you want your drone to capture where the cause are you have your car.",
            "You want to be able to see pedestrians and what we think is good is to develop something which is dedicated to inference because the workload.",
            "Is kind of different.",
            "We see a lot of people during inference that are interest."
        ],
        [
            "Did in using tricks such as Fusion, so my slides are not in the right order."
        ],
        [
            "But that's what it is.",
            "So I'm just going to."
        ],
        [
            "Back there, so this is an example.",
            "Often NVIDIA platform for embedded.",
            "So this is the TX one.",
            "It's a small bowl that people will use in drone, for example an it used to be that we have the same software stack, so this is to come back when I said the GPU has to be attached to something.",
            "I cannot comment on that right now.",
            "Each time, So what I can say is each time we have a new architecture, usually we bring that new architecture to server Workstation and invited.",
            "I cannot comment on the timeline, so is right.",
            "So what we wanted to point out also is this is the previous Maxwell architecture and what we did at the time is people will train a model and we have the same software stack everywhere.",
            "So the fact that it's not the same CPU is transparent to the user, right?",
            "So this is an example, we cafe where you just take your cafe model file an we have example on the web and you push your cafe model files on your T X1.",
            "And you can run inference without any modification in the code.",
            "And so we figured we wanted to do more about that and we wanted to make that inference even more efficient.",
            "So that's also."
        ],
        [
            "Why we did not GE?",
            "So the ID is when we do inference and we do that forward pass, we can use a lot of tricks to be faster.",
            "We confuse the layer.",
            "We can eliminate some layer, we can specialize some kernels.",
            "We can do a lot of autotuning from the target platform, and we can make sure that wherever that model is pushed, pushed is going to be tuned to work better.",
            "That's what GI is about, and so I invite you to have a look."
        ],
        [
            "At the website this is an example of performance numbers, so on the right you have the classical cafe, so GPU workload and on the left you have GE plus GPU workloads and so Ryan actually made that graph so you can talk to him at the end.",
            "What's very interesting is sometimes you see big improvements in performance and it's actually when we fusing layers both horizontally and vertically, right?",
            "And having our results using brings a lot of performance.",
            "Impact, yeah.",
            "The way accesses the performance is the time for each layer.",
            "Sorry.",
            "I was right.",
            "Yeah, I'm sorry it's a later time."
        ],
        [
            "OK, so that's why in France I mentioned some people don't want to see the GPU decoding level.",
            "They don't want to see the library level.",
            "They don't even want to know what a framework is.",
            "I guess that's not so many people in that audience, so I'm going to be fast.",
            "We also have a way of your eye called digits that is available in the labs that you can look into if you wish."
        ],
        [
            "Trader, so I'm done with the global presentation.",
            "So do you have any questions before I get into the introduction to programming and ends on yes?",
            "So when are we going to see floating .16 and other chips?",
            "So very good question.",
            "The question is when do we see FP 16 Anywhere on Titan X?",
            "There is no FP 16.",
            "There is a fee 16 on the GP 100.",
            "I mean there is very slow FP16, not so many physician units, so GP 100 there is F16 operations.",
            "So in the DGX one you have FP 16 we announced last month P 100 E which is the PCI E pendant of the GP 100 envy link that has SP 16.",
            "So it's the same chip that goes on the PCI E bus.",
            "And as of today, that's the only product we announced publicly.",
            "So right now, if you have a server that takes a regular call on PC, IE the solution for you is P-100 E that we announced last month.",
            "I mean end of June, and that should be available in the fall.",
            "I have no clue about pricing.",
            "That we can fill up that request.",
            "I don't know at all.",
            "I'm sorry.",
            "GPU.",
            "This is a hardware reason.",
            "Like the process you might have heard about like 16.",
            "You know 1614 the FinFET mechanics and there's only so much memory you can put on the die.",
            "Anne, right now when you see so few.",
            "I mean, we say that's a lot already.",
            "You can get up to 24 gigs.",
            "Regular GDDR memory.",
            "That's what we've shown before for HBM 2.",
            "It's only 16 gigs right now.",
            "It's mostly because of the adware process.",
            "It's not the die size is the fact it's how much memory you can put on the die and actually the yielding of getting those I built.",
            "And the other point, I would make there is that we are programming for a GPU.",
            "You're taking advantage of the.",
            "Ability to.",
            "Hi.",
            "Ability to overlap memory operations with computers, and so you don't need a lot of storage.",
            "You have registers that you contact switch in and out of, and then you're constantly you're using the GPU memory bandwidth as a kind of a buffer there, and so it's ability to contact swap between different stages of your computer.",
            "Block on IO coming in you should be considered somewhere else.",
            "That's that's how we think, and we've improved that with.",
            "P-100 'cause now you have more registers and more shared memory per SM, then you didn't pass architectures.",
            "And I see the answer we made to that question is also very interesting to me, because it's always a balance between the hardware and the software.",
            "So GPU's, or generate programming units right there not dedicate it to a specific task.",
            "There are some tasks that they do well, and all those answers are going to be a combination on how you program.",
            "And how the hardware is set up.",
            "So while we try to extract as much as we can of the hardware, having an idea of how it isn't the need help.",
            "So for the CUDA part, we're going to see a lot of the register and memory.",
            "So if some of you don't know what an SM is yet, don't worry.",
            "We'll see that.",
            "It's just you have to have an idea of what some things and it's it, but you don't have to know what to program individually.",
            "Just having an idea of the sizing underneath sufficient.",
            "Usually.",
            "Yes.",
            "Presented so you could review them.",
            "I can do anyone.",
            "Could you tell us more about the building points, accuracy and whether that's or how that effects the types of models taken?",
            "So the question is for those of us that are new to GPU's, can you tell me the influence of the plotting point accuracy on what we do training basically so for if the 16 and into a tie leave the experts to reply to you.",
            "It's always the easy answer is it's always a tradeoff.",
            "Between accuracy and training speed, right?",
            "If you train with a lower position.",
            "If you want to keep your accuracy, sometimes you have to be careful, especially when you want to do training.",
            "What happens in the gradient you want to be very careful that so there are some operations that are very easy.",
            "And that have absolutely no impact in in 15 an hour person that are much more tricky when you're training.",
            "Depending on what type of network you're training for in France.",
            "I would say that the community now agrees that you can infer in FP16 sometime in eight 8.",
            "Having a drop in accuracy of less than like 5% for any kind, of like Alex Nagel net come on, you know network type.",
            "For training the community still, maybe you can convince on that because I feel it's a fixed question, right?",
            "Still, it depends how you do it.",
            "If you do it smartly so that it learns to deal with low precision while it's raining.",
            "Then you can get away with very little precision in the actual computation.",
            "You still need the enough decision for restoring the whites because they are accumulating a lot of small changes, so these guys needs at least 16 bits and they need floating point.",
            "So yeah, but I guess a lot of this is recent research papers over the last year and more work needs to be done.",
            "To try these different variants on different architectures and datasets, but the early results that basically what they can get away.",
            "In fact, with a single bit computation on some architectures in datasets.",
            "But you still need a high precision on the way to entrain when once you trains, then you can quantize very very aggressively and still get pretty good results.",
            "So for us, the way we see it is as much as we can provide people with operation that is faster at every stage we're going to do it sometime.",
            "It's easy to put in all where sometimes it's likely harder.",
            "And I said, we're still generating generating purpose.",
            "Hardware right?",
            "So each time you see GPU, if you're interested in deep learning, I can only advise you to look at the FP 32, FP 16 and eat 8 numbers.",
            "I don't know if I know very few people in machine learning that look at 64, but in terms of like numerical computing we see a whole range.",
            "Of people so right now as a beginner FP 32 is safe.",
            "That's what most of the framework implement by default, but research is very active and we just want to make sure we support that correctly.",
            "So you could go down as low as a single bit per week.",
            "And he still work in terms of amount of computation, like the storage for the still in the storage room framework.",
            "For those with yeah and actually.",
            "So we.",
            "So that's a very good point too, because like when I showed the TX one before the little teeny tiny platform, this was the first platform where we had FP 16 built in in the hardware, and the first code we published were actually.",
            "We were very careful about the storage, we could do sometime FP 16 for storage and not for compute.",
            "So what you're going to see and what you might want to pay attention to is the difference between the precision in the compute and storage all the way around.",
            "So when we say FP 16, you're going to hear conversation like full FP 16 or with that kind of consideration.",
            "Yes.",
            "Can you talk about the?",
            "There is a great blog post written by Ryan.",
            "Parallel.",
            "I can.",
            "Ann will be around until Friday.",
            "If you want to talk to us about."
        ],
        [
            "So before I got you could as I mentioned to program correctly on the.",
            "Southwest side you have to have an idea for the hardware looks like so I don't.",
            "I'm not going to go into a full GPU architecture class."
        ],
        [
            "But I just want to define a few terms.",
            "The GPU, assuming components memory and streaming multiprocessor SMS.",
            "OK, so the memory is where you still data by definition and the SME's are where you do the computation, so the global memory of the GP.",
            "You can do that.",
            "You can see that as the ram that you would have on the CPU machine and it's accessible by both the GPU and CPU.",
            "That's what we call the global memory right now in GPS we see up to 24 gigs of global memory, so it's already.",
            "It's already much more than we saw some years ago.",
            "And you can have error checking on the memory in some GPU's in case you need it.",
            "So that's the part for storing data for computing GPU's use what we call this end.",
            "So streaming multiprocessors that perform the actual computation.",
            "So as Ryan mentioned before each SM its own control unit register execution pipeline Ancash, so DSM is actually the unit that performs the computation.",
            "OK, if it's not super clear, I'm going to have more drawings, so hopefully."
        ],
        [
            "Get clear.",
            "In each SM.",
            "You have many could, of course.",
            "Their architecture independent, they can do special function like a sinus sinus, whatever.",
            "Each SM.",
            "I shared memory plus L1 cache.",
            "And they have thousands of registers.",
            "So that's what the drawing looks like, right?",
            "The course, and that's what that's what an SM is."
        ],
        [
            "So I already mentioned several types of memory.",
            "I just want to give like a little synopsis here.",
            "So I had the global memory, those 12 or 24 gigs I talked about.",
            "There is the L2 cache on top of it underneath.",
            "Depending on you see this is going to stay on top of it and then for each SM you're going to have for justice L1 cache and shared mem.",
            "OK."
        ],
        [
            "So the reason why I'm saying that is because when you're going to program, you have to be aware on where is what it's going to be less and less important, soon with unified memory, specially for the difference between global memory and local memory.",
            "But we're going to see that later.",
            "Could."
        ],
        [
            "Programming level, so I took care of the memory.",
            "Now I'm going to tell you how we compute on the GPU.",
            "So there is a part of the code which is serial codes that execut on the CPU, whatever that CPU is.",
            "The product code executes on what we call the device.",
            "The device is the GPU.",
            "It executes through many threads.",
            "Across multiple processing elements.",
            "So that's basically what a CUDA C++ applications looks like.",
            "You have some serial code executing on the CPU gets product on the GPU back to the CPU back to the GPU and so on and so forth.",
            "That's how that's why we care so much about communications, because if that process not efficient we can see it's going to be problematic."
        ],
        [
            "A quick example, so it's an example which is slightly more complicated than the one I've shown at the beginning.",
            "So here what I'm doing is a simple vector operation where I do Y equals a X + Y.",
            "Which is what we call the sex by operation.",
            "So just basically adding to vector an ad in a wet weighted vector right?",
            "And so the first part is the standard C code.",
            "And the second part is the parallel C code, and as you see it's broadly code, but there is no loop.",
            "The loop is encapsulated in that cryptic instruction which defined blocks blogged in an thread.",
            "I haven't mentioned of global memory at the top and at the bottom I launch a function which is called parallel where I have the number of block and I have that #256 here that I'm going to tell afterwards.",
            "OK so basically what we do is we don't do the loop, but we provide a pseudo loop that is executing in blocks and threads and so on and so forth, OK."
        ],
        [
            "So.",
            "You've seen the previous slide I mentioned.",
            "This is a sax by Colonel.",
            "On the GPU, the parallel portion of the application was going to run on the GPU.",
            "We call that a kernel.",
            "An entire GPU can only execute execute a lot of kernels and many threads at the same time.",
            "The critter threads are defined to be very light switching very, very fast so that we can switch between straight very very quickly and we have thousands that can execute simultaneously.",
            "It makes it a bit difficult the first time you program it because you have to think of all the things that are happening at the same time that if you've been programming with languages like Matlab or or where you work on table, it's a bit similar.",
            "OK.",
            "So again, vocabulary, the CPU is what we're going to call the host.",
            "I'm sorry it confirms the age correctly.",
            "It executes function and the GPU, we call it the device, and it's going to execute kernels.",
            "So when we say we execute, these are kernel.",
            "It's on the GPU and a function is going to be on the CPU OK for everybody."
        ],
        [
            "So could a kernel OK. Low is a function which is executed on the GPU.",
            "That's what I said before, and it's actually an array of threads, so that's what define on the right.",
            "All these threads will execute exactly the same code.",
            "They're all the same, but they can take different paths.",
            "And it's read as an ID so that you can actually pinpoint it.",
            "You can select input, output data and it can make control decision.",
            "OK, so that's an example on the right.",
            "Well, I have a thread I ask you to function and I have an output.",
            "So that's this thread.",
            "So."
        ],
        [
            "Going to see that in the end, Zone Labs is going to depend on the GPU you have.",
            "Ice.",
            "Yes, so each thread cannot.",
            "So each core I'm going to get to that.",
            "So here each arrow is a thread.",
            "And threat can be survived to divide it into blocks."
        ],
        [
            "So a block is going to be a bunch of threads.",
            "Blocks will be grouped into a grid.",
            "OK."
        ],
        [
            "Panel is executed as a grid of blocks of threads.",
            "So when you execute the kernel, you actually push a full grid of Black Ops threads on the GPU at the same time."
        ],
        [
            "So it's red, so that's the parallel between the software and hardware here.",
            "It's red will be executed by a core.",
            "With thread, each thread block will be executed by an SM.",
            "In which each kernel is going to be executed by the GPU, GPU being able to have several kernels at the same time.",
            "OK for everybody."
        ],
        [
            "So.",
            "Now I'm going to enter a subject which is a very difficult subject.",
            "It's cooperation since a lot of things are happening at the same time.",
            "Sometimes you can cooperate, sometimes you cannot.",
            "So thread blocks there look operation inside the send block.",
            "This rates can cooperate.",
            "What this means they can load and store memory they will use.",
            "They can share results with each other inside the block and then you can synchronize with each other.",
            "This is all inside a specific block, not outside a block."
        ],
        [
            "Thread blocks allow for scalability.",
            "What I mean by that is blocks can execute in any order concurrently or sequentially, so you can see right away that having cooperation between blocks is not something you might want to have because the blocks are supposed to be independent, and that's what gives you stability, scalability and performance.",
            "So inside a block you can do whatever you want outside a block.",
            "You have to be careful.",
            "Because you don't control how and when it gets execute right, and so accountable scales across any number of SME's, and that's where it gets tricky.",
            "We're going to see that more in the end zone lab, but you cannot corporate between blocks, and especially you want to make sure you don't want to have block to wait for each other.",
            "That's not possible."
        ],
        [
            "Again, to memory.",
            "So I mentioned the hardware before.",
            "I'm just going to mention what it what it gives for the software pop so threads have register the only the memory."
        ],
        [
            "They have local memory.",
            "And the block of thread has shared memory."
        ],
        [
            "So shared memory is defined like this.",
            "It's allocated per block.",
            "And he has the safe lifetime of a block when your blogs on the share memory is gone.",
            "It's accessible by any trades in thread in the block that it looks for Corporation, and you can share data amongst red or you can manage on your own yocash.",
            "For example.",
            "This is particularly interesting when you do metric suppression and deep learning when you do gems, because like the you can reduce memory access doing that."
        ],
        [
            "So.",
            "Again, register local memory into Black Ops without shared memory and all the blocks can access the global memory.",
            "The reason why I'm making a bit first of it is like if you haven't played with it, you can say OK, I'm going to put everything in global memory and I'm going to be good.",
            "The problem is global memory is not as fast as local memory, so you want to make sure you benefit as much as you can from the different memory access so that you have to fast this access pattern possible."
        ],
        [
            "So the global memory is accessible by all the thread of any kernel.",
            "Because the kernel runs on the GPU.",
            "Is Lifetime is much longer, the data allocation is when you allocate you get it and when you deallocate by the host the CPU.",
            "So for those of you that are used to that.",
            "Usually if you use a framework for example, you launch a workload and you look at the GPU memory, you're going to see it being filled and at the end you're going to see the memory goes back to very few users and that's exactly what's going on.",
            "OK, this is by opposite to the local memory that disappears when the thread is gone."
        ],
        [
            "Memory management."
        ],
        [
            "Anne, I'm almost done.",
            "So again, CPU and GPU have separate memory spaces and the data is moved across a PCI E bus.",
            "For most of the GPS we know now we can move some of his friend dealing, but I'm not going to do that right now so we can use function to allocate, deallocate and copy, just like in.",
            "See, we're going to see that Indians on lamp.",
            "Pointers or pointers like there are just addresses, so if you have a pointer you can't know.",
            "Where it is, you can't know whether that pointer is on the CPU on the GPU, but as a programmer you might want to know because that might make a big difference.",
            "So.",
            "You must check the attributes of your pointers.",
            "You know where you point your pointer resides.",
            "We so I'm sorry it sounds like a lot of like do don't do that but I just want you to have like a summary of this because we're going to see that in a lab.",
            "We want to be careful when we difference Ng, because if you dare reference appointed that somewhere where it doesn't exist, you will have a bad crash and this is very difficult to find in the code even with the provider and everything that's that's hard to see those, even if the burger.",
            "So anytime you use a pointer, try to put a quick comment.",
            "You know where that pointer resides."
        ],
        [
            "Allocation of memory and release.",
            "It's basically like in C except you had CUDA at the beginning.",
            "Analog keramidas that set CUDA free.",
            "This is an example.",
            "So.",
            "Again, we have that hardware software balance that what we call device memory is the GPU memory and what we call host memory is the CPU memory."
        ],
        [
            "So.",
            "Cute and then copy.",
            "So that's an operation that sometimes.",
            "You know, it's difficult to to understand.",
            "She returns when the copy is complete, complete and it blocks a CPU thread.",
            "Until it's been copied.",
            "So what data transfers you start a CPU thread, it goes to the GPU like copy and then blocks until it comes back.",
            "So if you want to be efficient, that's something you want to talk about, and it doesn't start coping until all the previous cuticles are complete.",
            "So you can't copy while something else is going on.",
            "I'm not going to go too much into the memco Peosta device and device you open device to device will see that briefly in the labs.",
            "If you wish an at some point, you know she wanted to discuss, we can discuss unified virtual memory, but that's not something I'm going to cover here, but it used to be, and it's still the case that.",
            "Coping from host to device had to be done by the developer.",
            "With what we call unified virtual memory, that's not going to be the case anymore.",
            "It's going to be automatic.",
            "And that's what's going on here."
        ],
        [
            "OK, just a basic."
        ],
        [
            "Cution and a summary of all I said.",
            "Pilot code on the GPU.",
            "Cooler kernel is launched an execut on the device.",
            "The GPU by many thread.",
            "Threads are grouped into thread blocks.",
            "Parallel code is written for a thread.",
            "So the loop is a thread.",
            "Each thread is free to execute whatever he wants.",
            "An we have IDs for building threads and blocks."
        ],
        [
            "Thread or partition into thread blocks and the grid is all the blocks for a given launch.",
            "I'm sorry I'm reading, but it's just like I just wanted to cover it to be OK. A thread block are bunch of threads that are synchronized and they could communicate via share memory."
        ],
        [
            "Ideas and dimension which answered the question how many thread an eye voluntary won't answer that question.",
            "I'm doing that on purpose.",
            "I'm sorry, so why I won't answer that question is because the first exercise in the end zone lab is actually for you to size your GPU by playing with Block ID.",
            "An number of threads.",
            "So thread are 3D.",
            "They are unique within a block block or SU2D2 dimensional.",
            "And their unique within the grid and dimensions can set can be set at lunchtime.",
            "So in the end zone lab we start with a 1 dimensional for it to be simpler.",
            "But then you can use 2 dimensional or three dimensional and the built-in variable we're going to use our thread index lock index that give the IDs block dimension, Grid dimension and each GPU will have.",
            "Different asseman will have different values that are optimal for it, so sometimes you hear Oh yeah, I got GPU code, but I don't have it tuned.",
            "Most of the tuning from one generation of GPU to another OK is at that level making sure the kernel we launch all the exact site an if you look it's very funny because I was very surprised the first time I looked at the Alex net paper.",
            "If you look at it from an engineering perspective.",
            "Alex recently did a wonderful drawing because when you see the drawing, you basically see the GPU.",
            "It was working out.",
            "And that's very nice, because each showed that paper was actually made to be running on the GPU, or the sizing were actually adapted to the GPU was working on.",
            "And that's where the link between the actual dimensions sometime of the network and the hardware is so stringent.",
            "That's a that's emblematic example nowadays is not astringent, but having an application well, size for where it leaves usually makes the performance very very impressive.",
            "And actually that first code, like, could I connect two, was extremely stressful on NVIDIA GPU's for that reason because he was making it was using all that.",
            "All the tuning a GPU could have at the time."
        ],
        [
            "OK, another drawing thread.",
            "3D idees unit with a block.",
            "We got it."
        ],
        [
            "The picture how do you do that on the GPU?",
            "There is a special syntax which is the triple Chevron notation.",
            "And so I'm not going to spend a lot of time on it, but that's what it looks like, Colonel lower lower than 30 two 5012 like greater grade.",
            "And then the chevrons.",
            "And that's how you define your sizing."
        ],
        [
            "So this is just a quick additional and also another word.",
            "Account now launches on a grid off block.",
            "Each block is launched on one SM.",
            "And we also have another term which is called awop, where a block is divided into warps of 32 threads each.",
            "It's a 32 vector, so you might want to have it a multiple of 32.",
            "Because all the words execute the same instruction symmetrically, and that's very linked to the hardware.",
            "The blocks runs too complex to compression on an SM.",
            "It's starting on so you can't interrupt the block to do something else.",
            "So when you design, keep in mind that the block has to complete.",
            "If a block doesn't complete, it doesn't work.",
            "There is no migration, you can't move it at all."
        ],
        [
            "The blocks must be independent.",
            "We already tackled that a bit before.",
            "Be careful, there's no.",
            "They cannot synchronize, no luck.",
            "So if you're used to parallel programming sometimes on a different architecture, that's where it gets tricky.",
            "There is no synchronization, and this is essential to scalable."
        ],
        [
            "OK, I'm done questions so I won't answer that question, sorry.",
            "Yes.",
            "Is there a meeting?",
            "Please say that again.",
            "The divided addresses that are leading to this time.",
            "Is there a Wat meaning?",
            "Sorry yes, because if you have 32, so we're going to see that that's it.",
            "I mean, I see that that's the same question.",
            "So is there a meaning to the block site in size ID it depends depending on how many threads and blocks your GPU can execute, and so you just want to size it so that it's a good combination between what your application has to do and what your GPU looks like.",
            "So on purpose, the first question of the end Zone lab is for you to understand that meaning by sizing it.",
            "Right, so it's directly linked to hardware.",
            "It's just pushing blocks that fits the size of your GPU.",
            "Does it answer the question?",
            "And here's a question.",
            "OK, so your time to work."
        ],
        [
            "OK everybody, I only have 5 minutes left, so I'm going to be around during the break and keep on answering questions in you wish, but I want to wrap up with everybody.",
            "Anne."
        ],
        [
            "A quick thing.",
            "In this lab, we're running on Amazon.",
            "On an Amazon instance.",
            "But the software that is running on Amazon is the same everywhere it's available in nvidia.com, get CUDA, it's free and it works.",
            "It works anywhere.",
            "The reason I. I'm saying that is because I see a lot of people is like funny laptops.",
            "As I said at the beginning and that's why I have a gaming laptop.",
            "I don't play games very bad at it.",
            "I get.",
            "I destroy those people each time I start, but people look at my laptop like wow no.",
            "So the only thing I do is my laptop is what you've seen today and so this is running locally on my laptop.",
            "And it shows you what we ask you to do at the end of the lab, and it shows you the characteristics of the GPU that is in my laptop.",
            "And finally, I'm going to answer your first question here.",
            "You can see.",
            "Those numbers of the sizing of my GPU when I do what is called a device query, it's a sample which is provided with the CUDA software and it gives me the characteristic of my GPU.",
            "So the GPU I have here is a GTX970M.",
            "It's a Maxwell GM 200 for GPU, for laptops the maximum number of threads per block that I can have is 1024 and the maximum number of threads per multiprocessor is 2048.",
            "And my web size as we discussed before is 32, so that's how you know what you can do an at the end of the lab.",
            "They ask you to do the same.",
            "The processor using you're going to see it slightly older generation, but that's what it is.",
            "So if you're interested in trying if you have a laptop, it works on your laptop whether you have a Mac, Linux or Windows laptop.",
            "If you have a GPU, yes.",
            "So how many multiprocessors are there?",
            "So here I have 1000.",
            "280 could of course, so that makes that's how I can do it.",
            "So there are less score is a member of right sided neck.",
            "Yes, so the number of threads per multiprocessor, yes.",
            "OK, so that's what I said.",
            "That's the amount of memory when I'm running something that's where I can see whether it's running.",
            "So I want you to have a look at this.",
            "I haven't tried to do that in the lab.",
            "You can do that at the end of the lab.",
            "It's you'll have the comments and everything to execute that."
        ],
        [
            "And if you want to keep on trying so that as I said, that is going to be open for a few days and then we have a full app platform on Android at quicklab.com.",
            "So if you register for that lab during that time, you'll get free credits to be able to do a lot of labs on that on that platform.",
            "And if you have any questions, not always very responsive, but you're always welcome to send me an email and bug me until I reply, I usually do.",
            "Anne."
        ],
        [
            "But the last thing is I'm going to do like everybody.",
            "I didn't have alot of time to show that we are hiring like most of the companies we're having a lot of fun.",
            "We have a research full research program.",
            "We have an applied research program.",
            "We have a full automotive section.",
            "We have a lot of like different types of jobs including sales jobs or technical jobs.",
            "Ann, you're welcome to talk to Ryan to me or pen is here too.",
            "So if you're interested in joining NVIDIA, we're very happy to talk to you and.",
            "Thank you very much.",
            "Answer The questions during the break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much for coming having us here.",
                    "label": 0
                },
                {
                    "sent": "I also see much more much more.",
                    "label": 0
                },
                {
                    "sent": "I mean, many girls in the audience.",
                    "label": 0
                },
                {
                    "sent": "An last year was not so many girls, so keep it up.",
                    "label": 0
                },
                {
                    "sent": "So glad not to be the only one with a crazy laptop an so today we're going to do something which is in between a talk Anna hands online so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to start with a general talk about where we are, what we do for deep learning.",
                    "label": 0
                },
                {
                    "sent": "For those of you that I'm not as familiar with NVIDIA, I'm going to mention the libraries and the platform, and I'm going to also show you the new product, but I'm going to try to make that pop short because some of you have very good access to that information and the idea is to move to a second session where we're going to go and do an introduction to CUDA for GPU programming.",
                    "label": 0
                },
                {
                    "sent": "So in the last days you can toss you've seen.",
                    "label": 0
                },
                {
                    "sent": "Piano, you've seen a lot of things.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty sure a lot of you have seen GPU usage.",
                    "label": 0
                },
                {
                    "sent": "And what I'm trying to show you here is what happens underneath the scenes.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne.",
                    "label": 0
                },
                {
                    "sent": "Why those frameworks interact like that with the GPU.",
                    "label": 1
                },
                {
                    "sent": "What we're trying to do to make it easier for you guys to interact with the GPU at any level, right?",
                    "label": 0
                },
                {
                    "sent": "And then in the second part, we're going to end Zone Lab on Amazon, so most of you have a laptop will give you credential an will give you a chance to try 3 different labs.",
                    "label": 0
                },
                {
                    "sent": "So voluntarily I open three tabs.",
                    "label": 0
                },
                {
                    "sent": "I'll be mostly answering questions with Ryan on the first one, which is an introduction to GPU programming.",
                    "label": 1
                },
                {
                    "sent": "But if you are more familiar with GPU programming, you might want to try more advanced lab.",
                    "label": 1
                },
                {
                    "sent": "So I open two more advanced lab.",
                    "label": 0
                },
                {
                    "sent": "We will however, focus on the questions for the first lab.",
                    "label": 0
                },
                {
                    "sent": "OK for everybody.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so GPU computing.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I usually start with that slight and I like it very much.",
                    "label": 0
                },
                {
                    "sent": "It reason Atede yesterday very much when James Dean presented what they were doing.",
                    "label": 0
                },
                {
                    "sent": "GPU computing.",
                    "label": 0
                },
                {
                    "sent": "Is it your genius computing the GPU has to be attached to something and that something can be very different thing.",
                    "label": 0
                },
                {
                    "sent": "It can be a power CPU, it can be a non CPU, can be an X86 CPU and that means in terms of programming we're going to have to take care of it.",
                    "label": 0
                },
                {
                    "sent": "We're going to have to be aware of what is underneath this year, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to try to show that later.",
                    "label": 0
                },
                {
                    "sent": "So power is mostly service right now.",
                    "label": 0
                },
                {
                    "sent": "It's mostly embedded right now, and each statistiques is what we usually got laptops and servers.",
                    "label": 0
                },
                {
                    "sent": "But we're going to see later on that changes the what we do in terms of software we're trying to be generated, so we're trying to have everything works mostly everywhere.",
                    "label": 0
                },
                {
                    "sent": "But you know, there are some consideration.",
                    "label": 0
                },
                {
                    "sent": "We can't really scape.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "CUDA, so it's called GPU programming and there are several ways to program a GPU.",
                    "label": 0
                },
                {
                    "sent": "NVIDIA will try, so we develop CUDA sometime ago and the idea was to have a framework to program NVIDIA GPU's that makes it simple for everybody.",
                    "label": 1
                },
                {
                    "sent": "Anne here the purpose is just to show that the syntax is very close to the C syntax.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what we're going to do later, so I'm not going to spend a lot of time on this here, but the idea is just to be able to program the GPU and to do parallel programming without having to explicitly write the loops and everything.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's an example of a sum of two vectors are raising T an the GPU friendly version and CUDA an for most of you should be natural OK?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can interact with the GPU in CUDA or any direct GPU programming language that you might not want to go so low level.",
                    "label": 0
                },
                {
                    "sent": "So you might want to use a library library instead.",
                    "label": 0
                },
                {
                    "sent": "And so there are several drop-in acceleration libraries for application that we're going to see that we see for GPU's.",
                    "label": 1
                },
                {
                    "sent": "So here I've made like a Panorama of what we can do, so some of you might know stressed for salting or this kind of duration.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present.",
                    "label": 0
                },
                {
                    "sent": "CU DNN Ann mentioned today Q blasen crosspost very quickly, but the idea is again you have the GPU.",
                    "label": 0
                },
                {
                    "sent": "You can interact directly with the GPU.",
                    "label": 1
                },
                {
                    "sent": "You can program the GPU or you can decide not to be fully aware of the GPU and interact with the GPU to libraries.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So did John that Angie PS why I also GPU so good at deep learning?",
                    "label": 0
                },
                {
                    "sent": "Why are we here and why do you care?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the reason why most of people care is was very well actually tackled yesterday afternoon.",
                    "label": 0
                },
                {
                    "sent": "I'm very thankful to Jeff Dean because you made my job much more much easier working on the GPU makes everything so much faster that eat a little bit of research development by allowing quicker prototyping.",
                    "label": 1
                },
                {
                    "sent": "And this is a very famous example of the Google Data Center three years ago, where for the first experiment they needed 1000 CPU servers they needed.",
                    "label": 1
                },
                {
                    "sent": "16,000 cores and there was 2000 CPUs, so most of us we cannot handle that in our small University data center or in our local resource.",
                    "label": 1
                },
                {
                    "sent": "An one year afterwards, the same people that so people at Stanford did exactly the same experiment with just three servers.",
                    "label": 0
                },
                {
                    "sent": "So that's the kind of speedup we're talking about, and that makes all the prototyping.",
                    "label": 0
                },
                {
                    "sent": "There was not feasible before feasible that way.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, also people usually say Oh yeah, but GPU consume a lot of power.",
                    "label": 1
                },
                {
                    "sent": "I don't have that kind of power.",
                    "label": 0
                },
                {
                    "sent": "If you look at the ratio here for the same type of computation, would like better to look at the.",
                    "label": 0
                },
                {
                    "sent": "Instruction per Watt or like put that power conversation relatively to what you can achieve with it.",
                    "label": 0
                },
                {
                    "sent": "If you have any questions at anytime feel free to you know.",
                    "label": 0
                },
                {
                    "sent": "Raise your hand.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So very quickly in recent improvements Imagenet most of you are aware about the pedestrian detection.",
                    "label": 1
                },
                {
                    "sent": "I mean, you might have heard that NVIDIA is very well versed in that emotive right, not to like several companies around.",
                    "label": 0
                },
                {
                    "sent": "An so that's an example of pretty strange detection.",
                    "label": 1
                },
                {
                    "sent": "So in black it's regular traditional techniques.",
                    "label": 0
                },
                {
                    "sent": "An in red you have all the techniques that did the DNN based, and using a GPU's and specially for everything which is automotive related or pedestrian detection.",
                    "label": 0
                },
                {
                    "sent": "Right now we only see GPU based workloads.",
                    "label": 0
                },
                {
                    "sent": "An all this like for the last part of this pen like was in less than a year of the time frame, right?",
                    "label": 0
                },
                {
                    "sent": "So that was very very quick and that quick prototyping scheme we were talking about that could be was achievable with this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cody and so.",
                    "label": 0
                },
                {
                    "sent": "Specially for deep learning at NVIDIA we develop that library that is called Cody and the purpose is to give building blocks to accelerat most of the deep learning neural network.",
                    "label": 0
                },
                {
                    "sent": "Instructions and it started with convolutions.",
                    "label": 0
                },
                {
                    "sent": "It started in 2014, was the first version of Cody and I heard this morning something like, yeah, it depends on which you DNN version you have, so your speeds are going to depend on where you are, so I'm glad because that is what that slide is trying to show too.",
                    "label": 0
                },
                {
                    "sent": "So right now two years afterward, we're CU.",
                    "label": 0
                },
                {
                    "sent": "DNN version 5.",
                    "label": 0
                },
                {
                    "sent": "An combining software and hardware, we managed to be five times faster than we were two years ago, so again it was possible two years ago can be twice faster, so it allows for better prototyping, but also production.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Was it libraries that can be used in machine learning?",
                    "label": 0
                },
                {
                    "sent": "Because usually you know, we see people, not everybody is doing LCM, not everybody is doing convolution only.",
                    "label": 1
                },
                {
                    "sent": "And sometimes people might be interested by different type of linear algebra operations.",
                    "label": 0
                },
                {
                    "sent": "So I'm mentioning two other libraries.",
                    "label": 0
                },
                {
                    "sent": "This one is called Q glass.",
                    "label": 0
                },
                {
                    "sent": "It's a drop in for the linear algebra class library.",
                    "label": 0
                },
                {
                    "sent": "So for those of you that are interested is the subject of the optional end Zone lab number one.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to give more detail here, it's just GPU's are very good.",
                    "label": 1
                },
                {
                    "sent": "Linear algebra operation an we provide different way of doing that.",
                    "label": 1
                },
                {
                    "sent": "Something for the second line.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free.",
                    "label": 0
                },
                {
                    "sent": "It's accelerating, sparse operation is called Kuzbass.",
                    "label": 0
                },
                {
                    "sent": "Sensing provides a lot of like metrics operation for Spark metrics is that our GPU accelerated again, it's drop in.",
                    "label": 0
                },
                {
                    "sent": "You need to do that operation.",
                    "label": 0
                },
                {
                    "sent": "You call that function.",
                    "label": 0
                },
                {
                    "sent": "You get a result, so it's not you to take care of what the GPU is going to do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yesterday also was mentioned something very very interesting.",
                    "label": 0
                },
                {
                    "sent": "We see more and more multi GPU workload and we're going to see in an exception that we see also more and more of multi GPU hardware.",
                    "label": 1
                },
                {
                    "sent": "An it's it's difficult programming multiple GPU's an having efficient scaling is very very difficult.",
                    "label": 0
                },
                {
                    "sent": "But if one manages to do that.",
                    "label": 0
                },
                {
                    "sent": "Then it makes everything faster.",
                    "label": 0
                },
                {
                    "sent": "You can work easier and more data at the same time.",
                    "label": 0
                },
                {
                    "sent": "You could do that parallelism, but you can also work on bigger models if you want to model properties and so to try to make that easier for everybody.",
                    "label": 1
                },
                {
                    "sent": "We developed that library called Nickel and CCL an it's a collective library.",
                    "label": 0
                },
                {
                    "sent": "So what this means is it's a research library of accelerated routine to provide the classical operation people use when doing parallel computing like scatter, gather, reduce.",
                    "label": 1
                },
                {
                    "sent": "The pattern is developed after the MPI collective, so somebody mentioned MPI yesterday were using that kind of scheme to make sure people know your correction or find the operation they expect an it handles every internode communication in an optimal way, so you don't have to worry about how you imagine you have a box with a GPU's.",
                    "label": 1
                },
                {
                    "sent": "An you could put four GP on one CPU HP on the same CPU, and so on and so forth, and it ends up being a nightmare of having to figure out where your data pattern here is and where you communication pattern.",
                    "label": 0
                },
                {
                    "sent": "Here is the purpose of this is just to make it.",
                    "label": 0
                },
                {
                    "sent": "Easier for everybody and actually not having to worry of where your GPU's are.",
                    "label": 0
                },
                {
                    "sent": "So when you're using that library, having a GPU's or four GPU on single CPU is not going to change anything, or at least it's going to be minimal.",
                    "label": 0
                },
                {
                    "sent": "For most of the cases.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, the purpose is to make it easy for everybody so it looks very much like the classical C approach.",
                    "label": 0
                },
                {
                    "sent": "So this is just a small example of a single threaded or reduce operation here.",
                    "label": 0
                },
                {
                    "sent": "And that's how you called that library.",
                    "label": 0
                },
                {
                    "sent": "You include that library and you just called and or reduce and you get the data from all the GPU you had and we make sure that operation is tune from multi GPU platform.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm moving on.",
                    "label": 0
                },
                {
                    "sent": "I'm moving on fast quickly on purpose so that we can get to the end zone, but please don't be disturbed by that and feel free to stop me platform.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you might have heard that last week if I remember correctly, we launched a new product which is the Titan Express cow, so you should have received an email from a raffle.",
                    "label": 0
                },
                {
                    "sent": "The titanics by scale doesn't fit in my purse, so the raffle is such that we gather the email information and we're going to send the titanics Pascal to the winner.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I don't have the classical titanics Pascal here today.",
                    "label": 0
                },
                {
                    "sent": "As I said, it doesn't fit in my bag, so please don't forget to register for the raffle.",
                    "label": 0
                },
                {
                    "sent": "Some of you did.",
                    "label": 0
                },
                {
                    "sent": "And so, as you might have seen, it's very interesting because.",
                    "label": 0
                },
                {
                    "sent": "On the NVIDIA blog you can see the detail of that tub and the first thing we say.",
                    "label": 0
                },
                {
                    "sent": "It's 11th teraflops.",
                    "label": 0
                },
                {
                    "sent": "FP 32.",
                    "label": 0
                },
                {
                    "sent": "OK, but the second thing we say is that called Conduent 8.",
                    "label": 0
                },
                {
                    "sent": "And if you remember the talks from yesterday, it shows that that called somehow is made for deep learning development, right?",
                    "label": 0
                },
                {
                    "sent": "We're not looking at different GPUs having different set of instructions that can do different type of deep learning operations, and so that's a code that people can use for developing into 8 deep learning workloads.",
                    "label": 0
                },
                {
                    "sent": "So it has 3500 for CUDA cores.",
                    "label": 1
                },
                {
                    "sent": "I'm sorry I missed a word and plug gig of GDDR 5X RAM.",
                    "label": 0
                },
                {
                    "sent": "So that's the brand new product.",
                    "label": 0
                },
                {
                    "sent": "Don't forget to enter the raffle.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And yes, it doesn't fit in your bag.",
                    "label": 0
                },
                {
                    "sent": "This one is even bigger.",
                    "label": 0
                },
                {
                    "sent": "It doesn't fit in your back either.",
                    "label": 0
                },
                {
                    "sent": "It might not even fit in your car, but we hope it's going to fit in your data center.",
                    "label": 0
                },
                {
                    "sent": "This is the DGX 1.",
                    "label": 1
                },
                {
                    "sent": "So it was mentioned yesterday that not only multi GPU workloads where I'm becoming more important but also the communications between the GPU's were becoming more important.",
                    "label": 0
                },
                {
                    "sent": "And this one is very important for us, because that's the first product for which we do the full server that is dedicated to deep learning.",
                    "label": 0
                },
                {
                    "sent": "Anet uses a new type of interconnect between the GPU's, which has like a higher bandwidth than the PCI E. Best that people were using before but not only have we done that, we also make sure we took care about the IO.",
                    "label": 0
                },
                {
                    "sent": "Indie planning people don't only care about compute, they only care about pushing the data to the GPU and sometimes it can be a bottleneck.",
                    "label": 0
                },
                {
                    "sent": "Some benchmark I've done like only on synthetic data, but in daily life we are very conscious that if you work on images you need to bring those images to the GPU.",
                    "label": 0
                },
                {
                    "sent": "And this can be a bottleneck, so we make sure that this box can handle it in the best way possible.",
                    "label": 1
                },
                {
                    "sent": "So it uses a Tesla P-100 hybrid Commission.",
                    "label": 0
                },
                {
                    "sent": "That's the way the GPU's organized.",
                    "label": 0
                },
                {
                    "sent": "We can discuss that a bit more, and so thanks to Ryan, we brought a few 100 so Ryan Devine come and do the presentation.",
                    "label": 0
                },
                {
                    "sent": "And, well, so we'll have the P. 100 so that you can look and touch it.",
                    "label": 0
                },
                {
                    "sent": "Don't put it in your backpack.",
                    "label": 0
                },
                {
                    "sent": "It's not functional.",
                    "label": 0
                },
                {
                    "sent": "And on top of it, it's like you might not have the baseball to put it on so.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so the P-100 is.",
                    "label": 0
                },
                {
                    "sent": "This is actually a really exciting new GPU that we've announced an I'm the pass around the room, so you can all get your hands on it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "And so the first thing you're going to notice is a wait.",
                    "label": 0
                },
                {
                    "sent": "So not only is there some super interesting like hardware design going on this chip, but there's also some really interesting mechanical engineering we can talk about that after, but there's some really cool stuff that's just going on the baseboard.",
                    "label": 0
                },
                {
                    "sent": "Three things I'm going to talk about just to point out from architectural point of view is the chip, the new GPU, and the new architecture.",
                    "label": 0
                },
                {
                    "sent": "The memory, which is completely new and the envy link, which is I would say probably the biggest and most exciting change of this whole design.",
                    "label": 0
                },
                {
                    "sent": "So with Pascal, this is a completely new architecture.",
                    "label": 0
                },
                {
                    "sent": "You're not going to be surprised.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you it's faster.",
                    "label": 0
                },
                {
                    "sent": "Kind of a no brainer, but under the covers the SM design in the microarchitecture is also completely new or not completely new, but newer in the sense that we have a lot more SMS.",
                    "label": 0
                },
                {
                    "sent": "Each SM is maybe a little less powerful, but we keep all the same register and shared memory units per SM.",
                    "label": 0
                },
                {
                    "sent": "So now you have less cores per SM with the same amount of registers and shared memory, which is makes actually programming for those SME's a little bit easier.",
                    "label": 0
                },
                {
                    "sent": "Moving on to the high bandwidth memory so most people as you pass around, you're going to notice the units going around the outside and most people will say.",
                    "label": 0
                },
                {
                    "sent": "I bet that's the memory and you'd be wrong.",
                    "label": 0
                },
                {
                    "sent": "So on most GPU's memory is outside the die but with high bandwidth or HBM 2, we moved the memory onto the chip.",
                    "label": 0
                },
                {
                    "sent": "So this is a technology that we introduced called a chip on wafer on substrate so the GPU upsell on to that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so the GPU is a chip that sits on a piece of Silicon with an interposer and we stack the memory rate on the die.",
                    "label": 1
                },
                {
                    "sent": "So the benefits from that is a vast reduction in the amount of energy per memory operation.",
                    "label": 0
                },
                {
                    "sent": "So not only can we run, we greatly increase the memory bandwidth up to 720 gigabytes a second, but we've also greatly reduced the power per memory OP, and so that's a huge increase over.",
                    "label": 0
                },
                {
                    "sent": "I think we had like 200 and 32180 on the previous Maxwell generation, so there's a huge step up and finally envy link.",
                    "label": 0
                },
                {
                    "sent": "So on the back there's two plastic pins in there that you can pop off and you can look at the actual pins.",
                    "label": 0
                },
                {
                    "sent": "One of those connectors for the Envy link and the other is for the PCI E and for the power and this is attached directly on to the baseboard and envy link is.",
                    "label": 0
                },
                {
                    "sent": "Trace onto the onto the baseboard Anne.",
                    "label": 0
                },
                {
                    "sent": "Really, this is kind of the most unique architectural change that we've enabled with the P-100 because.",
                    "label": 0
                },
                {
                    "sent": "It basically is the multicore SMP for GPU's.",
                    "label": 1
                },
                {
                    "sent": "Now you have you can directly address memory for directly attached GPU's and then you can take and take advantage of nickel for the communication between.",
                    "label": 0
                },
                {
                    "sent": "So if you have can make if your algorithms can make use of aggregate distributed memory you now have 16 'cause we have 16 gigs of HBM 2 * 8 in the DGX one.",
                    "label": 0
                },
                {
                    "sent": "So you have 128 gigabytes of.",
                    "label": 0
                },
                {
                    "sent": "Directly accessible memory, so that can greatly increase the models that you can put in there, and you have very fast access to it.",
                    "label": 0
                },
                {
                    "sent": "And with that, I think I'll turn it back to Julie.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you, So what you can see it's it's brand new and it's getting us very excited.",
                    "label": 0
                },
                {
                    "sent": "We just hope it's going to get you as excited as we are.",
                    "label": 0
                },
                {
                    "sent": "An so this is a great for us.",
                    "label": 0
                },
                {
                    "sent": "This is a great knew.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "How do I improvement that?",
                    "label": 0
                },
                {
                    "sent": "We need to be able to program that hardware and we need we want everybody to be able to program that all there.",
                    "label": 0
                },
                {
                    "sent": "So Nickel was taking care of the multi GPU communication.",
                    "label": 0
                },
                {
                    "sent": "Now we see a lot of people are looking at inference right?",
                    "label": 0
                },
                {
                    "sent": "And this is the whole scheme on how we see the world in production.",
                    "label": 0
                },
                {
                    "sent": "Where in an image.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine an image workload.",
                    "label": 0
                },
                {
                    "sent": "People would take the image, put them in a repository of meant maybe welcome those images.",
                    "label": 0
                },
                {
                    "sent": "And work with you know Cafe Torciano.",
                    "label": 0
                },
                {
                    "sent": "So right 9 digits.",
                    "label": 0
                },
                {
                    "sent": "We have caffeine torch where we provide a web UI to help people develop their home.",
                    "label": 0
                },
                {
                    "sent": "Deep learning workload without knowing anything about frameworks and then we train, we test.",
                    "label": 0
                },
                {
                    "sent": "We set up a model an we want to do inference and when you want to do inference you can be interested in doing inference for very different applications.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the focus.",
                    "label": 0
                },
                {
                    "sent": "We have an inference.",
                    "label": 0
                },
                {
                    "sent": "OK so you have a drone you want your drone to capture where the cause are you have your car.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to see pedestrians and what we think is good is to develop something which is dedicated to inference because the workload.",
                    "label": 0
                },
                {
                    "sent": "Is kind of different.",
                    "label": 0
                },
                {
                    "sent": "We see a lot of people during inference that are interest.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did in using tricks such as Fusion, so my slides are not in the right order.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that's what it is.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back there, so this is an example.",
                    "label": 0
                },
                {
                    "sent": "Often NVIDIA platform for embedded.",
                    "label": 0
                },
                {
                    "sent": "So this is the TX one.",
                    "label": 0
                },
                {
                    "sent": "It's a small bowl that people will use in drone, for example an it used to be that we have the same software stack, so this is to come back when I said the GPU has to be attached to something.",
                    "label": 0
                },
                {
                    "sent": "I cannot comment on that right now.",
                    "label": 0
                },
                {
                    "sent": "Each time, So what I can say is each time we have a new architecture, usually we bring that new architecture to server Workstation and invited.",
                    "label": 0
                },
                {
                    "sent": "I cannot comment on the timeline, so is right.",
                    "label": 0
                },
                {
                    "sent": "So what we wanted to point out also is this is the previous Maxwell architecture and what we did at the time is people will train a model and we have the same software stack everywhere.",
                    "label": 0
                },
                {
                    "sent": "So the fact that it's not the same CPU is transparent to the user, right?",
                    "label": 0
                },
                {
                    "sent": "So this is an example, we cafe where you just take your cafe model file an we have example on the web and you push your cafe model files on your T X1.",
                    "label": 0
                },
                {
                    "sent": "And you can run inference without any modification in the code.",
                    "label": 0
                },
                {
                    "sent": "And so we figured we wanted to do more about that and we wanted to make that inference even more efficient.",
                    "label": 0
                },
                {
                    "sent": "So that's also.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why we did not GE?",
                    "label": 0
                },
                {
                    "sent": "So the ID is when we do inference and we do that forward pass, we can use a lot of tricks to be faster.",
                    "label": 0
                },
                {
                    "sent": "We confuse the layer.",
                    "label": 0
                },
                {
                    "sent": "We can eliminate some layer, we can specialize some kernels.",
                    "label": 0
                },
                {
                    "sent": "We can do a lot of autotuning from the target platform, and we can make sure that wherever that model is pushed, pushed is going to be tuned to work better.",
                    "label": 0
                },
                {
                    "sent": "That's what GI is about, and so I invite you to have a look.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the website this is an example of performance numbers, so on the right you have the classical cafe, so GPU workload and on the left you have GE plus GPU workloads and so Ryan actually made that graph so you can talk to him at the end.",
                    "label": 0
                },
                {
                    "sent": "What's very interesting is sometimes you see big improvements in performance and it's actually when we fusing layers both horizontally and vertically, right?",
                    "label": 0
                },
                {
                    "sent": "And having our results using brings a lot of performance.",
                    "label": 0
                },
                {
                    "sent": "Impact, yeah.",
                    "label": 0
                },
                {
                    "sent": "The way accesses the performance is the time for each layer.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "I was right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm sorry it's a later time.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's why in France I mentioned some people don't want to see the GPU decoding level.",
                    "label": 0
                },
                {
                    "sent": "They don't want to see the library level.",
                    "label": 0
                },
                {
                    "sent": "They don't even want to know what a framework is.",
                    "label": 0
                },
                {
                    "sent": "I guess that's not so many people in that audience, so I'm going to be fast.",
                    "label": 0
                },
                {
                    "sent": "We also have a way of your eye called digits that is available in the labs that you can look into if you wish.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trader, so I'm done with the global presentation.",
                    "label": 0
                },
                {
                    "sent": "So do you have any questions before I get into the introduction to programming and ends on yes?",
                    "label": 0
                },
                {
                    "sent": "So when are we going to see floating .16 and other chips?",
                    "label": 0
                },
                {
                    "sent": "So very good question.",
                    "label": 0
                },
                {
                    "sent": "The question is when do we see FP 16 Anywhere on Titan X?",
                    "label": 0
                },
                {
                    "sent": "There is no FP 16.",
                    "label": 0
                },
                {
                    "sent": "There is a fee 16 on the GP 100.",
                    "label": 0
                },
                {
                    "sent": "I mean there is very slow FP16, not so many physician units, so GP 100 there is F16 operations.",
                    "label": 0
                },
                {
                    "sent": "So in the DGX one you have FP 16 we announced last month P 100 E which is the PCI E pendant of the GP 100 envy link that has SP 16.",
                    "label": 0
                },
                {
                    "sent": "So it's the same chip that goes on the PCI E bus.",
                    "label": 0
                },
                {
                    "sent": "And as of today, that's the only product we announced publicly.",
                    "label": 0
                },
                {
                    "sent": "So right now, if you have a server that takes a regular call on PC, IE the solution for you is P-100 E that we announced last month.",
                    "label": 0
                },
                {
                    "sent": "I mean end of June, and that should be available in the fall.",
                    "label": 0
                },
                {
                    "sent": "I have no clue about pricing.",
                    "label": 0
                },
                {
                    "sent": "That we can fill up that request.",
                    "label": 0
                },
                {
                    "sent": "I don't know at all.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "GPU.",
                    "label": 0
                },
                {
                    "sent": "This is a hardware reason.",
                    "label": 0
                },
                {
                    "sent": "Like the process you might have heard about like 16.",
                    "label": 0
                },
                {
                    "sent": "You know 1614 the FinFET mechanics and there's only so much memory you can put on the die.",
                    "label": 0
                },
                {
                    "sent": "Anne, right now when you see so few.",
                    "label": 0
                },
                {
                    "sent": "I mean, we say that's a lot already.",
                    "label": 0
                },
                {
                    "sent": "You can get up to 24 gigs.",
                    "label": 0
                },
                {
                    "sent": "Regular GDDR memory.",
                    "label": 0
                },
                {
                    "sent": "That's what we've shown before for HBM 2.",
                    "label": 0
                },
                {
                    "sent": "It's only 16 gigs right now.",
                    "label": 0
                },
                {
                    "sent": "It's mostly because of the adware process.",
                    "label": 0
                },
                {
                    "sent": "It's not the die size is the fact it's how much memory you can put on the die and actually the yielding of getting those I built.",
                    "label": 0
                },
                {
                    "sent": "And the other point, I would make there is that we are programming for a GPU.",
                    "label": 0
                },
                {
                    "sent": "You're taking advantage of the.",
                    "label": 0
                },
                {
                    "sent": "Ability to.",
                    "label": 0
                },
                {
                    "sent": "Hi.",
                    "label": 0
                },
                {
                    "sent": "Ability to overlap memory operations with computers, and so you don't need a lot of storage.",
                    "label": 0
                },
                {
                    "sent": "You have registers that you contact switch in and out of, and then you're constantly you're using the GPU memory bandwidth as a kind of a buffer there, and so it's ability to contact swap between different stages of your computer.",
                    "label": 0
                },
                {
                    "sent": "Block on IO coming in you should be considered somewhere else.",
                    "label": 0
                },
                {
                    "sent": "That's that's how we think, and we've improved that with.",
                    "label": 0
                },
                {
                    "sent": "P-100 'cause now you have more registers and more shared memory per SM, then you didn't pass architectures.",
                    "label": 0
                },
                {
                    "sent": "And I see the answer we made to that question is also very interesting to me, because it's always a balance between the hardware and the software.",
                    "label": 0
                },
                {
                    "sent": "So GPU's, or generate programming units right there not dedicate it to a specific task.",
                    "label": 0
                },
                {
                    "sent": "There are some tasks that they do well, and all those answers are going to be a combination on how you program.",
                    "label": 0
                },
                {
                    "sent": "And how the hardware is set up.",
                    "label": 0
                },
                {
                    "sent": "So while we try to extract as much as we can of the hardware, having an idea of how it isn't the need help.",
                    "label": 0
                },
                {
                    "sent": "So for the CUDA part, we're going to see a lot of the register and memory.",
                    "label": 0
                },
                {
                    "sent": "So if some of you don't know what an SM is yet, don't worry.",
                    "label": 0
                },
                {
                    "sent": "We'll see that.",
                    "label": 0
                },
                {
                    "sent": "It's just you have to have an idea of what some things and it's it, but you don't have to know what to program individually.",
                    "label": 0
                },
                {
                    "sent": "Just having an idea of the sizing underneath sufficient.",
                    "label": 0
                },
                {
                    "sent": "Usually.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Presented so you could review them.",
                    "label": 0
                },
                {
                    "sent": "I can do anyone.",
                    "label": 0
                },
                {
                    "sent": "Could you tell us more about the building points, accuracy and whether that's or how that effects the types of models taken?",
                    "label": 0
                },
                {
                    "sent": "So the question is for those of us that are new to GPU's, can you tell me the influence of the plotting point accuracy on what we do training basically so for if the 16 and into a tie leave the experts to reply to you.",
                    "label": 0
                },
                {
                    "sent": "It's always the easy answer is it's always a tradeoff.",
                    "label": 0
                },
                {
                    "sent": "Between accuracy and training speed, right?",
                    "label": 0
                },
                {
                    "sent": "If you train with a lower position.",
                    "label": 0
                },
                {
                    "sent": "If you want to keep your accuracy, sometimes you have to be careful, especially when you want to do training.",
                    "label": 0
                },
                {
                    "sent": "What happens in the gradient you want to be very careful that so there are some operations that are very easy.",
                    "label": 0
                },
                {
                    "sent": "And that have absolutely no impact in in 15 an hour person that are much more tricky when you're training.",
                    "label": 0
                },
                {
                    "sent": "Depending on what type of network you're training for in France.",
                    "label": 0
                },
                {
                    "sent": "I would say that the community now agrees that you can infer in FP16 sometime in eight 8.",
                    "label": 0
                },
                {
                    "sent": "Having a drop in accuracy of less than like 5% for any kind, of like Alex Nagel net come on, you know network type.",
                    "label": 0
                },
                {
                    "sent": "For training the community still, maybe you can convince on that because I feel it's a fixed question, right?",
                    "label": 0
                },
                {
                    "sent": "Still, it depends how you do it.",
                    "label": 0
                },
                {
                    "sent": "If you do it smartly so that it learns to deal with low precision while it's raining.",
                    "label": 0
                },
                {
                    "sent": "Then you can get away with very little precision in the actual computation.",
                    "label": 0
                },
                {
                    "sent": "You still need the enough decision for restoring the whites because they are accumulating a lot of small changes, so these guys needs at least 16 bits and they need floating point.",
                    "label": 0
                },
                {
                    "sent": "So yeah, but I guess a lot of this is recent research papers over the last year and more work needs to be done.",
                    "label": 0
                },
                {
                    "sent": "To try these different variants on different architectures and datasets, but the early results that basically what they can get away.",
                    "label": 0
                },
                {
                    "sent": "In fact, with a single bit computation on some architectures in datasets.",
                    "label": 0
                },
                {
                    "sent": "But you still need a high precision on the way to entrain when once you trains, then you can quantize very very aggressively and still get pretty good results.",
                    "label": 0
                },
                {
                    "sent": "So for us, the way we see it is as much as we can provide people with operation that is faster at every stage we're going to do it sometime.",
                    "label": 0
                },
                {
                    "sent": "It's easy to put in all where sometimes it's likely harder.",
                    "label": 0
                },
                {
                    "sent": "And I said, we're still generating generating purpose.",
                    "label": 0
                },
                {
                    "sent": "Hardware right?",
                    "label": 0
                },
                {
                    "sent": "So each time you see GPU, if you're interested in deep learning, I can only advise you to look at the FP 32, FP 16 and eat 8 numbers.",
                    "label": 0
                },
                {
                    "sent": "I don't know if I know very few people in machine learning that look at 64, but in terms of like numerical computing we see a whole range.",
                    "label": 0
                },
                {
                    "sent": "Of people so right now as a beginner FP 32 is safe.",
                    "label": 0
                },
                {
                    "sent": "That's what most of the framework implement by default, but research is very active and we just want to make sure we support that correctly.",
                    "label": 0
                },
                {
                    "sent": "So you could go down as low as a single bit per week.",
                    "label": 0
                },
                {
                    "sent": "And he still work in terms of amount of computation, like the storage for the still in the storage room framework.",
                    "label": 0
                },
                {
                    "sent": "For those with yeah and actually.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "So that's a very good point too, because like when I showed the TX one before the little teeny tiny platform, this was the first platform where we had FP 16 built in in the hardware, and the first code we published were actually.",
                    "label": 0
                },
                {
                    "sent": "We were very careful about the storage, we could do sometime FP 16 for storage and not for compute.",
                    "label": 0
                },
                {
                    "sent": "So what you're going to see and what you might want to pay attention to is the difference between the precision in the compute and storage all the way around.",
                    "label": 0
                },
                {
                    "sent": "So when we say FP 16, you're going to hear conversation like full FP 16 or with that kind of consideration.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Can you talk about the?",
                    "label": 0
                },
                {
                    "sent": "There is a great blog post written by Ryan.",
                    "label": 0
                },
                {
                    "sent": "Parallel.",
                    "label": 0
                },
                {
                    "sent": "I can.",
                    "label": 0
                },
                {
                    "sent": "Ann will be around until Friday.",
                    "label": 0
                },
                {
                    "sent": "If you want to talk to us about.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before I got you could as I mentioned to program correctly on the.",
                    "label": 0
                },
                {
                    "sent": "Southwest side you have to have an idea for the hardware looks like so I don't.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into a full GPU architecture class.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I just want to define a few terms.",
                    "label": 0
                },
                {
                    "sent": "The GPU, assuming components memory and streaming multiprocessor SMS.",
                    "label": 0
                },
                {
                    "sent": "OK, so the memory is where you still data by definition and the SME's are where you do the computation, so the global memory of the GP.",
                    "label": 0
                },
                {
                    "sent": "You can do that.",
                    "label": 0
                },
                {
                    "sent": "You can see that as the ram that you would have on the CPU machine and it's accessible by both the GPU and CPU.",
                    "label": 1
                },
                {
                    "sent": "That's what we call the global memory right now in GPS we see up to 24 gigs of global memory, so it's already.",
                    "label": 0
                },
                {
                    "sent": "It's already much more than we saw some years ago.",
                    "label": 0
                },
                {
                    "sent": "And you can have error checking on the memory in some GPU's in case you need it.",
                    "label": 0
                },
                {
                    "sent": "So that's the part for storing data for computing GPU's use what we call this end.",
                    "label": 0
                },
                {
                    "sent": "So streaming multiprocessors that perform the actual computation.",
                    "label": 1
                },
                {
                    "sent": "So as Ryan mentioned before each SM its own control unit register execution pipeline Ancash, so DSM is actually the unit that performs the computation.",
                    "label": 0
                },
                {
                    "sent": "OK, if it's not super clear, I'm going to have more drawings, so hopefully.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get clear.",
                    "label": 0
                },
                {
                    "sent": "In each SM.",
                    "label": 0
                },
                {
                    "sent": "You have many could, of course.",
                    "label": 0
                },
                {
                    "sent": "Their architecture independent, they can do special function like a sinus sinus, whatever.",
                    "label": 0
                },
                {
                    "sent": "Each SM.",
                    "label": 0
                },
                {
                    "sent": "I shared memory plus L1 cache.",
                    "label": 0
                },
                {
                    "sent": "And they have thousands of registers.",
                    "label": 0
                },
                {
                    "sent": "So that's what the drawing looks like, right?",
                    "label": 0
                },
                {
                    "sent": "The course, and that's what that's what an SM is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I already mentioned several types of memory.",
                    "label": 0
                },
                {
                    "sent": "I just want to give like a little synopsis here.",
                    "label": 0
                },
                {
                    "sent": "So I had the global memory, those 12 or 24 gigs I talked about.",
                    "label": 1
                },
                {
                    "sent": "There is the L2 cache on top of it underneath.",
                    "label": 1
                },
                {
                    "sent": "Depending on you see this is going to stay on top of it and then for each SM you're going to have for justice L1 cache and shared mem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the reason why I'm saying that is because when you're going to program, you have to be aware on where is what it's going to be less and less important, soon with unified memory, specially for the difference between global memory and local memory.",
                    "label": 0
                },
                {
                    "sent": "But we're going to see that later.",
                    "label": 0
                },
                {
                    "sent": "Could.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Programming level, so I took care of the memory.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to tell you how we compute on the GPU.",
                    "label": 0
                },
                {
                    "sent": "So there is a part of the code which is serial codes that execut on the CPU, whatever that CPU is.",
                    "label": 0
                },
                {
                    "sent": "The product code executes on what we call the device.",
                    "label": 1
                },
                {
                    "sent": "The device is the GPU.",
                    "label": 0
                },
                {
                    "sent": "It executes through many threads.",
                    "label": 0
                },
                {
                    "sent": "Across multiple processing elements.",
                    "label": 1
                },
                {
                    "sent": "So that's basically what a CUDA C++ applications looks like.",
                    "label": 1
                },
                {
                    "sent": "You have some serial code executing on the CPU gets product on the GPU back to the CPU back to the GPU and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "That's how that's why we care so much about communications, because if that process not efficient we can see it's going to be problematic.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A quick example, so it's an example which is slightly more complicated than the one I've shown at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So here what I'm doing is a simple vector operation where I do Y equals a X + Y.",
                    "label": 0
                },
                {
                    "sent": "Which is what we call the sex by operation.",
                    "label": 0
                },
                {
                    "sent": "So just basically adding to vector an ad in a wet weighted vector right?",
                    "label": 0
                },
                {
                    "sent": "And so the first part is the standard C code.",
                    "label": 0
                },
                {
                    "sent": "And the second part is the parallel C code, and as you see it's broadly code, but there is no loop.",
                    "label": 0
                },
                {
                    "sent": "The loop is encapsulated in that cryptic instruction which defined blocks blogged in an thread.",
                    "label": 0
                },
                {
                    "sent": "I haven't mentioned of global memory at the top and at the bottom I launch a function which is called parallel where I have the number of block and I have that #256 here that I'm going to tell afterwards.",
                    "label": 0
                },
                {
                    "sent": "OK so basically what we do is we don't do the loop, but we provide a pseudo loop that is executing in blocks and threads and so on and so forth, OK.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You've seen the previous slide I mentioned.",
                    "label": 0
                },
                {
                    "sent": "This is a sax by Colonel.",
                    "label": 0
                },
                {
                    "sent": "On the GPU, the parallel portion of the application was going to run on the GPU.",
                    "label": 1
                },
                {
                    "sent": "We call that a kernel.",
                    "label": 1
                },
                {
                    "sent": "An entire GPU can only execute execute a lot of kernels and many threads at the same time.",
                    "label": 0
                },
                {
                    "sent": "The critter threads are defined to be very light switching very, very fast so that we can switch between straight very very quickly and we have thousands that can execute simultaneously.",
                    "label": 0
                },
                {
                    "sent": "It makes it a bit difficult the first time you program it because you have to think of all the things that are happening at the same time that if you've been programming with languages like Matlab or or where you work on table, it's a bit similar.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So again, vocabulary, the CPU is what we're going to call the host.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry it confirms the age correctly.",
                    "label": 0
                },
                {
                    "sent": "It executes function and the GPU, we call it the device, and it's going to execute kernels.",
                    "label": 0
                },
                {
                    "sent": "So when we say we execute, these are kernel.",
                    "label": 0
                },
                {
                    "sent": "It's on the GPU and a function is going to be on the CPU OK for everybody.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So could a kernel OK. Low is a function which is executed on the GPU.",
                    "label": 1
                },
                {
                    "sent": "That's what I said before, and it's actually an array of threads, so that's what define on the right.",
                    "label": 0
                },
                {
                    "sent": "All these threads will execute exactly the same code.",
                    "label": 1
                },
                {
                    "sent": "They're all the same, but they can take different paths.",
                    "label": 1
                },
                {
                    "sent": "And it's read as an ID so that you can actually pinpoint it.",
                    "label": 0
                },
                {
                    "sent": "You can select input, output data and it can make control decision.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's an example on the right.",
                    "label": 0
                },
                {
                    "sent": "Well, I have a thread I ask you to function and I have an output.",
                    "label": 0
                },
                {
                    "sent": "So that's this thread.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to see that in the end, Zone Labs is going to depend on the GPU you have.",
                    "label": 0
                },
                {
                    "sent": "Ice.",
                    "label": 0
                },
                {
                    "sent": "Yes, so each thread cannot.",
                    "label": 0
                },
                {
                    "sent": "So each core I'm going to get to that.",
                    "label": 0
                },
                {
                    "sent": "So here each arrow is a thread.",
                    "label": 0
                },
                {
                    "sent": "And threat can be survived to divide it into blocks.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a block is going to be a bunch of threads.",
                    "label": 1
                },
                {
                    "sent": "Blocks will be grouped into a grid.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Panel is executed as a grid of blocks of threads.",
                    "label": 0
                },
                {
                    "sent": "So when you execute the kernel, you actually push a full grid of Black Ops threads on the GPU at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's red, so that's the parallel between the software and hardware here.",
                    "label": 0
                },
                {
                    "sent": "It's red will be executed by a core.",
                    "label": 1
                },
                {
                    "sent": "With thread, each thread block will be executed by an SM.",
                    "label": 0
                },
                {
                    "sent": "In which each kernel is going to be executed by the GPU, GPU being able to have several kernels at the same time.",
                    "label": 1
                },
                {
                    "sent": "OK for everybody.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to enter a subject which is a very difficult subject.",
                    "label": 0
                },
                {
                    "sent": "It's cooperation since a lot of things are happening at the same time.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you can cooperate, sometimes you cannot.",
                    "label": 0
                },
                {
                    "sent": "So thread blocks there look operation inside the send block.",
                    "label": 1
                },
                {
                    "sent": "This rates can cooperate.",
                    "label": 1
                },
                {
                    "sent": "What this means they can load and store memory they will use.",
                    "label": 0
                },
                {
                    "sent": "They can share results with each other inside the block and then you can synchronize with each other.",
                    "label": 1
                },
                {
                    "sent": "This is all inside a specific block, not outside a block.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thread blocks allow for scalability.",
                    "label": 1
                },
                {
                    "sent": "What I mean by that is blocks can execute in any order concurrently or sequentially, so you can see right away that having cooperation between blocks is not something you might want to have because the blocks are supposed to be independent, and that's what gives you stability, scalability and performance.",
                    "label": 1
                },
                {
                    "sent": "So inside a block you can do whatever you want outside a block.",
                    "label": 1
                },
                {
                    "sent": "You have to be careful.",
                    "label": 0
                },
                {
                    "sent": "Because you don't control how and when it gets execute right, and so accountable scales across any number of SME's, and that's where it gets tricky.",
                    "label": 0
                },
                {
                    "sent": "We're going to see that more in the end zone lab, but you cannot corporate between blocks, and especially you want to make sure you don't want to have block to wait for each other.",
                    "label": 0
                },
                {
                    "sent": "That's not possible.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, to memory.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned the hardware before.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to mention what it what it gives for the software pop so threads have register the only the memory.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They have local memory.",
                    "label": 0
                },
                {
                    "sent": "And the block of thread has shared memory.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So shared memory is defined like this.",
                    "label": 1
                },
                {
                    "sent": "It's allocated per block.",
                    "label": 0
                },
                {
                    "sent": "And he has the safe lifetime of a block when your blogs on the share memory is gone.",
                    "label": 0
                },
                {
                    "sent": "It's accessible by any trades in thread in the block that it looks for Corporation, and you can share data amongst red or you can manage on your own yocash.",
                    "label": 1
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "This is particularly interesting when you do metric suppression and deep learning when you do gems, because like the you can reduce memory access doing that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Again, register local memory into Black Ops without shared memory and all the blocks can access the global memory.",
                    "label": 1
                },
                {
                    "sent": "The reason why I'm making a bit first of it is like if you haven't played with it, you can say OK, I'm going to put everything in global memory and I'm going to be good.",
                    "label": 0
                },
                {
                    "sent": "The problem is global memory is not as fast as local memory, so you want to make sure you benefit as much as you can from the different memory access so that you have to fast this access pattern possible.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the global memory is accessible by all the thread of any kernel.",
                    "label": 1
                },
                {
                    "sent": "Because the kernel runs on the GPU.",
                    "label": 0
                },
                {
                    "sent": "Is Lifetime is much longer, the data allocation is when you allocate you get it and when you deallocate by the host the CPU.",
                    "label": 0
                },
                {
                    "sent": "So for those of you that are used to that.",
                    "label": 0
                },
                {
                    "sent": "Usually if you use a framework for example, you launch a workload and you look at the GPU memory, you're going to see it being filled and at the end you're going to see the memory goes back to very few users and that's exactly what's going on.",
                    "label": 0
                },
                {
                    "sent": "OK, this is by opposite to the local memory that disappears when the thread is gone.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Memory management.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne, I'm almost done.",
                    "label": 0
                },
                {
                    "sent": "So again, CPU and GPU have separate memory spaces and the data is moved across a PCI E bus.",
                    "label": 1
                },
                {
                    "sent": "For most of the GPS we know now we can move some of his friend dealing, but I'm not going to do that right now so we can use function to allocate, deallocate and copy, just like in.",
                    "label": 0
                },
                {
                    "sent": "See, we're going to see that Indians on lamp.",
                    "label": 1
                },
                {
                    "sent": "Pointers or pointers like there are just addresses, so if you have a pointer you can't know.",
                    "label": 0
                },
                {
                    "sent": "Where it is, you can't know whether that pointer is on the CPU on the GPU, but as a programmer you might want to know because that might make a big difference.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You must check the attributes of your pointers.",
                    "label": 0
                },
                {
                    "sent": "You know where you point your pointer resides.",
                    "label": 0
                },
                {
                    "sent": "We so I'm sorry it sounds like a lot of like do don't do that but I just want you to have like a summary of this because we're going to see that in a lab.",
                    "label": 0
                },
                {
                    "sent": "We want to be careful when we difference Ng, because if you dare reference appointed that somewhere where it doesn't exist, you will have a bad crash and this is very difficult to find in the code even with the provider and everything that's that's hard to see those, even if the burger.",
                    "label": 0
                },
                {
                    "sent": "So anytime you use a pointer, try to put a quick comment.",
                    "label": 0
                },
                {
                    "sent": "You know where that pointer resides.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Allocation of memory and release.",
                    "label": 1
                },
                {
                    "sent": "It's basically like in C except you had CUDA at the beginning.",
                    "label": 1
                },
                {
                    "sent": "Analog keramidas that set CUDA free.",
                    "label": 0
                },
                {
                    "sent": "This is an example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Again, we have that hardware software balance that what we call device memory is the GPU memory and what we call host memory is the CPU memory.",
                    "label": 1
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Cute and then copy.",
                    "label": 0
                },
                {
                    "sent": "So that's an operation that sometimes.",
                    "label": 0
                },
                {
                    "sent": "You know, it's difficult to to understand.",
                    "label": 0
                },
                {
                    "sent": "She returns when the copy is complete, complete and it blocks a CPU thread.",
                    "label": 1
                },
                {
                    "sent": "Until it's been copied.",
                    "label": 0
                },
                {
                    "sent": "So what data transfers you start a CPU thread, it goes to the GPU like copy and then blocks until it comes back.",
                    "label": 1
                },
                {
                    "sent": "So if you want to be efficient, that's something you want to talk about, and it doesn't start coping until all the previous cuticles are complete.",
                    "label": 0
                },
                {
                    "sent": "So you can't copy while something else is going on.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go too much into the memco Peosta device and device you open device to device will see that briefly in the labs.",
                    "label": 0
                },
                {
                    "sent": "If you wish an at some point, you know she wanted to discuss, we can discuss unified virtual memory, but that's not something I'm going to cover here, but it used to be, and it's still the case that.",
                    "label": 0
                },
                {
                    "sent": "Coping from host to device had to be done by the developer.",
                    "label": 0
                },
                {
                    "sent": "With what we call unified virtual memory, that's not going to be the case anymore.",
                    "label": 0
                },
                {
                    "sent": "It's going to be automatic.",
                    "label": 0
                },
                {
                    "sent": "And that's what's going on here.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just a basic.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cution and a summary of all I said.",
                    "label": 0
                },
                {
                    "sent": "Pilot code on the GPU.",
                    "label": 0
                },
                {
                    "sent": "Cooler kernel is launched an execut on the device.",
                    "label": 0
                },
                {
                    "sent": "The GPU by many thread.",
                    "label": 0
                },
                {
                    "sent": "Threads are grouped into thread blocks.",
                    "label": 1
                },
                {
                    "sent": "Parallel code is written for a thread.",
                    "label": 1
                },
                {
                    "sent": "So the loop is a thread.",
                    "label": 1
                },
                {
                    "sent": "Each thread is free to execute whatever he wants.",
                    "label": 0
                },
                {
                    "sent": "An we have IDs for building threads and blocks.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thread or partition into thread blocks and the grid is all the blocks for a given launch.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I'm reading, but it's just like I just wanted to cover it to be OK. A thread block are bunch of threads that are synchronized and they could communicate via share memory.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ideas and dimension which answered the question how many thread an eye voluntary won't answer that question.",
                    "label": 0
                },
                {
                    "sent": "I'm doing that on purpose.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, so why I won't answer that question is because the first exercise in the end zone lab is actually for you to size your GPU by playing with Block ID.",
                    "label": 0
                },
                {
                    "sent": "An number of threads.",
                    "label": 0
                },
                {
                    "sent": "So thread are 3D.",
                    "label": 0
                },
                {
                    "sent": "They are unique within a block block or SU2D2 dimensional.",
                    "label": 1
                },
                {
                    "sent": "And their unique within the grid and dimensions can set can be set at lunchtime.",
                    "label": 1
                },
                {
                    "sent": "So in the end zone lab we start with a 1 dimensional for it to be simpler.",
                    "label": 0
                },
                {
                    "sent": "But then you can use 2 dimensional or three dimensional and the built-in variable we're going to use our thread index lock index that give the IDs block dimension, Grid dimension and each GPU will have.",
                    "label": 0
                },
                {
                    "sent": "Different asseman will have different values that are optimal for it, so sometimes you hear Oh yeah, I got GPU code, but I don't have it tuned.",
                    "label": 0
                },
                {
                    "sent": "Most of the tuning from one generation of GPU to another OK is at that level making sure the kernel we launch all the exact site an if you look it's very funny because I was very surprised the first time I looked at the Alex net paper.",
                    "label": 0
                },
                {
                    "sent": "If you look at it from an engineering perspective.",
                    "label": 0
                },
                {
                    "sent": "Alex recently did a wonderful drawing because when you see the drawing, you basically see the GPU.",
                    "label": 0
                },
                {
                    "sent": "It was working out.",
                    "label": 0
                },
                {
                    "sent": "And that's very nice, because each showed that paper was actually made to be running on the GPU, or the sizing were actually adapted to the GPU was working on.",
                    "label": 0
                },
                {
                    "sent": "And that's where the link between the actual dimensions sometime of the network and the hardware is so stringent.",
                    "label": 0
                },
                {
                    "sent": "That's a that's emblematic example nowadays is not astringent, but having an application well, size for where it leaves usually makes the performance very very impressive.",
                    "label": 0
                },
                {
                    "sent": "And actually that first code, like, could I connect two, was extremely stressful on NVIDIA GPU's for that reason because he was making it was using all that.",
                    "label": 0
                },
                {
                    "sent": "All the tuning a GPU could have at the time.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, another drawing thread.",
                    "label": 0
                },
                {
                    "sent": "3D idees unit with a block.",
                    "label": 0
                },
                {
                    "sent": "We got it.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The picture how do you do that on the GPU?",
                    "label": 0
                },
                {
                    "sent": "There is a special syntax which is the triple Chevron notation.",
                    "label": 0
                },
                {
                    "sent": "And so I'm not going to spend a lot of time on it, but that's what it looks like, Colonel lower lower than 30 two 5012 like greater grade.",
                    "label": 0
                },
                {
                    "sent": "And then the chevrons.",
                    "label": 0
                },
                {
                    "sent": "And that's how you define your sizing.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just a quick additional and also another word.",
                    "label": 0
                },
                {
                    "sent": "Account now launches on a grid off block.",
                    "label": 1
                },
                {
                    "sent": "Each block is launched on one SM.",
                    "label": 1
                },
                {
                    "sent": "And we also have another term which is called awop, where a block is divided into warps of 32 threads each.",
                    "label": 1
                },
                {
                    "sent": "It's a 32 vector, so you might want to have it a multiple of 32.",
                    "label": 0
                },
                {
                    "sent": "Because all the words execute the same instruction symmetrically, and that's very linked to the hardware.",
                    "label": 0
                },
                {
                    "sent": "The blocks runs too complex to compression on an SM.",
                    "label": 0
                },
                {
                    "sent": "It's starting on so you can't interrupt the block to do something else.",
                    "label": 0
                },
                {
                    "sent": "So when you design, keep in mind that the block has to complete.",
                    "label": 0
                },
                {
                    "sent": "If a block doesn't complete, it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "There is no migration, you can't move it at all.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The blocks must be independent.",
                    "label": 1
                },
                {
                    "sent": "We already tackled that a bit before.",
                    "label": 0
                },
                {
                    "sent": "Be careful, there's no.",
                    "label": 1
                },
                {
                    "sent": "They cannot synchronize, no luck.",
                    "label": 0
                },
                {
                    "sent": "So if you're used to parallel programming sometimes on a different architecture, that's where it gets tricky.",
                    "label": 0
                },
                {
                    "sent": "There is no synchronization, and this is essential to scalable.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm done questions so I won't answer that question, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Is there a meeting?",
                    "label": 0
                },
                {
                    "sent": "Please say that again.",
                    "label": 0
                },
                {
                    "sent": "The divided addresses that are leading to this time.",
                    "label": 0
                },
                {
                    "sent": "Is there a Wat meaning?",
                    "label": 0
                },
                {
                    "sent": "Sorry yes, because if you have 32, so we're going to see that that's it.",
                    "label": 0
                },
                {
                    "sent": "I mean, I see that that's the same question.",
                    "label": 0
                },
                {
                    "sent": "So is there a meaning to the block site in size ID it depends depending on how many threads and blocks your GPU can execute, and so you just want to size it so that it's a good combination between what your application has to do and what your GPU looks like.",
                    "label": 0
                },
                {
                    "sent": "So on purpose, the first question of the end Zone lab is for you to understand that meaning by sizing it.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's directly linked to hardware.",
                    "label": 0
                },
                {
                    "sent": "It's just pushing blocks that fits the size of your GPU.",
                    "label": 0
                },
                {
                    "sent": "Does it answer the question?",
                    "label": 0
                },
                {
                    "sent": "And here's a question.",
                    "label": 0
                },
                {
                    "sent": "OK, so your time to work.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK everybody, I only have 5 minutes left, so I'm going to be around during the break and keep on answering questions in you wish, but I want to wrap up with everybody.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A quick thing.",
                    "label": 0
                },
                {
                    "sent": "In this lab, we're running on Amazon.",
                    "label": 0
                },
                {
                    "sent": "On an Amazon instance.",
                    "label": 0
                },
                {
                    "sent": "But the software that is running on Amazon is the same everywhere it's available in nvidia.com, get CUDA, it's free and it works.",
                    "label": 1
                },
                {
                    "sent": "It works anywhere.",
                    "label": 0
                },
                {
                    "sent": "The reason I. I'm saying that is because I see a lot of people is like funny laptops.",
                    "label": 0
                },
                {
                    "sent": "As I said at the beginning and that's why I have a gaming laptop.",
                    "label": 0
                },
                {
                    "sent": "I don't play games very bad at it.",
                    "label": 0
                },
                {
                    "sent": "I get.",
                    "label": 0
                },
                {
                    "sent": "I destroy those people each time I start, but people look at my laptop like wow no.",
                    "label": 0
                },
                {
                    "sent": "So the only thing I do is my laptop is what you've seen today and so this is running locally on my laptop.",
                    "label": 0
                },
                {
                    "sent": "And it shows you what we ask you to do at the end of the lab, and it shows you the characteristics of the GPU that is in my laptop.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'm going to answer your first question here.",
                    "label": 0
                },
                {
                    "sent": "You can see.",
                    "label": 1
                },
                {
                    "sent": "Those numbers of the sizing of my GPU when I do what is called a device query, it's a sample which is provided with the CUDA software and it gives me the characteristic of my GPU.",
                    "label": 0
                },
                {
                    "sent": "So the GPU I have here is a GTX970M.",
                    "label": 0
                },
                {
                    "sent": "It's a Maxwell GM 200 for GPU, for laptops the maximum number of threads per block that I can have is 1024 and the maximum number of threads per multiprocessor is 2048.",
                    "label": 0
                },
                {
                    "sent": "And my web size as we discussed before is 32, so that's how you know what you can do an at the end of the lab.",
                    "label": 0
                },
                {
                    "sent": "They ask you to do the same.",
                    "label": 0
                },
                {
                    "sent": "The processor using you're going to see it slightly older generation, but that's what it is.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in trying if you have a laptop, it works on your laptop whether you have a Mac, Linux or Windows laptop.",
                    "label": 0
                },
                {
                    "sent": "If you have a GPU, yes.",
                    "label": 0
                },
                {
                    "sent": "So how many multiprocessors are there?",
                    "label": 0
                },
                {
                    "sent": "So here I have 1000.",
                    "label": 0
                },
                {
                    "sent": "280 could of course, so that makes that's how I can do it.",
                    "label": 0
                },
                {
                    "sent": "So there are less score is a member of right sided neck.",
                    "label": 0
                },
                {
                    "sent": "Yes, so the number of threads per multiprocessor, yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what I said.",
                    "label": 0
                },
                {
                    "sent": "That's the amount of memory when I'm running something that's where I can see whether it's running.",
                    "label": 0
                },
                {
                    "sent": "So I want you to have a look at this.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried to do that in the lab.",
                    "label": 0
                },
                {
                    "sent": "You can do that at the end of the lab.",
                    "label": 0
                },
                {
                    "sent": "It's you'll have the comments and everything to execute that.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you want to keep on trying so that as I said, that is going to be open for a few days and then we have a full app platform on Android at quicklab.com.",
                    "label": 0
                },
                {
                    "sent": "So if you register for that lab during that time, you'll get free credits to be able to do a lot of labs on that on that platform.",
                    "label": 0
                },
                {
                    "sent": "And if you have any questions, not always very responsive, but you're always welcome to send me an email and bug me until I reply, I usually do.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the last thing is I'm going to do like everybody.",
                    "label": 0
                },
                {
                    "sent": "I didn't have alot of time to show that we are hiring like most of the companies we're having a lot of fun.",
                    "label": 0
                },
                {
                    "sent": "We have a research full research program.",
                    "label": 0
                },
                {
                    "sent": "We have an applied research program.",
                    "label": 0
                },
                {
                    "sent": "We have a full automotive section.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of like different types of jobs including sales jobs or technical jobs.",
                    "label": 0
                },
                {
                    "sent": "Ann, you're welcome to talk to Ryan to me or pen is here too.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in joining NVIDIA, we're very happy to talk to you and.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Answer The questions during the break.",
                    "label": 0
                }
            ]
        }
    }
}