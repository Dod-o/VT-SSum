{
    "id": "ivekgau63647k25lyjjf2memoxxybne2",
    "title": "Introduction to the uRiKA Graphical Database System",
    "info": {
        "author": [
            "David Mizell, YarcData"
        ],
        "published": "Dec. 3, 2012",
        "recorded": "November 2012",
        "category": [
            "Top->Computer Science->Databases",
            "Top->Computer Science->Semantic Web->Applications"
        ]
    },
    "url": "http://videolectures.net/iswc2012_mizell_urika_system/",
    "segmentation": [
        [
            "Well, thanks everybody and good afternoon.",
            "Seems like I always get the job of keeping everybody awake after lunch.",
            "So I'm going to tell you about this system that we're market."
        ],
        [
            "Singing.",
            "Violating my own rule which is never to show an outline, but here's an outline.",
            "What what I'll tell you about what's this Eureka thing?",
            "What are we assuming about it going in and, and why is it different to the the the the secret sauce?",
            "Is the hardware that we're using?",
            "What's the software architecture?",
            "Well, is it fast?",
            "And what direction do we expect ahead with this technology?",
            "And who cares who might want to buy something like this?",
            "I'll try to confine it to 1/2 hour.",
            "You know it's the IBM speaker this morning.",
            "Got a 45 minute talk and I only get a half hour, but actually, that's logical 'cause it's a one to one mapping from our respective shares of the supercomputer market.",
            "Anyway, OK, here's the first question I get when I hit go to."
        ],
        [
            "Conference and they see my affiliation Cray.",
            "Are they still in business?",
            "Well, yeah, sorta Crazy research Inc. Was founded in 1972 by the famous guy, Seymour Cray.",
            "He left in the 90s, soon after that they were bought by Silicon Graphics, who own them from 96 to 2000, a period that the Cray Oldtimers referred to as the occupation.",
            "In 2000 they were sold to a small supercomputer startup in Seattle called Terra Computer Ontario changed its name to Crave the more recognizable name, and that's been in the arrangement ever since.",
            "At that time, Terra was developing this highly multithreaded supercomputer.",
            "When you're writing a novel, this is called foreshadowing.",
            "I'm going to.",
            "I'm going to go back to the multithreaded supercomputer later.",
            "But our main line product is big distributed memory."
        ],
        [
            "Here's here's a picture of 1.",
            "This is the Jaguar at Oakridge.",
            "Each one of these cabinets holds 64 processor boards, typically so there's only order of 40,000 processors.",
            "Quad Core processors in this system.",
            "When it was.",
            "First brought up in.",
            "I guess it was 2010 within a few weeks it had achieved sustained petaflops petaflops performance on several of their scientific applications, so.",
            "Pretty fast, it's right now being upgraded to.",
            "Cray XK7, which has NVIDIA GPU's on each board.",
            "So you've gotta now gotta vector unit on on on each board of the system and that will have a peak performance of 20 petaflops.",
            "And they've renamed that had to get rid of that cool cabinet painting there, 'cause they've renamed it the Titan.",
            "But anyway, that the other thing to say about it is I don't know how to program one of these.",
            "This is a distributed memory architecture and the classical way that super computer programmers program distributed memory machine is the message passing library called MPI.",
            "So you're using Fortran plus MPI or you're using C plus the MPI library."
        ],
        [
            "I work on a different box.",
            "Yep.",
            "I'm sorry.",
            "It's a shared nothing architecture that's correct.",
            "Eureka I didn't think of the acronym University already.",
            "If information, knowledge, appliance, choice of what letters get capitalized is left as an exercise.",
            "But basically it's this.",
            "It's a sparkle query engine in a Cray XMT.",
            "XMT is that multi threaded architecture that we've worked on quite awhile and it's the latest iteration of it.",
            "Think of it as standing for extremely multithreaded and I'll talk a little bit more about it.",
            "It uses for obvious reasons all of the infrastructure from the main product line, same cabinets, same interconnect, same cooling.",
            "And the custom multithreaded processor was designed to fit into an Opteron socket, so even the boards are pretty much the same as on the shared.",
            "Nothing mainline architecture, by the way.",
            "Anybody guess as to who?"
        ],
        [
            "These little hats are on top of the boxes.",
            "'s plumbing.",
            "This is a big hot machine.",
            "You have to work really hard to cool it.",
            "Jaguar not only simulates global warming, it contributes to it."
        ],
        [
            "OK, so what's your cdata?",
            "Well spelled the art part backwards.",
            "You'll get a hint.",
            "We spun off a subsidiary company to just focused on marketing this Eureka system.",
            "So let's go and talk about."
        ],
        [
            "Your ecosystem, and to do that first of all, I want to talk about.",
            "The assumptions that you and I might share, and some of them that we may not share.",
            "There's seems to be a fairly large gap between the assumptions that the semantic web academic community makes an what and the assumptions that we make inside the yard data company.",
            "We like sparkle.",
            "OK, it seems reasonable, seems useful.",
            "That's probably shared.",
            "RDF seems similarly, seems reasonable, useful, and it's nice that the RDF triples basically form source node, edge and sync mode of a component of a directed graph.",
            "What we may not share is that the RDF triples forms.",
            "Components of a directed graph is really all we care about.",
            "We don't care about the semantic web per say.",
            "We don't care about the Internet per say.",
            "We're making a fundamental assumption that the data is in our box.",
            "The.",
            "Data is in our box 'cause the customers that we have want to do fast queries against that data and the main aspect that they care about is that it's a directed graph.",
            "They want a graphic data database, a graphical database that they can submit queries against and get answers fast.",
            "So they want complex queries against a graph oriented database and they're willing to put all the data into one box in order to get it back.",
            "Get answers back fast.",
            "OK, that has some implications.",
            "One is well, we like start and sparkle as as a basis as a nice standard place to start with with tools that are associated with it.",
            "That makes things easier to work with, but we're not going to stop there.",
            "There are customers don't want us to.",
            "Also will do things beyond the what's available in the sparkle language.",
            "Kind of agnostic about inference and that may not be something that this Community shares.",
            "Some of our customers have ontologies and care about inference, others have none.",
            "OK, so."
        ],
        [
            "What about the hardware?",
            "Let's talk about the the the SMT hardware a little bit and this custom multithreaded processor while I do something like that.",
            "Well, the classical diagram processor speed is going up a lot faster than memory speed and forget about even trying to graph network speed on here."
        ],
        [
            "What do you do about that?",
            "Well, the classical commodity approach is you build a multicore chip, an you load it up with cash is you want to keep as much data as you can as close to the processor as you can and exploit the locality in the problem.",
            "By when you fetch one item, you get all of its neighbors because they're probably the next thing you're going to work on.",
            "There is the classical Cray approach, which is you build a vector machine, and then when you go fix data, you fetch a whole truckload of the data and hope that you'll work on that along time before you have to go go get the next batch of data so that you can answer, amortize the latency costs over.",
            "Lots of data, items that you work on, and finally, there's the latency tolerant approach that I'll describe later.",
            "Coming up where you multithread?",
            "So here."
        ],
        [
            "Is the idea?",
            "The XMT is processor."
        ],
        [
            "Has lots of hardware copies of the register set necessary to hold the context of a thread?",
            "Where you can think of as a conventional processor may hold one or a few sets of registers.",
            "The XMT has 128 copies of the general purpose registers.",
            "The stack pointer, the heap pointer, the program counter and the other registers that it needs to hold.",
            "The entire context of a thread.",
            "It has 128 copies of that hardware and the instruction execution hardware cycles between them.",
            "Every instruction cycle, finding the next one that's ready to issue an instruction into the pipeline.",
            "So what's the polling?",
            "Well, on a conventional processor, if you're fetching from memory, or, especially if you're fetching from a remote memory across the network.",
            "You're not going to do very many of those until your processor has to stall and wait for data to come back.",
            "It's like 100 to one cycles.",
            "That processor has to wait for something to get back from its local memory to get something across from remote memory is more like 1000 to one.",
            "It's like 1000 cycles.",
            "This thing has to wait until the fetch data arrives back here and it can work on it.",
            "In the multithreaded processor, you've got all of these copies of the hardware.",
            "Some of these threads are going to have to wait for their data to get back, but others will have something to do, and in particular what they'll have to do is issue more memory fetches.",
            "It keeps the processor busier, but who cares?",
            "Processors are free and supercomputer economics what you're paying for is the network and all of those memory chips that you're mine.",
            "So if you're keeping the processor busier and they're keeping it from stalling in particular, it's issuing lots more memory requests.",
            "Across the network.",
            "Now when you run an algorithm on a parallel computer.",
            "If you really tune that algorithm well.",
            "Getting going as fast as you can something is going to be the bottleneck.",
            "And whatever that is, if you can keep that model next saturated, you're doing the best you can do.",
            "Alright, well, what kind of algorithms have the characteristic that when you saturate the network, you're doing the best that you're going to do?"
        ],
        [
            "They have no locality or reference.",
            "There is no locality as in there usually isn't scientific computation that you can exploit by bringing all the data in its neighbors into into a cash.",
            "And the data is huge.",
            "If you put the database on your laptop, don't buy a Eureka box.",
            "It's only in that case where it has to be spread out across lots and lots of memory units that it makes sense to have this kind of an architecture.",
            "So no locality reference, but lots of parallelism in the problem.",
            "So you think about great big data structures?",
            "No locality of reference.",
            "And yeah, a lot of parallelism that almost defines graph problems and almost defines graph algorithms.",
            "So I say, well that that's the kind of thing that runs well on the XMT great big ugly graphs.",
            "You gotta ask by a marketing person.",
            "Is that a technical term?",
            "Great big ugly graph.",
            "So a little bit about."
        ],
        [
            "The software architecture.",
            "Here's what it looks like at present.",
            "So only Eureka box you've got perhaps a few hundred of the compute nodes.",
            "These custom multithreaded processors and you will also have a set of fairly standard looking Opteron blades running Linux, and they take care of IO.",
            "They take, they talk to the network, they run the compiler, and then they handle file IO.",
            "So they talked to the outside world in an unexpected way, and that's sort of typical of supercomputers.",
            "You strip as much operating system as you can out of the ones that are going to do the computation work, so that the OS just stays out of their way.",
            "But anyway, so we took advantage of this architecture in the following way.",
            "Sparkle query comes in.",
            "An on the front end on one of the service nodes we have a copy of Jenna, the open source Query Engine, an we cut out it's query engine.",
            "They are Q and we just keep the part that translates from sparkle into Sparkle algebra.",
            "So it translates it checks syntax and sends error messages, or if there is a a syntax error and translates the sparkle into sparkle.",
            "Algebra sins that across, and that's what drives the query engine.",
            "Then we send the results back across and they get displayed for the user.",
            "So the intent here was to be able to use more off the shelf tools then without having to do a lot of development on our part so that so that the.",
            "Nodes would have to be able to to talk to more of the outside world.",
            "This will change and in subsequent releases we're thinking we'll just.",
            "Parse have our own parser of sparkle because we're expecting to extend Sparkle and I'll talk a little bit more about that."
        ],
        [
            "OK.",
            "So SMT is a weird machine and it's somewhat difficult to learn how to program an for supercomputer people.",
            "It's quite different from what they usually got trained on, which was Fortran plus MPI or C plus simply MPI.",
            "It's not a message passing model at all, it's a big shared memory architecture.",
            "Physically, it's the same as any shared nothing architecture, but the hardware and that latency tolerance characteristic that it has.",
            "Allowed us to.",
            "Implement the hardware in such a way that it's one big uniform shared memory address space.",
            "So you write shared memory code and you think about it quite differently than you do an MPI code when you're thinking about MPI code, you're thinking about a whole lot of processing that's going to go on inside this box, and then once in awhile I'm going to send a message across when you're thinking about writing an XMT code, you're thinking about parallelism.",
            "It's very fine grain.",
            "It's like each iteration of a loop you think about being you sort of assume that it's being handled by a different thread and not only do you think about your algorithm, you think about the compiler because you've got this very powerful parallelizing compiler.",
            "That's also very idiosyncratic.",
            "And so you you learn to code in certain ways because you know the compiler compiles that kind of loop in a very efficient way, for example.",
            "Yeah, come home.",
            "Memory access.",
            "Well.",
            "It's fast, but we didn't repeal the spiel about speed of light.",
            "It's still gotten high latency, so the best you can do is have high bandwidth and Cray.",
            "That's what you're spending your millions on when you buy a Cray supercomputer or an IBM supercomputer.",
            "For that matter, you're you're buying a custom high bandwidth network.",
            "The program you still have to think that your memory is shared.",
            "So yeah, yeah, I definitely do.",
            "I don't care about how long the memory reference is going to take.",
            "I only care that I keep a lot of on the fly.",
            "So the only thing I really care about is I try to avoid an omdahl bottleneck where there's some sequential processing happening 'cause that costs me so much performance.",
            "OK, because we've got to, you know?",
            "Possibly terabytes of shared memory.",
            "For one thing, the databases memory resident, we convert all of the ur ice creams, YRI strings into integers in the in the usual fashion and then store those integers in memory.",
            "But the entire memory the entire database and its indexing data structures are memory resident.",
            "And we trade space.",
            "We've got lots of shared memory, so we trade space for time.",
            "Almost every choice we make.",
            "Lots of indexing.",
            "We hash the look ups and the comparisons we we do hash joins whenever.",
            "We're doing joints we know about how the compiler works, so we do things that the compiler is particularly good at, such as sweeping through a 1D array rather than a 2D array, and recurrences and reductions.",
            "It's implements.",
            "It generates code that's very efficient and parallel for in some cases.",
            "We're still learning on this stuff, but in some cases we are incorporating.",
            "Well known database optimizations try to do first the part of the query that produces the fewest results.",
            "So if you're joining with that, you're doing less work then.",
            "Then if you do the opposite.",
            "We do a lot of preprocessing.",
            "The conversion to energy or that I was telling you about our influencer that exists now is a forward inference or thanks.",
            "That we do it at preprocessing time and so on, and those tools that we do preprocessing with.",
            "Also run on the XMT compute nodes.",
            "Because they run in parallel or are much faster so we can do the preparation of a data and the translation of that data into the internal representation few minutes.",
            "Whereas we've seen comparable data from other systems that literally take hours.",
            "So.",
            "Well, is it fast?"
        ],
        [
            "This is where I'm going to annoy you because I didn't get permission to show you much performance data.",
            "This is the first time we've publicly shown any performance data, but it's only a tiny little peek inside the kimono, so I asked in advance for."
        ],
        [
            "Forgiveness, but here's what I've got.",
            "We hired some guys and gave him access to a shared nothing.",
            "Cray conventional supercomputer and then ran their custom code on it compared to the R Sparkle query engine on the Eureka box with the same number of processors.",
            "And we used Lubben query nine which Eric referred to this this morning.",
            "It's one of the loved one query and you guys all pronounce it loom.",
            "We didn't know anybody that shows you how wrong we are in a new to the 11 alright.",
            "Well, it works out because we thought Love Woman was too simple and benchmark?",
            "Yeah, well, that's that's what we invented.",
            "We developed and he thought it was too simple.",
            "So we decided we wrote a code that took all the students out of aluminum data set and sort of made a Facebook network between him and we use the arm at algorithm that Eric referred to so that the.",
            "The Facebook connections between the students had this power law characteristic, so some popular students had a huge number of edges coming in and out, and other students.",
            "The computer science majors only had only had a few edges coming out, and we call that leibham for level extended benchmark.",
            "So yeah, we did have lovingly.",
            "First Lumen leave him also worked.",
            "Questionable taste maybe anyway so.",
            "So the query 9 is the trying one of the triangular ones and love 'em where you look for a grad student that's taking the classes taught by his advisor.",
            "So you do it.",
            "You gotta do these joins to connect up all of the edges in the query and when you're on a distributed memory machine.",
            "The delays you hit when you're having to do lots and lots of remote references get in your way.",
            "Whereas even though the you're also doing remote memory references on your ecosystem, you're not stalling, so you're continuing to do lots and remote references an you finish the query a lot faster, and over here is a comparison with level query 14, which is a real simple query that blasts out a lot of results.",
            "I mean, it's just give me all the students or something really simplified like that so you you have a big IO that you've got to do.",
            "At the end of the query, because the result set is huge and this is just illustrating that the system has parallel file.",
            "Basically that's all I'm saying there, OK?"
        ],
        [
            "Here's the I think the really significant part.",
            "Came into use one."
        ],
        [
            "That's right, LBM 100K, a 100,000 universities.",
            "So it's like 25 billion triples."
        ],
        [
            "Here's what I think is really interesting.",
            "We scale like a bandit.",
            "If you scale up from level 8K to 25K to 100K.",
            "We stay pretty flat.",
            "Whereas when as you become overwhelmed by the Inter processor communication on a conventional distributed memory architecture, you're hurt worse and worse.",
            "So yeah, again, if you got a million triples, don't buy a Eureka box.",
            "If you got a billion, talk to us."
        ],
        [
            "OK, well So what else do we plan to do?",
            "What we're thinking about in 2013 with this product and it's in the hands of a few.",
            "I guess you could call him beta customers right now.",
            "There's a few customers that have the machine and are testing it and give us feedback.",
            "So.",
            "Sparkles pretty good at looking for a pattern in the data you showed me, his students taking class taught by a professor and he Co authored a paper with that professor.",
            "Another student in the same Department or something like that.",
            "You know?",
            "So elaborate pattern and the more joins you have to do, the better we look.",
            "What happens according to the guys that in my colleagues that talk to customers?",
            "I'm not allowed to talk to customers much 'cause management assumes that I would frighten him, 'cause I frightened management a lot.",
            "But anyway, the guys that do talk to customers say.",
            "That so many of the customers say sparkle sounds pretty good now.",
            "Can I do between the centrality on this?",
            "So they're asking for not only the sparkle pattern search, they're asking for classical graph algorithms, connected components, shortest path between the centrality clustering, etc, etc.",
            "They want that.",
            "So one of the things that we're thinking about to do in 2013 is extend Sparkle so that you might be able to do a sparkle query.",
            "Do a construct operator that creates the graph that you're interested in and then hand that up to a graph algorithm.",
            "So it performs and then analyzes that that newly created graph the way you want.",
            "Other things we'd like to workout.",
            "A way to do reification.",
            "Pretty fast security as the IBM speaker was mentioning we all we do now is materialization.",
            "We do forward inference, but some of the customers have been asking about dynamic inference, so that's kind of the direction we're heading.",
            "And."
        ],
        [
            "So and the final question I had on my original list was who cares and well, the market segments that we think are real promising.",
            "And this is this is kind of a new application area.",
            "People that want big graph oriented databases and the ones that seem to be showing interest.",
            "Well, there's there's interest in the Cyber security area.",
            "We have some classical intelligence agency customers whose whose data is just sort of naturally gets represented as a graph.",
            "Bad guy Excommunicates the bad guy.",
            "Why bad guy?",
            "Why flies to Berlin and so on?",
            "This is natural to represent these patterns as as a graph.",
            "Health and Human scientists as you guys are probably perfectly aware, the bioinformatics community has adopted sparkle in RDF very, very thoroughly.",
            "There's there's possible applications that some of the investment banks and other finance firms are interested in.",
            "Could you detect money laundering?",
            "That's sort of a pattern that you want to look for, and then there's a few things and just traditional high performance computing.",
            "For example, there's a.",
            "There's a research area in climate modeling called Teleconnections where Plaza places that are remote from each other geographically have a high correlation in their weather.",
            "Like every time you have a typhoon in Hong Kong, you have dust Storm in Topeka, KS or something like that.",
            "So that might build a graph of these.",
            "These places are just connected by high correlation and."
        ],
        [
            "I think that that should end the talk and we're hiring, and there's grad students in the room."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, thanks everybody and good afternoon.",
                    "label": 0
                },
                {
                    "sent": "Seems like I always get the job of keeping everybody awake after lunch.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to tell you about this system that we're market.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Singing.",
                    "label": 0
                },
                {
                    "sent": "Violating my own rule which is never to show an outline, but here's an outline.",
                    "label": 0
                },
                {
                    "sent": "What what I'll tell you about what's this Eureka thing?",
                    "label": 0
                },
                {
                    "sent": "What are we assuming about it going in and, and why is it different to the the the the secret sauce?",
                    "label": 1
                },
                {
                    "sent": "Is the hardware that we're using?",
                    "label": 0
                },
                {
                    "sent": "What's the software architecture?",
                    "label": 0
                },
                {
                    "sent": "Well, is it fast?",
                    "label": 0
                },
                {
                    "sent": "And what direction do we expect ahead with this technology?",
                    "label": 0
                },
                {
                    "sent": "And who cares who might want to buy something like this?",
                    "label": 0
                },
                {
                    "sent": "I'll try to confine it to 1/2 hour.",
                    "label": 0
                },
                {
                    "sent": "You know it's the IBM speaker this morning.",
                    "label": 0
                },
                {
                    "sent": "Got a 45 minute talk and I only get a half hour, but actually, that's logical 'cause it's a one to one mapping from our respective shares of the supercomputer market.",
                    "label": 0
                },
                {
                    "sent": "Anyway, OK, here's the first question I get when I hit go to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conference and they see my affiliation Cray.",
                    "label": 0
                },
                {
                    "sent": "Are they still in business?",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, sorta Crazy research Inc. Was founded in 1972 by the famous guy, Seymour Cray.",
                    "label": 1
                },
                {
                    "sent": "He left in the 90s, soon after that they were bought by Silicon Graphics, who own them from 96 to 2000, a period that the Cray Oldtimers referred to as the occupation.",
                    "label": 1
                },
                {
                    "sent": "In 2000 they were sold to a small supercomputer startup in Seattle called Terra Computer Ontario changed its name to Crave the more recognizable name, and that's been in the arrangement ever since.",
                    "label": 0
                },
                {
                    "sent": "At that time, Terra was developing this highly multithreaded supercomputer.",
                    "label": 0
                },
                {
                    "sent": "When you're writing a novel, this is called foreshadowing.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go back to the multithreaded supercomputer later.",
                    "label": 0
                },
                {
                    "sent": "But our main line product is big distributed memory.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's here's a picture of 1.",
                    "label": 0
                },
                {
                    "sent": "This is the Jaguar at Oakridge.",
                    "label": 0
                },
                {
                    "sent": "Each one of these cabinets holds 64 processor boards, typically so there's only order of 40,000 processors.",
                    "label": 0
                },
                {
                    "sent": "Quad Core processors in this system.",
                    "label": 0
                },
                {
                    "sent": "When it was.",
                    "label": 0
                },
                {
                    "sent": "First brought up in.",
                    "label": 0
                },
                {
                    "sent": "I guess it was 2010 within a few weeks it had achieved sustained petaflops petaflops performance on several of their scientific applications, so.",
                    "label": 0
                },
                {
                    "sent": "Pretty fast, it's right now being upgraded to.",
                    "label": 0
                },
                {
                    "sent": "Cray XK7, which has NVIDIA GPU's on each board.",
                    "label": 0
                },
                {
                    "sent": "So you've gotta now gotta vector unit on on on each board of the system and that will have a peak performance of 20 petaflops.",
                    "label": 0
                },
                {
                    "sent": "And they've renamed that had to get rid of that cool cabinet painting there, 'cause they've renamed it the Titan.",
                    "label": 0
                },
                {
                    "sent": "But anyway, that the other thing to say about it is I don't know how to program one of these.",
                    "label": 0
                },
                {
                    "sent": "This is a distributed memory architecture and the classical way that super computer programmers program distributed memory machine is the message passing library called MPI.",
                    "label": 0
                },
                {
                    "sent": "So you're using Fortran plus MPI or you're using C plus the MPI library.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I work on a different box.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "It's a shared nothing architecture that's correct.",
                    "label": 0
                },
                {
                    "sent": "Eureka I didn't think of the acronym University already.",
                    "label": 0
                },
                {
                    "sent": "If information, knowledge, appliance, choice of what letters get capitalized is left as an exercise.",
                    "label": 1
                },
                {
                    "sent": "But basically it's this.",
                    "label": 0
                },
                {
                    "sent": "It's a sparkle query engine in a Cray XMT.",
                    "label": 1
                },
                {
                    "sent": "XMT is that multi threaded architecture that we've worked on quite awhile and it's the latest iteration of it.",
                    "label": 0
                },
                {
                    "sent": "Think of it as standing for extremely multithreaded and I'll talk a little bit more about it.",
                    "label": 0
                },
                {
                    "sent": "It uses for obvious reasons all of the infrastructure from the main product line, same cabinets, same interconnect, same cooling.",
                    "label": 0
                },
                {
                    "sent": "And the custom multithreaded processor was designed to fit into an Opteron socket, so even the boards are pretty much the same as on the shared.",
                    "label": 0
                },
                {
                    "sent": "Nothing mainline architecture, by the way.",
                    "label": 0
                },
                {
                    "sent": "Anybody guess as to who?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These little hats are on top of the boxes.",
                    "label": 0
                },
                {
                    "sent": "'s plumbing.",
                    "label": 0
                },
                {
                    "sent": "This is a big hot machine.",
                    "label": 0
                },
                {
                    "sent": "You have to work really hard to cool it.",
                    "label": 0
                },
                {
                    "sent": "Jaguar not only simulates global warming, it contributes to it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so what's your cdata?",
                    "label": 0
                },
                {
                    "sent": "Well spelled the art part backwards.",
                    "label": 0
                },
                {
                    "sent": "You'll get a hint.",
                    "label": 0
                },
                {
                    "sent": "We spun off a subsidiary company to just focused on marketing this Eureka system.",
                    "label": 1
                },
                {
                    "sent": "So let's go and talk about.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Your ecosystem, and to do that first of all, I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "The assumptions that you and I might share, and some of them that we may not share.",
                    "label": 1
                },
                {
                    "sent": "There's seems to be a fairly large gap between the assumptions that the semantic web academic community makes an what and the assumptions that we make inside the yard data company.",
                    "label": 0
                },
                {
                    "sent": "We like sparkle.",
                    "label": 0
                },
                {
                    "sent": "OK, it seems reasonable, seems useful.",
                    "label": 0
                },
                {
                    "sent": "That's probably shared.",
                    "label": 0
                },
                {
                    "sent": "RDF seems similarly, seems reasonable, useful, and it's nice that the RDF triples basically form source node, edge and sync mode of a component of a directed graph.",
                    "label": 0
                },
                {
                    "sent": "What we may not share is that the RDF triples forms.",
                    "label": 1
                },
                {
                    "sent": "Components of a directed graph is really all we care about.",
                    "label": 1
                },
                {
                    "sent": "We don't care about the semantic web per say.",
                    "label": 0
                },
                {
                    "sent": "We don't care about the Internet per say.",
                    "label": 0
                },
                {
                    "sent": "We're making a fundamental assumption that the data is in our box.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Data is in our box 'cause the customers that we have want to do fast queries against that data and the main aspect that they care about is that it's a directed graph.",
                    "label": 1
                },
                {
                    "sent": "They want a graphic data database, a graphical database that they can submit queries against and get answers fast.",
                    "label": 0
                },
                {
                    "sent": "So they want complex queries against a graph oriented database and they're willing to put all the data into one box in order to get it back.",
                    "label": 0
                },
                {
                    "sent": "Get answers back fast.",
                    "label": 0
                },
                {
                    "sent": "OK, that has some implications.",
                    "label": 0
                },
                {
                    "sent": "One is well, we like start and sparkle as as a basis as a nice standard place to start with with tools that are associated with it.",
                    "label": 0
                },
                {
                    "sent": "That makes things easier to work with, but we're not going to stop there.",
                    "label": 0
                },
                {
                    "sent": "There are customers don't want us to.",
                    "label": 0
                },
                {
                    "sent": "Also will do things beyond the what's available in the sparkle language.",
                    "label": 1
                },
                {
                    "sent": "Kind of agnostic about inference and that may not be something that this Community shares.",
                    "label": 0
                },
                {
                    "sent": "Some of our customers have ontologies and care about inference, others have none.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What about the hardware?",
                    "label": 0
                },
                {
                    "sent": "Let's talk about the the the SMT hardware a little bit and this custom multithreaded processor while I do something like that.",
                    "label": 0
                },
                {
                    "sent": "Well, the classical diagram processor speed is going up a lot faster than memory speed and forget about even trying to graph network speed on here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do you do about that?",
                    "label": 0
                },
                {
                    "sent": "Well, the classical commodity approach is you build a multicore chip, an you load it up with cash is you want to keep as much data as you can as close to the processor as you can and exploit the locality in the problem.",
                    "label": 0
                },
                {
                    "sent": "By when you fetch one item, you get all of its neighbors because they're probably the next thing you're going to work on.",
                    "label": 0
                },
                {
                    "sent": "There is the classical Cray approach, which is you build a vector machine, and then when you go fix data, you fetch a whole truckload of the data and hope that you'll work on that along time before you have to go go get the next batch of data so that you can answer, amortize the latency costs over.",
                    "label": 0
                },
                {
                    "sent": "Lots of data, items that you work on, and finally, there's the latency tolerant approach that I'll describe later.",
                    "label": 0
                },
                {
                    "sent": "Coming up where you multithread?",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the idea?",
                    "label": 0
                },
                {
                    "sent": "The XMT is processor.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has lots of hardware copies of the register set necessary to hold the context of a thread?",
                    "label": 0
                },
                {
                    "sent": "Where you can think of as a conventional processor may hold one or a few sets of registers.",
                    "label": 1
                },
                {
                    "sent": "The XMT has 128 copies of the general purpose registers.",
                    "label": 0
                },
                {
                    "sent": "The stack pointer, the heap pointer, the program counter and the other registers that it needs to hold.",
                    "label": 0
                },
                {
                    "sent": "The entire context of a thread.",
                    "label": 0
                },
                {
                    "sent": "It has 128 copies of that hardware and the instruction execution hardware cycles between them.",
                    "label": 0
                },
                {
                    "sent": "Every instruction cycle, finding the next one that's ready to issue an instruction into the pipeline.",
                    "label": 0
                },
                {
                    "sent": "So what's the polling?",
                    "label": 0
                },
                {
                    "sent": "Well, on a conventional processor, if you're fetching from memory, or, especially if you're fetching from a remote memory across the network.",
                    "label": 0
                },
                {
                    "sent": "You're not going to do very many of those until your processor has to stall and wait for data to come back.",
                    "label": 0
                },
                {
                    "sent": "It's like 100 to one cycles.",
                    "label": 0
                },
                {
                    "sent": "That processor has to wait for something to get back from its local memory to get something across from remote memory is more like 1000 to one.",
                    "label": 0
                },
                {
                    "sent": "It's like 1000 cycles.",
                    "label": 0
                },
                {
                    "sent": "This thing has to wait until the fetch data arrives back here and it can work on it.",
                    "label": 0
                },
                {
                    "sent": "In the multithreaded processor, you've got all of these copies of the hardware.",
                    "label": 0
                },
                {
                    "sent": "Some of these threads are going to have to wait for their data to get back, but others will have something to do, and in particular what they'll have to do is issue more memory fetches.",
                    "label": 0
                },
                {
                    "sent": "It keeps the processor busier, but who cares?",
                    "label": 0
                },
                {
                    "sent": "Processors are free and supercomputer economics what you're paying for is the network and all of those memory chips that you're mine.",
                    "label": 0
                },
                {
                    "sent": "So if you're keeping the processor busier and they're keeping it from stalling in particular, it's issuing lots more memory requests.",
                    "label": 0
                },
                {
                    "sent": "Across the network.",
                    "label": 0
                },
                {
                    "sent": "Now when you run an algorithm on a parallel computer.",
                    "label": 0
                },
                {
                    "sent": "If you really tune that algorithm well.",
                    "label": 0
                },
                {
                    "sent": "Getting going as fast as you can something is going to be the bottleneck.",
                    "label": 0
                },
                {
                    "sent": "And whatever that is, if you can keep that model next saturated, you're doing the best you can do.",
                    "label": 0
                },
                {
                    "sent": "Alright, well, what kind of algorithms have the characteristic that when you saturate the network, you're doing the best that you're going to do?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They have no locality or reference.",
                    "label": 0
                },
                {
                    "sent": "There is no locality as in there usually isn't scientific computation that you can exploit by bringing all the data in its neighbors into into a cash.",
                    "label": 0
                },
                {
                    "sent": "And the data is huge.",
                    "label": 0
                },
                {
                    "sent": "If you put the database on your laptop, don't buy a Eureka box.",
                    "label": 0
                },
                {
                    "sent": "It's only in that case where it has to be spread out across lots and lots of memory units that it makes sense to have this kind of an architecture.",
                    "label": 0
                },
                {
                    "sent": "So no locality reference, but lots of parallelism in the problem.",
                    "label": 1
                },
                {
                    "sent": "So you think about great big data structures?",
                    "label": 0
                },
                {
                    "sent": "No locality of reference.",
                    "label": 0
                },
                {
                    "sent": "And yeah, a lot of parallelism that almost defines graph problems and almost defines graph algorithms.",
                    "label": 0
                },
                {
                    "sent": "So I say, well that that's the kind of thing that runs well on the XMT great big ugly graphs.",
                    "label": 0
                },
                {
                    "sent": "You gotta ask by a marketing person.",
                    "label": 0
                },
                {
                    "sent": "Is that a technical term?",
                    "label": 0
                },
                {
                    "sent": "Great big ugly graph.",
                    "label": 0
                },
                {
                    "sent": "So a little bit about.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The software architecture.",
                    "label": 0
                },
                {
                    "sent": "Here's what it looks like at present.",
                    "label": 0
                },
                {
                    "sent": "So only Eureka box you've got perhaps a few hundred of the compute nodes.",
                    "label": 1
                },
                {
                    "sent": "These custom multithreaded processors and you will also have a set of fairly standard looking Opteron blades running Linux, and they take care of IO.",
                    "label": 0
                },
                {
                    "sent": "They take, they talk to the network, they run the compiler, and then they handle file IO.",
                    "label": 0
                },
                {
                    "sent": "So they talked to the outside world in an unexpected way, and that's sort of typical of supercomputers.",
                    "label": 0
                },
                {
                    "sent": "You strip as much operating system as you can out of the ones that are going to do the computation work, so that the OS just stays out of their way.",
                    "label": 0
                },
                {
                    "sent": "But anyway, so we took advantage of this architecture in the following way.",
                    "label": 0
                },
                {
                    "sent": "Sparkle query comes in.",
                    "label": 0
                },
                {
                    "sent": "An on the front end on one of the service nodes we have a copy of Jenna, the open source Query Engine, an we cut out it's query engine.",
                    "label": 1
                },
                {
                    "sent": "They are Q and we just keep the part that translates from sparkle into Sparkle algebra.",
                    "label": 0
                },
                {
                    "sent": "So it translates it checks syntax and sends error messages, or if there is a a syntax error and translates the sparkle into sparkle.",
                    "label": 0
                },
                {
                    "sent": "Algebra sins that across, and that's what drives the query engine.",
                    "label": 0
                },
                {
                    "sent": "Then we send the results back across and they get displayed for the user.",
                    "label": 0
                },
                {
                    "sent": "So the intent here was to be able to use more off the shelf tools then without having to do a lot of development on our part so that so that the.",
                    "label": 0
                },
                {
                    "sent": "Nodes would have to be able to to talk to more of the outside world.",
                    "label": 0
                },
                {
                    "sent": "This will change and in subsequent releases we're thinking we'll just.",
                    "label": 0
                },
                {
                    "sent": "Parse have our own parser of sparkle because we're expecting to extend Sparkle and I'll talk a little bit more about that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So SMT is a weird machine and it's somewhat difficult to learn how to program an for supercomputer people.",
                    "label": 0
                },
                {
                    "sent": "It's quite different from what they usually got trained on, which was Fortran plus MPI or C plus simply MPI.",
                    "label": 0
                },
                {
                    "sent": "It's not a message passing model at all, it's a big shared memory architecture.",
                    "label": 0
                },
                {
                    "sent": "Physically, it's the same as any shared nothing architecture, but the hardware and that latency tolerance characteristic that it has.",
                    "label": 0
                },
                {
                    "sent": "Allowed us to.",
                    "label": 0
                },
                {
                    "sent": "Implement the hardware in such a way that it's one big uniform shared memory address space.",
                    "label": 1
                },
                {
                    "sent": "So you write shared memory code and you think about it quite differently than you do an MPI code when you're thinking about MPI code, you're thinking about a whole lot of processing that's going to go on inside this box, and then once in awhile I'm going to send a message across when you're thinking about writing an XMT code, you're thinking about parallelism.",
                    "label": 0
                },
                {
                    "sent": "It's very fine grain.",
                    "label": 0
                },
                {
                    "sent": "It's like each iteration of a loop you think about being you sort of assume that it's being handled by a different thread and not only do you think about your algorithm, you think about the compiler because you've got this very powerful parallelizing compiler.",
                    "label": 0
                },
                {
                    "sent": "That's also very idiosyncratic.",
                    "label": 0
                },
                {
                    "sent": "And so you you learn to code in certain ways because you know the compiler compiles that kind of loop in a very efficient way, for example.",
                    "label": 0
                },
                {
                    "sent": "Yeah, come home.",
                    "label": 0
                },
                {
                    "sent": "Memory access.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "It's fast, but we didn't repeal the spiel about speed of light.",
                    "label": 0
                },
                {
                    "sent": "It's still gotten high latency, so the best you can do is have high bandwidth and Cray.",
                    "label": 0
                },
                {
                    "sent": "That's what you're spending your millions on when you buy a Cray supercomputer or an IBM supercomputer.",
                    "label": 0
                },
                {
                    "sent": "For that matter, you're you're buying a custom high bandwidth network.",
                    "label": 0
                },
                {
                    "sent": "The program you still have to think that your memory is shared.",
                    "label": 0
                },
                {
                    "sent": "So yeah, yeah, I definitely do.",
                    "label": 0
                },
                {
                    "sent": "I don't care about how long the memory reference is going to take.",
                    "label": 0
                },
                {
                    "sent": "I only care that I keep a lot of on the fly.",
                    "label": 0
                },
                {
                    "sent": "So the only thing I really care about is I try to avoid an omdahl bottleneck where there's some sequential processing happening 'cause that costs me so much performance.",
                    "label": 0
                },
                {
                    "sent": "OK, because we've got to, you know?",
                    "label": 0
                },
                {
                    "sent": "Possibly terabytes of shared memory.",
                    "label": 0
                },
                {
                    "sent": "For one thing, the databases memory resident, we convert all of the ur ice creams, YRI strings into integers in the in the usual fashion and then store those integers in memory.",
                    "label": 0
                },
                {
                    "sent": "But the entire memory the entire database and its indexing data structures are memory resident.",
                    "label": 0
                },
                {
                    "sent": "And we trade space.",
                    "label": 1
                },
                {
                    "sent": "We've got lots of shared memory, so we trade space for time.",
                    "label": 1
                },
                {
                    "sent": "Almost every choice we make.",
                    "label": 0
                },
                {
                    "sent": "Lots of indexing.",
                    "label": 0
                },
                {
                    "sent": "We hash the look ups and the comparisons we we do hash joins whenever.",
                    "label": 1
                },
                {
                    "sent": "We're doing joints we know about how the compiler works, so we do things that the compiler is particularly good at, such as sweeping through a 1D array rather than a 2D array, and recurrences and reductions.",
                    "label": 0
                },
                {
                    "sent": "It's implements.",
                    "label": 0
                },
                {
                    "sent": "It generates code that's very efficient and parallel for in some cases.",
                    "label": 0
                },
                {
                    "sent": "We're still learning on this stuff, but in some cases we are incorporating.",
                    "label": 1
                },
                {
                    "sent": "Well known database optimizations try to do first the part of the query that produces the fewest results.",
                    "label": 0
                },
                {
                    "sent": "So if you're joining with that, you're doing less work then.",
                    "label": 0
                },
                {
                    "sent": "Then if you do the opposite.",
                    "label": 0
                },
                {
                    "sent": "We do a lot of preprocessing.",
                    "label": 0
                },
                {
                    "sent": "The conversion to energy or that I was telling you about our influencer that exists now is a forward inference or thanks.",
                    "label": 0
                },
                {
                    "sent": "That we do it at preprocessing time and so on, and those tools that we do preprocessing with.",
                    "label": 0
                },
                {
                    "sent": "Also run on the XMT compute nodes.",
                    "label": 0
                },
                {
                    "sent": "Because they run in parallel or are much faster so we can do the preparation of a data and the translation of that data into the internal representation few minutes.",
                    "label": 0
                },
                {
                    "sent": "Whereas we've seen comparable data from other systems that literally take hours.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, is it fast?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is where I'm going to annoy you because I didn't get permission to show you much performance data.",
                    "label": 0
                },
                {
                    "sent": "This is the first time we've publicly shown any performance data, but it's only a tiny little peek inside the kimono, so I asked in advance for.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Forgiveness, but here's what I've got.",
                    "label": 0
                },
                {
                    "sent": "We hired some guys and gave him access to a shared nothing.",
                    "label": 0
                },
                {
                    "sent": "Cray conventional supercomputer and then ran their custom code on it compared to the R Sparkle query engine on the Eureka box with the same number of processors.",
                    "label": 0
                },
                {
                    "sent": "And we used Lubben query nine which Eric referred to this this morning.",
                    "label": 0
                },
                {
                    "sent": "It's one of the loved one query and you guys all pronounce it loom.",
                    "label": 0
                },
                {
                    "sent": "We didn't know anybody that shows you how wrong we are in a new to the 11 alright.",
                    "label": 0
                },
                {
                    "sent": "Well, it works out because we thought Love Woman was too simple and benchmark?",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, that's that's what we invented.",
                    "label": 0
                },
                {
                    "sent": "We developed and he thought it was too simple.",
                    "label": 0
                },
                {
                    "sent": "So we decided we wrote a code that took all the students out of aluminum data set and sort of made a Facebook network between him and we use the arm at algorithm that Eric referred to so that the.",
                    "label": 0
                },
                {
                    "sent": "The Facebook connections between the students had this power law characteristic, so some popular students had a huge number of edges coming in and out, and other students.",
                    "label": 0
                },
                {
                    "sent": "The computer science majors only had only had a few edges coming out, and we call that leibham for level extended benchmark.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we did have lovingly.",
                    "label": 0
                },
                {
                    "sent": "First Lumen leave him also worked.",
                    "label": 0
                },
                {
                    "sent": "Questionable taste maybe anyway so.",
                    "label": 0
                },
                {
                    "sent": "So the query 9 is the trying one of the triangular ones and love 'em where you look for a grad student that's taking the classes taught by his advisor.",
                    "label": 0
                },
                {
                    "sent": "So you do it.",
                    "label": 0
                },
                {
                    "sent": "You gotta do these joins to connect up all of the edges in the query and when you're on a distributed memory machine.",
                    "label": 0
                },
                {
                    "sent": "The delays you hit when you're having to do lots and lots of remote references get in your way.",
                    "label": 0
                },
                {
                    "sent": "Whereas even though the you're also doing remote memory references on your ecosystem, you're not stalling, so you're continuing to do lots and remote references an you finish the query a lot faster, and over here is a comparison with level query 14, which is a real simple query that blasts out a lot of results.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just give me all the students or something really simplified like that so you you have a big IO that you've got to do.",
                    "label": 0
                },
                {
                    "sent": "At the end of the query, because the result set is huge and this is just illustrating that the system has parallel file.",
                    "label": 0
                },
                {
                    "sent": "Basically that's all I'm saying there, OK?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the I think the really significant part.",
                    "label": 0
                },
                {
                    "sent": "Came into use one.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's right, LBM 100K, a 100,000 universities.",
                    "label": 0
                },
                {
                    "sent": "So it's like 25 billion triples.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's what I think is really interesting.",
                    "label": 0
                },
                {
                    "sent": "We scale like a bandit.",
                    "label": 0
                },
                {
                    "sent": "If you scale up from level 8K to 25K to 100K.",
                    "label": 0
                },
                {
                    "sent": "We stay pretty flat.",
                    "label": 0
                },
                {
                    "sent": "Whereas when as you become overwhelmed by the Inter processor communication on a conventional distributed memory architecture, you're hurt worse and worse.",
                    "label": 0
                },
                {
                    "sent": "So yeah, again, if you got a million triples, don't buy a Eureka box.",
                    "label": 0
                },
                {
                    "sent": "If you got a billion, talk to us.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, well So what else do we plan to do?",
                    "label": 0
                },
                {
                    "sent": "What we're thinking about in 2013 with this product and it's in the hands of a few.",
                    "label": 0
                },
                {
                    "sent": "I guess you could call him beta customers right now.",
                    "label": 0
                },
                {
                    "sent": "There's a few customers that have the machine and are testing it and give us feedback.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Sparkles pretty good at looking for a pattern in the data you showed me, his students taking class taught by a professor and he Co authored a paper with that professor.",
                    "label": 1
                },
                {
                    "sent": "Another student in the same Department or something like that.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 1
                },
                {
                    "sent": "So elaborate pattern and the more joins you have to do, the better we look.",
                    "label": 0
                },
                {
                    "sent": "What happens according to the guys that in my colleagues that talk to customers?",
                    "label": 0
                },
                {
                    "sent": "I'm not allowed to talk to customers much 'cause management assumes that I would frighten him, 'cause I frightened management a lot.",
                    "label": 0
                },
                {
                    "sent": "But anyway, the guys that do talk to customers say.",
                    "label": 0
                },
                {
                    "sent": "That so many of the customers say sparkle sounds pretty good now.",
                    "label": 0
                },
                {
                    "sent": "Can I do between the centrality on this?",
                    "label": 0
                },
                {
                    "sent": "So they're asking for not only the sparkle pattern search, they're asking for classical graph algorithms, connected components, shortest path between the centrality clustering, etc, etc.",
                    "label": 1
                },
                {
                    "sent": "They want that.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that we're thinking about to do in 2013 is extend Sparkle so that you might be able to do a sparkle query.",
                    "label": 0
                },
                {
                    "sent": "Do a construct operator that creates the graph that you're interested in and then hand that up to a graph algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it performs and then analyzes that that newly created graph the way you want.",
                    "label": 1
                },
                {
                    "sent": "Other things we'd like to workout.",
                    "label": 1
                },
                {
                    "sent": "A way to do reification.",
                    "label": 0
                },
                {
                    "sent": "Pretty fast security as the IBM speaker was mentioning we all we do now is materialization.",
                    "label": 0
                },
                {
                    "sent": "We do forward inference, but some of the customers have been asking about dynamic inference, so that's kind of the direction we're heading.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So and the final question I had on my original list was who cares and well, the market segments that we think are real promising.",
                    "label": 0
                },
                {
                    "sent": "And this is this is kind of a new application area.",
                    "label": 0
                },
                {
                    "sent": "People that want big graph oriented databases and the ones that seem to be showing interest.",
                    "label": 0
                },
                {
                    "sent": "Well, there's there's interest in the Cyber security area.",
                    "label": 0
                },
                {
                    "sent": "We have some classical intelligence agency customers whose whose data is just sort of naturally gets represented as a graph.",
                    "label": 0
                },
                {
                    "sent": "Bad guy Excommunicates the bad guy.",
                    "label": 0
                },
                {
                    "sent": "Why bad guy?",
                    "label": 0
                },
                {
                    "sent": "Why flies to Berlin and so on?",
                    "label": 0
                },
                {
                    "sent": "This is natural to represent these patterns as as a graph.",
                    "label": 0
                },
                {
                    "sent": "Health and Human scientists as you guys are probably perfectly aware, the bioinformatics community has adopted sparkle in RDF very, very thoroughly.",
                    "label": 1
                },
                {
                    "sent": "There's there's possible applications that some of the investment banks and other finance firms are interested in.",
                    "label": 1
                },
                {
                    "sent": "Could you detect money laundering?",
                    "label": 0
                },
                {
                    "sent": "That's sort of a pattern that you want to look for, and then there's a few things and just traditional high performance computing.",
                    "label": 0
                },
                {
                    "sent": "For example, there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a research area in climate modeling called Teleconnections where Plaza places that are remote from each other geographically have a high correlation in their weather.",
                    "label": 0
                },
                {
                    "sent": "Like every time you have a typhoon in Hong Kong, you have dust Storm in Topeka, KS or something like that.",
                    "label": 0
                },
                {
                    "sent": "So that might build a graph of these.",
                    "label": 0
                },
                {
                    "sent": "These places are just connected by high correlation and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that that should end the talk and we're hiring, and there's grad students in the room.",
                    "label": 0
                }
            ]
        }
    }
}