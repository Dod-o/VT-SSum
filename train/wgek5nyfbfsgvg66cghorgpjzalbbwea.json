{
    "id": "wgek5nyfbfsgvg66cghorgpjzalbbwea",
    "title": "Generative Models for Relational Structures",
    "info": {
        "author": [
            "Edwin Hancock, Department of Computer Science, University of York"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_hancock_gmr/",
    "segmentation": [
        [
            "Within Hannon Richard Wilson on learning generative models of graphs and within the Sinbad context graphs represent representations of dissimilarity or structure.",
            "Arrangements of objects, and in this work we have been attempting to do is to try to cast the problem of learning models of how such representations distribute themselves in a generative probabilistic sense.",
            "So this work is of course supported by Sinbad and my work is supported as part of our society.",
            "Wilson Merit award"
        ],
        [
            "So graph represent representations of data, particularly computer vision.",
            "Have a number of advantages and disadvantages, advantages that they can capture object or scene structure in a manner that's invariant to changes in viewpoint, and they can abstract scene contents in an efficient way.",
            "The disadvantages, of course, are that they can be extremely fragile.",
            "They're sensitive to noise and segmentation error, and that if we tried to deal with with graphs as patterns, then the.",
            "Machine learning methodology available for processing them is very limited."
        ],
        [
            "So if we were thinking of of trying to to learn on sets of graphs, then there are a number of problems of increasing complexity that we might attempt to tackle.",
            "The first of these fools very naturally into the into the framework of Sinbad, and that is that we could try to cluster similar objects and represent them using class prototype.",
            "For instance a median using similarity or dissimilarity data.",
            "Now the problem with this approach is it simply tells you about the distributions of the graphs within the patent space doesn't tell you anything about the sources of structural variation which may occur in a class of graphs, so a slightly more sort of sophisticated approach which begins to go in that direction would be to extract features and then perform central clustering, when if you really want to do a proper job of learning the how the structural variations.",
            "In graph distribute themselves then what you have to do is to construct a generative model on a set of probability distributions to capture the modes of variation."
        ],
        [
            "So let's just give it some sort of examples of when this is simple and when this is difficult.",
            "So for instance, here I've generated a sequence of synthetic grafts from the corners in an image of the House which rotates with respect to the camera and underneath what I've done is I've shown the Delaunay triangulation of the feature points extracted from that graph, and this is a very contrived scene.",
            "First of all, I know the order of the feature points or their labels.",
            "And no feature point appears or disappears in the sequence."
        ],
        [
            "So if I then take the adjacency matrices for those graphs, stack limit along vectors in the order of the feature labels on the nodes and then perform PCA on those long vectors representing the adjacency matrices, then what happens is as I traverse the series of synthetic images corresponding to the different viewpoints of those houses.",
            "Then the graphs trace out continuous trajectory in the principal component space.",
            "So that's as ideal really as it could get.",
            "No, no nodes appear or disappear, and we know the correspondences or labels on the nodes."
        ],
        [
            "But this is actually what it looks like in practice.",
            "If you try to do this, we are using the sorts of feature detectors that are common in computer vision.",
            "So here a sequence of images of a toy house connected with the cameras camera pans on the object feature points extract extracted using a corner detector and then delivered a triangulated in the same way as a synthetic example and you see what happens is that the not only do the structure of the graph change.",
            "From viewpoint viewpoint, but actually have nodes appear and disappear due to a number of causes.",
            "For instance, occlusion would also false positives and false negatives delivered by the corner detector.",
            "So the simple scenario I pointed out on the previous slide just isn't workable."
        ],
        [
            "OK, So what I've done is I've sort of listed here.",
            "The problems if we're going to use graph structures to represent the proximity, proximity, or similarity of features in computer vision problems, then the algorithms that we're going to have to use to detect the feature points are going to be inherently I'm."
        ],
        [
            "Reliable and so.",
            "As a result, they're going to be both additional missing nodes due to segmentation error, and in addition to genuine variations in edge structure."
        ],
        [
            "Graphs and so image matching and recognition cannot be reduced to graph graph isomorphism or even subgraph isomorphism.",
            "Instead, inexact graph matching techniques have to be used, and things and computationally very."
        ],
        [
            "Pensive OK so this is really just a summary of why that analysis with graphs is difficult and the take home messages is a graphs.",
            "Don't reside naturally in a vector space because we need a correspondence order between the nodes and that and as a result it's not easy to compute statistical summaries which can be used to capture variations in structure such as the mean variance of a set of graphs."
        ],
        [
            "This list is this slide here really sort of reiterates the point I made earlier on about the sort of problems you might try to attack using learning in the graph domain and what I'm trying to do in this talk is really to focus in some detail on the last of these problems where I want to construct a generative model that can capture variations in graph structure over a set of exemplars.",
            "And in order to tackle this problem, what I need to do is is to establish correspondences between nodes.",
            "A set of graphs which I'm trying to analyze, and then have a means of characterizing variation in both node and edge structure in that family of graphs."
        ],
        [
            "So.",
            "We're not coming at this problem.",
            "Cold Turkey prior to December we had a number of attempts that are trying to remove threat list of topics that I showed you on the previous slide and so really the first piece of work with Antonio Robles Kelly and Andrea Torsello was to explore using pairwise clustering techniques to discover the class structure in sets of graphs using pairwise similarities.",
            "With with bin Laden, Richard Wilson, we did some work on graph clustering using graph spectral features and then in 2007 with Andrea Torsello I published a paper on.",
            "Constructing a generative model for variations in a restrictive class of structure trees using description length criteria and is this work which I really want to build on in this talk here to show how we can extend these ideas from trees to graph structure."
        ],
        [
            "OK, so if we want to build a generative model of a set of graphs out, broadly speaking, there are two approaches that we can adopt.",
            "The first is to work in the structural domain where we tried to define the probability distribution over a prototype structure together with parameters of a distribution.",
            "That cap captures variation in its structure.",
            "Another approach which we explored in some detail is the spectral domain and hear what we'd attempt to do is to capture the properties of graphs using the spectrum of the passing of the adjacency matrix in many ways, but the spectral domain approach can be thought of as a kind of transformation of the problem from the discrete graph domain violent embedding process into a point distribution domain.",
            "So it's his first topic which I really want to focus on in this lecture, which is really confronting head on the problem of how to characterize structural variations in graphs."
        ],
        [
            "OK, so let me just quickly review some of the concepts which we explored in the 2007 Pammy paper with Andrea and then explain how we extend that from trees to."
        ],
        [
            "Crafts.",
            "So description length has its origins back in the in the 1970s with two pieces of parallel work.",
            "Wallace and Freeman introduced a minimum message length approach and rising then introduced minimum description length and these two concepts are really hard to distinguish.",
            "They intersect in many ways and interrelated."
        ],
        [
            "But if you want to try to characterize what they do, minimum description length aims to try to select a model.",
            "Here the model parameters are simply a means to an end, and the parameters are usually estimated using maximum likelihood.",
            "And in this approach priors on parameters are usually assumed to be flat minimum message length.",
            "On the other hand, aims directly at the recovery of the model parameters, which are central.",
            "And the parameter prior maybe a lot more come."
        ],
        [
            "Blacks?",
            "So if in the both of these two 2 approaches, what you need is to try to code up the complexity of your model, you'll fit to the data and also its complexity in terms of number of parameters and their job to rather different coding schemes, and so usually we assume that the code links for the the model for an exponential distribution.",
            "Alternatives to this are universal codes and predictive codes.",
            "Look we look at ML.",
            "And MDL&ML has two part codes and MDL has one or two part codes."
        ],
        [
            "So in Android is what we did is we developed a mixture of true unions to try to describe a set of trees.",
            "Here the appearance or non appearance of a node of a tree in the data was modeled using a Bernoulli distribution.",
            "Total was MDL like because we made maximum likelihood estimates of the Bernoulli parameters, but it was also ML like in that we have to use the three part code for the data that matches the model to the data and the complexity of the model."
        ],
        [
            "So this is how the algorithm worked.",
            "The idea was to assign each of a sample of trees to it reunion.",
            "So we had some sort of merging process going on in order to construct a union tree or sort of super tree from which each of the trees observed in the data would be obtained by edit operations.",
            "And then the idea was to construct this tree through merging trees in the data, and this was controlled using a description links criterion."
        ],
        [
            "On his Nick sample of a set of data trees in the top row being merged together under this description length process to construct the final Super tree, and in this example the blackness of grayness of the the nodes represent their probability of occurrence or their frequency in the data."
        ],
        [
            "So what I want to do, it now is really to briefly explain how we've under the same bad project, how we've extended these ideas from trees to graphs.",
            "So what we're trying to do is to learn the generative model that can capture variations Ned structure when we don't.",
            "We're not given correspondence is we have to infer them from the data and the idea in this work is to follow the approach that Andrea and I developed in the family 2007 paper.",
            "And pose.",
            "The problem is that of learning a super graph representation.",
            "Using a description length criterion, now we need some probability distributions to approach this problem, and we've drawn again on a piece of prior work that I did with been low which appeared in the Pammy paper in 2001 where we we develop a probability distribution over a set for for the for the graph matching problem in a matrix setting, and then with this the probability distribution and this and the.",
            "Description length formulation to hand.",
            "We develop an ATM algorithm which can both.",
            "Locate node correspondances and estimator adjacency matrix of the Super graph that represents the set of graphs in our trading set.",
            "An interesting addition to this is to use the Von Neumann entropy to represent the complexity of the."
        ],
        [
            "Learn model.",
            "So let's just go through the ingredients of this very very quickly, and then I'll."
        ],
        [
            "Show you some results, so the the idea in matrix representation of graph matching in the work we've been low was to represent the problem using three through matrices.",
            "We have data graphs adjacency matrix which has binary elements are model graph adjacency matrix which also has binary elements and a matrix of indicators which show whether a pair of nodes in the data graph and the model graph.",
            "In correspondence with with each other and under assumption of Bernoulli errors in the matching process, then what happens is that we we find the probability the data graph given the model graph and the set of correspondences is governed by the probability distribution exponential probability distribution that I showed you on the bottom line and the thing to note about this is it's a product of sums of exponentials of basically the correlation between the data graph.",
            "Adjacency matrix and the model graph adjacency matrix under the set of correspondence."
        ],
        [
            "It is.",
            "So the idea is listed in this piece of work is to extend this this description where we have not just a single data graph that can be in correspondence with the model graph, But a whole set of possible graphs and a whole set of possible correspondences size I denote the set of graphs were trying to learn from by GVD of I and the corresponding set of correspondence matrices by SFI and the idea now.",
            "Is to try to learn the model graph adjacency matrix that best describes the set of data graphs."
        ],
        [
            "Given this is the probability distribution for that set up?",
            "So now we have a product over the data graphs of product over the nodes in the in each of the data graph, some over product.",
            "I've reached nodes in data graph and the sum over the model graph nodes.",
            "And here's the exponential distribution.",
            "Again of the correlation between the data graph adjacency matrix and the model graph adjacency matrix under the correspondence."
        ],
        [
            "This.",
            "So the aim now is to try to estimate each of the correspondence matrices and the model graph adjacency matrix.",
            "Under this probability distribution that can be set up as a maximum likelihood problem and the quantities required learned using the expectation maximization algorithm, so I won't go into the details, but they're described on these slides."
        ],
        [
            "Here, so here are the steps of the expectation maximization algorithm, and it turns out that to find the correspondence matrices what we're trying to do is to optimize a trace or correlation of the data graph and model graph adjacency matrices under the under the permutations allowed."
        ],
        [
            "So here's my examples of applying this to a data set.",
            "Surprisingly, granted, this is a new data set that we've generated, and so Lynn, her collected, went to the toy Shop and brought a number of interesting objects which you see here and then photographed them in a number of different poses.",
            "And then what she tried to do was to represent these objects again using SIFT features and learning graphs on safety features, and then applied this technique to learn class prototypes.",
            "For each of the."
        ],
        [
            "Each of the objects, so here's some examples of the the correspondence that we learn, but we estimate between different views."
        ],
        [
            "And here's an example of the.",
            "The algorithm running and what we've done is.",
            "We've measured the entropy of the of the solution and it's log likelihood and use it as the algorithm iterates him.",
            "Algorithm iterates for each of the different objects present.",
            "We reduce the entropy and we also maximize the light log likelihood.",
            "So that's really just a standard EM algorithm applied to the log likelihood function that we've developed for this sort of matrix representation, aimed at trying to learn."
        ],
        [
            "Basically matrix from model approach type graph, what we've?"
        ],
        [
            "Oh also tried to do in this work is to bring in the notion of model complexity.",
            "So for representing the for representing the set of graphs in our training set, the data graphs using a learned model graph represented in terms of adjacency matrix.",
            "What we need is a means of characterizing the complexity of that structure in order to understand the tradeoff between goodness of fit and.",
            "Complexity of model in the in the minimization of description length.",
            "So what we've done is to is to Rep is to represent the complexity of the model graph adjacency matrix using the Neumann entropy.",
            "So if we have the model graph adjacency matrix M, the corresponding degree matrix D for that for that model graph adjacency matrix leader class C&L is D -- M. We can compute the eigenvalues of the Laplacian matrix.",
            "And with those eigenvalues to hand the entropy, the Von Neumann entropy is just Lambda sum of Lambda log Lambda.",
            "Now that turns out not to be particularly tractable in the description length minimization framework that we've adopted.",
            "So what we've done was made in a quadratic approximation to the entropy.",
            "We assume that H, rather than being Lambda log Lambda, is Lambda times 1 minus Lambda, and under that assumption the Von Neumann.",
            "Entropy of the model graph is just the sum of the squares of its elements and."
        ],
        [
            "If we then pop that into.",
            "Description length criterion together with the probability distribution that I've shown you for the set of data graphs under the the model graph and the set of correspondence matrices, then this is the code length criterion that we have to minimize this, minus the log likelihood of the data given the model, and the correspondence is plus the Von Neumann entropy, which is just the sum of the squares of the elements of the adjacency matrix.",
            "And if you simplify this, it turns out that the quantity we need to minimize is just the coloration of the data graph and model graph adjacency matrix under the set of correspondences minus the sum of the squares of the elements in the model graph adjacency matrix."
        ],
        [
            "And so we've we've again applied this to.",
            "The the toy data that I've shown you and the technique is effective in minimizing the code length criterion.",
            "Datasets simultaneously minimizes both the log likelihood and the complexity of the model just to go."
        ],
        [
            "Back a few, a few slides to give you an example of what happens if we use this this model in a classification experiment.",
            "What we've done is we have used some of our graphs for training.",
            "We've retrait.",
            "We retained some of them for testing, and here are the results of applying different graph structures to classifying the training data.",
            "And so if we if we use the median graph, for instance, we get about.",
            "57% right and if we use our learn super graph representation, we improve that to around 8384%.",
            "So."
        ],
        [
            "Let's just conclude now what what I've done is I have shown how a generative model based on a sort of super graph representation can be used to learn a generative model of graph structure using a variant of the EM algorithm and also using a code length approach.",
            "And I've just demonstrated that this this technique.",
            "It learns models which are effective.",
            "Certainly more effective in the median graph for the problem classification.",
            "So thank you very much indeed.",
            "Thanks.",
            "Making them.",
            "Baby sister by name.",
            "Very interesting admin.",
            "There is this work by Andy Wong published in the 80s back in the 80s and later.",
            "Alberto sunfield.",
            "You added a little bit to it.",
            "Could you briefly sketch how this is related?",
            "In which way it extends the random graphs or how it relates to the random graphs?",
            "Yeah, I mean they.",
            "In that work, they sort of have a probability distribution on the appearance of nodes and edges.",
            "If I remember correctly, I don't.",
            "They don't parameterise work using the same Bernoulli distribution we use here, and they certainly don't try to fit it to data using AM or description makes a frameworks so intensity early development, probabilistic model.",
            "Words you can look on this work as applying recent developments from machine learning, particularly in description, links, approaches to the estimation of parameters, and of course, the distributions that we use are well different in terms of their parametric structure.",
            "Time for one more question.",
            "Find is my.",
            "Why should we use adjacency matrix as anything valuable in any formulation like this?",
            "In other words, when you have adjacency matrix appearing anywhere in the expression and you're doing minimization minimization or maximization.",
            "In fact, adjacency matrix undergoes rather convulsive changes as a result of these deletions and additions, why should we even use it?",
            "Well, I mean.",
            "I think number of reasons one is that I didn't go into any particular depth into the maximization algorithm that can actually be.",
            "If you work with the matrix representation, you can solve that using an eigenvector problem, and it turns out that the error measure underpinning these distributions is the correlation between the adjacency.",
            "Of the adjacency matrices for the model and the data under our current set of permutations, and if you go back to the work of Scotland under Higgins, it turns out that you can estimate the.",
            "The the permutation the set of correspondences that maximize that overlap in a straightforward way using an eigenvector method using singular value decomposition.",
            "So the point is it gives you a nice matrix way of optimizing the correspondence is that is for pragmatic reason.",
            "I think the other.",
            "The other reason is that the if you want to go if you want to soften the representation.",
            "Then the adjacency matrix provides something which can be naturally softened, and you can in in softening.",
            "You can interpret the departures of the elements from being binary 01 to them becoming probabilities.",
            "I agree with that.",
            "I was just wondering whether you can say something about the noise response or the noise behavior of the metrics if you if your metrics changes because of the underlying noise in the data, right?",
            "Well, any eigenvalue representation or any other derivative of adjacency matrix also gracefully changes or or not?",
            "I mean certainly they.",
            "I mean there probation now sees which tell you how the eigenvalues behave.",
            "They generally under under, under perturbations reasonably well behaved, recalls.",
            "The eigenvectors are notoriously noisy.",
            "I mean they can, they can shift direction in a very unstable way, but I mean we have the empirical proof that you know this is reasonably stable from our experiments, and so it was something we thought the subscribe interest tested.",
            "OK, let's thank Irene.",
            "Again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Within Hannon Richard Wilson on learning generative models of graphs and within the Sinbad context graphs represent representations of dissimilarity or structure.",
                    "label": 1
                },
                {
                    "sent": "Arrangements of objects, and in this work we have been attempting to do is to try to cast the problem of learning models of how such representations distribute themselves in a generative probabilistic sense.",
                    "label": 0
                },
                {
                    "sent": "So this work is of course supported by Sinbad and my work is supported as part of our society.",
                    "label": 0
                },
                {
                    "sent": "Wilson Merit award",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So graph represent representations of data, particularly computer vision.",
                    "label": 0
                },
                {
                    "sent": "Have a number of advantages and disadvantages, advantages that they can capture object or scene structure in a manner that's invariant to changes in viewpoint, and they can abstract scene contents in an efficient way.",
                    "label": 1
                },
                {
                    "sent": "The disadvantages, of course, are that they can be extremely fragile.",
                    "label": 1
                },
                {
                    "sent": "They're sensitive to noise and segmentation error, and that if we tried to deal with with graphs as patterns, then the.",
                    "label": 0
                },
                {
                    "sent": "Machine learning methodology available for processing them is very limited.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we were thinking of of trying to to learn on sets of graphs, then there are a number of problems of increasing complexity that we might attempt to tackle.",
                    "label": 1
                },
                {
                    "sent": "The first of these fools very naturally into the into the framework of Sinbad, and that is that we could try to cluster similar objects and represent them using class prototype.",
                    "label": 0
                },
                {
                    "sent": "For instance a median using similarity or dissimilarity data.",
                    "label": 0
                },
                {
                    "sent": "Now the problem with this approach is it simply tells you about the distributions of the graphs within the patent space doesn't tell you anything about the sources of structural variation which may occur in a class of graphs, so a slightly more sort of sophisticated approach which begins to go in that direction would be to extract features and then perform central clustering, when if you really want to do a proper job of learning the how the structural variations.",
                    "label": 1
                },
                {
                    "sent": "In graph distribute themselves then what you have to do is to construct a generative model on a set of probability distributions to capture the modes of variation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just give it some sort of examples of when this is simple and when this is difficult.",
                    "label": 0
                },
                {
                    "sent": "So for instance, here I've generated a sequence of synthetic grafts from the corners in an image of the House which rotates with respect to the camera and underneath what I've done is I've shown the Delaunay triangulation of the feature points extracted from that graph, and this is a very contrived scene.",
                    "label": 0
                },
                {
                    "sent": "First of all, I know the order of the feature points or their labels.",
                    "label": 0
                },
                {
                    "sent": "And no feature point appears or disappears in the sequence.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if I then take the adjacency matrices for those graphs, stack limit along vectors in the order of the feature labels on the nodes and then perform PCA on those long vectors representing the adjacency matrices, then what happens is as I traverse the series of synthetic images corresponding to the different viewpoints of those houses.",
                    "label": 1
                },
                {
                    "sent": "Then the graphs trace out continuous trajectory in the principal component space.",
                    "label": 0
                },
                {
                    "sent": "So that's as ideal really as it could get.",
                    "label": 0
                },
                {
                    "sent": "No, no nodes appear or disappear, and we know the correspondences or labels on the nodes.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is actually what it looks like in practice.",
                    "label": 0
                },
                {
                    "sent": "If you try to do this, we are using the sorts of feature detectors that are common in computer vision.",
                    "label": 0
                },
                {
                    "sent": "So here a sequence of images of a toy house connected with the cameras camera pans on the object feature points extract extracted using a corner detector and then delivered a triangulated in the same way as a synthetic example and you see what happens is that the not only do the structure of the graph change.",
                    "label": 0
                },
                {
                    "sent": "From viewpoint viewpoint, but actually have nodes appear and disappear due to a number of causes.",
                    "label": 0
                },
                {
                    "sent": "For instance, occlusion would also false positives and false negatives delivered by the corner detector.",
                    "label": 0
                },
                {
                    "sent": "So the simple scenario I pointed out on the previous slide just isn't workable.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what I've done is I've sort of listed here.",
                    "label": 0
                },
                {
                    "sent": "The problems if we're going to use graph structures to represent the proximity, proximity, or similarity of features in computer vision problems, then the algorithms that we're going to have to use to detect the feature points are going to be inherently I'm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reliable and so.",
                    "label": 0
                },
                {
                    "sent": "As a result, they're going to be both additional missing nodes due to segmentation error, and in addition to genuine variations in edge structure.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graphs and so image matching and recognition cannot be reduced to graph graph isomorphism or even subgraph isomorphism.",
                    "label": 0
                },
                {
                    "sent": "Instead, inexact graph matching techniques have to be used, and things and computationally very.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pensive OK so this is really just a summary of why that analysis with graphs is difficult and the take home messages is a graphs.",
                    "label": 0
                },
                {
                    "sent": "Don't reside naturally in a vector space because we need a correspondence order between the nodes and that and as a result it's not easy to compute statistical summaries which can be used to capture variations in structure such as the mean variance of a set of graphs.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This list is this slide here really sort of reiterates the point I made earlier on about the sort of problems you might try to attack using learning in the graph domain and what I'm trying to do in this talk is really to focus in some detail on the last of these problems where I want to construct a generative model that can capture variations in graph structure over a set of exemplars.",
                    "label": 0
                },
                {
                    "sent": "And in order to tackle this problem, what I need to do is is to establish correspondences between nodes.",
                    "label": 0
                },
                {
                    "sent": "A set of graphs which I'm trying to analyze, and then have a means of characterizing variation in both node and edge structure in that family of graphs.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're not coming at this problem.",
                    "label": 0
                },
                {
                    "sent": "Cold Turkey prior to December we had a number of attempts that are trying to remove threat list of topics that I showed you on the previous slide and so really the first piece of work with Antonio Robles Kelly and Andrea Torsello was to explore using pairwise clustering techniques to discover the class structure in sets of graphs using pairwise similarities.",
                    "label": 0
                },
                {
                    "sent": "With with bin Laden, Richard Wilson, we did some work on graph clustering using graph spectral features and then in 2007 with Andrea Torsello I published a paper on.",
                    "label": 1
                },
                {
                    "sent": "Constructing a generative model for variations in a restrictive class of structure trees using description length criteria and is this work which I really want to build on in this talk here to show how we can extend these ideas from trees to graph structure.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if we want to build a generative model of a set of graphs out, broadly speaking, there are two approaches that we can adopt.",
                    "label": 0
                },
                {
                    "sent": "The first is to work in the structural domain where we tried to define the probability distribution over a prototype structure together with parameters of a distribution.",
                    "label": 1
                },
                {
                    "sent": "That cap captures variation in its structure.",
                    "label": 0
                },
                {
                    "sent": "Another approach which we explored in some detail is the spectral domain and hear what we'd attempt to do is to capture the properties of graphs using the spectrum of the passing of the adjacency matrix in many ways, but the spectral domain approach can be thought of as a kind of transformation of the problem from the discrete graph domain violent embedding process into a point distribution domain.",
                    "label": 0
                },
                {
                    "sent": "So it's his first topic which I really want to focus on in this lecture, which is really confronting head on the problem of how to characterize structural variations in graphs.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me just quickly review some of the concepts which we explored in the 2007 Pammy paper with Andrea and then explain how we extend that from trees to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Crafts.",
                    "label": 0
                },
                {
                    "sent": "So description length has its origins back in the in the 1970s with two pieces of parallel work.",
                    "label": 0
                },
                {
                    "sent": "Wallace and Freeman introduced a minimum message length approach and rising then introduced minimum description length and these two concepts are really hard to distinguish.",
                    "label": 0
                },
                {
                    "sent": "They intersect in many ways and interrelated.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if you want to try to characterize what they do, minimum description length aims to try to select a model.",
                    "label": 0
                },
                {
                    "sent": "Here the model parameters are simply a means to an end, and the parameters are usually estimated using maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "And in this approach priors on parameters are usually assumed to be flat minimum message length.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, aims directly at the recovery of the model parameters, which are central.",
                    "label": 0
                },
                {
                    "sent": "And the parameter prior maybe a lot more come.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Blacks?",
                    "label": 0
                },
                {
                    "sent": "So if in the both of these two 2 approaches, what you need is to try to code up the complexity of your model, you'll fit to the data and also its complexity in terms of number of parameters and their job to rather different coding schemes, and so usually we assume that the code links for the the model for an exponential distribution.",
                    "label": 0
                },
                {
                    "sent": "Alternatives to this are universal codes and predictive codes.",
                    "label": 1
                },
                {
                    "sent": "Look we look at ML.",
                    "label": 1
                },
                {
                    "sent": "And MDL&ML has two part codes and MDL has one or two part codes.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in Android is what we did is we developed a mixture of true unions to try to describe a set of trees.",
                    "label": 0
                },
                {
                    "sent": "Here the appearance or non appearance of a node of a tree in the data was modeled using a Bernoulli distribution.",
                    "label": 0
                },
                {
                    "sent": "Total was MDL like because we made maximum likelihood estimates of the Bernoulli parameters, but it was also ML like in that we have to use the three part code for the data that matches the model to the data and the complexity of the model.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is how the algorithm worked.",
                    "label": 0
                },
                {
                    "sent": "The idea was to assign each of a sample of trees to it reunion.",
                    "label": 1
                },
                {
                    "sent": "So we had some sort of merging process going on in order to construct a union tree or sort of super tree from which each of the trees observed in the data would be obtained by edit operations.",
                    "label": 1
                },
                {
                    "sent": "And then the idea was to construct this tree through merging trees in the data, and this was controlled using a description links criterion.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On his Nick sample of a set of data trees in the top row being merged together under this description length process to construct the final Super tree, and in this example the blackness of grayness of the the nodes represent their probability of occurrence or their frequency in the data.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I want to do, it now is really to briefly explain how we've under the same bad project, how we've extended these ideas from trees to graphs.",
                    "label": 0
                },
                {
                    "sent": "So what we're trying to do is to learn the generative model that can capture variations Ned structure when we don't.",
                    "label": 1
                },
                {
                    "sent": "We're not given correspondence is we have to infer them from the data and the idea in this work is to follow the approach that Andrea and I developed in the family 2007 paper.",
                    "label": 0
                },
                {
                    "sent": "And pose.",
                    "label": 1
                },
                {
                    "sent": "The problem is that of learning a super graph representation.",
                    "label": 0
                },
                {
                    "sent": "Using a description length criterion, now we need some probability distributions to approach this problem, and we've drawn again on a piece of prior work that I did with been low which appeared in the Pammy paper in 2001 where we we develop a probability distribution over a set for for the for the graph matching problem in a matrix setting, and then with this the probability distribution and this and the.",
                    "label": 0
                },
                {
                    "sent": "Description length formulation to hand.",
                    "label": 0
                },
                {
                    "sent": "We develop an ATM algorithm which can both.",
                    "label": 0
                },
                {
                    "sent": "Locate node correspondances and estimator adjacency matrix of the Super graph that represents the set of graphs in our trading set.",
                    "label": 0
                },
                {
                    "sent": "An interesting addition to this is to use the Von Neumann entropy to represent the complexity of the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learn model.",
                    "label": 0
                },
                {
                    "sent": "So let's just go through the ingredients of this very very quickly, and then I'll.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show you some results, so the the idea in matrix representation of graph matching in the work we've been low was to represent the problem using three through matrices.",
                    "label": 0
                },
                {
                    "sent": "We have data graphs adjacency matrix which has binary elements are model graph adjacency matrix which also has binary elements and a matrix of indicators which show whether a pair of nodes in the data graph and the model graph.",
                    "label": 1
                },
                {
                    "sent": "In correspondence with with each other and under assumption of Bernoulli errors in the matching process, then what happens is that we we find the probability the data graph given the model graph and the set of correspondences is governed by the probability distribution exponential probability distribution that I showed you on the bottom line and the thing to note about this is it's a product of sums of exponentials of basically the correlation between the data graph.",
                    "label": 0
                },
                {
                    "sent": "Adjacency matrix and the model graph adjacency matrix under the set of correspondence.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "So the idea is listed in this piece of work is to extend this this description where we have not just a single data graph that can be in correspondence with the model graph, But a whole set of possible graphs and a whole set of possible correspondences size I denote the set of graphs were trying to learn from by GVD of I and the corresponding set of correspondence matrices by SFI and the idea now.",
                    "label": 1
                },
                {
                    "sent": "Is to try to learn the model graph adjacency matrix that best describes the set of data graphs.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given this is the probability distribution for that set up?",
                    "label": 0
                },
                {
                    "sent": "So now we have a product over the data graphs of product over the nodes in the in each of the data graph, some over product.",
                    "label": 0
                },
                {
                    "sent": "I've reached nodes in data graph and the sum over the model graph nodes.",
                    "label": 0
                },
                {
                    "sent": "And here's the exponential distribution.",
                    "label": 0
                },
                {
                    "sent": "Again of the correlation between the data graph adjacency matrix and the model graph adjacency matrix under the correspondence.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "So the aim now is to try to estimate each of the correspondence matrices and the model graph adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "Under this probability distribution that can be set up as a maximum likelihood problem and the quantities required learned using the expectation maximization algorithm, so I won't go into the details, but they're described on these slides.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, so here are the steps of the expectation maximization algorithm, and it turns out that to find the correspondence matrices what we're trying to do is to optimize a trace or correlation of the data graph and model graph adjacency matrices under the under the permutations allowed.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's my examples of applying this to a data set.",
                    "label": 0
                },
                {
                    "sent": "Surprisingly, granted, this is a new data set that we've generated, and so Lynn, her collected, went to the toy Shop and brought a number of interesting objects which you see here and then photographed them in a number of different poses.",
                    "label": 0
                },
                {
                    "sent": "And then what she tried to do was to represent these objects again using SIFT features and learning graphs on safety features, and then applied this technique to learn class prototypes.",
                    "label": 1
                },
                {
                    "sent": "For each of the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each of the objects, so here's some examples of the the correspondence that we learn, but we estimate between different views.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's an example of the.",
                    "label": 0
                },
                {
                    "sent": "The algorithm running and what we've done is.",
                    "label": 0
                },
                {
                    "sent": "We've measured the entropy of the of the solution and it's log likelihood and use it as the algorithm iterates him.",
                    "label": 1
                },
                {
                    "sent": "Algorithm iterates for each of the different objects present.",
                    "label": 0
                },
                {
                    "sent": "We reduce the entropy and we also maximize the light log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So that's really just a standard EM algorithm applied to the log likelihood function that we've developed for this sort of matrix representation, aimed at trying to learn.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically matrix from model approach type graph, what we've?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh also tried to do in this work is to bring in the notion of model complexity.",
                    "label": 0
                },
                {
                    "sent": "So for representing the for representing the set of graphs in our training set, the data graphs using a learned model graph represented in terms of adjacency matrix.",
                    "label": 1
                },
                {
                    "sent": "What we need is a means of characterizing the complexity of that structure in order to understand the tradeoff between goodness of fit and.",
                    "label": 1
                },
                {
                    "sent": "Complexity of model in the in the minimization of description length.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is to is to Rep is to represent the complexity of the model graph adjacency matrix using the Neumann entropy.",
                    "label": 0
                },
                {
                    "sent": "So if we have the model graph adjacency matrix M, the corresponding degree matrix D for that for that model graph adjacency matrix leader class C&L is D -- M. We can compute the eigenvalues of the Laplacian matrix.",
                    "label": 0
                },
                {
                    "sent": "And with those eigenvalues to hand the entropy, the Von Neumann entropy is just Lambda sum of Lambda log Lambda.",
                    "label": 1
                },
                {
                    "sent": "Now that turns out not to be particularly tractable in the description length minimization framework that we've adopted.",
                    "label": 0
                },
                {
                    "sent": "So what we've done was made in a quadratic approximation to the entropy.",
                    "label": 0
                },
                {
                    "sent": "We assume that H, rather than being Lambda log Lambda, is Lambda times 1 minus Lambda, and under that assumption the Von Neumann.",
                    "label": 0
                },
                {
                    "sent": "Entropy of the model graph is just the sum of the squares of its elements and.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we then pop that into.",
                    "label": 0
                },
                {
                    "sent": "Description length criterion together with the probability distribution that I've shown you for the set of data graphs under the the model graph and the set of correspondence matrices, then this is the code length criterion that we have to minimize this, minus the log likelihood of the data given the model, and the correspondence is plus the Von Neumann entropy, which is just the sum of the squares of the elements of the adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "And if you simplify this, it turns out that the quantity we need to minimize is just the coloration of the data graph and model graph adjacency matrix under the set of correspondences minus the sum of the squares of the elements in the model graph adjacency matrix.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we've we've again applied this to.",
                    "label": 0
                },
                {
                    "sent": "The the toy data that I've shown you and the technique is effective in minimizing the code length criterion.",
                    "label": 0
                },
                {
                    "sent": "Datasets simultaneously minimizes both the log likelihood and the complexity of the model just to go.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Back a few, a few slides to give you an example of what happens if we use this this model in a classification experiment.",
                    "label": 0
                },
                {
                    "sent": "What we've done is we have used some of our graphs for training.",
                    "label": 1
                },
                {
                    "sent": "We've retrait.",
                    "label": 0
                },
                {
                    "sent": "We retained some of them for testing, and here are the results of applying different graph structures to classifying the training data.",
                    "label": 0
                },
                {
                    "sent": "And so if we if we use the median graph, for instance, we get about.",
                    "label": 1
                },
                {
                    "sent": "57% right and if we use our learn super graph representation, we improve that to around 8384%.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's just conclude now what what I've done is I have shown how a generative model based on a sort of super graph representation can be used to learn a generative model of graph structure using a variant of the EM algorithm and also using a code length approach.",
                    "label": 1
                },
                {
                    "sent": "And I've just demonstrated that this this technique.",
                    "label": 0
                },
                {
                    "sent": "It learns models which are effective.",
                    "label": 0
                },
                {
                    "sent": "Certainly more effective in the median graph for the problem classification.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much indeed.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Making them.",
                    "label": 0
                },
                {
                    "sent": "Baby sister by name.",
                    "label": 0
                },
                {
                    "sent": "Very interesting admin.",
                    "label": 0
                },
                {
                    "sent": "There is this work by Andy Wong published in the 80s back in the 80s and later.",
                    "label": 0
                },
                {
                    "sent": "Alberto sunfield.",
                    "label": 0
                },
                {
                    "sent": "You added a little bit to it.",
                    "label": 0
                },
                {
                    "sent": "Could you briefly sketch how this is related?",
                    "label": 0
                },
                {
                    "sent": "In which way it extends the random graphs or how it relates to the random graphs?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean they.",
                    "label": 0
                },
                {
                    "sent": "In that work, they sort of have a probability distribution on the appearance of nodes and edges.",
                    "label": 0
                },
                {
                    "sent": "If I remember correctly, I don't.",
                    "label": 0
                },
                {
                    "sent": "They don't parameterise work using the same Bernoulli distribution we use here, and they certainly don't try to fit it to data using AM or description makes a frameworks so intensity early development, probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Words you can look on this work as applying recent developments from machine learning, particularly in description, links, approaches to the estimation of parameters, and of course, the distributions that we use are well different in terms of their parametric structure.",
                    "label": 0
                },
                {
                    "sent": "Time for one more question.",
                    "label": 0
                },
                {
                    "sent": "Find is my.",
                    "label": 0
                },
                {
                    "sent": "Why should we use adjacency matrix as anything valuable in any formulation like this?",
                    "label": 0
                },
                {
                    "sent": "In other words, when you have adjacency matrix appearing anywhere in the expression and you're doing minimization minimization or maximization.",
                    "label": 0
                },
                {
                    "sent": "In fact, adjacency matrix undergoes rather convulsive changes as a result of these deletions and additions, why should we even use it?",
                    "label": 0
                },
                {
                    "sent": "Well, I mean.",
                    "label": 0
                },
                {
                    "sent": "I think number of reasons one is that I didn't go into any particular depth into the maximization algorithm that can actually be.",
                    "label": 0
                },
                {
                    "sent": "If you work with the matrix representation, you can solve that using an eigenvector problem, and it turns out that the error measure underpinning these distributions is the correlation between the adjacency.",
                    "label": 0
                },
                {
                    "sent": "Of the adjacency matrices for the model and the data under our current set of permutations, and if you go back to the work of Scotland under Higgins, it turns out that you can estimate the.",
                    "label": 0
                },
                {
                    "sent": "The the permutation the set of correspondences that maximize that overlap in a straightforward way using an eigenvector method using singular value decomposition.",
                    "label": 0
                },
                {
                    "sent": "So the point is it gives you a nice matrix way of optimizing the correspondence is that is for pragmatic reason.",
                    "label": 0
                },
                {
                    "sent": "I think the other.",
                    "label": 0
                },
                {
                    "sent": "The other reason is that the if you want to go if you want to soften the representation.",
                    "label": 0
                },
                {
                    "sent": "Then the adjacency matrix provides something which can be naturally softened, and you can in in softening.",
                    "label": 0
                },
                {
                    "sent": "You can interpret the departures of the elements from being binary 01 to them becoming probabilities.",
                    "label": 0
                },
                {
                    "sent": "I agree with that.",
                    "label": 0
                },
                {
                    "sent": "I was just wondering whether you can say something about the noise response or the noise behavior of the metrics if you if your metrics changes because of the underlying noise in the data, right?",
                    "label": 0
                },
                {
                    "sent": "Well, any eigenvalue representation or any other derivative of adjacency matrix also gracefully changes or or not?",
                    "label": 0
                },
                {
                    "sent": "I mean certainly they.",
                    "label": 0
                },
                {
                    "sent": "I mean there probation now sees which tell you how the eigenvalues behave.",
                    "label": 0
                },
                {
                    "sent": "They generally under under, under perturbations reasonably well behaved, recalls.",
                    "label": 0
                },
                {
                    "sent": "The eigenvectors are notoriously noisy.",
                    "label": 0
                },
                {
                    "sent": "I mean they can, they can shift direction in a very unstable way, but I mean we have the empirical proof that you know this is reasonably stable from our experiments, and so it was something we thought the subscribe interest tested.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank Irene.",
                    "label": 0
                },
                {
                    "sent": "Again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}