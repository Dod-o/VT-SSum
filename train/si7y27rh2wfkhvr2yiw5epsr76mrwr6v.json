{
    "id": "si7y27rh2wfkhvr2yiw5epsr76mrwr6v",
    "title": "Multiview Fisher Discriminant Analysis",
    "info": {
        "author": [
            "Tom Diethe, Amazon"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Principal Component Analysis"
        ]
    },
    "url": "http://videolectures.net/lms08_diethe_mfda/",
    "segmentation": [
        [
            "Hi, I'm Tom.",
            "Dieter says joint work with David Harden and Joshua Taylor.",
            "And so it's looking at Multiview version of Fisher discriminant analysis.",
            "So I thought I had five minutes and then."
        ],
        [
            "Not too so I gotta brush."
        ],
        [
            "So basically we know that Fisher discriminant analysis is Bayes optimal for two.",
            "Normal distributions with equal covariance.",
            "An kernel Fisher is is the same in the feature space.",
            "So given this, the outputs of the classifier can be seen as probabilities if our assumptions are correct, of course.",
            "So if we simply sum the outputs of multiple Fishers, then.",
            "The the classification function will just choose the most most confident classifiers."
        ],
        [
            "Anne.",
            "So we have follow approach on the standard Fisher of Mika, which was a convex formulation which is shown to be equivalent to the standard Fisher.",
            "Which is quite nice and gives us various flexible options later on.",
            "As well as a way to solve it, of course.",
            "Anne."
        ],
        [
            "So there's two methods of doing this in the in the multiview setting.",
            "So here we are.",
            "We have H which is our loss function and P which is a regularization function and we are minimizing over slacks over all of the views.",
            "And.",
            "And in this situation here we are constraining each of the views by the by the same set of Slack variables."
        ],
        [
            "The second formulation is subtly different, but.",
            "It sort of yields fairly similar results, but is much more efficient.",
            "So in this in this situation we're actually summarizing we're taking the sum of the slacks.",
            "Sorry, the some of the views and constraining those slaps."
        ],
        [
            "So.",
            "Obviously a natural choice for the regularization would be simply the two norm, but quite interesting is the one norm 'cause that gives us."
        ],
        [
            "Spell Solutions and then we can also.",
            "Yeah yeah will do.",
            "Yeah, so I'll.",
            "Yeah, I'll skip the last couple so we can also drop the Gaussian assumption, which you know it's slightly strange what's going on there, but if you have a one, normally the slacks that the whole problem reduces to a linear program and it gives bar solutions, which is quite nice so."
        ],
        [
            "So basically, if you come over to the poster."
        ],
        [
            "E2 sets of data that we tried it on and some encouraging results.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, I'm Tom.",
                    "label": 0
                },
                {
                    "sent": "Dieter says joint work with David Harden and Joshua Taylor.",
                    "label": 0
                },
                {
                    "sent": "And so it's looking at Multiview version of Fisher discriminant analysis.",
                    "label": 1
                },
                {
                    "sent": "So I thought I had five minutes and then.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not too so I gotta brush.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically we know that Fisher discriminant analysis is Bayes optimal for two.",
                    "label": 1
                },
                {
                    "sent": "Normal distributions with equal covariance.",
                    "label": 1
                },
                {
                    "sent": "An kernel Fisher is is the same in the feature space.",
                    "label": 0
                },
                {
                    "sent": "So given this, the outputs of the classifier can be seen as probabilities if our assumptions are correct, of course.",
                    "label": 1
                },
                {
                    "sent": "So if we simply sum the outputs of multiple Fishers, then.",
                    "label": 0
                },
                {
                    "sent": "The the classification function will just choose the most most confident classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So we have follow approach on the standard Fisher of Mika, which was a convex formulation which is shown to be equivalent to the standard Fisher.",
                    "label": 0
                },
                {
                    "sent": "Which is quite nice and gives us various flexible options later on.",
                    "label": 0
                },
                {
                    "sent": "As well as a way to solve it, of course.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's two methods of doing this in the in the multiview setting.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "We have H which is our loss function and P which is a regularization function and we are minimizing over slacks over all of the views.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And in this situation here we are constraining each of the views by the by the same set of Slack variables.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second formulation is subtly different, but.",
                    "label": 0
                },
                {
                    "sent": "It sort of yields fairly similar results, but is much more efficient.",
                    "label": 0
                },
                {
                    "sent": "So in this in this situation we're actually summarizing we're taking the sum of the slacks.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the some of the views and constraining those slaps.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Obviously a natural choice for the regularization would be simply the two norm, but quite interesting is the one norm 'cause that gives us.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spell Solutions and then we can also.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah will do.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'll.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll skip the last couple so we can also drop the Gaussian assumption, which you know it's slightly strange what's going on there, but if you have a one, normally the slacks that the whole problem reduces to a linear program and it gives bar solutions, which is quite nice so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically, if you come over to the poster.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "E2 sets of data that we tried it on and some encouraging results.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}