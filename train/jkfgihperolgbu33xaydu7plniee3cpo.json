{
    "id": "jkfgihperolgbu33xaydu7plniee3cpo",
    "title": "A Bayesian Probability Calculus for Density Matrices",
    "info": {
        "author": [
            "Manfred K. Warmuth, Department of Computer Science, University of California Santa Cruz"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "August 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_warmuth_bpcdm/",
    "segmentation": [
        [
            "OK, last research talk of today and most like this is pretty wild stuff because I'm going to generalize the probability theory the way quantum physicists do and most likely going to be partitioned into two clusters, either going to perk up or fall asleep, But anyway.",
            "So that's me and Tim joint work with Team Accouchement.",
            "K."
        ],
        [
            "And then I'm going to talk a little bit about matrix algebra, and then I'm going to generalize probability theory.",
            "And then develop a generalized base rule."
        ],
        [
            "OK, so symmetric positive definite.",
            "The main object is going to be density matrices, which are symmetric matrices.",
            "They're going to be positive definite."
        ],
        [
            "Is this condition this term?",
            "I'm going to lay down interpret as a variance.",
            "The variance is positive, the variance in direction you and that race which is the sum of diagonal elements is 1.",
            "I'm going to argue that these are natural generalizations of probability theory.",
            "They are the mainstay of statistical physics and quantum physics."
        ],
        [
            "OK, So what is?",
            "What is a symmetric matrix due to the unit ball?",
            "If you multiply the unit ball by symmetric matrix, you grab it at the eigenvalues and stretch it.",
            "OK.",
            "So here."
        ],
        [
            "I showed the eigen values the eigen directions, the eigen directions and then you stretch in this direction.",
            "And you stretch in this direction, or you compress here and you stretch here anyway, the eye."
        ],
        [
            "Values are the ones where the direction is preserved.",
            "So if you look at this plot."
        ],
        [
            "I showed what happened to the unit ball.",
            "This point here Blue Point is just moved out.",
            "And this new point is smooth in a straight line.",
            "And that's the can."
        ],
        [
            "Mission for the icon for the definition of an eigenvalue vector.",
            "Now symmetric positive definite matrix have an eigendecomposition OK so you can write that matrix as the script is an orthogonal matrix of the eigenvectors.",
            "This is a diagonal memory matrix of the eigenvalues and then the orthogonal matrix transpose.",
            "This way you can rewrite this decomposition also this way and now notice that he is set the eigenvalues.",
            "These are the elements of the diagonal matrix and here sit there.",
            "Outer product formed by the eigen vectors.",
            "OK, symmetric positive Denson matrices have trace one, so the eigenvalues sum to one and they also non negative.",
            "So you see this, this is kind of a generalization of a mixture.",
            "This is a probability vector and you mix these outer products they called dietze."
        ],
        [
            "Pay the dietze.",
            "I can write as a degenerate ellipse.",
            "It's especially lips that has only one non zero eigen direction and all the other ones are zero and now.",
            "This mixture."
        ],
        [
            "Yeah.",
            "This mixture I."
        ],
        [
            "I can write this way you know, .2 times something like this point.",
            "3 times something at this point five times this dyett that forms a nice lips and then this ellipse has these axis which correspond to the eigen direction can also be decomposed.",
            "This way, using the eigen directions as the dyads.",
            "OK.",
            "So you can always see you can decompose a ellipse in many ways, but there always is one in two orthogonal dyads, and those are the eigen directions.",
            "And there's always an suffice."
        ],
        [
            "OK, so another thing that I want to point out is.",
            "That you can view a symmetric matrix.",
            "Any symmetric positive matrix as a covariance matrix, some some covariance of some cost function, and then if you look at the variance of the cost in a certain direction, it's one of those outer products.",
            "And I'm going to use this outer product a lot.",
            "I will plot it for you in a moment, but before I do that, let me rewrite it.",
            "I can rewrite.",
            "This is a number, so it's a trace.",
            "Also and then the trace cycles so you can write it this way."
        ],
        [
            "OK, let me plot this variance.",
            "Here's my lips, right?",
            "AU and then if I go in there and if I plot the variance in this direction, I will end up here.",
            "And you see, sort of.",
            "The variance is the largest along the maximum eigenvalue and the smallest along the minimum eigen direction.",
            "OK, OK, so now so this is a little bit of linear algebra and Mariana already did some of this and."
        ],
        [
            "Then I'm going to talk a little bit about generalization."
        ],
        [
            "Shens about of probability distributions.",
            "So assume for a moment let me review normal probability distribution.",
            "So assume you have five points in the finite dimensional case.",
            "These are our mental events.",
            "Imagine you have a 5 sided die sort of 65 OK, and then this would be an event.",
            "Right now this is a distribution, just gives you the probabilities of the elementary event, and then if you have one of those events, then by summing all the probabilities that are one where you were the elementary element is in the set.",
            "Then you get the probability of the event.",
            "You can also define expectations.",
            "So this is the normal way normal probability distribution.",
            "How do we generate?"
        ],
        [
            "Justice, So what we're going to do?",
            "First of all, elementary event is going to be one of those dyads.",
            "We have finite dimension.",
            "We have now infinitely many dyads.",
            "OK, there are these things and what's an event in event is?",
            "Anne again.",
            "Characterized by one of those vectors, but the vectors are now the eigenvalues and there's an eigensystem out there, too.",
            "So you see here I had something like this and now the same thing is along the diagonal of this matrix.",
            "But you also have an eigensystem, so this is a projection matrix.",
            "Projects down to a subspace.",
            "What's the distribution?",
            "A density matrix?",
            "It's the same."
        ],
        [
            "Sing as this, but now."
        ],
        [
            "You also have an eigensystem and this is a density matrix.",
            "So the game that we're playing is we not just learning a probability vector, which is now a vector of eigenvalues.",
            "We also learn the eigen direction directions as well.",
            "It's quite curious.",
            "K."
        ],
        [
            "OK, I skipped something.",
            "OK, good.",
            "So an the density matrix assigns a generalized probability to Dyett you.",
            "And this is what how the statistical physicists do it, and the curious thing is that if you sum this probability over an orthogonal set of direction, it sums to one.",
            "OK, so this is a probability.",
            "This is Uira unit, a set of orthogonal directions.",
            "I sum up these probabilities.",
            "I trace is linear.",
            "Can put this aside inside and if when these, organal and any of them then this seems to be identity.",
            "So in the trace of a was one.",
            "So this sums to one.",
            "OK, let me.",
            "To get this across to you that there's something really strange going on, let's define what the uniform distribution is.",
            "Here.",
            "This would be the uniform distribution.",
            "It's the 1 / 10 times the identity matrix.",
            "Notice that the traces one because you have N eigenvalues of size 1 / N good, so this is a distribution viewed as an ellipse.",
            "Now any unit direction has probability 1 / N. You have uncountably infinitely many.",
            "So what sense does this sum to one?",
            "Well, if you take any orthogonal set of them.",
            "Of course I said end of them N * 1 / N is 1, so the probability the total probability of any set of orthogonal direction sums to one.",
            "So this is very unusual way using sort of linear algebra.",
            "What happened before is if you had two events and they were disjoint, then the probability was some with some.",
            "Now if you have two directions and they are orthogonal, then their probabilities sum."
        ],
        [
            "OK, you can do also defined probabilities of events, so if this is a density matrix and this is one of those matrixes with 01 eigenvalues, then you can write the trace.",
            "This way it's linear and then it's kind of like the mixture of the variance along the eigen direction.",
            "Make various other interpretations as quantum measurements, but I'm not going to get into that."
        ],
        [
            "So here comes the main theorem.",
            "Gleason's theorem which says.",
            "Any scalar function from unit directions to R. Is a generalized probability measure if the probabilities you assign like between zero and one, and then if you have an orthogonal basis, they sum to one.",
            "For any orthogonal basis, if you have these two properties, then Gleason showed that the dimension is bigger than three.",
            "It always is described as one of those traces.",
            "There is a matrix that defines this measure.",
            "It's a very deep theorem.",
            "One Direction is easy, the other direction involves heavy group theory.",
            "There's a slight slightly different if you do slightly different definitions here.",
            "You can actually understand the proof, but police is proof is very complicated.",
            "Quantum physicists usually don't use this theorem for real spaces, they use it for complex spaces, but it's already completely sophisticated for real spaces."
        ],
        [
            "OK, so in the Convention, so now I want to get to Bayes rule OK.",
            "Baseball OK so base setup is usually you have when the finite dimensional case we have.",
            "Imagine you have five models, right?",
            "And the probability of the of the model is that Emma is chosen is proportional to the prior and then you have some assume their coins.",
            "So the probability prior probability that Emma is the true model is PMI.",
            "You see a coin flip and then you get the data likelihood probability of Y given in my and this is the theorem of total probability.",
            "I'm going to give you this kind of theorems for density matrices.",
            "Or is the other one?",
            "OK, well that's it.",
            "So."
        ],
        [
            "If the normal Bayes rule normal Bayes rule would be, here's your prior, so certain probabilities for your file for your.",
            "5 coins certain data likelihoods, and now what happens is you take the tool the red and the green in your componentwise.",
            "Multiplying re normalize.",
            "This is your first posterior.",
            "Great service, this is your first posterior.",
            "Notice that this was the highest data likelihood component, so this one is pulled up compared to here.",
            "If you apply the same thing twice, it pulled up more 3 * 4 times.",
            "So I see this update as a softmax calculation.",
            "You want to get the best model."
        ],
        [
            "Here's what the new rule is now.",
            "I'm not showing you yet what the meaning of all these things are.",
            "It's kind of mind boggling, but my prior is now density matrices.",
            "My data likelihood is also a symmetric positive definite matrix.",
            "You cannot multiply them somehow.",
            "You have to take a matrix log, which means you take a decomposition into take log of the diagonal you add, and then you exponentiate that's matrix exponential again, you take a decomposition of this inside matrix and take exponential of the diagonal.",
            "You get a matrix symmetric positive.",
            "Definite matrix and then you divide it and then you have a matrix of trace one which is again a density matrix ha in picture.",
            "This is what happens, it's completely mindboggling.",
            "Here's the prior.",
            "This is the data likelihood.",
            "Right, and this is the first update.",
            "You see that this was the prior and then the first update goes towards the green.",
            "It actually goes towards the largest axis of the green, second one third one 4th one 5th 161 until it homes into the axis.",
            "That correspond to the largest eigenvalue, so.",
            "Here I home in to concentrate on the largest component with my probability vector.",
            "And here I home into the longest axis.",
            "So I see this as a soft maximum eigenvalue calculation and this is the generalization of the Bayes rule for density matrix.",
            "I claim this big contentions physicists try to do it differently.",
            "I submitted a bunch of papers and I got huge feedback.",
            "Everybody has a quantum to say about quantum ice.",
            "And I got 10 pages of referee reports with mind boggling."
        ],
        [
            "So I removed a lot of the quantum language, so not to annoy anybody.",
            "So why this funny operation and where does it come from?",
            "You take you cannot multiply these two matrices is going to be positive definite.",
            "This is going to be positive definite.",
            "I cannot multiply them because multiplication of two positive definite matrix is not positive definite, not in general.",
            "So what you do is you take a log first, which turns you into.",
            "You still have a symmetric matrix, right?",
            "Because take the eigenvectors, take a log that the logs are now negative, but who cares?",
            "It's still symmetric.",
            "This is still symmetric symmetric things.",
            "You can add wow, you add them.",
            "And then you exponentiate.",
            "But if you exponentiate a symmetric matrix, you get positive and then you divide by trees.",
            "And I'm going to drive this update.",
            "OK."
        ],
        [
            "First, let me point out that the normal Basil is a special case.",
            "Why you take your data, likelihood, you write it.",
            "You write it as a diagonal matrix.",
            "You right?",
            "This is the diagonal matrix.",
            "And now I claim this is the posterior right because if I multiply this, it's componentwise multiplication, dividing their trace.",
            "So this is the old case, the eigensystem of both thing is the same, whereas here.",
            "The eigensystem of both matrices is not the same, and then you need this.",
            "I claim you need this massive rule.",
            "Too complicated for this short talk, too complicated so."
        ],
        [
            "Anne.",
            "The confidential base rule has beautiful intersection properties.",
            "Why, well, if both of these factors are zero, then you get a zero.",
            "If one is 0, you get a zero, and if both are nonzero, you don't get a zero.",
            "It's sort of like an end calculation."
        ],
        [
            "And this happens again.",
            "If this is your green ellipse, and this is your legacy blue ellipse, and this is your green ellipse, then the result lies in the intersection.",
            "So this is 1 positive definite matrix, another one then this ex log formula.",
            "The result is this.",
            "If you, well, there is the logs of zero are not defined, so things go bust.",
            "But it turns out.",
            "It's still well behaved and you can find things as a limit and you know it's it's the usual thing you do with relative entropies as well, and probably sauthier.",
            "Everything happens to be well behaved.",
            "You can define things as a limit.",
            "I won't get into that, but the point is that you can define this operation then, which is essentially this except for the fact that there's an instability at zero that they can be fixed.",
            "Then the new base will actually looks very similar to the old base rule.",
            "How did I come up with this update?",
            "OK, what kind of skipping?",
            "This way."
        ],
        [
            "Diversions plus eight times loss.",
            "I have happened to do this for very long time, and one of my favorite divergences.",
            "Of course, the relative entropy.",
            "Anne."
        ],
        [
            "So if you let me derive normal Bayes rule, normal Bayes rule, you can derive as a relative entropy to the prior.",
            "This is a relative entropy that require gamma.",
            "I am a parameter vectors to minimize over their probability vector, right?",
            "So I'm trying to find the probability the posterior I'm trying to find the probability vector so I do a relative entropy to the prior and then subtract off an expected log likelihood using the same coefficients and I trade this off by this ADA parameter.",
            "This is also gone.",
            "It's the end of the day.",
            "So it turns out if you minimize this thing and said 80 = 1, you get the normal basal and actually show you what's involved, at least in the calculation."
        ],
        [
            "What you do is you write down your relative entropy.",
            "Expected log likelihood and you put in a Gration in for the fact that things have the coefficients have to sum to one check derivatives.",
            "You get this X formula and then you assure the things.",
            "This is the issue that thing sum to one.",
            "You enforce this constraint.",
            "Then you have this formula.",
            "You said 80 = 1 and then you see the Bayes rule.",
            "So how did they get the other one?",
            "Actually I found out that Zellner does it that way too.",
            "Peter Bartlett told me pointed out these papers."
        ],
        [
            "Anne.",
            "So how do I get now?",
            "The fancier base rule?",
            "Well, I use a different Bregman divergences.",
            "I use a quantum relative entropy.",
            "It studio magaki.",
            "The corresponding entropy is to define Norman.",
            "And it looks looks like this.",
            "You have your parameter, which is now a density matrix.",
            "You measure these divergences and they have a fancier.",
            "Mixture loss you minimize this.",
            "You have to learn how to do derivatives, which I didn't know how to do of matrices you said 8 = 1 and then you get this funny X lock formula.",
            "This one.",
            "This form."
        ],
        [
            "Pops out and it makes sure that everything is well behaved.",
            "It makes sure.",
            "That you treat your parameters the right way and that you know you preserve positive definiteness.",
            "Now, I haven't shown you many things.",
            "One of the reasons why I'm doing this is.",
            "Found the balance generalized everything so."
        ],
        [
            "It goes through as before but with slightly different slight caveats.",
            "So for instance, look at the log of your data in the normal setting, you can write it at this way, and then you can throw away all but one term, and then you sort of compare against the log likelihood of the ice model times the prior on the ICE model.",
            "And that sort of.",
            "The map of bound for the maximum posterior upper story and this bound generalizes to the Matrix case as well.",
            "You get that jet.",
            "The log of the generalized probability is at most as big as now you look at these matrices in a certain direction.",
            "The direction of the log likelihood of the data in direction N minus the direction of the likelihood of the prior indirection, M. So all these things generalize, and it's completely pretty math.",
            "An OK, I haven't shown you what's the meaning of all of these variables are and it turns out what we have to do.",
            "But these kinds of things, where did they come?"
        ],
        [
            "Well, turns out we had developed a whole new probability calculus.",
            "And we sort of, let's say, 3/4 done.",
            "Um so."
        ],
        [
            "Joints you have to think about how to define joints.",
            "Normally joints between two events are just a probability solution on the pairs.",
            "Now I have to have spaces A&A Space B and I have to define a tensor product and define a density matrix on the tensor product space."
        ],
        [
            "Anne.",
            "I have to figure out how to find probabilities in this joint space, so remember, this one was defined as the density on the A space Times 8 times.",
            "8 * A transpose question is what is the dyett that goes in here so?"
        ],
        [
            "We try to have to learn about chronic products.",
            "It's a little bit too much right before dinner."
        ],
        [
            "But you can have these jointly specified dyads and then define the probability this way, but not all.",
            "All.",
            "Dietze in this, in this subspace AB can be written this way.",
            "Lots of interesting things."
        ],
        [
            "Anyway, we have calc calculus if conditionals, marginalization, theme of total probability, and all of that, and here."
        ],
        [
            "Sample rules you.",
            "Have the density on a joint and then you have to define a partial trace to get the density on just one subspace.",
            "This is how you kind of project out one dimension, etc.",
            "There's conditional formulas.",
            "This formula was known.",
            "Forget about this.",
            "This formula was known by certain atoms.",
            "Here's the course theorem of total probability using that funny operation.",
            "Here's our base rule.",
            "These things were before and then we build a whole calculus around it.",
            "This is another baseball.",
            "It goes on and on.",
            "It gets more complicated than the diagonal case.",
            "But very interesting, I think and.",
            "I have been you."
        ],
        [
            "Using this calculus sort of to do this online algorithm for principal component analysis that I talked about in my last talk implicitly in some way, there was only interested in expectations of the fancy math didn't come out, but if I would use proof tail bounds for that algorithm, then I believe I would use.",
            "I would need this kind of calculus.",
            "So with that and this picture so you remember it.",
            "I.",
            "Wish you a nice dinner."
        ],
        [
            "Unless you have questions.",
            "Any questions?",
            "Yes, that was it.",
            "OK, yes so.",
            "Is there so there is other generalization of prevailing out there and.",
            "Can you?",
            "Present some of those other generic generic prioritization as a special cable show.",
            "You're generally like fuzzy logic or something like that.",
            "No, I don't.",
            "I actually haven't studied those, and I have sort of experience with these divergences and with online learning.",
            "And this was the natural thing that fell in place.",
            "And then I because of work with code Suda and gonna Rach I.",
            "Developed one of those algorithms that were the parameter was a positive definite matrix pseudo worked on learning graphs for bio and he was trying to generalize things so we naturally ran into this thing and then I applied the same divergent that we discovered in that context to Bayes rule, and this is the natural fit that's that's where I come from.",
            "I don't know about other generalizations of probability, except the ones that we're investigating statistical physics.",
            "I have read some of those a little bit and our base rule is different from.",
            "The base rule that is sadistic.",
            "This would use because they are interested.",
            "This update is not what what the quantum physicist would do because they interested in unitary evolution, which means eigen various.",
            "They fix eigenvalue stay fixed, we change eigenvalues all the time.",
            "As a matter of fact, in the normal Bayes rule, when the eigensystem is fixed to unit, we only change eigenvalues, which is loggerheads with what the quantum physicists want to do.",
            "Yes.",
            "You have the same idea, yes.",
            "Yes.",
            "The systems are most likely almost alive.",
            "Doesn't have any.",
            "If they almost aligned, and if you wanted to exact calculations, you need to add think you need to do this X log thing.",
            "I don't know, you might be able to go through this formula and say if they are almost aligned then.",
            "Then yes, this plus is not going to come and eigensystem of both logs are going to be very similar, blah blah blah.",
            "And there's another I skipped over this.",
            "OK, there's a beautiful.",
            "This is beauty."
        ],
        [
            "To limit formula, this one here.",
            "Right, and it turns out that this in the limit is is again a positive definite matrix and I have pictures of this.",
            "So."
        ],
        [
            "Here this is 1 matrix is a pretty degenerate ellipse, and the variance plot around it is.",
            "It goes very far out because this is quite skinny.",
            "Here's the other ellipse.",
            "The variance is positive blue.",
            "The blue significance says it's positive.",
            "If you multiply SMT, you don't have a positive definite matrix.",
            "You get a negative variance.",
            "This U times U transpose times you attempts to matrix time Hugh is negative, so you get these negative ears.",
            "If you do the limit the years go away and I'll show you in pictures what happens.",
            "So."
        ],
        [
            "So this is S * T. You know the the this supposed to be along the longest axis, but it's not so.",
            "So there's negative years and it's not aligned properly if you take a sqrt X sqrt T * A ^2 Avesta squared of T, you do it sort of in four steps, like here, the ears get smaller and you could do it with infinitely many of these steps.",
            "Then you get the right operation.",
            "This is related to Feynman's path integrals.",
            "Yeah.",
            "Very pretty map.",
            "OK, any other questions?",
            "Yes, yes.",
            "A or computing exactly this time is product.",
            "Yes, if.",
            "Yeah we have a formula so that you can compute this thing, but it's at least."
        ],
        [
            "Order N cubed it.",
            "So now it's order N cubed because you have to compute eigen decompositions.",
            "And.",
            "It's expensive at this point.",
            "I've been trying to find ways to.",
            "To make it cheaper, the size of the matrix is order N squared, but the computation costs N ^3.",
            "So.",
            "One we've been serving it.",
            "You don't need to use a limit.",
            "There's another formula where you.",
            "Will you do some tricks?",
            "And.",
            "Yeah, when you take a log what you do for some reason is.",
            "When they see Rose, you just keep those zeros at zeros.",
            "You don't put an Infinity there, which are negative Infinity there, which should be and then you do a little bit of projections and it works out and you get a formula that is a little bit messier than this.",
            "That that involves exponentials and logs and.",
            "It is correct and can be coded computed in order in cube time.",
            "I can show you the formula.",
            "It also was known in some way by statistical physicists.",
            "We found it and it in great generality.",
            "OK.",
            "So I'm I'm not.",
            "I'm trying to steal the math that these guys got and do Bayesian inference with it.",
            "Questions.",
            "OK so yeah.",
            "That's wrong, yes.",
            "So I mean, finding the bedroom is not something you can make very much anyway.",
            "For all that you want any base for that.",
            "You want anyone.",
            "So why is the best one that you propose as there then?",
            "OK, well, because it goes back to very old principles, it goes back to a minimum relative entropy principle, and that is not so arbitrary all.",
            "All natural distributions are defined based on the minimum relative entropy principle and the base will itself is also defined, and that way you can define it."
        ],
        [
            "That way, and it's huge philosophical discussions on why it is the right rule, but this is not this is not.",
            "This is not an arbitrary thing that you can define the rule.",
            "This way."
        ],
        [
            "I am basically.",
            "Leverage of that fact.",
            "And just plug in here, the quantum relative entropy and the corresponding Lausanne boom it came out, I mean.",
            "I don't know.",
            "And then then then everything sort of fits in place, which is another hint that it's the right thing.",
            "But of course eventually you have to basically ask this question again in a year from now, here from that.",
            "OK.",
            "I have hints that using this relative, this relative entropy, the quantum relative entropy has beautiful properties.",
            "If you want to learn something about subspaces.",
            "It's just the right thing, and it's essentially because of the intersection property."
        ],
        [
            "I basically have an algorithm that generalizes the window algorithm too.",
            "To to the setting and the math is easy now because we know the techniques, but what you can do with it is quite intriguing.",
            "You basically can be as good as a subspace.",
            "You can learn a subspace.",
            "Even though you cannot find it, it's NP complete, but you can predict as well as the subspace.",
            "It's quite curious.",
            "More question.",
            "Yes.",
            "Simple question about this generalized probability.",
            "So you are saying that for disjoint events the probability stuff.",
            "In the normal case.",
            "In the base, in the normal case, the normal probability distribution case disjoint events, some, that's the axioms of probability."
        ],
        [
            "So then they said some intersection property, that formula right there is a generalization of that formula.",
            "Yeah, So what happens is?",
            "Yes, yes there is a space for the Intersect.",
            "What is what is the normal formula transferring to the probability of a Union B is the probability of A plus the union of B minus the probability of an intersection.",
            "And there's a related formula.",
            "I don't have it in my head for this space.",
            "For this thing, you have to intuitively what's underlying the normal probability theory is a lattice lattice with union an intersection and the corresponding lattice on the dietze on the directions is.",
            "Orthogonality and span.",
            "OK, it's it's.",
            "Been around for 60 years.",
            "Except that we.",
            "As machine learning statisticians haven't been using it.",
            "And I would have never looked at it.",
            "I mean, I didn't want to work on quantum stuff because I work already on pretty strange things.",
            "I don't need another strange thing.",
            "So about, it turns out you kind of you.",
            "You trust the math that you know, and then you kind of slowly slip into it.",
            "And then, oh wow, it's really beautiful, but I've never met Schrodinger's cat.",
            "OK. Any other questions?",
            "Yes.",
            "Are their intuitions on what's the independent events are in your days?",
            "Independence has to do with this Chronicle thing that the couple cronaca di coupling is very natural.",
            "Yeah, we we.",
            "We wrote a UI paper which is a short eight page thing and now we're working on a long version.",
            "OK, we're working on it.",
            "Pretty tough going.",
            "Other questions.",
            "Use a generalization.",
            "Generally, that means there's some.",
            "It has everything, the probability yes, mind more so.",
            "Person.",
            "Taking something to probability transfer them into this major situation.",
            "What is the animal?",
            "The end more is if you look at it this this question in the context of the Bayes rule.",
            "The normal Bayes rule is the diagonal case.",
            "Right, you can write it this way."
        ],
        [
            "Now what we do is we stick in a new eigensystem here and then we cannot do products anymore.",
            "We cannot do products, and we have that log.",
            "I have to do that log X formula to make sure that the resulting posterior is still a density matrix.",
            "So if the two guys have the same eigensystem, the two matrices no problem can just operate on the diagonal.",
            "But if you are two matrices, have a different eigen system, then you have to blend the different eigen systems in some extremely funny way.",
            "Via this"
        ],
        [
            "First taking the log.",
            "Adding and going back.",
            "This is classic of all Bregman type updates.",
            "You first apply the link function, which now happens to be a matrix.",
            "Log in that other domain you do your addition.",
            "It's the expectation parameter domain and then you go back.",
            "I think I'm getting hints that I'm supposed to stop so you can ask me this more questions over a beer.",
            "OK, thank you.",
            "So we know that we all know that there has been work overload already, so even he keep one more research.",
            "Still can only give him another.",
            "The best thing is never again.",
            "Thank you very much.",
            "Thank you, I look at this.",
            "Need.",
            "Oh, it's machine learning summer school Manford woman OK good."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, last research talk of today and most like this is pretty wild stuff because I'm going to generalize the probability theory the way quantum physicists do and most likely going to be partitioned into two clusters, either going to perk up or fall asleep, But anyway.",
                    "label": 0
                },
                {
                    "sent": "So that's me and Tim joint work with Team Accouchement.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I'm going to talk a little bit about matrix algebra, and then I'm going to generalize probability theory.",
                    "label": 0
                },
                {
                    "sent": "And then develop a generalized base rule.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so symmetric positive definite.",
                    "label": 0
                },
                {
                    "sent": "The main object is going to be density matrices, which are symmetric matrices.",
                    "label": 0
                },
                {
                    "sent": "They're going to be positive definite.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this condition this term?",
                    "label": 0
                },
                {
                    "sent": "I'm going to lay down interpret as a variance.",
                    "label": 0
                },
                {
                    "sent": "The variance is positive, the variance in direction you and that race which is the sum of diagonal elements is 1.",
                    "label": 1
                },
                {
                    "sent": "I'm going to argue that these are natural generalizations of probability theory.",
                    "label": 0
                },
                {
                    "sent": "They are the mainstay of statistical physics and quantum physics.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is?",
                    "label": 0
                },
                {
                    "sent": "What is a symmetric matrix due to the unit ball?",
                    "label": 1
                },
                {
                    "sent": "If you multiply the unit ball by symmetric matrix, you grab it at the eigenvalues and stretch it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I showed the eigen values the eigen directions, the eigen directions and then you stretch in this direction.",
                    "label": 0
                },
                {
                    "sent": "And you stretch in this direction, or you compress here and you stretch here anyway, the eye.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Values are the ones where the direction is preserved.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this plot.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I showed what happened to the unit ball.",
                    "label": 1
                },
                {
                    "sent": "This point here Blue Point is just moved out.",
                    "label": 1
                },
                {
                    "sent": "And this new point is smooth in a straight line.",
                    "label": 0
                },
                {
                    "sent": "And that's the can.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission for the icon for the definition of an eigenvalue vector.",
                    "label": 1
                },
                {
                    "sent": "Now symmetric positive definite matrix have an eigendecomposition OK so you can write that matrix as the script is an orthogonal matrix of the eigenvectors.",
                    "label": 1
                },
                {
                    "sent": "This is a diagonal memory matrix of the eigenvalues and then the orthogonal matrix transpose.",
                    "label": 0
                },
                {
                    "sent": "This way you can rewrite this decomposition also this way and now notice that he is set the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "These are the elements of the diagonal matrix and here sit there.",
                    "label": 0
                },
                {
                    "sent": "Outer product formed by the eigen vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, symmetric positive Denson matrices have trace one, so the eigenvalues sum to one and they also non negative.",
                    "label": 0
                },
                {
                    "sent": "So you see this, this is kind of a generalization of a mixture.",
                    "label": 1
                },
                {
                    "sent": "This is a probability vector and you mix these outer products they called dietze.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pay the dietze.",
                    "label": 0
                },
                {
                    "sent": "I can write as a degenerate ellipse.",
                    "label": 0
                },
                {
                    "sent": "It's especially lips that has only one non zero eigen direction and all the other ones are zero and now.",
                    "label": 0
                },
                {
                    "sent": "This mixture.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This mixture I.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can write this way you know, .2 times something like this point.",
                    "label": 0
                },
                {
                    "sent": "3 times something at this point five times this dyett that forms a nice lips and then this ellipse has these axis which correspond to the eigen direction can also be decomposed.",
                    "label": 0
                },
                {
                    "sent": "This way, using the eigen directions as the dyads.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you can always see you can decompose a ellipse in many ways, but there always is one in two orthogonal dyads, and those are the eigen directions.",
                    "label": 0
                },
                {
                    "sent": "And there's always an suffice.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so another thing that I want to point out is.",
                    "label": 0
                },
                {
                    "sent": "That you can view a symmetric matrix.",
                    "label": 0
                },
                {
                    "sent": "Any symmetric positive matrix as a covariance matrix, some some covariance of some cost function, and then if you look at the variance of the cost in a certain direction, it's one of those outer products.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to use this outer product a lot.",
                    "label": 0
                },
                {
                    "sent": "I will plot it for you in a moment, but before I do that, let me rewrite it.",
                    "label": 0
                },
                {
                    "sent": "I can rewrite.",
                    "label": 0
                },
                {
                    "sent": "This is a number, so it's a trace.",
                    "label": 0
                },
                {
                    "sent": "Also and then the trace cycles so you can write it this way.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let me plot this variance.",
                    "label": 0
                },
                {
                    "sent": "Here's my lips, right?",
                    "label": 0
                },
                {
                    "sent": "AU and then if I go in there and if I plot the variance in this direction, I will end up here.",
                    "label": 0
                },
                {
                    "sent": "And you see, sort of.",
                    "label": 0
                },
                {
                    "sent": "The variance is the largest along the maximum eigenvalue and the smallest along the minimum eigen direction.",
                    "label": 1
                },
                {
                    "sent": "OK, OK, so now so this is a little bit of linear algebra and Mariana already did some of this and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I'm going to talk a little bit about generalization.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shens about of probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So assume for a moment let me review normal probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So assume you have five points in the finite dimensional case.",
                    "label": 0
                },
                {
                    "sent": "These are our mental events.",
                    "label": 0
                },
                {
                    "sent": "Imagine you have a 5 sided die sort of 65 OK, and then this would be an event.",
                    "label": 0
                },
                {
                    "sent": "Right now this is a distribution, just gives you the probabilities of the elementary event, and then if you have one of those events, then by summing all the probabilities that are one where you were the elementary element is in the set.",
                    "label": 0
                },
                {
                    "sent": "Then you get the probability of the event.",
                    "label": 0
                },
                {
                    "sent": "You can also define expectations.",
                    "label": 0
                },
                {
                    "sent": "So this is the normal way normal probability distribution.",
                    "label": 0
                },
                {
                    "sent": "How do we generate?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Justice, So what we're going to do?",
                    "label": 0
                },
                {
                    "sent": "First of all, elementary event is going to be one of those dyads.",
                    "label": 0
                },
                {
                    "sent": "We have finite dimension.",
                    "label": 0
                },
                {
                    "sent": "We have now infinitely many dyads.",
                    "label": 0
                },
                {
                    "sent": "OK, there are these things and what's an event in event is?",
                    "label": 0
                },
                {
                    "sent": "Anne again.",
                    "label": 0
                },
                {
                    "sent": "Characterized by one of those vectors, but the vectors are now the eigenvalues and there's an eigensystem out there, too.",
                    "label": 0
                },
                {
                    "sent": "So you see here I had something like this and now the same thing is along the diagonal of this matrix.",
                    "label": 0
                },
                {
                    "sent": "But you also have an eigensystem, so this is a projection matrix.",
                    "label": 0
                },
                {
                    "sent": "Projects down to a subspace.",
                    "label": 0
                },
                {
                    "sent": "What's the distribution?",
                    "label": 0
                },
                {
                    "sent": "A density matrix?",
                    "label": 0
                },
                {
                    "sent": "It's the same.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing as this, but now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You also have an eigensystem and this is a density matrix.",
                    "label": 0
                },
                {
                    "sent": "So the game that we're playing is we not just learning a probability vector, which is now a vector of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "We also learn the eigen direction directions as well.",
                    "label": 0
                },
                {
                    "sent": "It's quite curious.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I skipped something.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "So an the density matrix assigns a generalized probability to Dyett you.",
                    "label": 0
                },
                {
                    "sent": "And this is what how the statistical physicists do it, and the curious thing is that if you sum this probability over an orthogonal set of direction, it sums to one.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a probability.",
                    "label": 0
                },
                {
                    "sent": "This is Uira unit, a set of orthogonal directions.",
                    "label": 0
                },
                {
                    "sent": "I sum up these probabilities.",
                    "label": 0
                },
                {
                    "sent": "I trace is linear.",
                    "label": 0
                },
                {
                    "sent": "Can put this aside inside and if when these, organal and any of them then this seems to be identity.",
                    "label": 0
                },
                {
                    "sent": "So in the trace of a was one.",
                    "label": 0
                },
                {
                    "sent": "So this sums to one.",
                    "label": 0
                },
                {
                    "sent": "OK, let me.",
                    "label": 0
                },
                {
                    "sent": "To get this across to you that there's something really strange going on, let's define what the uniform distribution is.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "This would be the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "It's the 1 / 10 times the identity matrix.",
                    "label": 0
                },
                {
                    "sent": "Notice that the traces one because you have N eigenvalues of size 1 / N good, so this is a distribution viewed as an ellipse.",
                    "label": 0
                },
                {
                    "sent": "Now any unit direction has probability 1 / N. You have uncountably infinitely many.",
                    "label": 0
                },
                {
                    "sent": "So what sense does this sum to one?",
                    "label": 0
                },
                {
                    "sent": "Well, if you take any orthogonal set of them.",
                    "label": 0
                },
                {
                    "sent": "Of course I said end of them N * 1 / N is 1, so the probability the total probability of any set of orthogonal direction sums to one.",
                    "label": 0
                },
                {
                    "sent": "So this is very unusual way using sort of linear algebra.",
                    "label": 0
                },
                {
                    "sent": "What happened before is if you had two events and they were disjoint, then the probability was some with some.",
                    "label": 0
                },
                {
                    "sent": "Now if you have two directions and they are orthogonal, then their probabilities sum.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, you can do also defined probabilities of events, so if this is a density matrix and this is one of those matrixes with 01 eigenvalues, then you can write the trace.",
                    "label": 0
                },
                {
                    "sent": "This way it's linear and then it's kind of like the mixture of the variance along the eigen direction.",
                    "label": 0
                },
                {
                    "sent": "Make various other interpretations as quantum measurements, but I'm not going to get into that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here comes the main theorem.",
                    "label": 0
                },
                {
                    "sent": "Gleason's theorem which says.",
                    "label": 0
                },
                {
                    "sent": "Any scalar function from unit directions to R. Is a generalized probability measure if the probabilities you assign like between zero and one, and then if you have an orthogonal basis, they sum to one.",
                    "label": 1
                },
                {
                    "sent": "For any orthogonal basis, if you have these two properties, then Gleason showed that the dimension is bigger than three.",
                    "label": 0
                },
                {
                    "sent": "It always is described as one of those traces.",
                    "label": 0
                },
                {
                    "sent": "There is a matrix that defines this measure.",
                    "label": 0
                },
                {
                    "sent": "It's a very deep theorem.",
                    "label": 0
                },
                {
                    "sent": "One Direction is easy, the other direction involves heavy group theory.",
                    "label": 0
                },
                {
                    "sent": "There's a slight slightly different if you do slightly different definitions here.",
                    "label": 0
                },
                {
                    "sent": "You can actually understand the proof, but police is proof is very complicated.",
                    "label": 0
                },
                {
                    "sent": "Quantum physicists usually don't use this theorem for real spaces, they use it for complex spaces, but it's already completely sophisticated for real spaces.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in the Convention, so now I want to get to Bayes rule OK.",
                    "label": 1
                },
                {
                    "sent": "Baseball OK so base setup is usually you have when the finite dimensional case we have.",
                    "label": 0
                },
                {
                    "sent": "Imagine you have five models, right?",
                    "label": 0
                },
                {
                    "sent": "And the probability of the of the model is that Emma is chosen is proportional to the prior and then you have some assume their coins.",
                    "label": 0
                },
                {
                    "sent": "So the probability prior probability that Emma is the true model is PMI.",
                    "label": 1
                },
                {
                    "sent": "You see a coin flip and then you get the data likelihood probability of Y given in my and this is the theorem of total probability.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you this kind of theorems for density matrices.",
                    "label": 1
                },
                {
                    "sent": "Or is the other one?",
                    "label": 0
                },
                {
                    "sent": "OK, well that's it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If the normal Bayes rule normal Bayes rule would be, here's your prior, so certain probabilities for your file for your.",
                    "label": 1
                },
                {
                    "sent": "5 coins certain data likelihoods, and now what happens is you take the tool the red and the green in your componentwise.",
                    "label": 0
                },
                {
                    "sent": "Multiplying re normalize.",
                    "label": 0
                },
                {
                    "sent": "This is your first posterior.",
                    "label": 0
                },
                {
                    "sent": "Great service, this is your first posterior.",
                    "label": 0
                },
                {
                    "sent": "Notice that this was the highest data likelihood component, so this one is pulled up compared to here.",
                    "label": 0
                },
                {
                    "sent": "If you apply the same thing twice, it pulled up more 3 * 4 times.",
                    "label": 0
                },
                {
                    "sent": "So I see this update as a softmax calculation.",
                    "label": 0
                },
                {
                    "sent": "You want to get the best model.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's what the new rule is now.",
                    "label": 0
                },
                {
                    "sent": "I'm not showing you yet what the meaning of all these things are.",
                    "label": 0
                },
                {
                    "sent": "It's kind of mind boggling, but my prior is now density matrices.",
                    "label": 0
                },
                {
                    "sent": "My data likelihood is also a symmetric positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "You cannot multiply them somehow.",
                    "label": 0
                },
                {
                    "sent": "You have to take a matrix log, which means you take a decomposition into take log of the diagonal you add, and then you exponentiate that's matrix exponential again, you take a decomposition of this inside matrix and take exponential of the diagonal.",
                    "label": 0
                },
                {
                    "sent": "You get a matrix symmetric positive.",
                    "label": 0
                },
                {
                    "sent": "Definite matrix and then you divide it and then you have a matrix of trace one which is again a density matrix ha in picture.",
                    "label": 0
                },
                {
                    "sent": "This is what happens, it's completely mindboggling.",
                    "label": 0
                },
                {
                    "sent": "Here's the prior.",
                    "label": 0
                },
                {
                    "sent": "This is the data likelihood.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is the first update.",
                    "label": 0
                },
                {
                    "sent": "You see that this was the prior and then the first update goes towards the green.",
                    "label": 0
                },
                {
                    "sent": "It actually goes towards the largest axis of the green, second one third one 4th one 5th 161 until it homes into the axis.",
                    "label": 0
                },
                {
                    "sent": "That correspond to the largest eigenvalue, so.",
                    "label": 0
                },
                {
                    "sent": "Here I home in to concentrate on the largest component with my probability vector.",
                    "label": 0
                },
                {
                    "sent": "And here I home into the longest axis.",
                    "label": 0
                },
                {
                    "sent": "So I see this as a soft maximum eigenvalue calculation and this is the generalization of the Bayes rule for density matrix.",
                    "label": 1
                },
                {
                    "sent": "I claim this big contentions physicists try to do it differently.",
                    "label": 0
                },
                {
                    "sent": "I submitted a bunch of papers and I got huge feedback.",
                    "label": 0
                },
                {
                    "sent": "Everybody has a quantum to say about quantum ice.",
                    "label": 0
                },
                {
                    "sent": "And I got 10 pages of referee reports with mind boggling.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I removed a lot of the quantum language, so not to annoy anybody.",
                    "label": 0
                },
                {
                    "sent": "So why this funny operation and where does it come from?",
                    "label": 0
                },
                {
                    "sent": "You take you cannot multiply these two matrices is going to be positive definite.",
                    "label": 0
                },
                {
                    "sent": "This is going to be positive definite.",
                    "label": 0
                },
                {
                    "sent": "I cannot multiply them because multiplication of two positive definite matrix is not positive definite, not in general.",
                    "label": 1
                },
                {
                    "sent": "So what you do is you take a log first, which turns you into.",
                    "label": 0
                },
                {
                    "sent": "You still have a symmetric matrix, right?",
                    "label": 0
                },
                {
                    "sent": "Because take the eigenvectors, take a log that the logs are now negative, but who cares?",
                    "label": 0
                },
                {
                    "sent": "It's still symmetric.",
                    "label": 0
                },
                {
                    "sent": "This is still symmetric symmetric things.",
                    "label": 0
                },
                {
                    "sent": "You can add wow, you add them.",
                    "label": 0
                },
                {
                    "sent": "And then you exponentiate.",
                    "label": 0
                },
                {
                    "sent": "But if you exponentiate a symmetric matrix, you get positive and then you divide by trees.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to drive this update.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, let me point out that the normal Basil is a special case.",
                    "label": 1
                },
                {
                    "sent": "Why you take your data, likelihood, you write it.",
                    "label": 0
                },
                {
                    "sent": "You write it as a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "You right?",
                    "label": 0
                },
                {
                    "sent": "This is the diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "And now I claim this is the posterior right because if I multiply this, it's componentwise multiplication, dividing their trace.",
                    "label": 0
                },
                {
                    "sent": "So this is the old case, the eigensystem of both thing is the same, whereas here.",
                    "label": 1
                },
                {
                    "sent": "The eigensystem of both matrices is not the same, and then you need this.",
                    "label": 0
                },
                {
                    "sent": "I claim you need this massive rule.",
                    "label": 0
                },
                {
                    "sent": "Too complicated for this short talk, too complicated so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The confidential base rule has beautiful intersection properties.",
                    "label": 0
                },
                {
                    "sent": "Why, well, if both of these factors are zero, then you get a zero.",
                    "label": 0
                },
                {
                    "sent": "If one is 0, you get a zero, and if both are nonzero, you don't get a zero.",
                    "label": 0
                },
                {
                    "sent": "It's sort of like an end calculation.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this happens again.",
                    "label": 0
                },
                {
                    "sent": "If this is your green ellipse, and this is your legacy blue ellipse, and this is your green ellipse, then the result lies in the intersection.",
                    "label": 1
                },
                {
                    "sent": "So this is 1 positive definite matrix, another one then this ex log formula.",
                    "label": 0
                },
                {
                    "sent": "The result is this.",
                    "label": 0
                },
                {
                    "sent": "If you, well, there is the logs of zero are not defined, so things go bust.",
                    "label": 1
                },
                {
                    "sent": "But it turns out.",
                    "label": 0
                },
                {
                    "sent": "It's still well behaved and you can find things as a limit and you know it's it's the usual thing you do with relative entropies as well, and probably sauthier.",
                    "label": 0
                },
                {
                    "sent": "Everything happens to be well behaved.",
                    "label": 1
                },
                {
                    "sent": "You can define things as a limit.",
                    "label": 0
                },
                {
                    "sent": "I won't get into that, but the point is that you can define this operation then, which is essentially this except for the fact that there's an instability at zero that they can be fixed.",
                    "label": 0
                },
                {
                    "sent": "Then the new base will actually looks very similar to the old base rule.",
                    "label": 0
                },
                {
                    "sent": "How did I come up with this update?",
                    "label": 0
                },
                {
                    "sent": "OK, what kind of skipping?",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diversions plus eight times loss.",
                    "label": 0
                },
                {
                    "sent": "I have happened to do this for very long time, and one of my favorite divergences.",
                    "label": 0
                },
                {
                    "sent": "Of course, the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you let me derive normal Bayes rule, normal Bayes rule, you can derive as a relative entropy to the prior.",
                    "label": 1
                },
                {
                    "sent": "This is a relative entropy that require gamma.",
                    "label": 0
                },
                {
                    "sent": "I am a parameter vectors to minimize over their probability vector, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm trying to find the probability the posterior I'm trying to find the probability vector so I do a relative entropy to the prior and then subtract off an expected log likelihood using the same coefficients and I trade this off by this ADA parameter.",
                    "label": 0
                },
                {
                    "sent": "This is also gone.",
                    "label": 0
                },
                {
                    "sent": "It's the end of the day.",
                    "label": 0
                },
                {
                    "sent": "So it turns out if you minimize this thing and said 80 = 1, you get the normal basal and actually show you what's involved, at least in the calculation.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What you do is you write down your relative entropy.",
                    "label": 0
                },
                {
                    "sent": "Expected log likelihood and you put in a Gration in for the fact that things have the coefficients have to sum to one check derivatives.",
                    "label": 0
                },
                {
                    "sent": "You get this X formula and then you assure the things.",
                    "label": 0
                },
                {
                    "sent": "This is the issue that thing sum to one.",
                    "label": 0
                },
                {
                    "sent": "You enforce this constraint.",
                    "label": 0
                },
                {
                    "sent": "Then you have this formula.",
                    "label": 0
                },
                {
                    "sent": "You said 80 = 1 and then you see the Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "So how did they get the other one?",
                    "label": 0
                },
                {
                    "sent": "Actually I found out that Zellner does it that way too.",
                    "label": 0
                },
                {
                    "sent": "Peter Bartlett told me pointed out these papers.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So how do I get now?",
                    "label": 0
                },
                {
                    "sent": "The fancier base rule?",
                    "label": 0
                },
                {
                    "sent": "Well, I use a different Bregman divergences.",
                    "label": 0
                },
                {
                    "sent": "I use a quantum relative entropy.",
                    "label": 0
                },
                {
                    "sent": "It studio magaki.",
                    "label": 0
                },
                {
                    "sent": "The corresponding entropy is to define Norman.",
                    "label": 0
                },
                {
                    "sent": "And it looks looks like this.",
                    "label": 0
                },
                {
                    "sent": "You have your parameter, which is now a density matrix.",
                    "label": 0
                },
                {
                    "sent": "You measure these divergences and they have a fancier.",
                    "label": 0
                },
                {
                    "sent": "Mixture loss you minimize this.",
                    "label": 0
                },
                {
                    "sent": "You have to learn how to do derivatives, which I didn't know how to do of matrices you said 8 = 1 and then you get this funny X lock formula.",
                    "label": 0
                },
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "This form.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pops out and it makes sure that everything is well behaved.",
                    "label": 0
                },
                {
                    "sent": "It makes sure.",
                    "label": 0
                },
                {
                    "sent": "That you treat your parameters the right way and that you know you preserve positive definiteness.",
                    "label": 0
                },
                {
                    "sent": "Now, I haven't shown you many things.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons why I'm doing this is.",
                    "label": 0
                },
                {
                    "sent": "Found the balance generalized everything so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It goes through as before but with slightly different slight caveats.",
                    "label": 0
                },
                {
                    "sent": "So for instance, look at the log of your data in the normal setting, you can write it at this way, and then you can throw away all but one term, and then you sort of compare against the log likelihood of the ice model times the prior on the ICE model.",
                    "label": 0
                },
                {
                    "sent": "And that sort of.",
                    "label": 0
                },
                {
                    "sent": "The map of bound for the maximum posterior upper story and this bound generalizes to the Matrix case as well.",
                    "label": 0
                },
                {
                    "sent": "You get that jet.",
                    "label": 0
                },
                {
                    "sent": "The log of the generalized probability is at most as big as now you look at these matrices in a certain direction.",
                    "label": 0
                },
                {
                    "sent": "The direction of the log likelihood of the data in direction N minus the direction of the likelihood of the prior indirection, M. So all these things generalize, and it's completely pretty math.",
                    "label": 0
                },
                {
                    "sent": "An OK, I haven't shown you what's the meaning of all of these variables are and it turns out what we have to do.",
                    "label": 0
                },
                {
                    "sent": "But these kinds of things, where did they come?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, turns out we had developed a whole new probability calculus.",
                    "label": 0
                },
                {
                    "sent": "And we sort of, let's say, 3/4 done.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joints you have to think about how to define joints.",
                    "label": 0
                },
                {
                    "sent": "Normally joints between two events are just a probability solution on the pairs.",
                    "label": 1
                },
                {
                    "sent": "Now I have to have spaces A&A Space B and I have to define a tensor product and define a density matrix on the tensor product space.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I have to figure out how to find probabilities in this joint space, so remember, this one was defined as the density on the A space Times 8 times.",
                    "label": 0
                },
                {
                    "sent": "8 * A transpose question is what is the dyett that goes in here so?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We try to have to learn about chronic products.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit too much right before dinner.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can have these jointly specified dyads and then define the probability this way, but not all.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "Dietze in this, in this subspace AB can be written this way.",
                    "label": 0
                },
                {
                    "sent": "Lots of interesting things.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, we have calc calculus if conditionals, marginalization, theme of total probability, and all of that, and here.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sample rules you.",
                    "label": 0
                },
                {
                    "sent": "Have the density on a joint and then you have to define a partial trace to get the density on just one subspace.",
                    "label": 0
                },
                {
                    "sent": "This is how you kind of project out one dimension, etc.",
                    "label": 0
                },
                {
                    "sent": "There's conditional formulas.",
                    "label": 0
                },
                {
                    "sent": "This formula was known.",
                    "label": 0
                },
                {
                    "sent": "Forget about this.",
                    "label": 0
                },
                {
                    "sent": "This formula was known by certain atoms.",
                    "label": 0
                },
                {
                    "sent": "Here's the course theorem of total probability using that funny operation.",
                    "label": 1
                },
                {
                    "sent": "Here's our base rule.",
                    "label": 0
                },
                {
                    "sent": "These things were before and then we build a whole calculus around it.",
                    "label": 0
                },
                {
                    "sent": "This is another baseball.",
                    "label": 0
                },
                {
                    "sent": "It goes on and on.",
                    "label": 0
                },
                {
                    "sent": "It gets more complicated than the diagonal case.",
                    "label": 0
                },
                {
                    "sent": "But very interesting, I think and.",
                    "label": 0
                },
                {
                    "sent": "I have been you.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using this calculus sort of to do this online algorithm for principal component analysis that I talked about in my last talk implicitly in some way, there was only interested in expectations of the fancy math didn't come out, but if I would use proof tail bounds for that algorithm, then I believe I would use.",
                    "label": 0
                },
                {
                    "sent": "I would need this kind of calculus.",
                    "label": 0
                },
                {
                    "sent": "So with that and this picture so you remember it.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Wish you a nice dinner.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unless you have questions.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Yes, that was it.",
                    "label": 0
                },
                {
                    "sent": "OK, yes so.",
                    "label": 0
                },
                {
                    "sent": "Is there so there is other generalization of prevailing out there and.",
                    "label": 0
                },
                {
                    "sent": "Can you?",
                    "label": 0
                },
                {
                    "sent": "Present some of those other generic generic prioritization as a special cable show.",
                    "label": 0
                },
                {
                    "sent": "You're generally like fuzzy logic or something like that.",
                    "label": 0
                },
                {
                    "sent": "No, I don't.",
                    "label": 0
                },
                {
                    "sent": "I actually haven't studied those, and I have sort of experience with these divergences and with online learning.",
                    "label": 0
                },
                {
                    "sent": "And this was the natural thing that fell in place.",
                    "label": 0
                },
                {
                    "sent": "And then I because of work with code Suda and gonna Rach I.",
                    "label": 0
                },
                {
                    "sent": "Developed one of those algorithms that were the parameter was a positive definite matrix pseudo worked on learning graphs for bio and he was trying to generalize things so we naturally ran into this thing and then I applied the same divergent that we discovered in that context to Bayes rule, and this is the natural fit that's that's where I come from.",
                    "label": 0
                },
                {
                    "sent": "I don't know about other generalizations of probability, except the ones that we're investigating statistical physics.",
                    "label": 0
                },
                {
                    "sent": "I have read some of those a little bit and our base rule is different from.",
                    "label": 0
                },
                {
                    "sent": "The base rule that is sadistic.",
                    "label": 0
                },
                {
                    "sent": "This would use because they are interested.",
                    "label": 0
                },
                {
                    "sent": "This update is not what what the quantum physicist would do because they interested in unitary evolution, which means eigen various.",
                    "label": 0
                },
                {
                    "sent": "They fix eigenvalue stay fixed, we change eigenvalues all the time.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact, in the normal Bayes rule, when the eigensystem is fixed to unit, we only change eigenvalues, which is loggerheads with what the quantum physicists want to do.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You have the same idea, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The systems are most likely almost alive.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have any.",
                    "label": 0
                },
                {
                    "sent": "If they almost aligned, and if you wanted to exact calculations, you need to add think you need to do this X log thing.",
                    "label": 0
                },
                {
                    "sent": "I don't know, you might be able to go through this formula and say if they are almost aligned then.",
                    "label": 0
                },
                {
                    "sent": "Then yes, this plus is not going to come and eigensystem of both logs are going to be very similar, blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "And there's another I skipped over this.",
                    "label": 0
                },
                {
                    "sent": "OK, there's a beautiful.",
                    "label": 0
                },
                {
                    "sent": "This is beauty.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To limit formula, this one here.",
                    "label": 0
                },
                {
                    "sent": "Right, and it turns out that this in the limit is is again a positive definite matrix and I have pictures of this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here this is 1 matrix is a pretty degenerate ellipse, and the variance plot around it is.",
                    "label": 0
                },
                {
                    "sent": "It goes very far out because this is quite skinny.",
                    "label": 0
                },
                {
                    "sent": "Here's the other ellipse.",
                    "label": 0
                },
                {
                    "sent": "The variance is positive blue.",
                    "label": 0
                },
                {
                    "sent": "The blue significance says it's positive.",
                    "label": 0
                },
                {
                    "sent": "If you multiply SMT, you don't have a positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "You get a negative variance.",
                    "label": 0
                },
                {
                    "sent": "This U times U transpose times you attempts to matrix time Hugh is negative, so you get these negative ears.",
                    "label": 0
                },
                {
                    "sent": "If you do the limit the years go away and I'll show you in pictures what happens.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is S * T. You know the the this supposed to be along the longest axis, but it's not so.",
                    "label": 0
                },
                {
                    "sent": "So there's negative years and it's not aligned properly if you take a sqrt X sqrt T * A ^2 Avesta squared of T, you do it sort of in four steps, like here, the ears get smaller and you could do it with infinitely many of these steps.",
                    "label": 0
                },
                {
                    "sent": "Then you get the right operation.",
                    "label": 0
                },
                {
                    "sent": "This is related to Feynman's path integrals.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Very pretty map.",
                    "label": 0
                },
                {
                    "sent": "OK, any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "A or computing exactly this time is product.",
                    "label": 0
                },
                {
                    "sent": "Yes, if.",
                    "label": 0
                },
                {
                    "sent": "Yeah we have a formula so that you can compute this thing, but it's at least.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Order N cubed it.",
                    "label": 0
                },
                {
                    "sent": "So now it's order N cubed because you have to compute eigen decompositions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It's expensive at this point.",
                    "label": 0
                },
                {
                    "sent": "I've been trying to find ways to.",
                    "label": 0
                },
                {
                    "sent": "To make it cheaper, the size of the matrix is order N squared, but the computation costs N ^3.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One we've been serving it.",
                    "label": 0
                },
                {
                    "sent": "You don't need to use a limit.",
                    "label": 0
                },
                {
                    "sent": "There's another formula where you.",
                    "label": 0
                },
                {
                    "sent": "Will you do some tricks?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah, when you take a log what you do for some reason is.",
                    "label": 0
                },
                {
                    "sent": "When they see Rose, you just keep those zeros at zeros.",
                    "label": 0
                },
                {
                    "sent": "You don't put an Infinity there, which are negative Infinity there, which should be and then you do a little bit of projections and it works out and you get a formula that is a little bit messier than this.",
                    "label": 0
                },
                {
                    "sent": "That that involves exponentials and logs and.",
                    "label": 0
                },
                {
                    "sent": "It is correct and can be coded computed in order in cube time.",
                    "label": 0
                },
                {
                    "sent": "I can show you the formula.",
                    "label": 0
                },
                {
                    "sent": "It also was known in some way by statistical physicists.",
                    "label": 0
                },
                {
                    "sent": "We found it and it in great generality.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I'm I'm not.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to steal the math that these guys got and do Bayesian inference with it.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "OK so yeah.",
                    "label": 0
                },
                {
                    "sent": "That's wrong, yes.",
                    "label": 0
                },
                {
                    "sent": "So I mean, finding the bedroom is not something you can make very much anyway.",
                    "label": 0
                },
                {
                    "sent": "For all that you want any base for that.",
                    "label": 0
                },
                {
                    "sent": "You want anyone.",
                    "label": 0
                },
                {
                    "sent": "So why is the best one that you propose as there then?",
                    "label": 0
                },
                {
                    "sent": "OK, well, because it goes back to very old principles, it goes back to a minimum relative entropy principle, and that is not so arbitrary all.",
                    "label": 0
                },
                {
                    "sent": "All natural distributions are defined based on the minimum relative entropy principle and the base will itself is also defined, and that way you can define it.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That way, and it's huge philosophical discussions on why it is the right rule, but this is not this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not an arbitrary thing that you can define the rule.",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I am basically.",
                    "label": 0
                },
                {
                    "sent": "Leverage of that fact.",
                    "label": 0
                },
                {
                    "sent": "And just plug in here, the quantum relative entropy and the corresponding Lausanne boom it came out, I mean.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "And then then then everything sort of fits in place, which is another hint that it's the right thing.",
                    "label": 0
                },
                {
                    "sent": "But of course eventually you have to basically ask this question again in a year from now, here from that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I have hints that using this relative, this relative entropy, the quantum relative entropy has beautiful properties.",
                    "label": 0
                },
                {
                    "sent": "If you want to learn something about subspaces.",
                    "label": 0
                },
                {
                    "sent": "It's just the right thing, and it's essentially because of the intersection property.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I basically have an algorithm that generalizes the window algorithm too.",
                    "label": 0
                },
                {
                    "sent": "To to the setting and the math is easy now because we know the techniques, but what you can do with it is quite intriguing.",
                    "label": 0
                },
                {
                    "sent": "You basically can be as good as a subspace.",
                    "label": 0
                },
                {
                    "sent": "You can learn a subspace.",
                    "label": 0
                },
                {
                    "sent": "Even though you cannot find it, it's NP complete, but you can predict as well as the subspace.",
                    "label": 0
                },
                {
                    "sent": "It's quite curious.",
                    "label": 0
                },
                {
                    "sent": "More question.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Simple question about this generalized probability.",
                    "label": 0
                },
                {
                    "sent": "So you are saying that for disjoint events the probability stuff.",
                    "label": 0
                },
                {
                    "sent": "In the normal case.",
                    "label": 0
                },
                {
                    "sent": "In the base, in the normal case, the normal probability distribution case disjoint events, some, that's the axioms of probability.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then they said some intersection property, that formula right there is a generalization of that formula.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what happens is?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes there is a space for the Intersect.",
                    "label": 0
                },
                {
                    "sent": "What is what is the normal formula transferring to the probability of a Union B is the probability of A plus the union of B minus the probability of an intersection.",
                    "label": 0
                },
                {
                    "sent": "And there's a related formula.",
                    "label": 0
                },
                {
                    "sent": "I don't have it in my head for this space.",
                    "label": 0
                },
                {
                    "sent": "For this thing, you have to intuitively what's underlying the normal probability theory is a lattice lattice with union an intersection and the corresponding lattice on the dietze on the directions is.",
                    "label": 0
                },
                {
                    "sent": "Orthogonality and span.",
                    "label": 0
                },
                {
                    "sent": "OK, it's it's.",
                    "label": 0
                },
                {
                    "sent": "Been around for 60 years.",
                    "label": 0
                },
                {
                    "sent": "Except that we.",
                    "label": 0
                },
                {
                    "sent": "As machine learning statisticians haven't been using it.",
                    "label": 0
                },
                {
                    "sent": "And I would have never looked at it.",
                    "label": 0
                },
                {
                    "sent": "I mean, I didn't want to work on quantum stuff because I work already on pretty strange things.",
                    "label": 0
                },
                {
                    "sent": "I don't need another strange thing.",
                    "label": 0
                },
                {
                    "sent": "So about, it turns out you kind of you.",
                    "label": 0
                },
                {
                    "sent": "You trust the math that you know, and then you kind of slowly slip into it.",
                    "label": 0
                },
                {
                    "sent": "And then, oh wow, it's really beautiful, but I've never met Schrodinger's cat.",
                    "label": 0
                },
                {
                    "sent": "OK. Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Are their intuitions on what's the independent events are in your days?",
                    "label": 0
                },
                {
                    "sent": "Independence has to do with this Chronicle thing that the couple cronaca di coupling is very natural.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we we.",
                    "label": 0
                },
                {
                    "sent": "We wrote a UI paper which is a short eight page thing and now we're working on a long version.",
                    "label": 0
                },
                {
                    "sent": "OK, we're working on it.",
                    "label": 0
                },
                {
                    "sent": "Pretty tough going.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Use a generalization.",
                    "label": 0
                },
                {
                    "sent": "Generally, that means there's some.",
                    "label": 0
                },
                {
                    "sent": "It has everything, the probability yes, mind more so.",
                    "label": 0
                },
                {
                    "sent": "Person.",
                    "label": 0
                },
                {
                    "sent": "Taking something to probability transfer them into this major situation.",
                    "label": 0
                },
                {
                    "sent": "What is the animal?",
                    "label": 0
                },
                {
                    "sent": "The end more is if you look at it this this question in the context of the Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "The normal Bayes rule is the diagonal case.",
                    "label": 0
                },
                {
                    "sent": "Right, you can write it this way.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what we do is we stick in a new eigensystem here and then we cannot do products anymore.",
                    "label": 0
                },
                {
                    "sent": "We cannot do products, and we have that log.",
                    "label": 0
                },
                {
                    "sent": "I have to do that log X formula to make sure that the resulting posterior is still a density matrix.",
                    "label": 0
                },
                {
                    "sent": "So if the two guys have the same eigensystem, the two matrices no problem can just operate on the diagonal.",
                    "label": 1
                },
                {
                    "sent": "But if you are two matrices, have a different eigen system, then you have to blend the different eigen systems in some extremely funny way.",
                    "label": 0
                },
                {
                    "sent": "Via this",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First taking the log.",
                    "label": 0
                },
                {
                    "sent": "Adding and going back.",
                    "label": 0
                },
                {
                    "sent": "This is classic of all Bregman type updates.",
                    "label": 0
                },
                {
                    "sent": "You first apply the link function, which now happens to be a matrix.",
                    "label": 0
                },
                {
                    "sent": "Log in that other domain you do your addition.",
                    "label": 0
                },
                {
                    "sent": "It's the expectation parameter domain and then you go back.",
                    "label": 0
                },
                {
                    "sent": "I think I'm getting hints that I'm supposed to stop so you can ask me this more questions over a beer.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So we know that we all know that there has been work overload already, so even he keep one more research.",
                    "label": 0
                },
                {
                    "sent": "Still can only give him another.",
                    "label": 0
                },
                {
                    "sent": "The best thing is never again.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Thank you, I look at this.",
                    "label": 0
                },
                {
                    "sent": "Need.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's machine learning summer school Manford woman OK good.",
                    "label": 0
                }
            ]
        }
    }
}