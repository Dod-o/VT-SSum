{
    "id": "yw2nyb2ylqswk25223e2eq637ihsaxpr",
    "title": "Machine Learning in the Cloud with GraphLab",
    "info": {
        "author": [
            "Carlos Guestrin, Computer Science Department, Carnegie Mellon University"
        ],
        "published": "Jan. 13, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_guestrin_kml/",
    "segmentation": [
        [
            "It is with great pleasure that I'm here for this workshop.",
            "I think it's a great important topic we've been working on for quite a bit, and I think it's really good that our community is putting some effort on it.",
            "I'm going to talk about today.",
            "Here is really work of my students, so they deserve all the credit.",
            "Yucheng Low J Gonzalez a Pocatello and at Ebix and I think most of them are here today.",
            "So bug them at the end and ask them questions.",
            "They do have a poster, I think right here to talk about more details and the issues that we're facing today.",
            "Machine learning one of them."
        ],
        [
            "Big issues I feel is the explosion in the size of the problems we're trying to solve and we see quite a bit of that today and we used to think that you know Wikipedia 30 million web page.",
            "It was a lot, but maybe the fact that there are 500 million users in Facebook starts giving you some pause or 3.6 billion images on Flickr.",
            "Or here is a very interesting statistics if you look at YouTube there's about 24 hours of video uploaded every minute.",
            "With this amount of information, how we're going to be able to process them, how we're going to be able to scale up our machine, learning algorithms to deal with this type of data well once?"
        ],
        [
            "That we enjoy for quite a few years was the fact that computers just kept getting faster.",
            "So if my algorithm wasn't that good five years ago, if I try it today, maybe it will be fast enough to scale up with this big problems.",
            "The issue as we know, is that about 2004 the scaling has kind of stopped because of some low level physics issues and the manufacturers have moved on to parallel performance.",
            "So parallel performance has been the focus of a lot of the computer architectures today.",
            "So I think it's very important to think about machine learning in this parallel concept context.",
            "Now."
        ],
        [
            "There's an array of different types of parallel machines that we've been using, and again we've seen some of these today, and the issue is for each one of these types of machines, we have to design new algorithms that deal with really difficult challenging, low level issues like race conditions.",
            "Or distributed state or moving data around.",
            "And if you go from one architecture to another, you have to totally re implement your system because you have different hardware specific APIs, different constraints, totally different systems to work with and this is a very challenging problem for us in the Community.",
            "Anne."
        ],
        [
            "And because of this challenge, what happens is the machine learning experts repeatedly solve the same problem?",
            "Or if your professor Carnegie Mellon.",
            "You see your grad students solving the same problem again again.",
            "Thank you U Tang enjoy I know.",
            "And.",
            "So the problems are how do we implement and debug the systems, which is extremely hard to debug, hard to implement?",
            "How do you get parallel performance out of different architectures?",
            "An even if you do that a month later, all you get is a sentence in your paper that says we implemented our algorithm in parallel and here are results.",
            "And that rarely gets you anywhere other than being able to say the curve is better than your curve.",
            "So because of that, we've moved on to using higher level abstractions that can be useful to getting away from having to deal with all these parallel issues."
        ],
        [
            "And a very popular obstructionism MapReduce abstraction.",
            "And so here is a very, very simplistic example.",
            "Or you can do it.",
            "MapReduce if you have a set of images like this, you might want to compute some features of each image, and we can do is send each one of these images."
        ],
        [
            "So CPU in parallel and compute those feature vectors."
        ],
        [
            "And this is called the map phase and in another phase you might be able."
        ],
        [
            "Might want to aggregate these vectors, maybe to compute some kind of gradient?",
            "And if you do that, you use the reduce phase that brings them together, aggregates, maybe isem together, averages them, and gives you an answer.",
            "So map reduce is really good for these types of problems.",
            "We can take the problem, break it independence of problems by computing features, and then aggregate the answers at the end.",
            "And because of that, they."
        ],
        [
            "Quite good for some machine learning tasks.",
            "If you want to feature extraction, nothing better.",
            "If you don't do cross validation just when they're running algorithm 10 times, then nothing better.",
            "But the question is where things are hard to do in machine learning or something like this MapReduce architecture.",
            "And a motivation?"
        ],
        [
            "This to understand this problem is the fact that a lot of algorithms and machine learning iterative.",
            "We have some set of data.",
            "Sorry yesterday to and we have to process it again and again.",
            "For example Gibbs sampling.",
            "You're going to take that data, run through some CPUs, complete some new samples, some new samples and so on.",
            "If you're using map, reduce what you have to do is take this data in replicated time and time again and just spread out in a bunch of different map phases.",
            "And what can happen is that you have barriers between the.",
            "Different phases of the algorithm, and if you have one slow processor because it has a lot of data or data that's hard to converge, it slows down entire pipeline and it might not be just one slow processor or one slow piece of data.",
            "It could be that."
        ],
        [
            "One part of data was hard.",
            "The next iteration, some other part was hard, some other part was hard.",
            "And so on.",
            "And so this just exemplifies one of the problems that you can have with map reduce.",
            "And there's all sorts of other low."
        ],
        [
            "Level issues you have to deal with.",
            "For example, the penalty that everything is written on disk in the standard implementation.",
            "So this is one way kind of systems kind of view of why MapReduce might not be a good thing.",
            "But what really got us into this type of research?"
        ],
        [
            "It was some work that we did a few years ago with join you Chang, where we thought how can we paralyze most basic email inference algorithm for graphical models.",
            "Belief propagation and belief propagation is a very naturally parallelizable algorithm because you can take every message in computing in parallel.",
            "And if you do that in a highly optimized in memory version of map reduce, you got some nice scaling as you increase the number of CPUs.",
            "So you're running time goes down as you add more CPUs.",
            "Unfortunately, there are some important aspects of the problem.",
            "For example, the fact that some parts of the model might converge while others don't, that leads to the design of other algorithms.",
            "For example, an algorithm that we called splash, which was faster than this MapReduce version algorithm.",
            "Even with one process.",
            "So one process for this algorithm was faster than eight processors running the standard map reduce version, so the limitations of what the way the map reduce works where you just break the problem into pencil problems and hope for the best.",
            "Are exemplified here in an algorithm that could not be implemented well if map reduce.",
            "And because of that.",
            "We have to think about new ways of building abstractions and the issue here was even those plus VP was a good algorithm that performs well and join you.",
            "Chang wrote an amazing parallel implementation.",
            "I have to say that it was for them, painful, painful and very painful to build that system to get this kind of graph.",
            "And so the question is if for every machine learning algorithm we have to redo this kind of low level aspects again and again and again, then we're in real trouble as a community.",
            "And if my produce is not."
        ],
        [
            "Right abstraction for dealing with this kind of problems.",
            "What is and?",
            "That's the question that we're asking and the type of example they're going to think about our problems.",
            "We have structured data, so here's a example from joy.",
            "A very simple example of structured data.",
            "Join my task.",
            "What is the probability that he is successful and that might depend on some other collaborators it works with and those collaborators successes and moved.",
            "And so it might depend on other people they work with.",
            "And so on.",
            "And if you think about this graph, you might ask, given this structured data.",
            "What is the probability that joy is going to be successful and this is an example of a graphical model, but this is also an example for structural problem that's hard to deal with with map reduce.",
            "For example, if you're doing Gibbs sampling a graphical model, you know the two nodes cannot be sampled at the same time, and there's interesting constraints are introduced into the problem.",
            "So since this is hard to map, reduce."
        ],
        [
            "And.",
            "Cross validation feature extraction so on is easy.",
            "The question is for other types of algorithms like belief propagation, sampling, learning, graphical models, neural networks, tensor factorization and so on.",
            "What type of abstractions can we use and what I'll talk about today is the work that we're doing on Graph Lab which is a parallel abstraction for representing sparse data an independent data like the one in the graphical model, not tell you more about that.",
            "That allows you to represent this types of problems here quite efficiently.",
            "And Joey, you Chang and Apple have a beautiful, beautiful implementation of.",
            "It is extremely fast and can solve these problems in a very, very efficient way."
        ],
        [
            "So.",
            "I'll just get to it.",
            "Our I think key inside here is that this structure problems or these problems machine learning that are not easy to map reduce.",
            "Have the following properties first.",
            "There's sparse data dependency often, like in a graphical model or a sparse SVM for example.",
            "There's a local computations you might want to do, like if you're doing Gibbs sampling, you're thinking about a node and the neighbors in the graph.",
            "Local updates and usually you deal with iterative algorithms like EM like optimization gradient methods like Gibbs sampling and so on.",
            "So what we focus on our algorithms that work on sparse data, they do local computations.",
            "And have this iterative update kind of."
        ],
        [
            "Framework.",
            "And this is what graph Lab supports.",
            "I think quite well, and so as an example, I've been hinting about group something quite a bit and we just ground the rest of the talk with examples within Gibbs sampling, so it gives something you have a graphical model like this relating a set of random variables and you might ask the questions so the graph model is sparse dependency.",
            "In our model, the local computation might say, given the neighbors of X6, given the current value, maybe this one is true and this one is false.",
            "Can I sample a new value for X6?",
            "And we do this computations again and again in an iterative fashion to obtain samples from the posterior distribution of this graphical model.",
            "So this is a great example that I'm going to use an exemplifies this idea that you have sparse dependencies.",
            "A graphical model, local updates, an iterative algorithm.",
            "So any any questions so far?",
            "I'm happy to answer questions during the talk.",
            "So Graph Lab is done specifically for."
        ],
        [
            "These types of needs and the abstracts away a lot of the hardware issues you have to deal with it automatically does that.",
            "Synchronization optimizes communication addresses, multiple hardware architectures.",
            "Specifically.",
            "Right now we have implementations for multicore clusters in cloud computing, and we're working on the GPU version of the algorithm of the abstraction in architecture, and so as a machine learning person, the way you should think about this.",
            "So you can think about this is.",
            "You have an algorithm that you'd like to paralyze, but you don't want to deal with the fact that you might have different cluster sizes, different types of computers you might have to move data around.",
            "You might have to deal with large data set size, optimized communication, race conditions, and all sorts of things.",
            "You just write it in terms of the abstraction they will describe, and then we'll deal with this.",
            "We might think as lower level systems issues and optimization issues with respect to the specific architectures.",
            "So the way the talk is going to work is I'm going to describe structure a little bit, and then I'm going to show different."
        ],
        [
            "Examples in different ML problems and how this approach is scaled.",
            "So Graph Lab is a framework that we've been building."
        ],
        [
            "And I think the natural way to think about Graph Lab is in terms of four components.",
            "The first component is a very natural one is the graph and the update function.",
            "So in Gibbs sampling you have the graphical model.",
            "An update function is a sampling of a node given its neighbors.",
            "The new components you have to think about if you implemented Graphlab program, what's called a shared data table where you deal with high level constants and sufficient statistics and gradients.",
            "They're shared across nodes.",
            "A schedule of where the updates will take place throughout the graph and a updates functions scopes, which I'll tell you more about which allow you to deal with issues like race conditions without having to address lower level issues in the problem.",
            "So this is the four components of graph lab and it's quite easy to implement the graph lab program, so I'm going to go through the components and give you an."
        ],
        [
            "Example in terms of global sampling.",
            "So.",
            "The first component graph is very natural and you can think about it as having data in the nodes and in the edges of the graph.",
            "And you can think about this.",
            "Edges in a natural kind of pairwise sense, you can think about this hyper edges and so on, but just think about it in the natural pairwise sense for now.",
            "And so in Gibbs sampling, we might have a graph where the nodes the nodes in the graph kamado in each node you have data that correspond to the current sample and maybe some node potentials and some edges might correspond to some edge potentials.",
            "As a simple example."
        ],
        [
            "Now the next step is an update function and update functions.",
            "Again, very intuitive concept is just something that takes on a node and can read a modified data within the node.",
            "The neighboring edges and the neighboring nodes.",
            "So again in the gives up states case we're modifying the nodes data and we get to look at the samples current samples in neighboring nodes and the potentials in the edges.",
            "This part is the very natural and intuitive I think part of graph lab that is very easy to specify in terms of a lot of our algorithms that we have in machine learning."
        ],
        [
            "Now a natural question from here is how do you schedule this update so?",
            "You might well Graphlab will do is take updates from a queue and just perform them in parallel if possible and do that in an iterative fashion."
        ],
        [
            "And this is what we call the scheduler or the update function.",
            "Now it's not sure what to say to use a basic schedule that maybe goes in a round Robin fashion.",
            "Abcdefg and so on according."
        ],
        [
            "So some permutation of the nodes.",
            "Now the issue with that is that it's often not enough for a lot of applications, so often what will happen is some parts of a problem my converge very quickly, while other parts might need a lot more effort.",
            "So what we want to be able to support in Graphlab is what's called dynamic scheduling, where you can dynamically assign nodes we perform and focus the computation again on parts that, for example, if you're doing belief propagation, have not converged yet, while other parts of graph might have converged and the same happens again."
        ],
        [
            "And in different application domains.",
            "So the way this works.",
            "Is it take to notes from the Q you perform their updates and some new nodes get inserted into the queue.",
            "So for example a maybe performance update and decide to insert B into the queue or I into the queue.",
            "And we can do that in a dynamic fashion, and you can write your own dynamic scheduler if you."
        ],
        [
            "Have that as an algorithm, but we provide you with a set of dynamics, schedulers.",
            "There are very natural.",
            "So for example, if you're doing belief propagation, then there's another game called Wildfire BP, which corresponds to a first in first out queue for the scheduler.",
            "There's another algorithm called residual BP, which corresponds to priority queue for the scheduler, and there's another algorithm called Slash VP that we I mentioned earlier corresponds to splash scheduler, so basically.",
            "By changing one flag in an algorithm, you can run experiments for three NIPS papers.",
            "Yeah, so that's all you have to do is change the flag and you can run different types of schedulers and this can make a lot of difference for some application domains.",
            "Now, so far I've talked about the update function, the graph and how we schedule computation, but often in a lot of of the applications and a lot of algorithms, not all the data can be viewed as being part of the graph."
        ],
        [
            "You might need some global information.",
            "For example, if you're computing a gradient in a graphical model, you might need to aggregate information from multiple nodes, or maybe the parameters of the algorithm there, shared in a kind of relational setting.",
            "May have sufficient statistics there computing some over vertices and so on, and for that it's not so natural to represent it as part of the graph."
        ],
        [
            "And well, Graph Lab provides is what we call a shared data table which stores this shared bits of information that you might have.",
            "Like this constants.",
            "For example, the total number of samples you might want to do, or a temperature in a linear algorithm, but also all sorts of other dynamic things that you might compute.",
            "So what we provide is a way to."
        ],
        [
            "Compute this operations which cause sync over a graph.",
            "We accumulate some information similar to the reduce step in map reduce and then we apply it to some of this shared data table in parallel.",
            "Computing this is called a fold operation in some sense and so for example, if my function was to add something across all nodes and applications to divide by the number of nodes, then what I'll do is just run this graph lab will go an average something over the nodes, and when it's done is going to divide it by.",
            "The number of nodes and just start into the shared data table which is visible to every node in the network, or every computation node."
        ],
        [
            "And so this third aspect is useful not just for constants, but when you have the sync operation, you can use for computing sufficient statistics, log likelihoods, gradients and all sorts of other things that are more global that need to be computed.",
            "So so far I talked about 3 aspects of the graph in the update function I talked about the scheduler and the shared data table, which is this high level bits of information which are shared across the nodes."
        ],
        [
            "The 4th aspect is safety and consistency.",
            "It's hard to write distributed algorithms.",
            "And I think one of the hardest parts of writing the script algorithm.",
            "And I do have a start up now and most of my days are spent on this.",
            "I were called race conditions."
        ],
        [
            "So here's an example for race condition.",
            "If you have one.",
            "What can happen is, let's say I'm doing something on this graph and I decide to run these two nodes.",
            "In parallel and one node, it does some computation and thinks that for this edge the values should be this blue graph, while the other node thinks that the value should be the green graph.",
            "Since this is a parallel update, you don't know which one will take precedence.",
            "You don't know exactly this kind of non deterministic process.",
            "You don't know can happen.",
            "So right race race condition means that these two contractor at the same time, you might end up with blue.",
            "Green mixture, which might have no meaning.",
            "It might be OK for some algorithms, but for other algorithms it might be disastrous.",
            "And so if you're writing a parallel algorithm, you have to think about these issues again and again, and I have to say this is a really painful issue and even within Graph Lab, as soon as the developing we're dealing with our own race conditions and it's hard and we don't want you guys to think about race conditions ever again.",
            "So the way we do that is by thinking about."
        ],
        [
            "What kind of abstract thinking about parametrization of the abstraction that allows you to be totally race free?",
            "So race free and deadlock free is something that we provide with a user tunable consistency mechanism.",
            "It's a very simple idea and then."
        ],
        [
            "Most basic type of the idea is what we call the scoping rules, so the scoping Rule says if I'm applying an update to this particular node in the middle, which other updates can perform at the same time.",
            "So since this node can touch data in a neighboring edges, an in a neighboring nodes, what we might have with full consistency is to say that I can't run any other update that might touch the same data.",
            "So in other words, I could not run this note here because it might touch this data and so that you can be run in parallel.",
            "So if useful consistency graph level guarantee that nobody touches the data at the same time.",
            "Now the problem is, for consistency can be quite restrictive, especially in highly connected models.",
            "Because if the graph is very connected, then there are really few nodes that can be run at the same time.",
            "And because of that we give you."
        ],
        [
            "All sorts of other update mechanisms, so if I do full consistency then that means that the only two nodes can be performed in parallel or those are two hops away from each other, which can again be difficult to have in practice in fully connected."
        ],
        [
            "Highly connected graphs.",
            "So in addition to for consistency, might choose to have.",
            "Edge consistency, which basically says that two nodes.",
            "Touch only the edge data so I can run this node and this node here in parallel and for example in belief propagation.",
            "This is what happens.",
            "Or in sampling give something.",
            "This is what happened.",
            "I can run this node in this note here in parallel and so you."
        ],
        [
            "Have to use edge consistency and not focus to see and so in this example you can run more nodes in parallel."
        ],
        [
            "We have a final type of consistent which is versus consistency, which is similar to the map operations in MapReduce and you can run every node in parallel.",
            "So for some update functions you might have vertex consistency needed, for others you might need only edge consistency and so on."
        ],
        [
            "And so here's an example.",
            "And the reason you need us, the reason you need this kind of consistency rules is that you want to be able to guarantee what's called sequential consistency of your algorithm, and this is a key property of a parallel algorithm.",
            "So if you design parallelism, you want the following property to hold usually.",
            "For all parallel executions of the algorithm, there exists a sequential execution on one processor that would have the same kind of outcome.",
            "So in other words, for this graph here, if I choose to run green, red, blue on CPU one and blue, green, green on CPU two, I want to make sure that the time is right such that there exists a sequential one CPU version that we've got in the same result.",
            "And this is a key property for proving correctness of your algorithm.",
            "So if you have an algorithm that works on one CPU and you have an architecture like Graph Lab which guarantees sequential consistency, then your algorithm will be automatically correct in the parallel setting.",
            "And this is what we guarantee for you.",
            "So in summary."
        ],
        [
            "The Graphlab abstraction?",
            "Is about this data representation and the update functions to share data table for things like global gradients.",
            "How we schedule data and how we want this update functions in scopes to happen.",
            "And so for the remainder of the talk I'm going to assume that we have this graph lab model and I want to show you results and various results in different architectures and on different types of machine learning algorithms.",
            "So let's start with the multicore setting."
        ],
        [
            "Inexperienced, I'll show you we've used."
        ],
        [
            "A C++ highly tuned into mentation with P threads and I'm going to show you results of 16 core machines and a variety of different types of algorithms were tested and I think I'm going to show you examples in a field because you know the number of experience is huge, but I'll start with a simple graphical modeler."
        ],
        [
            "Example, so this is some work.",
            "And they came out of Gary Miller's group, where they were trying to do some image denoising type of task on 3D retinal image data.",
            "And so here you have graphical model that has 1 million vertice is, it's a grid, is a 3D grid lattice.",
            "The update function is belief propagation and the sync function is what allows you to compute the gradients that are going to be taken when you do parameter learning.",
            "I'll say a million nodes graphical model is a pretty big model.",
            "But the other models that were going to later on quite a bit bigger than these, but the other problem is going to require bigger this.",
            "So this is the kind of."
        ],
        [
            "Get up there, we get will see a lot of these curves, Ann.",
            "On the X axis, as we increase the number of CPUs on the Y axis, is how much faster it gets and this dotted line would be if we got linear speedup.",
            "For example 16 CPUs we got 16 times faster, so this kind of an idealized line.",
            "And.",
            "You can try a variety of scheduling algorithms, schedulers and what you see is that we get close to linear speedup on the algorithm as we increase the number of processors that we use and hear.",
            "This plus schedule is slightly better than a priority queue based scheduler.",
            "Now since we have this architecture, you can try a lot of things.",
            "Once your algorithm is implemented within Graph Lab and let me show you one of the things that you could try to run and it's related actually to Johnson cyclist talk this morning in a way that he was thinking about interleaving gradient updates and the averaging process.",
            "Here what you can try is to say normally with graphical models you do inference that you converge and then you take a gradient step and interesting again.",
            "Step one thing that you could ask is what happens if you do parallel inference and gradient steps.",
            "So you do inferencing gradient updates at the same time, what happens?",
            "And this is just what happens answer and what happens is if you iterated versus if you do them simultaneously, you are about 3 times faster.",
            "And the quality of the answer."
        ],
        [
            "This is about the same, so we can't yet explain from a theoretical perspective because this is based on VP like updates and so on.",
            "Why exactly this is giving the correct answers or the same answers, but you get quite a bit of speedup and graph lab allows us to do this with a very very minimal change in what you tell it to do.",
            "So this is 1 example, let me move to the next example which is lost.",
            "So so this next few examples just exemplify some issues that come up when you when you try to run the algorithms in parallel settings so.",
            "A lot of us have been looking at these types of problems lately and the idea is you have a regression problem.",
            "So you want to fit Y with the feature vector X * W and you have an L1 regularization penalty, and the idea here is that X might be a sparse matrix, so faxes sparse matrix, you can think of as a sparse relationship between the weights and the wise.",
            "And you can implement this in graph lab and one issue that happens is if I use what's called the shooting algorithm for this.",
            "When I updating weights one I need to look at or influence Y1 and Y-3 and so any other weights that touches Y1 or Y-3 cannot be run at the same time, so we're only allowed to run one here and W 5 at the same time.",
            "And so because of that we require the full consistency model remain talked about different types of consistency model.",
            "Focusing model here is required for the correctness of the algorithm, so I can only."
        ],
        [
            "W1 and W5, and if you try to run the focus distance model, this is what happens.",
            "As increasing numbers, CPUs and you have a dense matrix, that means that a lot of things and touch each other.",
            "Your speed up is quite poor.",
            "With 16 processors you only get about two times speedup and even a sparse problem.",
            "You only get about four times speedup with 16 processors, which is not that exciting, you should say.",
            "But one thing that you can try.",
            "Just try with a change of flag is just say."
        ],
        [
            "If I just hope for the best for relaxed consistency, I don't require full consistency and they just try to run the algorithm.",
            "What kind of scaling do we get?",
            "And it turns out that you get much better scaling.",
            "Even in dense models then you got before and.",
            "Even though the correctness is not guaranteed, an two of my students are popular and justifiably have some nice ideas proving why this works.",
            "You actually get much, much nicer scaling.",
            "In this example, you get about 10 times or 16 processors, so again, you change the type of requirement that you put in the algorithm on the architecture."
        ],
        [
            "And you can get better scaling.",
            "Why do you not?",
            "See as much improvement for this part."
        ],
        [
            "Right so.",
            "I think because of the data loading issues and some issues related to that, I think this is kind of some initial experiments and we still have a lot to explore in this part."
        ],
        [
            "So the third example that I'm going to show you is the idea is an algorithm called coem that tries to do named entity recognition.",
            "So an immense recognition task and the idea is can I ask the question is dog and animal is Catalina Place and so on.",
            "And the way that it's done.",
            "As you can imagine, have a bipartite graph where you have the instances you found like the dog Australia, Catalina and so on, and then they related to other processes and found some X.",
            "Run quickly.",
            "Somebody travel to some X which was Australia.",
            "X is pleasant and so on.",
            "And based on this graph and knowing that Australia is a place, for example, we can propagate that information at them like algorithm to get.",
            "To get some named entity recognition type of outcomes and tomatoes group at CMU was working on some very large problems in this sum based on about 2 million vertices in this graph and 200 million edges.",
            "So there's a pretty large problems and they were so large that they were having a hard time.",
            "Dealing with this data set size and they were using Hadoop.",
            "With the cluster that Yahoo has provided us.",
            "And with 95 cores they were getting about 7 1/2 hours running time which they had to do because the data set was so large.",
            "Now this is the type of problem.",
            "The graph lab is good for.",
            "Their sparse interactions."
        ],
        [
            "So graph base is an iterative algorithm, and so on.",
            "And if you implement this in graph Lab.",
            "Again, you get very life.",
            "You get very nice scaling, not as much for the smaller problems as you do for the larger problems.",
            "Because the larger problem is more computation to them.",
            "And.",
            "If in the original Hadoop implementation of 95 cores it took 7 1/2 hours.",
            "With graph Lab with 16 cores it took about 30 minutes.",
            "So 60 * 6 times fewer CPUs 15 times faster.",
            "I think this is a exciting example of the types of things that you can do by thinking more about your computation."
        ],
        [
            "Then you do.",
            "If you do a MapReduce type presentation.",
            "So.",
            "The next part that we've been working on quite a bit lately is moving towards cloud computing, so I talked about the multi core types of results.",
            "Let me talk for a little bit about."
        ],
        [
            "The cloud computing setting turns out that it's really expensive.",
            "To build a cluster, it's not just expensive to buy the computers, but man we pay a lot to maintain them.",
            "And basically this computers are only used during the deadlines.",
            "Most of the time they're sitting idle.",
            "You guys should be working harder.",
            "I don't understand where they sitting idle.",
            "So.",
            "Because of that, there's been a move towards various cloud computing type of resources where you can buy time and access to hundreds or thousands of processors and only pay for the resources that you actually need.",
            "And in this setting I think this, I think a very exciting setting to move towards as a model for computation in machine learning.",
            "And so."
        ],
        [
            "To deal with this setting we had to deal with many significant issues, which I'm not going to talk about in a lot of detail, but basically instead of having the multicore setting where you have one shared memory, now we have memory distributed across many machines, so you have to optimize.",
            "For example how the data is partitioned and moved around.",
            "You have limited bandwidth between machines, so.",
            "We have a quite nice smart caching and interleaving of communication computation that allow you to deal with this limited bandwidth and the high latency so it takes time to send data between machines.",
            "This is some ideas that were implemented Graphlab based on pushing data preemptively andsome other latency hiding mechanisms that make latency a lot less of an issue and these are fairly hardcore, low level and middle level issues that I think most machine learning researchers don't want to deal with.",
            "And so if you don't want to deal with any of these, you can just.",
            "You know I'm being a little facetious, but you can just implement their stuff within graph Lab and will take care of those for you."
        ],
        [
            "And so.",
            "We have a highly optimized implementation which is testable from computer clusters.",
            "An Amazon EC2 an here's a nice thing.",
            "The team up for one of my students and the rest team have created a very nice script where all you have to do is have a easy to account and will automatically get the machines for you.",
            "Push the code out installing them.",
            "It's extremely easy to use, you don't have to know anything about EC2 basically, so if your stuff works in graph lab and you have a credit card.",
            "You cannot.",
            "You can use use the resource quite easily and we thoroughly experimented on the number of different case studies.",
            "I'm going to basically talk about three of them Co M."
        ],
        [
            "The one they just talked about.",
            "This factorization and video call segmentation.",
            "So the setting that will look at that there are two types of machines you can get from Amazon, one of regular nodes which have 8 cores and this is high performance computing nodes 16 cores where they try to put machines closer together for you, so there's less latency in communication.",
            "And so this is 2 settings we're going to talk about an.",
            "The first problem will talk about is the Co M problem."
        ],
        [
            "And there's not much to talk about here.",
            "I mean, before we had Hadoop taking seven half hours Graphlab taking 30 minutes, it's the same graph lab.",
            "It's implemented, same problem except we do it in EC2 and using 32 ECT machines.",
            "We solve the same problem in 80 seconds.",
            "Yeah.",
            "Improvement in time.",
            "There's all sorts of things that go into that.",
            "Not writing things to disk.",
            "The fact that we can focus computation so the schedulers can focus computation where it's needed on the parts of problem, having confers the fact that we deal directly with communication between nodes.",
            "So there's also underlying communication issues that we do because it's a graph.",
            "I know where my data is going when the graph is partitioned, so there's a lot of things that come together is not just a factor in writing to disk, you're not going to run central machine like you do with Hadoop.",
            "The underlying architecture is quite differently built.",
            "Why is it not so much faster?",
            "Why is what?",
            "What is the not so much faster?",
            "O32EC2 machines 256 processors.",
            "So, but it was about zero point 3% of Hadoop time.",
            "Compatible number of processors.",
            "I'll talk about money in a minute.",
            "The answer is not much money.",
            "But money is a beautiful thing when it comes to this results, and I have a beautiful graph related to money, yes.",
            "Kind of machines, the high performance ones.",
            "This is the high performance ones.",
            "Yeah, I'll show a graph.",
            "I think about comparing the two types of machines.",
            "High performance machines are better for a lot of competitions that we want to do.",
            "I would say.",
            "But we did a lot of these comparisons.",
            "So before I get to money before before I get to the money graph there when I show you I'll just make a little money statement writing this paper.",
            "What we did is not.",
            "I have a lot more experiments in EC2 then then I'm going to show.",
            "Today we have a lot more experiments cost about $2000.",
            "Anso 4000 four $1000 yeah sure.",
            "For $4000 and this wasn't a setting where we're doing lots of scaling experiments, trying lots of different machine sizes, trying lots of different types of ways of using the cluster, and because we didn't know as much about EC2.",
            "And let's say we.",
            "I mean me we had to rerun the same things again again and so on so.",
            "Compare that cost if the costs are cost me to maintain machines at Carnegie Mellon and bias on and think about how many papers we produce in the group.",
            "Guys, please, I think that's a very much cheaper than the amount of money I'm spending on cluster honestly.",
            "Anyway, before before I forget, let's try to finish the talk.",
            "So about zero point, 3% of Hadoop time.",
            "So this is the code name example.",
            "Here's the video called Sigma."
        ],
        [
            "Station example.",
            "I have a video and this is some images of Carnegie Mellon and what I do is segment.",
            "This image is such that this part here of Blue Sky has the same semantics as this part here of Blue Sky.",
            "So it's not just their segmented as image segmentation, but the segment in that kind of coherent way across different frames of the video.",
            "And there's a ton of data if you think about what you think about videos and it's hard to find a right kind."
        ],
        [
            "Segmentation and natural but naive idea is to break images into patches and just to say.",
            "Running M type of algorithm that tries to label patches with different segments and figure out what the appearances of Sky.",
            "Of course this is totally unlabeled.",
            "There's no examples of Sky, but I want to do it and discover through some kind of clustering that does examples of Sky and this is naive because it doesn't take the spatial structure into account.",
            "So a fancier better."
        ],
        [
            "Idea is to do the same kind of EM algorithm, But using some kind of graphical model structure where you say that neighboring patches within an image and across images have similar labels or similar types in the cluster.",
            "And you can do an M type of algorithm and this is not all right dear of doing this type of thing.",
            "This work by Dhruv Batra.",
            "But we implemented this within Graph Lab with a number of different extensions of how to do this in the video setting and we get some really kind of interesting results.",
            "So here we are."
        ],
        [
            "Huge problems, there's about 23 million nodes in this graph chamado an 390 million edges.",
            "And we get some very nice scaling.",
            "As in case number of processors and you can see the standard, there's a gap.",
            "I think about Factor 2 or so between the standard processors in the high performance processors, but for example with using 32 machines were getting a runtime of the order of 100 seconds or less on a graphical model with 20,000,000 nodes, which I think."
        ],
        [
            "Is a pre impressive running time?",
            "And because there's a cost benefit tradeoff, as increasing number of machines, you can build this pretty cool kinds of graphs.",
            "So is add more machines.",
            "I pay more money and the running time is less, so you get this nice kind of Nikkor or Pareto kind of curve where a few machines.",
            "You know it's cheap, but it takes a long time with a lot of machines, is expensive, but it doesn't help as much and there's a nicely in the curve where you get kind of the nice cost to running time ratio.",
            "And so here we're talking about $0.70 or something for doing inference in a graphical model of that size.",
            "So think about that setting.",
            "Two types of machines in Amazon EC2 cluster.",
            "There is the standard machines which they are not guaranteed to be Co located with each other and the high performance computing machines which are kind of more guaranteed to be closer together in there.",
            "Data warehouse.",
            "And they're a little faster too."
        ],
        [
            "OK, so pretty interesting tradeoff curves and.",
            "Here's the third example we talk about today.",
            "Briefly, is a type of Netflix challenge type of example where you want to do Bayesian tensor factorization so you have a Netflix style data where people rated movies at different times and you want to break it into some factors over users, movies and time, and the graph is Paris because you know there's a sparse relationship between movies and user ratings, and so on.",
            "And again, you can do.",
            "Various algorithms."
        ],
        [
            "This type of problem we tried one of the state of the art once an.",
            "As you increase the number of machines, we get better scaling again properties.",
            "HPC node.",
            "We get pretty good scaling properties of 32 nodes.",
            "We get about 18 times speedup.",
            "Across the cloud computing setting, which I think is quite impressive result and again you get this cost to time tradeoff.",
            "An it really does not make sense to buy regular nodes for this problem, as you can see the knee of the curve is about $0.80 for HPC nodes, while it's like $1.60 or dollar 50 for the regular nodes.",
            "And just as a.",
            "Just to give you a sense, the paper they're implemented from this 2010 paper by showing at all where they use one core and it took them about 2000 cycles per iteration and we used 156 cores.",
            "It takes about 6 seconds."
        ],
        [
            "So.",
            "Just to close off Graph Lab is available today.",
            "We have the multi core version available at one point 1 level and will distribute the cloud computing version soon.",
            "Go graph lab.ms.cmu.edu and we have.",
            "Acquire"
        ],
        [
            "Interesting plan of where we're working.",
            "It's a highly optimized native C, and there's a graph lab engine that can sit on top of multicore server setting and we're working on GPU and on top of this, if you're not so comfortable with the native C++ setting, we can think about Matlab, Java, or Python.",
            "So the native C plus."
        ],
        [
            "This implementation gives you access to more features and it's going to be faster for sure, but you can also get reasonable scaling if you use our Java API or a Python API an R."
        ],
        [
            "Latest.",
            "API is a MATLAB based one which is fresh off the press and I think it's quite exciting and in some applications that we tested it actually runs faster than Matlab.",
            "Because it compiles the code and there's."
        ],
        [
            "A bunch of things underneath that you don't have to worry about, so as an example you could write a MATLAB function.",
            "I'm not going to go through this, but it's like belief propagation, Matlab function and all you have is some extra function that says get your vertex data and graph lab will give that to you and add some some tasks to the queue.",
            "And the rest of your MATLAB code is more or less the same."
        ],
        [
            "So summarizing with graph lab, you will be able to implement your algorithm in terms of this four aspects data graph, update, function share data table, the scheduler, and the scope constraints and."
        ],
        [
            "I think it provides a very nice parallel structure for a lot of machine learning algorithms and compactly expressed a lot of iterative computations that we do today.",
            "Yeah, she is state of the art parallel performance an is I believe quite easy to use as an abstraction."
        ],
        [
            "And we're working on many issues related to this.",
            "So for example, this setting we have an address, an issue of robustness, which is addressed, for example by MapReduce today, and this GPU setting and so on.",
            "But you can imagine thinking about state of the art perform parallel performance for your."
        ],
        [
            "Algorithm here by implementing it within Graph Lab.",
            "So try it out and let us know what you think.",
            "Thank you very much."
        ],
        [
            "Alice.",
            "So what about the addressing the computational challenges?",
            "Talk about dealing with the CPU bottleneck, making computation safe, telecommunications safe and efficient.",
            "But as is illustrated in the cloud setting you when you move to these more distributed systems, you then start have to deal with other other bottlenecks like disk or network.",
            "So my question is and you could have slept a lot of that detail look like.",
            "But us.",
            "If I'm trying to sterilize my my outlook program.",
            "I'm I'm curious more about your sense of where should I spend my time.",
            "Should I spend more money so I spend more time thinking about how to locate data with computation?",
            "So the question.",
            "Right, the question is what's the bottleneck in the same parallel algorithms, Ann?",
            "So the philosophy that we're trying to propose here is that we're going to take care of, for example, Co.",
            "Locating data and computation for you.",
            "As you know, Graph Lab one point 1.0 son maybe that will get better overtime, but that's what we're trying to make better for you.",
            "What you can do for us is design parallel algorithms that can be written in a certain way that allow for the underlying optimization to happen.",
            "And so if you algorithm is expressed in terms of our abstraction, it's easier for us to provide that optimization of collocating data.",
            "Thinking about where updates to go, how to distribute the data, and so on and so that.",
            "So what's easiest for us in terms of limitations of the approach right now, I think it works best when the graph is sparser obviously, and it works best when they update functions.",
            "The more computation there isn't an update function.",
            "Right, so the simple update function, the harder it is for us, and we've tried problems with update function is very quick and we still do OK, but not always.",
            "But this is the limitations of the current type of architecture we have.",
            "But the idea is if you can implement this way, we're going to come up with different algorithms that go underneath that will optimize the kind of question that you asked.",
            "Does that help?",
            "My marriage license.",
            "Did you read this?",
            "I actually forget with that, what do we do?",
            "GPL right?",
            "L GPL yeah website.",
            "All.",
            "I I honestly I forgot OK here, let me rephrase this question.",
            "If you guys have a license that is most appropriately on, talk to us about different types of licenses will be open.",
            "We had a long discussion about this and we're on the fence about different types of license, so I'm happy to hear feedback from the community about the type of license you prefer.",
            "Updates.",
            "This.",
            "Horse adventure it's about what other kinds of abstraction considered along the way, and oh, what, how?",
            "Oh, I see.",
            "How many hours do you have?",
            "I have to say that we argued about these for a long, long time.",
            "The I think the most complex the hardest question to decide, was how simple we could make the sync operation.",
            "Because you can imagine having.",
            "Because right now we're assuming that is something that you can aggregate over the nodes.",
            "So certain associativity and distributivity properties are assumed of the function of trying to compute an.",
            "So all sorts of things came up when we try to discuss how to get that right, and I think we have a reasonable assumption right now.",
            "But that was kind of the biggest bottleneck.",
            "The other one was pretty natural, I thought.",
            "How about can you change the structure of the graph?",
            "The graph is user change overtime or change.",
            "So that that is in the works.",
            "It's not version 1.1.",
            "Not really so.",
            "You should think about as a single queue, but you can have and implement a single queue in the parallel setting.",
            "That's a mistake.",
            "It's a lot of different cues, and there's some smart way of dealing with different queues.",
            "Would you want to have different queues for yourself?",
            "Just raise 23 million.",
            "Yeah, yeah, no no, that's not how it's implemented.",
            "There's alot of things that are there in abstractions, not how it's implemented for you, but that's how we should think about.",
            "John.",
            "Yes, we did.",
            "We're just about this relative speedup graphs and I guess absolute numbers kind of matter to some extent.",
            "Also, can you give this some sense of like the absolute time for the lasso program for for the, for the last?",
            "So I don't know if I."
        ],
        [
            "I lost couple too if he remembers, but for we have absolute times for some algorithms up here.",
            "Can I go back?",
            "Yeah, so 66 seconds for 2656 cores for a problem that was.",
            "The whole Netflix data.",
            "Forget how big that is, honestly.",
            "Seconds for 230 million we can give you a lot more numbers.",
            "In fact, there's a tricky so Joey wants to jump in.",
            "That I'm sending the paper, of course.",
            "Yeah, it's hard to compare.",
            "There's a tricky thing with this type of graph which is relative speedups.",
            "You always want the relative speed up to be with respect to the best single core implementation, right?",
            "Otherwise you can have an algorithm that has great scaling.",
            "For example, MapReduce.",
            "BP has great scaling linear, but one processor is so slow, so that's something that you have to think about when you start thinking about parallel implementations.",
            "Cost versus time graph.",
            "I then will have their state.",
            "How come it's always this is overall cost for cost per node or not.",
            "This is the cost for your computation.",
            "How much it cost you to solve this machine learning problem?",
            "How come it doesn't?",
            "How come we just how come?",
            "It's a function, right?",
            "Because you could imagine that on one node it takes a long time, and even though it's not the only other people can do it because of the time it actually ends up costing more.",
            "Oh no, Oh no.",
            "Oh, I see I see I see.",
            "So wait, wait, so join.",
            "I actually don't know.",
            "Actually I thought I saw it.",
            "The X axis running time because you try to try to spawn smaller going right?",
            "Yeah, so think about this is a lower envelope or something.",
            "Right, it should, yeah.",
            "At least in an hour.",
            "OK so there is.",
            "There is a subtlety here in this graph.",
            "What's up?",
            "Yeah.",
            "So you have to pay for at least an hour, so this is like you can imagine you would have to do cross validation runs or something to use up that matter.",
            "But yeah, here here is assuming that you could infinitely divide EC2, which you can't, so the graph would be different for you.",
            "Any other questions?",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is with great pleasure that I'm here for this workshop.",
                    "label": 0
                },
                {
                    "sent": "I think it's a great important topic we've been working on for quite a bit, and I think it's really good that our community is putting some effort on it.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "Here is really work of my students, so they deserve all the credit.",
                    "label": 0
                },
                {
                    "sent": "Yucheng Low J Gonzalez a Pocatello and at Ebix and I think most of them are here today.",
                    "label": 0
                },
                {
                    "sent": "So bug them at the end and ask them questions.",
                    "label": 0
                },
                {
                    "sent": "They do have a poster, I think right here to talk about more details and the issues that we're facing today.",
                    "label": 0
                },
                {
                    "sent": "Machine learning one of them.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Big issues I feel is the explosion in the size of the problems we're trying to solve and we see quite a bit of that today and we used to think that you know Wikipedia 30 million web page.",
                    "label": 0
                },
                {
                    "sent": "It was a lot, but maybe the fact that there are 500 million users in Facebook starts giving you some pause or 3.6 billion images on Flickr.",
                    "label": 1
                },
                {
                    "sent": "Or here is a very interesting statistics if you look at YouTube there's about 24 hours of video uploaded every minute.",
                    "label": 0
                },
                {
                    "sent": "With this amount of information, how we're going to be able to process them, how we're going to be able to scale up our machine, learning algorithms to deal with this type of data well once?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we enjoy for quite a few years was the fact that computers just kept getting faster.",
                    "label": 0
                },
                {
                    "sent": "So if my algorithm wasn't that good five years ago, if I try it today, maybe it will be fast enough to scale up with this big problems.",
                    "label": 0
                },
                {
                    "sent": "The issue as we know, is that about 2004 the scaling has kind of stopped because of some low level physics issues and the manufacturers have moved on to parallel performance.",
                    "label": 0
                },
                {
                    "sent": "So parallel performance has been the focus of a lot of the computer architectures today.",
                    "label": 0
                },
                {
                    "sent": "So I think it's very important to think about machine learning in this parallel concept context.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's an array of different types of parallel machines that we've been using, and again we've seen some of these today, and the issue is for each one of these types of machines, we have to design new algorithms that deal with really difficult challenging, low level issues like race conditions.",
                    "label": 1
                },
                {
                    "sent": "Or distributed state or moving data around.",
                    "label": 1
                },
                {
                    "sent": "And if you go from one architecture to another, you have to totally re implement your system because you have different hardware specific APIs, different constraints, totally different systems to work with and this is a very challenging problem for us in the Community.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And because of this challenge, what happens is the machine learning experts repeatedly solve the same problem?",
                    "label": 1
                },
                {
                    "sent": "Or if your professor Carnegie Mellon.",
                    "label": 0
                },
                {
                    "sent": "You see your grad students solving the same problem again again.",
                    "label": 0
                },
                {
                    "sent": "Thank you U Tang enjoy I know.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "So the problems are how do we implement and debug the systems, which is extremely hard to debug, hard to implement?",
                    "label": 0
                },
                {
                    "sent": "How do you get parallel performance out of different architectures?",
                    "label": 1
                },
                {
                    "sent": "An even if you do that a month later, all you get is a sentence in your paper that says we implemented our algorithm in parallel and here are results.",
                    "label": 0
                },
                {
                    "sent": "And that rarely gets you anywhere other than being able to say the curve is better than your curve.",
                    "label": 0
                },
                {
                    "sent": "So because of that, we've moved on to using higher level abstractions that can be useful to getting away from having to deal with all these parallel issues.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a very popular obstructionism MapReduce abstraction.",
                    "label": 0
                },
                {
                    "sent": "And so here is a very, very simplistic example.",
                    "label": 0
                },
                {
                    "sent": "Or you can do it.",
                    "label": 0
                },
                {
                    "sent": "MapReduce if you have a set of images like this, you might want to compute some features of each image, and we can do is send each one of these images.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So CPU in parallel and compute those feature vectors.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is called the map phase and in another phase you might be able.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Might want to aggregate these vectors, maybe to compute some kind of gradient?",
                    "label": 0
                },
                {
                    "sent": "And if you do that, you use the reduce phase that brings them together, aggregates, maybe isem together, averages them, and gives you an answer.",
                    "label": 0
                },
                {
                    "sent": "So map reduce is really good for these types of problems.",
                    "label": 0
                },
                {
                    "sent": "We can take the problem, break it independence of problems by computing features, and then aggregate the answers at the end.",
                    "label": 0
                },
                {
                    "sent": "And because of that, they.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quite good for some machine learning tasks.",
                    "label": 1
                },
                {
                    "sent": "If you want to feature extraction, nothing better.",
                    "label": 0
                },
                {
                    "sent": "If you don't do cross validation just when they're running algorithm 10 times, then nothing better.",
                    "label": 0
                },
                {
                    "sent": "But the question is where things are hard to do in machine learning or something like this MapReduce architecture.",
                    "label": 0
                },
                {
                    "sent": "And a motivation?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This to understand this problem is the fact that a lot of algorithms and machine learning iterative.",
                    "label": 0
                },
                {
                    "sent": "We have some set of data.",
                    "label": 0
                },
                {
                    "sent": "Sorry yesterday to and we have to process it again and again.",
                    "label": 0
                },
                {
                    "sent": "For example Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "You're going to take that data, run through some CPUs, complete some new samples, some new samples and so on.",
                    "label": 0
                },
                {
                    "sent": "If you're using map, reduce what you have to do is take this data in replicated time and time again and just spread out in a bunch of different map phases.",
                    "label": 0
                },
                {
                    "sent": "And what can happen is that you have barriers between the.",
                    "label": 0
                },
                {
                    "sent": "Different phases of the algorithm, and if you have one slow processor because it has a lot of data or data that's hard to converge, it slows down entire pipeline and it might not be just one slow processor or one slow piece of data.",
                    "label": 0
                },
                {
                    "sent": "It could be that.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One part of data was hard.",
                    "label": 0
                },
                {
                    "sent": "The next iteration, some other part was hard, some other part was hard.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And so this just exemplifies one of the problems that you can have with map reduce.",
                    "label": 0
                },
                {
                    "sent": "And there's all sorts of other low.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Level issues you have to deal with.",
                    "label": 0
                },
                {
                    "sent": "For example, the penalty that everything is written on disk in the standard implementation.",
                    "label": 0
                },
                {
                    "sent": "So this is one way kind of systems kind of view of why MapReduce might not be a good thing.",
                    "label": 0
                },
                {
                    "sent": "But what really got us into this type of research?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It was some work that we did a few years ago with join you Chang, where we thought how can we paralyze most basic email inference algorithm for graphical models.",
                    "label": 0
                },
                {
                    "sent": "Belief propagation and belief propagation is a very naturally parallelizable algorithm because you can take every message in computing in parallel.",
                    "label": 0
                },
                {
                    "sent": "And if you do that in a highly optimized in memory version of map reduce, you got some nice scaling as you increase the number of CPUs.",
                    "label": 1
                },
                {
                    "sent": "So you're running time goes down as you add more CPUs.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there are some important aspects of the problem.",
                    "label": 0
                },
                {
                    "sent": "For example, the fact that some parts of the model might converge while others don't, that leads to the design of other algorithms.",
                    "label": 0
                },
                {
                    "sent": "For example, an algorithm that we called splash, which was faster than this MapReduce version algorithm.",
                    "label": 0
                },
                {
                    "sent": "Even with one process.",
                    "label": 0
                },
                {
                    "sent": "So one process for this algorithm was faster than eight processors running the standard map reduce version, so the limitations of what the way the map reduce works where you just break the problem into pencil problems and hope for the best.",
                    "label": 0
                },
                {
                    "sent": "Are exemplified here in an algorithm that could not be implemented well if map reduce.",
                    "label": 0
                },
                {
                    "sent": "And because of that.",
                    "label": 0
                },
                {
                    "sent": "We have to think about new ways of building abstractions and the issue here was even those plus VP was a good algorithm that performs well and join you.",
                    "label": 0
                },
                {
                    "sent": "Chang wrote an amazing parallel implementation.",
                    "label": 0
                },
                {
                    "sent": "I have to say that it was for them, painful, painful and very painful to build that system to get this kind of graph.",
                    "label": 0
                },
                {
                    "sent": "And so the question is if for every machine learning algorithm we have to redo this kind of low level aspects again and again and again, then we're in real trouble as a community.",
                    "label": 0
                },
                {
                    "sent": "And if my produce is not.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right abstraction for dealing with this kind of problems.",
                    "label": 0
                },
                {
                    "sent": "What is and?",
                    "label": 0
                },
                {
                    "sent": "That's the question that we're asking and the type of example they're going to think about our problems.",
                    "label": 0
                },
                {
                    "sent": "We have structured data, so here's a example from joy.",
                    "label": 0
                },
                {
                    "sent": "A very simple example of structured data.",
                    "label": 0
                },
                {
                    "sent": "Join my task.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that he is successful and that might depend on some other collaborators it works with and those collaborators successes and moved.",
                    "label": 0
                },
                {
                    "sent": "And so it might depend on other people they work with.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And if you think about this graph, you might ask, given this structured data.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that joy is going to be successful and this is an example of a graphical model, but this is also an example for structural problem that's hard to deal with with map reduce.",
                    "label": 0
                },
                {
                    "sent": "For example, if you're doing Gibbs sampling a graphical model, you know the two nodes cannot be sampled at the same time, and there's interesting constraints are introduced into the problem.",
                    "label": 0
                },
                {
                    "sent": "So since this is hard to map, reduce.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Cross validation feature extraction so on is easy.",
                    "label": 1
                },
                {
                    "sent": "The question is for other types of algorithms like belief propagation, sampling, learning, graphical models, neural networks, tensor factorization and so on.",
                    "label": 1
                },
                {
                    "sent": "What type of abstractions can we use and what I'll talk about today is the work that we're doing on Graph Lab which is a parallel abstraction for representing sparse data an independent data like the one in the graphical model, not tell you more about that.",
                    "label": 0
                },
                {
                    "sent": "That allows you to represent this types of problems here quite efficiently.",
                    "label": 0
                },
                {
                    "sent": "And Joey, you Chang and Apple have a beautiful, beautiful implementation of.",
                    "label": 0
                },
                {
                    "sent": "It is extremely fast and can solve these problems in a very, very efficient way.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'll just get to it.",
                    "label": 0
                },
                {
                    "sent": "Our I think key inside here is that this structure problems or these problems machine learning that are not easy to map reduce.",
                    "label": 0
                },
                {
                    "sent": "Have the following properties first.",
                    "label": 0
                },
                {
                    "sent": "There's sparse data dependency often, like in a graphical model or a sparse SVM for example.",
                    "label": 0
                },
                {
                    "sent": "There's a local computations you might want to do, like if you're doing Gibbs sampling, you're thinking about a node and the neighbors in the graph.",
                    "label": 0
                },
                {
                    "sent": "Local updates and usually you deal with iterative algorithms like EM like optimization gradient methods like Gibbs sampling and so on.",
                    "label": 0
                },
                {
                    "sent": "So what we focus on our algorithms that work on sparse data, they do local computations.",
                    "label": 1
                },
                {
                    "sent": "And have this iterative update kind of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Framework.",
                    "label": 0
                },
                {
                    "sent": "And this is what graph Lab supports.",
                    "label": 0
                },
                {
                    "sent": "I think quite well, and so as an example, I've been hinting about group something quite a bit and we just ground the rest of the talk with examples within Gibbs sampling, so it gives something you have a graphical model like this relating a set of random variables and you might ask the questions so the graph model is sparse dependency.",
                    "label": 0
                },
                {
                    "sent": "In our model, the local computation might say, given the neighbors of X6, given the current value, maybe this one is true and this one is false.",
                    "label": 0
                },
                {
                    "sent": "Can I sample a new value for X6?",
                    "label": 0
                },
                {
                    "sent": "And we do this computations again and again in an iterative fashion to obtain samples from the posterior distribution of this graphical model.",
                    "label": 0
                },
                {
                    "sent": "So this is a great example that I'm going to use an exemplifies this idea that you have sparse dependencies.",
                    "label": 0
                },
                {
                    "sent": "A graphical model, local updates, an iterative algorithm.",
                    "label": 0
                },
                {
                    "sent": "So any any questions so far?",
                    "label": 0
                },
                {
                    "sent": "I'm happy to answer questions during the talk.",
                    "label": 0
                },
                {
                    "sent": "So Graph Lab is done specifically for.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These types of needs and the abstracts away a lot of the hardware issues you have to deal with it automatically does that.",
                    "label": 0
                },
                {
                    "sent": "Synchronization optimizes communication addresses, multiple hardware architectures.",
                    "label": 1
                },
                {
                    "sent": "Specifically.",
                    "label": 0
                },
                {
                    "sent": "Right now we have implementations for multicore clusters in cloud computing, and we're working on the GPU version of the algorithm of the abstraction in architecture, and so as a machine learning person, the way you should think about this.",
                    "label": 0
                },
                {
                    "sent": "So you can think about this is.",
                    "label": 0
                },
                {
                    "sent": "You have an algorithm that you'd like to paralyze, but you don't want to deal with the fact that you might have different cluster sizes, different types of computers you might have to move data around.",
                    "label": 0
                },
                {
                    "sent": "You might have to deal with large data set size, optimized communication, race conditions, and all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "You just write it in terms of the abstraction they will describe, and then we'll deal with this.",
                    "label": 0
                },
                {
                    "sent": "We might think as lower level systems issues and optimization issues with respect to the specific architectures.",
                    "label": 0
                },
                {
                    "sent": "So the way the talk is going to work is I'm going to describe structure a little bit, and then I'm going to show different.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples in different ML problems and how this approach is scaled.",
                    "label": 0
                },
                {
                    "sent": "So Graph Lab is a framework that we've been building.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I think the natural way to think about Graph Lab is in terms of four components.",
                    "label": 0
                },
                {
                    "sent": "The first component is a very natural one is the graph and the update function.",
                    "label": 0
                },
                {
                    "sent": "So in Gibbs sampling you have the graphical model.",
                    "label": 0
                },
                {
                    "sent": "An update function is a sampling of a node given its neighbors.",
                    "label": 1
                },
                {
                    "sent": "The new components you have to think about if you implemented Graphlab program, what's called a shared data table where you deal with high level constants and sufficient statistics and gradients.",
                    "label": 1
                },
                {
                    "sent": "They're shared across nodes.",
                    "label": 0
                },
                {
                    "sent": "A schedule of where the updates will take place throughout the graph and a updates functions scopes, which I'll tell you more about which allow you to deal with issues like race conditions without having to address lower level issues in the problem.",
                    "label": 0
                },
                {
                    "sent": "So this is the four components of graph lab and it's quite easy to implement the graph lab program, so I'm going to go through the components and give you an.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example in terms of global sampling.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first component graph is very natural and you can think about it as having data in the nodes and in the edges of the graph.",
                    "label": 0
                },
                {
                    "sent": "And you can think about this.",
                    "label": 0
                },
                {
                    "sent": "Edges in a natural kind of pairwise sense, you can think about this hyper edges and so on, but just think about it in the natural pairwise sense for now.",
                    "label": 0
                },
                {
                    "sent": "And so in Gibbs sampling, we might have a graph where the nodes the nodes in the graph kamado in each node you have data that correspond to the current sample and maybe some node potentials and some edges might correspond to some edge potentials.",
                    "label": 0
                },
                {
                    "sent": "As a simple example.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the next step is an update function and update functions.",
                    "label": 1
                },
                {
                    "sent": "Again, very intuitive concept is just something that takes on a node and can read a modified data within the node.",
                    "label": 0
                },
                {
                    "sent": "The neighboring edges and the neighboring nodes.",
                    "label": 0
                },
                {
                    "sent": "So again in the gives up states case we're modifying the nodes data and we get to look at the samples current samples in neighboring nodes and the potentials in the edges.",
                    "label": 0
                },
                {
                    "sent": "This part is the very natural and intuitive I think part of graph lab that is very easy to specify in terms of a lot of our algorithms that we have in machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now a natural question from here is how do you schedule this update so?",
                    "label": 0
                },
                {
                    "sent": "You might well Graphlab will do is take updates from a queue and just perform them in parallel if possible and do that in an iterative fashion.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is what we call the scheduler or the update function.",
                    "label": 1
                },
                {
                    "sent": "Now it's not sure what to say to use a basic schedule that maybe goes in a round Robin fashion.",
                    "label": 0
                },
                {
                    "sent": "Abcdefg and so on according.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some permutation of the nodes.",
                    "label": 0
                },
                {
                    "sent": "Now the issue with that is that it's often not enough for a lot of applications, so often what will happen is some parts of a problem my converge very quickly, while other parts might need a lot more effort.",
                    "label": 0
                },
                {
                    "sent": "So what we want to be able to support in Graphlab is what's called dynamic scheduling, where you can dynamically assign nodes we perform and focus the computation again on parts that, for example, if you're doing belief propagation, have not converged yet, while other parts of graph might have converged and the same happens again.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in different application domains.",
                    "label": 0
                },
                {
                    "sent": "So the way this works.",
                    "label": 0
                },
                {
                    "sent": "Is it take to notes from the Q you perform their updates and some new nodes get inserted into the queue.",
                    "label": 0
                },
                {
                    "sent": "So for example a maybe performance update and decide to insert B into the queue or I into the queue.",
                    "label": 0
                },
                {
                    "sent": "And we can do that in a dynamic fashion, and you can write your own dynamic scheduler if you.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have that as an algorithm, but we provide you with a set of dynamics, schedulers.",
                    "label": 0
                },
                {
                    "sent": "There are very natural.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you're doing belief propagation, then there's another game called Wildfire BP, which corresponds to a first in first out queue for the scheduler.",
                    "label": 0
                },
                {
                    "sent": "There's another algorithm called residual BP, which corresponds to priority queue for the scheduler, and there's another algorithm called Slash VP that we I mentioned earlier corresponds to splash scheduler, so basically.",
                    "label": 1
                },
                {
                    "sent": "By changing one flag in an algorithm, you can run experiments for three NIPS papers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's all you have to do is change the flag and you can run different types of schedulers and this can make a lot of difference for some application domains.",
                    "label": 0
                },
                {
                    "sent": "Now, so far I've talked about the update function, the graph and how we schedule computation, but often in a lot of of the applications and a lot of algorithms, not all the data can be viewed as being part of the graph.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You might need some global information.",
                    "label": 1
                },
                {
                    "sent": "For example, if you're computing a gradient in a graphical model, you might need to aggregate information from multiple nodes, or maybe the parameters of the algorithm there, shared in a kind of relational setting.",
                    "label": 0
                },
                {
                    "sent": "May have sufficient statistics there computing some over vertices and so on, and for that it's not so natural to represent it as part of the graph.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And well, Graph Lab provides is what we call a shared data table which stores this shared bits of information that you might have.",
                    "label": 1
                },
                {
                    "sent": "Like this constants.",
                    "label": 0
                },
                {
                    "sent": "For example, the total number of samples you might want to do, or a temperature in a linear algorithm, but also all sorts of other dynamic things that you might compute.",
                    "label": 0
                },
                {
                    "sent": "So what we provide is a way to.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compute this operations which cause sync over a graph.",
                    "label": 0
                },
                {
                    "sent": "We accumulate some information similar to the reduce step in map reduce and then we apply it to some of this shared data table in parallel.",
                    "label": 0
                },
                {
                    "sent": "Computing this is called a fold operation in some sense and so for example, if my function was to add something across all nodes and applications to divide by the number of nodes, then what I'll do is just run this graph lab will go an average something over the nodes, and when it's done is going to divide it by.",
                    "label": 0
                },
                {
                    "sent": "The number of nodes and just start into the shared data table which is visible to every node in the network, or every computation node.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this third aspect is useful not just for constants, but when you have the sync operation, you can use for computing sufficient statistics, log likelihoods, gradients and all sorts of other things that are more global that need to be computed.",
                    "label": 0
                },
                {
                    "sent": "So so far I talked about 3 aspects of the graph in the update function I talked about the scheduler and the shared data table, which is this high level bits of information which are shared across the nodes.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The 4th aspect is safety and consistency.",
                    "label": 1
                },
                {
                    "sent": "It's hard to write distributed algorithms.",
                    "label": 0
                },
                {
                    "sent": "And I think one of the hardest parts of writing the script algorithm.",
                    "label": 0
                },
                {
                    "sent": "And I do have a start up now and most of my days are spent on this.",
                    "label": 0
                },
                {
                    "sent": "I were called race conditions.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example for race condition.",
                    "label": 0
                },
                {
                    "sent": "If you have one.",
                    "label": 0
                },
                {
                    "sent": "What can happen is, let's say I'm doing something on this graph and I decide to run these two nodes.",
                    "label": 0
                },
                {
                    "sent": "In parallel and one node, it does some computation and thinks that for this edge the values should be this blue graph, while the other node thinks that the value should be the green graph.",
                    "label": 0
                },
                {
                    "sent": "Since this is a parallel update, you don't know which one will take precedence.",
                    "label": 0
                },
                {
                    "sent": "You don't know exactly this kind of non deterministic process.",
                    "label": 0
                },
                {
                    "sent": "You don't know can happen.",
                    "label": 0
                },
                {
                    "sent": "So right race race condition means that these two contractor at the same time, you might end up with blue.",
                    "label": 0
                },
                {
                    "sent": "Green mixture, which might have no meaning.",
                    "label": 0
                },
                {
                    "sent": "It might be OK for some algorithms, but for other algorithms it might be disastrous.",
                    "label": 0
                },
                {
                    "sent": "And so if you're writing a parallel algorithm, you have to think about these issues again and again, and I have to say this is a really painful issue and even within Graph Lab, as soon as the developing we're dealing with our own race conditions and it's hard and we don't want you guys to think about race conditions ever again.",
                    "label": 0
                },
                {
                    "sent": "So the way we do that is by thinking about.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What kind of abstract thinking about parametrization of the abstraction that allows you to be totally race free?",
                    "label": 1
                },
                {
                    "sent": "So race free and deadlock free is something that we provide with a user tunable consistency mechanism.",
                    "label": 1
                },
                {
                    "sent": "It's a very simple idea and then.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most basic type of the idea is what we call the scoping rules, so the scoping Rule says if I'm applying an update to this particular node in the middle, which other updates can perform at the same time.",
                    "label": 0
                },
                {
                    "sent": "So since this node can touch data in a neighboring edges, an in a neighboring nodes, what we might have with full consistency is to say that I can't run any other update that might touch the same data.",
                    "label": 0
                },
                {
                    "sent": "So in other words, I could not run this note here because it might touch this data and so that you can be run in parallel.",
                    "label": 0
                },
                {
                    "sent": "So if useful consistency graph level guarantee that nobody touches the data at the same time.",
                    "label": 0
                },
                {
                    "sent": "Now the problem is, for consistency can be quite restrictive, especially in highly connected models.",
                    "label": 0
                },
                {
                    "sent": "Because if the graph is very connected, then there are really few nodes that can be run at the same time.",
                    "label": 0
                },
                {
                    "sent": "And because of that we give you.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All sorts of other update mechanisms, so if I do full consistency then that means that the only two nodes can be performed in parallel or those are two hops away from each other, which can again be difficult to have in practice in fully connected.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Highly connected graphs.",
                    "label": 0
                },
                {
                    "sent": "So in addition to for consistency, might choose to have.",
                    "label": 0
                },
                {
                    "sent": "Edge consistency, which basically says that two nodes.",
                    "label": 0
                },
                {
                    "sent": "Touch only the edge data so I can run this node and this node here in parallel and for example in belief propagation.",
                    "label": 0
                },
                {
                    "sent": "This is what happens.",
                    "label": 0
                },
                {
                    "sent": "Or in sampling give something.",
                    "label": 0
                },
                {
                    "sent": "This is what happened.",
                    "label": 0
                },
                {
                    "sent": "I can run this node in this note here in parallel and so you.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have to use edge consistency and not focus to see and so in this example you can run more nodes in parallel.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a final type of consistent which is versus consistency, which is similar to the map operations in MapReduce and you can run every node in parallel.",
                    "label": 0
                },
                {
                    "sent": "So for some update functions you might have vertex consistency needed, for others you might need only edge consistency and so on.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so here's an example.",
                    "label": 0
                },
                {
                    "sent": "And the reason you need us, the reason you need this kind of consistency rules is that you want to be able to guarantee what's called sequential consistency of your algorithm, and this is a key property of a parallel algorithm.",
                    "label": 0
                },
                {
                    "sent": "So if you design parallelism, you want the following property to hold usually.",
                    "label": 0
                },
                {
                    "sent": "For all parallel executions of the algorithm, there exists a sequential execution on one processor that would have the same kind of outcome.",
                    "label": 0
                },
                {
                    "sent": "So in other words, for this graph here, if I choose to run green, red, blue on CPU one and blue, green, green on CPU two, I want to make sure that the time is right such that there exists a sequential one CPU version that we've got in the same result.",
                    "label": 0
                },
                {
                    "sent": "And this is a key property for proving correctness of your algorithm.",
                    "label": 1
                },
                {
                    "sent": "So if you have an algorithm that works on one CPU and you have an architecture like Graph Lab which guarantees sequential consistency, then your algorithm will be automatically correct in the parallel setting.",
                    "label": 0
                },
                {
                    "sent": "And this is what we guarantee for you.",
                    "label": 0
                },
                {
                    "sent": "So in summary.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Graphlab abstraction?",
                    "label": 0
                },
                {
                    "sent": "Is about this data representation and the update functions to share data table for things like global gradients.",
                    "label": 1
                },
                {
                    "sent": "How we schedule data and how we want this update functions in scopes to happen.",
                    "label": 0
                },
                {
                    "sent": "And so for the remainder of the talk I'm going to assume that we have this graph lab model and I want to show you results and various results in different architectures and on different types of machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So let's start with the multicore setting.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Inexperienced, I'll show you we've used.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A C++ highly tuned into mentation with P threads and I'm going to show you results of 16 core machines and a variety of different types of algorithms were tested and I think I'm going to show you examples in a field because you know the number of experience is huge, but I'll start with a simple graphical modeler.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example, so this is some work.",
                    "label": 0
                },
                {
                    "sent": "And they came out of Gary Miller's group, where they were trying to do some image denoising type of task on 3D retinal image data.",
                    "label": 0
                },
                {
                    "sent": "And so here you have graphical model that has 1 million vertice is, it's a grid, is a 3D grid lattice.",
                    "label": 0
                },
                {
                    "sent": "The update function is belief propagation and the sync function is what allows you to compute the gradients that are going to be taken when you do parameter learning.",
                    "label": 1
                },
                {
                    "sent": "I'll say a million nodes graphical model is a pretty big model.",
                    "label": 1
                },
                {
                    "sent": "But the other models that were going to later on quite a bit bigger than these, but the other problem is going to require bigger this.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get up there, we get will see a lot of these curves, Ann.",
                    "label": 0
                },
                {
                    "sent": "On the X axis, as we increase the number of CPUs on the Y axis, is how much faster it gets and this dotted line would be if we got linear speedup.",
                    "label": 0
                },
                {
                    "sent": "For example 16 CPUs we got 16 times faster, so this kind of an idealized line.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You can try a variety of scheduling algorithms, schedulers and what you see is that we get close to linear speedup on the algorithm as we increase the number of processors that we use and hear.",
                    "label": 0
                },
                {
                    "sent": "This plus schedule is slightly better than a priority queue based scheduler.",
                    "label": 0
                },
                {
                    "sent": "Now since we have this architecture, you can try a lot of things.",
                    "label": 0
                },
                {
                    "sent": "Once your algorithm is implemented within Graph Lab and let me show you one of the things that you could try to run and it's related actually to Johnson cyclist talk this morning in a way that he was thinking about interleaving gradient updates and the averaging process.",
                    "label": 0
                },
                {
                    "sent": "Here what you can try is to say normally with graphical models you do inference that you converge and then you take a gradient step and interesting again.",
                    "label": 0
                },
                {
                    "sent": "Step one thing that you could ask is what happens if you do parallel inference and gradient steps.",
                    "label": 0
                },
                {
                    "sent": "So you do inferencing gradient updates at the same time, what happens?",
                    "label": 0
                },
                {
                    "sent": "And this is just what happens answer and what happens is if you iterated versus if you do them simultaneously, you are about 3 times faster.",
                    "label": 0
                },
                {
                    "sent": "And the quality of the answer.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is about the same, so we can't yet explain from a theoretical perspective because this is based on VP like updates and so on.",
                    "label": 0
                },
                {
                    "sent": "Why exactly this is giving the correct answers or the same answers, but you get quite a bit of speedup and graph lab allows us to do this with a very very minimal change in what you tell it to do.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 example, let me move to the next example which is lost.",
                    "label": 0
                },
                {
                    "sent": "So so this next few examples just exemplify some issues that come up when you when you try to run the algorithms in parallel settings so.",
                    "label": 0
                },
                {
                    "sent": "A lot of us have been looking at these types of problems lately and the idea is you have a regression problem.",
                    "label": 0
                },
                {
                    "sent": "So you want to fit Y with the feature vector X * W and you have an L1 regularization penalty, and the idea here is that X might be a sparse matrix, so faxes sparse matrix, you can think of as a sparse relationship between the weights and the wise.",
                    "label": 0
                },
                {
                    "sent": "And you can implement this in graph lab and one issue that happens is if I use what's called the shooting algorithm for this.",
                    "label": 0
                },
                {
                    "sent": "When I updating weights one I need to look at or influence Y1 and Y-3 and so any other weights that touches Y1 or Y-3 cannot be run at the same time, so we're only allowed to run one here and W 5 at the same time.",
                    "label": 0
                },
                {
                    "sent": "And so because of that we require the full consistency model remain talked about different types of consistency model.",
                    "label": 1
                },
                {
                    "sent": "Focusing model here is required for the correctness of the algorithm, so I can only.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "W1 and W5, and if you try to run the focus distance model, this is what happens.",
                    "label": 0
                },
                {
                    "sent": "As increasing numbers, CPUs and you have a dense matrix, that means that a lot of things and touch each other.",
                    "label": 0
                },
                {
                    "sent": "Your speed up is quite poor.",
                    "label": 0
                },
                {
                    "sent": "With 16 processors you only get about two times speedup and even a sparse problem.",
                    "label": 0
                },
                {
                    "sent": "You only get about four times speedup with 16 processors, which is not that exciting, you should say.",
                    "label": 0
                },
                {
                    "sent": "But one thing that you can try.",
                    "label": 0
                },
                {
                    "sent": "Just try with a change of flag is just say.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I just hope for the best for relaxed consistency, I don't require full consistency and they just try to run the algorithm.",
                    "label": 0
                },
                {
                    "sent": "What kind of scaling do we get?",
                    "label": 0
                },
                {
                    "sent": "And it turns out that you get much better scaling.",
                    "label": 0
                },
                {
                    "sent": "Even in dense models then you got before and.",
                    "label": 0
                },
                {
                    "sent": "Even though the correctness is not guaranteed, an two of my students are popular and justifiably have some nice ideas proving why this works.",
                    "label": 0
                },
                {
                    "sent": "You actually get much, much nicer scaling.",
                    "label": 0
                },
                {
                    "sent": "In this example, you get about 10 times or 16 processors, so again, you change the type of requirement that you put in the algorithm on the architecture.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can get better scaling.",
                    "label": 0
                },
                {
                    "sent": "Why do you not?",
                    "label": 0
                },
                {
                    "sent": "See as much improvement for this part.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "I think because of the data loading issues and some issues related to that, I think this is kind of some initial experiments and we still have a lot to explore in this part.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the third example that I'm going to show you is the idea is an algorithm called coem that tries to do named entity recognition.",
                    "label": 0
                },
                {
                    "sent": "So an immense recognition task and the idea is can I ask the question is dog and animal is Catalina Place and so on.",
                    "label": 1
                },
                {
                    "sent": "And the way that it's done.",
                    "label": 0
                },
                {
                    "sent": "As you can imagine, have a bipartite graph where you have the instances you found like the dog Australia, Catalina and so on, and then they related to other processes and found some X.",
                    "label": 0
                },
                {
                    "sent": "Run quickly.",
                    "label": 0
                },
                {
                    "sent": "Somebody travel to some X which was Australia.",
                    "label": 1
                },
                {
                    "sent": "X is pleasant and so on.",
                    "label": 0
                },
                {
                    "sent": "And based on this graph and knowing that Australia is a place, for example, we can propagate that information at them like algorithm to get.",
                    "label": 0
                },
                {
                    "sent": "To get some named entity recognition type of outcomes and tomatoes group at CMU was working on some very large problems in this sum based on about 2 million vertices in this graph and 200 million edges.",
                    "label": 0
                },
                {
                    "sent": "So there's a pretty large problems and they were so large that they were having a hard time.",
                    "label": 0
                },
                {
                    "sent": "Dealing with this data set size and they were using Hadoop.",
                    "label": 0
                },
                {
                    "sent": "With the cluster that Yahoo has provided us.",
                    "label": 0
                },
                {
                    "sent": "And with 95 cores they were getting about 7 1/2 hours running time which they had to do because the data set was so large.",
                    "label": 0
                },
                {
                    "sent": "Now this is the type of problem.",
                    "label": 0
                },
                {
                    "sent": "The graph lab is good for.",
                    "label": 0
                },
                {
                    "sent": "Their sparse interactions.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So graph base is an iterative algorithm, and so on.",
                    "label": 0
                },
                {
                    "sent": "And if you implement this in graph Lab.",
                    "label": 0
                },
                {
                    "sent": "Again, you get very life.",
                    "label": 0
                },
                {
                    "sent": "You get very nice scaling, not as much for the smaller problems as you do for the larger problems.",
                    "label": 0
                },
                {
                    "sent": "Because the larger problem is more computation to them.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If in the original Hadoop implementation of 95 cores it took 7 1/2 hours.",
                    "label": 0
                },
                {
                    "sent": "With graph Lab with 16 cores it took about 30 minutes.",
                    "label": 0
                },
                {
                    "sent": "So 60 * 6 times fewer CPUs 15 times faster.",
                    "label": 0
                },
                {
                    "sent": "I think this is a exciting example of the types of things that you can do by thinking more about your computation.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you do.",
                    "label": 0
                },
                {
                    "sent": "If you do a MapReduce type presentation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The next part that we've been working on quite a bit lately is moving towards cloud computing, so I talked about the multi core types of results.",
                    "label": 0
                },
                {
                    "sent": "Let me talk for a little bit about.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The cloud computing setting turns out that it's really expensive.",
                    "label": 0
                },
                {
                    "sent": "To build a cluster, it's not just expensive to buy the computers, but man we pay a lot to maintain them.",
                    "label": 0
                },
                {
                    "sent": "And basically this computers are only used during the deadlines.",
                    "label": 0
                },
                {
                    "sent": "Most of the time they're sitting idle.",
                    "label": 0
                },
                {
                    "sent": "You guys should be working harder.",
                    "label": 0
                },
                {
                    "sent": "I don't understand where they sitting idle.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Because of that, there's been a move towards various cloud computing type of resources where you can buy time and access to hundreds or thousands of processors and only pay for the resources that you actually need.",
                    "label": 1
                },
                {
                    "sent": "And in this setting I think this, I think a very exciting setting to move towards as a model for computation in machine learning.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To deal with this setting we had to deal with many significant issues, which I'm not going to talk about in a lot of detail, but basically instead of having the multicore setting where you have one shared memory, now we have memory distributed across many machines, so you have to optimize.",
                    "label": 0
                },
                {
                    "sent": "For example how the data is partitioned and moved around.",
                    "label": 0
                },
                {
                    "sent": "You have limited bandwidth between machines, so.",
                    "label": 0
                },
                {
                    "sent": "We have a quite nice smart caching and interleaving of communication computation that allow you to deal with this limited bandwidth and the high latency so it takes time to send data between machines.",
                    "label": 1
                },
                {
                    "sent": "This is some ideas that were implemented Graphlab based on pushing data preemptively andsome other latency hiding mechanisms that make latency a lot less of an issue and these are fairly hardcore, low level and middle level issues that I think most machine learning researchers don't want to deal with.",
                    "label": 0
                },
                {
                    "sent": "And so if you don't want to deal with any of these, you can just.",
                    "label": 0
                },
                {
                    "sent": "You know I'm being a little facetious, but you can just implement their stuff within graph Lab and will take care of those for you.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "We have a highly optimized implementation which is testable from computer clusters.",
                    "label": 1
                },
                {
                    "sent": "An Amazon EC2 an here's a nice thing.",
                    "label": 0
                },
                {
                    "sent": "The team up for one of my students and the rest team have created a very nice script where all you have to do is have a easy to account and will automatically get the machines for you.",
                    "label": 0
                },
                {
                    "sent": "Push the code out installing them.",
                    "label": 0
                },
                {
                    "sent": "It's extremely easy to use, you don't have to know anything about EC2 basically, so if your stuff works in graph lab and you have a credit card.",
                    "label": 0
                },
                {
                    "sent": "You cannot.",
                    "label": 0
                },
                {
                    "sent": "You can use use the resource quite easily and we thoroughly experimented on the number of different case studies.",
                    "label": 0
                },
                {
                    "sent": "I'm going to basically talk about three of them Co M.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The one they just talked about.",
                    "label": 0
                },
                {
                    "sent": "This factorization and video call segmentation.",
                    "label": 0
                },
                {
                    "sent": "So the setting that will look at that there are two types of machines you can get from Amazon, one of regular nodes which have 8 cores and this is high performance computing nodes 16 cores where they try to put machines closer together for you, so there's less latency in communication.",
                    "label": 1
                },
                {
                    "sent": "And so this is 2 settings we're going to talk about an.",
                    "label": 0
                },
                {
                    "sent": "The first problem will talk about is the Co M problem.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's not much to talk about here.",
                    "label": 0
                },
                {
                    "sent": "I mean, before we had Hadoop taking seven half hours Graphlab taking 30 minutes, it's the same graph lab.",
                    "label": 0
                },
                {
                    "sent": "It's implemented, same problem except we do it in EC2 and using 32 ECT machines.",
                    "label": 0
                },
                {
                    "sent": "We solve the same problem in 80 seconds.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Improvement in time.",
                    "label": 0
                },
                {
                    "sent": "There's all sorts of things that go into that.",
                    "label": 0
                },
                {
                    "sent": "Not writing things to disk.",
                    "label": 0
                },
                {
                    "sent": "The fact that we can focus computation so the schedulers can focus computation where it's needed on the parts of problem, having confers the fact that we deal directly with communication between nodes.",
                    "label": 0
                },
                {
                    "sent": "So there's also underlying communication issues that we do because it's a graph.",
                    "label": 0
                },
                {
                    "sent": "I know where my data is going when the graph is partitioned, so there's a lot of things that come together is not just a factor in writing to disk, you're not going to run central machine like you do with Hadoop.",
                    "label": 0
                },
                {
                    "sent": "The underlying architecture is quite differently built.",
                    "label": 0
                },
                {
                    "sent": "Why is it not so much faster?",
                    "label": 0
                },
                {
                    "sent": "Why is what?",
                    "label": 0
                },
                {
                    "sent": "What is the not so much faster?",
                    "label": 0
                },
                {
                    "sent": "O32EC2 machines 256 processors.",
                    "label": 0
                },
                {
                    "sent": "So, but it was about zero point 3% of Hadoop time.",
                    "label": 0
                },
                {
                    "sent": "Compatible number of processors.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about money in a minute.",
                    "label": 0
                },
                {
                    "sent": "The answer is not much money.",
                    "label": 0
                },
                {
                    "sent": "But money is a beautiful thing when it comes to this results, and I have a beautiful graph related to money, yes.",
                    "label": 0
                },
                {
                    "sent": "Kind of machines, the high performance ones.",
                    "label": 0
                },
                {
                    "sent": "This is the high performance ones.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll show a graph.",
                    "label": 0
                },
                {
                    "sent": "I think about comparing the two types of machines.",
                    "label": 0
                },
                {
                    "sent": "High performance machines are better for a lot of competitions that we want to do.",
                    "label": 0
                },
                {
                    "sent": "I would say.",
                    "label": 0
                },
                {
                    "sent": "But we did a lot of these comparisons.",
                    "label": 0
                },
                {
                    "sent": "So before I get to money before before I get to the money graph there when I show you I'll just make a little money statement writing this paper.",
                    "label": 0
                },
                {
                    "sent": "What we did is not.",
                    "label": 0
                },
                {
                    "sent": "I have a lot more experiments in EC2 then then I'm going to show.",
                    "label": 0
                },
                {
                    "sent": "Today we have a lot more experiments cost about $2000.",
                    "label": 0
                },
                {
                    "sent": "Anso 4000 four $1000 yeah sure.",
                    "label": 0
                },
                {
                    "sent": "For $4000 and this wasn't a setting where we're doing lots of scaling experiments, trying lots of different machine sizes, trying lots of different types of ways of using the cluster, and because we didn't know as much about EC2.",
                    "label": 0
                },
                {
                    "sent": "And let's say we.",
                    "label": 0
                },
                {
                    "sent": "I mean me we had to rerun the same things again again and so on so.",
                    "label": 0
                },
                {
                    "sent": "Compare that cost if the costs are cost me to maintain machines at Carnegie Mellon and bias on and think about how many papers we produce in the group.",
                    "label": 0
                },
                {
                    "sent": "Guys, please, I think that's a very much cheaper than the amount of money I'm spending on cluster honestly.",
                    "label": 0
                },
                {
                    "sent": "Anyway, before before I forget, let's try to finish the talk.",
                    "label": 0
                },
                {
                    "sent": "So about zero point, 3% of Hadoop time.",
                    "label": 1
                },
                {
                    "sent": "So this is the code name example.",
                    "label": 0
                },
                {
                    "sent": "Here's the video called Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Station example.",
                    "label": 0
                },
                {
                    "sent": "I have a video and this is some images of Carnegie Mellon and what I do is segment.",
                    "label": 0
                },
                {
                    "sent": "This image is such that this part here of Blue Sky has the same semantics as this part here of Blue Sky.",
                    "label": 1
                },
                {
                    "sent": "So it's not just their segmented as image segmentation, but the segment in that kind of coherent way across different frames of the video.",
                    "label": 1
                },
                {
                    "sent": "And there's a ton of data if you think about what you think about videos and it's hard to find a right kind.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Segmentation and natural but naive idea is to break images into patches and just to say.",
                    "label": 0
                },
                {
                    "sent": "Running M type of algorithm that tries to label patches with different segments and figure out what the appearances of Sky.",
                    "label": 0
                },
                {
                    "sent": "Of course this is totally unlabeled.",
                    "label": 0
                },
                {
                    "sent": "There's no examples of Sky, but I want to do it and discover through some kind of clustering that does examples of Sky and this is naive because it doesn't take the spatial structure into account.",
                    "label": 0
                },
                {
                    "sent": "So a fancier better.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea is to do the same kind of EM algorithm, But using some kind of graphical model structure where you say that neighboring patches within an image and across images have similar labels or similar types in the cluster.",
                    "label": 0
                },
                {
                    "sent": "And you can do an M type of algorithm and this is not all right dear of doing this type of thing.",
                    "label": 0
                },
                {
                    "sent": "This work by Dhruv Batra.",
                    "label": 0
                },
                {
                    "sent": "But we implemented this within Graph Lab with a number of different extensions of how to do this in the video setting and we get some really kind of interesting results.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Huge problems, there's about 23 million nodes in this graph chamado an 390 million edges.",
                    "label": 1
                },
                {
                    "sent": "And we get some very nice scaling.",
                    "label": 0
                },
                {
                    "sent": "As in case number of processors and you can see the standard, there's a gap.",
                    "label": 0
                },
                {
                    "sent": "I think about Factor 2 or so between the standard processors in the high performance processors, but for example with using 32 machines were getting a runtime of the order of 100 seconds or less on a graphical model with 20,000,000 nodes, which I think.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a pre impressive running time?",
                    "label": 0
                },
                {
                    "sent": "And because there's a cost benefit tradeoff, as increasing number of machines, you can build this pretty cool kinds of graphs.",
                    "label": 0
                },
                {
                    "sent": "So is add more machines.",
                    "label": 1
                },
                {
                    "sent": "I pay more money and the running time is less, so you get this nice kind of Nikkor or Pareto kind of curve where a few machines.",
                    "label": 0
                },
                {
                    "sent": "You know it's cheap, but it takes a long time with a lot of machines, is expensive, but it doesn't help as much and there's a nicely in the curve where you get kind of the nice cost to running time ratio.",
                    "label": 0
                },
                {
                    "sent": "And so here we're talking about $0.70 or something for doing inference in a graphical model of that size.",
                    "label": 0
                },
                {
                    "sent": "So think about that setting.",
                    "label": 0
                },
                {
                    "sent": "Two types of machines in Amazon EC2 cluster.",
                    "label": 0
                },
                {
                    "sent": "There is the standard machines which they are not guaranteed to be Co located with each other and the high performance computing machines which are kind of more guaranteed to be closer together in there.",
                    "label": 0
                },
                {
                    "sent": "Data warehouse.",
                    "label": 0
                },
                {
                    "sent": "And they're a little faster too.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so pretty interesting tradeoff curves and.",
                    "label": 0
                },
                {
                    "sent": "Here's the third example we talk about today.",
                    "label": 0
                },
                {
                    "sent": "Briefly, is a type of Netflix challenge type of example where you want to do Bayesian tensor factorization so you have a Netflix style data where people rated movies at different times and you want to break it into some factors over users, movies and time, and the graph is Paris because you know there's a sparse relationship between movies and user ratings, and so on.",
                    "label": 1
                },
                {
                    "sent": "And again, you can do.",
                    "label": 0
                },
                {
                    "sent": "Various algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This type of problem we tried one of the state of the art once an.",
                    "label": 0
                },
                {
                    "sent": "As you increase the number of machines, we get better scaling again properties.",
                    "label": 0
                },
                {
                    "sent": "HPC node.",
                    "label": 0
                },
                {
                    "sent": "We get pretty good scaling properties of 32 nodes.",
                    "label": 0
                },
                {
                    "sent": "We get about 18 times speedup.",
                    "label": 0
                },
                {
                    "sent": "Across the cloud computing setting, which I think is quite impressive result and again you get this cost to time tradeoff.",
                    "label": 0
                },
                {
                    "sent": "An it really does not make sense to buy regular nodes for this problem, as you can see the knee of the curve is about $0.80 for HPC nodes, while it's like $1.60 or dollar 50 for the regular nodes.",
                    "label": 1
                },
                {
                    "sent": "And just as a.",
                    "label": 0
                },
                {
                    "sent": "Just to give you a sense, the paper they're implemented from this 2010 paper by showing at all where they use one core and it took them about 2000 cycles per iteration and we used 156 cores.",
                    "label": 0
                },
                {
                    "sent": "It takes about 6 seconds.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just to close off Graph Lab is available today.",
                    "label": 1
                },
                {
                    "sent": "We have the multi core version available at one point 1 level and will distribute the cloud computing version soon.",
                    "label": 0
                },
                {
                    "sent": "Go graph lab.ms.cmu.edu and we have.",
                    "label": 0
                },
                {
                    "sent": "Acquire",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interesting plan of where we're working.",
                    "label": 0
                },
                {
                    "sent": "It's a highly optimized native C, and there's a graph lab engine that can sit on top of multicore server setting and we're working on GPU and on top of this, if you're not so comfortable with the native C++ setting, we can think about Matlab, Java, or Python.",
                    "label": 0
                },
                {
                    "sent": "So the native C plus.",
                    "label": 1
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This implementation gives you access to more features and it's going to be faster for sure, but you can also get reasonable scaling if you use our Java API or a Python API an R.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Latest.",
                    "label": 0
                },
                {
                    "sent": "API is a MATLAB based one which is fresh off the press and I think it's quite exciting and in some applications that we tested it actually runs faster than Matlab.",
                    "label": 0
                },
                {
                    "sent": "Because it compiles the code and there's.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bunch of things underneath that you don't have to worry about, so as an example you could write a MATLAB function.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through this, but it's like belief propagation, Matlab function and all you have is some extra function that says get your vertex data and graph lab will give that to you and add some some tasks to the queue.",
                    "label": 0
                },
                {
                    "sent": "And the rest of your MATLAB code is more or less the same.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So summarizing with graph lab, you will be able to implement your algorithm in terms of this four aspects data graph, update, function share data table, the scheduler, and the scope constraints and.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think it provides a very nice parallel structure for a lot of machine learning algorithms and compactly expressed a lot of iterative computations that we do today.",
                    "label": 0
                },
                {
                    "sent": "Yeah, she is state of the art parallel performance an is I believe quite easy to use as an abstraction.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we're working on many issues related to this.",
                    "label": 0
                },
                {
                    "sent": "So for example, this setting we have an address, an issue of robustness, which is addressed, for example by MapReduce today, and this GPU setting and so on.",
                    "label": 0
                },
                {
                    "sent": "But you can imagine thinking about state of the art perform parallel performance for your.",
                    "label": 1
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm here by implementing it within Graph Lab.",
                    "label": 0
                },
                {
                    "sent": "So try it out and let us know what you think.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alice.",
                    "label": 0
                },
                {
                    "sent": "So what about the addressing the computational challenges?",
                    "label": 0
                },
                {
                    "sent": "Talk about dealing with the CPU bottleneck, making computation safe, telecommunications safe and efficient.",
                    "label": 0
                },
                {
                    "sent": "But as is illustrated in the cloud setting you when you move to these more distributed systems, you then start have to deal with other other bottlenecks like disk or network.",
                    "label": 0
                },
                {
                    "sent": "So my question is and you could have slept a lot of that detail look like.",
                    "label": 0
                },
                {
                    "sent": "But us.",
                    "label": 0
                },
                {
                    "sent": "If I'm trying to sterilize my my outlook program.",
                    "label": 0
                },
                {
                    "sent": "I'm I'm curious more about your sense of where should I spend my time.",
                    "label": 0
                },
                {
                    "sent": "Should I spend more money so I spend more time thinking about how to locate data with computation?",
                    "label": 0
                },
                {
                    "sent": "So the question.",
                    "label": 0
                },
                {
                    "sent": "Right, the question is what's the bottleneck in the same parallel algorithms, Ann?",
                    "label": 0
                },
                {
                    "sent": "So the philosophy that we're trying to propose here is that we're going to take care of, for example, Co.",
                    "label": 0
                },
                {
                    "sent": "Locating data and computation for you.",
                    "label": 0
                },
                {
                    "sent": "As you know, Graph Lab one point 1.0 son maybe that will get better overtime, but that's what we're trying to make better for you.",
                    "label": 0
                },
                {
                    "sent": "What you can do for us is design parallel algorithms that can be written in a certain way that allow for the underlying optimization to happen.",
                    "label": 0
                },
                {
                    "sent": "And so if you algorithm is expressed in terms of our abstraction, it's easier for us to provide that optimization of collocating data.",
                    "label": 0
                },
                {
                    "sent": "Thinking about where updates to go, how to distribute the data, and so on and so that.",
                    "label": 0
                },
                {
                    "sent": "So what's easiest for us in terms of limitations of the approach right now, I think it works best when the graph is sparser obviously, and it works best when they update functions.",
                    "label": 0
                },
                {
                    "sent": "The more computation there isn't an update function.",
                    "label": 0
                },
                {
                    "sent": "Right, so the simple update function, the harder it is for us, and we've tried problems with update function is very quick and we still do OK, but not always.",
                    "label": 0
                },
                {
                    "sent": "But this is the limitations of the current type of architecture we have.",
                    "label": 0
                },
                {
                    "sent": "But the idea is if you can implement this way, we're going to come up with different algorithms that go underneath that will optimize the kind of question that you asked.",
                    "label": 0
                },
                {
                    "sent": "Does that help?",
                    "label": 0
                },
                {
                    "sent": "My marriage license.",
                    "label": 0
                },
                {
                    "sent": "Did you read this?",
                    "label": 0
                },
                {
                    "sent": "I actually forget with that, what do we do?",
                    "label": 0
                },
                {
                    "sent": "GPL right?",
                    "label": 0
                },
                {
                    "sent": "L GPL yeah website.",
                    "label": 0
                },
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "I I honestly I forgot OK here, let me rephrase this question.",
                    "label": 0
                },
                {
                    "sent": "If you guys have a license that is most appropriately on, talk to us about different types of licenses will be open.",
                    "label": 0
                },
                {
                    "sent": "We had a long discussion about this and we're on the fence about different types of license, so I'm happy to hear feedback from the community about the type of license you prefer.",
                    "label": 0
                },
                {
                    "sent": "Updates.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Horse adventure it's about what other kinds of abstraction considered along the way, and oh, what, how?",
                    "label": 0
                },
                {
                    "sent": "Oh, I see.",
                    "label": 0
                },
                {
                    "sent": "How many hours do you have?",
                    "label": 0
                },
                {
                    "sent": "I have to say that we argued about these for a long, long time.",
                    "label": 0
                },
                {
                    "sent": "The I think the most complex the hardest question to decide, was how simple we could make the sync operation.",
                    "label": 0
                },
                {
                    "sent": "Because you can imagine having.",
                    "label": 0
                },
                {
                    "sent": "Because right now we're assuming that is something that you can aggregate over the nodes.",
                    "label": 0
                },
                {
                    "sent": "So certain associativity and distributivity properties are assumed of the function of trying to compute an.",
                    "label": 0
                },
                {
                    "sent": "So all sorts of things came up when we try to discuss how to get that right, and I think we have a reasonable assumption right now.",
                    "label": 0
                },
                {
                    "sent": "But that was kind of the biggest bottleneck.",
                    "label": 0
                },
                {
                    "sent": "The other one was pretty natural, I thought.",
                    "label": 0
                },
                {
                    "sent": "How about can you change the structure of the graph?",
                    "label": 0
                },
                {
                    "sent": "The graph is user change overtime or change.",
                    "label": 0
                },
                {
                    "sent": "So that that is in the works.",
                    "label": 0
                },
                {
                    "sent": "It's not version 1.1.",
                    "label": 0
                },
                {
                    "sent": "Not really so.",
                    "label": 0
                },
                {
                    "sent": "You should think about as a single queue, but you can have and implement a single queue in the parallel setting.",
                    "label": 0
                },
                {
                    "sent": "That's a mistake.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of different cues, and there's some smart way of dealing with different queues.",
                    "label": 0
                },
                {
                    "sent": "Would you want to have different queues for yourself?",
                    "label": 0
                },
                {
                    "sent": "Just raise 23 million.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, no no, that's not how it's implemented.",
                    "label": 0
                },
                {
                    "sent": "There's alot of things that are there in abstractions, not how it's implemented for you, but that's how we should think about.",
                    "label": 0
                },
                {
                    "sent": "John.",
                    "label": 0
                },
                {
                    "sent": "Yes, we did.",
                    "label": 0
                },
                {
                    "sent": "We're just about this relative speedup graphs and I guess absolute numbers kind of matter to some extent.",
                    "label": 0
                },
                {
                    "sent": "Also, can you give this some sense of like the absolute time for the lasso program for for the, for the last?",
                    "label": 0
                },
                {
                    "sent": "So I don't know if I.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I lost couple too if he remembers, but for we have absolute times for some algorithms up here.",
                    "label": 0
                },
                {
                    "sent": "Can I go back?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so 66 seconds for 2656 cores for a problem that was.",
                    "label": 0
                },
                {
                    "sent": "The whole Netflix data.",
                    "label": 0
                },
                {
                    "sent": "Forget how big that is, honestly.",
                    "label": 0
                },
                {
                    "sent": "Seconds for 230 million we can give you a lot more numbers.",
                    "label": 0
                },
                {
                    "sent": "In fact, there's a tricky so Joey wants to jump in.",
                    "label": 0
                },
                {
                    "sent": "That I'm sending the paper, of course.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's hard to compare.",
                    "label": 0
                },
                {
                    "sent": "There's a tricky thing with this type of graph which is relative speedups.",
                    "label": 0
                },
                {
                    "sent": "You always want the relative speed up to be with respect to the best single core implementation, right?",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can have an algorithm that has great scaling.",
                    "label": 0
                },
                {
                    "sent": "For example, MapReduce.",
                    "label": 0
                },
                {
                    "sent": "BP has great scaling linear, but one processor is so slow, so that's something that you have to think about when you start thinking about parallel implementations.",
                    "label": 0
                },
                {
                    "sent": "Cost versus time graph.",
                    "label": 0
                },
                {
                    "sent": "I then will have their state.",
                    "label": 0
                },
                {
                    "sent": "How come it's always this is overall cost for cost per node or not.",
                    "label": 0
                },
                {
                    "sent": "This is the cost for your computation.",
                    "label": 0
                },
                {
                    "sent": "How much it cost you to solve this machine learning problem?",
                    "label": 0
                },
                {
                    "sent": "How come it doesn't?",
                    "label": 0
                },
                {
                    "sent": "How come we just how come?",
                    "label": 0
                },
                {
                    "sent": "It's a function, right?",
                    "label": 0
                },
                {
                    "sent": "Because you could imagine that on one node it takes a long time, and even though it's not the only other people can do it because of the time it actually ends up costing more.",
                    "label": 0
                },
                {
                    "sent": "Oh no, Oh no.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see I see I see.",
                    "label": 0
                },
                {
                    "sent": "So wait, wait, so join.",
                    "label": 0
                },
                {
                    "sent": "I actually don't know.",
                    "label": 0
                },
                {
                    "sent": "Actually I thought I saw it.",
                    "label": 0
                },
                {
                    "sent": "The X axis running time because you try to try to spawn smaller going right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so think about this is a lower envelope or something.",
                    "label": 0
                },
                {
                    "sent": "Right, it should, yeah.",
                    "label": 0
                },
                {
                    "sent": "At least in an hour.",
                    "label": 0
                },
                {
                    "sent": "OK so there is.",
                    "label": 0
                },
                {
                    "sent": "There is a subtlety here in this graph.",
                    "label": 0
                },
                {
                    "sent": "What's up?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So you have to pay for at least an hour, so this is like you can imagine you would have to do cross validation runs or something to use up that matter.",
                    "label": 0
                },
                {
                    "sent": "But yeah, here here is assuming that you could infinitely divide EC2, which you can't, so the graph would be different for you.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}