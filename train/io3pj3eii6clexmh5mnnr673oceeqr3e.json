{
    "id": "io3pj3eii6clexmh5mnnr673oceeqr3e",
    "title": "Cross-Lingual Document Retrieval through Hub Languages",
    "info": {
        "author": [
            "Jan Rupnik, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Jan. 11, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Information Retrieval",
            "Top->Computer Science->Multilingual Information Access",
            "Top->Computer Science->Optimization Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_rupnik_hub_languages/",
    "segmentation": [
        [
            "I'm beyond rupnick.",
            "This is joint work with Andre Mouton.",
            "Primo Scrabble we are working on.",
            "Cross lingual document retrieval through hub languages.",
            "So."
        ],
        [
            "I'll first tell you about the motivation.",
            "Then show your short demo, then explain about the method and tell you about some experiments."
        ],
        [
            "So our motivation is we want to learn language independent document representation.",
            "So this would enable us to compare the documents written in different languages which could then enable us to do cross lingual information retrieval, classification or clustering.",
            "So are are to find this language independent representation.",
            "We use an aligned comperable corpus, for example Wikipedia.",
            "So the documents there are just comperable, not necessarily translations, and we looked into the problem of missing data.",
            "For example, for some language pairs there may be very little or even no alignment information, so we don't have any.",
            "Documents to train, but there is a solution if both of these documents are well aligned to one major language, which we call a hub language."
        ],
        [
            "So.",
            "And this is the picture, so this is English, Slovenian and Hindi.",
            "And there are very many Wikipedia articles that are that are present in both in Slovenian and Hindi and put Slovenia and Hindi.",
            "Are both share a lot of articles with English.",
            "So the idea is if we can so now the."
        ],
        [
            "This.",
            "Our goal is to find sets of mappings P1P2 and P3 that can map the points into the this low dimensional common space so that ideally the points that are aligned so there are about the same subject should be mapped closed close to each other and then all the tasks are basically like monolingual tasks.",
            "So I can show you.",
            "Short example of what we implemented, so this is based on Wikipedia.",
            "Here we have two articles, one is English, one is Spanish and so we can, so they're on the same event, but they're not translations of each other, and the idea is that if we have this language independent representation, we can map these two articles into the same space and then compute similarity between them.",
            "And we see that we had.",
            "So this is cosine similarity in the.",
            "Common semantic space.",
            "Then our goal was to perform this, find this representation for documents for language pairs that have very little alignment information.",
            "So this would be, let's say.",
            "First, we can look at article about Abraham Lincoln.",
            "In one is English.",
            "One is Slovenian, so Slovenian Wikipedia is a lot smaller than English Wikipedia.",
            "But still there are like 50,000 aligned articles between them, and we can compute the similarity.",
            "So the similarity, even though it's the the articles are compareable, so the similarities still pretty strong.",
            "Now if we go to the next example, we have Wikipedia articles about Abraham Lincoln and one is in Slovenian and the other one is in Piedmontese A.",
            "So this is a language in northern Italy that spoken by 2 million people and Slovenian Piedmontese have almost the alignment information is very scarce and if we compute the similarity between them.",
            "We see that the number is very low, even though the articles are about the same subject.",
            "But if we take a random Wikipedia article from Piedmontese here.",
            "We get even lower similarity, so even though the similarity score was very low, there were still some some signal in that.",
            "So different articles generally have lower similarities, but we have to take into account that so the data here is very noisy and almost orthogonal in the semantic space.",
            "But still some some signal is retained.",
            "So.",
            "Now I can.",
            "I don't want to.",
            "Huh, so this was the the idea here.",
            "So we saw that.",
            "How this works in?"
        ],
        [
            "Practice so now I can tell you about the approach.",
            "So we first we have M languages and one language.",
            "The first language is the Hub language, which is well connected to the others.",
            "So the idea is we estimate the empirical cross covariance matrices and then stack them up and just use the covariance matrices related to the hub language.",
            "Because the other matrices might be unreliable.",
            "And perform a singular value decomposition of this.",
            "And by this we get initial set of subspaces for each of the non hub languages and the hub language and we use this subspace is to reduce the dimensionality of our problems.",
            "So instead of having hundreds of thousands of dimensions, we map it into our let's say 500 dimensional space.",
            "So, so this first step is very similar to Latin semantic indexing."
        ],
        [
            "And then the second step is once we are in this lower dimensional space, we can try to find the best the best directions that have maximal linear dependence between the so our languages and the hub language.",
            "So this can be.",
            "This can be opposed as an optimization problem, which can be solved as an eigenvalue problem.",
            "So here in two parts we use the fact that we discarded the other covariance matrices and used only the hub hub and hub related matrices.",
            "So this this enabled us to, so it's served.",
            "It has two purposes, so the other cross covariance matrices might be very unreliable.",
            "And the other thing is that we can get scaleability, scale up the algorithm by using this.",
            "So this is very convenient in Wikipedia because English language is is has this hub language characteristics.",
            "And the step one which I mentioned, the latent semantic indexing also regularize is this second step, so there's no need to for stronger regularization here because we are working in low dimensional spaces."
        ],
        [
            "So in the end, these mappings that I mentioned are just products of the first step.",
            "In the second step.",
            "So this second step is like a refinement of the of the topics, and this can be seen as topic vectors.",
            "Yep.",
            "Not.",
            "X2 to X3 language pairs, so if their alignments between them we didn't use them because typically if the alignment exists that also the alignments to the English one existed.",
            "So the documents were still used.",
            "And this cases where alignment existed between them and no alignment with the English.",
            "These are very rare, so we didn't throw away much information.",
            "Done so.",
            "Not help.",
            "So it can also be extended, but so this was the main thing was that if you have just a few documents here, the covariance matrix is very unreliable because it's a high dimensional matrix with just a few observation or even a few 100.",
            "It can more hurt the performance than it can help it.",
            "So this was the in the approach.",
            "We just use this.",
            "This covariance queries is based on this information."
        ],
        [
            "So we evaluated it on the Wikipedia on 8 languages, three well represented languages where we took English is the hub and five minority languages in the sense of Wikipedia sizes.",
            "Those were Slovenian Piedmontese, a variety variety, Crayola, and Hindi.",
            "So this is a language in the Filippini and this is high tea.",
            "When?"
        ],
        [
            "So this slide here we have two things, so one is how the size of the alignment set.",
            "So for certain language pairs we have no aligned documents, so we have to use the whole set of links to get something.",
            "And this set shows this matrix shows pairwise average rank, which is normalized between one and minus one.",
            "So one is perfect ranking.",
            "Zero is random ranking and for example, we can observe that PN MONTESSA and Slovenian even though they have very small aligned smaller line set.",
            "They we can still get.",
            "A solid performance.",
            "And the more extreme cases, Hindi and Piedmontese, we have zero alignment and we can still find.",
            "This common semantic space that enable that has ranking quality of 0.87.",
            "So we also notice that some languages are very problematic, for example.",
            "This righty.",
            "But we explored this wide.",
            "Why it doesn't work well and we found out that the Wikipedia was very the quality of articles was typically low.",
            "I'm I mean by that is that there's a lot of stubs or articles that just have some geographic location and coordinates and not not much else so.",
            "So that's why they were.",
            "It turns out that's a lot less useful information for this language mappings."
        ],
        [
            "So to conclude, we address this problem of learning the independent document representations under these extreme conditions were very little or no alignment information is available, and we propose the method that can take into Advantage Hub language, which is well languages, and the method is scalable.",
            "We evaluated on Wikipedia sing results.",
            "But as I said, there's still a lot of work to be done.",
            "As we saw in the demonstration."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm beyond rupnick.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Andre Mouton.",
                    "label": 0
                },
                {
                    "sent": "Primo Scrabble we are working on.",
                    "label": 0
                },
                {
                    "sent": "Cross lingual document retrieval through hub languages.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll first tell you about the motivation.",
                    "label": 0
                },
                {
                    "sent": "Then show your short demo, then explain about the method and tell you about some experiments.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our motivation is we want to learn language independent document representation.",
                    "label": 1
                },
                {
                    "sent": "So this would enable us to compare the documents written in different languages which could then enable us to do cross lingual information retrieval, classification or clustering.",
                    "label": 1
                },
                {
                    "sent": "So are are to find this language independent representation.",
                    "label": 1
                },
                {
                    "sent": "We use an aligned comperable corpus, for example Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So the documents there are just comperable, not necessarily translations, and we looked into the problem of missing data.",
                    "label": 0
                },
                {
                    "sent": "For example, for some language pairs there may be very little or even no alignment information, so we don't have any.",
                    "label": 0
                },
                {
                    "sent": "Documents to train, but there is a solution if both of these documents are well aligned to one major language, which we call a hub language.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And this is the picture, so this is English, Slovenian and Hindi.",
                    "label": 0
                },
                {
                    "sent": "And there are very many Wikipedia articles that are that are present in both in Slovenian and Hindi and put Slovenia and Hindi.",
                    "label": 0
                },
                {
                    "sent": "Are both share a lot of articles with English.",
                    "label": 0
                },
                {
                    "sent": "So the idea is if we can so now the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to find sets of mappings P1P2 and P3 that can map the points into the this low dimensional common space so that ideally the points that are aligned so there are about the same subject should be mapped closed close to each other and then all the tasks are basically like monolingual tasks.",
                    "label": 0
                },
                {
                    "sent": "So I can show you.",
                    "label": 0
                },
                {
                    "sent": "Short example of what we implemented, so this is based on Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "Here we have two articles, one is English, one is Spanish and so we can, so they're on the same event, but they're not translations of each other, and the idea is that if we have this language independent representation, we can map these two articles into the same space and then compute similarity between them.",
                    "label": 0
                },
                {
                    "sent": "And we see that we had.",
                    "label": 0
                },
                {
                    "sent": "So this is cosine similarity in the.",
                    "label": 0
                },
                {
                    "sent": "Common semantic space.",
                    "label": 0
                },
                {
                    "sent": "Then our goal was to perform this, find this representation for documents for language pairs that have very little alignment information.",
                    "label": 0
                },
                {
                    "sent": "So this would be, let's say.",
                    "label": 0
                },
                {
                    "sent": "First, we can look at article about Abraham Lincoln.",
                    "label": 0
                },
                {
                    "sent": "In one is English.",
                    "label": 0
                },
                {
                    "sent": "One is Slovenian, so Slovenian Wikipedia is a lot smaller than English Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "But still there are like 50,000 aligned articles between them, and we can compute the similarity.",
                    "label": 0
                },
                {
                    "sent": "So the similarity, even though it's the the articles are compareable, so the similarities still pretty strong.",
                    "label": 0
                },
                {
                    "sent": "Now if we go to the next example, we have Wikipedia articles about Abraham Lincoln and one is in Slovenian and the other one is in Piedmontese A.",
                    "label": 0
                },
                {
                    "sent": "So this is a language in northern Italy that spoken by 2 million people and Slovenian Piedmontese have almost the alignment information is very scarce and if we compute the similarity between them.",
                    "label": 0
                },
                {
                    "sent": "We see that the number is very low, even though the articles are about the same subject.",
                    "label": 0
                },
                {
                    "sent": "But if we take a random Wikipedia article from Piedmontese here.",
                    "label": 0
                },
                {
                    "sent": "We get even lower similarity, so even though the similarity score was very low, there were still some some signal in that.",
                    "label": 0
                },
                {
                    "sent": "So different articles generally have lower similarities, but we have to take into account that so the data here is very noisy and almost orthogonal in the semantic space.",
                    "label": 0
                },
                {
                    "sent": "But still some some signal is retained.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now I can.",
                    "label": 0
                },
                {
                    "sent": "I don't want to.",
                    "label": 0
                },
                {
                    "sent": "Huh, so this was the the idea here.",
                    "label": 0
                },
                {
                    "sent": "So we saw that.",
                    "label": 0
                },
                {
                    "sent": "How this works in?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Practice so now I can tell you about the approach.",
                    "label": 0
                },
                {
                    "sent": "So we first we have M languages and one language.",
                    "label": 0
                },
                {
                    "sent": "The first language is the Hub language, which is well connected to the others.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we estimate the empirical cross covariance matrices and then stack them up and just use the covariance matrices related to the hub language.",
                    "label": 0
                },
                {
                    "sent": "Because the other matrices might be unreliable.",
                    "label": 0
                },
                {
                    "sent": "And perform a singular value decomposition of this.",
                    "label": 0
                },
                {
                    "sent": "And by this we get initial set of subspaces for each of the non hub languages and the hub language and we use this subspace is to reduce the dimensionality of our problems.",
                    "label": 0
                },
                {
                    "sent": "So instead of having hundreds of thousands of dimensions, we map it into our let's say 500 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So, so this first step is very similar to Latin semantic indexing.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the second step is once we are in this lower dimensional space, we can try to find the best the best directions that have maximal linear dependence between the so our languages and the hub language.",
                    "label": 0
                },
                {
                    "sent": "So this can be.",
                    "label": 0
                },
                {
                    "sent": "This can be opposed as an optimization problem, which can be solved as an eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "So here in two parts we use the fact that we discarded the other covariance matrices and used only the hub hub and hub related matrices.",
                    "label": 0
                },
                {
                    "sent": "So this this enabled us to, so it's served.",
                    "label": 0
                },
                {
                    "sent": "It has two purposes, so the other cross covariance matrices might be very unreliable.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is that we can get scaleability, scale up the algorithm by using this.",
                    "label": 0
                },
                {
                    "sent": "So this is very convenient in Wikipedia because English language is is has this hub language characteristics.",
                    "label": 0
                },
                {
                    "sent": "And the step one which I mentioned, the latent semantic indexing also regularize is this second step, so there's no need to for stronger regularization here because we are working in low dimensional spaces.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the end, these mappings that I mentioned are just products of the first step.",
                    "label": 0
                },
                {
                    "sent": "In the second step.",
                    "label": 0
                },
                {
                    "sent": "So this second step is like a refinement of the of the topics, and this can be seen as topic vectors.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Not.",
                    "label": 0
                },
                {
                    "sent": "X2 to X3 language pairs, so if their alignments between them we didn't use them because typically if the alignment exists that also the alignments to the English one existed.",
                    "label": 0
                },
                {
                    "sent": "So the documents were still used.",
                    "label": 0
                },
                {
                    "sent": "And this cases where alignment existed between them and no alignment with the English.",
                    "label": 0
                },
                {
                    "sent": "These are very rare, so we didn't throw away much information.",
                    "label": 0
                },
                {
                    "sent": "Done so.",
                    "label": 0
                },
                {
                    "sent": "Not help.",
                    "label": 0
                },
                {
                    "sent": "So it can also be extended, but so this was the main thing was that if you have just a few documents here, the covariance matrix is very unreliable because it's a high dimensional matrix with just a few observation or even a few 100.",
                    "label": 0
                },
                {
                    "sent": "It can more hurt the performance than it can help it.",
                    "label": 0
                },
                {
                    "sent": "So this was the in the approach.",
                    "label": 0
                },
                {
                    "sent": "We just use this.",
                    "label": 0
                },
                {
                    "sent": "This covariance queries is based on this information.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we evaluated it on the Wikipedia on 8 languages, three well represented languages where we took English is the hub and five minority languages in the sense of Wikipedia sizes.",
                    "label": 1
                },
                {
                    "sent": "Those were Slovenian Piedmontese, a variety variety, Crayola, and Hindi.",
                    "label": 0
                },
                {
                    "sent": "So this is a language in the Filippini and this is high tea.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this slide here we have two things, so one is how the size of the alignment set.",
                    "label": 1
                },
                {
                    "sent": "So for certain language pairs we have no aligned documents, so we have to use the whole set of links to get something.",
                    "label": 0
                },
                {
                    "sent": "And this set shows this matrix shows pairwise average rank, which is normalized between one and minus one.",
                    "label": 1
                },
                {
                    "sent": "So one is perfect ranking.",
                    "label": 0
                },
                {
                    "sent": "Zero is random ranking and for example, we can observe that PN MONTESSA and Slovenian even though they have very small aligned smaller line set.",
                    "label": 0
                },
                {
                    "sent": "They we can still get.",
                    "label": 1
                },
                {
                    "sent": "A solid performance.",
                    "label": 0
                },
                {
                    "sent": "And the more extreme cases, Hindi and Piedmontese, we have zero alignment and we can still find.",
                    "label": 0
                },
                {
                    "sent": "This common semantic space that enable that has ranking quality of 0.87.",
                    "label": 0
                },
                {
                    "sent": "So we also notice that some languages are very problematic, for example.",
                    "label": 0
                },
                {
                    "sent": "This righty.",
                    "label": 0
                },
                {
                    "sent": "But we explored this wide.",
                    "label": 0
                },
                {
                    "sent": "Why it doesn't work well and we found out that the Wikipedia was very the quality of articles was typically low.",
                    "label": 0
                },
                {
                    "sent": "I'm I mean by that is that there's a lot of stubs or articles that just have some geographic location and coordinates and not not much else so.",
                    "label": 0
                },
                {
                    "sent": "So that's why they were.",
                    "label": 0
                },
                {
                    "sent": "It turns out that's a lot less useful information for this language mappings.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, we address this problem of learning the independent document representations under these extreme conditions were very little or no alignment information is available, and we propose the method that can take into Advantage Hub language, which is well languages, and the method is scalable.",
                    "label": 1
                },
                {
                    "sent": "We evaluated on Wikipedia sing results.",
                    "label": 0
                },
                {
                    "sent": "But as I said, there's still a lot of work to be done.",
                    "label": 0
                },
                {
                    "sent": "As we saw in the demonstration.",
                    "label": 0
                }
            ]
        }
    }
}