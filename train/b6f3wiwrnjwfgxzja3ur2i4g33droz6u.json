{
    "id": "b6f3wiwrnjwfgxzja3ur2i4g33droz6u",
    "title": "Exact learning curves for Gaussian process regression on large random graphs",
    "info": {
        "author": [
            "Matthew Urry, Department of Mathematics, King's College London"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Regression",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/nips2010_urry_elc/",
    "segmentation": [
        [
            "OK, we're we're interested in learning.",
            "On discrete, but you learning using Gaussian process on discrete spaces quite a bit as well is known in continuous spaces and we want to apply at this discrete space.",
            "Gaussian process are a Bayesian learning method where we assume a a Gaussian distribution for the prior over functions with a with a known mean and covariance function.",
            "If we assume a gallon Gaussian noise as well, then observation observations become a lot for noise and and observations.",
            "Predictions have come a lot easier, they just become Gaussian distributions.",
            "We're interested in studying the learning curve than the average mean square error as a function of a number of examples.",
            "This is relatively well understood indiscrete spaces, and there's been quite a few approximations to this in continuous space has been quite a few approximations this, but we want to know how it how it performs indiscrete spaces we use.",
            "We use GPS to learn a function defined on the vertices of a graph is a good example on the left.",
            "Here you'll see squared exponential function and on the right we were considering functions from a random walk kernel, which is sort of equivalent to the square exponential on this graph case.",
            "Green Green is positive numbers and red is negative.",
            "In last year's NIPS, we submitted a paper on applying and approximations fans would reasonably accurate continuous spaces to our discrete case.",
            "We focus specifically on random regular graphs and use a random walk kernel.",
            "Watching function works OK."
        ],
        [
            "We found that it performed badly in the midsection learning curves and it's the noise as the noise decrease.",
            "This is shown on the plot and the right the solid lines are.",
            "The are the prediction to learning curve and the dot dotted lines of the numerics for learning curve.",
            "So one would like to see whether if we include graph structure we could get a more accurate prediction.",
            "So in our in the paper submitted this year, we've exploited graphs."
        ],
        [
            "Produce more accurate results.",
            "We derive approximations on the range of graphene samples that are restricted only by a arbitrary but fixed degree distribution.",
            "And these are these predictions we arrive at become exact in the large graph limit, we rewrite our learning curve in terms of the partition function zed which I've written on the on the thing at first term and this is our covariance term.",
            "Second term is our likelihood term, where aniza number of examples vertex I've seen and Sigma squared is the variance of the noise and final term is the generator.",
            "So if we differentiate it gives us back our learning curve.",
            "We want to apply.",
            "Team effort, but first we must get rewrite this so that we only have immediate neighbor interactions.",
            "To do this, we must first eliminate the inverse, which we do use in there for your transform and integrate 90 F and introduce extra variables to eliminate the random walk as the fervent an immediate neighbor.",
            "Random walk doing this gives us a complex valued Gaussian graphical model."
        ],
        [
            "As you can see here we are on these two plots.",
            "I've plotted the results after solving this complex Gaussian model.",
            "The Dash line here is the old prediction.",
            "The triangles is the new cavity protection.",
            "The dotted line is the numerics.",
            "As you can see, pretty much lies straight on top of the numerics and this is only for 500 nodes.",
            "It should become even more accurate as the number of vertices increase.",
            "If you'd like to know more about the work we've done over other types of graphs including price on graphs and comparisons to previous approximation.",
            "Please come back.",
            "Come to our poster at TI-84.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we're we're interested in learning.",
                    "label": 0
                },
                {
                    "sent": "On discrete, but you learning using Gaussian process on discrete spaces quite a bit as well is known in continuous spaces and we want to apply at this discrete space.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process are a Bayesian learning method where we assume a a Gaussian distribution for the prior over functions with a with a known mean and covariance function.",
                    "label": 1
                },
                {
                    "sent": "If we assume a gallon Gaussian noise as well, then observation observations become a lot for noise and and observations.",
                    "label": 0
                },
                {
                    "sent": "Predictions have come a lot easier, they just become Gaussian distributions.",
                    "label": 0
                },
                {
                    "sent": "We're interested in studying the learning curve than the average mean square error as a function of a number of examples.",
                    "label": 1
                },
                {
                    "sent": "This is relatively well understood indiscrete spaces, and there's been quite a few approximations to this in continuous space has been quite a few approximations this, but we want to know how it how it performs indiscrete spaces we use.",
                    "label": 0
                },
                {
                    "sent": "We use GPS to learn a function defined on the vertices of a graph is a good example on the left.",
                    "label": 1
                },
                {
                    "sent": "Here you'll see squared exponential function and on the right we were considering functions from a random walk kernel, which is sort of equivalent to the square exponential on this graph case.",
                    "label": 0
                },
                {
                    "sent": "Green Green is positive numbers and red is negative.",
                    "label": 0
                },
                {
                    "sent": "In last year's NIPS, we submitted a paper on applying and approximations fans would reasonably accurate continuous spaces to our discrete case.",
                    "label": 0
                },
                {
                    "sent": "We focus specifically on random regular graphs and use a random walk kernel.",
                    "label": 0
                },
                {
                    "sent": "Watching function works OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We found that it performed badly in the midsection learning curves and it's the noise as the noise decrease.",
                    "label": 1
                },
                {
                    "sent": "This is shown on the plot and the right the solid lines are.",
                    "label": 1
                },
                {
                    "sent": "The are the prediction to learning curve and the dot dotted lines of the numerics for learning curve.",
                    "label": 1
                },
                {
                    "sent": "So one would like to see whether if we include graph structure we could get a more accurate prediction.",
                    "label": 0
                },
                {
                    "sent": "So in our in the paper submitted this year, we've exploited graphs.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Produce more accurate results.",
                    "label": 0
                },
                {
                    "sent": "We derive approximations on the range of graphene samples that are restricted only by a arbitrary but fixed degree distribution.",
                    "label": 1
                },
                {
                    "sent": "And these are these predictions we arrive at become exact in the large graph limit, we rewrite our learning curve in terms of the partition function zed which I've written on the on the thing at first term and this is our covariance term.",
                    "label": 1
                },
                {
                    "sent": "Second term is our likelihood term, where aniza number of examples vertex I've seen and Sigma squared is the variance of the noise and final term is the generator.",
                    "label": 0
                },
                {
                    "sent": "So if we differentiate it gives us back our learning curve.",
                    "label": 1
                },
                {
                    "sent": "We want to apply.",
                    "label": 0
                },
                {
                    "sent": "Team effort, but first we must get rewrite this so that we only have immediate neighbor interactions.",
                    "label": 1
                },
                {
                    "sent": "To do this, we must first eliminate the inverse, which we do use in there for your transform and integrate 90 F and introduce extra variables to eliminate the random walk as the fervent an immediate neighbor.",
                    "label": 0
                },
                {
                    "sent": "Random walk doing this gives us a complex valued Gaussian graphical model.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As you can see here we are on these two plots.",
                    "label": 0
                },
                {
                    "sent": "I've plotted the results after solving this complex Gaussian model.",
                    "label": 0
                },
                {
                    "sent": "The Dash line here is the old prediction.",
                    "label": 0
                },
                {
                    "sent": "The triangles is the new cavity protection.",
                    "label": 0
                },
                {
                    "sent": "The dotted line is the numerics.",
                    "label": 0
                },
                {
                    "sent": "As you can see, pretty much lies straight on top of the numerics and this is only for 500 nodes.",
                    "label": 0
                },
                {
                    "sent": "It should become even more accurate as the number of vertices increase.",
                    "label": 0
                },
                {
                    "sent": "If you'd like to know more about the work we've done over other types of graphs including price on graphs and comparisons to previous approximation.",
                    "label": 1
                },
                {
                    "sent": "Please come back.",
                    "label": 0
                },
                {
                    "sent": "Come to our poster at TI-84.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}