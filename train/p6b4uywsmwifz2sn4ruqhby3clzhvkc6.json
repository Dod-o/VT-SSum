{
    "id": "p6b4uywsmwifz2sn4ruqhby3clzhvkc6",
    "title": "Incompatibilities(?) between PAC-Bayes and Exploration",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "April 14, 2010",
        "recorded": "March 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/pacbayesian_langford_ibpb/",
    "segmentation": [
        [
            "He's been working on learning in various exploration settings and when I heard about this workshop, I started thinking about.",
            "How are these two ideas related, so I wanted it seems like there's.",
            "There's some basic problems, so I guess I wanted to show you these and see what kind of thoughts you had."
        ],
        [
            "So first question is, what is a PAC Bayes bound right and?",
            "David, of course, what would you?",
            "There's a particular mathematical formula which we call the back face found, but maybe there's some insights in that formula which which are.",
            "Using a prior.",
            "Interesting.",
            "OK, so there's we also have a definition of what a PAC Bayes bound is.",
            "Yeah, quite agree with them.",
            "Anybody have a different definition?",
            "No, OK, so there's several things that I think so."
        ],
        [
            "Come up, one of them is it's tight bound.",
            "That's it, nothing.",
            "Well, that may not be the defining characteristic.",
            "It's certainly the reason why we're here.",
            "Um?",
            "Another one is this notion of having a prior."
        ],
        [
            "Right, so you want to have an what does that mean?",
            "We're having a prior means is you have different hypothesis, different different predictors and maybe you compete with this one a lot harder than you compete with that one because the prior on this one was much smaller in the prior on that one.",
            "So it's kind of."
        ],
        [
            "Way of specifying how much you want to compete with different things.",
            "And then there's a third element, which is indifference.",
            "So you don't pay for irrelevant decisions.",
            "So what I mean by this?",
            "So you can indeed apply a PAC Bayes bound on infinite BC dimension set up it sees an on many natural distributions.",
            "He will actually give you sensible results.",
            "Because you don't pay for irrelevant decisions.",
            "OK, so.",
            "So.",
            "Achieving two and three seem to be problematic in situations where there exploration.",
            "So what I mean by exploration?",
            "I mean in situations where the learning algorithm controls which information it gets.",
            "Right?",
            "So."
        ],
        [
            "So let me so when I'm first going to do is I'm going to define those two and three little bit better, and then I'll discuss two distinct notions of exploration.",
            "One of them is active learning.",
            "We're learning album choose which labels to get.",
            "Neither one is contextual.",
            "Bandits where we're an algorithm makes a decision about what to do and get feedback about just that one choice that it made."
        ],
        [
            "OK, so.",
            "I realized as I was speaking that there's something which I think of is.",
            "It is other people were speaking that this isn't going to think of is basic, but maybe it's not understood by everyone here, which is that.",
            "A pack base style bound can be proved in an online setting.",
            "And it actually can proved without assumptions that the the world is IID.",
            "So there's nobody here from the adversarial online learning community, but but you can indeed do something like that.",
            "And I think in fact they're doing it.",
            "Of it was motivated by the fact based bound to some extent.",
            "OK, so I'm going to talk about an online setting because because of the settings, some of the exploration settings are ones where online learning is sort of necessary to talk about because.",
            "No, I mean like what Manford Warmath works on.",
            "They call it the relative entropy method or something like that.",
            "I don't know, but.",
            "It's basically the same thing.",
            "So the reason why we talk about learning here is because in order to talk about exploration, you have to really be thinking about an online setting because the learning algorithms, information state changes overtime, right?",
            "So you need to be able to describe what it means to get more information.",
            "So in a supervised setting we could imagine.",
            "Something happening over and over where first we see features and then we choose a label and then we see the true label and then and then we just repeat over and over again and the exact choice.",
            "Of how we predict is not determined.",
            "We're going to try to find that learning algorithm whose goal is going to be to compete with some hypothesis class.",
            "So we have some set of predictors and we want to we want to do nearly as well as the best predictor in this set.",
            "OK, so."
        ],
        [
            "Let me tell you a typical.",
            "Algorithm so personal.",
            "Although it's the case that what I'm telling you.",
            "This is a very different algorithm that will actually work in every sales.",
            "I think I'm actually going to talk about my ID setting because it's simpler, so we're going to mention that there's some distribution over XY pairs, and then we have the error rate and we also have an empirical error rate over the examples that we've seen so far.",
            "OK, so."
        ],
        [
            "Um?",
            "If you're in an IID setting, it turns out that if you run.",
            "Empirical risk minimization, which is also called follow the leader.",
            "On each round you end up having making pretty good predictions.",
            "So you're just going to on each round, choose a path, this which has the smallest error rate, empirically rate.",
            "And now."
        ],
        [
            "No.",
            "The claim is that for all ID distributions D for all hypothesis H / T timesteps with probability with high probability.",
            "Now you have.",
            "Maybe let's just think about.",
            "Is actually two ways to interpret this.",
            "You can talk about the regret that's accumulated over the time steps.",
            "We can talk about the instantaneous regret at the end.",
            "Let's just think about this trip.",
            "Then for the moment.",
            "OK, so there's the error rate of the process we output at the end, and then there's the error rate of the best hypothesis.",
            "And the claim is that it's bounded by something like this.",
            "And I'll be using.",
            "I'll be using regret throughout, which is going to be some sort of difference in error rates.",
            "Queso.",
            "So far I haven't really talked about how this relates to PAC Bayes."
        ],
        [
            "But now we can do that.",
            "So.",
            "If you're interested in luckiness, you can modify this to work.",
            "According to Oakland Razor bound.",
            "So what happens then is you have log of introduce a prior of the set of policies.",
            "And now you regret bound is going to differ depending upon which hypothesis each star is.",
            "And so for hypothesis, which have a large prior, you're going to compete hard with them.",
            "And for once, we have a very small prior going to.",
            "You can allow a lot of slop.",
            "OK, so this.",
            "This is related to the basic observation, which is which is, which is really handy in supervised learning, which is that.",
            "If you have two paths spaces and you want to learn well Spec 2.",
            "The Union of the two.",
            "Then basically you can just learn well on one and the other and there's a little bit of stuff, so she did with this, but.",
            "But there's no, there's no real cost.",
            "Star is the.",
            "That's right.",
            "I'm surprised at the.",
            "Beyond.",
            "Hold on, yeah, you can.",
            "Actually this is a different way to parameterized things that you have P of the argument here.",
            "Yeah."
        ],
        [
            "OK, so as far as indifference goes.",
            "You can just go back to having a flat prior over the prophecies and now.",
            "Indifferent says that if you have a Q of H. Then you can subtract off the entropy of that Q.",
            "From your complexity term, right?",
            "And that means that if you have.",
            "Two assets in the second one is like all good parties.",
            "Then the regret of learning on the Union is less than or equal to the rigor of learning on just the first one.",
            "So it's it's it's very handy.",
            "OK, so we can think about luckiness.",
            "An indifference is sort of two different things, right?",
            "So one is kind of a variable competition and the other is we gotta subtract off the decisions that we don't need to make.",
            "We don't need to.",
            "This is more like the.",
            "This paper along time ago by Geoff Hinton talking about the bits back argument so that you don't care about decisions you don't need to make.",
            "So let's say there's a bunch of hypothesis, all of which have a good empirical error rate.",
            "Then if you take an expectation spectacu.",
            "Well, I didn't write out that what I mean here is the expectation is that you have a path this.",
            "You have also have a good empirical error rate.",
            "And the claim is that you also have a good true error rate.",
            "Where we're good, we're also good, is relaxed by by this quantity.",
            "Within different, we don't.",
            "You don't have to decide in one month.",
            "Yeah, so you're indifferent between two different paths is right.",
            "Do you have two good hypotheses you don't really care which one?",
            "Just randomize between the two an you don't pay for.",
            "You don't pay the complexity of choosing one of them.",
            "Right?",
            "So we just chose one of them.",
            "Then it would be like this, but.",
            "We subtract off a bit if there's.",
            "There's two.",
            "This is the entropy of Q.",
            "To never teach, you have two age.",
            "Oh yeah, you're right.",
            "Alright, alright sorry bout that.",
            "OK, so this is what I mean by luckiness an indifference."
        ],
        [
            "OK, so now let's think about what happens in active learning."
        ],
        [
            "So first of all, let me tell you the setting in active learning.",
            "So setting active learning is just like supervised learning, except the world only tells you the label if you requested.",
            "So by default, Step 3 doesn't happen, and learning out of him has to say, please tell me the label.",
            "So now the goal is going to be the same, except you have this kind of added goal of minimizing your label complexity.",
            "So you want to want to pay as few people as possible to produce labels for learning algorithm will get into a good solution."
        ],
        [
            "OK, so.",
            "Here's a typical algorithm.",
            "This is one that.",
            "Century desk to worked on.",
            "It's kind of a sketch of it, so you have some version space which is going to be like a stateful variable.",
            "So initially it's going to be equal to the set of all hypothesis and then based upon the observations that we get, we're going to be restricting the set of policies that we care about.",
            "OK, so first we're going to we're in this kind of iterative loop, so we're going to observe some features.",
            "And then we're going to predict according to the argument on our set on a restricted set of boxes.",
            "And now we're going to check and see if there are two hypotheses in our restriction set which disagree.",
            "And if there are two, they're going to ask the label.",
            "And then we're going to get some information, so we're going to.",
            "So we're going to use a sample complexity bound to rule out.",
            "Hypotheses.",
            "Which are kind of probably not optimal, so we have a set of prophecies.",
            "One of them is.",
            "The set is going to be optimal.",
            "We're going to maintain that as an invariant, and then we're going to.",
            "We're going to be getting samples which tell us information about whether H is better.",
            "H prime is better.",
            "So suppose that H is better.",
            "At some point we'll have enough samples that we can rule out H prime and then H prime will be removed from the set of remaining hypothesis.",
            "So this is this is a typical style of algorithm here.",
            "How?"
        ],
        [
            "OK, so.",
            "The claim is.",
            "For all ID distributions, default path sets H or T timesteps.",
            "With high probability, the regret of the active learning algorithm is going to be similar to the record of the supervised learning algorithm.",
            "It's actually about a factor of two worse.",
            "Um?",
            "And OK, so the number of labels.",
            "Is going to be what is this?",
            "So we have the log of the nerve properties.",
            "We have the log of T. We have another log of T here.",
            "Then we have a log of one over Delta, so this is is like log T squared.",
            "Let's ignore this term.",
            "And then.",
            "And I guess the thing which is interesting is that this can be much less than T. Potentially right?",
            "In supervised learning this, this is the number of labels is T. But this could be maybe much smaller than T. But this is this funny thing here this data.",
            "Which is this disagreement coefficient?",
            "No."
        ],
        [
            "We only get a label when we requested.",
            "Right do it."
        ],
        [
            "I agree with each other in a restricted set, so we start.",
            "Initially it's we ask for label every time there's a disagreement.",
            "Which is essentially every time.",
            "But as we are learning progress is it will discover that many prophecies are not going to be optimal.",
            "And that means that all the remaining set of paths may agree on the label at some point.",
            "And so you cease to ask for the label.",
            "No, because they just just saying if they all agree on this One X doesn't say they all agree on all axes.",
            "What do you mean by probably not optimal?",
            "So I mean, you can apply sample complexity bounds, which gives you gives you a large DB.",
            "This is with probability point.",
            "So point really, really small.",
            "This process is not going to be the optimal one.",
            "Use this bound each time to make this right.",
            "Exponentially great listen exponentially degrade because you can in the worst case you could think about doing a union bound over T time steps.",
            "And you know that T is a small number compared to the number of prophecies, so even multiplying math space by a factor of T is or even doesn't matter that much.",
            "Right, yeah?",
            "Then you will not end up with your unique I put asunder.",
            "So we're not trying to get a unique hypothesis, we're just trying to get a process which will be good after tee time steps, right?",
            "And there is some tradeoff.",
            "So so when I when I said the theorem there is like a factor of two or so between the supervised regrets and the active learning regret.",
            "And if you think about it, that's intuitive because.",
            "What we're doing here is we're looking at pairs of prophecies.",
            "So if you think about."
        ],
        [
            "Log of the size of the past, the CET squared.",
            "This can be 2 log of size of offset, so you get a factor of 2 ish.",
            "And this is.",
            "Yeah, so the question is, what is the disagreement coefficient so the discriminant coefficient is some measure of the compatibility between.",
            "The distribution D and they both said age, so this is really a status of DH.",
            "Her yeah.",
            "Kisses so let me try to give you a better idea so.",
            "Yes, this comes from D. Yeah.",
            "So this understanding of active learning didn't exist a few years ago.",
            "Previous to a few years ago.",
            "Since we all actually learning algorithms were either beige and learning algorithms or or, they only worked in the realizable case.",
            "So this.",
            "This works when.",
            "This algorithm that I'm telling you here always works regardless of how much noise there is and you get this small label complexity when the area happens to be small.",
            "Area, to the best of both hips to be small.",
            "But unlike many active learning algorithms, you're not controlling your not sample in X near a problematic point, decision about which algorithms.",
            "Yeah, so how does this work?",
            "So what does it mean to have two policies which disagree?",
            "Maybe that means you have a version space over linear separators.",
            "And you know, there's some sort of margin where you're sure that everything outside of this.",
            "All linear separators, corresponding decisions outside of this margin are decided.",
            "Yeah, but you're not biasing the choice of XI mean.",
            "It might be expensive to get an X.",
            "In that case you wouldn't want to buy us the choice of ecstasy in the near the decision boundary of what you've learned so far.",
            "But this is basing the X, yeah?",
            "Image from from whatever your underlying distribution is from D, but your rejection sampling on the X and introduces a bias.",
            "Yes, but it was expensive to get X.",
            "Might want to.",
            "Not sample from the sample from some other distribution.",
            "I could be wrong standard, I did nothing.",
            "Yeah, so so most active learning is motivated by X is cheap in the labels that are expensive.",
            "In which is?",
            "This is fine.",
            "But also the idea being passive.",
            "D. Showing the axis, I think it's something that you really get you closer to ideal situation and then you can manage to advance.",
            "And if you have an aggressive way of asking for extra time.",
            "I tend to be given by by nature.",
            "Then you make too many bias and then you find will.",
            "I see, yes, that's exactly right, so there's a lot of these algorithms which try to choose like the single X, which is close to the decision boundary.",
            "And the problem with those algorithms is that they can end up focusing on the noise in the learning in the data and then producing decision surface which is just nowhere even near these algorithms are asymptotically inconsistent, they will never converge to the right solution in your set of policies.",
            "Wondering about this so if you wouldn't say rather I give you a big mushroom exit, you can have every time you can choose from that bag.",
            "Not not really from the back so I give you a part of.",
            "Exit.",
            "Every time you can go over all these guys and there wouldn't be a big tiempo right?",
            "Because here it seems like you have to wait until something comes.",
            "Block ratio between many you like and how many?",
            "Yes.",
            "I need exponentially many more samples than I would really want to, so you think about sort of a setting where you see all of your unlabeled data up front and then you wanted to choose.",
            "So there is a statement that can be made here.",
            "So, So what?",
            "T you should think of T is not the number of unlabeled data points, but rather is the number of labeled data points given to achieve the supervised learning algorithm.",
            "To achieve some error rate.",
            "Next to each other you see, so we have a supervised learning algorithm which given T examples has some error rate.",
            "We have an active learning algorithm which given these this number of examples achieves essentially the same error rate.",
            "Her same regret, right?",
            "Yeah, so so that's maybe that's a even when you're in this sort of batch setting.",
            "That's a way to define T which which is consistent with these.",
            "Could do better.",
            "I yes yes.",
            "Yeah, so that actually is related to.",
            "Wanna conclusions OK so?"
        ],
        [
            "Let me tell you what the disagreement coefficient is.",
            "OK, so.",
            "OK, so there's some awful hypothesis.",
            "And there's all the properties which are within epsilon of it, narrate, right?",
            "So let's define a restriction of the set of boxes called H Step Salon.",
            "OK, so now is going to be in restriction of this set of.",
            "Unlabeled examples, which corresponds to the right which corresponds to.",
            "The set though, so we're going to look at all the hypoth season dates of epsilon.",
            "We're going to ask does it was disagrees with the best.",
            "Kiss wait wait what?",
            "It's it's the region which of the feature space you're going to care about.",
            "Once you've ruled out all the dumb hypotheses.",
            "Right, and then we're going to have this agreement coefficient, which is going to be the maximum ratio of these.",
            "So this fraction of feature space.",
            "Oh, we have a workshop for awhile.",
            "Freedom toys.",
            "Thank you, go to 403.",
            "We go to 403 where we have our class every year.",
            "So it says.",
            "OK, so you have the measure of the features near 2 that you care about when you're near the optimum, and then you have.",
            "You know the distance.",
            "So this ratio is the disagreement coefficient, yeah?",
            "That's right, I forgot to write in.",
            "There exists HNH epsilon here.",
            "But this is not something that you can define.",
            "It can calculate definitely, not so.",
            "You cannot calculate this, but the algorithm doesn't actually need to know Theta.",
            "The algorithm is oblivious that room just runs and then something is true about its output.",
            "OK, so."
        ],
        [
            "So what is the description when you have two parties?",
            "What is the disagreement coefficient when you have two hypothesis, when H sub epsilon consists of H1 and H2?",
            "What's that?",
            "Really, let's say that they disagree at least somewhere.",
            "Everybody."
        ],
        [
            "So is the answer to yes.",
            "Yeah, so it's one because you know there's some fraction of space to disagree on, and that epsilon in the worst case could be the size equal to the fraction and so.",
            "Yeah this one.",
            "OK so.",
            "Another simple cases when you have thresholds in on the real line, right so or just think about you have thresholds on the line.",
            "So just think about step functions.",
            "And we have any distribution over for the unlabeled data.",
            "Any continuous distribution.",
            "So what's the disagreement coefficient?",
            "What's over one?",
            "But it's not actually one.",
            "Turned out to be too."
        ],
        [
            "Do you have the office hypothesis and then you have an epsilon measure over this direction that you could disagree with in epsilon measure this direction that you disagree with so it's 2.",
            "Right?",
            "And there's linear separators through the original in Rd with the uniform data distribution.",
            "So we have the unlabeled data is uniform over the sphere.",
            "So what is?",
            "But this one is not gonna be able to get it if you don't know it, but it turns out that it's Rudy.",
            "Yeah.",
            "Right, so so I guess the fact that is less than D means that active learning can give you some speedups there over over just supervised learning.",
            "Yeah, on the sphere.",
            "If it's uniform in the sphere and projected to be uniform on the sphere in the same analysis applies.",
            "Number of.",
            "So.",
            "There should be a right?",
            "So what I'm not telling you I guess, is that."
        ],
        [
            "Um?",
            "This can be replaced with BC dimension.",
            "OK, so this is the discriminant."
        ],
        [
            "Efficient and.",
            "Give you some notion of.",
            "Of what's compatible and what's not, and."
        ],
        [
            "OK, so now what happens with luckiness?",
            "To luckiness is variable competition depending upon the prior right.",
            "Can we achieve that in this kind of setting?",
            "So I think the answer."
        ],
        [
            "Actually, no, you can't very easily.",
            "So you have a disagreement coefficient.",
            "For the union of two paths sets.",
            "An only bound that I can come up with.",
            "Is like the sum plus maybe a constant?",
            "So it says that the label complexity of learning on two paths sets.",
            "It could be the sum of the label complexity of learning this plus little complexity learning on this.",
            "And it's very bad if you're interested in.",
            "In a prior, because you could, you could imagine having a prior which is.",
            "I mean, maybe maybe I bought this at one.",
            "All the parties over small, prior and here they all have large prior.",
            "Um?",
            "It seems that we have to have to pay to explore this hypothesis set.",
            "Even though we made up wanting to just go with that bus this year.",
            "News.",
            "Artifact of your structure.",
            "Yeah, I'll get to that.",
            "Ha."
        ],
        [
            "OK, So what about indifference?",
            "OK.",
            "So in differences, if there's irrelevant choices, so we have two paths which are both optimal.",
            "We don't pay for the fact that we have two, we.",
            "Well, I guess in the original PAC Bayes bound is tracked off a bit from your complexity, which is great.",
            "Um?",
            "It it works.",
            "The algorithm is looking at one hypothesis at a time, so it by definition."
        ],
        [
            "Yeah, so we could imagine I'm willing.",
            "I'm happy enough to.",
            "Go back into the algorithm and and let you predict according to a distribution over past.",
            "Easier if you want.",
            "I would think it's a relatively minor modification of this style of algorithm, right?",
            "Ha.",
            "Crime in HS?",
            "What do you like that?",
            "That's basically that's the decision whether you ask for later, that's right.",
            "And also removing things.",
            "Distribution.",
            "What's that?",
            "Condition on the fact it's not over there in the distribution shrinks down to something more concentrated.",
            "Space space.",
            "Maybe I don't know how to do that.",
            "In situation where you are small.",
            "And.",
            "It will not be likely that too good.",
            "I will defend.",
            "So."
        ],
        [
            "I."
        ],
        [
            "Think it's you actually cannot get indifference, and here's the logic so.",
            "Let's say we have H2 is a bunch of hypothesis which all achieved the minimum rate on each one.",
            "Then how can we bound the disagreement coefficient right?",
            "So we OK, we have the original, but then we have something which is proportional to the size of the H2.",
            "Because each of these different issues could be sucking a different portion of the ex space into consideration.",
            "And that's that's really bad.",
            "I mean linear in the size of office spaces.",
            "This is pretty catastrophic.",
            "Yeah he.",
            "Sorry it's very small, say 0.",
            "Danny.",
            "Retrieve your.",
            "That may be true.",
            "755 yeah, so we don't know that they're all the same.",
            "We don't know if they're all good, but we don't know that all same there right?",
            "There?",
            "Obviously not all the same, and in what they predict, right?",
            "And so.",
            "Would just be wasted if it would be terribly wasted.",
            "To indifferent seems to not apply in a pretty strong sense here."
        ],
        [
            "OK, so so objections that you."
        ],
        [
            "I have so the first was with David said.",
            "We're comparing upper bounds upper bounds and that sets.",
            "Pretty problematic thing to do, but actually there's there's a lower bound involving a disagreement coefficient in the VC dimension.",
            "So if you replace the log of size of office with the VC dimension, there's a similar structured lower bound.",
            "So the disagreement condition is not like a random definition, that's an artifact of the proof.",
            "It claims that this is really getting at something.",
            "In the structure of the problem itself.",
            "Number."
        ],
        [
            "Yeah, yeah.",
            "OK, so."
        ],
        [
            "OK, so should we care about active learning?",
            "Maybe maybe the solution to this is OK, who you know?",
            "So you have some setting, but I don't care about it because it doesn't look nice.",
            "And I'm actually someone synthetic to this.",
            "It does kind of look kind of loose and unclean and what not right?",
            "But but."
        ],
        [
            "The simple answer to this so empirically actually learning is used all the time.",
            "People really care about it, so this is the best analysis that we have for pregnancy, active learning and so.",
            "I mean, we're just trying to to understand what people are doing and help guide learning algorithms and that's basically what we should be trying to do.",
            "Another stronger claim is that.",
            "Well, actually from this line of work we're starting to get useful algorithms.",
            "Things actually reduced the label complexity required in order to achieve some particular error rate.",
            "And then.",
            "And then OK, so maybe you don't find these convincing, but but I actually came with a different exploration setting as well.",
            "The claim is true."
        ],
        [
            "To see some similar behavior there.",
            "OK, so I'm going to switch to a new setting.",
            "It's also an exploration."
        ],
        [
            "It is not the active learning exploration setting.",
            "OK, so in this new setting what happens is things change here.",
            "So previously.",
            "In supervised learning, the world just tells you what the true label is.",
            "Here we're going to choose some particular.",
            "So.",
            "This is the problem with jet lag.",
            "What had is a, so we're going to choose some action, which I could also call label.",
            "Just a question of definitions.",
            "And we're only only going to see a reward for that particular chosen.",
            "Action or label?",
            "OK, so we're not going to learn this was the right label, or is it going to learn?",
            "Oh yeah, this was a good idea.",
            "Or oh this was a bad idea.",
            "So and then the goal is going to be the same as in supervised learning.",
            "We're going to.",
            "We're going to want to compete with a set of bases.",
            "Yes, so we start wanting to look at the cumulative regret pretty strongly in this setting rather than the instantaneous final."
        ],
        [
            "At.",
            "OK, so.",
            "This turns out to be very easy and motivate at Yahoo because for example you could want to choose news stories that interest people visiting Yahoo, right?",
            "Maybe you have features of the sort only looked at this kind of story in the past and then then you have some set of new stories and you wanted to choose them right?",
            "So you can phrase this as a contextual bandit.",
            "Setting.",
            "Problem is that this is this is 1 application claiming that there are dozens or hundreds of applications.",
            "Anything involving getting information from a user.",
            "He's going to have this kind of structure because you can never reset a user and say OK go back in time and tell me how you would have liked it if I had done this instead.",
            "OK, so let's talk about a simple hour."
        ],
        [
            "For this, so we're going to keep track of instantaneous regret are so instantaneous regret.",
            "I mean, I'm going to assume there's some distribution.",
            "It's going to be over.",
            "Features and the reward for each of the different actions.",
            "And now we're going to have a data set which is going to consist of.",
            "Features the choice that was taken and the reward for that choice.",
            "And then we're going to have a.",
            "We have a measure of how.",
            "I shouldn't really use E. We have a measure of how good, and I thought this is, which is which.",
            "Is this?",
            "This is like the analog of the empirical error rate, it's.",
            "For the rounds when they pass, this agrees with the action that was taken.",
            "We get a credit equal to the reward.",
            "And we're going to actually want to maximize this.",
            "In the typo, let's ignore PYI set PY to be equal to .5."
        ],
        [
            "OK, so now.",
            "I see so this is going to be an argmax now.",
            "So here's the algorithm.",
            "The algorithm is with probability 1 -- R. We're going to choose the hypothesis which has.",
            "The largest.",
            "Iprime of HS so was probably 1 -- R for our instantaneous regret, which is going to be given to us by sample complexity bound.",
            "I'm going to just go with the Arg.",
            "Max should be a Max and otherwise we're going to choose an action at random.",
            "Never going to observe the reward, and if it was a randomly action is a randomly taken action.",
            "We're going to update our set of examples, but only if it was randomly chosen.",
            "And then we can't just throw in the examples from the rounds that what we exploited.",
            "This is like an explore versus exploit decision here, right so?",
            "Probably once or we're going to exploit.",
            "We're going to choose the best action that we know, and with probability R, we're going to explore, we're going to choose an action at random.",
            "Are is a number between zero and one.",
            "It does change so.",
            "The idea is that ours are instantaneous regret so.",
            "Let's go back to."
        ],
        [
            "Here.",
            "This is instantaneous regret.",
            "This is 1 example of instantaneous regret.",
            "And so we have.",
            "We don't know that, but we can perhaps bound that.",
            "We have very simple complexity bounds.",
            "Yeah, something like that."
        ],
        [
            "Which is it?"
        ],
        [
            "This.",
            "Here yeah.",
            "OK, so this should be it really be a Max.",
            "But we're going to says we're going to choose the hypothesis.",
            "I see.",
            "We're finding a good number of errors.",
            "OK, so there is no S there.",
            "How RBA?",
            "He will not tend to increase the zero.",
            "For example, that's alright.",
            "Can there's no hypothesis what?",
            "That is changing.",
            "During the process.",
            "So what we can do is we can plug in that supervised learning bound.",
            "And then.",
            "And then we can see what will happen in high probability.",
            "It will turn out to be.",
            "So previously we had a square root and now we get a 1/3.",
            "Um?",
            "Ark.",
            "Yes, oh, it could be.",
            "Could be a mess.",
            "I mean, it could be you could plug in some broader complexity, whatever that would be.",
            "It would be hard to evaluate.",
            "More and more yeah.",
            "Yeah, so actually you can plug in.",
            "OK, so if I made the rewards be either zero or one.",
            "This factor of 2 doesn't matter because it's the same everywhere.",
            "The factor of two just becomes not too, and when you don't choose uniform randomly.",
            "So if you just if you make this 01 then you can.",
            "This is also 01.",
            "And so we're just going to have another 01 random variable.",
            "We're taking the expectation of.",
            "And so all the bounds you're used to from supervised learning will allow you to get an R. Yes.",
            "Yes, so there's always one caveat, which is that you have to do with the martingale aspect of things.",
            "But typically there's a way to do that.",
            "OK, so this is this is that maybe the simplest algorithm for me to tell you?",
            "I didn't want to get more complex, but let me just tell you that there is another algorithm called Exp 4P which allows you to replace three with two.",
            "Which is which is interesting, but nevertheless it is kind of immaterial to this discussion, because.",
            "Um?",
            "'cause the same issues will occur with the four P's with epic greedy."
        ],
        [
            "OK.",
            "So this is just saying that R can be any sample complexity pound.",
            "You're all quite useful next bound to think."
        ],
        [
            "Um?",
            "OK, So what happens luckiness?",
            "Cortana.",
            "It all works.",
            "Does it work?",
            "So we want to compete.",
            "We have a set of policies we want to compete with, some subset harder than we compete with a different subset.",
            "What happens to?",
            "To our regrets.",
            "Using that bound bound with the.",
            "Agents yeah.",
            "So what would our be?",
            "Would you would you don't know advance, and that's that's the root of the problem."
        ],
        [
            "So that's the problem.",
            "You don't if you have a non uniform prior.",
            "Then R. In I don't know what you are you would choose, but the thing which is safe is to choose the Max of this which is which is not going to go anywhere useful, right?",
            "If you have a candle with infinite set of hypotheses with you know decreasing prior as we go out.",
            "This is going to always be one and.",
            "1st.",
            "To decide whether to actually use that agent.",
            "Part of the observation, create your prior and then enough.",
            "So I say it again.",
            "Activation for designing your prior basically and the other half.",
            "Play your game.",
            "Like fire.",
            "Yeah, learning the prior is a pretty delicate process here because.",
            "Because we need to make sure we explore the set of actions sufficiently well.",
            "So this problem is shared with 64P.",
            "There is some minimal amount of exploration XP for P, and if you if you want to a variable competition type thing then you end up wanting to drive exploration very small in one case and large in the other case, and you don't know which to choose.",
            "OK, you can also see that there's a problem.",
            "You can have two hypothesis spaces.",
            "Which choose, let's say we had more than two arms available to actions available, right?",
            "Maybe we have we have 4, right?",
            "We have the space which just choose amongst these two actions we have another space to choose amongst these two actions.",
            "And there's no sense in which exploring well for this to both space is going to help you choose amongst these apostasies.",
            "So your your complexity of exploration.",
            "Your regret is going to end up adding again.",
            "Which is which is not great.",
            "And what if your prior is distribution dependent?",
            "Meaning you will not notice prior.",
            "Say that I will put more weight on good classifier.",
            "Possible to estimate?",
            "Because then you will not have this problem.",
            "Darkness will be fulfilled because.",
            "Your prior notice distribution.",
            "Yeah, I don't know how to do this.",
            "Jungle talk"
        ],
        [
            "OK. OK, So what about indifference?",
            "So there's two hypotheses that are both equally good.",
            "Then you shouldn't pay for choosing amongst those two.",
            "So it ends up working."
        ],
        [
            "This is the one positive answer.",
            "No.",
            "So you can replace the argument.",
            "With an expectation over some Q.",
            "You can put in a PAC Bayes bound member.",
            "You can choose any bound for.",
            "To compute R. And will end up benefiting from the back basement.",
            "Right, so that's great, and in some sense Exp.",
            "Four P is formalizing the fact that Indifferences is what you want because it uses indifference in a strong sense.",
            "Did that in order to get the 1/2 power on the regret."
        ],
        [
            "OK, so.",
            "There's something, so I'm pretty convinced this is something not working.",
            "PAC Bayes, like in active learning setting, it seems like these the luckiness and indifference are pretty problem problematic there.",
            "Um?",
            "One thing which is striking about the active learning algorithms is that they're not using unsupervised data as a resource.",
            "They're saying that on the same set of data where supervised learning algorithm asks for every label.",
            "We compete with it and we ask for much fewer labels.",
            "But it seems like there's some potential to have a result which says that.",
            "If you give my active learning algorithm, hope access to a whole bunch more unlabeled data that can compete with with non super with the supervised learning algorithm which just has fewer unlabeled data points that sees.",
            "But it's tricky to do."
        ],
        [
            "Um?",
            "So for additional bands, simply indifferent works out OK.",
            "But there's substantial problems with luckiness.",
            "And that's interesting because.",
            "Can I get the root of this question, which is sort of going around which is?",
            "In order to really solve these problems well, do you need to be basean right?",
            "Because there are some people who use Bayesian approaches for these.",
            "And because they believe in their prior, they believe that the problem is actually drawn from the prior.",
            "They end up not having.",
            "Not having to worry about these things so much, right?"
        ],
        [
            "OK, so there's a little bit more discussion about brace expiration topics at lunch.net.",
            "This is a blog that."
        ],
        [
            "And then there's a bibliography if you're interested.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He's been working on learning in various exploration settings and when I heard about this workshop, I started thinking about.",
                    "label": 0
                },
                {
                    "sent": "How are these two ideas related, so I wanted it seems like there's.",
                    "label": 0
                },
                {
                    "sent": "There's some basic problems, so I guess I wanted to show you these and see what kind of thoughts you had.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first question is, what is a PAC Bayes bound right and?",
                    "label": 1
                },
                {
                    "sent": "David, of course, what would you?",
                    "label": 0
                },
                {
                    "sent": "There's a particular mathematical formula which we call the back face found, but maybe there's some insights in that formula which which are.",
                    "label": 0
                },
                {
                    "sent": "Using a prior.",
                    "label": 0
                },
                {
                    "sent": "Interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's we also have a definition of what a PAC Bayes bound is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, quite agree with them.",
                    "label": 0
                },
                {
                    "sent": "Anybody have a different definition?",
                    "label": 0
                },
                {
                    "sent": "No, OK, so there's several things that I think so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come up, one of them is it's tight bound.",
                    "label": 0
                },
                {
                    "sent": "That's it, nothing.",
                    "label": 0
                },
                {
                    "sent": "Well, that may not be the defining characteristic.",
                    "label": 0
                },
                {
                    "sent": "It's certainly the reason why we're here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Another one is this notion of having a prior.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so you want to have an what does that mean?",
                    "label": 0
                },
                {
                    "sent": "We're having a prior means is you have different hypothesis, different different predictors and maybe you compete with this one a lot harder than you compete with that one because the prior on this one was much smaller in the prior on that one.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Way of specifying how much you want to compete with different things.",
                    "label": 0
                },
                {
                    "sent": "And then there's a third element, which is indifference.",
                    "label": 0
                },
                {
                    "sent": "So you don't pay for irrelevant decisions.",
                    "label": 1
                },
                {
                    "sent": "So what I mean by this?",
                    "label": 0
                },
                {
                    "sent": "So you can indeed apply a PAC Bayes bound on infinite BC dimension set up it sees an on many natural distributions.",
                    "label": 0
                },
                {
                    "sent": "He will actually give you sensible results.",
                    "label": 0
                },
                {
                    "sent": "Because you don't pay for irrelevant decisions.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Achieving two and three seem to be problematic in situations where there exploration.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by exploration?",
                    "label": 0
                },
                {
                    "sent": "I mean in situations where the learning algorithm controls which information it gets.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me so when I'm first going to do is I'm going to define those two and three little bit better, and then I'll discuss two distinct notions of exploration.",
                    "label": 0
                },
                {
                    "sent": "One of them is active learning.",
                    "label": 1
                },
                {
                    "sent": "We're learning album choose which labels to get.",
                    "label": 0
                },
                {
                    "sent": "Neither one is contextual.",
                    "label": 0
                },
                {
                    "sent": "Bandits where we're an algorithm makes a decision about what to do and get feedback about just that one choice that it made.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I realized as I was speaking that there's something which I think of is.",
                    "label": 0
                },
                {
                    "sent": "It is other people were speaking that this isn't going to think of is basic, but maybe it's not understood by everyone here, which is that.",
                    "label": 0
                },
                {
                    "sent": "A pack base style bound can be proved in an online setting.",
                    "label": 0
                },
                {
                    "sent": "And it actually can proved without assumptions that the the world is IID.",
                    "label": 1
                },
                {
                    "sent": "So there's nobody here from the adversarial online learning community, but but you can indeed do something like that.",
                    "label": 0
                },
                {
                    "sent": "And I think in fact they're doing it.",
                    "label": 0
                },
                {
                    "sent": "Of it was motivated by the fact based bound to some extent.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to talk about an online setting because because of the settings, some of the exploration settings are ones where online learning is sort of necessary to talk about because.",
                    "label": 0
                },
                {
                    "sent": "No, I mean like what Manford Warmath works on.",
                    "label": 0
                },
                {
                    "sent": "They call it the relative entropy method or something like that.",
                    "label": 0
                },
                {
                    "sent": "I don't know, but.",
                    "label": 0
                },
                {
                    "sent": "It's basically the same thing.",
                    "label": 0
                },
                {
                    "sent": "So the reason why we talk about learning here is because in order to talk about exploration, you have to really be thinking about an online setting because the learning algorithms, information state changes overtime, right?",
                    "label": 0
                },
                {
                    "sent": "So you need to be able to describe what it means to get more information.",
                    "label": 0
                },
                {
                    "sent": "So in a supervised setting we could imagine.",
                    "label": 0
                },
                {
                    "sent": "Something happening over and over where first we see features and then we choose a label and then we see the true label and then and then we just repeat over and over again and the exact choice.",
                    "label": 0
                },
                {
                    "sent": "Of how we predict is not determined.",
                    "label": 0
                },
                {
                    "sent": "We're going to try to find that learning algorithm whose goal is going to be to compete with some hypothesis class.",
                    "label": 1
                },
                {
                    "sent": "So we have some set of predictors and we want to we want to do nearly as well as the best predictor in this set.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me tell you a typical.",
                    "label": 0
                },
                {
                    "sent": "Algorithm so personal.",
                    "label": 0
                },
                {
                    "sent": "Although it's the case that what I'm telling you.",
                    "label": 0
                },
                {
                    "sent": "This is a very different algorithm that will actually work in every sales.",
                    "label": 0
                },
                {
                    "sent": "I think I'm actually going to talk about my ID setting because it's simpler, so we're going to mention that there's some distribution over XY pairs, and then we have the error rate and we also have an empirical error rate over the examples that we've seen so far.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If you're in an IID setting, it turns out that if you run.",
                    "label": 0
                },
                {
                    "sent": "Empirical risk minimization, which is also called follow the leader.",
                    "label": 0
                },
                {
                    "sent": "On each round you end up having making pretty good predictions.",
                    "label": 0
                },
                {
                    "sent": "So you're just going to on each round, choose a path, this which has the smallest error rate, empirically rate.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "The claim is that for all ID distributions D for all hypothesis H / T timesteps with probability with high probability.",
                    "label": 1
                },
                {
                    "sent": "Now you have.",
                    "label": 0
                },
                {
                    "sent": "Maybe let's just think about.",
                    "label": 0
                },
                {
                    "sent": "Is actually two ways to interpret this.",
                    "label": 0
                },
                {
                    "sent": "You can talk about the regret that's accumulated over the time steps.",
                    "label": 0
                },
                {
                    "sent": "We can talk about the instantaneous regret at the end.",
                    "label": 0
                },
                {
                    "sent": "Let's just think about this trip.",
                    "label": 0
                },
                {
                    "sent": "Then for the moment.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's the error rate of the process we output at the end, and then there's the error rate of the best hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And the claim is that it's bounded by something like this.",
                    "label": 0
                },
                {
                    "sent": "And I'll be using.",
                    "label": 0
                },
                {
                    "sent": "I'll be using regret throughout, which is going to be some sort of difference in error rates.",
                    "label": 0
                },
                {
                    "sent": "Queso.",
                    "label": 0
                },
                {
                    "sent": "So far I haven't really talked about how this relates to PAC Bayes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now we can do that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in luckiness, you can modify this to work.",
                    "label": 0
                },
                {
                    "sent": "According to Oakland Razor bound.",
                    "label": 0
                },
                {
                    "sent": "So what happens then is you have log of introduce a prior of the set of policies.",
                    "label": 0
                },
                {
                    "sent": "And now you regret bound is going to differ depending upon which hypothesis each star is.",
                    "label": 0
                },
                {
                    "sent": "And so for hypothesis, which have a large prior, you're going to compete hard with them.",
                    "label": 0
                },
                {
                    "sent": "And for once, we have a very small prior going to.",
                    "label": 0
                },
                {
                    "sent": "You can allow a lot of slop.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                },
                {
                    "sent": "This is related to the basic observation, which is which is, which is really handy in supervised learning, which is that.",
                    "label": 0
                },
                {
                    "sent": "If you have two paths spaces and you want to learn well Spec 2.",
                    "label": 0
                },
                {
                    "sent": "The Union of the two.",
                    "label": 0
                },
                {
                    "sent": "Then basically you can just learn well on one and the other and there's a little bit of stuff, so she did with this, but.",
                    "label": 0
                },
                {
                    "sent": "But there's no, there's no real cost.",
                    "label": 0
                },
                {
                    "sent": "Star is the.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "I'm surprised at the.",
                    "label": 0
                },
                {
                    "sent": "Beyond.",
                    "label": 0
                },
                {
                    "sent": "Hold on, yeah, you can.",
                    "label": 0
                },
                {
                    "sent": "Actually this is a different way to parameterized things that you have P of the argument here.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as far as indifference goes.",
                    "label": 0
                },
                {
                    "sent": "You can just go back to having a flat prior over the prophecies and now.",
                    "label": 0
                },
                {
                    "sent": "Indifferent says that if you have a Q of H. Then you can subtract off the entropy of that Q.",
                    "label": 0
                },
                {
                    "sent": "From your complexity term, right?",
                    "label": 0
                },
                {
                    "sent": "And that means that if you have.",
                    "label": 0
                },
                {
                    "sent": "Two assets in the second one is like all good parties.",
                    "label": 0
                },
                {
                    "sent": "Then the regret of learning on the Union is less than or equal to the rigor of learning on just the first one.",
                    "label": 0
                },
                {
                    "sent": "So it's it's it's very handy.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can think about luckiness.",
                    "label": 0
                },
                {
                    "sent": "An indifference is sort of two different things, right?",
                    "label": 0
                },
                {
                    "sent": "So one is kind of a variable competition and the other is we gotta subtract off the decisions that we don't need to make.",
                    "label": 0
                },
                {
                    "sent": "We don't need to.",
                    "label": 0
                },
                {
                    "sent": "This is more like the.",
                    "label": 0
                },
                {
                    "sent": "This paper along time ago by Geoff Hinton talking about the bits back argument so that you don't care about decisions you don't need to make.",
                    "label": 0
                },
                {
                    "sent": "So let's say there's a bunch of hypothesis, all of which have a good empirical error rate.",
                    "label": 0
                },
                {
                    "sent": "Then if you take an expectation spectacu.",
                    "label": 0
                },
                {
                    "sent": "Well, I didn't write out that what I mean here is the expectation is that you have a path this.",
                    "label": 0
                },
                {
                    "sent": "You have also have a good empirical error rate.",
                    "label": 0
                },
                {
                    "sent": "And the claim is that you also have a good true error rate.",
                    "label": 0
                },
                {
                    "sent": "Where we're good, we're also good, is relaxed by by this quantity.",
                    "label": 0
                },
                {
                    "sent": "Within different, we don't.",
                    "label": 0
                },
                {
                    "sent": "You don't have to decide in one month.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you're indifferent between two different paths is right.",
                    "label": 0
                },
                {
                    "sent": "Do you have two good hypotheses you don't really care which one?",
                    "label": 0
                },
                {
                    "sent": "Just randomize between the two an you don't pay for.",
                    "label": 0
                },
                {
                    "sent": "You don't pay the complexity of choosing one of them.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So we just chose one of them.",
                    "label": 0
                },
                {
                    "sent": "Then it would be like this, but.",
                    "label": 0
                },
                {
                    "sent": "We subtract off a bit if there's.",
                    "label": 0
                },
                {
                    "sent": "There's two.",
                    "label": 0
                },
                {
                    "sent": "This is the entropy of Q.",
                    "label": 0
                },
                {
                    "sent": "To never teach, you have two age.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, you're right.",
                    "label": 0
                },
                {
                    "sent": "Alright, alright sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what I mean by luckiness an indifference.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now let's think about what happens in active learning.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, let me tell you the setting in active learning.",
                    "label": 0
                },
                {
                    "sent": "So setting active learning is just like supervised learning, except the world only tells you the label if you requested.",
                    "label": 1
                },
                {
                    "sent": "So by default, Step 3 doesn't happen, and learning out of him has to say, please tell me the label.",
                    "label": 1
                },
                {
                    "sent": "So now the goal is going to be the same, except you have this kind of added goal of minimizing your label complexity.",
                    "label": 0
                },
                {
                    "sent": "So you want to want to pay as few people as possible to produce labels for learning algorithm will get into a good solution.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here's a typical algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is one that.",
                    "label": 0
                },
                {
                    "sent": "Century desk to worked on.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a sketch of it, so you have some version space which is going to be like a stateful variable.",
                    "label": 1
                },
                {
                    "sent": "So initially it's going to be equal to the set of all hypothesis and then based upon the observations that we get, we're going to be restricting the set of policies that we care about.",
                    "label": 0
                },
                {
                    "sent": "OK, so first we're going to we're in this kind of iterative loop, so we're going to observe some features.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to predict according to the argument on our set on a restricted set of boxes.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to check and see if there are two hypotheses in our restriction set which disagree.",
                    "label": 0
                },
                {
                    "sent": "And if there are two, they're going to ask the label.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to get some information, so we're going to.",
                    "label": 0
                },
                {
                    "sent": "So we're going to use a sample complexity bound to rule out.",
                    "label": 1
                },
                {
                    "sent": "Hypotheses.",
                    "label": 0
                },
                {
                    "sent": "Which are kind of probably not optimal, so we have a set of prophecies.",
                    "label": 0
                },
                {
                    "sent": "One of them is.",
                    "label": 0
                },
                {
                    "sent": "The set is going to be optimal.",
                    "label": 0
                },
                {
                    "sent": "We're going to maintain that as an invariant, and then we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to be getting samples which tell us information about whether H is better.",
                    "label": 0
                },
                {
                    "sent": "H prime is better.",
                    "label": 0
                },
                {
                    "sent": "So suppose that H is better.",
                    "label": 0
                },
                {
                    "sent": "At some point we'll have enough samples that we can rule out H prime and then H prime will be removed from the set of remaining hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a typical style of algorithm here.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The claim is.",
                    "label": 0
                },
                {
                    "sent": "For all ID distributions, default path sets H or T timesteps.",
                    "label": 1
                },
                {
                    "sent": "With high probability, the regret of the active learning algorithm is going to be similar to the record of the supervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's actually about a factor of two worse.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And OK, so the number of labels.",
                    "label": 0
                },
                {
                    "sent": "Is going to be what is this?",
                    "label": 0
                },
                {
                    "sent": "So we have the log of the nerve properties.",
                    "label": 0
                },
                {
                    "sent": "We have the log of T. We have another log of T here.",
                    "label": 0
                },
                {
                    "sent": "Then we have a log of one over Delta, so this is is like log T squared.",
                    "label": 0
                },
                {
                    "sent": "Let's ignore this term.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And I guess the thing which is interesting is that this can be much less than T. Potentially right?",
                    "label": 0
                },
                {
                    "sent": "In supervised learning this, this is the number of labels is T. But this could be maybe much smaller than T. But this is this funny thing here this data.",
                    "label": 0
                },
                {
                    "sent": "Which is this disagreement coefficient?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We only get a label when we requested.",
                    "label": 0
                },
                {
                    "sent": "Right do it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I agree with each other in a restricted set, so we start.",
                    "label": 0
                },
                {
                    "sent": "Initially it's we ask for label every time there's a disagreement.",
                    "label": 0
                },
                {
                    "sent": "Which is essentially every time.",
                    "label": 1
                },
                {
                    "sent": "But as we are learning progress is it will discover that many prophecies are not going to be optimal.",
                    "label": 0
                },
                {
                    "sent": "And that means that all the remaining set of paths may agree on the label at some point.",
                    "label": 0
                },
                {
                    "sent": "And so you cease to ask for the label.",
                    "label": 1
                },
                {
                    "sent": "No, because they just just saying if they all agree on this One X doesn't say they all agree on all axes.",
                    "label": 0
                },
                {
                    "sent": "What do you mean by probably not optimal?",
                    "label": 0
                },
                {
                    "sent": "So I mean, you can apply sample complexity bounds, which gives you gives you a large DB.",
                    "label": 0
                },
                {
                    "sent": "This is with probability point.",
                    "label": 0
                },
                {
                    "sent": "So point really, really small.",
                    "label": 0
                },
                {
                    "sent": "This process is not going to be the optimal one.",
                    "label": 0
                },
                {
                    "sent": "Use this bound each time to make this right.",
                    "label": 0
                },
                {
                    "sent": "Exponentially great listen exponentially degrade because you can in the worst case you could think about doing a union bound over T time steps.",
                    "label": 0
                },
                {
                    "sent": "And you know that T is a small number compared to the number of prophecies, so even multiplying math space by a factor of T is or even doesn't matter that much.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah?",
                    "label": 0
                },
                {
                    "sent": "Then you will not end up with your unique I put asunder.",
                    "label": 0
                },
                {
                    "sent": "So we're not trying to get a unique hypothesis, we're just trying to get a process which will be good after tee time steps, right?",
                    "label": 0
                },
                {
                    "sent": "And there is some tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So so when I when I said the theorem there is like a factor of two or so between the supervised regrets and the active learning regret.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, that's intuitive because.",
                    "label": 0
                },
                {
                    "sent": "What we're doing here is we're looking at pairs of prophecies.",
                    "label": 0
                },
                {
                    "sent": "So if you think about.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Log of the size of the past, the CET squared.",
                    "label": 0
                },
                {
                    "sent": "This can be 2 log of size of offset, so you get a factor of 2 ish.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the question is, what is the disagreement coefficient so the discriminant coefficient is some measure of the compatibility between.",
                    "label": 0
                },
                {
                    "sent": "The distribution D and they both said age, so this is really a status of DH.",
                    "label": 0
                },
                {
                    "sent": "Her yeah.",
                    "label": 0
                },
                {
                    "sent": "Kisses so let me try to give you a better idea so.",
                    "label": 0
                },
                {
                    "sent": "Yes, this comes from D. Yeah.",
                    "label": 0
                },
                {
                    "sent": "So this understanding of active learning didn't exist a few years ago.",
                    "label": 0
                },
                {
                    "sent": "Previous to a few years ago.",
                    "label": 0
                },
                {
                    "sent": "Since we all actually learning algorithms were either beige and learning algorithms or or, they only worked in the realizable case.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "This works when.",
                    "label": 0
                },
                {
                    "sent": "This algorithm that I'm telling you here always works regardless of how much noise there is and you get this small label complexity when the area happens to be small.",
                    "label": 0
                },
                {
                    "sent": "Area, to the best of both hips to be small.",
                    "label": 0
                },
                {
                    "sent": "But unlike many active learning algorithms, you're not controlling your not sample in X near a problematic point, decision about which algorithms.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so how does this work?",
                    "label": 0
                },
                {
                    "sent": "So what does it mean to have two policies which disagree?",
                    "label": 0
                },
                {
                    "sent": "Maybe that means you have a version space over linear separators.",
                    "label": 1
                },
                {
                    "sent": "And you know, there's some sort of margin where you're sure that everything outside of this.",
                    "label": 0
                },
                {
                    "sent": "All linear separators, corresponding decisions outside of this margin are decided.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you're not biasing the choice of XI mean.",
                    "label": 1
                },
                {
                    "sent": "It might be expensive to get an X.",
                    "label": 0
                },
                {
                    "sent": "In that case you wouldn't want to buy us the choice of ecstasy in the near the decision boundary of what you've learned so far.",
                    "label": 0
                },
                {
                    "sent": "But this is basing the X, yeah?",
                    "label": 0
                },
                {
                    "sent": "Image from from whatever your underlying distribution is from D, but your rejection sampling on the X and introduces a bias.",
                    "label": 0
                },
                {
                    "sent": "Yes, but it was expensive to get X.",
                    "label": 0
                },
                {
                    "sent": "Might want to.",
                    "label": 0
                },
                {
                    "sent": "Not sample from the sample from some other distribution.",
                    "label": 0
                },
                {
                    "sent": "I could be wrong standard, I did nothing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so most active learning is motivated by X is cheap in the labels that are expensive.",
                    "label": 0
                },
                {
                    "sent": "In which is?",
                    "label": 0
                },
                {
                    "sent": "This is fine.",
                    "label": 0
                },
                {
                    "sent": "But also the idea being passive.",
                    "label": 0
                },
                {
                    "sent": "D. Showing the axis, I think it's something that you really get you closer to ideal situation and then you can manage to advance.",
                    "label": 0
                },
                {
                    "sent": "And if you have an aggressive way of asking for extra time.",
                    "label": 0
                },
                {
                    "sent": "I tend to be given by by nature.",
                    "label": 0
                },
                {
                    "sent": "Then you make too many bias and then you find will.",
                    "label": 0
                },
                {
                    "sent": "I see, yes, that's exactly right, so there's a lot of these algorithms which try to choose like the single X, which is close to the decision boundary.",
                    "label": 0
                },
                {
                    "sent": "And the problem with those algorithms is that they can end up focusing on the noise in the learning in the data and then producing decision surface which is just nowhere even near these algorithms are asymptotically inconsistent, they will never converge to the right solution in your set of policies.",
                    "label": 0
                },
                {
                    "sent": "Wondering about this so if you wouldn't say rather I give you a big mushroom exit, you can have every time you can choose from that bag.",
                    "label": 0
                },
                {
                    "sent": "Not not really from the back so I give you a part of.",
                    "label": 0
                },
                {
                    "sent": "Exit.",
                    "label": 0
                },
                {
                    "sent": "Every time you can go over all these guys and there wouldn't be a big tiempo right?",
                    "label": 0
                },
                {
                    "sent": "Because here it seems like you have to wait until something comes.",
                    "label": 0
                },
                {
                    "sent": "Block ratio between many you like and how many?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I need exponentially many more samples than I would really want to, so you think about sort of a setting where you see all of your unlabeled data up front and then you wanted to choose.",
                    "label": 0
                },
                {
                    "sent": "So there is a statement that can be made here.",
                    "label": 0
                },
                {
                    "sent": "So, So what?",
                    "label": 0
                },
                {
                    "sent": "T you should think of T is not the number of unlabeled data points, but rather is the number of labeled data points given to achieve the supervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "To achieve some error rate.",
                    "label": 0
                },
                {
                    "sent": "Next to each other you see, so we have a supervised learning algorithm which given T examples has some error rate.",
                    "label": 0
                },
                {
                    "sent": "We have an active learning algorithm which given these this number of examples achieves essentially the same error rate.",
                    "label": 0
                },
                {
                    "sent": "Her same regret, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so that's maybe that's a even when you're in this sort of batch setting.",
                    "label": 0
                },
                {
                    "sent": "That's a way to define T which which is consistent with these.",
                    "label": 0
                },
                {
                    "sent": "Could do better.",
                    "label": 0
                },
                {
                    "sent": "I yes yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that actually is related to.",
                    "label": 0
                },
                {
                    "sent": "Wanna conclusions OK so?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me tell you what the disagreement coefficient is.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's some awful hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And there's all the properties which are within epsilon of it, narrate, right?",
                    "label": 0
                },
                {
                    "sent": "So let's define a restriction of the set of boxes called H Step Salon.",
                    "label": 0
                },
                {
                    "sent": "OK, so now is going to be in restriction of this set of.",
                    "label": 0
                },
                {
                    "sent": "Unlabeled examples, which corresponds to the right which corresponds to.",
                    "label": 0
                },
                {
                    "sent": "The set though, so we're going to look at all the hypoth season dates of epsilon.",
                    "label": 0
                },
                {
                    "sent": "We're going to ask does it was disagrees with the best.",
                    "label": 0
                },
                {
                    "sent": "Kiss wait wait what?",
                    "label": 0
                },
                {
                    "sent": "It's it's the region which of the feature space you're going to care about.",
                    "label": 0
                },
                {
                    "sent": "Once you've ruled out all the dumb hypotheses.",
                    "label": 0
                },
                {
                    "sent": "Right, and then we're going to have this agreement coefficient, which is going to be the maximum ratio of these.",
                    "label": 0
                },
                {
                    "sent": "So this fraction of feature space.",
                    "label": 0
                },
                {
                    "sent": "Oh, we have a workshop for awhile.",
                    "label": 0
                },
                {
                    "sent": "Freedom toys.",
                    "label": 0
                },
                {
                    "sent": "Thank you, go to 403.",
                    "label": 0
                },
                {
                    "sent": "We go to 403 where we have our class every year.",
                    "label": 0
                },
                {
                    "sent": "So it says.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have the measure of the features near 2 that you care about when you're near the optimum, and then you have.",
                    "label": 0
                },
                {
                    "sent": "You know the distance.",
                    "label": 0
                },
                {
                    "sent": "So this ratio is the disagreement coefficient, yeah?",
                    "label": 1
                },
                {
                    "sent": "That's right, I forgot to write in.",
                    "label": 0
                },
                {
                    "sent": "There exists HNH epsilon here.",
                    "label": 0
                },
                {
                    "sent": "But this is not something that you can define.",
                    "label": 0
                },
                {
                    "sent": "It can calculate definitely, not so.",
                    "label": 0
                },
                {
                    "sent": "You cannot calculate this, but the algorithm doesn't actually need to know Theta.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is oblivious that room just runs and then something is true about its output.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the description when you have two parties?",
                    "label": 0
                },
                {
                    "sent": "What is the disagreement coefficient when you have two hypothesis, when H sub epsilon consists of H1 and H2?",
                    "label": 1
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "Really, let's say that they disagree at least somewhere.",
                    "label": 0
                },
                {
                    "sent": "Everybody.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is the answer to yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's one because you know there's some fraction of space to disagree on, and that epsilon in the worst case could be the size equal to the fraction and so.",
                    "label": 0
                },
                {
                    "sent": "Yeah this one.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "Another simple cases when you have thresholds in on the real line, right so or just think about you have thresholds on the line.",
                    "label": 1
                },
                {
                    "sent": "So just think about step functions.",
                    "label": 0
                },
                {
                    "sent": "And we have any distribution over for the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Any continuous distribution.",
                    "label": 0
                },
                {
                    "sent": "So what's the disagreement coefficient?",
                    "label": 1
                },
                {
                    "sent": "What's over one?",
                    "label": 0
                },
                {
                    "sent": "But it's not actually one.",
                    "label": 0
                },
                {
                    "sent": "Turned out to be too.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do you have the office hypothesis and then you have an epsilon measure over this direction that you could disagree with in epsilon measure this direction that you disagree with so it's 2.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And there's linear separators through the original in Rd with the uniform data distribution.",
                    "label": 1
                },
                {
                    "sent": "So we have the unlabeled data is uniform over the sphere.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                },
                {
                    "sent": "But this one is not gonna be able to get it if you don't know it, but it turns out that it's Rudy.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, so so I guess the fact that is less than D means that active learning can give you some speedups there over over just supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, on the sphere.",
                    "label": 0
                },
                {
                    "sent": "If it's uniform in the sphere and projected to be uniform on the sphere in the same analysis applies.",
                    "label": 0
                },
                {
                    "sent": "Number of.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There should be a right?",
                    "label": 0
                },
                {
                    "sent": "So what I'm not telling you I guess, is that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This can be replaced with BC dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the discriminant.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Efficient and.",
                    "label": 0
                },
                {
                    "sent": "Give you some notion of.",
                    "label": 0
                },
                {
                    "sent": "Of what's compatible and what's not, and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now what happens with luckiness?",
                    "label": 0
                },
                {
                    "sent": "To luckiness is variable competition depending upon the prior right.",
                    "label": 0
                },
                {
                    "sent": "Can we achieve that in this kind of setting?",
                    "label": 0
                },
                {
                    "sent": "So I think the answer.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, no, you can't very easily.",
                    "label": 0
                },
                {
                    "sent": "So you have a disagreement coefficient.",
                    "label": 0
                },
                {
                    "sent": "For the union of two paths sets.",
                    "label": 0
                },
                {
                    "sent": "An only bound that I can come up with.",
                    "label": 0
                },
                {
                    "sent": "Is like the sum plus maybe a constant?",
                    "label": 0
                },
                {
                    "sent": "So it says that the label complexity of learning on two paths sets.",
                    "label": 1
                },
                {
                    "sent": "It could be the sum of the label complexity of learning this plus little complexity learning on this.",
                    "label": 0
                },
                {
                    "sent": "And it's very bad if you're interested in.",
                    "label": 1
                },
                {
                    "sent": "In a prior, because you could, you could imagine having a prior which is.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe maybe I bought this at one.",
                    "label": 0
                },
                {
                    "sent": "All the parties over small, prior and here they all have large prior.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It seems that we have to have to pay to explore this hypothesis set.",
                    "label": 0
                },
                {
                    "sent": "Even though we made up wanting to just go with that bus this year.",
                    "label": 0
                },
                {
                    "sent": "News.",
                    "label": 0
                },
                {
                    "sent": "Artifact of your structure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll get to that.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what about indifference?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in differences, if there's irrelevant choices, so we have two paths which are both optimal.",
                    "label": 0
                },
                {
                    "sent": "We don't pay for the fact that we have two, we.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess in the original PAC Bayes bound is tracked off a bit from your complexity, which is great.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It it works.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is looking at one hypothesis at a time, so it by definition.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so we could imagine I'm willing.",
                    "label": 0
                },
                {
                    "sent": "I'm happy enough to.",
                    "label": 0
                },
                {
                    "sent": "Go back into the algorithm and and let you predict according to a distribution over past.",
                    "label": 1
                },
                {
                    "sent": "Easier if you want.",
                    "label": 0
                },
                {
                    "sent": "I would think it's a relatively minor modification of this style of algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "Crime in HS?",
                    "label": 0
                },
                {
                    "sent": "What do you like that?",
                    "label": 1
                },
                {
                    "sent": "That's basically that's the decision whether you ask for later, that's right.",
                    "label": 0
                },
                {
                    "sent": "And also removing things.",
                    "label": 0
                },
                {
                    "sent": "Distribution.",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "Condition on the fact it's not over there in the distribution shrinks down to something more concentrated.",
                    "label": 0
                },
                {
                    "sent": "Space space.",
                    "label": 0
                },
                {
                    "sent": "Maybe I don't know how to do that.",
                    "label": 0
                },
                {
                    "sent": "In situation where you are small.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It will not be likely that too good.",
                    "label": 0
                },
                {
                    "sent": "I will defend.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think it's you actually cannot get indifference, and here's the logic so.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have H2 is a bunch of hypothesis which all achieved the minimum rate on each one.",
                    "label": 0
                },
                {
                    "sent": "Then how can we bound the disagreement coefficient right?",
                    "label": 0
                },
                {
                    "sent": "So we OK, we have the original, but then we have something which is proportional to the size of the H2.",
                    "label": 0
                },
                {
                    "sent": "Because each of these different issues could be sucking a different portion of the ex space into consideration.",
                    "label": 0
                },
                {
                    "sent": "And that's that's really bad.",
                    "label": 0
                },
                {
                    "sent": "I mean linear in the size of office spaces.",
                    "label": 0
                },
                {
                    "sent": "This is pretty catastrophic.",
                    "label": 0
                },
                {
                    "sent": "Yeah he.",
                    "label": 0
                },
                {
                    "sent": "Sorry it's very small, say 0.",
                    "label": 0
                },
                {
                    "sent": "Danny.",
                    "label": 0
                },
                {
                    "sent": "Retrieve your.",
                    "label": 0
                },
                {
                    "sent": "That may be true.",
                    "label": 0
                },
                {
                    "sent": "755 yeah, so we don't know that they're all the same.",
                    "label": 0
                },
                {
                    "sent": "We don't know if they're all good, but we don't know that all same there right?",
                    "label": 0
                },
                {
                    "sent": "There?",
                    "label": 0
                },
                {
                    "sent": "Obviously not all the same, and in what they predict, right?",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Would just be wasted if it would be terribly wasted.",
                    "label": 0
                },
                {
                    "sent": "To indifferent seems to not apply in a pretty strong sense here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so so objections that you.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have so the first was with David said.",
                    "label": 0
                },
                {
                    "sent": "We're comparing upper bounds upper bounds and that sets.",
                    "label": 0
                },
                {
                    "sent": "Pretty problematic thing to do, but actually there's there's a lower bound involving a disagreement coefficient in the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "So if you replace the log of size of office with the VC dimension, there's a similar structured lower bound.",
                    "label": 0
                },
                {
                    "sent": "So the disagreement condition is not like a random definition, that's an artifact of the proof.",
                    "label": 0
                },
                {
                    "sent": "It claims that this is really getting at something.",
                    "label": 0
                },
                {
                    "sent": "In the structure of the problem itself.",
                    "label": 0
                },
                {
                    "sent": "Number.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so should we care about active learning?",
                    "label": 1
                },
                {
                    "sent": "Maybe maybe the solution to this is OK, who you know?",
                    "label": 0
                },
                {
                    "sent": "So you have some setting, but I don't care about it because it doesn't look nice.",
                    "label": 0
                },
                {
                    "sent": "And I'm actually someone synthetic to this.",
                    "label": 0
                },
                {
                    "sent": "It does kind of look kind of loose and unclean and what not right?",
                    "label": 0
                },
                {
                    "sent": "But but.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The simple answer to this so empirically actually learning is used all the time.",
                    "label": 0
                },
                {
                    "sent": "People really care about it, so this is the best analysis that we have for pregnancy, active learning and so.",
                    "label": 1
                },
                {
                    "sent": "I mean, we're just trying to to understand what people are doing and help guide learning algorithms and that's basically what we should be trying to do.",
                    "label": 0
                },
                {
                    "sent": "Another stronger claim is that.",
                    "label": 0
                },
                {
                    "sent": "Well, actually from this line of work we're starting to get useful algorithms.",
                    "label": 1
                },
                {
                    "sent": "Things actually reduced the label complexity required in order to achieve some particular error rate.",
                    "label": 1
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And then OK, so maybe you don't find these convincing, but but I actually came with a different exploration setting as well.",
                    "label": 0
                },
                {
                    "sent": "The claim is true.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To see some similar behavior there.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to switch to a new setting.",
                    "label": 0
                },
                {
                    "sent": "It's also an exploration.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is not the active learning exploration setting.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this new setting what happens is things change here.",
                    "label": 0
                },
                {
                    "sent": "So previously.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning, the world just tells you what the true label is.",
                    "label": 1
                },
                {
                    "sent": "Here we're going to choose some particular.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the problem with jet lag.",
                    "label": 0
                },
                {
                    "sent": "What had is a, so we're going to choose some action, which I could also call label.",
                    "label": 0
                },
                {
                    "sent": "Just a question of definitions.",
                    "label": 0
                },
                {
                    "sent": "And we're only only going to see a reward for that particular chosen.",
                    "label": 0
                },
                {
                    "sent": "Action or label?",
                    "label": 0
                },
                {
                    "sent": "OK, so we're not going to learn this was the right label, or is it going to learn?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, this was a good idea.",
                    "label": 0
                },
                {
                    "sent": "Or oh this was a bad idea.",
                    "label": 0
                },
                {
                    "sent": "So and then the goal is going to be the same as in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We're going to.",
                    "label": 1
                },
                {
                    "sent": "We're going to want to compete with a set of bases.",
                    "label": 1
                },
                {
                    "sent": "Yes, so we start wanting to look at the cumulative regret pretty strongly in this setting rather than the instantaneous final.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This turns out to be very easy and motivate at Yahoo because for example you could want to choose news stories that interest people visiting Yahoo, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe you have features of the sort only looked at this kind of story in the past and then then you have some set of new stories and you wanted to choose them right?",
                    "label": 0
                },
                {
                    "sent": "So you can phrase this as a contextual bandit.",
                    "label": 0
                },
                {
                    "sent": "Setting.",
                    "label": 0
                },
                {
                    "sent": "Problem is that this is this is 1 application claiming that there are dozens or hundreds of applications.",
                    "label": 0
                },
                {
                    "sent": "Anything involving getting information from a user.",
                    "label": 0
                },
                {
                    "sent": "He's going to have this kind of structure because you can never reset a user and say OK go back in time and tell me how you would have liked it if I had done this instead.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's talk about a simple hour.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this, so we're going to keep track of instantaneous regret are so instantaneous regret.",
                    "label": 1
                },
                {
                    "sent": "I mean, I'm going to assume there's some distribution.",
                    "label": 0
                },
                {
                    "sent": "It's going to be over.",
                    "label": 0
                },
                {
                    "sent": "Features and the reward for each of the different actions.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to have a data set which is going to consist of.",
                    "label": 0
                },
                {
                    "sent": "Features the choice that was taken and the reward for that choice.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have a.",
                    "label": 0
                },
                {
                    "sent": "We have a measure of how.",
                    "label": 0
                },
                {
                    "sent": "I shouldn't really use E. We have a measure of how good, and I thought this is, which is which.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                },
                {
                    "sent": "This is like the analog of the empirical error rate, it's.",
                    "label": 0
                },
                {
                    "sent": "For the rounds when they pass, this agrees with the action that was taken.",
                    "label": 0
                },
                {
                    "sent": "We get a credit equal to the reward.",
                    "label": 0
                },
                {
                    "sent": "And we're going to actually want to maximize this.",
                    "label": 0
                },
                {
                    "sent": "In the typo, let's ignore PYI set PY to be equal to .5.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "I see so this is going to be an argmax now.",
                    "label": 0
                },
                {
                    "sent": "So here's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is with probability 1 -- R. We're going to choose the hypothesis which has.",
                    "label": 1
                },
                {
                    "sent": "The largest.",
                    "label": 0
                },
                {
                    "sent": "Iprime of HS so was probably 1 -- R for our instantaneous regret, which is going to be given to us by sample complexity bound.",
                    "label": 0
                },
                {
                    "sent": "I'm going to just go with the Arg.",
                    "label": 0
                },
                {
                    "sent": "Max should be a Max and otherwise we're going to choose an action at random.",
                    "label": 0
                },
                {
                    "sent": "Never going to observe the reward, and if it was a randomly action is a randomly taken action.",
                    "label": 0
                },
                {
                    "sent": "We're going to update our set of examples, but only if it was randomly chosen.",
                    "label": 0
                },
                {
                    "sent": "And then we can't just throw in the examples from the rounds that what we exploited.",
                    "label": 0
                },
                {
                    "sent": "This is like an explore versus exploit decision here, right so?",
                    "label": 0
                },
                {
                    "sent": "Probably once or we're going to exploit.",
                    "label": 0
                },
                {
                    "sent": "We're going to choose the best action that we know, and with probability R, we're going to explore, we're going to choose an action at random.",
                    "label": 1
                },
                {
                    "sent": "Are is a number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "It does change so.",
                    "label": 0
                },
                {
                    "sent": "The idea is that ours are instantaneous regret so.",
                    "label": 0
                },
                {
                    "sent": "Let's go back to.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "This is instantaneous regret.",
                    "label": 0
                },
                {
                    "sent": "This is 1 example of instantaneous regret.",
                    "label": 0
                },
                {
                    "sent": "And so we have.",
                    "label": 0
                },
                {
                    "sent": "We don't know that, but we can perhaps bound that.",
                    "label": 0
                },
                {
                    "sent": "We have very simple complexity bounds.",
                    "label": 0
                },
                {
                    "sent": "Yeah, something like that.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is it?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Here yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so this should be it really be a Max.",
                    "label": 0
                },
                {
                    "sent": "But we're going to says we're going to choose the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "I see.",
                    "label": 0
                },
                {
                    "sent": "We're finding a good number of errors.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is no S there.",
                    "label": 0
                },
                {
                    "sent": "How RBA?",
                    "label": 0
                },
                {
                    "sent": "He will not tend to increase the zero.",
                    "label": 0
                },
                {
                    "sent": "For example, that's alright.",
                    "label": 0
                },
                {
                    "sent": "Can there's no hypothesis what?",
                    "label": 0
                },
                {
                    "sent": "That is changing.",
                    "label": 0
                },
                {
                    "sent": "During the process.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can plug in that supervised learning bound.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "And then we can see what will happen in high probability.",
                    "label": 0
                },
                {
                    "sent": "It will turn out to be.",
                    "label": 0
                },
                {
                    "sent": "So previously we had a square root and now we get a 1/3.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Ark.",
                    "label": 0
                },
                {
                    "sent": "Yes, oh, it could be.",
                    "label": 0
                },
                {
                    "sent": "Could be a mess.",
                    "label": 0
                },
                {
                    "sent": "I mean, it could be you could plug in some broader complexity, whatever that would be.",
                    "label": 0
                },
                {
                    "sent": "It would be hard to evaluate.",
                    "label": 0
                },
                {
                    "sent": "More and more yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so actually you can plug in.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I made the rewards be either zero or one.",
                    "label": 0
                },
                {
                    "sent": "This factor of 2 doesn't matter because it's the same everywhere.",
                    "label": 0
                },
                {
                    "sent": "The factor of two just becomes not too, and when you don't choose uniform randomly.",
                    "label": 0
                },
                {
                    "sent": "So if you just if you make this 01 then you can.",
                    "label": 0
                },
                {
                    "sent": "This is also 01.",
                    "label": 0
                },
                {
                    "sent": "And so we're just going to have another 01 random variable.",
                    "label": 0
                },
                {
                    "sent": "We're taking the expectation of.",
                    "label": 0
                },
                {
                    "sent": "And so all the bounds you're used to from supervised learning will allow you to get an R. Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so there's always one caveat, which is that you have to do with the martingale aspect of things.",
                    "label": 0
                },
                {
                    "sent": "But typically there's a way to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is that maybe the simplest algorithm for me to tell you?",
                    "label": 0
                },
                {
                    "sent": "I didn't want to get more complex, but let me just tell you that there is another algorithm called Exp 4P which allows you to replace three with two.",
                    "label": 0
                },
                {
                    "sent": "Which is which is interesting, but nevertheless it is kind of immaterial to this discussion, because.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "'cause the same issues will occur with the four P's with epic greedy.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is just saying that R can be any sample complexity pound.",
                    "label": 0
                },
                {
                    "sent": "You're all quite useful next bound to think.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, So what happens luckiness?",
                    "label": 0
                },
                {
                    "sent": "Cortana.",
                    "label": 0
                },
                {
                    "sent": "It all works.",
                    "label": 0
                },
                {
                    "sent": "Does it work?",
                    "label": 0
                },
                {
                    "sent": "So we want to compete.",
                    "label": 0
                },
                {
                    "sent": "We have a set of policies we want to compete with, some subset harder than we compete with a different subset.",
                    "label": 0
                },
                {
                    "sent": "What happens to?",
                    "label": 0
                },
                {
                    "sent": "To our regrets.",
                    "label": 0
                },
                {
                    "sent": "Using that bound bound with the.",
                    "label": 0
                },
                {
                    "sent": "Agents yeah.",
                    "label": 0
                },
                {
                    "sent": "So what would our be?",
                    "label": 0
                },
                {
                    "sent": "Would you would you don't know advance, and that's that's the root of the problem.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the problem.",
                    "label": 0
                },
                {
                    "sent": "You don't if you have a non uniform prior.",
                    "label": 0
                },
                {
                    "sent": "Then R. In I don't know what you are you would choose, but the thing which is safe is to choose the Max of this which is which is not going to go anywhere useful, right?",
                    "label": 0
                },
                {
                    "sent": "If you have a candle with infinite set of hypotheses with you know decreasing prior as we go out.",
                    "label": 0
                },
                {
                    "sent": "This is going to always be one and.",
                    "label": 0
                },
                {
                    "sent": "1st.",
                    "label": 0
                },
                {
                    "sent": "To decide whether to actually use that agent.",
                    "label": 0
                },
                {
                    "sent": "Part of the observation, create your prior and then enough.",
                    "label": 0
                },
                {
                    "sent": "So I say it again.",
                    "label": 0
                },
                {
                    "sent": "Activation for designing your prior basically and the other half.",
                    "label": 0
                },
                {
                    "sent": "Play your game.",
                    "label": 0
                },
                {
                    "sent": "Like fire.",
                    "label": 0
                },
                {
                    "sent": "Yeah, learning the prior is a pretty delicate process here because.",
                    "label": 0
                },
                {
                    "sent": "Because we need to make sure we explore the set of actions sufficiently well.",
                    "label": 0
                },
                {
                    "sent": "So this problem is shared with 64P.",
                    "label": 1
                },
                {
                    "sent": "There is some minimal amount of exploration XP for P, and if you if you want to a variable competition type thing then you end up wanting to drive exploration very small in one case and large in the other case, and you don't know which to choose.",
                    "label": 0
                },
                {
                    "sent": "OK, you can also see that there's a problem.",
                    "label": 0
                },
                {
                    "sent": "You can have two hypothesis spaces.",
                    "label": 1
                },
                {
                    "sent": "Which choose, let's say we had more than two arms available to actions available, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe we have we have 4, right?",
                    "label": 0
                },
                {
                    "sent": "We have the space which just choose amongst these two actions we have another space to choose amongst these two actions.",
                    "label": 0
                },
                {
                    "sent": "And there's no sense in which exploring well for this to both space is going to help you choose amongst these apostasies.",
                    "label": 0
                },
                {
                    "sent": "So your your complexity of exploration.",
                    "label": 1
                },
                {
                    "sent": "Your regret is going to end up adding again.",
                    "label": 0
                },
                {
                    "sent": "Which is which is not great.",
                    "label": 0
                },
                {
                    "sent": "And what if your prior is distribution dependent?",
                    "label": 0
                },
                {
                    "sent": "Meaning you will not notice prior.",
                    "label": 0
                },
                {
                    "sent": "Say that I will put more weight on good classifier.",
                    "label": 0
                },
                {
                    "sent": "Possible to estimate?",
                    "label": 0
                },
                {
                    "sent": "Because then you will not have this problem.",
                    "label": 0
                },
                {
                    "sent": "Darkness will be fulfilled because.",
                    "label": 0
                },
                {
                    "sent": "Your prior notice distribution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know how to do this.",
                    "label": 0
                },
                {
                    "sent": "Jungle talk",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. OK, So what about indifference?",
                    "label": 0
                },
                {
                    "sent": "So there's two hypotheses that are both equally good.",
                    "label": 0
                },
                {
                    "sent": "Then you shouldn't pay for choosing amongst those two.",
                    "label": 0
                },
                {
                    "sent": "So it ends up working.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the one positive answer.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So you can replace the argument.",
                    "label": 0
                },
                {
                    "sent": "With an expectation over some Q.",
                    "label": 0
                },
                {
                    "sent": "You can put in a PAC Bayes bound member.",
                    "label": 0
                },
                {
                    "sent": "You can choose any bound for.",
                    "label": 0
                },
                {
                    "sent": "To compute R. And will end up benefiting from the back basement.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's great, and in some sense Exp.",
                    "label": 0
                },
                {
                    "sent": "Four P is formalizing the fact that Indifferences is what you want because it uses indifference in a strong sense.",
                    "label": 0
                },
                {
                    "sent": "Did that in order to get the 1/2 power on the regret.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "There's something, so I'm pretty convinced this is something not working.",
                    "label": 0
                },
                {
                    "sent": "PAC Bayes, like in active learning setting, it seems like these the luckiness and indifference are pretty problem problematic there.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "One thing which is striking about the active learning algorithms is that they're not using unsupervised data as a resource.",
                    "label": 1
                },
                {
                    "sent": "They're saying that on the same set of data where supervised learning algorithm asks for every label.",
                    "label": 0
                },
                {
                    "sent": "We compete with it and we ask for much fewer labels.",
                    "label": 0
                },
                {
                    "sent": "But it seems like there's some potential to have a result which says that.",
                    "label": 0
                },
                {
                    "sent": "If you give my active learning algorithm, hope access to a whole bunch more unlabeled data that can compete with with non super with the supervised learning algorithm which just has fewer unlabeled data points that sees.",
                    "label": 0
                },
                {
                    "sent": "But it's tricky to do.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So for additional bands, simply indifferent works out OK.",
                    "label": 1
                },
                {
                    "sent": "But there's substantial problems with luckiness.",
                    "label": 1
                },
                {
                    "sent": "And that's interesting because.",
                    "label": 0
                },
                {
                    "sent": "Can I get the root of this question, which is sort of going around which is?",
                    "label": 0
                },
                {
                    "sent": "In order to really solve these problems well, do you need to be basean right?",
                    "label": 0
                },
                {
                    "sent": "Because there are some people who use Bayesian approaches for these.",
                    "label": 0
                },
                {
                    "sent": "And because they believe in their prior, they believe that the problem is actually drawn from the prior.",
                    "label": 0
                },
                {
                    "sent": "They end up not having.",
                    "label": 0
                },
                {
                    "sent": "Not having to worry about these things so much, right?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there's a little bit more discussion about brace expiration topics at lunch.net.",
                    "label": 0
                },
                {
                    "sent": "This is a blog that.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's a bibliography if you're interested.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}