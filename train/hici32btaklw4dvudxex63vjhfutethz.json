{
    "id": "hici32btaklw4dvudxex63vjhfutethz",
    "title": "A New Convex Relaxation for Tensor Completion",
    "info": {
        "author": [
            "Bernardino Romera-Paredes, Department of Engineering Science, University of Oxford"
        ],
        "published": "Nov. 7, 2013",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Decision Support",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/lsoldm2013_romera_paredes_tensor_completion/",
    "segmentation": [
        [
            "I'm going to talk about the with multi task learning.",
            "Also very related problem.",
            "Convex relaxation for tensor completion that we we've been researching and wear with my supervisor's Massimiliano pointed and also now yeah."
        ],
        [
            "So I'm first I will talk about the apparently we're facing and the motivation behind it.",
            "Then I'll explain the proposed solutions.",
            "Basically there are two variants.",
            "When it's a non convex and the other one is convex, then we present.",
            "I present some experiments on this and then I'll come back to the convex approach to try to make it better than fine.",
            "I will point out the main points of of the talk."
        ],
        [
            "So our problem is pretty much cogent analysis where we would like to do is to learn how people value our product in terms of the attributes describing this product or the service.",
            "So for instance, here we have an example on restaurants.",
            "So in the left vector that we see here is a vector describing a restaurant.",
            "So we have several attributes like kind of cuisine, location.",
            "Price and so on.",
            "And then we would like to learn one way vector for each subject so that we learn how they ponder the attributes there in order to value arrest each restaurant.",
            "So."
        ],
        [
            "In this kind of scenario, in this kind of situation, it makes sense.",
            "Or is this a reasonable assumption to think that the way that their son commonalities in a way different people ponder the attributes of the restaurants in this case?",
            "So that's the objective at the end of multi task learning.",
            "So we can apply to task lighting in order to learn the weight vectors of of the of the subjects.",
            "So one way of doing this is by, for instance, optimizing this problem that we have there.",
            "Where we minimize the loss function with respect to the data and also we constraint the methods of weights that we have.",
            "Here we constrain the rank of it.",
            "This can be seen at the end as a generalization of this completion or collaborative filtering, where we have rather than having the inputs of the methods, we have several regression problems.",
            "So this is a modeling works quite well.",
            "Thing in this case, but what if we want to get also information about different aspects of restaurants?",
            "For instance, we can also."
        ],
        [
            "And try to predict how different people evaluate the food quality of the restaurant, the service quality, overall quite in general or many other aspects.",
            "So in that case we have we don't have a metric anymore, but we have this this tensor an in that in this case we want to learn a function for each combination of aspect of a restaurant, an subject.",
            "So the difference with respect to the previous case.",
            "When we played multitask learning, is that in the previous case each task was was identified by one index.",
            "In this case very one identity by one subject, but now its task is identified by more than one.",
            "In particular.",
            "In this case, each task is identified by a person an.",
            "An aspect like this.",
            "This woman here on foot.",
            "So if we apply normal without learning approaches, we will be missing information that is very valuable.",
            "This structural information or the tasks.",
            "An Furthermore and."
        ],
        [
            "By considering this structure, we have the additional advantage about this nice transfer learning property, which is that we can learn some tasks even when no instances having provided as long as there are some other related tasks we do which do have training instances.",
            "So for instance, here in this example.",
            "The columns that are in white are tasks with no training instances, but we can still learn the whole tensor because we have instances for or other tasks.",
            "So."
        ],
        [
            "In this scenario, given that we are dealing with tensors, we apply multilinear Algebra 2 in order to model the whole scenario.",
            "An in particular we.",
            "Wisdom in projective is to minimize this function there where W is this set of the tensor of weights that we use.",
            "So before F is a loss function which relates the tensor with the data.",
            "And then the second term is a regularizer, which encourages the solution that answer to be as simple as possible, and we measure these simplicity in terms of the rank of the different methods isation of the tensor.",
            "So what is amortization would be a question.",
            "So basically this station is a way to rearrange the elements of a tensor in terms of metrics.",
            "They are in and most dense.",
            "Are there any different matrices station?",
            "So for instance in the previous case we have three more tensors tensor, so we have three different matrices stations, one for each mode.",
            "So in one in the first one for instance, we see that there is one column for each weight I normalization.",
            "There would be one column for each subject and so on.",
            "But at the end none of them we have the same elements as in the original tensor, but just rearrange.",
            "In a different way.",
            "And then what we want to do is to minimize the rank of these matrices simultaneously.",
            "So."
        ],
        [
            "First, we apply a nonconvex approach based on the type of composition so that added composition can be seen in a way as a extension of single ability decomposition to tensors.",
            "Where we express a tensor as a multiplication between a core tensor which is usually smaller times a set of matrices, one for each for each mode.",
            "Then we reform.",
            "We can reformulate the previous objective function by replacing the original tensor by its decomposition and then trying to solve it by doing alternating minimization.",
            "But the point of this is that of course sometimes we can get local solutions which could be far from the global global optimum.",
            "So we got some that we're we've been interested in working in doing a study approach for this.",
            "So given that.",
            "Regularizer like we want to use the rank the not."
        ],
        [
            "Well, the first thing the first idea that comes to mind is to replace it by that rational, and that's what we do here.",
            "An in fact this has been done by many several papers before in the context of tensor completion.",
            "Like you can be a senior editor.",
            "In particular, the last last two authors proposed to use."
        ],
        [
            "Alternating direction method of multipliers to minimize.",
            "These this objective function.",
            "So Admn is well known optimization framework which is specially suitable to be applied to this problem 'cause we have in our case a composite regularizer.",
            "And the Adm allows allows us to the couple these arising different terms and optimize them separately.",
            "So I will go quickly through it.",
            "Basically the main idea or the main point is to the couple.",
            "Then the regularizer by introducing new auxiliary variables and then.",
            "Constraining them to be equal to the original set of variables.",
            "An then we can workout the documented Lagrangian and then."
        ],
        [
            "We optimize it.",
            "By following the updates equations.",
            "So in here I just would like to point out that the trace norm is only optimized.",
            "It's only taking into account in the second step.",
            "In the second update equation.",
            "Which basically it involves the computation of the proximity operator of the placement.",
            "OK, so."
        ],
        [
            "So we did some experiments in order to compare both multi multi linear multitasking approaches as well as compared to other normal multitasking approaches in different datasets.",
            "An that's there so that we get.",
            "In both cases we see or.",
            "We put the learning curve so the X axis is the number of training instances available and the Y axis is mean squared error.",
            "So in the first case, that doesn't is arrested asset, which is pretty similar to where the example arrive 74 explained before, and we see that as long as the training set is big enough, the linear approaches gets get better results.",
            "Also, in the second leader set, we have employed solar pane leader set.",
            "Here the objective was to predict different but the degree of activation on different facial action units, different facial muscles.",
            "From different from several subjects.",
            "Ann again.",
            "In this case we see the same same trend.",
            "So.",
            "We can from this paraments gets on also does that we have done that.",
            "In this settings where we have that references by multiple indices, the multilinear approaches get a significant improvement.",
            "But also we see that the nonconvex approach is gets usually better results.",
            "Then they come back."
        ],
        [
            "One so that makes us think about our choice of the convex relaxation of the original problem.",
            "It seems it seems that the natural thing to do is to replace the rank by the trace norm, but perhaps the question would be, is it actually the best commercialization that we can we can get?",
            "And that's the topic of the remaining part of this talk.",
            "So."
        ],
        [
            "To start with it.",
            "So good to recall the definition of envelope, so the convex envelope of function is basically the largest.",
            "The largest convex function which is upper bounded by the original function in a defined predefined set.",
            "So to see an example, we can consider the cardinality of a vector's original function an and then we can consider a set of interest.",
            "When we see here where all the elements of a vector of vectors are constrained to be less or equal and a fixed amount.",
            "And then it can be.",
            "It can be proved that in this case the convex envelope of the cardinality is the L1 norm of the vector over over them.",
            "So here we see a fear.",
            "Anne.",
            "Where the black line is is a renal function.",
            "the Korean ID of 1 dimensional vector.",
            "And then we plot it.",
            "Convex envelopes considering different different set sizes.",
            "So here one important point of this slide is that the smaller the set is, the tighter is the convex envelope.",
            "So going closer to."
        ],
        [
            "Our problem.",
            "In the normal multitask learning problem without any scenario, we would like to learn a matrix W and we would like to constraint the rank.",
            "So previous work, but biface facile.",
            "He solves she solves that.",
            "The comic symbol of their rank is trace Norm.",
            "That's that's why many other papers after this justified that replacement of their rank by the trace norm is the best conversation, and that's in.",
            "This is indeed the best thing we can do.",
            "In the case that we only have that, we only constrain the rank of 1 one matrix.",
            "But it is also important to emphasize that this convex envelope is being calculated taking into account or using the set of the spectral bowl of the metric.",
            "That is, the set of all matrices whose spectral norm is less or equal than them.",
            "So in our case it is a little bit more complicated because we constraint the rank of several matrices that have the same elements.",
            "In common, so the approach that we have been using another people also have been using this.",
            "As I said, the sum of the trace norms, but by doing so we are implicitly assuming.",
            "But do we have the same for all four limitation of that answer?",
            "That means that we are assuming that all the materialization of the tensor have the same spectral norm.",
            "But that is in general that is not true as one might think.",
            "Different medicines have different spectral norms.",
            "So there."
        ],
        [
            "Or we can do better by considering a different property that that would be that that is invariant to different positions.",
            "So I'm on the options that are available on the property that invariant.",
            "We've seen that the previous norms is very appealing.",
            "It is well made for two reasons.",
            "First, as I said, it is invited to different medications cause at the end.",
            "All the medications have the same elements, so therefore means noisy environment.",
            "Also, the second the second property is that the following is a spectral function, so it makes.",
            "They're working out the complex, enveloped feasible task.",
            "In particular because of.",
            "Because of this, calculating the envelope of the rank can be reduced to calculate the convex envelope of the cardinality of the singular values of the original matrix in the in the L2 ball.",
            "So."
        ],
        [
            "Um?",
            "This is the function that this is the convex envelope of the cardinality of a vector in the L2 ball in two dimensions is a representation, so it has a set of.",
            "Nice properties, one of them is for instance that the cardinality of a vector and the comic symbol above it has the same values on the boundary of the ball.",
            "An however, there is the resultant convex envelope is difficult to compute explicitly when the dimension is higher than two.",
            "But nevertheless, it's still feasible to compute its proxamol operator, which is all that we need in order to optimize the whole function by using the alternative method of multipliers.",
            "So."
        ],
        [
            "We have done several experiments on this.",
            "We have been reported in in the paper that we will be presenting NIPS this year.",
            "In all these experiments we have.",
            "Anne.",
            "We tried to do that on tensor completion problems.",
            "So in the first one we tried to.",
            "Build like a video in terms of signing puts a tensor in the second periment we try to build a whole tensor of exam scores in terms of some of some inputs.",
            "Again, we got the learning curve, so they actually the training instances are available.",
            "The inputs of the transfers are available and why is that in this case it would mean square error.",
            "So we can see that the approach that we developed based on based on the comics number 1D norm.",
            "On their L2 ball gets better performance than than the than just using the sum of trace norms, which is kind of the standard method you used so far.",
            "And also it's important to see that.",
            "Well, in the product we have below is.",
            "So they run the computer running time for the learning of the two methods and we see that our method, our method takes longer.",
            "However radio between.",
            "Between the computational time of our method and a trace, norm gets smaller and smaller as the size of the tensor increases.",
            "So for instance, at the beginning, where the answer is only 20 by 20 by 20, the radio is almost 30 times longer for our.",
            "For our approach, for instance, when that answer is 200 by 202 hundred, day rate is smaller than two.",
            "So."
        ],
        [
            "Used to.",
            "Emphasize the main points of of this talk.",
            "Basically, multilinear multitask learning approaches account for this scenario, where tasks can be referenced it or described by multiple indices.",
            "Anne.",
            "In fact, when we have compared them to normal approaches, we see that there is a significant improvement in those cases.",
            "Furthermore also I would like to emphasize as well that.",
            "We here we have considered a tensor of weights, aspect of restaurants and an interview as well.",
            "We can consider different other examples we can consider.",
            "For instance time as an additional mode.",
            "Anne.",
            "Then also the second main point is that in the convex approach we have found out that the trace norm, which is kind of the standard method used so far, is not the best option for tensor regularization, and instead we propose an alternative.",
            "Convex approach which is based on the convex envelope of their ranking.",
            "Therefore it is not.",
            "And if you read this book so that covers pretty much everything I wanted to say.",
            "Thank you for your attention.",
            "I'll be happy to answer any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about the with multi task learning.",
                    "label": 0
                },
                {
                    "sent": "Also very related problem.",
                    "label": 0
                },
                {
                    "sent": "Convex relaxation for tensor completion that we we've been researching and wear with my supervisor's Massimiliano pointed and also now yeah.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm first I will talk about the apparently we're facing and the motivation behind it.",
                    "label": 0
                },
                {
                    "sent": "Then I'll explain the proposed solutions.",
                    "label": 0
                },
                {
                    "sent": "Basically there are two variants.",
                    "label": 0
                },
                {
                    "sent": "When it's a non convex and the other one is convex, then we present.",
                    "label": 0
                },
                {
                    "sent": "I present some experiments on this and then I'll come back to the convex approach to try to make it better than fine.",
                    "label": 1
                },
                {
                    "sent": "I will point out the main points of of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our problem is pretty much cogent analysis where we would like to do is to learn how people value our product in terms of the attributes describing this product or the service.",
                    "label": 1
                },
                {
                    "sent": "So for instance, here we have an example on restaurants.",
                    "label": 0
                },
                {
                    "sent": "So in the left vector that we see here is a vector describing a restaurant.",
                    "label": 0
                },
                {
                    "sent": "So we have several attributes like kind of cuisine, location.",
                    "label": 0
                },
                {
                    "sent": "Price and so on.",
                    "label": 0
                },
                {
                    "sent": "And then we would like to learn one way vector for each subject so that we learn how they ponder the attributes there in order to value arrest each restaurant.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this kind of scenario, in this kind of situation, it makes sense.",
                    "label": 0
                },
                {
                    "sent": "Or is this a reasonable assumption to think that the way that their son commonalities in a way different people ponder the attributes of the restaurants in this case?",
                    "label": 0
                },
                {
                    "sent": "So that's the objective at the end of multi task learning.",
                    "label": 0
                },
                {
                    "sent": "So we can apply to task lighting in order to learn the weight vectors of of the of the subjects.",
                    "label": 0
                },
                {
                    "sent": "So one way of doing this is by, for instance, optimizing this problem that we have there.",
                    "label": 0
                },
                {
                    "sent": "Where we minimize the loss function with respect to the data and also we constraint the methods of weights that we have.",
                    "label": 0
                },
                {
                    "sent": "Here we constrain the rank of it.",
                    "label": 0
                },
                {
                    "sent": "This can be seen at the end as a generalization of this completion or collaborative filtering, where we have rather than having the inputs of the methods, we have several regression problems.",
                    "label": 0
                },
                {
                    "sent": "So this is a modeling works quite well.",
                    "label": 0
                },
                {
                    "sent": "Thing in this case, but what if we want to get also information about different aspects of restaurants?",
                    "label": 0
                },
                {
                    "sent": "For instance, we can also.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And try to predict how different people evaluate the food quality of the restaurant, the service quality, overall quite in general or many other aspects.",
                    "label": 0
                },
                {
                    "sent": "So in that case we have we don't have a metric anymore, but we have this this tensor an in that in this case we want to learn a function for each combination of aspect of a restaurant, an subject.",
                    "label": 0
                },
                {
                    "sent": "So the difference with respect to the previous case.",
                    "label": 0
                },
                {
                    "sent": "When we played multitask learning, is that in the previous case each task was was identified by one index.",
                    "label": 0
                },
                {
                    "sent": "In this case very one identity by one subject, but now its task is identified by more than one.",
                    "label": 0
                },
                {
                    "sent": "In particular.",
                    "label": 0
                },
                {
                    "sent": "In this case, each task is identified by a person an.",
                    "label": 0
                },
                {
                    "sent": "An aspect like this.",
                    "label": 0
                },
                {
                    "sent": "This woman here on foot.",
                    "label": 0
                },
                {
                    "sent": "So if we apply normal without learning approaches, we will be missing information that is very valuable.",
                    "label": 0
                },
                {
                    "sent": "This structural information or the tasks.",
                    "label": 0
                },
                {
                    "sent": "An Furthermore and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By considering this structure, we have the additional advantage about this nice transfer learning property, which is that we can learn some tasks even when no instances having provided as long as there are some other related tasks we do which do have training instances.",
                    "label": 1
                },
                {
                    "sent": "So for instance, here in this example.",
                    "label": 0
                },
                {
                    "sent": "The columns that are in white are tasks with no training instances, but we can still learn the whole tensor because we have instances for or other tasks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this scenario, given that we are dealing with tensors, we apply multilinear Algebra 2 in order to model the whole scenario.",
                    "label": 0
                },
                {
                    "sent": "An in particular we.",
                    "label": 0
                },
                {
                    "sent": "Wisdom in projective is to minimize this function there where W is this set of the tensor of weights that we use.",
                    "label": 1
                },
                {
                    "sent": "So before F is a loss function which relates the tensor with the data.",
                    "label": 0
                },
                {
                    "sent": "And then the second term is a regularizer, which encourages the solution that answer to be as simple as possible, and we measure these simplicity in terms of the rank of the different methods isation of the tensor.",
                    "label": 0
                },
                {
                    "sent": "So what is amortization would be a question.",
                    "label": 0
                },
                {
                    "sent": "So basically this station is a way to rearrange the elements of a tensor in terms of metrics.",
                    "label": 0
                },
                {
                    "sent": "They are in and most dense.",
                    "label": 0
                },
                {
                    "sent": "Are there any different matrices station?",
                    "label": 0
                },
                {
                    "sent": "So for instance in the previous case we have three more tensors tensor, so we have three different matrices stations, one for each mode.",
                    "label": 0
                },
                {
                    "sent": "So in one in the first one for instance, we see that there is one column for each weight I normalization.",
                    "label": 0
                },
                {
                    "sent": "There would be one column for each subject and so on.",
                    "label": 0
                },
                {
                    "sent": "But at the end none of them we have the same elements as in the original tensor, but just rearrange.",
                    "label": 0
                },
                {
                    "sent": "In a different way.",
                    "label": 0
                },
                {
                    "sent": "And then what we want to do is to minimize the rank of these matrices simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, we apply a nonconvex approach based on the type of composition so that added composition can be seen in a way as a extension of single ability decomposition to tensors.",
                    "label": 1
                },
                {
                    "sent": "Where we express a tensor as a multiplication between a core tensor which is usually smaller times a set of matrices, one for each for each mode.",
                    "label": 0
                },
                {
                    "sent": "Then we reform.",
                    "label": 1
                },
                {
                    "sent": "We can reformulate the previous objective function by replacing the original tensor by its decomposition and then trying to solve it by doing alternating minimization.",
                    "label": 0
                },
                {
                    "sent": "But the point of this is that of course sometimes we can get local solutions which could be far from the global global optimum.",
                    "label": 0
                },
                {
                    "sent": "So we got some that we're we've been interested in working in doing a study approach for this.",
                    "label": 0
                },
                {
                    "sent": "So given that.",
                    "label": 0
                },
                {
                    "sent": "Regularizer like we want to use the rank the not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the first thing the first idea that comes to mind is to replace it by that rational, and that's what we do here.",
                    "label": 0
                },
                {
                    "sent": "An in fact this has been done by many several papers before in the context of tensor completion.",
                    "label": 0
                },
                {
                    "sent": "Like you can be a senior editor.",
                    "label": 0
                },
                {
                    "sent": "In particular, the last last two authors proposed to use.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alternating direction method of multipliers to minimize.",
                    "label": 1
                },
                {
                    "sent": "These this objective function.",
                    "label": 0
                },
                {
                    "sent": "So Admn is well known optimization framework which is specially suitable to be applied to this problem 'cause we have in our case a composite regularizer.",
                    "label": 0
                },
                {
                    "sent": "And the Adm allows allows us to the couple these arising different terms and optimize them separately.",
                    "label": 0
                },
                {
                    "sent": "So I will go quickly through it.",
                    "label": 0
                },
                {
                    "sent": "Basically the main idea or the main point is to the couple.",
                    "label": 0
                },
                {
                    "sent": "Then the regularizer by introducing new auxiliary variables and then.",
                    "label": 0
                },
                {
                    "sent": "Constraining them to be equal to the original set of variables.",
                    "label": 0
                },
                {
                    "sent": "An then we can workout the documented Lagrangian and then.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We optimize it.",
                    "label": 0
                },
                {
                    "sent": "By following the updates equations.",
                    "label": 0
                },
                {
                    "sent": "So in here I just would like to point out that the trace norm is only optimized.",
                    "label": 0
                },
                {
                    "sent": "It's only taking into account in the second step.",
                    "label": 0
                },
                {
                    "sent": "In the second update equation.",
                    "label": 0
                },
                {
                    "sent": "Which basically it involves the computation of the proximity operator of the placement.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we did some experiments in order to compare both multi multi linear multitasking approaches as well as compared to other normal multitasking approaches in different datasets.",
                    "label": 0
                },
                {
                    "sent": "An that's there so that we get.",
                    "label": 0
                },
                {
                    "sent": "In both cases we see or.",
                    "label": 0
                },
                {
                    "sent": "We put the learning curve so the X axis is the number of training instances available and the Y axis is mean squared error.",
                    "label": 1
                },
                {
                    "sent": "So in the first case, that doesn't is arrested asset, which is pretty similar to where the example arrive 74 explained before, and we see that as long as the training set is big enough, the linear approaches gets get better results.",
                    "label": 0
                },
                {
                    "sent": "Also, in the second leader set, we have employed solar pane leader set.",
                    "label": 0
                },
                {
                    "sent": "Here the objective was to predict different but the degree of activation on different facial action units, different facial muscles.",
                    "label": 0
                },
                {
                    "sent": "From different from several subjects.",
                    "label": 0
                },
                {
                    "sent": "Ann again.",
                    "label": 0
                },
                {
                    "sent": "In this case we see the same same trend.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can from this paraments gets on also does that we have done that.",
                    "label": 0
                },
                {
                    "sent": "In this settings where we have that references by multiple indices, the multilinear approaches get a significant improvement.",
                    "label": 0
                },
                {
                    "sent": "But also we see that the nonconvex approach is gets usually better results.",
                    "label": 0
                },
                {
                    "sent": "Then they come back.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One so that makes us think about our choice of the convex relaxation of the original problem.",
                    "label": 0
                },
                {
                    "sent": "It seems it seems that the natural thing to do is to replace the rank by the trace norm, but perhaps the question would be, is it actually the best commercialization that we can we can get?",
                    "label": 0
                },
                {
                    "sent": "And that's the topic of the remaining part of this talk.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To start with it.",
                    "label": 0
                },
                {
                    "sent": "So good to recall the definition of envelope, so the convex envelope of function is basically the largest.",
                    "label": 1
                },
                {
                    "sent": "The largest convex function which is upper bounded by the original function in a defined predefined set.",
                    "label": 1
                },
                {
                    "sent": "So to see an example, we can consider the cardinality of a vector's original function an and then we can consider a set of interest.",
                    "label": 0
                },
                {
                    "sent": "When we see here where all the elements of a vector of vectors are constrained to be less or equal and a fixed amount.",
                    "label": 0
                },
                {
                    "sent": "And then it can be.",
                    "label": 0
                },
                {
                    "sent": "It can be proved that in this case the convex envelope of the cardinality is the L1 norm of the vector over over them.",
                    "label": 0
                },
                {
                    "sent": "So here we see a fear.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Where the black line is is a renal function.",
                    "label": 0
                },
                {
                    "sent": "the Korean ID of 1 dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "And then we plot it.",
                    "label": 0
                },
                {
                    "sent": "Convex envelopes considering different different set sizes.",
                    "label": 0
                },
                {
                    "sent": "So here one important point of this slide is that the smaller the set is, the tighter is the convex envelope.",
                    "label": 1
                },
                {
                    "sent": "So going closer to.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our problem.",
                    "label": 0
                },
                {
                    "sent": "In the normal multitask learning problem without any scenario, we would like to learn a matrix W and we would like to constraint the rank.",
                    "label": 1
                },
                {
                    "sent": "So previous work, but biface facile.",
                    "label": 0
                },
                {
                    "sent": "He solves she solves that.",
                    "label": 0
                },
                {
                    "sent": "The comic symbol of their rank is trace Norm.",
                    "label": 0
                },
                {
                    "sent": "That's that's why many other papers after this justified that replacement of their rank by the trace norm is the best conversation, and that's in.",
                    "label": 0
                },
                {
                    "sent": "This is indeed the best thing we can do.",
                    "label": 1
                },
                {
                    "sent": "In the case that we only have that, we only constrain the rank of 1 one matrix.",
                    "label": 1
                },
                {
                    "sent": "But it is also important to emphasize that this convex envelope is being calculated taking into account or using the set of the spectral bowl of the metric.",
                    "label": 1
                },
                {
                    "sent": "That is, the set of all matrices whose spectral norm is less or equal than them.",
                    "label": 0
                },
                {
                    "sent": "So in our case it is a little bit more complicated because we constraint the rank of several matrices that have the same elements.",
                    "label": 0
                },
                {
                    "sent": "In common, so the approach that we have been using another people also have been using this.",
                    "label": 0
                },
                {
                    "sent": "As I said, the sum of the trace norms, but by doing so we are implicitly assuming.",
                    "label": 0
                },
                {
                    "sent": "But do we have the same for all four limitation of that answer?",
                    "label": 0
                },
                {
                    "sent": "That means that we are assuming that all the materialization of the tensor have the same spectral norm.",
                    "label": 0
                },
                {
                    "sent": "But that is in general that is not true as one might think.",
                    "label": 0
                },
                {
                    "sent": "Different medicines have different spectral norms.",
                    "label": 0
                },
                {
                    "sent": "So there.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or we can do better by considering a different property that that would be that that is invariant to different positions.",
                    "label": 0
                },
                {
                    "sent": "So I'm on the options that are available on the property that invariant.",
                    "label": 0
                },
                {
                    "sent": "We've seen that the previous norms is very appealing.",
                    "label": 1
                },
                {
                    "sent": "It is well made for two reasons.",
                    "label": 0
                },
                {
                    "sent": "First, as I said, it is invited to different medications cause at the end.",
                    "label": 0
                },
                {
                    "sent": "All the medications have the same elements, so therefore means noisy environment.",
                    "label": 0
                },
                {
                    "sent": "Also, the second the second property is that the following is a spectral function, so it makes.",
                    "label": 0
                },
                {
                    "sent": "They're working out the complex, enveloped feasible task.",
                    "label": 0
                },
                {
                    "sent": "In particular because of.",
                    "label": 0
                },
                {
                    "sent": "Because of this, calculating the envelope of the rank can be reduced to calculate the convex envelope of the cardinality of the singular values of the original matrix in the in the L2 ball.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is the function that this is the convex envelope of the cardinality of a vector in the L2 ball in two dimensions is a representation, so it has a set of.",
                    "label": 0
                },
                {
                    "sent": "Nice properties, one of them is for instance that the cardinality of a vector and the comic symbol above it has the same values on the boundary of the ball.",
                    "label": 0
                },
                {
                    "sent": "An however, there is the resultant convex envelope is difficult to compute explicitly when the dimension is higher than two.",
                    "label": 1
                },
                {
                    "sent": "But nevertheless, it's still feasible to compute its proxamol operator, which is all that we need in order to optimize the whole function by using the alternative method of multipliers.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have done several experiments on this.",
                    "label": 1
                },
                {
                    "sent": "We have been reported in in the paper that we will be presenting NIPS this year.",
                    "label": 0
                },
                {
                    "sent": "In all these experiments we have.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We tried to do that on tensor completion problems.",
                    "label": 1
                },
                {
                    "sent": "So in the first one we tried to.",
                    "label": 0
                },
                {
                    "sent": "Build like a video in terms of signing puts a tensor in the second periment we try to build a whole tensor of exam scores in terms of some of some inputs.",
                    "label": 0
                },
                {
                    "sent": "Again, we got the learning curve, so they actually the training instances are available.",
                    "label": 0
                },
                {
                    "sent": "The inputs of the transfers are available and why is that in this case it would mean square error.",
                    "label": 0
                },
                {
                    "sent": "So we can see that the approach that we developed based on based on the comics number 1D norm.",
                    "label": 0
                },
                {
                    "sent": "On their L2 ball gets better performance than than the than just using the sum of trace norms, which is kind of the standard method you used so far.",
                    "label": 0
                },
                {
                    "sent": "And also it's important to see that.",
                    "label": 0
                },
                {
                    "sent": "Well, in the product we have below is.",
                    "label": 0
                },
                {
                    "sent": "So they run the computer running time for the learning of the two methods and we see that our method, our method takes longer.",
                    "label": 1
                },
                {
                    "sent": "However radio between.",
                    "label": 0
                },
                {
                    "sent": "Between the computational time of our method and a trace, norm gets smaller and smaller as the size of the tensor increases.",
                    "label": 0
                },
                {
                    "sent": "So for instance, at the beginning, where the answer is only 20 by 20 by 20, the radio is almost 30 times longer for our.",
                    "label": 0
                },
                {
                    "sent": "For our approach, for instance, when that answer is 200 by 202 hundred, day rate is smaller than two.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Used to.",
                    "label": 0
                },
                {
                    "sent": "Emphasize the main points of of this talk.",
                    "label": 0
                },
                {
                    "sent": "Basically, multilinear multitask learning approaches account for this scenario, where tasks can be referenced it or described by multiple indices.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In fact, when we have compared them to normal approaches, we see that there is a significant improvement in those cases.",
                    "label": 0
                },
                {
                    "sent": "Furthermore also I would like to emphasize as well that.",
                    "label": 0
                },
                {
                    "sent": "We here we have considered a tensor of weights, aspect of restaurants and an interview as well.",
                    "label": 0
                },
                {
                    "sent": "We can consider different other examples we can consider.",
                    "label": 0
                },
                {
                    "sent": "For instance time as an additional mode.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 1
                },
                {
                    "sent": "Then also the second main point is that in the convex approach we have found out that the trace norm, which is kind of the standard method used so far, is not the best option for tensor regularization, and instead we propose an alternative.",
                    "label": 1
                },
                {
                    "sent": "Convex approach which is based on the convex envelope of their ranking.",
                    "label": 0
                },
                {
                    "sent": "Therefore it is not.",
                    "label": 0
                },
                {
                    "sent": "And if you read this book so that covers pretty much everything I wanted to say.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "I'll be happy to answer any questions.",
                    "label": 0
                }
            ]
        }
    }
}