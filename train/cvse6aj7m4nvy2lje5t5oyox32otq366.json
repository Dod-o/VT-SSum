{
    "id": "cvse6aj7m4nvy2lje5t5oyox32otq366",
    "title": "Embedded Methods",
    "info": {
        "author": [
            "Isabelle Guyon, Clopinet"
        ],
        "published": "July 5, 2007",
        "recorded": "July 2007",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bootcamp07_guyon_emb/",
    "segmentation": [
        [
            "So welcome to the third lecture.",
            "We are now going to dive into embedded methods.",
            "So in the previous lecture I told you that there are feature selection methods or various levels of complexity.",
            "And they have been classified in different ways, like univariate methods, multivariate methods.",
            "And there is also this classification filter wrapper.",
            "Now we're going to look into a set of methods that also called embedded methods which perform feature selection in the process of learning, and that are efficient both at finding good subsets of features.",
            "And are computationally efficient.",
            "So wait until everybody comes in.",
            "OK, I'll start with the introduction so that we have at the video.",
            "Not an interruption of two minutes.",
            "This lecture is on the embedded methods, which are methods of doing feature selection in the process of learning."
        ],
        [
            "We distinguish between three types of method of picture selection.",
            "Filter methods, Ruffer methods and embedded methods.",
            "As we've seen in the previous lecture.",
            "Filter methods consists in using some ad hoc criterion to come up with some reduced feature, subset them, which is then eventually used for making prediction, but not necessarily.",
            "Rapper misses in the country are very tight to a learning machine.",
            "But learning machines are in that case used as a black box.",
            "Candidates feature subsets are proposed to the Learning Machine and assessed with the performance of the Learning Machine and then eventually you know we're trying new.",
            "A new feature subsets until we satisfy a stopping criteria.",
            "The difference with embedded methods is that embedded methods perform feature selection in the process of learning an output.",
            "Both feature subset and a predictor.",
            "So in order to design a new embedded method, you need to understand the learning algorithm itself.",
            "Whereas in order to design a new wrapper method, you can take any of the shelf package of learning machine and you just have to write the wrap around code.",
            "There is, you know, the search method and the evaluation."
        ],
        [
            "So for filters.",
            "The methods used include the criterion that measures feature relevance and feature subset relevance.",
            "Then as far as search is concerned.",
            "It's trivial.",
            "Usually you just take the ranked features and then you build as I explained here, nested subsets of features following the ordering that you obtain.",
            "And for assessment.",
            "People often use statistical tests like I explained briefly in the previous lecture.",
            "And as a result, filters are usually relatively robust against overfitting.",
            "But they may fail to select the most useful features because the they don't try to optimize the performance of the running machine.",
            "So if the goal really is to optimize performance, this may not be."
        ],
        [
            "The right way to go.",
            "Rappers.",
            "On the contrary, users, a criterion measure of usefulness of the feature subset.",
            "A search the search to space of all possible feature subsets and for assessment they usually use cross validation.",
            "So in principle they can find more useful features useful for making predictions, but they are prone to over."
        ],
        [
            "Fitting.",
            "Embedded methods also use, you know, the.",
            "Usefulness of the feature subset as a criterion.",
            "The difference with rappers is that the search is guided by the learning process.",
            "And cross validation is also used for assessment, so they are similar to rapper, but there are less computationally expensive and they're less prone to."
        ],
        [
            "Overfitting.",
            "So the three ingredients of feature selection are summarized in that graph.",
            "The three regions are.",
            "The criterion of selection that you're using that you have to define first, and it's very domain dependent.",
            "What do you want to do with your features?",
            "Why are you selecting those features in the 1st place?",
            "You need to define a criterion.",
            "Then there is the search method, so you're going to consider multiple feature subsets.",
            "What is your search strategy?",
            "And finally, the last ingredient is assessment.",
            "How are you going to decide whether you've met the criterion that you wanted to meet in the 1st place?",
            "You want to evaluate this criterion on just using just a few examples that you have available for training.",
            "So you may want to use cross validation performance bounds or statistical tests depending.",
            "So you have these 3 dimensions?",
            "What are you using for criterion you could use single feature relevance and there are many criteria of single feature relevance like the signal to noise ratio we saw.",
            "the T test will see many more in the practical session.",
            "Then there is.",
            "There are criteria that use relevance in context like the relief criterion.",
            "I didn't show it to you yet, but in the practical class will play with the relief criterion.",
            "And then there are many features, subsets or relevance criteria and using eventually the performance of the learning machine.",
            "Then for the search, we've seen many options, single feature ranking forward or backward.",
            "Search some Horace tick or stochastic search and exhaustive search.",
            "So in this you know 3 dimensional domain.",
            "I've shaded those that correspond to filters, so filters mostly use.",
            "You know, these types of approach.",
            "And if you go to rappers, you have a pretty different picture right?",
            "From filters to rappers or filters and represent pretty orthogonal and embedded methods make you know yet different types of choices."
        ],
        [
            "Now let me give you some examples.",
            "You've seen that graph you in the previous lecture.",
            "It represents the space of all possible feature subsets.",
            "For a case where you have only four features and each little bubble here represents one state in the search space for all possible subsets of four features.",
            "And the state is denoted by presence or absence of that feature.",
            "One mean presence and 0 means absence.",
            "So if we do forward selection, we start at the root node, which is the empty feature set.",
            "And then.",
            "We are trying to add 1 feature so we can add the first feature or second feature or third feature etc.",
            "And then we'll be assessing the predictive power of this single feature as the first step, and will be selecting the one which is most predictive according to a given quite end.",
            "For example, the cross validation of a given classifier 'cause relation result of again classifier, and then we're going to add a second feature, and we again have you know, multiple possibilities.",
            "And we're going to select the second feature etc etc.",
            "So at the first step we have N possibilities because there aren't possible features we can choose from.",
            "At the second step, we have N -- 1 possibilities etc etc.",
            "So in total we have N plus and minus 1 + 8 -- 2, which I didn't write.",
            "The result here is N * N -- 1 / 2 possibilities.",
            "This is referred to as the sequential Forward selection SFS, or simply forward selection.",
            "What is the difference between?",
            "Forward selection as a rapper Anford selection as an embedded method.",
            "In the case of 1st selection as an embedded method, instead of doing, you know, searching with all these errors the time."
        ],
        [
            "Going here.",
            "You're only looking at the single path.",
            "And how can you do that?",
            "Well, it's because you are guiding your search?",
            "Instead of considering.",
            "All possible features at the first step, you're going to bet.",
            "On a single one, and that's the one that you're going to use.",
            "And to determine it, you're going to use, for example, the gradient of your cost function or something of the sort.",
            "And the in the end, what happens is that you have tried only N. Different feature subsets.",
            "Instead of trying an N -- 1 / 2."
        ],
        [
            "This is an example of a four selection algorithm with Gram Schmidt organization.",
            "Showing you you know how it is possible to only try 1 feature at each step.",
            "So first you select a feature.",
            "That is maximally correlated with your target.",
            "In fact, it has the maximum cosine.",
            "And then for each remaining feature.",
            "You first project.",
            "The feature onto the target.",
            "In the North space of the already existing features.",
            "So you're considering only the remaining informative part of the feature.",
            "Everything that's already explained that is on the projection of the already.",
            "Selected features you ignored.",
            "And so you consider only the residual.",
            "And you compute the cosine.",
            "Of SI with the target in the projection onto the North space, so into the residual.",
            "And then this you select the feature that has maximum cosine with the target in the projection and you eat right.",
            "And you see that in this way you always consider only one feature at each step in the forward selection.",
            "So it turns out that this particular embedded method is optimal with respect to the linear least square predictor."
        ],
        [
            "There are the."
        ],
        [
            "Yes.",
            "In the first step.",
            "So if we have like any features right, we have to look for another one of them, right?",
            "So I mean what you have in your diagram was like in the first, no, you just went to one of them right?",
            "Then look at the other ones.",
            "But here for example in the first one we looked through all of them and take that one which has the larger maximum cosine.",
            "Alright, it's true that you rank all of them.",
            "But you only assess one, the one that you picked.",
            "I mean, the picking is based on your assessment of life.",
            "Is target cosine right?",
            "You're right, it's arguable.",
            "Yeah, it's arguable anyway.",
            "You look for an out them and then pick just one.",
            "Right, that's true.",
            "Next it's true, it's arguable.",
            "Yeah, we'll need to think about it deeply to see how this affects the statistical complexity of the method.",
            "Probably discuss that with you over lunch."
        ],
        [
            "Then here's another example of Fort selection with the decision trees.",
            "In a I think in the introduction lecture I've shown you how tree classifiers work.",
            "Yeah, here you have two features, feature X, One F1 and feature F2 and this is kind of represents no scatter plot of your examples again.",
            "And you can set a threshold of one of the features in order to try to separate as well as possible the two classes.",
            "And in each subset that you obtain.",
            "You have now.",
            "A bigger proportion of examples of one of the particular classes, so here you have a bigger proportion of the widen.",
            "Here a bigger proportion of the black then in the origonal distribution, and progressively you're trying to achieve that to go to smaller and smaller subdivisions of the data in which you're going to have purity.",
            "That is, you're going to have represents only of 1 category.",
            "So if you choose F1 to do the split, then you get at the next stage you get.",
            "You know this box and this box that I'm represented split here and you're going to try to do another split.",
            "For example according to feature F2 in order to get boxes that are pure again etc etc.",
            "But you see that in the process of building your decision tree you performing feature selection because at each stage you have to decide which feature you're going to pick.",
            "In order to best build the new dichotomy.",
            "And the criterion that's often used for such decisions is based on entropy reduction.",
            "So work towards not purity.",
            "And as it turns out, entropy is very much connected to a mutual information, so the criterion used by decision trees is very similar to ranking features of."
        ],
        [
            "Link to a mutual information.",
            "Now instead of doing for selection, you could do back for denominations.",
            "You could start in your state space from a set of the set of all features and working your way towards, eliminating by emulating features towards the empty set.",
            "And again, you can do this in a wrapper setting by trying all possible features at each day."
        ],
        [
            "Age or you can do in an embedded setting in which you try only.",
            "One feature at a time, so maybe I'm just going to argue again that in fact it's a hidden way of trying everything."
        ],
        [
            "Well, we'll see.",
            "For example, one example of that is the backward elimination method, called the RFE, where you start with all features you train a learning machine on.",
            "On the current subset of features, by minimizing Aris functional and then for each remaining feature XI, you estimate without retraining the change in the objective resulting from the removal of XI.",
            "So in a way of course you investigate the features, but you estimate the change in the cost function without actually performing the retraining.",
            "So it's both.",
            "It's a computer.",
            "It's at least a computational saving, and it might be also a statistical saving, so we need to investigate that.",
            "So we remove the feature that results in improving or least degrading J and then we iterate over that.",
            "And so it's an embedded method for support vector machines for kernel methods and for neural net."
        ],
        [
            "Works.",
            "Moving one step further.",
            "We can, instead of considering walking into this space in a discreet way by either adding a feature or removing a feature.",
            "We can transform or searched into a searching in a continuous space.",
            "So here we had this discrete space in which we had a vector of plus and minus 1 + 1.",
            "Means I take this feature or sorry 0 means I don't take the feature, so it's 01.",
            "Instead, you can replace that by a vector of continuous values, saying how much you like that feature, how much you would like to keep it.",
            "And the the advantage of doing that is that now you can perform gradient descent.",
            "Or you can do a lot of other optimization."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So welcome to the third lecture.",
                    "label": 0
                },
                {
                    "sent": "We are now going to dive into embedded methods.",
                    "label": 1
                },
                {
                    "sent": "So in the previous lecture I told you that there are feature selection methods or various levels of complexity.",
                    "label": 0
                },
                {
                    "sent": "And they have been classified in different ways, like univariate methods, multivariate methods.",
                    "label": 0
                },
                {
                    "sent": "And there is also this classification filter wrapper.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to look into a set of methods that also called embedded methods which perform feature selection in the process of learning, and that are efficient both at finding good subsets of features.",
                    "label": 0
                },
                {
                    "sent": "And are computationally efficient.",
                    "label": 0
                },
                {
                    "sent": "So wait until everybody comes in.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll start with the introduction so that we have at the video.",
                    "label": 0
                },
                {
                    "sent": "Not an interruption of two minutes.",
                    "label": 0
                },
                {
                    "sent": "This lecture is on the embedded methods, which are methods of doing feature selection in the process of learning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We distinguish between three types of method of picture selection.",
                    "label": 0
                },
                {
                    "sent": "Filter methods, Ruffer methods and embedded methods.",
                    "label": 1
                },
                {
                    "sent": "As we've seen in the previous lecture.",
                    "label": 0
                },
                {
                    "sent": "Filter methods consists in using some ad hoc criterion to come up with some reduced feature, subset them, which is then eventually used for making prediction, but not necessarily.",
                    "label": 0
                },
                {
                    "sent": "Rapper misses in the country are very tight to a learning machine.",
                    "label": 0
                },
                {
                    "sent": "But learning machines are in that case used as a black box.",
                    "label": 0
                },
                {
                    "sent": "Candidates feature subsets are proposed to the Learning Machine and assessed with the performance of the Learning Machine and then eventually you know we're trying new.",
                    "label": 1
                },
                {
                    "sent": "A new feature subsets until we satisfy a stopping criteria.",
                    "label": 0
                },
                {
                    "sent": "The difference with embedded methods is that embedded methods perform feature selection in the process of learning an output.",
                    "label": 1
                },
                {
                    "sent": "Both feature subset and a predictor.",
                    "label": 0
                },
                {
                    "sent": "So in order to design a new embedded method, you need to understand the learning algorithm itself.",
                    "label": 0
                },
                {
                    "sent": "Whereas in order to design a new wrapper method, you can take any of the shelf package of learning machine and you just have to write the wrap around code.",
                    "label": 0
                },
                {
                    "sent": "There is, you know, the search method and the evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for filters.",
                    "label": 0
                },
                {
                    "sent": "The methods used include the criterion that measures feature relevance and feature subset relevance.",
                    "label": 0
                },
                {
                    "sent": "Then as far as search is concerned.",
                    "label": 0
                },
                {
                    "sent": "It's trivial.",
                    "label": 0
                },
                {
                    "sent": "Usually you just take the ranked features and then you build as I explained here, nested subsets of features following the ordering that you obtain.",
                    "label": 0
                },
                {
                    "sent": "And for assessment.",
                    "label": 0
                },
                {
                    "sent": "People often use statistical tests like I explained briefly in the previous lecture.",
                    "label": 1
                },
                {
                    "sent": "And as a result, filters are usually relatively robust against overfitting.",
                    "label": 1
                },
                {
                    "sent": "But they may fail to select the most useful features because the they don't try to optimize the performance of the running machine.",
                    "label": 1
                },
                {
                    "sent": "So if the goal really is to optimize performance, this may not be.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The right way to go.",
                    "label": 0
                },
                {
                    "sent": "Rappers.",
                    "label": 0
                },
                {
                    "sent": "On the contrary, users, a criterion measure of usefulness of the feature subset.",
                    "label": 1
                },
                {
                    "sent": "A search the search to space of all possible feature subsets and for assessment they usually use cross validation.",
                    "label": 1
                },
                {
                    "sent": "So in principle they can find more useful features useful for making predictions, but they are prone to over.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fitting.",
                    "label": 0
                },
                {
                    "sent": "Embedded methods also use, you know, the.",
                    "label": 1
                },
                {
                    "sent": "Usefulness of the feature subset as a criterion.",
                    "label": 0
                },
                {
                    "sent": "The difference with rappers is that the search is guided by the learning process.",
                    "label": 1
                },
                {
                    "sent": "And cross validation is also used for assessment, so they are similar to rapper, but there are less computationally expensive and they're less prone to.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overfitting.",
                    "label": 0
                },
                {
                    "sent": "So the three ingredients of feature selection are summarized in that graph.",
                    "label": 1
                },
                {
                    "sent": "The three regions are.",
                    "label": 0
                },
                {
                    "sent": "The criterion of selection that you're using that you have to define first, and it's very domain dependent.",
                    "label": 0
                },
                {
                    "sent": "What do you want to do with your features?",
                    "label": 0
                },
                {
                    "sent": "Why are you selecting those features in the 1st place?",
                    "label": 0
                },
                {
                    "sent": "You need to define a criterion.",
                    "label": 0
                },
                {
                    "sent": "Then there is the search method, so you're going to consider multiple feature subsets.",
                    "label": 0
                },
                {
                    "sent": "What is your search strategy?",
                    "label": 0
                },
                {
                    "sent": "And finally, the last ingredient is assessment.",
                    "label": 0
                },
                {
                    "sent": "How are you going to decide whether you've met the criterion that you wanted to meet in the 1st place?",
                    "label": 0
                },
                {
                    "sent": "You want to evaluate this criterion on just using just a few examples that you have available for training.",
                    "label": 0
                },
                {
                    "sent": "So you may want to use cross validation performance bounds or statistical tests depending.",
                    "label": 1
                },
                {
                    "sent": "So you have these 3 dimensions?",
                    "label": 1
                },
                {
                    "sent": "What are you using for criterion you could use single feature relevance and there are many criteria of single feature relevance like the signal to noise ratio we saw.",
                    "label": 0
                },
                {
                    "sent": "the T test will see many more in the practical session.",
                    "label": 1
                },
                {
                    "sent": "Then there is.",
                    "label": 0
                },
                {
                    "sent": "There are criteria that use relevance in context like the relief criterion.",
                    "label": 1
                },
                {
                    "sent": "I didn't show it to you yet, but in the practical class will play with the relief criterion.",
                    "label": 0
                },
                {
                    "sent": "And then there are many features, subsets or relevance criteria and using eventually the performance of the learning machine.",
                    "label": 0
                },
                {
                    "sent": "Then for the search, we've seen many options, single feature ranking forward or backward.",
                    "label": 1
                },
                {
                    "sent": "Search some Horace tick or stochastic search and exhaustive search.",
                    "label": 0
                },
                {
                    "sent": "So in this you know 3 dimensional domain.",
                    "label": 0
                },
                {
                    "sent": "I've shaded those that correspond to filters, so filters mostly use.",
                    "label": 0
                },
                {
                    "sent": "You know, these types of approach.",
                    "label": 0
                },
                {
                    "sent": "And if you go to rappers, you have a pretty different picture right?",
                    "label": 0
                },
                {
                    "sent": "From filters to rappers or filters and represent pretty orthogonal and embedded methods make you know yet different types of choices.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let me give you some examples.",
                    "label": 0
                },
                {
                    "sent": "You've seen that graph you in the previous lecture.",
                    "label": 0
                },
                {
                    "sent": "It represents the space of all possible feature subsets.",
                    "label": 0
                },
                {
                    "sent": "For a case where you have only four features and each little bubble here represents one state in the search space for all possible subsets of four features.",
                    "label": 0
                },
                {
                    "sent": "And the state is denoted by presence or absence of that feature.",
                    "label": 0
                },
                {
                    "sent": "One mean presence and 0 means absence.",
                    "label": 0
                },
                {
                    "sent": "So if we do forward selection, we start at the root node, which is the empty feature set.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We are trying to add 1 feature so we can add the first feature or second feature or third feature etc.",
                    "label": 0
                },
                {
                    "sent": "And then we'll be assessing the predictive power of this single feature as the first step, and will be selecting the one which is most predictive according to a given quite end.",
                    "label": 0
                },
                {
                    "sent": "For example, the cross validation of a given classifier 'cause relation result of again classifier, and then we're going to add a second feature, and we again have you know, multiple possibilities.",
                    "label": 0
                },
                {
                    "sent": "And we're going to select the second feature etc etc.",
                    "label": 0
                },
                {
                    "sent": "So at the first step we have N possibilities because there aren't possible features we can choose from.",
                    "label": 0
                },
                {
                    "sent": "At the second step, we have N -- 1 possibilities etc etc.",
                    "label": 0
                },
                {
                    "sent": "So in total we have N plus and minus 1 + 8 -- 2, which I didn't write.",
                    "label": 0
                },
                {
                    "sent": "The result here is N * N -- 1 / 2 possibilities.",
                    "label": 0
                },
                {
                    "sent": "This is referred to as the sequential Forward selection SFS, or simply forward selection.",
                    "label": 1
                },
                {
                    "sent": "What is the difference between?",
                    "label": 0
                },
                {
                    "sent": "Forward selection as a rapper Anford selection as an embedded method.",
                    "label": 0
                },
                {
                    "sent": "In the case of 1st selection as an embedded method, instead of doing, you know, searching with all these errors the time.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going here.",
                    "label": 0
                },
                {
                    "sent": "You're only looking at the single path.",
                    "label": 0
                },
                {
                    "sent": "And how can you do that?",
                    "label": 0
                },
                {
                    "sent": "Well, it's because you are guiding your search?",
                    "label": 0
                },
                {
                    "sent": "Instead of considering.",
                    "label": 0
                },
                {
                    "sent": "All possible features at the first step, you're going to bet.",
                    "label": 0
                },
                {
                    "sent": "On a single one, and that's the one that you're going to use.",
                    "label": 0
                },
                {
                    "sent": "And to determine it, you're going to use, for example, the gradient of your cost function or something of the sort.",
                    "label": 0
                },
                {
                    "sent": "And the in the end, what happens is that you have tried only N. Different feature subsets.",
                    "label": 0
                },
                {
                    "sent": "Instead of trying an N -- 1 / 2.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is an example of a four selection algorithm with Gram Schmidt organization.",
                    "label": 0
                },
                {
                    "sent": "Showing you you know how it is possible to only try 1 feature at each step.",
                    "label": 0
                },
                {
                    "sent": "So first you select a feature.",
                    "label": 0
                },
                {
                    "sent": "That is maximally correlated with your target.",
                    "label": 0
                },
                {
                    "sent": "In fact, it has the maximum cosine.",
                    "label": 0
                },
                {
                    "sent": "And then for each remaining feature.",
                    "label": 1
                },
                {
                    "sent": "You first project.",
                    "label": 1
                },
                {
                    "sent": "The feature onto the target.",
                    "label": 1
                },
                {
                    "sent": "In the North space of the already existing features.",
                    "label": 0
                },
                {
                    "sent": "So you're considering only the remaining informative part of the feature.",
                    "label": 0
                },
                {
                    "sent": "Everything that's already explained that is on the projection of the already.",
                    "label": 0
                },
                {
                    "sent": "Selected features you ignored.",
                    "label": 0
                },
                {
                    "sent": "And so you consider only the residual.",
                    "label": 1
                },
                {
                    "sent": "And you compute the cosine.",
                    "label": 0
                },
                {
                    "sent": "Of SI with the target in the projection onto the North space, so into the residual.",
                    "label": 1
                },
                {
                    "sent": "And then this you select the feature that has maximum cosine with the target in the projection and you eat right.",
                    "label": 1
                },
                {
                    "sent": "And you see that in this way you always consider only one feature at each step in the forward selection.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that this particular embedded method is optimal with respect to the linear least square predictor.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "In the first step.",
                    "label": 0
                },
                {
                    "sent": "So if we have like any features right, we have to look for another one of them, right?",
                    "label": 0
                },
                {
                    "sent": "So I mean what you have in your diagram was like in the first, no, you just went to one of them right?",
                    "label": 1
                },
                {
                    "sent": "Then look at the other ones.",
                    "label": 0
                },
                {
                    "sent": "But here for example in the first one we looked through all of them and take that one which has the larger maximum cosine.",
                    "label": 1
                },
                {
                    "sent": "Alright, it's true that you rank all of them.",
                    "label": 0
                },
                {
                    "sent": "But you only assess one, the one that you picked.",
                    "label": 0
                },
                {
                    "sent": "I mean, the picking is based on your assessment of life.",
                    "label": 0
                },
                {
                    "sent": "Is target cosine right?",
                    "label": 0
                },
                {
                    "sent": "You're right, it's arguable.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's arguable anyway.",
                    "label": 0
                },
                {
                    "sent": "You look for an out them and then pick just one.",
                    "label": 0
                },
                {
                    "sent": "Right, that's true.",
                    "label": 0
                },
                {
                    "sent": "Next it's true, it's arguable.",
                    "label": 1
                },
                {
                    "sent": "Yeah, we'll need to think about it deeply to see how this affects the statistical complexity of the method.",
                    "label": 0
                },
                {
                    "sent": "Probably discuss that with you over lunch.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then here's another example of Fort selection with the decision trees.",
                    "label": 0
                },
                {
                    "sent": "In a I think in the introduction lecture I've shown you how tree classifiers work.",
                    "label": 1
                },
                {
                    "sent": "Yeah, here you have two features, feature X, One F1 and feature F2 and this is kind of represents no scatter plot of your examples again.",
                    "label": 0
                },
                {
                    "sent": "And you can set a threshold of one of the features in order to try to separate as well as possible the two classes.",
                    "label": 0
                },
                {
                    "sent": "And in each subset that you obtain.",
                    "label": 0
                },
                {
                    "sent": "You have now.",
                    "label": 0
                },
                {
                    "sent": "A bigger proportion of examples of one of the particular classes, so here you have a bigger proportion of the widen.",
                    "label": 0
                },
                {
                    "sent": "Here a bigger proportion of the black then in the origonal distribution, and progressively you're trying to achieve that to go to smaller and smaller subdivisions of the data in which you're going to have purity.",
                    "label": 0
                },
                {
                    "sent": "That is, you're going to have represents only of 1 category.",
                    "label": 1
                },
                {
                    "sent": "So if you choose F1 to do the split, then you get at the next stage you get.",
                    "label": 0
                },
                {
                    "sent": "You know this box and this box that I'm represented split here and you're going to try to do another split.",
                    "label": 0
                },
                {
                    "sent": "For example according to feature F2 in order to get boxes that are pure again etc etc.",
                    "label": 0
                },
                {
                    "sent": "But you see that in the process of building your decision tree you performing feature selection because at each stage you have to decide which feature you're going to pick.",
                    "label": 0
                },
                {
                    "sent": "In order to best build the new dichotomy.",
                    "label": 0
                },
                {
                    "sent": "And the criterion that's often used for such decisions is based on entropy reduction.",
                    "label": 0
                },
                {
                    "sent": "So work towards not purity.",
                    "label": 1
                },
                {
                    "sent": "And as it turns out, entropy is very much connected to a mutual information, so the criterion used by decision trees is very similar to ranking features of.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Link to a mutual information.",
                    "label": 0
                },
                {
                    "sent": "Now instead of doing for selection, you could do back for denominations.",
                    "label": 0
                },
                {
                    "sent": "You could start in your state space from a set of the set of all features and working your way towards, eliminating by emulating features towards the empty set.",
                    "label": 0
                },
                {
                    "sent": "And again, you can do this in a wrapper setting by trying all possible features at each day.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Age or you can do in an embedded setting in which you try only.",
                    "label": 0
                },
                {
                    "sent": "One feature at a time, so maybe I'm just going to argue again that in fact it's a hidden way of trying everything.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we'll see.",
                    "label": 0
                },
                {
                    "sent": "For example, one example of that is the backward elimination method, called the RFE, where you start with all features you train a learning machine on.",
                    "label": 0
                },
                {
                    "sent": "On the current subset of features, by minimizing Aris functional and then for each remaining feature XI, you estimate without retraining the change in the objective resulting from the removal of XI.",
                    "label": 1
                },
                {
                    "sent": "So in a way of course you investigate the features, but you estimate the change in the cost function without actually performing the retraining.",
                    "label": 0
                },
                {
                    "sent": "So it's both.",
                    "label": 0
                },
                {
                    "sent": "It's a computer.",
                    "label": 0
                },
                {
                    "sent": "It's at least a computational saving, and it might be also a statistical saving, so we need to investigate that.",
                    "label": 1
                },
                {
                    "sent": "So we remove the feature that results in improving or least degrading J and then we iterate over that.",
                    "label": 0
                },
                {
                    "sent": "And so it's an embedded method for support vector machines for kernel methods and for neural net.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Works.",
                    "label": 0
                },
                {
                    "sent": "Moving one step further.",
                    "label": 0
                },
                {
                    "sent": "We can, instead of considering walking into this space in a discreet way by either adding a feature or removing a feature.",
                    "label": 0
                },
                {
                    "sent": "We can transform or searched into a searching in a continuous space.",
                    "label": 1
                },
                {
                    "sent": "So here we had this discrete space in which we had a vector of plus and minus 1 + 1.",
                    "label": 0
                },
                {
                    "sent": "Means I take this feature or sorry 0 means I don't take the feature, so it's 01.",
                    "label": 0
                },
                {
                    "sent": "Instead, you can replace that by a vector of continuous values, saying how much you like that feature, how much you would like to keep it.",
                    "label": 0
                },
                {
                    "sent": "And the the advantage of doing that is that now you can perform gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Or you can do a lot of other optimization.",
                    "label": 0
                }
            ]
        }
    }
}