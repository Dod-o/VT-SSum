{
    "id": "6rizyx4z2sliojnn37csl54kqfj2bzeg",
    "title": "Gaussian Variances and Large Scale Bayesian Inference",
    "info": {
        "author": [
            "Matthias W. Seeger, Laboratory for Probabilistic Machine Learning, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science",
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_seeger_bayesian_inference/",
    "segmentation": [
        [
            "Thanks very much to the organizers for giving the opportunity here.",
            "So if numerical analysis is inference then I would like to point out that inference is also very much numerical mathematics, or at least it's very much driven by numerical mathematics, especially if you want to do this at very large scale.",
            "So, for example, I'm going to show you some kind of example where you need to do sampling optimization, but not not over the real line, but over something like an image.",
            "And this is possible only because we can.",
            "Draw a numerical mathematics algorithms and possibly have to adapt them a little bit, which is why I mean personally.",
            "For me this interface strengthening this interface would be really immensely important.",
            "Apart from that, I'm going to keep it really on the very light side.",
            "First, talk some very simple examples how this interface actually works.",
            "In order to get."
        ],
        [
            "Done, just give you 2 motivating examples here.",
            "So this is an image reconstruction pipeline, modern one, which you could use in order to reconstruct mymar images or CT images or whatever we call this inverse problems.",
            "So you can think about this as a statistician.",
            "You would think about this as something that there are certain ideal image somewhere which is a latent variable, very high dimensional one, and you acquire data from that which is usually linear data.",
            "So linear projections in this case you would acquire Fourier coefficients which are arranged in some spatial pattern which you control by the machine, and it's kind of called the sequence.",
            "Or the design by statisticians so you know the design.",
            "You know the measurements up to noise you want to guess what that image was, and you use all your prior information in order to do that very well.",
            "And people have made a lot of progress in the last.",
            "Depending on how you count in the last 15 years and.",
            "And they're becoming very, very good at doing this.",
            "I give you the measurements.",
            "I give you the design.",
            "What's the image right?",
            "We have very fast algorithms.",
            "They use L1 and so on.",
            "And this is basic optimization problems, so I'm interested in a little bit of a different problem."
        ],
        [
            "So if you, I mean nothing really tells you what the design should be, right?",
            "So you can actually program this.",
            "This is no problem at all.",
            "This is like bread and butter.",
            "For more people to program these designs, there's also there are certain costs associated with these designs.",
            "Roughly, each of these columns cost you the same time, so it's linear.",
            "It grows linear in the number of columns.",
            "The less you assemble, the faster you are, and scan time is the major limiting factor in MRI today.",
            "So so the problem would now be how to optimize that design right?",
            "And how to do this in a data driven way."
        ],
        [
            "And each so one way how you could do this is by using Bayesian inference over something like the whole image.",
            "So this is a little bit how it looks like.",
            "An idea is very, very simple.",
            "Suppose you could compute something like the posterior over an image model.",
            "Given these these measurements, you would get a posterior distribution over this bitmap, and then you could simply just ask that posterior distribution where it spreads to mostly align your next measurement along there.",
            "And this is a bit like a kind of a sampling optimization, and I'm not sure if I would call this quadrature, but what your goal here is to shrink to posterior mass as quickly as possible to some point, which would be a very good image, right?",
            "And this you can do this sequentially if you don't like that, then you can also do this little bit more clever, but in this work we just did discretely sequentially, But the really hard part here is to get that posterior and to get these design scores and to get them computed in a kind of a finite time, so that you.",
            "Can iterate this and get it done with.",
            "Obviously it is.",
            "You would want to do this in real time, but."
        ],
        [
            "There yet another example which is a bit simpler, but.",
            "Is a blind deconvolution problems.",
            "So for example if you have a kind of.",
            "For whatever reason, you have an image like this taken with your camera and you will want to clean it up.",
            "You will have to kind of guess what the block on was that gave rise to this blurry image here, and if you do not have fancy like motion sensors then you have to guess that from the data itself, right?",
            "So this is a difficult inverse problem where you not only have to guess what the clean image was, but also what the Colonel was and people figured out over the years that a very good way of doing this is to treat it as a Bayesian problem, integrate over the latent image and then you estimate over the kernel.",
            "And this is like a little bit like an EM algorithm, right?",
            "So this is almost kind of the optimal way of doing this, given that your model is good because it's maximal acute estimation, right?",
            "Again, you need to kind of somehow integrate over that images, which is a very high dimensional.",
            "Signal backed by a model that's not really Gaussian."
        ],
        [
            "So this is a little bit how such a model would look like.",
            "This is really just state of the art image model.",
            "You have to likelihood here, which you can assume is Gaussian Gaussian noise.",
            "Here is to design and then you have a prior that's not Gaussian.",
            "That's very important because images are simply just not Gaussian.",
            "If you want to have a one line explanation why all of this compressed sensing works, it's exactly that images are not Gaussian.",
            "They're probably more like something like Laplacian.",
            "So you can put Laplacian potentials on something like wavelet coefficients image gradients.",
            "There's a lot of theory behind wavelets, but the main.",
            "Purpose for us is split that you can simply compute them very rapidly.",
            "And so then, so how?",
            "So what is kind of configurable here?",
            "While you have a design matrix that's given by the application?",
            "And then you have this other thing which is given by by whatever you would like to choose.",
            "You need to choose something that you can multiply with quickly and so here are a few examples.",
            "So if you want to do denoising, then the X would be a diagonal matrix.",
            "If you want to deconvolution it's a convolution.",
            "If you want to, Emery Construction is a Fourier transform matrix.",
            "So now what we need to do is to do inference in this model.",
            "That's the posterior.",
            "If we cannot do this exactly, we want to approximate it so that we get still still get the results that we want.",
            "And again, this is a very high dimensional thing.",
            "You think about something like 100,000 to a million pixels, and if it's a, it's MRI.",
            "It's twice that because it's complex."
        ],
        [
            "And then you can run this model with Laplace potentials and other things as well."
        ],
        [
            "It's fairly general, so what's the very rough idea how to how we get this done computationally we use something called variational inference.",
            "Very simple idea.",
            "You have a non Gaussian integral to do you have a non Gaussian distribution?",
            "Want to estimate?",
            "You want to approximate and you do this by repeatedly fitting it with the Gaussian.",
            "OK so you use a Gaussian computations in order to get your job done on the non Gaussian side and this is very much very good analogy that you might think about this if you don't know much about inferences.",
            "The Newton algorithm fits quadratics to convex functions, so you're not minimizing a convex function, you're not minimizing a quadratic, but you're using the fact that quadratics are easy to minimize in order to get the job done on the other function right?",
            "Exactly the same idea is striving.",
            "Variational inference.",
            "Just."
        ],
        [
            "Using the fact that we can deal with Gaussians well.",
            "So very, very, very briefly, how does this work?",
            "You kind of find an upper bound to kind of this key function here, which is the negative log partition function, which is the moment generating function for the whole thing, so that that function encodes the moments that we want.",
            "So if you get a good approximation, which in this case is abound, then you can hope that by taking derivatives of that approximation you get good approximation to these moments.",
            "In this case we get a bound fairly easily by.",
            "I'm not really getting into it, but it's a very simple idea, you just.",
            "Lower bound is non Gaussians by Gaussians and then you plug them into the integral.",
            "You get a bound.",
            "And then you optimize over the parameters that describe how broad discussions are, and that's in fact a convex optimization problem.",
            "For this Laplace potentials.",
            "It's a big convex optimization problem and kind of the rest of the talk.",
            "I want to talk a little bit about how you can at least approximately solve this."
        ],
        [
            "So let me first give you 2 slides.",
            "Explanation why many inference methods that we looked at before actually are not very fast and the reason is this they are kind of.",
            "In order to do inference you have to do certain things, possibly repeatedly.",
            "You have to do things which are very expensive if to compute Gaussian variances and for numerical mathematicians it's just you need to compute the diagonal of an of an inverse, right?",
            "This is not something you usually find in American math books, but that's what you have to do.",
            "Then you have to do something which you will find in every book and that's solving linear systems or minimizing.",
            "Traffic functions.",
            "For example, the Newton algorithm has to do this all the time and then you have to do things you don't even want to think about like linear time, things OK.",
            "So many algorithms are not as fast as they could be.",
            "Is that they kind of."
        ],
        [
            "Do this in a random or in a cyclic ordering so you do some very simple thing.",
            "Then you compute this very expensive variant system.",
            "You solve a system and you just do this in the order and this doesn't.",
            "If you look at it like that, it doesn't really sound very possible right?",
            "You should do this very often.",
            "Then you should do this kind of now and then and you should do this as little as possible, right?",
            "Only if you really have to do it.",
            "If you cannot do anything else that would only require these guys anymore, right?",
            "And I did.",
            "This is kind of the idea behind the algorithm that we came up with is that we managed to reorder computations so that this actually happens, and it's still a converge."
        ],
        [
            "Algorithm and the algorithm is using a little bit of convex convex convexity, so what's the idea?",
            "So if you look at the criterion, you need to optimize, then you quickly realize that it has at least one part, which is really tricky.",
            "So that part leads to the fact that if you want to compute the gradient, you have to do a difficult computation, but that part is not only."
        ],
        [
            "Difficult and coupled.",
            "It's also concave.",
            "And so you can get rid of the coupling by replacing the concave part by by a linear part, right?",
            "It's just eventuality.",
            "So instead of the concave part you have to add linear plane in there.",
            "The refitting might take you some time, but once you've done that, you can optimize, minimize the upper bound, much, much more easily and much more quickly.",
            "If you do this, you plug it all together.",
            "So this set is now the kind of the normal vector of displaying."
        ],
        [
            "And if you fix that for the moment, just fitted your plane, then you need to solve an optimization problem that everybody in optimization knows very well how to solve.",
            "This is this is like one of the oldest optimization problem.",
            "If this is a quadratic part which is not in this case, then this is least squares estimation.",
            "And if this is kind of a decoupled non quadratic part then we have very good algorithms today to do this right simply because almost everybody in scientific computing needs this.",
            "So this is a penalized least squares problem and there you have lots of algorithms that can do that.",
            "Very efficiently and you don't need to compute these very heavy."
        ],
        [
            "Things so you need to compute them when you refit, abound, or to refit.",
            "This tangent, of course you have to compute its normal vector, and that is exactly the Gaussian variances problem.",
            "So there you have to do it, but you only have to do it rarely, so the whole idea behind that algorithm is that your basic."
        ],
        [
            "Reordering things so that the hardest part you have to do is as least often as possible.",
            "So in particular, you need to compute the variances much less frequently than you need to compute something like Gaussian means.",
            "Gaussian mean computations are solving linear systems.",
            "OK, so that's the main idea of this algorithm, but now in the rest of the talk, I really want US OS the numerical method and American methods going to hopefully help us to get a handle on these variances."
        ],
        [
            "It's not only that I need to run my algorithm, I also once I've run my algorithm converged.",
            "I need to now plan the next step.",
            "I need to make the next measurement, and this looks a little bit like this.",
            "This this really looks more complicated than it is.",
            "You need to compute an information gain, which very intuitively is just the amount of information.",
            "The amount of uncertainty that you get rid of if you make a new measurement.",
            "Uncertainties measured by entropies, that's before you did the measurement.",
            "That's after you."
        ],
        [
            "Approximate this a little bit by basically replacing.",
            "Of course the non Gaussians by Gaussians.",
            "Once you've done that, you exploit a very pleasant property of the Gaussian that it doesn't.",
            "The covariance doesn't depend on what you condition on and you simply get a score like this.",
            "So you again need to and if you look at this closely then again it looks exactly like the variance is, because here you have the covariance and then it's projected along your potential new measurements.",
            "So again, you need to compute these variances here, right?"
        ],
        [
            "So that's really a bit of a problem, right?",
            "If you say that this is a scalable approach, then you need to address all of them.",
            "All of these things, and they really.",
            "The problem here is the covariance matrix.",
            "It's a very, very big matrix which has kind of nice parts here to it, but they're certainly dense parts, so there's nothing like very sparse here.",
            "And then you need to invert it, which of course we want to can even store this metrics.",
            "And then you need to kind of compute these always the same like left right multiplication and then diagonal.",
            "And this kind of the same thing, it's like.",
            "Block diagonal block diagonal variant of the same thing.",
            "So this is what we need to do.",
            "So how do we get?",
            "How do we get a handle on these variances?"
        ],
        [
            "First of all, they are really much more difficult to get the Gaussian means, which is probably also why you really won't find them in many textbooks and American math.",
            "All the means you just have to solve one linear system right for all of them.",
            "All the variances were native, baseline way would be to solve 1 system for each variance component.",
            "Pretty much related to what you need to do in Gaussian processes.",
            "You can get all the means by 1 system.",
            "You can compute that before hand, but if you want to have variances you have to redo computations all the time, so that is obviously not an option here.",
            "If N is 1,000,000 or 100,000 and we cannot do this right, we need to get by with solving and maybe 50 or one."
        ],
        [
            "Systems, so you might say, why don't you use loopy BP?",
            "While the situation for loopy BP is exactly how you would expect so if it converges, the means are exact.",
            "So you have them, whether that's the best way of getting them is another is something else, but the variances are completely wrong, right?",
            "So and so.",
            "Basically people even argue that the major part of that computation is left undone by loopy and there's the right now, no way to really correct this in order to get the variance is fixed there certain proposals, but they only work in special cases."
        ],
        [
            "Then of course there are tractable cases.",
            "If you have a tree, everything simple.",
            "If you have a sparse matrix, possibly it could be factorized, and then you can get the variances by by one more step.",
            "All of this doesn't happen, unfortunately.",
            "In our inverse problems.",
            "So we need to do something else.",
            "We need to use.",
            "Actually, some pretty pretty."
        ],
        [
            "Crude approximations and let me just go quickly over how two of them look like, so this is not something that only we need.",
            "This is something that many other people need in physics and in even in piddies people start to now worry about uncertainties which they didn't do for a long time.",
            "And of course, in remote sensing, so here's some."
        ],
        [
            "Very very simple idea how how one could estimate these variances.",
            "Ideas to plug in to plug in a low rank approximation.",
            "So suppose you could get a matrix that would have this funny property.",
            "It's a very slim matrix so long and slim.",
            "And this low rank matrix is close to identity, right?",
            "Suppose you had something like that.",
            "Of course there's nothing like that, but maybe we can approximate something like this.",
            "Then you can plug it in here somewhere here.",
            "Plug it in here.",
            "Then you have to lowering.",
            "Here you sum over lowering here.",
            "This is not such a big number and then you just need to solve these systems here.",
            "So that's so that's nice."
        ],
        [
            "Now we only need to solve linear systems instead of an obvious question.",
            "How do I choose?",
            "That funny matrix here, possibly even dependent on what that matrix A is.",
            "How do I choose it so that I get so that this is not too bad?"
        ],
        [
            "So one idea, one general idea is to use these hotema matrices.",
            "It's kind of an idea that comes from coding theory.",
            "Hadamar matrices are something that you can really torture you.",
            "Numerical mathematics algorithms with they are.",
            "They are also orthogonal matrices, but they're only plus minus ones, and then you can just pick columns."
        ],
        [
            "From these matrices, maybe the first L columns and then you normalize them so the diagonal is really the identity and then you use that one."
        ],
        [
            "And that kind of works not so badly.",
            "It's a deterministic estimator.",
            "So no matter what that a is, you always use the same one, and it's kind of intuition is to maximize the smallest angle between any pairs of rows of V, which is kind of something that of course for the identity would be true, but for a low rank matrix you cannot really get.",
            "So that's a nice and simple thing that's simple."
        ],
        [
            "Another idea would be just let's use Monte Carlo estimation.",
            "Let's just sample from that Gaussian, and then let's just estimate the variances in the usual way.",
            "So if I could sample from that Gaussian.",
            "In a fairly efficient way, then I could just use these samples in order to estimate these variants that just plug my samples in here, right?",
            "Multiply with B squared and then I do my Monte Carlo average.",
            "So now how can I?",
            "How can I get?",
            "How can I get these samples?"
        ],
        [
            "Well, there's a simple trick here.",
            "It's called perturbing map.",
            "So the idea is that you first sample random vector from a very funny distribution, which is a Gaussian that has the inverse covariance matrix as the covariance matrix and that is easy to do if you look at this metrics then you see that it has a lot of structure.",
            "It's kind of always matrix times matrix transpose and then some stuff in the middle.",
            "So you just sample 2 random vectors, Gaussian O mean, you plug them in, you multiply them and you get the right result.",
            "So this doesn't cost you anything.",
            "That's one of these peanuts.",
            "Computations."
        ],
        [
            "And then you solve systems and now we need to solve 1 system for every sample that we get.",
            "And this is now very easy.",
            "Right now you solve this system and then you can show that the covariance left right multiplication with a inverse.",
            "That was a.",
            "So we have a inverse.",
            "So that's the part urban map estimator, and it's just a direct Monte Carlo.",
            "No Markov chain here.",
            "It's an exact Monte Carlo estimator from this Gaussian.",
            "And then you do this as often as you can afford.",
            "Which depends on your hardware, maybe?",
            "I don't know.",
            "Maybe 50 or 100 times.",
            "The most we are sometimes only using 30 and then you estimate these variances."
        ],
        [
            "That's the nice thing about this estimator is that it's unbiased, it's an unbiased estimator, so that's very, very easy to see.",
            "So in on average, over each component is correct.",
            "So especially if you use these variances in a kind of a way in which there are some in which they are linearly combined, that estimators are very good estimator.",
            "But each component has a substantial noise on it for sure, don't have a plot here, but I uploaded it once and there's a lot of noise.",
            "But if you average across components, it's a pretty good estimator.",
            "They are."
        ],
        [
            "Open questions which, if anybody could give me a hint?",
            "I would be very happy.",
            "Is well.",
            "First of all, that's probably easy to answer.",
            "Is that actually the best distribution that I sample from independent Lee?",
            "Is that really the Gaussian?",
            "Because in the end I just need these variances.",
            "Is it really optimal to sample from that Gaussian or color sample from something else?",
            "And then another one, which would be very important is could I use dependent samples in order to reduce the variances and still stay unbiased?",
            "Or kind of bias variance tradeoff?",
            "I mean I'm happy to be a little bit biased, but I want to reduce the variance here, so if anybody has an idea then please tell me about it.",
            "Good so I'm almost done.",
            "So this is basically what we're using right now in order to get these variances.",
            "Of course you can also just say yes.",
            "How large so in the MRI example it's it's five.",
            "OK so you have 256 by 256 images.",
            "Now you have to multiply it with two because it's complex.",
            "So it's about five 512 * 250, so about 120,000 by 120,000.",
            "And that is still on the small side because I actually MRI is usually 3 dimensional and then you're in the many many millions.",
            "So, but this is actually then again small for real numerical math.",
            "People who do whatever Geo statistics where you have billions and whatever variables.",
            "So the important thing here is that it absolutely has to scale.",
            "Everything has to scale linearly or or N log N, which is kind of the Fourier transform.",
            "Everything beyond that is impossible.",
            "In practice.",
            "So."
        ],
        [
            "So.",
            "Very obvious idea, but it's kind of interesting to explore hybrid approaches which say OK, so maybe I can.",
            "Maybe maybe I have a big mess in terms of covariance, but there is some structure, some level, so then we can actually use the fact that we know about structure in graphical models in order to get a hybrid algorithm.",
            "That kind of more or less makes use of these numerical mathematics mathematics approaches only when we really need them right.",
            "And here's a very simple example.",
            "In MRI you have many slices which just lie next to each other.",
            "In 3D MRI you also have that it's even bigger, so it's just think about these slices or being next to each other.",
            "Of course there statistically very heavily dependent on each other, so you would want to have something like a sequence model here between the slices and then you see that if I now want to do, you know the variances over all these this huge cube, then you can just exploit the fact that your model only links neighboring slices in order to get kind of a hybrid approach, which would come."
        ],
        [
            "Bind something like common message passing.",
            "So you're passing your Gaussian messages between these slices, maybe back and forth.",
            "And you would then use the numerical mathematics primitives in order to approximate these messages in the common game matrices and so on, right?",
            "And this is of course not again something that we have come up with.",
            "This is actually used in geostatistics.",
            "In weather simulations are quite heavily, they called this approximate.",
            "Basically, approximate common smoothers, which are based on low rank decompositions of the of the relevant matrices there.",
            "9."
        ],
        [
            "So this is my talk, so I think that.",
            "Kind of goes the wrong way around in terms of the workshop, but I think it's also very important in order to kind of maybe also in order to get the numerical mathematicians interested.",
            "We also have to present them our problems, and we have to present the problems that we really need them to that we cannot just run with what Matlab Pass, but we need to understand a bit more.",
            "So in this case I'm kind of advocating to use reductions to computational state of the art instead of just cooking up some message passing thing that might not be very fast.",
            "Let's reduce our computations to the state of the art of the people who really know how to solve things at very large scales.",
            "So that usually involves convex."
        ],
        [
            "And and I think we know a lot about this in the field now, because we've done this interface since many, many years.",
            "For example, penalized least squares map estimation and so on.",
            "Possibly more difficult things by."
        ],
        [
            "It also involves numerical mathematics beyond just calling Matlab things.",
            "In my case, it's the Gaussian cover, and since we have a lot of work in machine learning, doing low rank approximations, we need to solve linear systems.",
            "We need to deal with a major headache which is called preconditioning, parallelization, and so on, and quadrature of course in this case I don't need quadrature, but usually in variational inference you also need quadrature."
        ],
        [
            "And I think maybe what this workshop can contribute to is to work on the interface beyond between machine learning and numerical mathematics, which is in provable.",
            "I think if you compare it to the interface that we have with convex optimization, it really, really have to go beyond MATLAB, because Matlab is only doing something that more or less serves other people like like physicists and so on, and signal processing people.",
            "So we need to really understand what our requirements and what are the limitations of.",
            "Of the layer below.",
            "I was already been mentioned that the standards of these two fields are very, very different and we can certainly not expect that our standards are somehow even taken seriously by the numerical math people.",
            "But they also have to understand that we have to do deal with much more well defined problems and much in some sense, much bigger problems sometimes.",
            "So the goals are also different than we.",
            "These two fields really have to come together, and I think one thing we should target for is that we become serious numerical math customers.",
            "So in the moment all they're doing is.",
            "Maybe I'm being unfair, but I always hear PZ, PZ, PZ, PZ, PZ, PZ and that's fine.",
            "PDS are very important but there's other important things with large scale data analysis and with Bayesian inference and we need to kind of make sure that this is understood that we also have needs which are not trust PDS.",
            "Sometimes they are, but sometimes they're not.",
            "Sometimes we need to solve systems that have a more complicated structure in piddies.",
            "OK, that's my talk.",
            "Thanks very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks very much to the organizers for giving the opportunity here.",
                    "label": 0
                },
                {
                    "sent": "So if numerical analysis is inference then I would like to point out that inference is also very much numerical mathematics, or at least it's very much driven by numerical mathematics, especially if you want to do this at very large scale.",
                    "label": 0
                },
                {
                    "sent": "So, for example, I'm going to show you some kind of example where you need to do sampling optimization, but not not over the real line, but over something like an image.",
                    "label": 0
                },
                {
                    "sent": "And this is possible only because we can.",
                    "label": 0
                },
                {
                    "sent": "Draw a numerical mathematics algorithms and possibly have to adapt them a little bit, which is why I mean personally.",
                    "label": 0
                },
                {
                    "sent": "For me this interface strengthening this interface would be really immensely important.",
                    "label": 0
                },
                {
                    "sent": "Apart from that, I'm going to keep it really on the very light side.",
                    "label": 0
                },
                {
                    "sent": "First, talk some very simple examples how this interface actually works.",
                    "label": 0
                },
                {
                    "sent": "In order to get.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Done, just give you 2 motivating examples here.",
                    "label": 0
                },
                {
                    "sent": "So this is an image reconstruction pipeline, modern one, which you could use in order to reconstruct mymar images or CT images or whatever we call this inverse problems.",
                    "label": 0
                },
                {
                    "sent": "So you can think about this as a statistician.",
                    "label": 0
                },
                {
                    "sent": "You would think about this as something that there are certain ideal image somewhere which is a latent variable, very high dimensional one, and you acquire data from that which is usually linear data.",
                    "label": 0
                },
                {
                    "sent": "So linear projections in this case you would acquire Fourier coefficients which are arranged in some spatial pattern which you control by the machine, and it's kind of called the sequence.",
                    "label": 0
                },
                {
                    "sent": "Or the design by statisticians so you know the design.",
                    "label": 0
                },
                {
                    "sent": "You know the measurements up to noise you want to guess what that image was, and you use all your prior information in order to do that very well.",
                    "label": 0
                },
                {
                    "sent": "And people have made a lot of progress in the last.",
                    "label": 0
                },
                {
                    "sent": "Depending on how you count in the last 15 years and.",
                    "label": 0
                },
                {
                    "sent": "And they're becoming very, very good at doing this.",
                    "label": 0
                },
                {
                    "sent": "I give you the measurements.",
                    "label": 0
                },
                {
                    "sent": "I give you the design.",
                    "label": 0
                },
                {
                    "sent": "What's the image right?",
                    "label": 0
                },
                {
                    "sent": "We have very fast algorithms.",
                    "label": 0
                },
                {
                    "sent": "They use L1 and so on.",
                    "label": 0
                },
                {
                    "sent": "And this is basic optimization problems, so I'm interested in a little bit of a different problem.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you, I mean nothing really tells you what the design should be, right?",
                    "label": 0
                },
                {
                    "sent": "So you can actually program this.",
                    "label": 0
                },
                {
                    "sent": "This is no problem at all.",
                    "label": 0
                },
                {
                    "sent": "This is like bread and butter.",
                    "label": 0
                },
                {
                    "sent": "For more people to program these designs, there's also there are certain costs associated with these designs.",
                    "label": 0
                },
                {
                    "sent": "Roughly, each of these columns cost you the same time, so it's linear.",
                    "label": 0
                },
                {
                    "sent": "It grows linear in the number of columns.",
                    "label": 0
                },
                {
                    "sent": "The less you assemble, the faster you are, and scan time is the major limiting factor in MRI today.",
                    "label": 0
                },
                {
                    "sent": "So so the problem would now be how to optimize that design right?",
                    "label": 0
                },
                {
                    "sent": "And how to do this in a data driven way.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And each so one way how you could do this is by using Bayesian inference over something like the whole image.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit how it looks like.",
                    "label": 0
                },
                {
                    "sent": "An idea is very, very simple.",
                    "label": 0
                },
                {
                    "sent": "Suppose you could compute something like the posterior over an image model.",
                    "label": 0
                },
                {
                    "sent": "Given these these measurements, you would get a posterior distribution over this bitmap, and then you could simply just ask that posterior distribution where it spreads to mostly align your next measurement along there.",
                    "label": 0
                },
                {
                    "sent": "And this is a bit like a kind of a sampling optimization, and I'm not sure if I would call this quadrature, but what your goal here is to shrink to posterior mass as quickly as possible to some point, which would be a very good image, right?",
                    "label": 0
                },
                {
                    "sent": "And this you can do this sequentially if you don't like that, then you can also do this little bit more clever, but in this work we just did discretely sequentially, But the really hard part here is to get that posterior and to get these design scores and to get them computed in a kind of a finite time, so that you.",
                    "label": 0
                },
                {
                    "sent": "Can iterate this and get it done with.",
                    "label": 0
                },
                {
                    "sent": "Obviously it is.",
                    "label": 0
                },
                {
                    "sent": "You would want to do this in real time, but.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There yet another example which is a bit simpler, but.",
                    "label": 0
                },
                {
                    "sent": "Is a blind deconvolution problems.",
                    "label": 1
                },
                {
                    "sent": "So for example if you have a kind of.",
                    "label": 0
                },
                {
                    "sent": "For whatever reason, you have an image like this taken with your camera and you will want to clean it up.",
                    "label": 0
                },
                {
                    "sent": "You will have to kind of guess what the block on was that gave rise to this blurry image here, and if you do not have fancy like motion sensors then you have to guess that from the data itself, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a difficult inverse problem where you not only have to guess what the clean image was, but also what the Colonel was and people figured out over the years that a very good way of doing this is to treat it as a Bayesian problem, integrate over the latent image and then you estimate over the kernel.",
                    "label": 0
                },
                {
                    "sent": "And this is like a little bit like an EM algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "So this is almost kind of the optimal way of doing this, given that your model is good because it's maximal acute estimation, right?",
                    "label": 0
                },
                {
                    "sent": "Again, you need to kind of somehow integrate over that images, which is a very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Signal backed by a model that's not really Gaussian.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a little bit how such a model would look like.",
                    "label": 0
                },
                {
                    "sent": "This is really just state of the art image model.",
                    "label": 0
                },
                {
                    "sent": "You have to likelihood here, which you can assume is Gaussian Gaussian noise.",
                    "label": 0
                },
                {
                    "sent": "Here is to design and then you have a prior that's not Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That's very important because images are simply just not Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If you want to have a one line explanation why all of this compressed sensing works, it's exactly that images are not Gaussian.",
                    "label": 0
                },
                {
                    "sent": "They're probably more like something like Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So you can put Laplacian potentials on something like wavelet coefficients image gradients.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of theory behind wavelets, but the main.",
                    "label": 0
                },
                {
                    "sent": "Purpose for us is split that you can simply compute them very rapidly.",
                    "label": 0
                },
                {
                    "sent": "And so then, so how?",
                    "label": 0
                },
                {
                    "sent": "So what is kind of configurable here?",
                    "label": 0
                },
                {
                    "sent": "While you have a design matrix that's given by the application?",
                    "label": 0
                },
                {
                    "sent": "And then you have this other thing which is given by by whatever you would like to choose.",
                    "label": 0
                },
                {
                    "sent": "You need to choose something that you can multiply with quickly and so here are a few examples.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do denoising, then the X would be a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "If you want to deconvolution it's a convolution.",
                    "label": 0
                },
                {
                    "sent": "If you want to, Emery Construction is a Fourier transform matrix.",
                    "label": 0
                },
                {
                    "sent": "So now what we need to do is to do inference in this model.",
                    "label": 0
                },
                {
                    "sent": "That's the posterior.",
                    "label": 0
                },
                {
                    "sent": "If we cannot do this exactly, we want to approximate it so that we get still still get the results that we want.",
                    "label": 0
                },
                {
                    "sent": "And again, this is a very high dimensional thing.",
                    "label": 0
                },
                {
                    "sent": "You think about something like 100,000 to a million pixels, and if it's a, it's MRI.",
                    "label": 0
                },
                {
                    "sent": "It's twice that because it's complex.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you can run this model with Laplace potentials and other things as well.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's fairly general, so what's the very rough idea how to how we get this done computationally we use something called variational inference.",
                    "label": 0
                },
                {
                    "sent": "Very simple idea.",
                    "label": 0
                },
                {
                    "sent": "You have a non Gaussian integral to do you have a non Gaussian distribution?",
                    "label": 0
                },
                {
                    "sent": "Want to estimate?",
                    "label": 0
                },
                {
                    "sent": "You want to approximate and you do this by repeatedly fitting it with the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "OK so you use a Gaussian computations in order to get your job done on the non Gaussian side and this is very much very good analogy that you might think about this if you don't know much about inferences.",
                    "label": 0
                },
                {
                    "sent": "The Newton algorithm fits quadratics to convex functions, so you're not minimizing a convex function, you're not minimizing a quadratic, but you're using the fact that quadratics are easy to minimize in order to get the job done on the other function right?",
                    "label": 0
                },
                {
                    "sent": "Exactly the same idea is striving.",
                    "label": 0
                },
                {
                    "sent": "Variational inference.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using the fact that we can deal with Gaussians well.",
                    "label": 0
                },
                {
                    "sent": "So very, very, very briefly, how does this work?",
                    "label": 0
                },
                {
                    "sent": "You kind of find an upper bound to kind of this key function here, which is the negative log partition function, which is the moment generating function for the whole thing, so that that function encodes the moments that we want.",
                    "label": 1
                },
                {
                    "sent": "So if you get a good approximation, which in this case is abound, then you can hope that by taking derivatives of that approximation you get good approximation to these moments.",
                    "label": 0
                },
                {
                    "sent": "In this case we get a bound fairly easily by.",
                    "label": 0
                },
                {
                    "sent": "I'm not really getting into it, but it's a very simple idea, you just.",
                    "label": 0
                },
                {
                    "sent": "Lower bound is non Gaussians by Gaussians and then you plug them into the integral.",
                    "label": 0
                },
                {
                    "sent": "You get a bound.",
                    "label": 0
                },
                {
                    "sent": "And then you optimize over the parameters that describe how broad discussions are, and that's in fact a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "For this Laplace potentials.",
                    "label": 1
                },
                {
                    "sent": "It's a big convex optimization problem and kind of the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "I want to talk a little bit about how you can at least approximately solve this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me first give you 2 slides.",
                    "label": 0
                },
                {
                    "sent": "Explanation why many inference methods that we looked at before actually are not very fast and the reason is this they are kind of.",
                    "label": 0
                },
                {
                    "sent": "In order to do inference you have to do certain things, possibly repeatedly.",
                    "label": 0
                },
                {
                    "sent": "You have to do things which are very expensive if to compute Gaussian variances and for numerical mathematicians it's just you need to compute the diagonal of an of an inverse, right?",
                    "label": 0
                },
                {
                    "sent": "This is not something you usually find in American math books, but that's what you have to do.",
                    "label": 0
                },
                {
                    "sent": "Then you have to do something which you will find in every book and that's solving linear systems or minimizing.",
                    "label": 0
                },
                {
                    "sent": "Traffic functions.",
                    "label": 0
                },
                {
                    "sent": "For example, the Newton algorithm has to do this all the time and then you have to do things you don't even want to think about like linear time, things OK.",
                    "label": 0
                },
                {
                    "sent": "So many algorithms are not as fast as they could be.",
                    "label": 0
                },
                {
                    "sent": "Is that they kind of.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do this in a random or in a cyclic ordering so you do some very simple thing.",
                    "label": 0
                },
                {
                    "sent": "Then you compute this very expensive variant system.",
                    "label": 0
                },
                {
                    "sent": "You solve a system and you just do this in the order and this doesn't.",
                    "label": 0
                },
                {
                    "sent": "If you look at it like that, it doesn't really sound very possible right?",
                    "label": 0
                },
                {
                    "sent": "You should do this very often.",
                    "label": 0
                },
                {
                    "sent": "Then you should do this kind of now and then and you should do this as little as possible, right?",
                    "label": 0
                },
                {
                    "sent": "Only if you really have to do it.",
                    "label": 0
                },
                {
                    "sent": "If you cannot do anything else that would only require these guys anymore, right?",
                    "label": 0
                },
                {
                    "sent": "And I did.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the idea behind the algorithm that we came up with is that we managed to reorder computations so that this actually happens, and it's still a converge.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm and the algorithm is using a little bit of convex convex convexity, so what's the idea?",
                    "label": 0
                },
                {
                    "sent": "So if you look at the criterion, you need to optimize, then you quickly realize that it has at least one part, which is really tricky.",
                    "label": 0
                },
                {
                    "sent": "So that part leads to the fact that if you want to compute the gradient, you have to do a difficult computation, but that part is not only.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difficult and coupled.",
                    "label": 0
                },
                {
                    "sent": "It's also concave.",
                    "label": 0
                },
                {
                    "sent": "And so you can get rid of the coupling by replacing the concave part by by a linear part, right?",
                    "label": 0
                },
                {
                    "sent": "It's just eventuality.",
                    "label": 0
                },
                {
                    "sent": "So instead of the concave part you have to add linear plane in there.",
                    "label": 0
                },
                {
                    "sent": "The refitting might take you some time, but once you've done that, you can optimize, minimize the upper bound, much, much more easily and much more quickly.",
                    "label": 0
                },
                {
                    "sent": "If you do this, you plug it all together.",
                    "label": 0
                },
                {
                    "sent": "So this set is now the kind of the normal vector of displaying.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if you fix that for the moment, just fitted your plane, then you need to solve an optimization problem that everybody in optimization knows very well how to solve.",
                    "label": 0
                },
                {
                    "sent": "This is this is like one of the oldest optimization problem.",
                    "label": 0
                },
                {
                    "sent": "If this is a quadratic part which is not in this case, then this is least squares estimation.",
                    "label": 0
                },
                {
                    "sent": "And if this is kind of a decoupled non quadratic part then we have very good algorithms today to do this right simply because almost everybody in scientific computing needs this.",
                    "label": 0
                },
                {
                    "sent": "So this is a penalized least squares problem and there you have lots of algorithms that can do that.",
                    "label": 1
                },
                {
                    "sent": "Very efficiently and you don't need to compute these very heavy.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things so you need to compute them when you refit, abound, or to refit.",
                    "label": 0
                },
                {
                    "sent": "This tangent, of course you have to compute its normal vector, and that is exactly the Gaussian variances problem.",
                    "label": 0
                },
                {
                    "sent": "So there you have to do it, but you only have to do it rarely, so the whole idea behind that algorithm is that your basic.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reordering things so that the hardest part you have to do is as least often as possible.",
                    "label": 0
                },
                {
                    "sent": "So in particular, you need to compute the variances much less frequently than you need to compute something like Gaussian means.",
                    "label": 1
                },
                {
                    "sent": "Gaussian mean computations are solving linear systems.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the main idea of this algorithm, but now in the rest of the talk, I really want US OS the numerical method and American methods going to hopefully help us to get a handle on these variances.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not only that I need to run my algorithm, I also once I've run my algorithm converged.",
                    "label": 0
                },
                {
                    "sent": "I need to now plan the next step.",
                    "label": 0
                },
                {
                    "sent": "I need to make the next measurement, and this looks a little bit like this.",
                    "label": 0
                },
                {
                    "sent": "This this really looks more complicated than it is.",
                    "label": 0
                },
                {
                    "sent": "You need to compute an information gain, which very intuitively is just the amount of information.",
                    "label": 0
                },
                {
                    "sent": "The amount of uncertainty that you get rid of if you make a new measurement.",
                    "label": 0
                },
                {
                    "sent": "Uncertainties measured by entropies, that's before you did the measurement.",
                    "label": 0
                },
                {
                    "sent": "That's after you.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximate this a little bit by basically replacing.",
                    "label": 0
                },
                {
                    "sent": "Of course the non Gaussians by Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Once you've done that, you exploit a very pleasant property of the Gaussian that it doesn't.",
                    "label": 0
                },
                {
                    "sent": "The covariance doesn't depend on what you condition on and you simply get a score like this.",
                    "label": 0
                },
                {
                    "sent": "So you again need to and if you look at this closely then again it looks exactly like the variance is, because here you have the covariance and then it's projected along your potential new measurements.",
                    "label": 0
                },
                {
                    "sent": "So again, you need to compute these variances here, right?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's really a bit of a problem, right?",
                    "label": 0
                },
                {
                    "sent": "If you say that this is a scalable approach, then you need to address all of them.",
                    "label": 0
                },
                {
                    "sent": "All of these things, and they really.",
                    "label": 0
                },
                {
                    "sent": "The problem here is the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "It's a very, very big matrix which has kind of nice parts here to it, but they're certainly dense parts, so there's nothing like very sparse here.",
                    "label": 0
                },
                {
                    "sent": "And then you need to invert it, which of course we want to can even store this metrics.",
                    "label": 0
                },
                {
                    "sent": "And then you need to kind of compute these always the same like left right multiplication and then diagonal.",
                    "label": 0
                },
                {
                    "sent": "And this kind of the same thing, it's like.",
                    "label": 0
                },
                {
                    "sent": "Block diagonal block diagonal variant of the same thing.",
                    "label": 0
                },
                {
                    "sent": "So this is what we need to do.",
                    "label": 0
                },
                {
                    "sent": "So how do we get?",
                    "label": 0
                },
                {
                    "sent": "How do we get a handle on these variances?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, they are really much more difficult to get the Gaussian means, which is probably also why you really won't find them in many textbooks and American math.",
                    "label": 1
                },
                {
                    "sent": "All the means you just have to solve one linear system right for all of them.",
                    "label": 0
                },
                {
                    "sent": "All the variances were native, baseline way would be to solve 1 system for each variance component.",
                    "label": 0
                },
                {
                    "sent": "Pretty much related to what you need to do in Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "You can get all the means by 1 system.",
                    "label": 0
                },
                {
                    "sent": "You can compute that before hand, but if you want to have variances you have to redo computations all the time, so that is obviously not an option here.",
                    "label": 0
                },
                {
                    "sent": "If N is 1,000,000 or 100,000 and we cannot do this right, we need to get by with solving and maybe 50 or one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Systems, so you might say, why don't you use loopy BP?",
                    "label": 0
                },
                {
                    "sent": "While the situation for loopy BP is exactly how you would expect so if it converges, the means are exact.",
                    "label": 1
                },
                {
                    "sent": "So you have them, whether that's the best way of getting them is another is something else, but the variances are completely wrong, right?",
                    "label": 0
                },
                {
                    "sent": "So and so.",
                    "label": 1
                },
                {
                    "sent": "Basically people even argue that the major part of that computation is left undone by loopy and there's the right now, no way to really correct this in order to get the variance is fixed there certain proposals, but they only work in special cases.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then of course there are tractable cases.",
                    "label": 0
                },
                {
                    "sent": "If you have a tree, everything simple.",
                    "label": 0
                },
                {
                    "sent": "If you have a sparse matrix, possibly it could be factorized, and then you can get the variances by by one more step.",
                    "label": 0
                },
                {
                    "sent": "All of this doesn't happen, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "In our inverse problems.",
                    "label": 0
                },
                {
                    "sent": "So we need to do something else.",
                    "label": 0
                },
                {
                    "sent": "We need to use.",
                    "label": 0
                },
                {
                    "sent": "Actually, some pretty pretty.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Crude approximations and let me just go quickly over how two of them look like, so this is not something that only we need.",
                    "label": 0
                },
                {
                    "sent": "This is something that many other people need in physics and in even in piddies people start to now worry about uncertainties which they didn't do for a long time.",
                    "label": 0
                },
                {
                    "sent": "And of course, in remote sensing, so here's some.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very very simple idea how how one could estimate these variances.",
                    "label": 0
                },
                {
                    "sent": "Ideas to plug in to plug in a low rank approximation.",
                    "label": 0
                },
                {
                    "sent": "So suppose you could get a matrix that would have this funny property.",
                    "label": 0
                },
                {
                    "sent": "It's a very slim matrix so long and slim.",
                    "label": 0
                },
                {
                    "sent": "And this low rank matrix is close to identity, right?",
                    "label": 0
                },
                {
                    "sent": "Suppose you had something like that.",
                    "label": 0
                },
                {
                    "sent": "Of course there's nothing like that, but maybe we can approximate something like this.",
                    "label": 0
                },
                {
                    "sent": "Then you can plug it in here somewhere here.",
                    "label": 0
                },
                {
                    "sent": "Plug it in here.",
                    "label": 0
                },
                {
                    "sent": "Then you have to lowering.",
                    "label": 0
                },
                {
                    "sent": "Here you sum over lowering here.",
                    "label": 0
                },
                {
                    "sent": "This is not such a big number and then you just need to solve these systems here.",
                    "label": 0
                },
                {
                    "sent": "So that's so that's nice.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we only need to solve linear systems instead of an obvious question.",
                    "label": 1
                },
                {
                    "sent": "How do I choose?",
                    "label": 0
                },
                {
                    "sent": "That funny matrix here, possibly even dependent on what that matrix A is.",
                    "label": 0
                },
                {
                    "sent": "How do I choose it so that I get so that this is not too bad?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one idea, one general idea is to use these hotema matrices.",
                    "label": 0
                },
                {
                    "sent": "It's kind of an idea that comes from coding theory.",
                    "label": 0
                },
                {
                    "sent": "Hadamar matrices are something that you can really torture you.",
                    "label": 0
                },
                {
                    "sent": "Numerical mathematics algorithms with they are.",
                    "label": 0
                },
                {
                    "sent": "They are also orthogonal matrices, but they're only plus minus ones, and then you can just pick columns.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From these matrices, maybe the first L columns and then you normalize them so the diagonal is really the identity and then you use that one.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that kind of works not so badly.",
                    "label": 0
                },
                {
                    "sent": "It's a deterministic estimator.",
                    "label": 0
                },
                {
                    "sent": "So no matter what that a is, you always use the same one, and it's kind of intuition is to maximize the smallest angle between any pairs of rows of V, which is kind of something that of course for the identity would be true, but for a low rank matrix you cannot really get.",
                    "label": 1
                },
                {
                    "sent": "So that's a nice and simple thing that's simple.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another idea would be just let's use Monte Carlo estimation.",
                    "label": 0
                },
                {
                    "sent": "Let's just sample from that Gaussian, and then let's just estimate the variances in the usual way.",
                    "label": 0
                },
                {
                    "sent": "So if I could sample from that Gaussian.",
                    "label": 0
                },
                {
                    "sent": "In a fairly efficient way, then I could just use these samples in order to estimate these variants that just plug my samples in here, right?",
                    "label": 0
                },
                {
                    "sent": "Multiply with B squared and then I do my Monte Carlo average.",
                    "label": 0
                },
                {
                    "sent": "So now how can I?",
                    "label": 0
                },
                {
                    "sent": "How can I get?",
                    "label": 0
                },
                {
                    "sent": "How can I get these samples?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, there's a simple trick here.",
                    "label": 0
                },
                {
                    "sent": "It's called perturbing map.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you first sample random vector from a very funny distribution, which is a Gaussian that has the inverse covariance matrix as the covariance matrix and that is easy to do if you look at this metrics then you see that it has a lot of structure.",
                    "label": 0
                },
                {
                    "sent": "It's kind of always matrix times matrix transpose and then some stuff in the middle.",
                    "label": 0
                },
                {
                    "sent": "So you just sample 2 random vectors, Gaussian O mean, you plug them in, you multiply them and you get the right result.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't cost you anything.",
                    "label": 0
                },
                {
                    "sent": "That's one of these peanuts.",
                    "label": 0
                },
                {
                    "sent": "Computations.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you solve systems and now we need to solve 1 system for every sample that we get.",
                    "label": 0
                },
                {
                    "sent": "And this is now very easy.",
                    "label": 0
                },
                {
                    "sent": "Right now you solve this system and then you can show that the covariance left right multiplication with a inverse.",
                    "label": 0
                },
                {
                    "sent": "That was a.",
                    "label": 0
                },
                {
                    "sent": "So we have a inverse.",
                    "label": 0
                },
                {
                    "sent": "So that's the part urban map estimator, and it's just a direct Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "No Markov chain here.",
                    "label": 0
                },
                {
                    "sent": "It's an exact Monte Carlo estimator from this Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And then you do this as often as you can afford.",
                    "label": 0
                },
                {
                    "sent": "Which depends on your hardware, maybe?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe 50 or 100 times.",
                    "label": 0
                },
                {
                    "sent": "The most we are sometimes only using 30 and then you estimate these variances.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the nice thing about this estimator is that it's unbiased, it's an unbiased estimator, so that's very, very easy to see.",
                    "label": 0
                },
                {
                    "sent": "So in on average, over each component is correct.",
                    "label": 0
                },
                {
                    "sent": "So especially if you use these variances in a kind of a way in which there are some in which they are linearly combined, that estimators are very good estimator.",
                    "label": 0
                },
                {
                    "sent": "But each component has a substantial noise on it for sure, don't have a plot here, but I uploaded it once and there's a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "But if you average across components, it's a pretty good estimator.",
                    "label": 0
                },
                {
                    "sent": "They are.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Open questions which, if anybody could give me a hint?",
                    "label": 0
                },
                {
                    "sent": "I would be very happy.",
                    "label": 0
                },
                {
                    "sent": "Is well.",
                    "label": 0
                },
                {
                    "sent": "First of all, that's probably easy to answer.",
                    "label": 0
                },
                {
                    "sent": "Is that actually the best distribution that I sample from independent Lee?",
                    "label": 0
                },
                {
                    "sent": "Is that really the Gaussian?",
                    "label": 0
                },
                {
                    "sent": "Because in the end I just need these variances.",
                    "label": 0
                },
                {
                    "sent": "Is it really optimal to sample from that Gaussian or color sample from something else?",
                    "label": 0
                },
                {
                    "sent": "And then another one, which would be very important is could I use dependent samples in order to reduce the variances and still stay unbiased?",
                    "label": 0
                },
                {
                    "sent": "Or kind of bias variance tradeoff?",
                    "label": 0
                },
                {
                    "sent": "I mean I'm happy to be a little bit biased, but I want to reduce the variance here, so if anybody has an idea then please tell me about it.",
                    "label": 0
                },
                {
                    "sent": "Good so I'm almost done.",
                    "label": 0
                },
                {
                    "sent": "So this is basically what we're using right now in order to get these variances.",
                    "label": 0
                },
                {
                    "sent": "Of course you can also just say yes.",
                    "label": 0
                },
                {
                    "sent": "How large so in the MRI example it's it's five.",
                    "label": 0
                },
                {
                    "sent": "OK so you have 256 by 256 images.",
                    "label": 0
                },
                {
                    "sent": "Now you have to multiply it with two because it's complex.",
                    "label": 0
                },
                {
                    "sent": "So it's about five 512 * 250, so about 120,000 by 120,000.",
                    "label": 0
                },
                {
                    "sent": "And that is still on the small side because I actually MRI is usually 3 dimensional and then you're in the many many millions.",
                    "label": 0
                },
                {
                    "sent": "So, but this is actually then again small for real numerical math.",
                    "label": 0
                },
                {
                    "sent": "People who do whatever Geo statistics where you have billions and whatever variables.",
                    "label": 0
                },
                {
                    "sent": "So the important thing here is that it absolutely has to scale.",
                    "label": 0
                },
                {
                    "sent": "Everything has to scale linearly or or N log N, which is kind of the Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "Everything beyond that is impossible.",
                    "label": 0
                },
                {
                    "sent": "In practice.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Very obvious idea, but it's kind of interesting to explore hybrid approaches which say OK, so maybe I can.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe I have a big mess in terms of covariance, but there is some structure, some level, so then we can actually use the fact that we know about structure in graphical models in order to get a hybrid algorithm.",
                    "label": 0
                },
                {
                    "sent": "That kind of more or less makes use of these numerical mathematics mathematics approaches only when we really need them right.",
                    "label": 0
                },
                {
                    "sent": "And here's a very simple example.",
                    "label": 0
                },
                {
                    "sent": "In MRI you have many slices which just lie next to each other.",
                    "label": 0
                },
                {
                    "sent": "In 3D MRI you also have that it's even bigger, so it's just think about these slices or being next to each other.",
                    "label": 0
                },
                {
                    "sent": "Of course there statistically very heavily dependent on each other, so you would want to have something like a sequence model here between the slices and then you see that if I now want to do, you know the variances over all these this huge cube, then you can just exploit the fact that your model only links neighboring slices in order to get kind of a hybrid approach, which would come.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bind something like common message passing.",
                    "label": 1
                },
                {
                    "sent": "So you're passing your Gaussian messages between these slices, maybe back and forth.",
                    "label": 0
                },
                {
                    "sent": "And you would then use the numerical mathematics primitives in order to approximate these messages in the common game matrices and so on, right?",
                    "label": 0
                },
                {
                    "sent": "And this is of course not again something that we have come up with.",
                    "label": 0
                },
                {
                    "sent": "This is actually used in geostatistics.",
                    "label": 0
                },
                {
                    "sent": "In weather simulations are quite heavily, they called this approximate.",
                    "label": 1
                },
                {
                    "sent": "Basically, approximate common smoothers, which are based on low rank decompositions of the of the relevant matrices there.",
                    "label": 0
                },
                {
                    "sent": "9.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is my talk, so I think that.",
                    "label": 0
                },
                {
                    "sent": "Kind of goes the wrong way around in terms of the workshop, but I think it's also very important in order to kind of maybe also in order to get the numerical mathematicians interested.",
                    "label": 0
                },
                {
                    "sent": "We also have to present them our problems, and we have to present the problems that we really need them to that we cannot just run with what Matlab Pass, but we need to understand a bit more.",
                    "label": 0
                },
                {
                    "sent": "So in this case I'm kind of advocating to use reductions to computational state of the art instead of just cooking up some message passing thing that might not be very fast.",
                    "label": 0
                },
                {
                    "sent": "Let's reduce our computations to the state of the art of the people who really know how to solve things at very large scales.",
                    "label": 0
                },
                {
                    "sent": "So that usually involves convex.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And and I think we know a lot about this in the field now, because we've done this interface since many, many years.",
                    "label": 0
                },
                {
                    "sent": "For example, penalized least squares map estimation and so on.",
                    "label": 1
                },
                {
                    "sent": "Possibly more difficult things by.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It also involves numerical mathematics beyond just calling Matlab things.",
                    "label": 1
                },
                {
                    "sent": "In my case, it's the Gaussian cover, and since we have a lot of work in machine learning, doing low rank approximations, we need to solve linear systems.",
                    "label": 1
                },
                {
                    "sent": "We need to deal with a major headache which is called preconditioning, parallelization, and so on, and quadrature of course in this case I don't need quadrature, but usually in variational inference you also need quadrature.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I think maybe what this workshop can contribute to is to work on the interface beyond between machine learning and numerical mathematics, which is in provable.",
                    "label": 1
                },
                {
                    "sent": "I think if you compare it to the interface that we have with convex optimization, it really, really have to go beyond MATLAB, because Matlab is only doing something that more or less serves other people like like physicists and so on, and signal processing people.",
                    "label": 0
                },
                {
                    "sent": "So we need to really understand what our requirements and what are the limitations of.",
                    "label": 0
                },
                {
                    "sent": "Of the layer below.",
                    "label": 0
                },
                {
                    "sent": "I was already been mentioned that the standards of these two fields are very, very different and we can certainly not expect that our standards are somehow even taken seriously by the numerical math people.",
                    "label": 0
                },
                {
                    "sent": "But they also have to understand that we have to do deal with much more well defined problems and much in some sense, much bigger problems sometimes.",
                    "label": 0
                },
                {
                    "sent": "So the goals are also different than we.",
                    "label": 0
                },
                {
                    "sent": "These two fields really have to come together, and I think one thing we should target for is that we become serious numerical math customers.",
                    "label": 0
                },
                {
                    "sent": "So in the moment all they're doing is.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm being unfair, but I always hear PZ, PZ, PZ, PZ, PZ, PZ and that's fine.",
                    "label": 1
                },
                {
                    "sent": "PDS are very important but there's other important things with large scale data analysis and with Bayesian inference and we need to kind of make sure that this is understood that we also have needs which are not trust PDS.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they are, but sometimes they're not.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we need to solve systems that have a more complicated structure in piddies.",
                    "label": 0
                },
                {
                    "sent": "OK, that's my talk.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                }
            ]
        }
    }
}