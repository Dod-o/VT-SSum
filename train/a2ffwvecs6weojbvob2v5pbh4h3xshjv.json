{
    "id": "a2ffwvecs6weojbvob2v5pbh4h3xshjv",
    "title": "Analyzing and Escaping Local Optima in Planning as Inference for Partially Observable Domains",
    "info": {
        "produced by": [
            "Data & Web Mining Lab"
        ],
        "author": [
            "Pascal Poupart, School of Computer Science, University of Waterloo"
        ],
        "published": "Nov. 30, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Decision Support",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_poupart_domains/",
    "segmentation": [
        [
            "OK, so I'm going to talk about analyzing an escaping local optimum planning as in French for partial observable domains.",
            "OK, and this is also joint work with copious flag and map to stand for the Free University of Berlin in Germany."
        ],
        [
            "OK, so in this talk when I consider one particular approach of complaint planning that has become popular in recent years, which is pending as inference and we're going to focus on partial observable domain.",
            "So that means we're going to look at palm DP.",
            "So, partial observable Markov decision processes.",
            "Now when we do planning as inference gauges are going to convert the palm DP into a mixture of that.",
            "An invasion networks an.",
            "As a result, planning can now be framed as simply.",
            "Our problem of maximizing likelihood.",
            "I'll explain that in details, but the key is that by doing this then it opens the door to using many interests.",
            "Algorithms do planning, so this is all nice, except that the original problem, the original planning problem was non convex and doing this transformation actually doesn't change its nature, so it remains a nonconvex optimization problem.",
            "So even though we have not many and Prince algorithms that can solve this, we will all suffer from local Optima as well.",
            "I mean, machine learning this is also an issue, but in general and panning is a more severe issue, so that's why we're looking at that.",
            "OK, so in this talk there will be 2 contributions, so the first one will be an analysis of EMS local optimize.",
            "What I mean by an analysis is I'm going to interpret what it means to be stuck into local optimal in terms of time, so will give a meaning to this, and then there will be 2 escaped."
        ],
        [
            "OK, so here's the outline.",
            "1st I'll just review quickly.",
            "Water pump, tipis, and playing as inference.",
            "Then we'll go into the meat of the subjects, will pull up to my interpretation.",
            "The main thing here.",
            "So if you don't understand the talk, or if you don't remember much, then the main thing to remember is that when you get stuck into a local optimum will show is that if we just correspond to doing it one step, look ahead.",
            "OK, so that's what young guys.",
            "And then I'll explain how to escape from local Optima using two techniques or for research and mold splitting, and then I'll show you some experience."
        ],
        [
            "OK, So what is the palm DP?",
            "So it's a regular Markov decision process where we don't get to observe this state, so we've got some observations here that are correlated with the States and I'll do this as part of the rain Forsman learning session.",
            "Here we're really looking at a planning problem, so we're assuming that we know what are the transitions as well as the reward function and the observation function, so we know everything we don't have to get some samples.",
            "OK, so just need to optimize the costs."
        ],
        [
            "OK, so very quickly high is also optimization done.",
            "Traditionally you would be fine, believes that our distributions over states because we don't get to observe them.",
            "You could have made those beliefs by basophil trying.",
            "So is it just based their own and then a policy with severe mapping from beliefs to actions and you could evaluate a policy simply by taking at discounted some of the expected reward and the best policy is obviously the one that will give you.",
            "The highest expected sum of reward and a certificate to verify optimality is Bellman's equation, which is right here.",
            "OK so."
        ],
        [
            "That being said, let's look now at one class of policies.",
            "I find same controllers.",
            "So fine.",
            "Said controllers are very interesting because of their simplicity and it can be downloaded in very very small devices.",
            "So here the idea is that we've got a bunch of nodes, and for each node there's an action.",
            "So if you know you execute that action, then you receive an observation that will tell you which edge to follow.",
            "So if you get 01.",
            "We go to this node, execute a one again and keep on going.",
            "So it's really just a final state on that we executed.",
            "So then the problem possible optimization in this context is really the problem.",
            "Simply defining what would be the best action mapping and next node mapping.",
            "In general, we're going to let those things be probabilistic, so we're going to have conditional distributions.",
            "So."
        ],
        [
            "So in 1999, Michelangelo and some coauthors actually realize that we could fold this controller type of policy into the graphical model of funding.",
            "So here I've got again the graphical model that I showed you a few slides ago.",
            "I can augment it now with one more set of variables labeled MIND correspond to the node of the controller that the agent will be in at any point in time.",
            "And now the policy is re encoded by those red marks simply because the choice of the action is conditional on the current node and the next node that you would end up in this condition on their current note as well as the observation received.",
            "OK, so now you can see where we're going.",
            "So in terms of I guess we planning as inference ideas that were slowly starting to convert from deep into it a regular dynamic Bayesian network where the policy is is essentially some initial distributions as.",
            "Find a vision network.",
            "Alright."
        ],
        [
            "So.",
            "Then in 2006 Mac to say propose that since we've got this view, perhaps you could simply ask how we could find what is the best conditional distribution in this graphical model that would maximize our expected reward.",
            "But let's try to do this in a way that would really have a dynamic Bayesian network as a problem that we still have some rewards here that are utilities are not random variables and we've got some discount."
        ],
        [
            "Factor, so one idea is that we could simply normalize the rewards so the rewards normally are any real number, and if we simply re normalize by subtracting the minimum reward and dividing by the range, then everything will be between zero and one and that can be interpreted as a probability.",
            "And now we can make the reward just be some binary variables with probability that's proportional to the original reward function.",
            "So what's left now is how to deal with the discount factor, and so this is where."
        ],
        [
            "When we could do is simply introduce a mixture of an invasion networks simply because rewards need to be added so mixed chairs or mixture models or traditionally falls that are decompose additively, so this makes sense here, and each dynamic Bayesian network is going to have probability that's proportional to the discount factor race to the number of time steps that's included in that dynamic vision network.",
            "So now I gotta conversion where everything is essentially proportional.",
            "And now the value of a policy is nothing more than simply the probability that our bar would be equal to 1."
        ],
        [
            "OK, so given that now, finding an optimal policy can be phrased as an inference problem, where we would simply find what is what are the initial distributions here that would maximize the probability that power bar is equal to 1.",
            "Now how do we do this?",
            "There's lots of algorithms as I mentioned before, but let's start with the simplest one expectation maximization."
        ],
        [
            "The problem with any of the algorithms including yeah, is that they're going to get stuck into some local Optima, and So what I'm going to do now is go through analysis an essentially showing you that what PM ensures is just one step lookahead optimality.",
            "And based on that, then the natural thing to do if you want to escape is to look at multiple step to look ahead.",
            "An another approach would be to split some goals.",
            "So I say these things in more details."
        ],
        [
            "OK, so let's go into the details of.",
            "Yeah, it is going to be a few equations, but I'll get back to some high level ideas in a minute if.",
            "No, you're not comfortable with Visa.",
            "OK, so M&R context here is used to update the powders, and here I've got the distribution over actions for each shell and the distribution over next nodes given the current hold, an observation, these parameters are only updated in a way that we multiply their current setting by some expression.",
            "So we do in multiplicative update.",
            "That's 1:00 PM does an if we're starting to local optimum.",
            "Well, it means that the multiplicative update here won't change what those values are so far."
        ],
        [
            "Yes, we can.",
            "Formally the first theorem then essentially says that if we've got a policy that's a stable fixed point of VM, then those two conditions have to hold and essentially specifying webtools expr."
        ],
        [
            "Actions that I had in the previous high school G here engage here when."
        ],
        [
            "Maybe like an essentially they must be equal to the largest possible value that they can take for any action here and for any next node here.",
            "OK, so then itself is not too interesting."
        ],
        [
            "Thanks, but now let's see how we can use that to really understand what it means to be stuck into local Optima.",
            "So I've got the equations for YM right here and also global optimality as specified by dominance equation.",
            "So most here are nothing more than balance equation that's been split in two in the context of controllers.",
            "So when this holds, I know that I've got an optimal policy and when this holds here I know that I'm stuck.",
            "If I'm using him.",
            "Now these equations are actually very similar, but you've got some differences.",
            "So in green I've circled samples differences and you'll notice that here I've got some terms that are bounded functions and hear some terms that are bad.",
            "So what are those bad as if I?"
        ],
        [
            "Go back to my slide here for the details of.",
            "Yeah, beta is nothing more than the backward term that you would compute in India.",
            "Now it wasn't clear to us what it meant, but from those."
        ],
        [
            "Quations actually we can show that were better really is just as rescaled version of the value function, so you can actually interpret the backward terms in EM as just the value function.",
            "So now because it's a rescaled version, it doesn't change anything, because here we've got a Max and here in our maximum when you maximized, if you just re scale things by a constant factor, it doesn't change anything.",
            "OK, the next differences that here I'm going to copy that are very close one and here with the reward function as we explained before.",
            "These things are just rescaled version of each other, so again it doesn't change anything.",
            "The last differences are that have got Alpha and some beliefs how far corresponds to the forward terms in EM and those bees are beliefs or or distributions over states.",
            "It turns out that the forward terms any M can be interpreted as the occupancy frequencies of the controller and if you re normalize these things to sum up to one, they correspond to beliefs.",
            "So it looks like, again, there's no difference, but it turns out that here for global optimality those equations have to hold for every belief, and actually we know that if we have an optimal policy then we should be executing the optimal thing in every single belief right?",
            "Whereas when TM says is that you're going to be picking the best action only in the beliefs that are proportional to the occupancy frequencies of the nodes of the controller.",
            "So it's really just answering.",
            "Optimality with their respective subset of the movie or another way to interpret this is that because it's only with respect to the nose of the controllers that we're really just looking at one step ahead optimality."
        ],
        [
            "So if you got a second theorem, that essentially confirms that the condition that I had before, or indeed the ones that are necessary but not sufficient for global optimality an as I explained, we can interpret that as just saying that PM ensures once that look ahead of now.",
            "Alright, so based on that now we can look at escaped techniques.",
            "The first one will simply generalize this one step to multiple step.",
            "In a second."
        ],
        [
            "Amazon spare so for multi step.",
            "When we do is we look at every node in our control room.",
            "So let's take this one for example.",
            "And what we can do is just look ahead search meaning that I simulate all possible trajectories of actions and observations and check for each belief is.",
            "Or am I picking the optimal action or not?",
            "If I realize that there's a belief where I'm suboptimal, then all I have to do is add some notes to the controller.",
            "And corresponds to the path where I found something suboptimal.",
            "And you see, normally I am only ensures one step lookahead optimality from each belief that's proportional to the occupancy frequencies of each node.",
            "But now we're going to do this search sign for each node, but for multiple steps right?",
            "And so in the limit, we can guarantee you know global optimality if we go far enough in the search."
        ],
        [
            "OK, the second approach is actually inspired by some work with respect to hidden Markov models, where the idea is that we could simply pay each node one at a time and see if we could split it and perhaps re optimize the powers of the new nodes in such a way that perhaps we're going to improve the controller.",
            "And you can actually interpret this as looking for.",
            "I guess the optimal two step.",
            "Look ahead that would be possible from the beliefs that are normally encountered in that node.",
            "So it's.",
            "So different approach, but it has its advantages in the sense that it will generally be optimal to expect the two step look ahead.",
            "And it's also fairly simple, because we're simply just reusing them as well."
        ],
        [
            "OK, so let's see how they compare first in terms of complexity.",
            "So M by itself the complexity with respect to center, the controller with quadratic.",
            "Now if we include node splitting to get out of local Optima, the complexity becomes quiet.",
            "If an intuition here is that for every node I'm going to try to do a split and then the split means that I'm going to run them so and I'm gonna have to do this as as I wrote a controller, so there's an additional quadratic term that presents as a result of that.",
            "So what it means that no spinning is going to have a hard time to scale."
        ],
        [
            "With respect."
        ],
        [
            "With respect to the number of nodes in the controller.",
            "Now for four search we have better complexity with respect to the side of the controller, but then we have to do a search and that's exponential with respected in depth.",
            "So that could be bad.",
            "On the other hand, I mean a lot of people have been looking recently at doing forward search for online techniques, and there are ways of I guess need to be mitigating this complexity by doing a branch and bound or sampling etc.",
            "So here I'm just showing all the worst case complexity."
        ],
        [
            "So now, in practice we tested this on 6 problems.",
            "I'm going to show you some graphs where two of them at a table.",
            "I'm in the first problem here.",
            "Cheese taxi.",
            "This is a benchmark where the optimal policy actually has a sequence of action that has to be executed exactly, and if you give me just a little bit from it, then you're going to have you're going to miss the goal and have very little reward.",
            "As a result, most of the techniques that are doing some kind of gradient descent or policy search are subject to local Optima or just going to fail because they have to go through a Canyon before they reach that.",
            "Precise sequence of actions and the never get to find the optimal pause.",
            "So here when we see is that four search eventually manages to escape, whereas no splitting an random restart is just stuck no matter what.",
            "In this graph I have two versions of Forward Search.",
            "One is 4 search from each one of the nodes an otherwise the auto type.",
            "Of course we could do that is the obvious one with me to just to afford charge from the initial belief as opposed to each one of the nodes.",
            "So the tradeoff is that you could do one big forward search from the initial belief or several small four search from each belief from each node, and in general it will be better to just with from.",
            "Each note and this is what this graph shows here."
        ],
        [
            "OK, the second problem that I'm going to show you doesn't have a long sequence of actions that we should not deviate from, but on the other hand it has a lot of small local Optima.",
            "You can see here that I actually it's an alternate technique that works best, and then we've got the two variants of four search and random restarts."
        ],
        [
            "OK. We tested this on 6 problems and we want to compare as well with other approaches that work on with controllers.",
            "So we included here.",
            "Source of bias about Idpol situation plus escape.",
            "Q SLP is quadratically constrained.",
            "Linear programming.",
            "PBS LS is stochastic linear, sarcastic, global search.",
            "We've got our four switch technique and old splitting.",
            "The highlight is that forward search OK was best in two problems and came close for the other ones.",
            "We can see how good would be the alcohol policy by looking at an upper bound that was computed based on.",
            "I think it was HSV I2 as well as the policy found by source up.",
            "If we just let it run for 100,000 seconds.",
            "OK, so it means that the alcohol policy is somewhere in between those values here.",
            "So when they match then we know what is the optimal value.",
            "OK, in Parathesis we've got the size of the controller and all the techniques that are in light blue here are generally meant to work on small controller, so they'll be fairly good in that sense, so I suppose I should point based approach so it's not meant person to look for the smallest representation, But I think it's included in since it's one of the leading techniques and here we can see that it works well as long as you just let it happen.",
            "A lot of Alpha factors that are essentially the same as the number of nodes in a controller, but if we restrict that too small number of awful vectors, just like all the other techniques, then it doesn't work as well.",
            "No spitting did best on tree problems, but it actually failed miserably on two of the problems that actually had some local Optima that required more than a two step look ahead."
        ],
        [
            "OK, so to conclude what I talked about today is how we can allies on the local Optima when we're using.",
            "Yeah for planning as inference and this analysis I tried to convey to you know what it means and how to interpret those local optimum.",
            "And with that.",
            "Then we also talked about two techniques to escape those local Optima and they each have their pros and cons.",
            "And our future work.",
            "Now that we can deal with local Optima, the next thing to do is to scale this.",
            "So in fact we had.",
            "I guess some issues sometimes with some other reviews where it would prefer us to scale things instead of.",
            "Deal with local optimal, but I view in general is that there's no point scaling something that produces garbage.",
            "It's better to just get good results 1st and then after that scale things.",
            "So that's our next step.",
            "And there's also a lot of interest in using a controller based approaches in decentralized company peaks, and there they have another type of local Optima issues of this analysis would not apply directly, but it would be interesting to extend it.",
            "That's it, thank you.",
            "OK questions for skull.",
            "So, but the.",
            "Instead of waiting to do something.",
            "So after we get something, sure.",
            "So yes, we we could do this.",
            "So in fact, as I mentioned, any interest algorithm could be used.",
            "When I sing I guess is that EM is a deterministic algorithm, so it's easier to analyze.",
            "Give something being sarcastic with me a lot harder Twilight.",
            "But yeah, so other people have use particle filtration, Gibbs sampling to do the inference, especially in the case of continuous problems.",
            "Escape the problem.",
            "Unfortunately, I don't see how we could like.",
            "I think for each algorithm you have to remove the analysis because the conditions under which it gets stock may be different.",
            "Or questions?",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about analyzing an escaping local optimum planning as in French for partial observable domains.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is also joint work with copious flag and map to stand for the Free University of Berlin in Germany.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in this talk when I consider one particular approach of complaint planning that has become popular in recent years, which is pending as inference and we're going to focus on partial observable domain.",
                    "label": 0
                },
                {
                    "sent": "So that means we're going to look at palm DP.",
                    "label": 0
                },
                {
                    "sent": "So, partial observable Markov decision processes.",
                    "label": 0
                },
                {
                    "sent": "Now when we do planning as inference gauges are going to convert the palm DP into a mixture of that.",
                    "label": 1
                },
                {
                    "sent": "An invasion networks an.",
                    "label": 0
                },
                {
                    "sent": "As a result, planning can now be framed as simply.",
                    "label": 0
                },
                {
                    "sent": "Our problem of maximizing likelihood.",
                    "label": 0
                },
                {
                    "sent": "I'll explain that in details, but the key is that by doing this then it opens the door to using many interests.",
                    "label": 0
                },
                {
                    "sent": "Algorithms do planning, so this is all nice, except that the original problem, the original planning problem was non convex and doing this transformation actually doesn't change its nature, so it remains a nonconvex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So even though we have not many and Prince algorithms that can solve this, we will all suffer from local Optima as well.",
                    "label": 0
                },
                {
                    "sent": "I mean, machine learning this is also an issue, but in general and panning is a more severe issue, so that's why we're looking at that.",
                    "label": 1
                },
                {
                    "sent": "OK, so in this talk there will be 2 contributions, so the first one will be an analysis of EMS local optimize.",
                    "label": 0
                },
                {
                    "sent": "What I mean by an analysis is I'm going to interpret what it means to be stuck into local optimal in terms of time, so will give a meaning to this, and then there will be 2 escaped.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's the outline.",
                    "label": 0
                },
                {
                    "sent": "1st I'll just review quickly.",
                    "label": 0
                },
                {
                    "sent": "Water pump, tipis, and playing as inference.",
                    "label": 1
                },
                {
                    "sent": "Then we'll go into the meat of the subjects, will pull up to my interpretation.",
                    "label": 0
                },
                {
                    "sent": "The main thing here.",
                    "label": 0
                },
                {
                    "sent": "So if you don't understand the talk, or if you don't remember much, then the main thing to remember is that when you get stuck into a local optimum will show is that if we just correspond to doing it one step, look ahead.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what young guys.",
                    "label": 1
                },
                {
                    "sent": "And then I'll explain how to escape from local Optima using two techniques or for research and mold splitting, and then I'll show you some experience.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what is the palm DP?",
                    "label": 0
                },
                {
                    "sent": "So it's a regular Markov decision process where we don't get to observe this state, so we've got some observations here that are correlated with the States and I'll do this as part of the rain Forsman learning session.",
                    "label": 0
                },
                {
                    "sent": "Here we're really looking at a planning problem, so we're assuming that we know what are the transitions as well as the reward function and the observation function, so we know everything we don't have to get some samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so just need to optimize the costs.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so very quickly high is also optimization done.",
                    "label": 0
                },
                {
                    "sent": "Traditionally you would be fine, believes that our distributions over states because we don't get to observe them.",
                    "label": 0
                },
                {
                    "sent": "You could have made those beliefs by basophil trying.",
                    "label": 0
                },
                {
                    "sent": "So is it just based their own and then a policy with severe mapping from beliefs to actions and you could evaluate a policy simply by taking at discounted some of the expected reward and the best policy is obviously the one that will give you.",
                    "label": 0
                },
                {
                    "sent": "The highest expected sum of reward and a certificate to verify optimality is Bellman's equation, which is right here.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That being said, let's look now at one class of policies.",
                    "label": 0
                },
                {
                    "sent": "I find same controllers.",
                    "label": 0
                },
                {
                    "sent": "So fine.",
                    "label": 0
                },
                {
                    "sent": "Said controllers are very interesting because of their simplicity and it can be downloaded in very very small devices.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is that we've got a bunch of nodes, and for each node there's an action.",
                    "label": 0
                },
                {
                    "sent": "So if you know you execute that action, then you receive an observation that will tell you which edge to follow.",
                    "label": 0
                },
                {
                    "sent": "So if you get 01.",
                    "label": 0
                },
                {
                    "sent": "We go to this node, execute a one again and keep on going.",
                    "label": 0
                },
                {
                    "sent": "So it's really just a final state on that we executed.",
                    "label": 0
                },
                {
                    "sent": "So then the problem possible optimization in this context is really the problem.",
                    "label": 0
                },
                {
                    "sent": "Simply defining what would be the best action mapping and next node mapping.",
                    "label": 0
                },
                {
                    "sent": "In general, we're going to let those things be probabilistic, so we're going to have conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in 1999, Michelangelo and some coauthors actually realize that we could fold this controller type of policy into the graphical model of funding.",
                    "label": 0
                },
                {
                    "sent": "So here I've got again the graphical model that I showed you a few slides ago.",
                    "label": 0
                },
                {
                    "sent": "I can augment it now with one more set of variables labeled MIND correspond to the node of the controller that the agent will be in at any point in time.",
                    "label": 0
                },
                {
                    "sent": "And now the policy is re encoded by those red marks simply because the choice of the action is conditional on the current node and the next node that you would end up in this condition on their current note as well as the observation received.",
                    "label": 0
                },
                {
                    "sent": "OK, so now you can see where we're going.",
                    "label": 0
                },
                {
                    "sent": "So in terms of I guess we planning as inference ideas that were slowly starting to convert from deep into it a regular dynamic Bayesian network where the policy is is essentially some initial distributions as.",
                    "label": 0
                },
                {
                    "sent": "Find a vision network.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Then in 2006 Mac to say propose that since we've got this view, perhaps you could simply ask how we could find what is the best conditional distribution in this graphical model that would maximize our expected reward.",
                    "label": 0
                },
                {
                    "sent": "But let's try to do this in a way that would really have a dynamic Bayesian network as a problem that we still have some rewards here that are utilities are not random variables and we've got some discount.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Factor, so one idea is that we could simply normalize the rewards so the rewards normally are any real number, and if we simply re normalize by subtracting the minimum reward and dividing by the range, then everything will be between zero and one and that can be interpreted as a probability.",
                    "label": 0
                },
                {
                    "sent": "And now we can make the reward just be some binary variables with probability that's proportional to the original reward function.",
                    "label": 0
                },
                {
                    "sent": "So what's left now is how to deal with the discount factor, and so this is where.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we could do is simply introduce a mixture of an invasion networks simply because rewards need to be added so mixed chairs or mixture models or traditionally falls that are decompose additively, so this makes sense here, and each dynamic Bayesian network is going to have probability that's proportional to the discount factor race to the number of time steps that's included in that dynamic vision network.",
                    "label": 0
                },
                {
                    "sent": "So now I gotta conversion where everything is essentially proportional.",
                    "label": 0
                },
                {
                    "sent": "And now the value of a policy is nothing more than simply the probability that our bar would be equal to 1.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so given that now, finding an optimal policy can be phrased as an inference problem, where we would simply find what is what are the initial distributions here that would maximize the probability that power bar is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Now how do we do this?",
                    "label": 0
                },
                {
                    "sent": "There's lots of algorithms as I mentioned before, but let's start with the simplest one expectation maximization.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem with any of the algorithms including yeah, is that they're going to get stuck into some local Optima, and So what I'm going to do now is go through analysis an essentially showing you that what PM ensures is just one step lookahead optimality.",
                    "label": 1
                },
                {
                    "sent": "And based on that, then the natural thing to do if you want to escape is to look at multiple step to look ahead.",
                    "label": 0
                },
                {
                    "sent": "An another approach would be to split some goals.",
                    "label": 0
                },
                {
                    "sent": "So I say these things in more details.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's go into the details of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it is going to be a few equations, but I'll get back to some high level ideas in a minute if.",
                    "label": 0
                },
                {
                    "sent": "No, you're not comfortable with Visa.",
                    "label": 0
                },
                {
                    "sent": "OK, so M&R context here is used to update the powders, and here I've got the distribution over actions for each shell and the distribution over next nodes given the current hold, an observation, these parameters are only updated in a way that we multiply their current setting by some expression.",
                    "label": 0
                },
                {
                    "sent": "So we do in multiplicative update.",
                    "label": 0
                },
                {
                    "sent": "That's 1:00 PM does an if we're starting to local optimum.",
                    "label": 0
                },
                {
                    "sent": "Well, it means that the multiplicative update here won't change what those values are so far.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, we can.",
                    "label": 0
                },
                {
                    "sent": "Formally the first theorem then essentially says that if we've got a policy that's a stable fixed point of VM, then those two conditions have to hold and essentially specifying webtools expr.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions that I had in the previous high school G here engage here when.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe like an essentially they must be equal to the largest possible value that they can take for any action here and for any next node here.",
                    "label": 0
                },
                {
                    "sent": "OK, so then itself is not too interesting.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks, but now let's see how we can use that to really understand what it means to be stuck into local Optima.",
                    "label": 0
                },
                {
                    "sent": "So I've got the equations for YM right here and also global optimality as specified by dominance equation.",
                    "label": 1
                },
                {
                    "sent": "So most here are nothing more than balance equation that's been split in two in the context of controllers.",
                    "label": 0
                },
                {
                    "sent": "So when this holds, I know that I've got an optimal policy and when this holds here I know that I'm stuck.",
                    "label": 0
                },
                {
                    "sent": "If I'm using him.",
                    "label": 0
                },
                {
                    "sent": "Now these equations are actually very similar, but you've got some differences.",
                    "label": 0
                },
                {
                    "sent": "So in green I've circled samples differences and you'll notice that here I've got some terms that are bounded functions and hear some terms that are bad.",
                    "label": 0
                },
                {
                    "sent": "So what are those bad as if I?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back to my slide here for the details of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, beta is nothing more than the backward term that you would compute in India.",
                    "label": 0
                },
                {
                    "sent": "Now it wasn't clear to us what it meant, but from those.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quations actually we can show that were better really is just as rescaled version of the value function, so you can actually interpret the backward terms in EM as just the value function.",
                    "label": 0
                },
                {
                    "sent": "So now because it's a rescaled version, it doesn't change anything, because here we've got a Max and here in our maximum when you maximized, if you just re scale things by a constant factor, it doesn't change anything.",
                    "label": 0
                },
                {
                    "sent": "OK, the next differences that here I'm going to copy that are very close one and here with the reward function as we explained before.",
                    "label": 0
                },
                {
                    "sent": "These things are just rescaled version of each other, so again it doesn't change anything.",
                    "label": 0
                },
                {
                    "sent": "The last differences are that have got Alpha and some beliefs how far corresponds to the forward terms in EM and those bees are beliefs or or distributions over states.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the forward terms any M can be interpreted as the occupancy frequencies of the controller and if you re normalize these things to sum up to one, they correspond to beliefs.",
                    "label": 0
                },
                {
                    "sent": "So it looks like, again, there's no difference, but it turns out that here for global optimality those equations have to hold for every belief, and actually we know that if we have an optimal policy then we should be executing the optimal thing in every single belief right?",
                    "label": 0
                },
                {
                    "sent": "Whereas when TM says is that you're going to be picking the best action only in the beliefs that are proportional to the occupancy frequencies of the nodes of the controller.",
                    "label": 0
                },
                {
                    "sent": "So it's really just answering.",
                    "label": 0
                },
                {
                    "sent": "Optimality with their respective subset of the movie or another way to interpret this is that because it's only with respect to the nose of the controllers that we're really just looking at one step ahead optimality.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you got a second theorem, that essentially confirms that the condition that I had before, or indeed the ones that are necessary but not sufficient for global optimality an as I explained, we can interpret that as just saying that PM ensures once that look ahead of now.",
                    "label": 1
                },
                {
                    "sent": "Alright, so based on that now we can look at escaped techniques.",
                    "label": 0
                },
                {
                    "sent": "The first one will simply generalize this one step to multiple step.",
                    "label": 0
                },
                {
                    "sent": "In a second.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amazon spare so for multi step.",
                    "label": 0
                },
                {
                    "sent": "When we do is we look at every node in our control room.",
                    "label": 0
                },
                {
                    "sent": "So let's take this one for example.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is just look ahead search meaning that I simulate all possible trajectories of actions and observations and check for each belief is.",
                    "label": 0
                },
                {
                    "sent": "Or am I picking the optimal action or not?",
                    "label": 0
                },
                {
                    "sent": "If I realize that there's a belief where I'm suboptimal, then all I have to do is add some notes to the controller.",
                    "label": 0
                },
                {
                    "sent": "And corresponds to the path where I found something suboptimal.",
                    "label": 0
                },
                {
                    "sent": "And you see, normally I am only ensures one step lookahead optimality from each belief that's proportional to the occupancy frequencies of each node.",
                    "label": 0
                },
                {
                    "sent": "But now we're going to do this search sign for each node, but for multiple steps right?",
                    "label": 0
                },
                {
                    "sent": "And so in the limit, we can guarantee you know global optimality if we go far enough in the search.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the second approach is actually inspired by some work with respect to hidden Markov models, where the idea is that we could simply pay each node one at a time and see if we could split it and perhaps re optimize the powers of the new nodes in such a way that perhaps we're going to improve the controller.",
                    "label": 0
                },
                {
                    "sent": "And you can actually interpret this as looking for.",
                    "label": 0
                },
                {
                    "sent": "I guess the optimal two step.",
                    "label": 0
                },
                {
                    "sent": "Look ahead that would be possible from the beliefs that are normally encountered in that node.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "So different approach, but it has its advantages in the sense that it will generally be optimal to expect the two step look ahead.",
                    "label": 0
                },
                {
                    "sent": "And it's also fairly simple, because we're simply just reusing them as well.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's see how they compare first in terms of complexity.",
                    "label": 0
                },
                {
                    "sent": "So M by itself the complexity with respect to center, the controller with quadratic.",
                    "label": 0
                },
                {
                    "sent": "Now if we include node splitting to get out of local Optima, the complexity becomes quiet.",
                    "label": 1
                },
                {
                    "sent": "If an intuition here is that for every node I'm going to try to do a split and then the split means that I'm going to run them so and I'm gonna have to do this as as I wrote a controller, so there's an additional quadratic term that presents as a result of that.",
                    "label": 0
                },
                {
                    "sent": "So what it means that no spinning is going to have a hard time to scale.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With respect.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With respect to the number of nodes in the controller.",
                    "label": 0
                },
                {
                    "sent": "Now for four search we have better complexity with respect to the side of the controller, but then we have to do a search and that's exponential with respected in depth.",
                    "label": 0
                },
                {
                    "sent": "So that could be bad.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I mean a lot of people have been looking recently at doing forward search for online techniques, and there are ways of I guess need to be mitigating this complexity by doing a branch and bound or sampling etc.",
                    "label": 0
                },
                {
                    "sent": "So here I'm just showing all the worst case complexity.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now, in practice we tested this on 6 problems.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you some graphs where two of them at a table.",
                    "label": 0
                },
                {
                    "sent": "I'm in the first problem here.",
                    "label": 0
                },
                {
                    "sent": "Cheese taxi.",
                    "label": 0
                },
                {
                    "sent": "This is a benchmark where the optimal policy actually has a sequence of action that has to be executed exactly, and if you give me just a little bit from it, then you're going to have you're going to miss the goal and have very little reward.",
                    "label": 0
                },
                {
                    "sent": "As a result, most of the techniques that are doing some kind of gradient descent or policy search are subject to local Optima or just going to fail because they have to go through a Canyon before they reach that.",
                    "label": 0
                },
                {
                    "sent": "Precise sequence of actions and the never get to find the optimal pause.",
                    "label": 0
                },
                {
                    "sent": "So here when we see is that four search eventually manages to escape, whereas no splitting an random restart is just stuck no matter what.",
                    "label": 0
                },
                {
                    "sent": "In this graph I have two versions of Forward Search.",
                    "label": 0
                },
                {
                    "sent": "One is 4 search from each one of the nodes an otherwise the auto type.",
                    "label": 0
                },
                {
                    "sent": "Of course we could do that is the obvious one with me to just to afford charge from the initial belief as opposed to each one of the nodes.",
                    "label": 0
                },
                {
                    "sent": "So the tradeoff is that you could do one big forward search from the initial belief or several small four search from each belief from each node, and in general it will be better to just with from.",
                    "label": 0
                },
                {
                    "sent": "Each note and this is what this graph shows here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the second problem that I'm going to show you doesn't have a long sequence of actions that we should not deviate from, but on the other hand it has a lot of small local Optima.",
                    "label": 0
                },
                {
                    "sent": "You can see here that I actually it's an alternate technique that works best, and then we've got the two variants of four search and random restarts.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. We tested this on 6 problems and we want to compare as well with other approaches that work on with controllers.",
                    "label": 0
                },
                {
                    "sent": "So we included here.",
                    "label": 0
                },
                {
                    "sent": "Source of bias about Idpol situation plus escape.",
                    "label": 0
                },
                {
                    "sent": "Q SLP is quadratically constrained.",
                    "label": 0
                },
                {
                    "sent": "Linear programming.",
                    "label": 0
                },
                {
                    "sent": "PBS LS is stochastic linear, sarcastic, global search.",
                    "label": 0
                },
                {
                    "sent": "We've got our four switch technique and old splitting.",
                    "label": 0
                },
                {
                    "sent": "The highlight is that forward search OK was best in two problems and came close for the other ones.",
                    "label": 0
                },
                {
                    "sent": "We can see how good would be the alcohol policy by looking at an upper bound that was computed based on.",
                    "label": 0
                },
                {
                    "sent": "I think it was HSV I2 as well as the policy found by source up.",
                    "label": 0
                },
                {
                    "sent": "If we just let it run for 100,000 seconds.",
                    "label": 0
                },
                {
                    "sent": "OK, so it means that the alcohol policy is somewhere in between those values here.",
                    "label": 0
                },
                {
                    "sent": "So when they match then we know what is the optimal value.",
                    "label": 0
                },
                {
                    "sent": "OK, in Parathesis we've got the size of the controller and all the techniques that are in light blue here are generally meant to work on small controller, so they'll be fairly good in that sense, so I suppose I should point based approach so it's not meant person to look for the smallest representation, But I think it's included in since it's one of the leading techniques and here we can see that it works well as long as you just let it happen.",
                    "label": 0
                },
                {
                    "sent": "A lot of Alpha factors that are essentially the same as the number of nodes in a controller, but if we restrict that too small number of awful vectors, just like all the other techniques, then it doesn't work as well.",
                    "label": 0
                },
                {
                    "sent": "No spitting did best on tree problems, but it actually failed miserably on two of the problems that actually had some local Optima that required more than a two step look ahead.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to conclude what I talked about today is how we can allies on the local Optima when we're using.",
                    "label": 0
                },
                {
                    "sent": "Yeah for planning as inference and this analysis I tried to convey to you know what it means and how to interpret those local optimum.",
                    "label": 0
                },
                {
                    "sent": "And with that.",
                    "label": 0
                },
                {
                    "sent": "Then we also talked about two techniques to escape those local Optima and they each have their pros and cons.",
                    "label": 0
                },
                {
                    "sent": "And our future work.",
                    "label": 0
                },
                {
                    "sent": "Now that we can deal with local Optima, the next thing to do is to scale this.",
                    "label": 1
                },
                {
                    "sent": "So in fact we had.",
                    "label": 0
                },
                {
                    "sent": "I guess some issues sometimes with some other reviews where it would prefer us to scale things instead of.",
                    "label": 0
                },
                {
                    "sent": "Deal with local optimal, but I view in general is that there's no point scaling something that produces garbage.",
                    "label": 0
                },
                {
                    "sent": "It's better to just get good results 1st and then after that scale things.",
                    "label": 0
                },
                {
                    "sent": "So that's our next step.",
                    "label": 0
                },
                {
                    "sent": "And there's also a lot of interest in using a controller based approaches in decentralized company peaks, and there they have another type of local Optima issues of this analysis would not apply directly, but it would be interesting to extend it.",
                    "label": 0
                },
                {
                    "sent": "That's it, thank you.",
                    "label": 0
                },
                {
                    "sent": "OK questions for skull.",
                    "label": 0
                },
                {
                    "sent": "So, but the.",
                    "label": 0
                },
                {
                    "sent": "Instead of waiting to do something.",
                    "label": 0
                },
                {
                    "sent": "So after we get something, sure.",
                    "label": 0
                },
                {
                    "sent": "So yes, we we could do this.",
                    "label": 0
                },
                {
                    "sent": "So in fact, as I mentioned, any interest algorithm could be used.",
                    "label": 0
                },
                {
                    "sent": "When I sing I guess is that EM is a deterministic algorithm, so it's easier to analyze.",
                    "label": 0
                },
                {
                    "sent": "Give something being sarcastic with me a lot harder Twilight.",
                    "label": 0
                },
                {
                    "sent": "But yeah, so other people have use particle filtration, Gibbs sampling to do the inference, especially in the case of continuous problems.",
                    "label": 0
                },
                {
                    "sent": "Escape the problem.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I don't see how we could like.",
                    "label": 0
                },
                {
                    "sent": "I think for each algorithm you have to remove the analysis because the conditions under which it gets stock may be different.",
                    "label": 0
                },
                {
                    "sent": "Or questions?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}