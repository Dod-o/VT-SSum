{
    "id": "ok52rarfe36gx7g5gl25jzp4jzdcphmo",
    "title": "Microphone Array Driven Speech Recognition: Influence of Localization on the Word Error Rate",
    "info": {
        "author": [
            "Matthias Wolfel, University of Karlsruhe"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "June 2005",
        "category": [
            "Top->Computer Science->Speech Analysis"
        ]
    },
    "url": "http://videolectures.net/mlmi04uk_wolfel_ilwer/",
    "segmentation": [
        [
            "Then the work which I have done together with mankind, nickel and John McDonnell.",
            "So we just heard the title.",
            "So I will keep on going.",
            "So here we see the basic layout from the chilled seminar room which we have in classroom which we evaluated on just."
        ],
        [
            "Like Stanford maybe did before.",
            "So what we have here we have basically the speaker area, but the white board.",
            "We have four cameras in every corner, which are basically all located through the speaker position.",
            "We have sweet tabletop microphones which are not used for for our evolution, which I presented here.",
            "Then we have 4T shaped arrays.",
            "For the Speaker position estimates and from the audio, we only used this.",
            "We erase here.",
            "Of course we.",
            "We would get that the speaker mostly facing the audience and not facing the whiteboard.",
            "You also had a close talking microphone for this speaker and we had a Mark 3 microphone away provided blindness."
        ],
        [
            "On this lights usysa components.",
            "So here we can see the use sensors.",
            "From the teacher to raise, we basically extracted the audio features from the cameras.",
            "We extracted the video features and diffuse basically the features with the particle particle filter for person tracking.",
            "So from the particular they basically got the location.",
            "And with these location which was fitted into the beamforming, basically steered or beam to the estimated post position.",
            "And we used for the beamforming.",
            "We here use the mark three away, which is 64 channels.",
            "It should give some some better signature races than using just like 4 microphones for example.",
            "And after the beamforming, Jesse the feet in the output from the beam from into our ASR system and then we got our text typos, so hoping for bringing the sampling rate.",
            "Here is 44.1 kilohertz with 24 bits and in the ISS system we have downsampled to 16 kilohertz and 16 bit.",
            "So, as I mentioned before, we have used the particular filter for our data Fusion."
        ],
        [
            "So in particular, Fitzer represents an unknown probability density function by a set of random samples.",
            "Each particular represents a position and has an associated weight.",
            "And.",
            "The evolution of the particular set is a two stage process which is guided by the observation and the motion model model.",
            "So the first step would be the prediction step.",
            "So on your old particles and the weights.",
            "So you just take the particles at high rates and you get a new set of weights.",
            "You have a basically a Goshen estimation noise, so you get some new particles which are close to the very likely particulars before and then you the measuring step.",
            "The second step will basically give new ways to this newly generated particles.",
            "For.",
            "The audio features we."
        ],
        [
            "Had the assumption that the speaker is basically the person who's speaking most of the time, but the audience is being quiet.",
            "To get our solid public values, we have calculated first the time delay of arrival from every particle.",
            "So you have a particular in the room and you estimated time delay of arrival through every microphone.",
            "Then we have determined the value using the phase transformation.",
            "So for the feeding in the time delay of arrival.",
            "So for our particular time did arrival we have looked at the amplitude basically from the phase transformation.",
            "And if the value would be below 3, we just dismissed it.",
            "And we repeat this process, overall microphone pairs, and thereafter we normalize it.",
            "You get like a sort of probability value.",
            "He"
        ],
        [
            "You can see the.",
            "For camera fuse.",
            "So this is speaker.",
            "We see that the speaker is standing and it's also moving like so something on the slides for example.",
            "We usually had no laser pointer, so it's a lot of people going through this like 2% something for example.",
            "And we have the audience, which are usually sitting.",
            "And of course moving less."
        ],
        [
            "So these are also our true assumptions.",
            "To get the video features so the lecture is standing and moving while the audio audio audience is basically sitting and not moving a lot.",
            "So there we have the two problems.",
            "We have to locate the lecturer and we have to.",
            "Distinguish the lecture from the audience.",
            "So we have to look for features which are more specific for the lecturer then for the audience.",
            "So he had to get out."
        ],
        [
            "Sort of like public value for the particular we calculated two different features.",
            "One was based on dynamic foreground segmentation and the second one was.",
            "Basically a face detector and upper bodies detector which were concatenated.",
            "And then these this dynamic urban segmentations and the detectors were basically combined.",
            "So you have using here the weights in the fourth point.",
            "Dynamic programming, segmentation had the weight of 0.3 and the detectors.",
            "Had a rate of 0.7 so this was repeated over all four cameras and then we again normalized to have the probability stick value."
        ],
        [
            "So here we we.",
            "We seen this equation how we basically propagated then you particulars.",
            "In this space.",
            "So we just use the Goshen distribution.",
            "And here we see how to to combine basically the audio and video features.",
            "So this part basically stands for the the audio input and the second part stands for the video input and we had Alpha is basically the weight from waiting the audio and video features which was set to zero point.",
            "4 Here so the video input is a little bit more important than the audio input.",
            "And so we repeated this process for all particles and later on we just combined all the particles.",
            "So in our experience we have used the number of 300 for the particulars.",
            "So here we have a video of the tree."
        ],
        [
            "Same procedure, so here we have one speaker presenting.",
            "We have the box basically indicates the 3D head position.",
            "On the right we have the different particles, so if you have a bright particular, it means it has a high VAD, so it means it's very likely that there will be a speaker precision, and if you have a red value, I hope you can see it.",
            "It's the weight is not that high.",
            "On this image we basically have the our detectors, so the green and churches Bobs is basically a face detector.",
            "So one is a front detector and one is a side face detector and the Big Blue box is another body detector and on this image we have the foreground, the segmentation.",
            "So the the box basically indicates the speaker position.",
            "So."
        ],
        [
            "Already come to my results.",
            "So here we have used the average or Canadian distance because the distance measure how, how good we are and we have compared the audio only video only.",
            "And then we've used those informations.",
            "So on this Columbia, see that we have evaluated overall slides.",
            "So you see that the audio has every chair from 46.1%, and if you use only speech frames, not surprisingly, we have a better estimate because this particular feature is losing track of the speakers.",
            "Stop speaking and then he moves through, not different position.",
            "So for this effect is not happening for video features because of the the speaker is always in the field from the cameras, so sometimes it can happen that basically 1 camera is not seeing the speaker because he's he's hidden somewhere from and maybe if he's standing in a corner.",
            "But in general we have at least two hours to be infused.",
            "We can follow and track the speaker.",
            "And combining the video with the audio with the betting vector of 0.4 for the audio as I have mentioned before, we see that we have an additional gain and for overall frenzy have an average error from 30.5 and four only speech frames, which are of course more interesting for the speech recognition.",
            "We have even a lower average air from 29.1%.",
            "Centimeters thanks.",
            "So on this slide we have."
        ],
        [
            "Generated the.",
            "Word error rates.",
            "So we be used common speech recognition system, basically similar to the ones presented before by the other speakers.",
            "So it's a state of the art system.",
            "We use adaptation.",
            "VTL NMLPMLR.",
            "And the training or T as well as we ended ours.",
            "So basically the meeting system plus broadcast news and the far distance model where meteor be adapted to far distance.",
            "Microphones so.",
            "Just using one single microphone from the Mark 3 microphone array, we haven't heard a rate of 66.5%.",
            "If you use the estimated position from the audio features only, we have an estimate and word error rate from 59.8% with the video features only, it's 59.1.",
            "The estimated position using both modalities is 58.4 and the labeled position which.",
            "Should be like the groundtruth hasn't got a rate of 55.8% and we also have evaluated the close talking microphone, so this performance is 34.0%.",
            "So that the evolution is done on the chill evolution data from 2005 January set as presented by Steve before.",
            "So to make the relationship between."
        ],
        [
            "The average error in the range of word error rate.",
            "More apparent.",
            "We have basically plotted the the average error through the word error rate and you can see that basically it's a nearly linear relationship, so the label positions.",
            "So here it's plotted this 15 butts enabled positions are like.",
            "Our guess would be the the error from the hand labeled positions is around like 10 centimeters, so he may be here.",
            "So we see that there's a nearly linear relationship, and that we actually gain invert error rate.",
            "Having a better speaker estimation.",
            "So that was my last night."
        ],
        [
            "So.",
            "Are you just tracking that?",
            "Separate results for the question and answer.",
            "No, this is only on the on the lecture itself, so we had a question answer after like.",
            "Now what we have are so so we have only taken the first part.",
            "No, we have not touched the data actually.",
            "In your positioning, how did you initialize positions?",
            "It's just basically randomly, so you know the rule geometry.",
            "So you basically you just read all the 300 particles over the room and then you start.",
            "So basically in the first we might have just 10 particulars, which might have a good estimate and then basically run please 10 particles around this area basically generate this 300 new particulars and then basically after some interventions.",
            "It's basically following the speaker.",
            "OK well thanks again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the work which I have done together with mankind, nickel and John McDonnell.",
                    "label": 1
                },
                {
                    "sent": "So we just heard the title.",
                    "label": 0
                },
                {
                    "sent": "So I will keep on going.",
                    "label": 0
                },
                {
                    "sent": "So here we see the basic layout from the chilled seminar room which we have in classroom which we evaluated on just.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like Stanford maybe did before.",
                    "label": 0
                },
                {
                    "sent": "So what we have here we have basically the speaker area, but the white board.",
                    "label": 0
                },
                {
                    "sent": "We have four cameras in every corner, which are basically all located through the speaker position.",
                    "label": 0
                },
                {
                    "sent": "We have sweet tabletop microphones which are not used for for our evolution, which I presented here.",
                    "label": 0
                },
                {
                    "sent": "Then we have 4T shaped arrays.",
                    "label": 0
                },
                {
                    "sent": "For the Speaker position estimates and from the audio, we only used this.",
                    "label": 0
                },
                {
                    "sent": "We erase here.",
                    "label": 0
                },
                {
                    "sent": "Of course we.",
                    "label": 0
                },
                {
                    "sent": "We would get that the speaker mostly facing the audience and not facing the whiteboard.",
                    "label": 0
                },
                {
                    "sent": "You also had a close talking microphone for this speaker and we had a Mark 3 microphone away provided blindness.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On this lights usysa components.",
                    "label": 0
                },
                {
                    "sent": "So here we can see the use sensors.",
                    "label": 0
                },
                {
                    "sent": "From the teacher to raise, we basically extracted the audio features from the cameras.",
                    "label": 1
                },
                {
                    "sent": "We extracted the video features and diffuse basically the features with the particle particle filter for person tracking.",
                    "label": 1
                },
                {
                    "sent": "So from the particular they basically got the location.",
                    "label": 0
                },
                {
                    "sent": "And with these location which was fitted into the beamforming, basically steered or beam to the estimated post position.",
                    "label": 0
                },
                {
                    "sent": "And we used for the beamforming.",
                    "label": 0
                },
                {
                    "sent": "We here use the mark three away, which is 64 channels.",
                    "label": 0
                },
                {
                    "sent": "It should give some some better signature races than using just like 4 microphones for example.",
                    "label": 0
                },
                {
                    "sent": "And after the beamforming, Jesse the feet in the output from the beam from into our ASR system and then we got our text typos, so hoping for bringing the sampling rate.",
                    "label": 0
                },
                {
                    "sent": "Here is 44.1 kilohertz with 24 bits and in the ISS system we have downsampled to 16 kilohertz and 16 bit.",
                    "label": 0
                },
                {
                    "sent": "So, as I mentioned before, we have used the particular filter for our data Fusion.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in particular, Fitzer represents an unknown probability density function by a set of random samples.",
                    "label": 1
                },
                {
                    "sent": "Each particular represents a position and has an associated weight.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The evolution of the particular set is a two stage process which is guided by the observation and the motion model model.",
                    "label": 1
                },
                {
                    "sent": "So the first step would be the prediction step.",
                    "label": 0
                },
                {
                    "sent": "So on your old particles and the weights.",
                    "label": 0
                },
                {
                    "sent": "So you just take the particles at high rates and you get a new set of weights.",
                    "label": 0
                },
                {
                    "sent": "You have a basically a Goshen estimation noise, so you get some new particles which are close to the very likely particulars before and then you the measuring step.",
                    "label": 0
                },
                {
                    "sent": "The second step will basically give new ways to this newly generated particles.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "The audio features we.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Had the assumption that the speaker is basically the person who's speaking most of the time, but the audience is being quiet.",
                    "label": 1
                },
                {
                    "sent": "To get our solid public values, we have calculated first the time delay of arrival from every particle.",
                    "label": 0
                },
                {
                    "sent": "So you have a particular in the room and you estimated time delay of arrival through every microphone.",
                    "label": 0
                },
                {
                    "sent": "Then we have determined the value using the phase transformation.",
                    "label": 0
                },
                {
                    "sent": "So for the feeding in the time delay of arrival.",
                    "label": 1
                },
                {
                    "sent": "So for our particular time did arrival we have looked at the amplitude basically from the phase transformation.",
                    "label": 1
                },
                {
                    "sent": "And if the value would be below 3, we just dismissed it.",
                    "label": 0
                },
                {
                    "sent": "And we repeat this process, overall microphone pairs, and thereafter we normalize it.",
                    "label": 1
                },
                {
                    "sent": "You get like a sort of probability value.",
                    "label": 0
                },
                {
                    "sent": "He",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see the.",
                    "label": 0
                },
                {
                    "sent": "For camera fuse.",
                    "label": 0
                },
                {
                    "sent": "So this is speaker.",
                    "label": 0
                },
                {
                    "sent": "We see that the speaker is standing and it's also moving like so something on the slides for example.",
                    "label": 0
                },
                {
                    "sent": "We usually had no laser pointer, so it's a lot of people going through this like 2% something for example.",
                    "label": 0
                },
                {
                    "sent": "And we have the audience, which are usually sitting.",
                    "label": 0
                },
                {
                    "sent": "And of course moving less.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are also our true assumptions.",
                    "label": 0
                },
                {
                    "sent": "To get the video features so the lecture is standing and moving while the audio audio audience is basically sitting and not moving a lot.",
                    "label": 1
                },
                {
                    "sent": "So there we have the two problems.",
                    "label": 0
                },
                {
                    "sent": "We have to locate the lecturer and we have to.",
                    "label": 0
                },
                {
                    "sent": "Distinguish the lecture from the audience.",
                    "label": 0
                },
                {
                    "sent": "So we have to look for features which are more specific for the lecturer then for the audience.",
                    "label": 1
                },
                {
                    "sent": "So he had to get out.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of like public value for the particular we calculated two different features.",
                    "label": 0
                },
                {
                    "sent": "One was based on dynamic foreground segmentation and the second one was.",
                    "label": 1
                },
                {
                    "sent": "Basically a face detector and upper bodies detector which were concatenated.",
                    "label": 0
                },
                {
                    "sent": "And then these this dynamic urban segmentations and the detectors were basically combined.",
                    "label": 0
                },
                {
                    "sent": "So you have using here the weights in the fourth point.",
                    "label": 0
                },
                {
                    "sent": "Dynamic programming, segmentation had the weight of 0.3 and the detectors.",
                    "label": 0
                },
                {
                    "sent": "Had a rate of 0.7 so this was repeated over all four cameras and then we again normalized to have the probability stick value.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we we.",
                    "label": 0
                },
                {
                    "sent": "We seen this equation how we basically propagated then you particulars.",
                    "label": 0
                },
                {
                    "sent": "In this space.",
                    "label": 0
                },
                {
                    "sent": "So we just use the Goshen distribution.",
                    "label": 0
                },
                {
                    "sent": "And here we see how to to combine basically the audio and video features.",
                    "label": 0
                },
                {
                    "sent": "So this part basically stands for the the audio input and the second part stands for the video input and we had Alpha is basically the weight from waiting the audio and video features which was set to zero point.",
                    "label": 0
                },
                {
                    "sent": "4 Here so the video input is a little bit more important than the audio input.",
                    "label": 0
                },
                {
                    "sent": "And so we repeated this process for all particles and later on we just combined all the particles.",
                    "label": 1
                },
                {
                    "sent": "So in our experience we have used the number of 300 for the particulars.",
                    "label": 0
                },
                {
                    "sent": "So here we have a video of the tree.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Same procedure, so here we have one speaker presenting.",
                    "label": 0
                },
                {
                    "sent": "We have the box basically indicates the 3D head position.",
                    "label": 1
                },
                {
                    "sent": "On the right we have the different particles, so if you have a bright particular, it means it has a high VAD, so it means it's very likely that there will be a speaker precision, and if you have a red value, I hope you can see it.",
                    "label": 0
                },
                {
                    "sent": "It's the weight is not that high.",
                    "label": 0
                },
                {
                    "sent": "On this image we basically have the our detectors, so the green and churches Bobs is basically a face detector.",
                    "label": 0
                },
                {
                    "sent": "So one is a front detector and one is a side face detector and the Big Blue box is another body detector and on this image we have the foreground, the segmentation.",
                    "label": 0
                },
                {
                    "sent": "So the the box basically indicates the speaker position.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Already come to my results.",
                    "label": 0
                },
                {
                    "sent": "So here we have used the average or Canadian distance because the distance measure how, how good we are and we have compared the audio only video only.",
                    "label": 1
                },
                {
                    "sent": "And then we've used those informations.",
                    "label": 0
                },
                {
                    "sent": "So on this Columbia, see that we have evaluated overall slides.",
                    "label": 0
                },
                {
                    "sent": "So you see that the audio has every chair from 46.1%, and if you use only speech frames, not surprisingly, we have a better estimate because this particular feature is losing track of the speakers.",
                    "label": 0
                },
                {
                    "sent": "Stop speaking and then he moves through, not different position.",
                    "label": 0
                },
                {
                    "sent": "So for this effect is not happening for video features because of the the speaker is always in the field from the cameras, so sometimes it can happen that basically 1 camera is not seeing the speaker because he's he's hidden somewhere from and maybe if he's standing in a corner.",
                    "label": 0
                },
                {
                    "sent": "But in general we have at least two hours to be infused.",
                    "label": 0
                },
                {
                    "sent": "We can follow and track the speaker.",
                    "label": 0
                },
                {
                    "sent": "And combining the video with the audio with the betting vector of 0.4 for the audio as I have mentioned before, we see that we have an additional gain and for overall frenzy have an average error from 30.5 and four only speech frames, which are of course more interesting for the speech recognition.",
                    "label": 0
                },
                {
                    "sent": "We have even a lower average air from 29.1%.",
                    "label": 0
                },
                {
                    "sent": "Centimeters thanks.",
                    "label": 0
                },
                {
                    "sent": "So on this slide we have.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generated the.",
                    "label": 0
                },
                {
                    "sent": "Word error rates.",
                    "label": 0
                },
                {
                    "sent": "So we be used common speech recognition system, basically similar to the ones presented before by the other speakers.",
                    "label": 0
                },
                {
                    "sent": "So it's a state of the art system.",
                    "label": 0
                },
                {
                    "sent": "We use adaptation.",
                    "label": 0
                },
                {
                    "sent": "VTL NMLPMLR.",
                    "label": 0
                },
                {
                    "sent": "And the training or T as well as we ended ours.",
                    "label": 0
                },
                {
                    "sent": "So basically the meeting system plus broadcast news and the far distance model where meteor be adapted to far distance.",
                    "label": 0
                },
                {
                    "sent": "Microphones so.",
                    "label": 0
                },
                {
                    "sent": "Just using one single microphone from the Mark 3 microphone array, we haven't heard a rate of 66.5%.",
                    "label": 1
                },
                {
                    "sent": "If you use the estimated position from the audio features only, we have an estimate and word error rate from 59.8% with the video features only, it's 59.1.",
                    "label": 1
                },
                {
                    "sent": "The estimated position using both modalities is 58.4 and the labeled position which.",
                    "label": 1
                },
                {
                    "sent": "Should be like the groundtruth hasn't got a rate of 55.8% and we also have evaluated the close talking microphone, so this performance is 34.0%.",
                    "label": 0
                },
                {
                    "sent": "So that the evolution is done on the chill evolution data from 2005 January set as presented by Steve before.",
                    "label": 0
                },
                {
                    "sent": "So to make the relationship between.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The average error in the range of word error rate.",
                    "label": 1
                },
                {
                    "sent": "More apparent.",
                    "label": 1
                },
                {
                    "sent": "We have basically plotted the the average error through the word error rate and you can see that basically it's a nearly linear relationship, so the label positions.",
                    "label": 0
                },
                {
                    "sent": "So here it's plotted this 15 butts enabled positions are like.",
                    "label": 0
                },
                {
                    "sent": "Our guess would be the the error from the hand labeled positions is around like 10 centimeters, so he may be here.",
                    "label": 0
                },
                {
                    "sent": "So we see that there's a nearly linear relationship, and that we actually gain invert error rate.",
                    "label": 0
                },
                {
                    "sent": "Having a better speaker estimation.",
                    "label": 0
                },
                {
                    "sent": "So that was my last night.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Are you just tracking that?",
                    "label": 0
                },
                {
                    "sent": "Separate results for the question and answer.",
                    "label": 0
                },
                {
                    "sent": "No, this is only on the on the lecture itself, so we had a question answer after like.",
                    "label": 0
                },
                {
                    "sent": "Now what we have are so so we have only taken the first part.",
                    "label": 0
                },
                {
                    "sent": "No, we have not touched the data actually.",
                    "label": 0
                },
                {
                    "sent": "In your positioning, how did you initialize positions?",
                    "label": 0
                },
                {
                    "sent": "It's just basically randomly, so you know the rule geometry.",
                    "label": 0
                },
                {
                    "sent": "So you basically you just read all the 300 particles over the room and then you start.",
                    "label": 0
                },
                {
                    "sent": "So basically in the first we might have just 10 particulars, which might have a good estimate and then basically run please 10 particles around this area basically generate this 300 new particulars and then basically after some interventions.",
                    "label": 0
                },
                {
                    "sent": "It's basically following the speaker.",
                    "label": 0
                },
                {
                    "sent": "OK well thanks again.",
                    "label": 0
                }
            ]
        }
    }
}