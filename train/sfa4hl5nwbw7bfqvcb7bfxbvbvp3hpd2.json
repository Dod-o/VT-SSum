{
    "id": "sfa4hl5nwbw7bfqvcb7bfxbvbvp3hpd2",
    "title": "Optimization Algorithms in Machine Learning",
    "info": {
        "author": [
            "Stephen J. Wright, Computer Sciences Department, University of Wisconsin-Madison"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_wright_oaml/",
    "segmentation": [
        [
            "Thank you.",
            "Testing testing alright that works OK.",
            "Thank you Francis for that introduction and I really appreciate the opportunity to speak here.",
            "Plan on having a break in the middle so you can recover a little bit and I can recover.",
            "I do have a lot of material.",
            "I looked at Francis Box Tutorial from last year and 156 slides so I felt like I had to make an attempt.",
            "And as you can see I only got to 82 so it might only be half as good, but we'll see.",
            "But but there is a lot of stuff and I'm going to have to go over it pretty quickly.",
            "But my main aim is to really cover a lot of ground so that if you're interested you can get hold of the slides which are already on the web.",
            "If you go to the page on the NIPS website for this talk, you'll see a link to them, and so you can get them and you know go back to the parts that you're interested in.",
            "Also, I've got periodically I've got a list of five or six papers that pertain to each of the five things that I hope to talk about.",
            "And again, these are just certainly not meant to be exhaustive, but there are jumping off point if you want to go and learn more about a specific topic.",
            "OK, so by way of."
        ],
        [
            "Introduction I just wanted to say that optimization as a field it's been around since 6070 years, but it's really going through a very vital growing.",
            "Right now and a lot of the impetus is coming from application fields and in particular machine learning.",
            "We're getting tremendous cross fertilization between machine learning and optimization.",
            "Back in the old days away that optimization people had workers, they pick a standard paradigm like linear programming, quadratic programming.",
            "NLP to me that means nonlinear programming.",
            "I believe it means something different to some people here, energy programming.",
            "So people are sort of working algorithms and they'd write software and they throw the software over the fence and people would pick it up and use it on their application.",
            "But now nowadays the applications are often too big and too challenging and and two sort of highly structured to really take full advantage of that mode of working.",
            "And so there has to be a different way of doing research or application.",
            "People work more directly with optimization people.",
            "An essentially methods sort of customized methods are built up from a toolkit of algorithmic components, and that's happening over and over again and across the whole range of applications.",
            "So what that means is that it's important if you want to use optimization.",
            "It really helps to know about some of the fundamental aspects of algorithms so that when you come to actually as well.",
            "Of course as the demands of your application so that when you come to designing an algorithm, you sort of know how to put the pieces together in the right way, so I'm."
        ],
        [
            "Going to hope to cover these five topics.",
            "1st Order methods.",
            "By that I mean methods that use gradient information.",
            "Secondly, stochastic gradient methods.",
            "These are methods that don't even have first order information.",
            "They only have maybe an estimate of the gradient or a piece of the gradient part.",
            "Three sort of relates very much to Bulmans tutorial this morning.",
            "There's a lot of optimization problems where you actually want to sort of a structured solution, and the way you get that is to add a regularizer onto the objective that you're trying to minimize.",
            "And it's usually non smooth and like an L1 norm or something, so a little bit about that.",
            "I'll say a little bit about using higher order information.",
            "We actually try to use maybe secondary of information or estimates there of and that's that's useful too.",
            "And Lastly, there's a technique that people who do SVM have known all about for no more than about 15 years where, which is where you don't try to optimize overall of variables at once, but you break the problem into chunks and just optimize over a subset of variables.",
            "So the other five topics are.",
            "Some are longer than others.",
            "Maybe in the first part I'll get through the first 2 by."
        ],
        [
            "Yeah, 15 minutes from now.",
            "So OK, so in talking about first order methods I'm going to use a very simple, very restricted setting, but I'm going to have things much more general in this in mind, But basically for purposes of description, I'm going to assume that you're trying to minimize a smooth convex function.",
            "Not only is it convex, but its eigenvalues of the Hessian between the lower and upper bound, and sometimes I'll let the lower bound B0.",
            "The upper bound is LL will then be a Lipschitz constant on the gradient OK?",
            "If mu is positive, it means we're dealing with a strictly convex function, and this is the.",
            "More or less a standard definition of convexity.",
            "And functions like this have this sort of bowl shaped and their comparatively easy to optimize.",
            "Another quantity that keeps coming up in the description is this Kappa, which is sort of a measure of the conditioning of the problem, and it's simply the ratio of the largest to the smallest eigenvalues.",
            "And sometimes I'll just restrict attention to the simplest really simple case of a quadratic convex function F where the Hessian is this matrix A, which again has its eigenvalues between these two bounds.",
            "OK, so that's the setting."
        ],
        [
            "As I said, we actually have, you know, we're really interested in doing much more general problems in this, such as problems where F is non smooth, maybe continuous, but maybe doesn't have a derivative.",
            "Even sometimes we will.",
            "In fact, in a lot of applications it's really hard to evaluate.",
            "If, for example, if you're doing an SVM, you have to run through all the data just to get a value of F, and that clearly often is not desirable.",
            "Sometimes you won't be able to get the gradient, you'll only have it estimate.",
            "You might want to impose some sort of constraint on the unknowns.",
            "Or you might have a nonsmooth regularization term, which I've already mentioned, so when I'm describing algorithms here, I'm trying to describe mostly algorithms that can be extended to handle situations like this."
        ],
        [
            "OK, so here's maybe the simplest, most fundamental 1st order method of all, and that simply generates a sequence of iterates.",
            "XK, by moving along the negative gradient direction, so evaluate the gradient of F and take some step along that direction.",
            "So the big issue is how do you choose Alpha here?",
            "And in standard optimization, if you read my book and many other books, there are chapters on how you do the line search.",
            "So one way is you actually make some effort to find the minimizer of F. Along this direction, so you try to choose Alpha to actually minimize F along that direction.",
            "It's a lot of work.",
            "There are a lot of tricky methods.",
            "Sometimes it's appropriate, but it's usually it's pretty tricky to come up with a reliable, efficient method to do that, but it has been done.",
            "Another thing that almost everyone, probably you know a lot of you in this room have done is to pick some initial guess of Alpha and take that step and see if it decreases F, and if it doesn't, you just take half of that distance.",
            "And try again and keep going until you get one that works.",
            "Actually decreases the function, so that's easy to implement.",
            "But there's an even more trivial thing you can do and that is you don't even bother about testing to see if the function goes downhill.",
            "You just take some out, choose Alpha by some rule that maybe depends on some knowledge of the function, like you might have bounds on the upper, the the maximum eigenvalue of the Hessian, and also the minimum eigenvalue.",
            "And maybe you can formulate some rules based on those and maybe also based on.",
            "What iteration you currently at?",
            "And traditionally, analysis for these first 2.",
            "You can find him in the standard optimization literature.",
            "They can prove things like you'll eventually get to a minimum, and if you're dealing with a convex function, it will be the minimum.",
            "The analysis for three is quite has quite a different flavor and it focuses.",
            "It tends to focus more on not just on the fact of convergence, but also on the rate at which you converge.",
            "OK."
        ],
        [
            "So I'm not going to give too many more details, but I will about the 1st two.",
            "I'm going to talk more about the third one where you choose Alpha according to some simple rule.",
            "Now you can use Taylor's theorem.",
            "Taylor's theorem is a ubiquitous tool in analyzing nonlinear optimization methods, and you will learn Taylor series expansions in you know, first or second semester calculus.",
            "And that keeps coming up.",
            "And so here by just expanding along this."
        ],
        [
            "But you know, plugging this into F and expanding it as a function of Alpha and doing a bit of manipulation and using the fact that the Hessian is bounded above by L or the eigenvalues bounded by L, you can prove."
        ],
        [
            "That you get this kind of change in F as you take this step.",
            "And now if you set Alpha to be 1 / L for instance, you can just substitute that into there and you'll find that F actually decreases is guaranteed to decrease by at least this amount.",
            "OK, so this very simple choices step length is guaranteed to give an improvement in F. Now you can mess around with this a little bit using very elementary arguments, and in fact I had these arguments on the slide, but my students made me take him out, but it's very, very simple.",
            "You just sort of take some reciprocal Zanu some bounds and stuff, and you end up being able to show that after you've taken K steps, the error in the function value is bounded by some constant that depends on Ellen.",
            "Also, on the initial error that should be X star there.",
            "Sorry, divided by K + 1.",
            "So you get this famous 1 / K. One of a K type convergence behavior.",
            "All I've assumed here.",
            "I haven't assumed that we're dealing with this strongly convex function, just with a convex function where the upper bound L is known.",
            "So this is this is the classic 1 / K convergence.",
            "Right now you can do better than that if you know that the function is strongly convex if mu is positive, you can use a different step length somewhat different from.",
            "This might be even almost twice as long, and again by plugging into here and messing around with a little bit, you can get this kind of convergence rate.",
            "That the error this time the error in X not in not in F goes down at this sort of geometric rate.",
            "Now this thing in the parentheses is strictly less than one, and if Kappa is large, if you got an ill condition problem, it's only a little bit less than one, but it's still strictly less than one, and so this is actually a lot better than this, right?",
            "This is a sub linear convergence rate.",
            "This is geometric, so if you can get a geometric convergence rate, you should certainly.",
            "You know, go with that, but you've had to assume, or you've had to assume, strong convexity here."
        ],
        [
            "Now, so we've got 1 / K under the convexity assumption.",
            "Without strong convexity we can get 1 / K. In fact, Nesterov has this nice example in his book from 2004 that shows that that really the best you can do with a with a method that moves along the gradient direction is 1 / K ^2.",
            "Then I won't go too much into the details of this, except it's a simple quadratic function where the Hessian is tridiagonal Ann, and it turns out that if you start at zero and take the kinds of steps that I've mentioned.",
            "Even if you make really good choices of Alpha, you can only resolve at least one component of X at each iteration, and so after I say an over 2 steps, you're only the best you can hope for is to get the first N / 2 components right?",
            "So you're always going to have error in the last.",
            "You know, you know the ones you haven't touched yet, and you can show it for this example that the difference between F&F star after K iterations is at least some multiple of 1 / K ^2.",
            "And so you can't really do better than K squared.",
            "So in a sense, this is kind of a speed limit."
        ],
        [
            "And in a moment we'll see methods that sort of achieve this.",
            "This speed limit.",
            "Now you might ask, so I took.",
            "So far I've looked at really, really simple choices of Alpha just by 1 / L Two overview plus L. What if you really try hard to find the exact minimizing Alpha every time you take a step, you picked the perfect Alpha, the one that gives you the best possible value of F along that step.",
            "Can you do better?",
            "Well, let's try that idea out with a convex quadratic.",
            "Where a is fixed here.",
            "OK so if I you know if I take the gradient the gradient this case is just a times X an I search for the exact minimizing Alpha.",
            "It turns out that it's the argument of this.",
            "I just get this by plugging X minus Alpha grad F into the definition of F. I find this explicit formula for Alpha and you can show that if you use that Alpha you get this convergence rate.",
            "Again, it's a geometric rate at least, but you can see that the constant is the same constant.",
            "I got two slides ago when I was taking this.",
            "You know very naive step length, so it looks like I haven't gained anything at all.",
            "From finding the exact minimizer, OK, I get the same sort of convergence rate, which is a little bit surprising.",
            "But it does maybe point out the limitations of."
        ],
        [
            "Just moving in the negative gradient direction.",
            "So here's sort of a breakthrough idea, and in its simplest form it was called the heavy ball method and appeared in the Russian literature and maybe 30 years ago or so.",
            "And the idea is, instead of just stepping in the negative gradient direction, you toss in a component which is equal to the last step that you just took.",
            "So now the now your step is a combination of the latest gradient and the previous step.",
            "It's called Heavyball cousin.",
            "In a sense, this is kind of a momentum term, right?",
            "It's kind of your tending to move along the same direction that you just moved in, but you tweak it a little bit with the latest gradient, OK?",
            "Now you've got two knobs to turn.",
            "You get to choose not only Alpha, but also beta.",
            "So the question is by turning, those two knobs, can you get a faster convergence rate?",
            "Well, this turns out to be easy to analyze two and the way you analyze it is you just collapse two successive errors into a single vector.",
            "So I formed this vector WK which is dimension 2 N. An I look at what this recurrences in terms of WK.",
            "Well, it turns out it's approximately this get from WKWK plus one year approximately operate on WK with a linear operator B or a matrix B2N by two and matrix that looks like this and there's a little remainder term which is due to the nonlinearity in F OK. And so what's be look like, well, be is a Block 2 by two matrix very simple, except that there's a Hessian of F appearing here, and when we are close to the solution, you know this WK will be small and be will be very close.",
            "Will be is essentially this.",
            "So our aim is then to choose Alpha and beta so that this B has nice spectral properties and you'll know that when you got a linear recurrence and you're looking to converge to zero, the name of the game is to try and make sure your occurrence matrix B has its eigenvalues strictly less than one and as much less than one as you can possibly make them.",
            "So we're going to try to choose Alpha and beta to minimize the maximum eigenvalue of B."
        ],
        [
            "OK.",
            "So that's the game.",
            "Now you can do a similarity transformation on beta.",
            "Just reduce this hash in at the extar to just its diagonal with just the eigen values on the diagonal.",
            "And when you look at that, you can sort of now write down what the eigenvalues are.",
            "Pretty much just in closed form, and it turns out that you know it's pretty simple.",
            "A little bit of simple arithmetic to figure out what Alpha and beta are, and here they are.",
            "Your optimal Alpha is that your optimal beta is that if you got a new condition problem where caparas large beta is pretty close to one, health is going to be pretty small in general, OK, and so you're only taking a little bit of a nudge in the negative gradient direction.",
            "You're tending mostly to move in the same direction as the previous step.",
            "That should talk.",
            "But this is the convergence rate.",
            "OK, it looks like the linear convergence rate that we were getting earlier, but there's a really important difference.",
            "Not only that, we've just got the square root of Kappa in the denominate here, not Kappa, and that can be a huge difference."
        ],
        [
            "OK, in fact, if your goal is to take is to reduce your initial error by a factor epsilon.",
            "Say you want to figure out how many steps do I need to take to reduce the error by epsilon.",
            "If you've got a rate constant like this, it's going to take you something like Kappa over 2 log epsilon steps.",
            "If you got a rate constant like this, the one we just got, it's going to take your root cap over 2.",
            "So if CAP is 100, which is still at moderately conditioned problem, not too bad, you need 10 times as many steps.",
            "If you're using one of these constants so the heavy ball method can be a remarkable improvement on just tonight."
        ],
        [
            "We've gradient descent.",
            "Now conjugate gradient is probably something I love.",
            "You have heard of in, you know if you've taken any kind of linear algebra course or optimization course.",
            "The idea of conjugating is very, very similar to heavy ball.",
            "In fact, you know they really almost identical innocence.",
            "They're motivated maybe slightly differently, but you can see in kanji gradient the step that you take is in a direction PK&P.",
            "Again is just a combination of the latest gradient and the previous step.",
            "OK, and really conjugating there are just different methods of choosing how to balance off these two components.",
            "And as I said, motivated a little bit differently.",
            "In fact, we can identify it with a heavy ball method by.",
            "Noting the relationship between the gamma in the concert gradient an the beta in the heavy wall and we'll get you know this is pretty much the same thing.",
            "Now there are many variants of CG CG of course originally came up as a way to solve linear equations with symmetric positive definite coefficient matrices.",
            "But that's the same as minimizing a quadratic convex objective, and you can generalize that to methods for minimizing general convex objective functions F. And so there have been a variety of methods proposed over the last 30 years which basically are different ways of choosing Alpha and gamma in these formula up here.",
            "So Alpha, you generally choose to sort of get an approximate minimum of F along that direction and gamma you choose again by some formula.",
            "Again, each of these different methods has a different way to choose gamma.",
            "They all collapse to be the same thing.",
            "If there happens to be a quadratic, OK.",
            "There's a very rich convergence theory associated with kanji grading, which I won't go into.",
            "You know the results, like if the if the Hessian has a favorable spectrum with the eigenvalues of the Hessian and kind of clustered, you tend to get very fast convergence and the number of steps is related to the number of clusters and things like that.",
            "But ultimately the rate that the thing that we're most interested in here is that if you're dealing with a quadratic, if you get a linear convergence again with with one of these constants 1 to 1 -- 2 over root Kappa.",
            "So like just like the heavy ball method, much more favorable and just steepest descent.",
            "So if you look in chapter five of my book with Knoxville in 2006, there's a description of.",
            "Conjugate gradient there.",
            "Now."
        ],
        [
            "This is where things start getting a little bit spooky when we talk about accelerated 1st order methods.",
            "Now I've talked about methods that converge like 1 /, K sub linear rate.",
            "If you if all you know is the function is convex and we talked about methods that have a linear rate related to the square root of conditioning.",
            "So first of all can you do better than that?",
            "And Secondly can you have a method that sort of automatically chooses the step length to get the best of both worlds?",
            "OK and in fact there is such a method and I think Nesterov came up with it in 1983.",
            "And I've heard that he sort of just came up with it by accident by sort of messing around with formulas and things fell into place.",
            "He's giving a talk by the way, on the weekend at the NIPS workshops up in Whistler.",
            "So he'll be talking about things related to this, I'm sure.",
            "But it's a.",
            "It's a little bit mysterious because.",
            "Because you essentially have two sequences instead of one, OK, you don't just have a sequence XC, you also maintain a sequence.",
            "YK and the two things kind of interleaved with each other.",
            "You generate one of each on each iteration.",
            "So you start off with setting the same initial point for both sequences, and you get the next XC by just taking a standard short step gradient step from the latest YC.",
            "So this is what we've been doing so far in the first few slides.",
            "Now this is where things get weird.",
            "You sort of do this.",
            "You solve a couple of scalar equations to find an Alpha K inabata K. And they depend on this this Kappa, so you should give it an estimate of Kappa that conditioning your estimate of the conditioning.",
            "Other problem.",
            "And then you set the new Y by taking a combination of the last step.",
            "You take the latest X and you add on some multiple of the previous step in X.",
            "That gives you the new why?",
            "Now I'm not going to make any attempt at all to explain this to you, 'cause it's quite mysterious, and even when you see the analysis.",
            "Even that's not very illuminating as to what's happening."
        ],
        [
            "But I can at least draw a picture of what's going on.",
            "These Red Arrows here are just the steepest descent steps we take from each Y to get to the next X.",
            "And then to get from the new X to the next Y, we sort of take the previous X step and we keep heading in that direction by some distance beta K OK, which comes out of that weird formula alright?"
        ],
        [
            "Now there's a second method which is somewhat simpler.",
            "It's due to beckon tabule and it's very very similar in the sense that you also get the next X by taking a short step gradient from the latest Y.",
            "And again, here's the equation you have to solve is a little bit easier, and again you just extrapolate from the previous ex step, keep going in that direction by this quantity related to this TK thing and you get to the next why that way?",
            "So it's very, very simple."
        ],
        [
            "So what convergence rates do we get for this method?",
            "Well, again, it is the sort of the best of these of geometric great things related to square root of Kappa, which is good.",
            "That's the best geometric rate we've seen so far, even for the heavy ball method.",
            "And if you've only got weak convexity, that is when Kappa is Infinity, then this thing is 1.",
            "So it's not very helpful.",
            "But then in that case you get a 1 / K ^2 right?",
            "And that's sort of the speed limit.",
            "So so this is in some sense sort of an optimal behavior from this kind of method.",
            "As I said, the analysis is not very intuitive.",
            "The analysis in the back end to bull paper."
        ],
        [
            "Pfister"
        ],
        [
            "Is a little bit more intuitive.",
            "It's about two or three pages and you know it's very technical, but it's."
        ],
        [
            "Also very elementary.",
            "So you know if you're interested, IRA."
        ],
        [
            "So you do that paper and you can see how it's done.",
            "Here's a completely different kind of method due to Basel and bull wine.",
            "It's a gradient method, so again, you're only taking steps in the negative gradient direction.",
            "But here you're not interested in decreasing effort every iteration.",
            "OK, you're prepared to sacrifice temporary increases in F in order to get some sort of faster overall rate.",
            "Now what if you look at this formula?",
            "Here we've been looking at this already, but if this Alpha were replaced by the inverse of the Hessian.",
            "That would be Newton's method.",
            "OK, it's a workhorse method in nonlinear optimization.",
            "What they do in Basel?",
            "Boy is take a one parameter approximation.",
            "They think of this.",
            "Alpha is a one parameter approximation of the inverse Hessian.",
            "And I choose the value of Alpha by making it behind behave like the inverse session would have behaved according to Taylor's theorem, over the step that you just took.",
            "So they look at the difference of the last two gradients and they look at the difference of the last two X is and they say let me choose Alpha so that it sort of makes ass approximately equal Alpha times Y.",
            "So it sort of does the same thing as the inverse Hessian would do.",
            "And I use that as the step length and you can solve this formula exactly just explicitly just by doing these two in a product in a division.",
            "OK, you can show that that at least lies in the spectrum.",
            "If you want this to approximate the inverse Hessian, at least it falls somewhere between 1 / L and one over mu, which is the spectrum of the inverse Hessian.",
            "So it passes that sanity check.",
            "OK, now how does it behave?"
        ],
        [
            "Well, here's a picture.",
            "This is the classic picture.",
            "You may well have seen before of applying steepest descent to a quadratic function in two variables.",
            "So if you got a convex quadratic in two variables, it's minimum is obviously right there.",
            "It's the contours are elliptic, unless the eigenvalues happen to be identical, in which case this circular if you do steepest descent on that where you try to enforce a decrease in effort every step and you start out here, you tend to get the zigzagging behavior intended.",
            "Just go back and forth across the Valley.",
            "And there's still until the steps become so short that they just gives up and stop.",
            "And you still might be quite a distance from the solution when that happens in Basel I born, they tend to take steps that sort of ridiculously overshoot the minimum along this direction and go way up here somewhere, and then repeat the process again, move back in the negative gradient direction, and come back here and you can see that these intermediate idiots are going to have function values that are absolutely huge.",
            "But they're sort of setting you up so that the next iteratee makes further makes a more progress along the Valley.",
            "Then it's sort of a greedy this greedy zigzagging would have done OK, so you can see that least there's potential if you're looking at what happens over a series of steps to make more, faster progress.",
            "In fact, in their original paper from 1988, which is well worth reading.",
            "It's only like 8 or 9 pages long, but there's some very nonstandard analysis using some very strange.",
            "Very strange math to show why when you apply to a two variable quadratic.",
            "This thing makes sense even though you're not.",
            "You've got this so called non monotone method.",
            "You are not trying to decrease F every time.",
            "There are many, many variants of this OK. And a lot of magic and you."
        ],
        [
            "You silly things like you can hold Alpha constant for three or four or five successive iterations.",
            "That seems to help.",
            "You can take out for to be the exact best step from the previous iteration, which again seems like a bizarre thing to do.",
            "All of these things seem to help.",
            "There is some analysis for this, but only in fairly restricted cases as being quite a bit of literature recently by these authors.",
            "If you Google these names, you'll find it, but you know it's not especially well understood.",
            "It's just that in certain.",
            "Problems seems to do the right thing.",
            "OK, primal dual averaging.",
            "This is again returning to these kinds of methods that I."
        ],
        [
            "Was talking about earlier.",
            "The idea here is that you take a step on the basis of forming a linear approximation to F, But you don't just take the linear approximation around the latest point XK.",
            "What you do is average all the previous X is that you visited.",
            "All the exercise that you visited up to now.",
            "And you take your linear model to be the average of the linear models at all the previous steps.",
            "OK, so I'm dividing by K + 1 here.",
            "And you add on this term at the end to kind of stabilize things, right?",
            "You sort of anchor yourself at the initial iterate X nought.",
            "And you say that you have a convex quadratic term here to stop you from straying too far off into the you know off into the outer reaches of the solution space.",
            "Now this is easy to minimize 'cause this is just a linear term plus a quadratic term, so you can write down explicitly what XC plus one is.",
            "And you can see that you're sort of taking a linear model where your gradient estimate, as I said, is the average of all the gradients you found so far.",
            "What can you put well?",
            "Why are we interested in this kind of method?",
            "One reason is that you can generalize this pretty well if you're dealing with a non smooth function.",
            "For instance, you can replace each of these guys with an element of the subgradient and the convergence theory is almost the same, so this has the advantage that it generalizes pretty well."
        ],
        [
            "Windows Drive analyzes this.",
            "I guess.",
            "The paper on this came out just last year and.",
            "A lot of people have been reading his paper.",
            "It's been quite an important paper, but when he analyzes it, he focuses not on the sequence of X is, but on the sequence of running averages of the axis.",
            "So you take this X bar K to be the average of all the X is that you've encountered so far, and what you can show is that the function values at the X bar case converge like one over root K. OK, now that's lower than what we've been seeing so far.",
            "But as I said, it's advantage that it generalizes nicely.",
            "The case where F is non smooth, it also generalizes nicely to where you're only estimating the gradient where you don't actually have a precise gradient, but just an estimate.",
            "And again, the analysis is in that case of course you're dealing with expectations of F, But the analysis is very similar, so it has that advantage.",
            "OK, I think I'm going backwards."
        ],
        [
            "OK, now everything I've said so far pertains to the unconstrained case.",
            "We just try to minimize F, but of course often you want to actually stay inside some feasible set.",
            "So how do all these methods change when you start requiring that when you want all your iterates to stay in some closed convex set Omega?",
            "Well, most of the methods I've described so far or a lot of them at least.",
            "Generalized very easily to this case.",
            "In fact, the one I've just been talking about, this primal dual averaging where you've got this average gradient in this prox term.",
            "If you just trust in the restriction here that you also want X when you're finding the new iteratee, you just want to restrict it to being an Omega, that's it, you know the subproblem is exactly the same, except that you're adding this restriction.",
            "And again, the analysis is almost unchanged, so this method has the advantage that it generalizes immediately to the case.",
            "Uh, vay convex constraint set."
        ],
        [
            "Similarly, Nesterov's method, one of the ones I talked about earlier this accelerated method with the magic here to select the Alpha and beta.",
            "Again, it's completely unchanged.",
            "All of this is exactly the same as what I had before, except for this step where you move, you get the new X from the latest Y by taking a step in the negative gradient direction that has to change because we want the wise to stay inside the set Omega.",
            "And in fact, this is how it changes you.",
            "Choose the next X to be as close as you can get to that point that you would have gone to, and the unconstrained case while staying in Omega.",
            "Alright, that's really the only change you make to the method."
        ],
        [
            "And the convergence theory pretty much stays the same.",
            "Now, what about when you've got not an explicit constraints that Omega, but you are trying to minimize F plus tower time, some regularization term?",
            "And this was the topic of Abhumans tutorial this morning.",
            "There are a lot of applications like compressed sensing and lasso and so on.",
            "We've got some smooth objective here and you've got something non smooth here but simple like an L1 norm.",
            "So how do these methods generalize to this case?",
            "All the methods I've talked almost of assumed smoothness, but they are sort of generalizable to nonsmooth.",
            "So what do you do in that case?",
            "Well, again, a lot of them generalize if you can explicitly handle this site.",
            "If the regularization term, if you can deal with that explicitly, they often generalize very easily.",
            "So, for example, the one where we would move where we would just take a short step in the negative gradient direction.",
            "If I can replace that by minimizing this thing, which is, you know, as close as you can get to that short step, plus the regularizer, exactly as it appears here.",
            "If this subproblem is easy to solve, which it is in many cases, such as when this is the L1 norm, then most of those methods like nest rows, method gradient methods, and so on generalize immediately.",
            "The theory can go right through, so the Pfister method, for instance, which is one of the ones I had up earlier on.",
            "It immediately general is in fact the analysis in Vista just works with this regularised case, and if you care about is young constraint thing, you can just set this to zero and it'll still work exactly the same.",
            "So the bottom line here is that for a lot of unconstrained gradient, simple gradient 1st order methods.",
            "It's often not that big a deal if you want to toss in a regularization term or an explicit constraint set.",
            "So this is the end of the first part."
        ],
        [
            "And these are a few references where you can go for further information and I'd be happy to give you more if you need more.",
            "There's a 2004 book by Nurse Drivers is paper by back into Bull.",
            "Actually there's a 2007 or 8 paper.",
            "They've written a review paper which you can get from tables website, which is a little more self contained and up-to-date.",
            "There's an old book by Polyak which is where the heavy ball come from.",
            "It's very beautiful book actually.",
            "It's got conjugate gradient and and and.",
            "Steepest descent and things like that.",
            "And there's the Basel born paper that I mentioned.",
            "And then there's this very recent paper by Nest Drop.",
            "OK."
        ],
        [
            "The second part of the talk.",
            "This is the part where we can't even get a first derivative.",
            "OK, so I'm going to talk about stochastic and incremental gradient, so the setting here is that I'm going to allow F to be non smooth.",
            "I'm going to assume that you know I can't economically get a function value, so I can't go off and test a point to see how good it is.",
            "At least that you know I can't reliably do that, and I'm going to assume that at any point I've got access to some sort of estimate of a gradient or a sub gradient.",
            "In the case of a non smooth function.",
            "So here's a couple of common problems that fall into this setting.",
            "There's one where your objective is actually the way you've got a function F, which depends on your variables X, but also on some random variable and what you're interested in minimizing the expectation of that over the random variable.",
            "OK, so that's your objective F of X are related.",
            "Setting is where your F of X is actually made up of the sum of objective.",
            "Some finite sum.",
            "OK, so these were each of these is convex and non smooth."
        ],
        [
            "Now these would ring a Bell with a lot of you.",
            "Because SVM, the standard SVM setting has this form.",
            "Primal SVM typically had some sort of regularizer.",
            "Here, one or more at two norm squared, and then it has this loss function which is made up over some of losses over each piece of data in the training in the training set.",
            "So for instance, in linear SVM this loss function L might have this form.",
            "The variable might be W and your loss function is function of W. Transpose the feature vectors and the classifier and the label.",
            "Why I OK another very closely related cases logistic regression where the loss functions are the some of these log functions in the regularizer might be a one norm there, so this is something that obviously this setting obviously fits well and machine learning."
        ],
        [
            "OK, just mention here of terminology and notation since I'm now dealing more explicitly with nonsmooth functions, I have to talk about subgrade."
        ],
        [
            "Yes, I've got a picture of Subgradients here.",
            "If this is mine on the graph of my nonsmooth F, you can see that it's mostly smooth, except at this point here, where there's a kink.",
            "So at that kink I can define a bunch of supporting hyperplanes, and because this is a 1D function, the hyperplanes are just lines.",
            "And they're just planes that kind of lie below the graph of the function.",
            "The slopes of each of these lines would be an element of this subgradient.",
            "OK, so the subgrade in this case would be an Inter."
        ],
        [
            "On the real line.",
            "So in general the subgradient is some sort of convex set.",
            "We can still talk about strong convexity.",
            "When I was dealing with smooth functions, mu is a lower bound on the eigenvalues of the hash and I can still even though the Hessian doesn't exist, 'cause I don't even have a grading in the non smooth case.",
            "I can still talk about modulus of convexity mu and I can still have a relationship that looks like a lower bound on the 2nd order Taylor series.",
            "OK, so classical stochastic approximation or stochastic gradient descent method."
        ],
        [
            "What I assume here is that I've got access at any point X. I've got access to this thing big G of X and PSI, which is some estimate of the gradient of this gradient, or a subgradient at X. OK, so you can get that by sampling site from that distribution that it lives in according to its distribution function and justice justice, valuating G at that point.",
            "So the basic stochastic approximation scheme is that you just take a step in the direction of Big G. That's it, OK. Notice by the way, again, the critical issue is how do you choose the alphas just as it was earlier?",
            "And we're going to talk about how to do that.",
            "I just notice in passing here that the obviously each time you run this, you're going to get a different result because you're going to be sampling and picking a random czyca every time, and obviously your ex K plus one depends on all those eyes that you've encountered so far, so it's a random variable, but depends on all those eyes that you encountered so far."
        ],
        [
            "Now here have actually been adventurous and against the advice of my students, I left the analysis in here.",
            "I figured that even if you couldn't follow it in real time, if you're interested, you could go back and look at it.",
            "The reason I left it in, is it because it's so simple.",
            "It really literally fits on two or three slides, and so I thought it was worth putting up.",
            "Now what we can analyze in this case is the expectation of the error in X.",
            "So I define this little AK to be the two norm of X K -- X star squared and the expectation of that.",
            "The assumption I'm going to make is that my subgradient estimate is in such as sort of a bounded variance.",
            "OK, and this can be guaranteed in a lot of cases of interest that if I take the expectation of the square of Geo verci that's bounded by some M squared OK. Now let's analyze the convergence.",
            "Now remember, this is the step that I'm taking here.",
            "So I'm going to look at what happens or what the error is at XK plus one.",
            "So I'm going from here to here.",
            "I'm just plugging in the definition of X K + 1.",
            "And now I'm just expanding out that two norm squared term and I get XK minus X star term.",
            "Here I get this cross term which is a cross between XK minus X star and the G term, and then I get Alpha squared times the two norm of G. Well, I've just learned how to bound that in expectation, so when I take the expectation of both sides, this gets bounded by M. ^2, that's easy.",
            "The slightly tricky one is the expectation of his middle term OK, and this is where you have to be a little bit careful, because as I pointed out earlier, the XC depends on all the random variables IK that you've encountered so far, and the expectation of G with respect as I K is in the subgradient.",
            "So you can sort of use those two facts and use some conditional expected."
        ],
        [
            "Oceans, again, you know it's really not hard.",
            "To figure out a bound on that middle term and that turns out to be the critical thing.",
            "And once you get about on the middle term, you find out that."
        ],
        [
            "Going back here, you find out that a K plus one is just a K minus."
        ],
        [
            "Something noise here, in fact, that something nice is just too mu times Alpha K times.",
            "OK you can show and then we've got this last term Alpha K ^2 * M ^2.",
            "So we've got a nice thing here, which kind of gives us some hope that AK is getting smaller.",
            "The expectation of the error is going down.",
            "You can see that this multiple here is less than one, so provided this isn't too big, I'm going to be able to decrease.",
            "AKA at every step.",
            "So the critical point is, how do I choose these Alpha case to make that happen?",
            "Well, it turns out the magic way to define Alpha K is 1 / K times mu, where mu is the modulus of convexity.",
            "In case the iteration number.",
            "So this is where I leave this as an exercise for you.",
            "If you plug this into this and do about four in equipment, go through a sequence of four very very simple inequality's, you can show that AK is bounded by some constant over 2 * K where the constant depends on the initial error.",
            "Bound on the variance and the modulus of convexity, so you get in expectation you get sublinear convergence but at a 1 / K rate.",
            "Now that's really pretty amazing, because when we right back at the start of the talk, I gave you a steepest descent.",
            "Short steps, keepers descent method that got 1 / K right OK, and here we're getting a 1 / K rate even with this very crude estimate of the gradient, we don't even need an exact gradient.",
            "We do however need strong convexity in this case, alright, but we're still getting the same rate."
        ],
        [
            "So, OK, So what happens if we don't know mu if we don't have a good estimate or if we're not even dealing with a strongly convex function, what can we do in that case?",
            "Well, it turns out that we can Patch up what the method I've just described.",
            "In fact, there's this paper by Nemerofsky and Judisch Ian Shapiro, and land that appeared last year, and so I upped.",
            "Where they describe this and you can recover 1 / sqrt K right?",
            "So it's not quite as fast, but it's tends to be more robust.",
            "It tends to be much less sensitive to choice of parameters."
        ],
        [
            "And again, the main trick is that again, you don't work with the sequence of X is you work with the sequence of running, running weighted averages of the axis.",
            "Again, these X bar key terms, which are weighted averages according to the values of Alpha.",
            "It turns out that the magic choice for Alpha in this case is some constant over N * sqrt K square root of the iteration number rather than K itself, and what you can show for that choice is that the F value converges to the optimal value.",
            "At this rate, log K / K to the 1/2, so not as fast as 1 / K, but still at least it's going to zero at a reasonable rate.",
            "And Moreover, it's not very sensitive to the choice of this parameter, which is a big advantage over the.",
            "The standard stochastic approximation in standing static approximation.",
            "If you get Mew wrong, this is incredibly slow.",
            "You have to get you have to get Mew as an underestimate of the modulus.",
            "If you're overestimated, you get a terrible result."
        ],
        [
            "OK, again I was brave and I put the analysis for this on one or two slides and again my motivation is that it's extremely simple.",
            "So if you if you want you can go and look at the slides and satisfy yourself that you know how to do it.",
            "There's really nothing mysterious about this.",
            "Now here is the argument.",
            "You basically take that sequence of Acks that I had before, I just changed the indexed I for some reason, but this is exactly the formula we ended up with a moment ago.",
            "You can use convexity and the fact that your latest GI, which is your estimate of the subgradient, lies in the subdifferential to bound this term below an you can get to this situation where the expectation of the error in F times Alpha Rai.",
            "Is less than or equal to this quantity here?",
            "Now you simply sum both sides from one up to from I = 1 up to K. You notice you get some telescoping terms here.",
            "Everything just tends to collapse.",
            "And you left with this relationship.",
            "OK, which depends on the errors in each of the F have encountered so far depends on the sequence of alphas.",
            "Depends on the initial error.",
            "A1 is the initial error and depends on M which is your bound on the variance of the gradient estimate.",
            "And this is."
        ],
        [
            "Why you use the fact that you're dealing with X bar K the weighted average of X is.",
            "Now it's very simple using the convexity of F. This just comes directly from the direction of definition of convexity.",
            "X bar K is the weighted average of the X case waited by the alphas and according to convexity F of X, Parque is the way to the same weighted average of the FS waited by the Alpha.",
            "So this is elementary.",
            "So if I if I use that definition there an divide both sides by the sum of the alphas.",
            "This is what I get that the expected error in F with the X bar K enter it is bounded above by this thing that involves the sum of the Alpha K squared divided by the sum of the Alpha case.",
            "And so you have potential here.",
            "By choosing the Alpha case appropriately, you have potential to squeeze this to zero at a reasonable rate.",
            "And the answer is I already gave."
        ],
        [
            "Where the answer a few slides ago.",
            "The answer is if I choose Alpha K According to this formula I can plug."
        ],
        [
            "It into this formula here."
        ],
        [
            "I get this one of us get this log K of a sqrt K right?",
            "And here is the analysis here.",
            "OK uses an elementary bound on the sum of 1 over.",
            "I use an elementary bound in the denominator.",
            "This is it.",
            "Nothing complicated here.",
            "I wanted to put it out there because I wanted to show you how simple it was.",
            "There are other things you can do so you could work with a paradigm where you've got a budget of valuations upfront.",
            "You know you're only going to take any steps, some big N, and using that you can figure out maybe a different way of choosing the alphas like constant step length or so."
        ],
        [
            "So other things you can do.",
            "You might have heard of Mirror Descent.",
            "It's a term that Nemerofsky uses a lot in his papers and various other people.",
            "Mirror descent is pretty much just what I described, but generalized to the setting where instead of using the two norm here, you use some other norm, based maybe on a Bregman distance.",
            "And also you might restrict attention to a set Omega, so you might design the norm so that you get strong convexity with respect to that norm over the set Omega.",
            "OK, so it's just a generalization of what I've said so far.",
            "Now, the step that I as I defined it earlier was just XK minus Alpha times G. Now it turns out that's exactly what you get from minimizing this, so this is just another way to write down the definition of this stochastic approximation step.",
            "But if I write it this way, it gives us a path to generalizing it to the constraint case into the regularised case.",
            "OK."
        ],
        [
            "So for example, if we want to.",
            "Constrain the iterates to this set Omega all I have to do in this problem is to add a constraint.",
            "ZZ Belong Store mega to this subproblem.",
            "And I get a lot of the same properties holding OK.",
            "So this is a little bit about how you design A mirror descent method where you have a normal.",
            "It's maybe not the Euclidean norm and you have a set that's maybe not the entire space RN.",
            "So you can you can generate, you can come up with a distance generating function Omega.",
            "That has this kind of strong convexity property with respect to the norm that you've chosen, which no longer is necessarily necessarily the Euclidean norm.",
            "You define something called approx function, which is sort of the deviation of Omega from linearity.",
            "OK, it's a difference between.",
            "The difference in Omega between Z and X -- a linear prediction of the difference.",
            "OK, so it's the deviation.",
            "From linearity there's a picture of that on the next slide, so this is my Omega.",
            "This curved line here the."
        ],
        [
            "V of X&Z is what you get by taking a tangent at X and seeing how different it is from the true function value.",
            "By the time you get to Z, OK."
        ],
        [
            "That's the Bregman distance.",
            "And some people really like to use the term Bregman a lot, so I thought I'd better tell you what it was.",
            "OK, so you can, you can use the Bregman distance in this scheme that I described."
        ],
        [
            "Earlier this basic stochastic approximation scheme by replacing the prox term here with the Bregman distance and by plugging in your."
        ],
        [
            "Mega.",
            "In the augment and so this is the generalization of stochastic approximation to the constraint case with the general Bregman distance."
        ],
        [
            "Now, what are some examples of useful, interesting constraint sets and their appropriate Bregman distance, as well as the one we've been talking bout already?",
            "Which is where we just use the two norm, which works for any Omega.",
            "But this is a constraint that a lot of people are interested in, the one where it's a simplex, the one where all the axes are the components of X are constrained to be non negative, and they're constrained to sum up some up to one.",
            "OK, now in that set it turns out that this Omega vex this entropy function generates this pregnant distance.",
            "And that this thing has a strong convexity property with respect to this set.",
            "So you can redefine stochastic approximation using this V and this and this Omega and you can get methods with the same sorts of convergence properties that I've described.",
            "And I'll just finish up."
        ],
        [
            "This section with a mention of incremental gradient.",
            "There are people that have been working, particularly it's a kiss and his collaborators have been working in this incremental gradient setting format 20 years or more.",
            "This is the case where your F specifically is a finite sum of FIS of component objectives and the way these methods typically work is they just grab one of these FIS get its gradient and do something with that.",
            "OK, so it's a special case in a sense of stochastic approximation.",
            "But there is the option and some variance of incremental gradient sort of cycle around the eyes.",
            "So they make sure you touch all the eyes.",
            "Others just pick them randomly with replacement at every step OK.",
            "So I'll just mention something.",
            "Bertsekas has a paper coming out.",
            "There's a volume called optimization and Machine learning which is going to appear in the Nips Workshop series and some of you here have contributed chapters to it.",
            "I think it should appear sometime in the next five or five months I believe, but some cases contributed a chapter to that which deals with this problem, and he mentions a lot of different variants of methods of this type.",
            "For example, you can generalize the heavy ball method that I talked about to this setting.",
            "Where instead of using the grade of F, you just use the latest grade of FI.",
            "In this time here.",
            "You can get approaches like dual averaging where you have a cyclic choice of the eyes and you just take your linear function.",
            "Your linear approximating function to be the average of those choices over the last M iterations, where M is the number of terms in the sum.",
            "As a recent paper by Al Hero and some people and some collaborators on that topic.",
            "In the basic."
        ],
        [
            "Method where you just take a negative gradient step in the latest component that you've selected.",
            "But Sankus gets these bounds which at least tell you that you're not converging to something crazy.",
            "So if you've got a constant step length scheme where you're always moving the same distance Alpha, he shows that in the limit you at least get within this far of the optimal F OK.",
            "If you do a cyclic choice, you get one bound on the deviation.",
            "If you do a random choice, you get a different bound, so it's not guaranteeing convergence of of F to the true value, but it's showing that you're not doing anything crazy.",
            "And it also points out a way that you can consider taking varying choices of Alpha.",
            "You could consider embedding this in some method that decreases Alpha as you go, and thereby you might be able to squeeze out a convergence result.",
            "In that case OK.",
            "So he actually proves this in the more general context.",
            "In, particularly considers a case where you've got, you're adding on that non smooth term.",
            "I should."
        ],
        [
            "Mention that these methods stochastic gradient methods have been the subject of a pretty significant stream of research in the in the machine learning community for some years, and I mentioned some codes here, SSD and Pegasus, and numerous people that have been working on this.",
            "In fact, it was almost I think they work in the two communities.",
            "That was pretty much independent and the connection was only made fairly recently.",
            "But the methods end up being very, very similar.",
            "Similar choices of step length, somewhat related convergence theory.",
            "There was a tutorial by Naughty Srebro and Tewari at a recent ICML, which went into much more detail on what I've just talked about in the second topic, and I refer you to that For more information, and I mentioned also this stream of related work in online convex programming that I think grew out of this paper of zinkevich.",
            "I apologize if I don't get my machine learning.",
            "Reference is correct, but I think this is a paper that a lot of you probably know.",
            "It's a somewhat different setting because there's no idea assumption.",
            "As I understand it.",
            "But again, you're minimizing sort of a sequence of convex functions that are presented to you after each time you take a step, and you get similar sorts of convergence."
        ],
        [
            "Dates to what I was showing earlier.",
            "So here's my reference list for Part 2 of the talk.",
            "I think I'll just leave that up there and we should take a 10 minute break or so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Testing testing alright that works OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you Francis for that introduction and I really appreciate the opportunity to speak here.",
                    "label": 0
                },
                {
                    "sent": "Plan on having a break in the middle so you can recover a little bit and I can recover.",
                    "label": 0
                },
                {
                    "sent": "I do have a lot of material.",
                    "label": 0
                },
                {
                    "sent": "I looked at Francis Box Tutorial from last year and 156 slides so I felt like I had to make an attempt.",
                    "label": 0
                },
                {
                    "sent": "And as you can see I only got to 82 so it might only be half as good, but we'll see.",
                    "label": 0
                },
                {
                    "sent": "But but there is a lot of stuff and I'm going to have to go over it pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "But my main aim is to really cover a lot of ground so that if you're interested you can get hold of the slides which are already on the web.",
                    "label": 0
                },
                {
                    "sent": "If you go to the page on the NIPS website for this talk, you'll see a link to them, and so you can get them and you know go back to the parts that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "Also, I've got periodically I've got a list of five or six papers that pertain to each of the five things that I hope to talk about.",
                    "label": 0
                },
                {
                    "sent": "And again, these are just certainly not meant to be exhaustive, but there are jumping off point if you want to go and learn more about a specific topic.",
                    "label": 0
                },
                {
                    "sent": "OK, so by way of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Introduction I just wanted to say that optimization as a field it's been around since 6070 years, but it's really going through a very vital growing.",
                    "label": 1
                },
                {
                    "sent": "Right now and a lot of the impetus is coming from application fields and in particular machine learning.",
                    "label": 1
                },
                {
                    "sent": "We're getting tremendous cross fertilization between machine learning and optimization.",
                    "label": 0
                },
                {
                    "sent": "Back in the old days away that optimization people had workers, they pick a standard paradigm like linear programming, quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "NLP to me that means nonlinear programming.",
                    "label": 0
                },
                {
                    "sent": "I believe it means something different to some people here, energy programming.",
                    "label": 0
                },
                {
                    "sent": "So people are sort of working algorithms and they'd write software and they throw the software over the fence and people would pick it up and use it on their application.",
                    "label": 0
                },
                {
                    "sent": "But now nowadays the applications are often too big and too challenging and and two sort of highly structured to really take full advantage of that mode of working.",
                    "label": 0
                },
                {
                    "sent": "And so there has to be a different way of doing research or application.",
                    "label": 1
                },
                {
                    "sent": "People work more directly with optimization people.",
                    "label": 0
                },
                {
                    "sent": "An essentially methods sort of customized methods are built up from a toolkit of algorithmic components, and that's happening over and over again and across the whole range of applications.",
                    "label": 1
                },
                {
                    "sent": "So what that means is that it's important if you want to use optimization.",
                    "label": 1
                },
                {
                    "sent": "It really helps to know about some of the fundamental aspects of algorithms so that when you come to actually as well.",
                    "label": 0
                },
                {
                    "sent": "Of course as the demands of your application so that when you come to designing an algorithm, you sort of know how to put the pieces together in the right way, so I'm.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to hope to cover these five topics.",
                    "label": 0
                },
                {
                    "sent": "1st Order methods.",
                    "label": 0
                },
                {
                    "sent": "By that I mean methods that use gradient information.",
                    "label": 0
                },
                {
                    "sent": "Secondly, stochastic gradient methods.",
                    "label": 0
                },
                {
                    "sent": "These are methods that don't even have first order information.",
                    "label": 0
                },
                {
                    "sent": "They only have maybe an estimate of the gradient or a piece of the gradient part.",
                    "label": 0
                },
                {
                    "sent": "Three sort of relates very much to Bulmans tutorial this morning.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of optimization problems where you actually want to sort of a structured solution, and the way you get that is to add a regularizer onto the objective that you're trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "And it's usually non smooth and like an L1 norm or something, so a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "I'll say a little bit about using higher order information.",
                    "label": 0
                },
                {
                    "sent": "We actually try to use maybe secondary of information or estimates there of and that's that's useful too.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, there's a technique that people who do SVM have known all about for no more than about 15 years where, which is where you don't try to optimize overall of variables at once, but you break the problem into chunks and just optimize over a subset of variables.",
                    "label": 0
                },
                {
                    "sent": "So the other five topics are.",
                    "label": 0
                },
                {
                    "sent": "Some are longer than others.",
                    "label": 0
                },
                {
                    "sent": "Maybe in the first part I'll get through the first 2 by.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, 15 minutes from now.",
                    "label": 0
                },
                {
                    "sent": "So OK, so in talking about first order methods I'm going to use a very simple, very restricted setting, but I'm going to have things much more general in this in mind, But basically for purposes of description, I'm going to assume that you're trying to minimize a smooth convex function.",
                    "label": 0
                },
                {
                    "sent": "Not only is it convex, but its eigenvalues of the Hessian between the lower and upper bound, and sometimes I'll let the lower bound B0.",
                    "label": 0
                },
                {
                    "sent": "The upper bound is LL will then be a Lipschitz constant on the gradient OK?",
                    "label": 1
                },
                {
                    "sent": "If mu is positive, it means we're dealing with a strictly convex function, and this is the.",
                    "label": 0
                },
                {
                    "sent": "More or less a standard definition of convexity.",
                    "label": 0
                },
                {
                    "sent": "And functions like this have this sort of bowl shaped and their comparatively easy to optimize.",
                    "label": 0
                },
                {
                    "sent": "Another quantity that keeps coming up in the description is this Kappa, which is sort of a measure of the conditioning of the problem, and it's simply the ratio of the largest to the smallest eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And sometimes I'll just restrict attention to the simplest really simple case of a quadratic convex function F where the Hessian is this matrix A, which again has its eigenvalues between these two bounds.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the setting.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said, we actually have, you know, we're really interested in doing much more general problems in this, such as problems where F is non smooth, maybe continuous, but maybe doesn't have a derivative.",
                    "label": 1
                },
                {
                    "sent": "Even sometimes we will.",
                    "label": 0
                },
                {
                    "sent": "In fact, in a lot of applications it's really hard to evaluate.",
                    "label": 0
                },
                {
                    "sent": "If, for example, if you're doing an SVM, you have to run through all the data just to get a value of F, and that clearly often is not desirable.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you won't be able to get the gradient, you'll only have it estimate.",
                    "label": 1
                },
                {
                    "sent": "You might want to impose some sort of constraint on the unknowns.",
                    "label": 0
                },
                {
                    "sent": "Or you might have a nonsmooth regularization term, which I've already mentioned, so when I'm describing algorithms here, I'm trying to describe mostly algorithms that can be extended to handle situations like this.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's maybe the simplest, most fundamental 1st order method of all, and that simply generates a sequence of iterates.",
                    "label": 0
                },
                {
                    "sent": "XK, by moving along the negative gradient direction, so evaluate the gradient of F and take some step along that direction.",
                    "label": 0
                },
                {
                    "sent": "So the big issue is how do you choose Alpha here?",
                    "label": 0
                },
                {
                    "sent": "And in standard optimization, if you read my book and many other books, there are chapters on how you do the line search.",
                    "label": 0
                },
                {
                    "sent": "So one way is you actually make some effort to find the minimizer of F. Along this direction, so you try to choose Alpha to actually minimize F along that direction.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of work.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of tricky methods.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's appropriate, but it's usually it's pretty tricky to come up with a reliable, efficient method to do that, but it has been done.",
                    "label": 0
                },
                {
                    "sent": "Another thing that almost everyone, probably you know a lot of you in this room have done is to pick some initial guess of Alpha and take that step and see if it decreases F, and if it doesn't, you just take half of that distance.",
                    "label": 0
                },
                {
                    "sent": "And try again and keep going until you get one that works.",
                    "label": 0
                },
                {
                    "sent": "Actually decreases the function, so that's easy to implement.",
                    "label": 0
                },
                {
                    "sent": "But there's an even more trivial thing you can do and that is you don't even bother about testing to see if the function goes downhill.",
                    "label": 0
                },
                {
                    "sent": "You just take some out, choose Alpha by some rule that maybe depends on some knowledge of the function, like you might have bounds on the upper, the the maximum eigenvalue of the Hessian, and also the minimum eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "And maybe you can formulate some rules based on those and maybe also based on.",
                    "label": 1
                },
                {
                    "sent": "What iteration you currently at?",
                    "label": 1
                },
                {
                    "sent": "And traditionally, analysis for these first 2.",
                    "label": 0
                },
                {
                    "sent": "You can find him in the standard optimization literature.",
                    "label": 1
                },
                {
                    "sent": "They can prove things like you'll eventually get to a minimum, and if you're dealing with a convex function, it will be the minimum.",
                    "label": 0
                },
                {
                    "sent": "The analysis for three is quite has quite a different flavor and it focuses.",
                    "label": 0
                },
                {
                    "sent": "It tends to focus more on not just on the fact of convergence, but also on the rate at which you converge.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm not going to give too many more details, but I will about the 1st two.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk more about the third one where you choose Alpha according to some simple rule.",
                    "label": 0
                },
                {
                    "sent": "Now you can use Taylor's theorem.",
                    "label": 0
                },
                {
                    "sent": "Taylor's theorem is a ubiquitous tool in analyzing nonlinear optimization methods, and you will learn Taylor series expansions in you know, first or second semester calculus.",
                    "label": 0
                },
                {
                    "sent": "And that keeps coming up.",
                    "label": 0
                },
                {
                    "sent": "And so here by just expanding along this.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you know, plugging this into F and expanding it as a function of Alpha and doing a bit of manipulation and using the fact that the Hessian is bounded above by L or the eigenvalues bounded by L, you can prove.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you get this kind of change in F as you take this step.",
                    "label": 0
                },
                {
                    "sent": "And now if you set Alpha to be 1 / L for instance, you can just substitute that into there and you'll find that F actually decreases is guaranteed to decrease by at least this amount.",
                    "label": 0
                },
                {
                    "sent": "OK, so this very simple choices step length is guaranteed to give an improvement in F. Now you can mess around with this a little bit using very elementary arguments, and in fact I had these arguments on the slide, but my students made me take him out, but it's very, very simple.",
                    "label": 0
                },
                {
                    "sent": "You just sort of take some reciprocal Zanu some bounds and stuff, and you end up being able to show that after you've taken K steps, the error in the function value is bounded by some constant that depends on Ellen.",
                    "label": 0
                },
                {
                    "sent": "Also, on the initial error that should be X star there.",
                    "label": 0
                },
                {
                    "sent": "Sorry, divided by K + 1.",
                    "label": 0
                },
                {
                    "sent": "So you get this famous 1 / K. One of a K type convergence behavior.",
                    "label": 0
                },
                {
                    "sent": "All I've assumed here.",
                    "label": 0
                },
                {
                    "sent": "I haven't assumed that we're dealing with this strongly convex function, just with a convex function where the upper bound L is known.",
                    "label": 0
                },
                {
                    "sent": "So this is this is the classic 1 / K convergence.",
                    "label": 0
                },
                {
                    "sent": "Right now you can do better than that if you know that the function is strongly convex if mu is positive, you can use a different step length somewhat different from.",
                    "label": 0
                },
                {
                    "sent": "This might be even almost twice as long, and again by plugging into here and messing around with a little bit, you can get this kind of convergence rate.",
                    "label": 0
                },
                {
                    "sent": "That the error this time the error in X not in not in F goes down at this sort of geometric rate.",
                    "label": 0
                },
                {
                    "sent": "Now this thing in the parentheses is strictly less than one, and if Kappa is large, if you got an ill condition problem, it's only a little bit less than one, but it's still strictly less than one, and so this is actually a lot better than this, right?",
                    "label": 0
                },
                {
                    "sent": "This is a sub linear convergence rate.",
                    "label": 0
                },
                {
                    "sent": "This is geometric, so if you can get a geometric convergence rate, you should certainly.",
                    "label": 0
                },
                {
                    "sent": "You know, go with that, but you've had to assume, or you've had to assume, strong convexity here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, so we've got 1 / K under the convexity assumption.",
                    "label": 0
                },
                {
                    "sent": "Without strong convexity we can get 1 / K. In fact, Nesterov has this nice example in his book from 2004 that shows that that really the best you can do with a with a method that moves along the gradient direction is 1 / K ^2.",
                    "label": 1
                },
                {
                    "sent": "Then I won't go too much into the details of this, except it's a simple quadratic function where the Hessian is tridiagonal Ann, and it turns out that if you start at zero and take the kinds of steps that I've mentioned.",
                    "label": 1
                },
                {
                    "sent": "Even if you make really good choices of Alpha, you can only resolve at least one component of X at each iteration, and so after I say an over 2 steps, you're only the best you can hope for is to get the first N / 2 components right?",
                    "label": 0
                },
                {
                    "sent": "So you're always going to have error in the last.",
                    "label": 0
                },
                {
                    "sent": "You know, you know the ones you haven't touched yet, and you can show it for this example that the difference between F&F star after K iterations is at least some multiple of 1 / K ^2.",
                    "label": 1
                },
                {
                    "sent": "And so you can't really do better than K squared.",
                    "label": 1
                },
                {
                    "sent": "So in a sense, this is kind of a speed limit.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in a moment we'll see methods that sort of achieve this.",
                    "label": 0
                },
                {
                    "sent": "This speed limit.",
                    "label": 0
                },
                {
                    "sent": "Now you might ask, so I took.",
                    "label": 0
                },
                {
                    "sent": "So far I've looked at really, really simple choices of Alpha just by 1 / L Two overview plus L. What if you really try hard to find the exact minimizing Alpha every time you take a step, you picked the perfect Alpha, the one that gives you the best possible value of F along that step.",
                    "label": 1
                },
                {
                    "sent": "Can you do better?",
                    "label": 0
                },
                {
                    "sent": "Well, let's try that idea out with a convex quadratic.",
                    "label": 0
                },
                {
                    "sent": "Where a is fixed here.",
                    "label": 0
                },
                {
                    "sent": "OK so if I you know if I take the gradient the gradient this case is just a times X an I search for the exact minimizing Alpha.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it's the argument of this.",
                    "label": 0
                },
                {
                    "sent": "I just get this by plugging X minus Alpha grad F into the definition of F. I find this explicit formula for Alpha and you can show that if you use that Alpha you get this convergence rate.",
                    "label": 0
                },
                {
                    "sent": "Again, it's a geometric rate at least, but you can see that the constant is the same constant.",
                    "label": 0
                },
                {
                    "sent": "I got two slides ago when I was taking this.",
                    "label": 0
                },
                {
                    "sent": "You know very naive step length, so it looks like I haven't gained anything at all.",
                    "label": 0
                },
                {
                    "sent": "From finding the exact minimizer, OK, I get the same sort of convergence rate, which is a little bit surprising.",
                    "label": 0
                },
                {
                    "sent": "But it does maybe point out the limitations of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just moving in the negative gradient direction.",
                    "label": 0
                },
                {
                    "sent": "So here's sort of a breakthrough idea, and in its simplest form it was called the heavy ball method and appeared in the Russian literature and maybe 30 years ago or so.",
                    "label": 0
                },
                {
                    "sent": "And the idea is, instead of just stepping in the negative gradient direction, you toss in a component which is equal to the last step that you just took.",
                    "label": 0
                },
                {
                    "sent": "So now the now your step is a combination of the latest gradient and the previous step.",
                    "label": 1
                },
                {
                    "sent": "It's called Heavyball cousin.",
                    "label": 0
                },
                {
                    "sent": "In a sense, this is kind of a momentum term, right?",
                    "label": 0
                },
                {
                    "sent": "It's kind of your tending to move along the same direction that you just moved in, but you tweak it a little bit with the latest gradient, OK?",
                    "label": 0
                },
                {
                    "sent": "Now you've got two knobs to turn.",
                    "label": 0
                },
                {
                    "sent": "You get to choose not only Alpha, but also beta.",
                    "label": 0
                },
                {
                    "sent": "So the question is by turning, those two knobs, can you get a faster convergence rate?",
                    "label": 0
                },
                {
                    "sent": "Well, this turns out to be easy to analyze two and the way you analyze it is you just collapse two successive errors into a single vector.",
                    "label": 1
                },
                {
                    "sent": "So I formed this vector WK which is dimension 2 N. An I look at what this recurrences in terms of WK.",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out it's approximately this get from WKWK plus one year approximately operate on WK with a linear operator B or a matrix B2N by two and matrix that looks like this and there's a little remainder term which is due to the nonlinearity in F OK. And so what's be look like, well, be is a Block 2 by two matrix very simple, except that there's a Hessian of F appearing here, and when we are close to the solution, you know this WK will be small and be will be very close.",
                    "label": 0
                },
                {
                    "sent": "Will be is essentially this.",
                    "label": 0
                },
                {
                    "sent": "So our aim is then to choose Alpha and beta so that this B has nice spectral properties and you'll know that when you got a linear recurrence and you're looking to converge to zero, the name of the game is to try and make sure your occurrence matrix B has its eigenvalues strictly less than one and as much less than one as you can possibly make them.",
                    "label": 0
                },
                {
                    "sent": "So we're going to try to choose Alpha and beta to minimize the maximum eigenvalue of B.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's the game.",
                    "label": 0
                },
                {
                    "sent": "Now you can do a similarity transformation on beta.",
                    "label": 0
                },
                {
                    "sent": "Just reduce this hash in at the extar to just its diagonal with just the eigen values on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "And when you look at that, you can sort of now write down what the eigenvalues are.",
                    "label": 0
                },
                {
                    "sent": "Pretty much just in closed form, and it turns out that you know it's pretty simple.",
                    "label": 0
                },
                {
                    "sent": "A little bit of simple arithmetic to figure out what Alpha and beta are, and here they are.",
                    "label": 0
                },
                {
                    "sent": "Your optimal Alpha is that your optimal beta is that if you got a new condition problem where caparas large beta is pretty close to one, health is going to be pretty small in general, OK, and so you're only taking a little bit of a nudge in the negative gradient direction.",
                    "label": 0
                },
                {
                    "sent": "You're tending mostly to move in the same direction as the previous step.",
                    "label": 0
                },
                {
                    "sent": "That should talk.",
                    "label": 0
                },
                {
                    "sent": "But this is the convergence rate.",
                    "label": 0
                },
                {
                    "sent": "OK, it looks like the linear convergence rate that we were getting earlier, but there's a really important difference.",
                    "label": 0
                },
                {
                    "sent": "Not only that, we've just got the square root of Kappa in the denominate here, not Kappa, and that can be a huge difference.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, in fact, if your goal is to take is to reduce your initial error by a factor epsilon.",
                    "label": 1
                },
                {
                    "sent": "Say you want to figure out how many steps do I need to take to reduce the error by epsilon.",
                    "label": 1
                },
                {
                    "sent": "If you've got a rate constant like this, it's going to take you something like Kappa over 2 log epsilon steps.",
                    "label": 0
                },
                {
                    "sent": "If you got a rate constant like this, the one we just got, it's going to take your root cap over 2.",
                    "label": 1
                },
                {
                    "sent": "So if CAP is 100, which is still at moderately conditioned problem, not too bad, you need 10 times as many steps.",
                    "label": 0
                },
                {
                    "sent": "If you're using one of these constants so the heavy ball method can be a remarkable improvement on just tonight.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We've gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Now conjugate gradient is probably something I love.",
                    "label": 1
                },
                {
                    "sent": "You have heard of in, you know if you've taken any kind of linear algebra course or optimization course.",
                    "label": 0
                },
                {
                    "sent": "The idea of conjugating is very, very similar to heavy ball.",
                    "label": 0
                },
                {
                    "sent": "In fact, you know they really almost identical innocence.",
                    "label": 0
                },
                {
                    "sent": "They're motivated maybe slightly differently, but you can see in kanji gradient the step that you take is in a direction PK&P.",
                    "label": 0
                },
                {
                    "sent": "Again is just a combination of the latest gradient and the previous step.",
                    "label": 0
                },
                {
                    "sent": "OK, and really conjugating there are just different methods of choosing how to balance off these two components.",
                    "label": 0
                },
                {
                    "sent": "And as I said, motivated a little bit differently.",
                    "label": 0
                },
                {
                    "sent": "In fact, we can identify it with a heavy ball method by.",
                    "label": 1
                },
                {
                    "sent": "Noting the relationship between the gamma in the concert gradient an the beta in the heavy wall and we'll get you know this is pretty much the same thing.",
                    "label": 0
                },
                {
                    "sent": "Now there are many variants of CG CG of course originally came up as a way to solve linear equations with symmetric positive definite coefficient matrices.",
                    "label": 0
                },
                {
                    "sent": "But that's the same as minimizing a quadratic convex objective, and you can generalize that to methods for minimizing general convex objective functions F. And so there have been a variety of methods proposed over the last 30 years which basically are different ways of choosing Alpha and gamma in these formula up here.",
                    "label": 0
                },
                {
                    "sent": "So Alpha, you generally choose to sort of get an approximate minimum of F along that direction and gamma you choose again by some formula.",
                    "label": 0
                },
                {
                    "sent": "Again, each of these different methods has a different way to choose gamma.",
                    "label": 0
                },
                {
                    "sent": "They all collapse to be the same thing.",
                    "label": 0
                },
                {
                    "sent": "If there happens to be a quadratic, OK.",
                    "label": 1
                },
                {
                    "sent": "There's a very rich convergence theory associated with kanji grading, which I won't go into.",
                    "label": 0
                },
                {
                    "sent": "You know the results, like if the if the Hessian has a favorable spectrum with the eigenvalues of the Hessian and kind of clustered, you tend to get very fast convergence and the number of steps is related to the number of clusters and things like that.",
                    "label": 0
                },
                {
                    "sent": "But ultimately the rate that the thing that we're most interested in here is that if you're dealing with a quadratic, if you get a linear convergence again with with one of these constants 1 to 1 -- 2 over root Kappa.",
                    "label": 0
                },
                {
                    "sent": "So like just like the heavy ball method, much more favorable and just steepest descent.",
                    "label": 0
                },
                {
                    "sent": "So if you look in chapter five of my book with Knoxville in 2006, there's a description of.",
                    "label": 0
                },
                {
                    "sent": "Conjugate gradient there.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is where things start getting a little bit spooky when we talk about accelerated 1st order methods.",
                    "label": 0
                },
                {
                    "sent": "Now I've talked about methods that converge like 1 /, K sub linear rate.",
                    "label": 1
                },
                {
                    "sent": "If you if all you know is the function is convex and we talked about methods that have a linear rate related to the square root of conditioning.",
                    "label": 1
                },
                {
                    "sent": "So first of all can you do better than that?",
                    "label": 1
                },
                {
                    "sent": "And Secondly can you have a method that sort of automatically chooses the step length to get the best of both worlds?",
                    "label": 0
                },
                {
                    "sent": "OK and in fact there is such a method and I think Nesterov came up with it in 1983.",
                    "label": 0
                },
                {
                    "sent": "And I've heard that he sort of just came up with it by accident by sort of messing around with formulas and things fell into place.",
                    "label": 0
                },
                {
                    "sent": "He's giving a talk by the way, on the weekend at the NIPS workshops up in Whistler.",
                    "label": 0
                },
                {
                    "sent": "So he'll be talking about things related to this, I'm sure.",
                    "label": 0
                },
                {
                    "sent": "But it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit mysterious because.",
                    "label": 0
                },
                {
                    "sent": "Because you essentially have two sequences instead of one, OK, you don't just have a sequence XC, you also maintain a sequence.",
                    "label": 0
                },
                {
                    "sent": "YK and the two things kind of interleaved with each other.",
                    "label": 0
                },
                {
                    "sent": "You generate one of each on each iteration.",
                    "label": 0
                },
                {
                    "sent": "So you start off with setting the same initial point for both sequences, and you get the next XC by just taking a standard short step gradient step from the latest YC.",
                    "label": 0
                },
                {
                    "sent": "So this is what we've been doing so far in the first few slides.",
                    "label": 0
                },
                {
                    "sent": "Now this is where things get weird.",
                    "label": 0
                },
                {
                    "sent": "You sort of do this.",
                    "label": 0
                },
                {
                    "sent": "You solve a couple of scalar equations to find an Alpha K inabata K. And they depend on this this Kappa, so you should give it an estimate of Kappa that conditioning your estimate of the conditioning.",
                    "label": 0
                },
                {
                    "sent": "Other problem.",
                    "label": 0
                },
                {
                    "sent": "And then you set the new Y by taking a combination of the last step.",
                    "label": 0
                },
                {
                    "sent": "You take the latest X and you add on some multiple of the previous step in X.",
                    "label": 0
                },
                {
                    "sent": "That gives you the new why?",
                    "label": 0
                },
                {
                    "sent": "Now I'm not going to make any attempt at all to explain this to you, 'cause it's quite mysterious, and even when you see the analysis.",
                    "label": 0
                },
                {
                    "sent": "Even that's not very illuminating as to what's happening.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I can at least draw a picture of what's going on.",
                    "label": 0
                },
                {
                    "sent": "These Red Arrows here are just the steepest descent steps we take from each Y to get to the next X.",
                    "label": 0
                },
                {
                    "sent": "And then to get from the new X to the next Y, we sort of take the previous X step and we keep heading in that direction by some distance beta K OK, which comes out of that weird formula alright?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there's a second method which is somewhat simpler.",
                    "label": 0
                },
                {
                    "sent": "It's due to beckon tabule and it's very very similar in the sense that you also get the next X by taking a short step gradient from the latest Y.",
                    "label": 0
                },
                {
                    "sent": "And again, here's the equation you have to solve is a little bit easier, and again you just extrapolate from the previous ex step, keep going in that direction by this quantity related to this TK thing and you get to the next why that way?",
                    "label": 0
                },
                {
                    "sent": "So it's very, very simple.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what convergence rates do we get for this method?",
                    "label": 0
                },
                {
                    "sent": "Well, again, it is the sort of the best of these of geometric great things related to square root of Kappa, which is good.",
                    "label": 0
                },
                {
                    "sent": "That's the best geometric rate we've seen so far, even for the heavy ball method.",
                    "label": 0
                },
                {
                    "sent": "And if you've only got weak convexity, that is when Kappa is Infinity, then this thing is 1.",
                    "label": 0
                },
                {
                    "sent": "So it's not very helpful.",
                    "label": 0
                },
                {
                    "sent": "But then in that case you get a 1 / K ^2 right?",
                    "label": 0
                },
                {
                    "sent": "And that's sort of the speed limit.",
                    "label": 0
                },
                {
                    "sent": "So so this is in some sense sort of an optimal behavior from this kind of method.",
                    "label": 0
                },
                {
                    "sent": "As I said, the analysis is not very intuitive.",
                    "label": 0
                },
                {
                    "sent": "The analysis in the back end to bull paper.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pfister",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a little bit more intuitive.",
                    "label": 0
                },
                {
                    "sent": "It's about two or three pages and you know it's very technical, but it's.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also very elementary.",
                    "label": 0
                },
                {
                    "sent": "So you know if you're interested, IRA.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you do that paper and you can see how it's done.",
                    "label": 0
                },
                {
                    "sent": "Here's a completely different kind of method due to Basel and bull wine.",
                    "label": 1
                },
                {
                    "sent": "It's a gradient method, so again, you're only taking steps in the negative gradient direction.",
                    "label": 1
                },
                {
                    "sent": "But here you're not interested in decreasing effort every iteration.",
                    "label": 0
                },
                {
                    "sent": "OK, you're prepared to sacrifice temporary increases in F in order to get some sort of faster overall rate.",
                    "label": 1
                },
                {
                    "sent": "Now what if you look at this formula?",
                    "label": 0
                },
                {
                    "sent": "Here we've been looking at this already, but if this Alpha were replaced by the inverse of the Hessian.",
                    "label": 0
                },
                {
                    "sent": "That would be Newton's method.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a workhorse method in nonlinear optimization.",
                    "label": 0
                },
                {
                    "sent": "What they do in Basel?",
                    "label": 0
                },
                {
                    "sent": "Boy is take a one parameter approximation.",
                    "label": 0
                },
                {
                    "sent": "They think of this.",
                    "label": 1
                },
                {
                    "sent": "Alpha is a one parameter approximation of the inverse Hessian.",
                    "label": 0
                },
                {
                    "sent": "And I choose the value of Alpha by making it behind behave like the inverse session would have behaved according to Taylor's theorem, over the step that you just took.",
                    "label": 0
                },
                {
                    "sent": "So they look at the difference of the last two gradients and they look at the difference of the last two X is and they say let me choose Alpha so that it sort of makes ass approximately equal Alpha times Y.",
                    "label": 0
                },
                {
                    "sent": "So it sort of does the same thing as the inverse Hessian would do.",
                    "label": 0
                },
                {
                    "sent": "And I use that as the step length and you can solve this formula exactly just explicitly just by doing these two in a product in a division.",
                    "label": 0
                },
                {
                    "sent": "OK, you can show that that at least lies in the spectrum.",
                    "label": 0
                },
                {
                    "sent": "If you want this to approximate the inverse Hessian, at least it falls somewhere between 1 / L and one over mu, which is the spectrum of the inverse Hessian.",
                    "label": 0
                },
                {
                    "sent": "So it passes that sanity check.",
                    "label": 0
                },
                {
                    "sent": "OK, now how does it behave?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, here's a picture.",
                    "label": 0
                },
                {
                    "sent": "This is the classic picture.",
                    "label": 0
                },
                {
                    "sent": "You may well have seen before of applying steepest descent to a quadratic function in two variables.",
                    "label": 1
                },
                {
                    "sent": "So if you got a convex quadratic in two variables, it's minimum is obviously right there.",
                    "label": 0
                },
                {
                    "sent": "It's the contours are elliptic, unless the eigenvalues happen to be identical, in which case this circular if you do steepest descent on that where you try to enforce a decrease in effort every step and you start out here, you tend to get the zigzagging behavior intended.",
                    "label": 0
                },
                {
                    "sent": "Just go back and forth across the Valley.",
                    "label": 0
                },
                {
                    "sent": "And there's still until the steps become so short that they just gives up and stop.",
                    "label": 0
                },
                {
                    "sent": "And you still might be quite a distance from the solution when that happens in Basel I born, they tend to take steps that sort of ridiculously overshoot the minimum along this direction and go way up here somewhere, and then repeat the process again, move back in the negative gradient direction, and come back here and you can see that these intermediate idiots are going to have function values that are absolutely huge.",
                    "label": 0
                },
                {
                    "sent": "But they're sort of setting you up so that the next iteratee makes further makes a more progress along the Valley.",
                    "label": 0
                },
                {
                    "sent": "Then it's sort of a greedy this greedy zigzagging would have done OK, so you can see that least there's potential if you're looking at what happens over a series of steps to make more, faster progress.",
                    "label": 0
                },
                {
                    "sent": "In fact, in their original paper from 1988, which is well worth reading.",
                    "label": 0
                },
                {
                    "sent": "It's only like 8 or 9 pages long, but there's some very nonstandard analysis using some very strange.",
                    "label": 0
                },
                {
                    "sent": "Very strange math to show why when you apply to a two variable quadratic.",
                    "label": 0
                },
                {
                    "sent": "This thing makes sense even though you're not.",
                    "label": 0
                },
                {
                    "sent": "You've got this so called non monotone method.",
                    "label": 0
                },
                {
                    "sent": "You are not trying to decrease F every time.",
                    "label": 0
                },
                {
                    "sent": "There are many, many variants of this OK. And a lot of magic and you.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You silly things like you can hold Alpha constant for three or four or five successive iterations.",
                    "label": 1
                },
                {
                    "sent": "That seems to help.",
                    "label": 0
                },
                {
                    "sent": "You can take out for to be the exact best step from the previous iteration, which again seems like a bizarre thing to do.",
                    "label": 1
                },
                {
                    "sent": "All of these things seem to help.",
                    "label": 0
                },
                {
                    "sent": "There is some analysis for this, but only in fairly restricted cases as being quite a bit of literature recently by these authors.",
                    "label": 0
                },
                {
                    "sent": "If you Google these names, you'll find it, but you know it's not especially well understood.",
                    "label": 0
                },
                {
                    "sent": "It's just that in certain.",
                    "label": 0
                },
                {
                    "sent": "Problems seems to do the right thing.",
                    "label": 0
                },
                {
                    "sent": "OK, primal dual averaging.",
                    "label": 0
                },
                {
                    "sent": "This is again returning to these kinds of methods that I.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was talking about earlier.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that you take a step on the basis of forming a linear approximation to F, But you don't just take the linear approximation around the latest point XK.",
                    "label": 0
                },
                {
                    "sent": "What you do is average all the previous X is that you visited.",
                    "label": 0
                },
                {
                    "sent": "All the exercise that you visited up to now.",
                    "label": 0
                },
                {
                    "sent": "And you take your linear model to be the average of the linear models at all the previous steps.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm dividing by K + 1 here.",
                    "label": 0
                },
                {
                    "sent": "And you add on this term at the end to kind of stabilize things, right?",
                    "label": 0
                },
                {
                    "sent": "You sort of anchor yourself at the initial iterate X nought.",
                    "label": 0
                },
                {
                    "sent": "And you say that you have a convex quadratic term here to stop you from straying too far off into the you know off into the outer reaches of the solution space.",
                    "label": 0
                },
                {
                    "sent": "Now this is easy to minimize 'cause this is just a linear term plus a quadratic term, so you can write down explicitly what XC plus one is.",
                    "label": 0
                },
                {
                    "sent": "And you can see that you're sort of taking a linear model where your gradient estimate, as I said, is the average of all the gradients you found so far.",
                    "label": 0
                },
                {
                    "sent": "What can you put well?",
                    "label": 0
                },
                {
                    "sent": "Why are we interested in this kind of method?",
                    "label": 0
                },
                {
                    "sent": "One reason is that you can generalize this pretty well if you're dealing with a non smooth function.",
                    "label": 0
                },
                {
                    "sent": "For instance, you can replace each of these guys with an element of the subgradient and the convergence theory is almost the same, so this has the advantage that it generalizes pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Windows Drive analyzes this.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "The paper on this came out just last year and.",
                    "label": 0
                },
                {
                    "sent": "A lot of people have been reading his paper.",
                    "label": 0
                },
                {
                    "sent": "It's been quite an important paper, but when he analyzes it, he focuses not on the sequence of X is, but on the sequence of running averages of the axis.",
                    "label": 0
                },
                {
                    "sent": "So you take this X bar K to be the average of all the X is that you've encountered so far, and what you can show is that the function values at the X bar case converge like one over root K. OK, now that's lower than what we've been seeing so far.",
                    "label": 0
                },
                {
                    "sent": "But as I said, it's advantage that it generalizes nicely.",
                    "label": 0
                },
                {
                    "sent": "The case where F is non smooth, it also generalizes nicely to where you're only estimating the gradient where you don't actually have a precise gradient, but just an estimate.",
                    "label": 0
                },
                {
                    "sent": "And again, the analysis is in that case of course you're dealing with expectations of F, But the analysis is very similar, so it has that advantage.",
                    "label": 0
                },
                {
                    "sent": "OK, I think I'm going backwards.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now everything I've said so far pertains to the unconstrained case.",
                    "label": 1
                },
                {
                    "sent": "We just try to minimize F, but of course often you want to actually stay inside some feasible set.",
                    "label": 0
                },
                {
                    "sent": "So how do all these methods change when you start requiring that when you want all your iterates to stay in some closed convex set Omega?",
                    "label": 1
                },
                {
                    "sent": "Well, most of the methods I've described so far or a lot of them at least.",
                    "label": 0
                },
                {
                    "sent": "Generalized very easily to this case.",
                    "label": 0
                },
                {
                    "sent": "In fact, the one I've just been talking about, this primal dual averaging where you've got this average gradient in this prox term.",
                    "label": 1
                },
                {
                    "sent": "If you just trust in the restriction here that you also want X when you're finding the new iteratee, you just want to restrict it to being an Omega, that's it, you know the subproblem is exactly the same, except that you're adding this restriction.",
                    "label": 0
                },
                {
                    "sent": "And again, the analysis is almost unchanged, so this method has the advantage that it generalizes immediately to the case.",
                    "label": 0
                },
                {
                    "sent": "Uh, vay convex constraint set.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similarly, Nesterov's method, one of the ones I talked about earlier this accelerated method with the magic here to select the Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "Again, it's completely unchanged.",
                    "label": 0
                },
                {
                    "sent": "All of this is exactly the same as what I had before, except for this step where you move, you get the new X from the latest Y by taking a step in the negative gradient direction that has to change because we want the wise to stay inside the set Omega.",
                    "label": 0
                },
                {
                    "sent": "And in fact, this is how it changes you.",
                    "label": 0
                },
                {
                    "sent": "Choose the next X to be as close as you can get to that point that you would have gone to, and the unconstrained case while staying in Omega.",
                    "label": 0
                },
                {
                    "sent": "Alright, that's really the only change you make to the method.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the convergence theory pretty much stays the same.",
                    "label": 0
                },
                {
                    "sent": "Now, what about when you've got not an explicit constraints that Omega, but you are trying to minimize F plus tower time, some regularization term?",
                    "label": 0
                },
                {
                    "sent": "And this was the topic of Abhumans tutorial this morning.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of applications like compressed sensing and lasso and so on.",
                    "label": 0
                },
                {
                    "sent": "We've got some smooth objective here and you've got something non smooth here but simple like an L1 norm.",
                    "label": 0
                },
                {
                    "sent": "So how do these methods generalize to this case?",
                    "label": 0
                },
                {
                    "sent": "All the methods I've talked almost of assumed smoothness, but they are sort of generalizable to nonsmooth.",
                    "label": 0
                },
                {
                    "sent": "So what do you do in that case?",
                    "label": 0
                },
                {
                    "sent": "Well, again, a lot of them generalize if you can explicitly handle this site.",
                    "label": 0
                },
                {
                    "sent": "If the regularization term, if you can deal with that explicitly, they often generalize very easily.",
                    "label": 0
                },
                {
                    "sent": "So, for example, the one where we would move where we would just take a short step in the negative gradient direction.",
                    "label": 0
                },
                {
                    "sent": "If I can replace that by minimizing this thing, which is, you know, as close as you can get to that short step, plus the regularizer, exactly as it appears here.",
                    "label": 0
                },
                {
                    "sent": "If this subproblem is easy to solve, which it is in many cases, such as when this is the L1 norm, then most of those methods like nest rows, method gradient methods, and so on generalize immediately.",
                    "label": 0
                },
                {
                    "sent": "The theory can go right through, so the Pfister method, for instance, which is one of the ones I had up earlier on.",
                    "label": 0
                },
                {
                    "sent": "It immediately general is in fact the analysis in Vista just works with this regularised case, and if you care about is young constraint thing, you can just set this to zero and it'll still work exactly the same.",
                    "label": 0
                },
                {
                    "sent": "So the bottom line here is that for a lot of unconstrained gradient, simple gradient 1st order methods.",
                    "label": 0
                },
                {
                    "sent": "It's often not that big a deal if you want to toss in a regularization term or an explicit constraint set.",
                    "label": 0
                },
                {
                    "sent": "So this is the end of the first part.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are a few references where you can go for further information and I'd be happy to give you more if you need more.",
                    "label": 0
                },
                {
                    "sent": "There's a 2004 book by Nurse Drivers is paper by back into Bull.",
                    "label": 0
                },
                {
                    "sent": "Actually there's a 2007 or 8 paper.",
                    "label": 0
                },
                {
                    "sent": "They've written a review paper which you can get from tables website, which is a little more self contained and up-to-date.",
                    "label": 0
                },
                {
                    "sent": "There's an old book by Polyak which is where the heavy ball come from.",
                    "label": 0
                },
                {
                    "sent": "It's very beautiful book actually.",
                    "label": 0
                },
                {
                    "sent": "It's got conjugate gradient and and and.",
                    "label": 0
                },
                {
                    "sent": "Steepest descent and things like that.",
                    "label": 0
                },
                {
                    "sent": "And there's the Basel born paper that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "And then there's this very recent paper by Nest Drop.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second part of the talk.",
                    "label": 1
                },
                {
                    "sent": "This is the part where we can't even get a first derivative.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to talk about stochastic and incremental gradient, so the setting here is that I'm going to allow F to be non smooth.",
                    "label": 1
                },
                {
                    "sent": "I'm going to assume that you know I can't economically get a function value, so I can't go off and test a point to see how good it is.",
                    "label": 0
                },
                {
                    "sent": "At least that you know I can't reliably do that, and I'm going to assume that at any point I've got access to some sort of estimate of a gradient or a sub gradient.",
                    "label": 0
                },
                {
                    "sent": "In the case of a non smooth function.",
                    "label": 0
                },
                {
                    "sent": "So here's a couple of common problems that fall into this setting.",
                    "label": 0
                },
                {
                    "sent": "There's one where your objective is actually the way you've got a function F, which depends on your variables X, but also on some random variable and what you're interested in minimizing the expectation of that over the random variable.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's your objective F of X are related.",
                    "label": 0
                },
                {
                    "sent": "Setting is where your F of X is actually made up of the sum of objective.",
                    "label": 0
                },
                {
                    "sent": "Some finite sum.",
                    "label": 1
                },
                {
                    "sent": "OK, so these were each of these is convex and non smooth.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now these would ring a Bell with a lot of you.",
                    "label": 0
                },
                {
                    "sent": "Because SVM, the standard SVM setting has this form.",
                    "label": 0
                },
                {
                    "sent": "Primal SVM typically had some sort of regularizer.",
                    "label": 0
                },
                {
                    "sent": "Here, one or more at two norm squared, and then it has this loss function which is made up over some of losses over each piece of data in the training in the training set.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in linear SVM this loss function L might have this form.",
                    "label": 0
                },
                {
                    "sent": "The variable might be W and your loss function is function of W. Transpose the feature vectors and the classifier and the label.",
                    "label": 0
                },
                {
                    "sent": "Why I OK another very closely related cases logistic regression where the loss functions are the some of these log functions in the regularizer might be a one norm there, so this is something that obviously this setting obviously fits well and machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just mention here of terminology and notation since I'm now dealing more explicitly with nonsmooth functions, I have to talk about subgrade.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, I've got a picture of Subgradients here.",
                    "label": 0
                },
                {
                    "sent": "If this is mine on the graph of my nonsmooth F, you can see that it's mostly smooth, except at this point here, where there's a kink.",
                    "label": 0
                },
                {
                    "sent": "So at that kink I can define a bunch of supporting hyperplanes, and because this is a 1D function, the hyperplanes are just lines.",
                    "label": 0
                },
                {
                    "sent": "And they're just planes that kind of lie below the graph of the function.",
                    "label": 0
                },
                {
                    "sent": "The slopes of each of these lines would be an element of this subgradient.",
                    "label": 0
                },
                {
                    "sent": "OK, so the subgrade in this case would be an Inter.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the real line.",
                    "label": 0
                },
                {
                    "sent": "So in general the subgradient is some sort of convex set.",
                    "label": 0
                },
                {
                    "sent": "We can still talk about strong convexity.",
                    "label": 0
                },
                {
                    "sent": "When I was dealing with smooth functions, mu is a lower bound on the eigenvalues of the hash and I can still even though the Hessian doesn't exist, 'cause I don't even have a grading in the non smooth case.",
                    "label": 0
                },
                {
                    "sent": "I can still talk about modulus of convexity mu and I can still have a relationship that looks like a lower bound on the 2nd order Taylor series.",
                    "label": 0
                },
                {
                    "sent": "OK, so classical stochastic approximation or stochastic gradient descent method.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I assume here is that I've got access at any point X. I've got access to this thing big G of X and PSI, which is some estimate of the gradient of this gradient, or a subgradient at X. OK, so you can get that by sampling site from that distribution that it lives in according to its distribution function and justice justice, valuating G at that point.",
                    "label": 0
                },
                {
                    "sent": "So the basic stochastic approximation scheme is that you just take a step in the direction of Big G. That's it, OK. Notice by the way, again, the critical issue is how do you choose the alphas just as it was earlier?",
                    "label": 0
                },
                {
                    "sent": "And we're going to talk about how to do that.",
                    "label": 0
                },
                {
                    "sent": "I just notice in passing here that the obviously each time you run this, you're going to get a different result because you're going to be sampling and picking a random czyca every time, and obviously your ex K plus one depends on all those eyes that you've encountered so far, so it's a random variable, but depends on all those eyes that you encountered so far.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now here have actually been adventurous and against the advice of my students, I left the analysis in here.",
                    "label": 0
                },
                {
                    "sent": "I figured that even if you couldn't follow it in real time, if you're interested, you could go back and look at it.",
                    "label": 0
                },
                {
                    "sent": "The reason I left it in, is it because it's so simple.",
                    "label": 0
                },
                {
                    "sent": "It really literally fits on two or three slides, and so I thought it was worth putting up.",
                    "label": 0
                },
                {
                    "sent": "Now what we can analyze in this case is the expectation of the error in X.",
                    "label": 0
                },
                {
                    "sent": "So I define this little AK to be the two norm of X K -- X star squared and the expectation of that.",
                    "label": 0
                },
                {
                    "sent": "The assumption I'm going to make is that my subgradient estimate is in such as sort of a bounded variance.",
                    "label": 0
                },
                {
                    "sent": "OK, and this can be guaranteed in a lot of cases of interest that if I take the expectation of the square of Geo verci that's bounded by some M squared OK. Now let's analyze the convergence.",
                    "label": 0
                },
                {
                    "sent": "Now remember, this is the step that I'm taking here.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to look at what happens or what the error is at XK plus one.",
                    "label": 0
                },
                {
                    "sent": "So I'm going from here to here.",
                    "label": 0
                },
                {
                    "sent": "I'm just plugging in the definition of X K + 1.",
                    "label": 0
                },
                {
                    "sent": "And now I'm just expanding out that two norm squared term and I get XK minus X star term.",
                    "label": 0
                },
                {
                    "sent": "Here I get this cross term which is a cross between XK minus X star and the G term, and then I get Alpha squared times the two norm of G. Well, I've just learned how to bound that in expectation, so when I take the expectation of both sides, this gets bounded by M. ^2, that's easy.",
                    "label": 0
                },
                {
                    "sent": "The slightly tricky one is the expectation of his middle term OK, and this is where you have to be a little bit careful, because as I pointed out earlier, the XC depends on all the random variables IK that you've encountered so far, and the expectation of G with respect as I K is in the subgradient.",
                    "label": 0
                },
                {
                    "sent": "So you can sort of use those two facts and use some conditional expected.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans, again, you know it's really not hard.",
                    "label": 0
                },
                {
                    "sent": "To figure out a bound on that middle term and that turns out to be the critical thing.",
                    "label": 0
                },
                {
                    "sent": "And once you get about on the middle term, you find out that.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going back here, you find out that a K plus one is just a K minus.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something noise here, in fact, that something nice is just too mu times Alpha K times.",
                    "label": 0
                },
                {
                    "sent": "OK you can show and then we've got this last term Alpha K ^2 * M ^2.",
                    "label": 0
                },
                {
                    "sent": "So we've got a nice thing here, which kind of gives us some hope that AK is getting smaller.",
                    "label": 0
                },
                {
                    "sent": "The expectation of the error is going down.",
                    "label": 0
                },
                {
                    "sent": "You can see that this multiple here is less than one, so provided this isn't too big, I'm going to be able to decrease.",
                    "label": 0
                },
                {
                    "sent": "AKA at every step.",
                    "label": 0
                },
                {
                    "sent": "So the critical point is, how do I choose these Alpha case to make that happen?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out the magic way to define Alpha K is 1 / K times mu, where mu is the modulus of convexity.",
                    "label": 0
                },
                {
                    "sent": "In case the iteration number.",
                    "label": 0
                },
                {
                    "sent": "So this is where I leave this as an exercise for you.",
                    "label": 0
                },
                {
                    "sent": "If you plug this into this and do about four in equipment, go through a sequence of four very very simple inequality's, you can show that AK is bounded by some constant over 2 * K where the constant depends on the initial error.",
                    "label": 0
                },
                {
                    "sent": "Bound on the variance and the modulus of convexity, so you get in expectation you get sublinear convergence but at a 1 / K rate.",
                    "label": 0
                },
                {
                    "sent": "Now that's really pretty amazing, because when we right back at the start of the talk, I gave you a steepest descent.",
                    "label": 0
                },
                {
                    "sent": "Short steps, keepers descent method that got 1 / K right OK, and here we're getting a 1 / K rate even with this very crude estimate of the gradient, we don't even need an exact gradient.",
                    "label": 0
                },
                {
                    "sent": "We do however need strong convexity in this case, alright, but we're still getting the same rate.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, OK, So what happens if we don't know mu if we don't have a good estimate or if we're not even dealing with a strongly convex function, what can we do in that case?",
                    "label": 1
                },
                {
                    "sent": "Well, it turns out that we can Patch up what the method I've just described.",
                    "label": 0
                },
                {
                    "sent": "In fact, there's this paper by Nemerofsky and Judisch Ian Shapiro, and land that appeared last year, and so I upped.",
                    "label": 0
                },
                {
                    "sent": "Where they describe this and you can recover 1 / sqrt K right?",
                    "label": 0
                },
                {
                    "sent": "So it's not quite as fast, but it's tends to be more robust.",
                    "label": 0
                },
                {
                    "sent": "It tends to be much less sensitive to choice of parameters.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, the main trick is that again, you don't work with the sequence of X is you work with the sequence of running, running weighted averages of the axis.",
                    "label": 0
                },
                {
                    "sent": "Again, these X bar key terms, which are weighted averages according to the values of Alpha.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the magic choice for Alpha in this case is some constant over N * sqrt K square root of the iteration number rather than K itself, and what you can show for that choice is that the F value converges to the optimal value.",
                    "label": 0
                },
                {
                    "sent": "At this rate, log K / K to the 1/2, so not as fast as 1 / K, but still at least it's going to zero at a reasonable rate.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, it's not very sensitive to the choice of this parameter, which is a big advantage over the.",
                    "label": 0
                },
                {
                    "sent": "The standard stochastic approximation in standing static approximation.",
                    "label": 0
                },
                {
                    "sent": "If you get Mew wrong, this is incredibly slow.",
                    "label": 0
                },
                {
                    "sent": "You have to get you have to get Mew as an underestimate of the modulus.",
                    "label": 0
                },
                {
                    "sent": "If you're overestimated, you get a terrible result.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, again I was brave and I put the analysis for this on one or two slides and again my motivation is that it's extremely simple.",
                    "label": 0
                },
                {
                    "sent": "So if you if you want you can go and look at the slides and satisfy yourself that you know how to do it.",
                    "label": 0
                },
                {
                    "sent": "There's really nothing mysterious about this.",
                    "label": 0
                },
                {
                    "sent": "Now here is the argument.",
                    "label": 0
                },
                {
                    "sent": "You basically take that sequence of Acks that I had before, I just changed the indexed I for some reason, but this is exactly the formula we ended up with a moment ago.",
                    "label": 0
                },
                {
                    "sent": "You can use convexity and the fact that your latest GI, which is your estimate of the subgradient, lies in the subdifferential to bound this term below an you can get to this situation where the expectation of the error in F times Alpha Rai.",
                    "label": 0
                },
                {
                    "sent": "Is less than or equal to this quantity here?",
                    "label": 0
                },
                {
                    "sent": "Now you simply sum both sides from one up to from I = 1 up to K. You notice you get some telescoping terms here.",
                    "label": 0
                },
                {
                    "sent": "Everything just tends to collapse.",
                    "label": 0
                },
                {
                    "sent": "And you left with this relationship.",
                    "label": 0
                },
                {
                    "sent": "OK, which depends on the errors in each of the F have encountered so far depends on the sequence of alphas.",
                    "label": 0
                },
                {
                    "sent": "Depends on the initial error.",
                    "label": 0
                },
                {
                    "sent": "A1 is the initial error and depends on M which is your bound on the variance of the gradient estimate.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why you use the fact that you're dealing with X bar K the weighted average of X is.",
                    "label": 0
                },
                {
                    "sent": "Now it's very simple using the convexity of F. This just comes directly from the direction of definition of convexity.",
                    "label": 0
                },
                {
                    "sent": "X bar K is the weighted average of the X case waited by the alphas and according to convexity F of X, Parque is the way to the same weighted average of the FS waited by the Alpha.",
                    "label": 0
                },
                {
                    "sent": "So this is elementary.",
                    "label": 0
                },
                {
                    "sent": "So if I if I use that definition there an divide both sides by the sum of the alphas.",
                    "label": 0
                },
                {
                    "sent": "This is what I get that the expected error in F with the X bar K enter it is bounded above by this thing that involves the sum of the Alpha K squared divided by the sum of the Alpha case.",
                    "label": 0
                },
                {
                    "sent": "And so you have potential here.",
                    "label": 0
                },
                {
                    "sent": "By choosing the Alpha case appropriately, you have potential to squeeze this to zero at a reasonable rate.",
                    "label": 0
                },
                {
                    "sent": "And the answer is I already gave.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the answer a few slides ago.",
                    "label": 0
                },
                {
                    "sent": "The answer is if I choose Alpha K According to this formula I can plug.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It into this formula here.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I get this one of us get this log K of a sqrt K right?",
                    "label": 0
                },
                {
                    "sent": "And here is the analysis here.",
                    "label": 0
                },
                {
                    "sent": "OK uses an elementary bound on the sum of 1 over.",
                    "label": 0
                },
                {
                    "sent": "I use an elementary bound in the denominator.",
                    "label": 0
                },
                {
                    "sent": "This is it.",
                    "label": 0
                },
                {
                    "sent": "Nothing complicated here.",
                    "label": 0
                },
                {
                    "sent": "I wanted to put it out there because I wanted to show you how simple it was.",
                    "label": 0
                },
                {
                    "sent": "There are other things you can do so you could work with a paradigm where you've got a budget of valuations upfront.",
                    "label": 0
                },
                {
                    "sent": "You know you're only going to take any steps, some big N, and using that you can figure out maybe a different way of choosing the alphas like constant step length or so.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So other things you can do.",
                    "label": 0
                },
                {
                    "sent": "You might have heard of Mirror Descent.",
                    "label": 0
                },
                {
                    "sent": "It's a term that Nemerofsky uses a lot in his papers and various other people.",
                    "label": 0
                },
                {
                    "sent": "Mirror descent is pretty much just what I described, but generalized to the setting where instead of using the two norm here, you use some other norm, based maybe on a Bregman distance.",
                    "label": 0
                },
                {
                    "sent": "And also you might restrict attention to a set Omega, so you might design the norm so that you get strong convexity with respect to that norm over the set Omega.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just a generalization of what I've said so far.",
                    "label": 0
                },
                {
                    "sent": "Now, the step that I as I defined it earlier was just XK minus Alpha times G. Now it turns out that's exactly what you get from minimizing this, so this is just another way to write down the definition of this stochastic approximation step.",
                    "label": 0
                },
                {
                    "sent": "But if I write it this way, it gives us a path to generalizing it to the constraint case into the regularised case.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, if we want to.",
                    "label": 0
                },
                {
                    "sent": "Constrain the iterates to this set Omega all I have to do in this problem is to add a constraint.",
                    "label": 0
                },
                {
                    "sent": "ZZ Belong Store mega to this subproblem.",
                    "label": 0
                },
                {
                    "sent": "And I get a lot of the same properties holding OK.",
                    "label": 1
                },
                {
                    "sent": "So this is a little bit about how you design A mirror descent method where you have a normal.",
                    "label": 1
                },
                {
                    "sent": "It's maybe not the Euclidean norm and you have a set that's maybe not the entire space RN.",
                    "label": 0
                },
                {
                    "sent": "So you can you can generate, you can come up with a distance generating function Omega.",
                    "label": 1
                },
                {
                    "sent": "That has this kind of strong convexity property with respect to the norm that you've chosen, which no longer is necessarily necessarily the Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "You define something called approx function, which is sort of the deviation of Omega from linearity.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a difference between.",
                    "label": 0
                },
                {
                    "sent": "The difference in Omega between Z and X -- a linear prediction of the difference.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's the deviation.",
                    "label": 0
                },
                {
                    "sent": "From linearity there's a picture of that on the next slide, so this is my Omega.",
                    "label": 0
                },
                {
                    "sent": "This curved line here the.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "V of X&Z is what you get by taking a tangent at X and seeing how different it is from the true function value.",
                    "label": 0
                },
                {
                    "sent": "By the time you get to Z, OK.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the Bregman distance.",
                    "label": 0
                },
                {
                    "sent": "And some people really like to use the term Bregman a lot, so I thought I'd better tell you what it was.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can, you can use the Bregman distance in this scheme that I described.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Earlier this basic stochastic approximation scheme by replacing the prox term here with the Bregman distance and by plugging in your.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mega.",
                    "label": 0
                },
                {
                    "sent": "In the augment and so this is the generalization of stochastic approximation to the constraint case with the general Bregman distance.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, what are some examples of useful, interesting constraint sets and their appropriate Bregman distance, as well as the one we've been talking bout already?",
                    "label": 1
                },
                {
                    "sent": "Which is where we just use the two norm, which works for any Omega.",
                    "label": 1
                },
                {
                    "sent": "But this is a constraint that a lot of people are interested in, the one where it's a simplex, the one where all the axes are the components of X are constrained to be non negative, and they're constrained to sum up some up to one.",
                    "label": 0
                },
                {
                    "sent": "OK, now in that set it turns out that this Omega vex this entropy function generates this pregnant distance.",
                    "label": 0
                },
                {
                    "sent": "And that this thing has a strong convexity property with respect to this set.",
                    "label": 0
                },
                {
                    "sent": "So you can redefine stochastic approximation using this V and this and this Omega and you can get methods with the same sorts of convergence properties that I've described.",
                    "label": 0
                },
                {
                    "sent": "And I'll just finish up.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This section with a mention of incremental gradient.",
                    "label": 0
                },
                {
                    "sent": "There are people that have been working, particularly it's a kiss and his collaborators have been working in this incremental gradient setting format 20 years or more.",
                    "label": 0
                },
                {
                    "sent": "This is the case where your F specifically is a finite sum of FIS of component objectives and the way these methods typically work is they just grab one of these FIS get its gradient and do something with that.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a special case in a sense of stochastic approximation.",
                    "label": 0
                },
                {
                    "sent": "But there is the option and some variance of incremental gradient sort of cycle around the eyes.",
                    "label": 0
                },
                {
                    "sent": "So they make sure you touch all the eyes.",
                    "label": 0
                },
                {
                    "sent": "Others just pick them randomly with replacement at every step OK.",
                    "label": 0
                },
                {
                    "sent": "So I'll just mention something.",
                    "label": 0
                },
                {
                    "sent": "Bertsekas has a paper coming out.",
                    "label": 0
                },
                {
                    "sent": "There's a volume called optimization and Machine learning which is going to appear in the Nips Workshop series and some of you here have contributed chapters to it.",
                    "label": 0
                },
                {
                    "sent": "I think it should appear sometime in the next five or five months I believe, but some cases contributed a chapter to that which deals with this problem, and he mentions a lot of different variants of methods of this type.",
                    "label": 0
                },
                {
                    "sent": "For example, you can generalize the heavy ball method that I talked about to this setting.",
                    "label": 0
                },
                {
                    "sent": "Where instead of using the grade of F, you just use the latest grade of FI.",
                    "label": 0
                },
                {
                    "sent": "In this time here.",
                    "label": 0
                },
                {
                    "sent": "You can get approaches like dual averaging where you have a cyclic choice of the eyes and you just take your linear function.",
                    "label": 1
                },
                {
                    "sent": "Your linear approximating function to be the average of those choices over the last M iterations, where M is the number of terms in the sum.",
                    "label": 0
                },
                {
                    "sent": "As a recent paper by Al Hero and some people and some collaborators on that topic.",
                    "label": 0
                },
                {
                    "sent": "In the basic.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Method where you just take a negative gradient step in the latest component that you've selected.",
                    "label": 0
                },
                {
                    "sent": "But Sankus gets these bounds which at least tell you that you're not converging to something crazy.",
                    "label": 0
                },
                {
                    "sent": "So if you've got a constant step length scheme where you're always moving the same distance Alpha, he shows that in the limit you at least get within this far of the optimal F OK.",
                    "label": 0
                },
                {
                    "sent": "If you do a cyclic choice, you get one bound on the deviation.",
                    "label": 1
                },
                {
                    "sent": "If you do a random choice, you get a different bound, so it's not guaranteeing convergence of of F to the true value, but it's showing that you're not doing anything crazy.",
                    "label": 0
                },
                {
                    "sent": "And it also points out a way that you can consider taking varying choices of Alpha.",
                    "label": 0
                },
                {
                    "sent": "You could consider embedding this in some method that decreases Alpha as you go, and thereby you might be able to squeeze out a convergence result.",
                    "label": 0
                },
                {
                    "sent": "In that case OK.",
                    "label": 0
                },
                {
                    "sent": "So he actually proves this in the more general context.",
                    "label": 1
                },
                {
                    "sent": "In, particularly considers a case where you've got, you're adding on that non smooth term.",
                    "label": 0
                },
                {
                    "sent": "I should.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mention that these methods stochastic gradient methods have been the subject of a pretty significant stream of research in the in the machine learning community for some years, and I mentioned some codes here, SSD and Pegasus, and numerous people that have been working on this.",
                    "label": 1
                },
                {
                    "sent": "In fact, it was almost I think they work in the two communities.",
                    "label": 0
                },
                {
                    "sent": "That was pretty much independent and the connection was only made fairly recently.",
                    "label": 0
                },
                {
                    "sent": "But the methods end up being very, very similar.",
                    "label": 0
                },
                {
                    "sent": "Similar choices of step length, somewhat related convergence theory.",
                    "label": 1
                },
                {
                    "sent": "There was a tutorial by Naughty Srebro and Tewari at a recent ICML, which went into much more detail on what I've just talked about in the second topic, and I refer you to that For more information, and I mentioned also this stream of related work in online convex programming that I think grew out of this paper of zinkevich.",
                    "label": 0
                },
                {
                    "sent": "I apologize if I don't get my machine learning.",
                    "label": 0
                },
                {
                    "sent": "Reference is correct, but I think this is a paper that a lot of you probably know.",
                    "label": 0
                },
                {
                    "sent": "It's a somewhat different setting because there's no idea assumption.",
                    "label": 0
                },
                {
                    "sent": "As I understand it.",
                    "label": 1
                },
                {
                    "sent": "But again, you're minimizing sort of a sequence of convex functions that are presented to you after each time you take a step, and you get similar sorts of convergence.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dates to what I was showing earlier.",
                    "label": 0
                },
                {
                    "sent": "So here's my reference list for Part 2 of the talk.",
                    "label": 0
                },
                {
                    "sent": "I think I'll just leave that up there and we should take a 10 minute break or so.",
                    "label": 0
                }
            ]
        }
    }
}