{
    "id": "twzjpt5jzcoohaxdez4wpj7yvra4ut5k",
    "title": "How overall coverage of class association rules affects the accuracy of the classifier?",
    "info": {
        "author": [
            "Jamolbek Mattiev, Fakulteta za matematiko, naravoslovje in informacijske tehnologije (FAMNIT), Univerza na Primorskem"
        ],
        "published": "Nov. 14, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/sikdd2019_mattiev_class_association/",
    "segmentation": [
        [
            "Hello everybody, I am John Wick material from was Becky Stan originally and now studying researching at the Slovenia in the University of Primorska.",
            "Today I'm going to present the our research.",
            "This is the preliminary step to our final future work, the how overall coverage of classification rules affect to the accuracy of the classifier.",
            "And we did with my supervisor the prank of check also cost."
        ],
        [
            "And.",
            "The introduction of the classification and Association rule mining are two important data mining techniques.",
            "And.",
            "The researchers have been proposed recently.",
            "The several classification at the method based on class Association rules or the colored also associative classification.",
            "And associate classification is a data mining approach that combines these two classification through mining, the techniques data mining.",
            "And in this work, we propose a simple classification method by pruning by selecting some reasonable number of rules per class.",
            "To build the simple and accurate classifier and then we find a computer overall coverage and the average real coverage of the classifier that we perform.",
            "We have performed experiments on the 11 datasets from UCI.",
            "And.",
            "Then we compare our results with some well known the classification method such as knife base Part Ripper, ANSI 4.5."
        ],
        [
            "And the problem statement is we have given the simple relational table and when we end transactions.",
            "Described by L distinct attributes and classified into see no one classes.",
            "And our goals are the first to generate the complete set of classification rules.",
            "And this strong rules that satisfy user specified minimum support and minimum confidence constraints.",
            "And then we extract some reasonable number of rules strong rules.",
            "To build a classifier.",
            "And here is there is no any method, not an algorithm to choose the reasonable number.",
            "Just we did this by experiment by analyzing.",
            "And the finally we compute the overall coverage of the hour, the classifier and also accuracy."
        ],
        [
            "And for the first step.",
            "To generate the complete set of cards is usually split into two parts.",
            "1st, we generate the frequent item sets.",
            "And then through this frequent itemset, we will generate the classification rules for the first step, we apply the minimum support to generate the frequent itemset.",
            "It suggested to use a priority algorithm.",
            "This is the best one to generate.",
            "Frequent itemset.",
            "Here is the important thing is to apply the minimum support.",
            "If we applied, the minimum support is high then we may lose some interesting strong rules.",
            "And if we set it to the low, this minimum support then we may get.",
            "The huge number of Association rules that's going to lead to combinatorial complexity later, and that's why he really important to apply the appropriate minimum support.",
            "This is also done by experiment by analyzing the data set and 2nd then through this frequent itemsets we generate the this is the straightforward, the step just we generate the class Association rules through these frequent itemsets and here we apply also minimum confidence thread.",
            "Threshold."
        ],
        [
            "To extract the strong class Association rules.",
            "And here is our proposed method.",
            "So this is to build the cluster.",
            "The classifier here.",
            "First we generate the from first to the third line, we generate the all class complete set of strong classes, assessor rules, and then we sort them by confidence and then by support.",
            "Here is the sorting criteria.",
            "If the confidence are.",
            "The equal then we checked.",
            "We sort by support and if support are also equal.",
            "Then we take the one that has the field attribute in the left hand side of the rule.",
            "And so then we group the older class Association rules.",
            "And by their class label if for example class values 3, then we group all the classes officials into three groups.",
            "And then.",
            "After grouping we extract.",
            "Here is also very important to apply this the number we.",
            "Extract the.",
            "Here is K. The extract the number of rules that are equal to K from each group.",
            "To form our final classifier.",
            "Here we can."
        ],
        [
            "See with the example.",
            "It started from already generated the class social rules here.",
            "And here is support and the confidence.",
            "That's already satisfied.",
            "The minimum support and minimum confidence constraints."
        ],
        [
            "So in the first step we sort the cards by the confidence and then by the support."
        ],
        [
            "And then in the next step we group all the classes are special rules by their class values.",
            "This is the first class, the second class and 3rd class."
        ],
        [
            "And in the last step.",
            "We here we apply the new rules, the number of rules per class is 2 and so we.",
            "Extract the first 2 rules from each group because we already sorted and the all the strong class Association rules and are on the head.",
            "So we extracted 2 rules from each group.",
            "Here this is our final classifier and we will classify the example by this classifier and here is.",
            "This is based on selected based on the number of rules per class.",
            "Yeah, but which which rules you extract for each class?",
            "The ones with the biggest supporter confidence, strong class Association rules that we already sorted and so top from the top.",
            "And here is the new example.",
            "So how does it work?",
            "Our classifier our classifier?",
            "At first we will classify this example with our classifier classifier and this example are classified by force fields and the last rule.",
            "And then the corresponding Lee.",
            "The class labels are.",
            "233 of this other rules, and so now our classifier predicts the class label as three cause.",
            "Here is majority.",
            "Then we take the majority if they are equal then we will take the first one.",
            "And if none of our rules can classify the examples, so then we will take the majority class from the data set."
        ],
        [
            "OK, then, in the next algorithm is to compute the overall coverage.",
            "An average roll coverage of the classifier.",
            "So how we compute the overall coverage so?",
            "The recount transaction that are covered by the classifier and divided by the total number.",
            "This is going to be the.",
            "The overall coverage and the average roll coverages.",
            "We count the old transaction that are covered by each rule divided by the total number of the transaction divided by North, and then we take average of each rule and this is the average roll coverage.",
            "And this is the simple algorithm."
        ],
        [
            "OK, so experimental evaluation.",
            "We use the 11 datasets from UCI machine learning repository.",
            "And then we compare our results with.",
            "The well known classification methods.",
            "The four methods that I mentioned before.",
            "And here we used Vaca software, the open source open source software to explore other classification methods and four hours.",
            "Implemented Java program.",
            "So for the testing we use 10 times random split for both our method and for other classification methods.",
            "Anne.",
            "10 times random split method.",
            "And then in order to get enough results, enough rules the for our classifier and to achieve the reasonable overall coverage, we applied the number of rules per class for 50 for all datasets for all experiments, and for other classification algorithms.",
            "We applied.",
            "The default parameters.",
            "Here is the result.",
            "OK, first."
        ],
        [
            "If you analyze the table the hour.",
            "Yeah, our that.",
            "Proposed method achieves the higher accuracy than the average accuracy than C 4.5.",
            "And naive Bayes, average accuracy on average accuracies."
        ],
        [
            "Therefore the standard deviations.",
            "They were higher on Hayastan limp datasets and this means the accuracy.",
            "The accuracy differences between accuracy in 10 times random split.",
            "The testing work fluctuated.",
            "The Ant was reasonable high.",
            "So never achieve were better.",
            "So the average was better.",
            "Our result achieved the better, not yeah better little bit from C45 and naive Bayes slightly higher.",
            "And negligible percentage.",
            "This is part.",
            "This is the Reaper and this is 4.5.",
            "Sorry this is my face.",
            "Yeah this is short and.",
            "And.",
            "The.",
            "For the sound activation and the next."
        ],
        [
            "Person is.",
            "The hour.",
            "Method achieved.",
            "The less than 80% the overall coverage on breast cancer.",
            "And car evaluation and Tic Tac toy datasets.",
            "And also in this data set.",
            "Our method, the achieved worse accuracy than the other classification methods also."
        ],
        [
            "And this means this overall coverage affected to the accuracy also."
        ],
        [
            "And then.",
            "If we say about average roll coverage was surprisingly high on vote and limp and spec datasets, but it seems not to affect the accuracy."
        ],
        [
            "OK, for conclusion and future work, the our comparison on selected this data set.",
            "They show that with the decreasing of overall coverage.",
            "Our method tends to get slightly lower accuracy.",
            "And this fact is not surprising 'cause.",
            "Most the uncovered examples.",
            "Get classified by the majority classifier.",
            "Here we can see also.",
            "On some"
        ],
        [
            "Just a second.",
            "Yeah, here even our accuracy were higher accuracy was higher than the overall coverage.",
            "This means the uncovered examples get classified by the majority classifier.",
            "OK, the vendor code.",
            "It is about 85% the.",
            "The accuracy of our classifier is similar or even better than the C 4.5 and naive Bayes.",
            "Average roll coverage of our process.",
            "Proposed method didn't affect to the accuracy, but in our future work we plan to build more comprehensive, more meaningful classifier.",
            "Based on class action rules and this time we are planning to use.",
            "To consider this over a little coverage when we are selecting the rules.",
            "Before our final classifier.",
            "And this was our preliminary approach to our future work.",
            "And the acknowledgement and.",
            "This is it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everybody, I am John Wick material from was Becky Stan originally and now studying researching at the Slovenia in the University of Primorska.",
                    "label": 0
                },
                {
                    "sent": "Today I'm going to present the our research.",
                    "label": 0
                },
                {
                    "sent": "This is the preliminary step to our final future work, the how overall coverage of classification rules affect to the accuracy of the classifier.",
                    "label": 1
                },
                {
                    "sent": "And we did with my supervisor the prank of check also cost.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The introduction of the classification and Association rule mining are two important data mining techniques.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The researchers have been proposed recently.",
                    "label": 1
                },
                {
                    "sent": "The several classification at the method based on class Association rules or the colored also associative classification.",
                    "label": 0
                },
                {
                    "sent": "And associate classification is a data mining approach that combines these two classification through mining, the techniques data mining.",
                    "label": 1
                },
                {
                    "sent": "And in this work, we propose a simple classification method by pruning by selecting some reasonable number of rules per class.",
                    "label": 1
                },
                {
                    "sent": "To build the simple and accurate classifier and then we find a computer overall coverage and the average real coverage of the classifier that we perform.",
                    "label": 0
                },
                {
                    "sent": "We have performed experiments on the 11 datasets from UCI.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Then we compare our results with some well known the classification method such as knife base Part Ripper, ANSI 4.5.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the problem statement is we have given the simple relational table and when we end transactions.",
                    "label": 0
                },
                {
                    "sent": "Described by L distinct attributes and classified into see no one classes.",
                    "label": 1
                },
                {
                    "sent": "And our goals are the first to generate the complete set of classification rules.",
                    "label": 0
                },
                {
                    "sent": "And this strong rules that satisfy user specified minimum support and minimum confidence constraints.",
                    "label": 1
                },
                {
                    "sent": "And then we extract some reasonable number of rules strong rules.",
                    "label": 0
                },
                {
                    "sent": "To build a classifier.",
                    "label": 0
                },
                {
                    "sent": "And here is there is no any method, not an algorithm to choose the reasonable number.",
                    "label": 0
                },
                {
                    "sent": "Just we did this by experiment by analyzing.",
                    "label": 0
                },
                {
                    "sent": "And the finally we compute the overall coverage of the hour, the classifier and also accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for the first step.",
                    "label": 0
                },
                {
                    "sent": "To generate the complete set of cards is usually split into two parts.",
                    "label": 1
                },
                {
                    "sent": "1st, we generate the frequent item sets.",
                    "label": 0
                },
                {
                    "sent": "And then through this frequent itemset, we will generate the classification rules for the first step, we apply the minimum support to generate the frequent itemset.",
                    "label": 0
                },
                {
                    "sent": "It suggested to use a priority algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is the best one to generate.",
                    "label": 0
                },
                {
                    "sent": "Frequent itemset.",
                    "label": 0
                },
                {
                    "sent": "Here is the important thing is to apply the minimum support.",
                    "label": 1
                },
                {
                    "sent": "If we applied, the minimum support is high then we may lose some interesting strong rules.",
                    "label": 0
                },
                {
                    "sent": "And if we set it to the low, this minimum support then we may get.",
                    "label": 0
                },
                {
                    "sent": "The huge number of Association rules that's going to lead to combinatorial complexity later, and that's why he really important to apply the appropriate minimum support.",
                    "label": 0
                },
                {
                    "sent": "This is also done by experiment by analyzing the data set and 2nd then through this frequent itemsets we generate the this is the straightforward, the step just we generate the class Association rules through these frequent itemsets and here we apply also minimum confidence thread.",
                    "label": 0
                },
                {
                    "sent": "Threshold.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To extract the strong class Association rules.",
                    "label": 0
                },
                {
                    "sent": "And here is our proposed method.",
                    "label": 0
                },
                {
                    "sent": "So this is to build the cluster.",
                    "label": 0
                },
                {
                    "sent": "The classifier here.",
                    "label": 0
                },
                {
                    "sent": "First we generate the from first to the third line, we generate the all class complete set of strong classes, assessor rules, and then we sort them by confidence and then by support.",
                    "label": 0
                },
                {
                    "sent": "Here is the sorting criteria.",
                    "label": 0
                },
                {
                    "sent": "If the confidence are.",
                    "label": 0
                },
                {
                    "sent": "The equal then we checked.",
                    "label": 0
                },
                {
                    "sent": "We sort by support and if support are also equal.",
                    "label": 0
                },
                {
                    "sent": "Then we take the one that has the field attribute in the left hand side of the rule.",
                    "label": 0
                },
                {
                    "sent": "And so then we group the older class Association rules.",
                    "label": 0
                },
                {
                    "sent": "And by their class label if for example class values 3, then we group all the classes officials into three groups.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "After grouping we extract.",
                    "label": 0
                },
                {
                    "sent": "Here is also very important to apply this the number we.",
                    "label": 0
                },
                {
                    "sent": "Extract the.",
                    "label": 0
                },
                {
                    "sent": "Here is K. The extract the number of rules that are equal to K from each group.",
                    "label": 0
                },
                {
                    "sent": "To form our final classifier.",
                    "label": 0
                },
                {
                    "sent": "Here we can.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See with the example.",
                    "label": 0
                },
                {
                    "sent": "It started from already generated the class social rules here.",
                    "label": 0
                },
                {
                    "sent": "And here is support and the confidence.",
                    "label": 0
                },
                {
                    "sent": "That's already satisfied.",
                    "label": 0
                },
                {
                    "sent": "The minimum support and minimum confidence constraints.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the first step we sort the cards by the confidence and then by the support.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then in the next step we group all the classes are special rules by their class values.",
                    "label": 0
                },
                {
                    "sent": "This is the first class, the second class and 3rd class.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in the last step.",
                    "label": 0
                },
                {
                    "sent": "We here we apply the new rules, the number of rules per class is 2 and so we.",
                    "label": 0
                },
                {
                    "sent": "Extract the first 2 rules from each group because we already sorted and the all the strong class Association rules and are on the head.",
                    "label": 0
                },
                {
                    "sent": "So we extracted 2 rules from each group.",
                    "label": 1
                },
                {
                    "sent": "Here this is our final classifier and we will classify the example by this classifier and here is.",
                    "label": 0
                },
                {
                    "sent": "This is based on selected based on the number of rules per class.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but which which rules you extract for each class?",
                    "label": 0
                },
                {
                    "sent": "The ones with the biggest supporter confidence, strong class Association rules that we already sorted and so top from the top.",
                    "label": 0
                },
                {
                    "sent": "And here is the new example.",
                    "label": 1
                },
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "Our classifier our classifier?",
                    "label": 0
                },
                {
                    "sent": "At first we will classify this example with our classifier classifier and this example are classified by force fields and the last rule.",
                    "label": 1
                },
                {
                    "sent": "And then the corresponding Lee.",
                    "label": 1
                },
                {
                    "sent": "The class labels are.",
                    "label": 0
                },
                {
                    "sent": "233 of this other rules, and so now our classifier predicts the class label as three cause.",
                    "label": 1
                },
                {
                    "sent": "Here is majority.",
                    "label": 0
                },
                {
                    "sent": "Then we take the majority if they are equal then we will take the first one.",
                    "label": 0
                },
                {
                    "sent": "And if none of our rules can classify the examples, so then we will take the majority class from the data set.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, then, in the next algorithm is to compute the overall coverage.",
                    "label": 1
                },
                {
                    "sent": "An average roll coverage of the classifier.",
                    "label": 1
                },
                {
                    "sent": "So how we compute the overall coverage so?",
                    "label": 0
                },
                {
                    "sent": "The recount transaction that are covered by the classifier and divided by the total number.",
                    "label": 0
                },
                {
                    "sent": "This is going to be the.",
                    "label": 0
                },
                {
                    "sent": "The overall coverage and the average roll coverages.",
                    "label": 0
                },
                {
                    "sent": "We count the old transaction that are covered by each rule divided by the total number of the transaction divided by North, and then we take average of each rule and this is the average roll coverage.",
                    "label": 1
                },
                {
                    "sent": "And this is the simple algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so experimental evaluation.",
                    "label": 0
                },
                {
                    "sent": "We use the 11 datasets from UCI machine learning repository.",
                    "label": 1
                },
                {
                    "sent": "And then we compare our results with.",
                    "label": 1
                },
                {
                    "sent": "The well known classification methods.",
                    "label": 0
                },
                {
                    "sent": "The four methods that I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "And here we used Vaca software, the open source open source software to explore other classification methods and four hours.",
                    "label": 0
                },
                {
                    "sent": "Implemented Java program.",
                    "label": 0
                },
                {
                    "sent": "So for the testing we use 10 times random split for both our method and for other classification methods.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "10 times random split method.",
                    "label": 0
                },
                {
                    "sent": "And then in order to get enough results, enough rules the for our classifier and to achieve the reasonable overall coverage, we applied the number of rules per class for 50 for all datasets for all experiments, and for other classification algorithms.",
                    "label": 1
                },
                {
                    "sent": "We applied.",
                    "label": 0
                },
                {
                    "sent": "The default parameters.",
                    "label": 0
                },
                {
                    "sent": "Here is the result.",
                    "label": 0
                },
                {
                    "sent": "OK, first.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you analyze the table the hour.",
                    "label": 0
                },
                {
                    "sent": "Yeah, our that.",
                    "label": 0
                },
                {
                    "sent": "Proposed method achieves the higher accuracy than the average accuracy than C 4.5.",
                    "label": 0
                },
                {
                    "sent": "And naive Bayes, average accuracy on average accuracies.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Therefore the standard deviations.",
                    "label": 0
                },
                {
                    "sent": "They were higher on Hayastan limp datasets and this means the accuracy.",
                    "label": 0
                },
                {
                    "sent": "The accuracy differences between accuracy in 10 times random split.",
                    "label": 0
                },
                {
                    "sent": "The testing work fluctuated.",
                    "label": 0
                },
                {
                    "sent": "The Ant was reasonable high.",
                    "label": 0
                },
                {
                    "sent": "So never achieve were better.",
                    "label": 0
                },
                {
                    "sent": "So the average was better.",
                    "label": 0
                },
                {
                    "sent": "Our result achieved the better, not yeah better little bit from C45 and naive Bayes slightly higher.",
                    "label": 0
                },
                {
                    "sent": "And negligible percentage.",
                    "label": 0
                },
                {
                    "sent": "This is part.",
                    "label": 0
                },
                {
                    "sent": "This is the Reaper and this is 4.5.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is my face.",
                    "label": 0
                },
                {
                    "sent": "Yeah this is short and.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "For the sound activation and the next.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Person is.",
                    "label": 0
                },
                {
                    "sent": "The hour.",
                    "label": 0
                },
                {
                    "sent": "Method achieved.",
                    "label": 0
                },
                {
                    "sent": "The less than 80% the overall coverage on breast cancer.",
                    "label": 1
                },
                {
                    "sent": "And car evaluation and Tic Tac toy datasets.",
                    "label": 0
                },
                {
                    "sent": "And also in this data set.",
                    "label": 1
                },
                {
                    "sent": "Our method, the achieved worse accuracy than the other classification methods also.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this means this overall coverage affected to the accuracy also.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "If we say about average roll coverage was surprisingly high on vote and limp and spec datasets, but it seems not to affect the accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, for conclusion and future work, the our comparison on selected this data set.",
                    "label": 1
                },
                {
                    "sent": "They show that with the decreasing of overall coverage.",
                    "label": 1
                },
                {
                    "sent": "Our method tends to get slightly lower accuracy.",
                    "label": 1
                },
                {
                    "sent": "And this fact is not surprising 'cause.",
                    "label": 0
                },
                {
                    "sent": "Most the uncovered examples.",
                    "label": 1
                },
                {
                    "sent": "Get classified by the majority classifier.",
                    "label": 0
                },
                {
                    "sent": "Here we can see also.",
                    "label": 0
                },
                {
                    "sent": "On some",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a second.",
                    "label": 0
                },
                {
                    "sent": "Yeah, here even our accuracy were higher accuracy was higher than the overall coverage.",
                    "label": 0
                },
                {
                    "sent": "This means the uncovered examples get classified by the majority classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, the vendor code.",
                    "label": 0
                },
                {
                    "sent": "It is about 85% the.",
                    "label": 0
                },
                {
                    "sent": "The accuracy of our classifier is similar or even better than the C 4.5 and naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "Average roll coverage of our process.",
                    "label": 0
                },
                {
                    "sent": "Proposed method didn't affect to the accuracy, but in our future work we plan to build more comprehensive, more meaningful classifier.",
                    "label": 0
                },
                {
                    "sent": "Based on class action rules and this time we are planning to use.",
                    "label": 0
                },
                {
                    "sent": "To consider this over a little coverage when we are selecting the rules.",
                    "label": 0
                },
                {
                    "sent": "Before our final classifier.",
                    "label": 0
                },
                {
                    "sent": "And this was our preliminary approach to our future work.",
                    "label": 0
                },
                {
                    "sent": "And the acknowledgement and.",
                    "label": 0
                },
                {
                    "sent": "This is it.",
                    "label": 0
                }
            ]
        }
    }
}