{
    "id": "4zovykw7ac7kfw4nhggyhk3v2gazqg42",
    "title": "Kingman's Coalescent for Hierarchical Representations",
    "info": {
        "author": [
            "Dilan G\u00f6r\u00fcr, Yahoo! Research"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss2012_gorur_kingman/",
    "segmentation": [
        [
            "So I apologize from the beginning if this doesn't flow as well as the other talks.",
            "I had a little short of time to prepare and I will need to sometimes switch between slides and some numbering of the slides are not correct, so hopefully everything else is."
        ],
        [
            "Will work fine.",
            "So as you said, I'll be talking about kingdoms coalescent for hierarchical representations.",
            "So first I'll tell you what I mean by hierarchical representations.",
            "Then I'll tell you what I mean by Kingman's coalescent and then talk about how to how to construct models using King Mills Coalescent and how to sample from the prior, how to generate data from this model.",
            "And then how to do inference and then some applications?"
        ],
        [
            "So hierarchical representations are everywhere.",
            "You're probably familiar with the evolutionary tree.",
            "This is 1 very prominent model of of hierarchy is basically or one prominent way of using hierarchies to represent the life basically."
        ],
        [
            "Or you probably also know the language tree, so the languages are related to each other.",
            "There are language families and.",
            "Bigger family and families and so on.",
            "South languages.",
            "Some languages are more related to each other than other languages, and so on.",
            "So people came up with this kind of a tree to represent the relationship between languages.",
            "You may also."
        ],
        [
            "So be interested in simply visual taxonomies rather than the genetic relationship between things.",
            "You may just want to organize things in terms of their visual appearance is so that.",
            "When you see a particular color of bird, for example, you go to this taxonomy and you scroll through this taxonomy to find the class of bird that you're looking for, rather than having to sort through all the birds and having to look at them one by one so.",
            "Another use of hierarchical representations, therefore, is to represent the data in a good format.",
            "To be able to search quickly through it."
        ],
        [
            "So why do we want to do hierarchical clustering or hierarchical representations?",
            "As I said, you may want to discover underlying structure in the data when we believe that the data has an underlying tree structure, like in modeling geneologies or in modeling language families.",
            "Or we may simply want to visualize data to understand the relationship among data items.",
            "Or we may simply want to summarize data.",
            "And use hierarchical representations as a way to partition data into different, possibly unrelated groups.",
            "So.",
            "And these these?",
            "Goals maybe that doesn't need to be mutually exclusive, so you may want to discover the underlying structure in the data to be able to visualize it better, for example, and so on.",
            "So hierarchical representations gives it gives us shorter search time.",
            "It gives us more intuitive relationship between groups of data points as opposed to flat clustering where the only put you would have is the relationship between the items.",
            "Lying in the same cluster.",
            "In hierarchical clustering you would have different.",
            "Different amount of variability between different groups of clusters because they are lying under a tree for example, and there is more structure in the representation.",
            "So you may be able to capture more information about the data.",
            "So even if the data does not naturally come in a hierarchical form, you may still get a better representation, but a better model of the data by using hierarchical models.",
            "And it's easier and nicer to visualize."
        ],
        [
            "OK, so in this talk I'm using hierarchical clustering and hierarchical representations interchangeably.",
            "Basically I'm using the same type of model Kingman scores, and for doing both heichel representations is something more general than hierarchical clustering, but for ease of presentation I'll be focusing on hierarchical clustering.",
            "When I describe the things, so there are different ways that were developed to do hierarchical clustering.",
            "I think the most prominent one is the traditional agglomerative clustering, where you compute you define a distance between data points and the distance between clusters of data points, and you iteratively start from the bottom, iteratively construct the tree by comparing the distances and combining the clusters of data points until you get to the root.",
            "Another method is probabilistic.",
            "North of clustering, where you define a probability distribution over data points or groups of data points and you have a different criterion this time rather than distance, you use this.",
            "You use something like this, so the probability of the joint.",
            "Of Q&R as opposed the ratio of that with probability of Q alone and R alone so.",
            "Which whichever pair of cluster has a higher score than you merge this, and again you iteratively go until the top.",
            "So these methods assume some probability distribution, but they don't.",
            "They don't model the data, the generative process that generated the data as I have probability distribution.",
            "So there are methods that have done that as well.",
            "So these authors.",
            "Have defined a probability distribution over trees and then define that restructured likelihood and try to do inference to get the posterior of the trees given the data points.",
            "So I'm going to be focusing on these types of models.",
            "Actually this work by tall is the work which uses the kingdoms coalescent as a prior over trees to construct this model."
        ],
        [
            "So as I said in in Bayesian hierarchical clustering, we use a tree structured graphical model as the data generating process.",
            "And we define a prior to compute the posterior over trees and as a prior over trees we use kingman's coalescent.",
            "So why do we want to do Bayesian hierarchical clustering?",
            "And why do we want to use Kingman's curse?",
            "And for that is because first we want a hierarchy because the hierarchical representation is more powerful than flat representations.",
            "The reason why we want to use Kingman scores and for for the prior is because it.",
            "Admits efficient and relatively easily implemented implementable inference algorithms.",
            "OK, and this method inherits the advantages of Bayesian approaches.",
            "Such as it can handle the notion of uncertainty over tree structures.",
            "It has meaningful branch lengths.",
            "It has a well defined objective function and it can deal with missing data.",
            "And I'll come to this point as well, but this is one of the most important points in using Kingman's curse, and it's produces infinitely exchangeable models and therefore it has predictive semantics.",
            "I'll come to that."
        ],
        [
            "Only two.",
            "OK, so a bit more detail about the structure of the model, so as I said, we're going to use three structured likelihood and prior over trees to formulate the whole model.",
            "The likelihood will be Markov process less defined on the tree that it was forward in time.",
            "So given given a tree like this where we have data on the leaves, we generate the data by starting at the at the root, going towards the leaves along the branches of the tree.",
            "And we define a Markov process that runs on this tree that transitions the data points along the branches to different types.",
            "And the prior that we will assume is kingman's coalescent."
        ],
        [
            "OK, So what is Kingman skill isn't?",
            "It's a model of populations, so it was proposed in population genetics to model the populations in order to be able to study.",
            "Certain things about population, such as the genealogy or the mutation rate or the effective population size or so, and so on.",
            "And it's a very prominent model in population genetics.",
            "It's it's one of the most important models in population genetics.",
            "Soaking once cousins assumes that each individual has exactly one parent.",
            "So therefore, is the haploid generation.",
            "Therefore, the geology of a population of individuals is a tree.",
            "OK, so this is 1 assumption that's not very realistic of, for example, human population, but there are extensions to this.",
            "Basic model, but in this lecture we're only going to consider this type of trees OK, and this type of hierarchy is we're not going to consider any graph.",
            "Any other graphical structured entries.",
            "So Kim's kersaint can be obtained as an infinite limit of the Wright Fisher model, which is also a model from population genetics."
        ],
        [
            "And this model starts off by saying.",
            "Again assuming haploid organisms and it also assumes non overlapping generations and that each generation has an individuals.",
            "So it's circle here represents an individual and each column is a generation and ignore this for the moment, so there are N generations at the population and there are several generations and.",
            "So in this model we measure time between generations as 1 / N, where N is the population size OK. And the Wright Fisher model says each individual chooses their parent uniformly randomly from the previous generation.",
            "So these errors show the parents that individuals choose in the previous generation.",
            "So if you go backwards in time and.",
            "Basically, let every individual choose his parents uniformly randomly from the previous generation, and then you focus on a subset of the population.",
            "You see that if you go enough back in time, then.",
            "All individuals in that subset is going to coalesce into one common ancestor, OK.",
            "So."
        ],
        [
            "Just to take it to more slowly, we have a population of size N and we start at time zero and."
        ],
        [
            "At the next generation again, we have a population size of North, and now we're at time minus 1 / N and so."
        ],
        [
            "Incorrect so."
        ],
        [
            "We're going backwards in time."
        ],
        [
            "And it's at iteration.",
            "Each individual chooses apparent uniformly randomly from the previous generation.",
            "Therefore, there will be some overlap between the parents of some individuals, but and therefore some individuals will not be chosen."
        ],
        [
            "And if you carry this on until.",
            "Ancient."
        ],
        [
            "Pest."
        ],
        [
            "And as I said, if you focus on a particular subset of individuals and if we trace their ancestry back to ancient past, then we see that no matter what subset we chose, it's going to call us at one single common ancestor.",
            "OK, so this is what the Wright Fisher model says."
        ],
        [
            "So we arrive at Kingman's coalescent.",
            "When we take N to Infinity.",
            "So when we take the number of individuals in the population to Infinity.",
            "Note that we were measuring time in terms of 1 / N. The time between generations.",
            "So when we take N to Infinity, the time between generations go to 0, so this becomes rather than a discrete time process discrete time process over finitely many individuals, it becomes continuous time process over infinitely many individuals.",
            "OK.",
            "So."
        ],
        [
            "We basically then."
        ],
        [
            "If we look at this tree, then we can represent that tree."
        ],
        [
            "Like this?",
            "Well, now the branches represent infinitely many generations in between a particular child and its ancestor.",
            "OK, so when I represent this kingdoms coalescent tree?",
            "Immediate parent in the graphical model sense.",
            "The immediate parent is actually not an immediate parent.",
            "In general, Jacot says it's an ancestor.",
            "OK, so it's a shared ancestor.",
            "So yeah, so to recap, Kingman School St is a distribution over geological trees of the population.",
            "And it's assumed that each individual has exactly one parent, and the nodes represent common ancestors, and each branch represents multiple generations."
        ],
        [
            "So there are very important properties for of kingdoms coalescent that that is appealing to machine learning people because it makes it fun to work with these models in practice and it makes it also possible to work with these models in practice.",
            "So one important thing is that the constant times are independent from the tree structure by three structure, I mean who pairs up with who, so the way the way.",
            "So these trees are generated from kingless coalescent.",
            "You can first sample who who coalesces with who, who shares a common ancestor with who first, and then sample times.",
            "And you can do this separately.",
            "So and this makes it easier to model these with these processes.",
            "And the distribution over over three structures is simply uniform.",
            "So each pair of individual is equally likely to call us with each other.",
            "And it has two important self similar properties.",
            "One of them is that if you look at Kingman, Skales and sample, and if you integrate out.",
            "Apart of this, so you just get rid of a part of the nodes.",
            "What you have remaining the distribution of over the trees that you have remaining is still a kingless coalescent and similarly.",
            "Again, if you start off with a kimlasca lesson 3 at a particular time, and now you integrate over everything until this point and now you look at trees only starting at this time, you still have the same distribution, so it's still a kingless coalescent.",
            "OK, so these properties lead to infinite exchangeability, which is property of Bayesian nonparametric models.",
            "You're going to hear more about Bayesian Nonparametrics tomorrow.",
            "From Peter Orbans Ann.",
            "I think from Zubin as well but just just to summarize briefly, basically this infinite exchangeability.",
            "You can think of it as meaning it doesn't matter on which order you observe the data points.",
            "There are still the distribution is still going to be the same, plus you can marginalized out data points.",
            "You get the same form of distribution.",
            "You can add new data points to get the same form of distribution.",
            "So.",
            "Therefore, the first thing I said marginalizing out the data points and getting the same form of distribution allows us to use these models on finitely many data but still be nonparametric.",
            "The second property of this, introducing the points and being able to still stay in the same distribution allows us to be able to do predictions without having to change the model."
        ],
        [
            "OK, so this is what the priors from kingdoms coalescent look like, so these are this is time and this time scale for all these plots are the same and the number of leaves in all of these are increasing like this.",
            "OK so.",
            "So there are like infinitely many nodes in between, but we sort of discard the ones that are only have one child."
        ],
        [
            "You mean you mean in this picture?",
            "There are infinitely many generations it in between, but.",
            "Have a single child.",
            "Well, when I was talking about integrating out, I was talking about like integrating out a part of the tree.",
            "So you're right, there is also infinitely many things on this, but we don't have to represent them explicitly because the model doesn't even refer to that.",
            "But another thing that is infinite is actually there are infinitely many nodes as well, right?",
            "There are infinitely many leaves because we arrived at."
        ],
        [
            "In most coalescent, remember as N goes to Infinity, where N is the population size, so.",
            "But when we have finitely many data, think of it as we have infinitely many notes here, but we have only finitely many data.",
            "Which is this subset?",
            "Let's say so when we model this in this finite subset using Kingman's coalescent.",
            "Effectively we're kind of integrating out everything that doesn't belong to that red tree, denoted here."
        ],
        [
            "OK, so so this is what the these are.",
            "Some examples from the prior and one thing to note is that although there is a lot more leaf nodes here than here, the time scale is or the time to the most recent common ancestor or the root of the tree.",
            "Is roughly the same, right is on the same order of magnitude, so this is something very interesting of kingman's colors and and that's because this happens because the time to coalesce scales with exponential distribution or with parameter and choose to where N is the number of individuals current population.",
            "OK, so when we have less individuals then.",
            "Then it is the first cousin, Time happens later.",
            "OK, and when we have more individuals, the first coalescent times happen much, much faster.",
            "So.",
            "That's basically why we have such a structure.",
            "We have this similar scaling in the in the time for the most recent common ancestor.",
            "Another thing that you probably notice by looking at this, although we haven't observed any data and we haven't said anything about the data yet already in the prior, there is some kind of a clustering going on, right?",
            "So you see that some some points are, some leaves are closer to each other when you look at the length of the branches, then other leaves for example.",
            "So this motivates us to use keymas coalescent for doing clustering because it has a naturally occurring costing property."
        ],
        [
            "OK, so how do we sample?",
            "Is there question?",
            "OK, OK. How do we sample from the prior of kingdoms 'cause and so as I mentioned earlier, the every pair of individual is.",
            "Can call us with each other equally likely right?",
            "And Kimmons constant assumes that a repair of a repair of individuals causes at a time that is distributed with exponential rate one with parameter one.",
            "And the pair with the shortest time classes.",
            "So in order to be able to sample what we can do is we can sample times for every possible pairing of the data from exponential distribution with rate one and then choose the one that has that has the shortest coalescent time which is.",
            "This in this pair, for example.",
            "So we call this them into a new ancestor.",
            "The second stage we again do the same thing.",
            "We sample for all remaining points.",
            "We sample at time from again exponential distribution one, because no matter at what time we start again, the properties of the distribution doesn't change, right?",
            "So we sample times for all individuals from exponential rate one and choose the one with the shortest codes and time to call as we call that the winning pair.",
            "So the the one with the shortest.",
            "Closing time wins the race in a way.",
            "And we have the Collinson tree.",
            "So another way."
        ],
        [
            "So Janet, from the prior from the coalescent is using order statistics, so as I said, the each pair of individuals coalesce with exponential rate 1 right?",
            "And there an individuals.",
            "Therefore their end choose to.",
            "Pairs of individuals so.",
            "At iteration I we have this many individuals.",
            "So now if we consider the order statistics of the exponential distribution, this is the.",
            "This is the first.",
            "This is a short in the distribution of the shortest time to call is among this many exponentials with rate one.",
            "So what we can do is we can simply sample the time from this distribution and pick a pair uniformly at random and then again remove cost pair at their parents.",
            "Repeat until everyone is merged.",
            "I'm hungry in this algorithm.",
            "You may put your tree binary right by construction, yeah?",
            "Well, in the other one as well, because you're in this one."
        ],
        [
            "As well, because you're sampling from.",
            "Don't worry, one parent can have multiple.",
            "OK, I think that's a very good point.",
            "King must caulescent assume."
        ],
        [
            "Binary trees, so the picture I showed you was from Wright Fisher model.",
            "I think that you're referring to that one.",
            "Wright Fisher model doesn't assume binary trees, but now because time is time is continuous, then there is zero probability for more than two individuals to call us at the same time to the same parent.",
            "OK, so yeah, it's always binary trees when it comes looking this person again there are extensions of kingdoms coalescent that assume that relaxes assumption and can can have non binary trees.",
            "But in this one we always assume binary trees.",
            "OK, so in this one we pick a coalescent time from this distribution.",
            "Given the number of individuals we call us them.",
            "At the peak time here, let's say and then.",
            "We carry everyone to that time because they haven't coalesced yet, so we now have them all here and we sample from again an exponential distribution, this time with of course different end because we have one less individuals and we go on like this until we construct the full tree.",
            "OK, so this is another way to sample from the King's cousin prior.",
            "And the probability of the of the sample from Kingless cousin has this probability.",
            "So the normalizing constant of the exponential distribution cancels out with the uniform distribution over pairs of individuals."
        ],
        [
            "OK, so.",
            "Now we know how."
        ],
        [
            "So the sample from the prior, let's see how we sample from the for the data.",
            "OK, so remember this was the Wright Fisher model and."
        ],
        [
            "So we can again think of the data generation by first looking at the Wright, Wright, Fisher model, so individuals.",
            "So before individuals they don't have identity's because they were from the prior right.",
            "We were talking about the prior distribution.",
            "Now we're talking about the actual data generating process.",
            "So we can assume that data the individuals may be of different types.",
            "So here color represents a type."
        ],
        [
            "And each child is basically a duplicate of the parent or."
        ],
        [
            "It goes on to under mutation and it.",
            "Mutates into a different type.",
            "For example, this child is from this pink parent, but it is not yellow because it went under mutation, right?",
            "So."
        ],
        [
            "If we go on like this."
        ],
        [
            "As we see that although this subpopulation originated from the same Adam and Eve, there is still some variability among the population because of limitation."
        ],
        [
            "So this is what we do in Kingman.",
            "School isn't as well we have this binary tree.",
            "We dropped mutations along the tree branches.",
            "Dropping mutations mean we decide on where these mutations will happen by simply running opposing process down the tree, and then we go along the branches, changing the type of the node.",
            "At every mutation point, according to the transition kernel OK."
        ],
        [
            "OK, so now we have a tree structured likelihood and we know how to generate."
        ],
        [
            "Data.",
            "As I said, this is just a Markov process that goes down the tree."
        ],
        [
            "So now we can generate different types of data, so the types of data that we generate will depend on the type of mutation process that we assume, for example.",
            "A particular generative process can say there is a person process with uniform rate on the tree and at each spike it generates a completely new individual.",
            "So this is a model from population genetics.",
            "This called Infinite levels model.",
            "So every time there is limitation it is something completely different from the rest of the population because it samples from my continuous distribution.",
            "So one import 1 interesting.",
            "Connection is that the marginal distribution induced on the leaves has a Chinese restaurant process partitioning structure.",
            "If you don't know about this yet, you're going to learn about it later.",
            "I think tomorrow Peter Robbins is going to talk about it, so you can.",
            "Just for now, you can just keep in mind that kimmons calls and has some relation to the additional process or the Chinese restaurant process as well."
        ],
        [
            "Another mutation model you can think of is multinomial mutation.",
            "Basically we have a vector representing the data and at each mutation event one of the dimensions of this data changes to something else and what it changes to is encoded by the Markov transition kernel, so it changes to one of the possible states with some probability.",
            "It is encoded by the Markov transition kernel.",
            "And at the leaves again we have the data."
        ],
        [
            "This is another example."
        ],
        [
            "It doesn't all need to be discrete, so the data can be continuous as well.",
            "The data generated by this process, for example, we can assume my Brownian motion.",
            "So let's say that the data starts at at this time 0.",
            "So these are the values of the data points.",
            "So the nodes go under Brownian motion and at every split point the branches will go.",
            "Through Independent Brownian motion.",
            "OK, so the locations where these points end up are basically the.",
            "Data identity's and of course you can think of multiple dimensional Brownian motion as well."
        ],
        [
            "OK, so.",
            "Now."
        ],
        [
            "How about inference?",
            "So as I said we have we have data at the leaves which is generated by picking.",
            "Particulare parent, Particulare most recent common ancestor and dropping mutations along the branches of the tree and going along the branches and mutating this root note according to the macro transition kernel to get the data here.",
            "Here these vectors represent.",
            "Basically this is a 1 dimensional data with five possible states, like represents the state that.",
            "Data takes at that point in time.",
            "OK, so we start with this.",
            "This mutation caused it to transition to this state and then this mutation caused it.",
            "Just transition it back to this state and so on until we get this state vector OK.",
            "So this is still going forward.",
            "This is still data generation."
        ],
        [
            "And in inference, what we want is we observe these data at the leaves and we want to go back and reconstruct the tree.",
            "OK, so when we do inference we don't observe this tree structure or the times of the mutations or anything like that, right?",
            "We don't we don't observe anything but the data.",
            "So given the data we want to go back and reconstruct the tree.",
            "And we may.",
            "We may do this by basically trying to reconstruct every detail about the tree, including the tree structure, the time of when things happen, and call us the mutation events and what these mutation events cause the data points to turn into an so on right?",
            "But there will be lots of possibilities for that, especially because time is continues, there will be infinitely many.",
            "Possible trees, so there's not very feasible to represent every possible 301 class of models that try to do inference on these kinds of problems is basically they don't represent these times, but they represent the coalescent and mutation events and what everything mutates into and so on, but."
        ],
        [
            "I'm going to talk about a different type of approach where we integrate out mutations and we represent things in terms of messages on the tree.",
            "So you can think of this as we represent the state vectors as distribution over the nodes rather than these."
        ],
        [
            "Can't white vectors.",
            "OK, so we represent."
        ],
        [
            "We don't know what went on in between these data points, for example, but we know what the data looked like so we know at the time that passed between the the observation of the data points and the coalescent events, right?",
            "So we can have an idea about what are the possible States and with what probability for example."
        ],
        [
            "OK, so.",
            "This is the notation we're going to use.",
            "We have X data points at the leaves and we have these latent internal nodes which we do not buy.",
            "Huawei and the subscript denotes who coalesced to construct that Y, and we have a state that is sampled from the equilibrium distribution.",
            "Basically estate this in the distant past.",
            "With Pi we represent the tree and Pi T is basically the partially constructed 3 until time T, so we go from time 0 towards the past times and Pi T tells us the partitioning structure that we get of these data points until a particular time.",
            "For example, at this time we would have X1 and Y23 and Y-45 and so on.",
            "And coalescent events happen at times, T0T1T2 and so on, and the time lapse between consecutive colors and events are shown by Delta."
        ],
        [
            "So here this denotes the transition kernel.",
            "So in order to be able to compute the probability of."
        ],
        [
            "This whole thing, the the data, the latent variables, and this set again, which is a latent variable which denotes the equilibrium state."
        ],
        [
            "We can simply use calculus, right?",
            "We can, we can say, OK, the Markov transition kernels tell us how things turn into other things.",
            "So this basically tells us what's the probability of going to a particular state if you're coming from a particular state.",
            "OK, and there are two children per parent, so that's why we have two things here.",
            "And this comes from the transition.",
            "The equilibrium distribution and this is basically the probability of transition from the equilibrium distribution to the most recent common ancestor.",
            "You don't need to get all the details about these equations is just I want to give.",
            "A high level view of how we do what we do, but don't worry about the equation details."
        ],
        [
            "So.",
            "Basically what we do is we use."
        ],
        [
            "We use this equation, but we know that we haven't observed Y or that we only observed X, so we need to somehow get rid of YN set.",
            "So what we do is we integrate."
        ],
        [
            "Come out by using message passing.",
            "Again, if you're not familiar with message passing, don't worry about it.",
            "The only take home message from this whole slide is the fact that the probability of the data at the leaves that we observed given the particular tree that we have, is given as a product of these functions.",
            "OK, so these functions that are encoded with set and.",
            "If you want to know details, we can talk up after the lecture, but basically this is the form of the of the likelihood function.",
            "Which we obtained by simply integrating out the hidden variables."
        ],
        [
            "OK, so we have all the ingredients we need right?",
            "We have the likelihood which is of this form and we have the prior which is of this form.",
            "So to get the posterior we combine the likelihood and prior so the posterior is proportional to this here.",
            "So this comes from the prior and this comes from the likelihood and.",
            "There is a, so both the prior and likelihood has this product over N -- 1 things because when we have an individuals at the start then we have N -- 1 coalescent events until we get to the most recent common ancestor.",
            "OK, so that's why we have N -- 1 terms in the product.",
            "So and in the posterior, of course, when we combine these, we will have an minus one products right?",
            "And one interesting observation is that this comes from the likelihood prior this comes from the likelihood, so combining these will have the posterior.",
            "So although it's not exactly true, we can view the ice term.",
            "Here is like a local posterior OK. And we use this observation to construct our proposal distribution to do the inference basically.",
            "So what we're going to do do is we'll start at the leaf nodes.",
            "Ann will propose a agglomerative algorithm that will iteratively construct the tree by proposing coalescent times an pairs to call us OK, and for that we need proposal distribution, and we're going to use this observation that this is coming from the prior.",
            "This is coming from the likelihood.",
            "Therefore this is somewhat like at iteration.",
            "This ice product is somewhat like the local posterior.",
            "OK."
        ],
        [
            "So just a very brief recap or not recap very brief crash course for sequential Monte Carlo.",
            "So who is familiar with?",
            "Monte Carlo techniques or Sequential Monte Carlo.",
            "OK.",
            "So yeah, so you do have some background on it then so important sampling is basically a technique where we want to sample from a particular distribution, let's say P. But it's hard to sample from it.",
            "We don't know how to sample from it or it takes too much time.",
            "Too much computation time to sample from it.",
            "So what we do is we construct proposal distribution Q, let's say, and we sample from Q rather than sampling from P. And we we have some weights that tell us how important each of these samples are so that we weight the samples with these importance weight and some of these samples up to get the.",
            "Get this distribution that we are interested in."
        ],
        [
            "Sequential importance sampling is basically a sequential way of doing important sampling.",
            "The proposal distribution is constructed sequentially, hence the name sequential.",
            "And the important ways are also constructed sequentially.",
            "OK, so because this is sequential, it gathers more information over iterations about the data or about the model.",
            "Therefore, some early samples may not be as representative as in earlier iterations.",
            "So if you started off with some samples that had high weight.",
            "They are representative of the distribution that you want to represent later on at the during sampling.",
            "These weights may shrink simply because you learn more things about the data and the data didn't tell you what your samples expected it to tell in the very beginning, so some of the weights may shrink.",
            "So and this is called sample degeneracy.",
            "So the way to combat this is basically."
        ],
        [
            "Having sequential Monte Carlo or sequential importance sampling resampling So what you do is you have the sequential important sampling you put on top of that I re sampling step which basically means you look at the distribution of weights you sample.",
            "You sample the samples or you sample the generated points.",
            "Let's say according to their rates and you prune out the ones that didn't get picked and you keep the ones that did get picked so that you have a better representation of your current.",
            "Data collection OK.",
            "So that was the case."
        ],
        [
            "Course to sequential Monte Carlo.",
            "OK, so here what we're going to do is.",
            "We're going to talk about two algorithms, one sequential Monte Carlo, and one greedy algorithm in detail.",
            "And I'll also go over some other sequential Monte Carlo algorithms that are proposed for Kingman's coalescent.",
            "One common aspect of all these algorithms that I'm going to talk about is that we start off with leaves.",
            "As our data points, right so we start off with these data points and we iteratively construct this tree or construct sample trees multiple multiple of them.",
            "If we are using sequential Monte Carlo that is representative of the posterior trees that generated the data.",
            "OK, so at each iteration we need to decide which pair to call us and when to call us.",
            "That is going to be.",
            "The question."
        ],
        [
            "So.",
            "Uh.",
            "As I said, we're going to use these.",
            "The term that comes from the prior and the term that comes from the likelihood we're going to take this and use that as the proposal distribution for for our sampling algorithm.",
            "OK, so this this algorithm that was done by a sad can and tell them, and Roy assumes that this is the proposal distribution and at each iteration it samples a call sometime from the prior.",
            "And then let's say these were our data points.",
            "Let's say we decided that they're going to call us here, because that's the time that our prior told us to call us.",
            "Now we evaluate this so because because now this is fixed, right?",
            "Because now we have Delta already.",
            "The time to call us so we evaluate this at that time and that gives us something that is like the probability of that particular pair to coalesce.",
            "So we do that for valuate this for all possible pairs we have a distribution like this for all possible pairs in the population, so this takes tell you the probability the proposal probability of coalescing for a particular pair, another pair, another pair as well.",
            "So from this distribution we choose which pair to coalesce and then call us that pair.",
            "Let's say we chose this pair to call us.",
            "So we call this them.",
            "At this point that we already chose from the prior and now we have one less individual to worry about in the population, and we do the same thing iteratively until we get to the root of the tree, OK. Of course we need to compute weights for the sequential Monte Carlo and various sample particles.",
            "If rates diverge, and if more than one point left we repeat this whole thing and so on.",
            "But these are all common for all sequential Monte Carlo algorithm, so the important thing is how we propose things.",
            "OK, so another."
        ],
        [
            "Rhythm that was developed by these guys again is again using the same type of local posterior as the proposal distribution.",
            "But this time, rather than sampling times from the prior, what it does is it first chooses which pair to coalesce by.",
            "Again looking at some distribution of probabilities for the pairs and the probabilities for each pair is given by this integral.",
            "Basically, this is the proposal distribution for this pair, right?",
            "So?",
            "And it's a distribution over time and over pairs, so if we integrate.",
            "A time from this we have this right, so we have.",
            "We have an expression that tells us again a proposal probability for a particular pair to coalesce.",
            "OK, so we construct this one and we choose which pair to coalesce by sampling from this distribution.",
            "And then we sample the coalescent time for that pair from this distribution.",
            "It's basically going back to this what we took the integral off we.",
            "This time we have we know which pair to do this.",
            "For so we sample a time for that pair and then again do the same thing that we did before March.",
            "The perrington endpoint remove merging pairs from the representation.",
            "Compute weighs 3 sample gone OK, so again, the difference between this is this is sampling times from the prior and then sampling the pairs to coalesce given the time right?",
            "And this is sampling appare.",
            "By evaluating this integral for every possible pair.",
            "And then sampling time.",
            "OK, so these are both.",
            "Correct sampling algorithms.",
            "Just a different take on the proposal distributions.",
            "So one thing they have in common though, is that they are sampling a coalescent time for so sorry this is evaluating this integral for every possible pair of individuals at the current iteration, and because this expression changes at every iteration, it has to recompute everything.",
            "OK, so there is a order of N squared pairs at each iteration, and there are N -- 1.",
            "Patience so there is an order of N cubed computations for this.",
            "Algorithm."
        ],
        [
            "And this algorithm also has a cubic scaling, because although it's, it's not doing that integral, which which is very costly, so this is more cost effective.",
            "Still it is evaluating this.",
            "Value for every pair of individuals that exist at the current time.",
            "Right, so again, there are an escort individuals and their end on the order of N iterations, so the computations here is again CU.",
            "So."
        ],
        [
            "So.",
            "One way to scale up.",
            "Info."
        ],
        [
            "This is basically trying to avoid this cubic scaling and focusing only on a subset of pairs, and the idea is to focus on the similar points by using the similarity information between pairs of points."
        ],
        [
            "So.",
            "We have we have these individuals right, and we're going to again use the second algorithm.",
            "The proposal distribution, that is like the second algorithm.",
            "So this is the local posterior."
        ],
        [
            "And will integrate out the time so that we have the probability of coalescing four pairs of individuals.",
            "But the difference is we're not going to do that for every possible pair of individuals.",
            "We're going to do it only for a subset.",
            "So."
        ],
        [
            "This would have been the distribution if we were to do it for every possible pair of individuals.",
            "Rather than that."
        ],
        [
            "We're going to ignore this part.",
            "We're only going to do this computation for this part only, and we're going to decide who to do computations for by looking at nearest neighbors of individuals and limiting the pairs to a subset of pairs."
        ],
        [
            "And.",
            "If we want to use this in a sequential Monte Carlo scheme, then we need to make sure that these guys have posted probability of coalescing, so we're going to set their probability to coalesce to a constant that's not zero OK."
        ],
        [
            "Let's say the length of this data pipeline is going to be the probability for the other guys to coalesce, and the probability of these are going to be given by this integral here."
        ],
        [
            "So what we do?",
            "OK, and if we if we include more pairs than we are."
        ],
        [
            "Going to.",
            "Have a more efficient or better representation of our problem of our distribution that we actually want to sample from."
        ],
        [
            "OK, so."
        ],
        [
            "As I said, this is.",
            "This is how we obtained the speedup by ignoring some of the pairs, not really ignoring, but by setting them to a constant their probability to a constant and doing this cost of computation for only a subset."
        ],
        [
            "And in order to be able to do things really quickly, what we do is we maintain a priority cube using fast nearest neighbors so.",
            "I think I'll see."
        ],
        [
            "It doesn't go here, so let's say we have a bunch of individuals.",
            "What we do is we search for the K nearest neighbor of each individual.",
            "So K is perimetre of the algorithm.",
            "So let's say two nearest neighbors.",
            "We search for the two nearest neighbor of each of these individuals like this, for example like that and so on.",
            "And then we put them.",
            "On a table like this, right?",
            "So we have the nearest neighbors here, so not every pair is represented in this nearest neighbor table because they don't appear in the nearest neighbor graph.",
            "So there's this rest of the pairs.",
            "And then.",
            "We have we would have had this distribution if we've already evaluates the probability of a repair coalescing.",
            "But as I said, we're going to only focus on a subset of pairs, so not even on all the neighboring individuals.",
            "But we're going to limit it to a finite number, which I do not here by R. So we will evaluate that integral for these are pairs, and for rest of the pairs, regardless whether they appear in the.",
            "Nearest neighbor table or not, we're going to set their value, their cost and probability to constant vector constant constant value.",
            "And then.",
            "Will sample from this distribution.",
            "Let's say we sampled these guys, then we're going to coalesce them into.",
            "Knew ancestor, we're going to get rid of the guys that coalesced from representation.",
            "And also from the tree from the from the priority queue.",
            "And then include the new guy in the representation.",
            "Hope this is a little messed up.",
            "Sorry for that, so we're going to include the new guy in the representation.",
            "We're going to look for its two nearest neighbors in this case, or K nearest neighbors in general and we'll decide where to put them in the prior to Q.",
            "Let's say here and there.",
            "Will update the priority queue and will go on by computing the probability of causing for only only these guys and approximating these as constant and choosing who to call us this time.",
            "For example, we chose these guys to call us into a new individual and so on.",
            "So this is how we maintain the priority queue."
        ],
        [
            "OK, so just.",
            "With the new substitute part with the constant probability.",
            "No, we normalize the remaining parts well, these are all these probabilities that you get from that integral are unnormalized probabilities anyway.",
            "But yeah, you're right.",
            "I mean you do.",
            "You do need to renormalize every."
        ],
        [
            "So, but again, as I said, when you do that integral that it doesn't give you a value between zero and one.",
            "Or a value that like these want sum up to one.",
            "So you need to renormalize anyway.",
            "In other questions."
        ],
        [
            "OK, just two.",
            "Go over the algorithm as a whole.",
            "So what we do is we maintain a priority queue of pairs according to their distance or their similarity, and at each station what we do is we first for the first are pairs.",
            "We compute the unnormalized probability this queue of the pair.",
            "We set the probability of the rest of the pairs to some constant.",
            "Sample appeared to coalesce from that distribution that sticks that I was showing you and then sample the colors on time for that pair.",
            "We remove merging pairs from the representation and include new ancestor in the representation.",
            "Update the priority queue computer SMC rates, resample particles.",
            "If they diverge, and if more than one point left, reiterate OK.",
            "So this is the SMC algorithm."
        ],
        [
            "The greedy algorithm is basically a really simple variance of this, so we do things deterministically and greedily in the greedy algorithm.",
            "We picked the pair with maximum probability to coalesce rather than sampling OK, so rather than."
        ],
        [
            "Using.",
            "This distribution to sample from we look at the one that has largest probability in this one.",
            "In this case is the second one.",
            "So we set that to be the pair to call us at that iteration."
        ],
        [
            "So this is a deterministic move given the probabilities and we set the coalescent time for that pair to the mean of the distribution rather than sampling from that distribution.",
            "OK, so because this is going to be picking deterministically the one with the maximum probability, we don't need to worry about setting the probability of the rest of the things to a constant and so on, because that constant is going to be smaller than the maximum probability anyway.",
            "Um?",
            "OK, and the rest is basically OK. We get rid of these parts that have to do with SMC as well, so we get the greedy algorithm.",
            "So it's really easy to go to the greedy algorithm.",
            "Once you have the SMC algorithm or the other way around."
        ],
        [
            "OK, so.",
            "This is what the computational costs breaks down to, so we have we have a cost of evaluating the integral I was talking about integrating over the coalescent times for the pairs.",
            "That is something very costly, so there is the C, which is a very large constant and then there is this R which is the number of things that we want to we decide to do computations for.",
            "The top are pairs and N is the number of individuals.",
            "In the Origonal population that we started off with and we still have that end here because there are N -- 1 iterations until we get to the most recent common ancestor.",
            "So this is, this part is coming from evaluating that integral for our pairs.",
            "There is this part which comes from the nearest neighbor search so.",
            "I wrote this for the KD tree for example, so there is the Alpha K factor, so for searching for Kay neighbors and it takes log N time to search for one neighbor in KD trees or in cover trees as well.",
            "And again there are any iterations and then there is this are of North so it's the cost of re sampling and it's also a function of.",
            "And it depends on the particular resampling algorithm that you choose and how often you you choose to do resampling.",
            "And so on South.",
            "And you may want to re balance your KD trees and so on.",
            "South all that factors into this are often so.",
            "The dominating factor here, as you can see, is the analogman factor, so this algorithm overall scales with analog.",
            "The algorithm that I was talking about before this one scales with a similar.",
            "See the cost here.",
            "Technically I shouldn't have written constants in these or equations, but I just wrote them to emphasize that they are really big constants, so there is a C factor coming from evaluating those integrals.",
            "And then N cube from having to do things over all iterations for every possible pairing.",
            "And then there's another algorithm that we developed that I didn't talk about here, which has better scaling.",
            "It is quadratic as opposed to cubic with a similar constant.",
            "Its management manages to do things quadratically in this case because it saves some iterations by by keeping the coalescent time constant over all iterations.",
            "So it does things for all possible pairs, but at every iteration it only needs to do things for the pairs that it hasn't done things before that it hasn't done computations for before.",
            "Again, if you're interested, we can talk about that later, but just so just to emphasize that this is doing things by.",
            "Keeping some computations that it has done before, so making being able to make use of those computations, whereas this is doing things more efficiently by avoiding doing some computations."
        ],
        [
            "And this is like the comparison of overall costs."
        ],
        [
            "OK, so.",
            "I'm going to go onto some applications.",
            "Any guarantee about the pallative samples to generate?",
            "Runtime guarantees producer in the end.",
            "Q arrows into generate.",
            "Better quality, yeah, that's a very good question, so I am going to show later comparison between the quality of samples.",
            "The short answer is no, we don't have guarantees for for any SMC algorithm.",
            "Basically, the more the rule of thumb is, the more particles you have, the better your approximation is going to get, and in theory only if you have infinitely many particles, you're going to get.",
            "The correct distribution, but experiments showed that at least empirically.",
            "The quality of samples is pretty good compared to the other algorithms.",
            "So all the algorithms that I talked about here are generally like my personal experience is that they are generally better than the other algorithms that exist in population genetics that I haven't talked about, which actually represent the mutation events and the coalescent events and integrates out the times as opposed to us integrating out limitations and representing the times.",
            "So these algorithms typically.",
            "Produce better samples.",
            "Plus, if you if you are interested in how they compare how these algorithms compare among each other, so the first algorithm I talked about, the one that samples times from the prior.",
            "As you can imagine, it is.",
            "It produces the least efficient samples simply because it's sampling times from the prior.",
            "Um?",
            "The last algorithm I talked about, if you look at only one sample then it is pretty inefficient as well, so it gives you quite bad samples if you only look one.",
            "Look at 1 sample, but the fact that you can handle a lot more samples because of computation time using this last algorithm I talked about.",
            "Basically answer.",
            "Makes it makes it's the best performing algorithm simply because now you can get generate many more samples.",
            "You can get rid of the bad samples by resampling and so you end up getting a better representation of your posterior but not serviceable guarantees.",
            "OK, a short break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I apologize from the beginning if this doesn't flow as well as the other talks.",
                    "label": 0
                },
                {
                    "sent": "I had a little short of time to prepare and I will need to sometimes switch between slides and some numbering of the slides are not correct, so hopefully everything else is.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will work fine.",
                    "label": 0
                },
                {
                    "sent": "So as you said, I'll be talking about kingdoms coalescent for hierarchical representations.",
                    "label": 1
                },
                {
                    "sent": "So first I'll tell you what I mean by hierarchical representations.",
                    "label": 0
                },
                {
                    "sent": "Then I'll tell you what I mean by Kingman's coalescent and then talk about how to how to construct models using King Mills Coalescent and how to sample from the prior, how to generate data from this model.",
                    "label": 0
                },
                {
                    "sent": "And then how to do inference and then some applications?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hierarchical representations are everywhere.",
                    "label": 0
                },
                {
                    "sent": "You're probably familiar with the evolutionary tree.",
                    "label": 0
                },
                {
                    "sent": "This is 1 very prominent model of of hierarchy is basically or one prominent way of using hierarchies to represent the life basically.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or you probably also know the language tree, so the languages are related to each other.",
                    "label": 0
                },
                {
                    "sent": "There are language families and.",
                    "label": 0
                },
                {
                    "sent": "Bigger family and families and so on.",
                    "label": 0
                },
                {
                    "sent": "South languages.",
                    "label": 0
                },
                {
                    "sent": "Some languages are more related to each other than other languages, and so on.",
                    "label": 0
                },
                {
                    "sent": "So people came up with this kind of a tree to represent the relationship between languages.",
                    "label": 0
                },
                {
                    "sent": "You may also.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So be interested in simply visual taxonomies rather than the genetic relationship between things.",
                    "label": 0
                },
                {
                    "sent": "You may just want to organize things in terms of their visual appearance is so that.",
                    "label": 0
                },
                {
                    "sent": "When you see a particular color of bird, for example, you go to this taxonomy and you scroll through this taxonomy to find the class of bird that you're looking for, rather than having to sort through all the birds and having to look at them one by one so.",
                    "label": 0
                },
                {
                    "sent": "Another use of hierarchical representations, therefore, is to represent the data in a good format.",
                    "label": 0
                },
                {
                    "sent": "To be able to search quickly through it.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why do we want to do hierarchical clustering or hierarchical representations?",
                    "label": 0
                },
                {
                    "sent": "As I said, you may want to discover underlying structure in the data when we believe that the data has an underlying tree structure, like in modeling geneologies or in modeling language families.",
                    "label": 1
                },
                {
                    "sent": "Or we may simply want to visualize data to understand the relationship among data items.",
                    "label": 0
                },
                {
                    "sent": "Or we may simply want to summarize data.",
                    "label": 1
                },
                {
                    "sent": "And use hierarchical representations as a way to partition data into different, possibly unrelated groups.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And these these?",
                    "label": 0
                },
                {
                    "sent": "Goals maybe that doesn't need to be mutually exclusive, so you may want to discover the underlying structure in the data to be able to visualize it better, for example, and so on.",
                    "label": 0
                },
                {
                    "sent": "So hierarchical representations gives it gives us shorter search time.",
                    "label": 0
                },
                {
                    "sent": "It gives us more intuitive relationship between groups of data points as opposed to flat clustering where the only put you would have is the relationship between the items.",
                    "label": 0
                },
                {
                    "sent": "Lying in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "In hierarchical clustering you would have different.",
                    "label": 0
                },
                {
                    "sent": "Different amount of variability between different groups of clusters because they are lying under a tree for example, and there is more structure in the representation.",
                    "label": 0
                },
                {
                    "sent": "So you may be able to capture more information about the data.",
                    "label": 1
                },
                {
                    "sent": "So even if the data does not naturally come in a hierarchical form, you may still get a better representation, but a better model of the data by using hierarchical models.",
                    "label": 0
                },
                {
                    "sent": "And it's easier and nicer to visualize.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in this talk I'm using hierarchical clustering and hierarchical representations interchangeably.",
                    "label": 1
                },
                {
                    "sent": "Basically I'm using the same type of model Kingman scores, and for doing both heichel representations is something more general than hierarchical clustering, but for ease of presentation I'll be focusing on hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "When I describe the things, so there are different ways that were developed to do hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "I think the most prominent one is the traditional agglomerative clustering, where you compute you define a distance between data points and the distance between clusters of data points, and you iteratively start from the bottom, iteratively construct the tree by comparing the distances and combining the clusters of data points until you get to the root.",
                    "label": 0
                },
                {
                    "sent": "Another method is probabilistic.",
                    "label": 0
                },
                {
                    "sent": "North of clustering, where you define a probability distribution over data points or groups of data points and you have a different criterion this time rather than distance, you use this.",
                    "label": 0
                },
                {
                    "sent": "You use something like this, so the probability of the joint.",
                    "label": 0
                },
                {
                    "sent": "Of Q&R as opposed the ratio of that with probability of Q alone and R alone so.",
                    "label": 0
                },
                {
                    "sent": "Which whichever pair of cluster has a higher score than you merge this, and again you iteratively go until the top.",
                    "label": 0
                },
                {
                    "sent": "So these methods assume some probability distribution, but they don't.",
                    "label": 0
                },
                {
                    "sent": "They don't model the data, the generative process that generated the data as I have probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So there are methods that have done that as well.",
                    "label": 0
                },
                {
                    "sent": "So these authors.",
                    "label": 0
                },
                {
                    "sent": "Have defined a probability distribution over trees and then define that restructured likelihood and try to do inference to get the posterior of the trees given the data points.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to be focusing on these types of models.",
                    "label": 0
                },
                {
                    "sent": "Actually this work by tall is the work which uses the kingdoms coalescent as a prior over trees to construct this model.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as I said in in Bayesian hierarchical clustering, we use a tree structured graphical model as the data generating process.",
                    "label": 1
                },
                {
                    "sent": "And we define a prior to compute the posterior over trees and as a prior over trees we use kingman's coalescent.",
                    "label": 0
                },
                {
                    "sent": "So why do we want to do Bayesian hierarchical clustering?",
                    "label": 0
                },
                {
                    "sent": "And why do we want to use Kingman's curse?",
                    "label": 0
                },
                {
                    "sent": "And for that is because first we want a hierarchy because the hierarchical representation is more powerful than flat representations.",
                    "label": 1
                },
                {
                    "sent": "The reason why we want to use Kingman scores and for for the prior is because it.",
                    "label": 0
                },
                {
                    "sent": "Admits efficient and relatively easily implemented implementable inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "OK, and this method inherits the advantages of Bayesian approaches.",
                    "label": 0
                },
                {
                    "sent": "Such as it can handle the notion of uncertainty over tree structures.",
                    "label": 0
                },
                {
                    "sent": "It has meaningful branch lengths.",
                    "label": 0
                },
                {
                    "sent": "It has a well defined objective function and it can deal with missing data.",
                    "label": 0
                },
                {
                    "sent": "And I'll come to this point as well, but this is one of the most important points in using Kingman's curse, and it's produces infinitely exchangeable models and therefore it has predictive semantics.",
                    "label": 0
                },
                {
                    "sent": "I'll come to that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only two.",
                    "label": 0
                },
                {
                    "sent": "OK, so a bit more detail about the structure of the model, so as I said, we're going to use three structured likelihood and prior over trees to formulate the whole model.",
                    "label": 0
                },
                {
                    "sent": "The likelihood will be Markov process less defined on the tree that it was forward in time.",
                    "label": 1
                },
                {
                    "sent": "So given given a tree like this where we have data on the leaves, we generate the data by starting at the at the root, going towards the leaves along the branches of the tree.",
                    "label": 0
                },
                {
                    "sent": "And we define a Markov process that runs on this tree that transitions the data points along the branches to different types.",
                    "label": 0
                },
                {
                    "sent": "And the prior that we will assume is kingman's coalescent.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is Kingman skill isn't?",
                    "label": 0
                },
                {
                    "sent": "It's a model of populations, so it was proposed in population genetics to model the populations in order to be able to study.",
                    "label": 0
                },
                {
                    "sent": "Certain things about population, such as the genealogy or the mutation rate or the effective population size or so, and so on.",
                    "label": 0
                },
                {
                    "sent": "And it's a very prominent model in population genetics.",
                    "label": 0
                },
                {
                    "sent": "It's it's one of the most important models in population genetics.",
                    "label": 0
                },
                {
                    "sent": "Soaking once cousins assumes that each individual has exactly one parent.",
                    "label": 1
                },
                {
                    "sent": "So therefore, is the haploid generation.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the geology of a population of individuals is a tree.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is 1 assumption that's not very realistic of, for example, human population, but there are extensions to this.",
                    "label": 0
                },
                {
                    "sent": "Basic model, but in this lecture we're only going to consider this type of trees OK, and this type of hierarchy is we're not going to consider any graph.",
                    "label": 0
                },
                {
                    "sent": "Any other graphical structured entries.",
                    "label": 0
                },
                {
                    "sent": "So Kim's kersaint can be obtained as an infinite limit of the Wright Fisher model, which is also a model from population genetics.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this model starts off by saying.",
                    "label": 0
                },
                {
                    "sent": "Again assuming haploid organisms and it also assumes non overlapping generations and that each generation has an individuals.",
                    "label": 1
                },
                {
                    "sent": "So it's circle here represents an individual and each column is a generation and ignore this for the moment, so there are N generations at the population and there are several generations and.",
                    "label": 0
                },
                {
                    "sent": "So in this model we measure time between generations as 1 / N, where N is the population size OK. And the Wright Fisher model says each individual chooses their parent uniformly randomly from the previous generation.",
                    "label": 1
                },
                {
                    "sent": "So these errors show the parents that individuals choose in the previous generation.",
                    "label": 0
                },
                {
                    "sent": "So if you go backwards in time and.",
                    "label": 0
                },
                {
                    "sent": "Basically, let every individual choose his parents uniformly randomly from the previous generation, and then you focus on a subset of the population.",
                    "label": 0
                },
                {
                    "sent": "You see that if you go enough back in time, then.",
                    "label": 0
                },
                {
                    "sent": "All individuals in that subset is going to coalesce into one common ancestor, OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to take it to more slowly, we have a population of size N and we start at time zero and.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the next generation again, we have a population size of North, and now we're at time minus 1 / N and so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Incorrect so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going backwards in time.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's at iteration.",
                    "label": 0
                },
                {
                    "sent": "Each individual chooses apparent uniformly randomly from the previous generation.",
                    "label": 1
                },
                {
                    "sent": "Therefore, there will be some overlap between the parents of some individuals, but and therefore some individuals will not be chosen.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you carry this on until.",
                    "label": 0
                },
                {
                    "sent": "Ancient.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pest.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as I said, if you focus on a particular subset of individuals and if we trace their ancestry back to ancient past, then we see that no matter what subset we chose, it's going to call us at one single common ancestor.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what the Wright Fisher model says.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we arrive at Kingman's coalescent.",
                    "label": 1
                },
                {
                    "sent": "When we take N to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So when we take the number of individuals in the population to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Note that we were measuring time in terms of 1 / N. The time between generations.",
                    "label": 0
                },
                {
                    "sent": "So when we take N to Infinity, the time between generations go to 0, so this becomes rather than a discrete time process discrete time process over finitely many individuals, it becomes continuous time process over infinitely many individuals.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We basically then.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at this tree, then we can represent that tree.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "Well, now the branches represent infinitely many generations in between a particular child and its ancestor.",
                    "label": 0
                },
                {
                    "sent": "OK, so when I represent this kingdoms coalescent tree?",
                    "label": 0
                },
                {
                    "sent": "Immediate parent in the graphical model sense.",
                    "label": 0
                },
                {
                    "sent": "The immediate parent is actually not an immediate parent.",
                    "label": 0
                },
                {
                    "sent": "In general, Jacot says it's an ancestor.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a shared ancestor.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so to recap, Kingman School St is a distribution over geological trees of the population.",
                    "label": 1
                },
                {
                    "sent": "And it's assumed that each individual has exactly one parent, and the nodes represent common ancestors, and each branch represents multiple generations.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are very important properties for of kingdoms coalescent that that is appealing to machine learning people because it makes it fun to work with these models in practice and it makes it also possible to work with these models in practice.",
                    "label": 0
                },
                {
                    "sent": "So one important thing is that the constant times are independent from the tree structure by three structure, I mean who pairs up with who, so the way the way.",
                    "label": 1
                },
                {
                    "sent": "So these trees are generated from kingless coalescent.",
                    "label": 0
                },
                {
                    "sent": "You can first sample who who coalesces with who, who shares a common ancestor with who first, and then sample times.",
                    "label": 0
                },
                {
                    "sent": "And you can do this separately.",
                    "label": 0
                },
                {
                    "sent": "So and this makes it easier to model these with these processes.",
                    "label": 0
                },
                {
                    "sent": "And the distribution over over three structures is simply uniform.",
                    "label": 1
                },
                {
                    "sent": "So each pair of individual is equally likely to call us with each other.",
                    "label": 0
                },
                {
                    "sent": "And it has two important self similar properties.",
                    "label": 0
                },
                {
                    "sent": "One of them is that if you look at Kingman, Skales and sample, and if you integrate out.",
                    "label": 0
                },
                {
                    "sent": "Apart of this, so you just get rid of a part of the nodes.",
                    "label": 0
                },
                {
                    "sent": "What you have remaining the distribution of over the trees that you have remaining is still a kingless coalescent and similarly.",
                    "label": 0
                },
                {
                    "sent": "Again, if you start off with a kimlasca lesson 3 at a particular time, and now you integrate over everything until this point and now you look at trees only starting at this time, you still have the same distribution, so it's still a kingless coalescent.",
                    "label": 0
                },
                {
                    "sent": "OK, so these properties lead to infinite exchangeability, which is property of Bayesian nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "You're going to hear more about Bayesian Nonparametrics tomorrow.",
                    "label": 0
                },
                {
                    "sent": "From Peter Orbans Ann.",
                    "label": 0
                },
                {
                    "sent": "I think from Zubin as well but just just to summarize briefly, basically this infinite exchangeability.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as meaning it doesn't matter on which order you observe the data points.",
                    "label": 0
                },
                {
                    "sent": "There are still the distribution is still going to be the same, plus you can marginalized out data points.",
                    "label": 0
                },
                {
                    "sent": "You get the same form of distribution.",
                    "label": 0
                },
                {
                    "sent": "You can add new data points to get the same form of distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the first thing I said marginalizing out the data points and getting the same form of distribution allows us to use these models on finitely many data but still be nonparametric.",
                    "label": 0
                },
                {
                    "sent": "The second property of this, introducing the points and being able to still stay in the same distribution allows us to be able to do predictions without having to change the model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is what the priors from kingdoms coalescent look like, so these are this is time and this time scale for all these plots are the same and the number of leaves in all of these are increasing like this.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "So there are like infinitely many nodes in between, but we sort of discard the ones that are only have one child.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You mean you mean in this picture?",
                    "label": 0
                },
                {
                    "sent": "There are infinitely many generations it in between, but.",
                    "label": 0
                },
                {
                    "sent": "Have a single child.",
                    "label": 0
                },
                {
                    "sent": "Well, when I was talking about integrating out, I was talking about like integrating out a part of the tree.",
                    "label": 0
                },
                {
                    "sent": "So you're right, there is also infinitely many things on this, but we don't have to represent them explicitly because the model doesn't even refer to that.",
                    "label": 0
                },
                {
                    "sent": "But another thing that is infinite is actually there are infinitely many nodes as well, right?",
                    "label": 0
                },
                {
                    "sent": "There are infinitely many leaves because we arrived at.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In most coalescent, remember as N goes to Infinity, where N is the population size, so.",
                    "label": 0
                },
                {
                    "sent": "But when we have finitely many data, think of it as we have infinitely many notes here, but we have only finitely many data.",
                    "label": 0
                },
                {
                    "sent": "Which is this subset?",
                    "label": 0
                },
                {
                    "sent": "Let's say so when we model this in this finite subset using Kingman's coalescent.",
                    "label": 1
                },
                {
                    "sent": "Effectively we're kind of integrating out everything that doesn't belong to that red tree, denoted here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so this is what the these are.",
                    "label": 0
                },
                {
                    "sent": "Some examples from the prior and one thing to note is that although there is a lot more leaf nodes here than here, the time scale is or the time to the most recent common ancestor or the root of the tree.",
                    "label": 1
                },
                {
                    "sent": "Is roughly the same, right is on the same order of magnitude, so this is something very interesting of kingman's colors and and that's because this happens because the time to coalesce scales with exponential distribution or with parameter and choose to where N is the number of individuals current population.",
                    "label": 0
                },
                {
                    "sent": "OK, so when we have less individuals then.",
                    "label": 0
                },
                {
                    "sent": "Then it is the first cousin, Time happens later.",
                    "label": 0
                },
                {
                    "sent": "OK, and when we have more individuals, the first coalescent times happen much, much faster.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's basically why we have such a structure.",
                    "label": 0
                },
                {
                    "sent": "We have this similar scaling in the in the time for the most recent common ancestor.",
                    "label": 0
                },
                {
                    "sent": "Another thing that you probably notice by looking at this, although we haven't observed any data and we haven't said anything about the data yet already in the prior, there is some kind of a clustering going on, right?",
                    "label": 0
                },
                {
                    "sent": "So you see that some some points are, some leaves are closer to each other when you look at the length of the branches, then other leaves for example.",
                    "label": 0
                },
                {
                    "sent": "So this motivates us to use keymas coalescent for doing clustering because it has a naturally occurring costing property.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we sample?",
                    "label": 0
                },
                {
                    "sent": "Is there question?",
                    "label": 0
                },
                {
                    "sent": "OK, OK. How do we sample from the prior of kingdoms 'cause and so as I mentioned earlier, the every pair of individual is.",
                    "label": 0
                },
                {
                    "sent": "Can call us with each other equally likely right?",
                    "label": 0
                },
                {
                    "sent": "And Kimmons constant assumes that a repair of a repair of individuals causes at a time that is distributed with exponential rate one with parameter one.",
                    "label": 0
                },
                {
                    "sent": "And the pair with the shortest time classes.",
                    "label": 0
                },
                {
                    "sent": "So in order to be able to sample what we can do is we can sample times for every possible pairing of the data from exponential distribution with rate one and then choose the one that has that has the shortest coalescent time which is.",
                    "label": 0
                },
                {
                    "sent": "This in this pair, for example.",
                    "label": 0
                },
                {
                    "sent": "So we call this them into a new ancestor.",
                    "label": 0
                },
                {
                    "sent": "The second stage we again do the same thing.",
                    "label": 0
                },
                {
                    "sent": "We sample for all remaining points.",
                    "label": 0
                },
                {
                    "sent": "We sample at time from again exponential distribution one, because no matter at what time we start again, the properties of the distribution doesn't change, right?",
                    "label": 0
                },
                {
                    "sent": "So we sample times for all individuals from exponential rate one and choose the one with the shortest codes and time to call as we call that the winning pair.",
                    "label": 0
                },
                {
                    "sent": "So the the one with the shortest.",
                    "label": 0
                },
                {
                    "sent": "Closing time wins the race in a way.",
                    "label": 0
                },
                {
                    "sent": "And we have the Collinson tree.",
                    "label": 0
                },
                {
                    "sent": "So another way.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Janet, from the prior from the coalescent is using order statistics, so as I said, the each pair of individuals coalesce with exponential rate 1 right?",
                    "label": 0
                },
                {
                    "sent": "And there an individuals.",
                    "label": 0
                },
                {
                    "sent": "Therefore their end choose to.",
                    "label": 0
                },
                {
                    "sent": "Pairs of individuals so.",
                    "label": 0
                },
                {
                    "sent": "At iteration I we have this many individuals.",
                    "label": 0
                },
                {
                    "sent": "So now if we consider the order statistics of the exponential distribution, this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the first.",
                    "label": 0
                },
                {
                    "sent": "This is a short in the distribution of the shortest time to call is among this many exponentials with rate one.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can simply sample the time from this distribution and pick a pair uniformly at random and then again remove cost pair at their parents.",
                    "label": 1
                },
                {
                    "sent": "Repeat until everyone is merged.",
                    "label": 1
                },
                {
                    "sent": "I'm hungry in this algorithm.",
                    "label": 0
                },
                {
                    "sent": "You may put your tree binary right by construction, yeah?",
                    "label": 0
                },
                {
                    "sent": "Well, in the other one as well, because you're in this one.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well, because you're sampling from.",
                    "label": 0
                },
                {
                    "sent": "Don't worry, one parent can have multiple.",
                    "label": 0
                },
                {
                    "sent": "OK, I think that's a very good point.",
                    "label": 0
                },
                {
                    "sent": "King must caulescent assume.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Binary trees, so the picture I showed you was from Wright Fisher model.",
                    "label": 0
                },
                {
                    "sent": "I think that you're referring to that one.",
                    "label": 0
                },
                {
                    "sent": "Wright Fisher model doesn't assume binary trees, but now because time is time is continuous, then there is zero probability for more than two individuals to call us at the same time to the same parent.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, it's always binary trees when it comes looking this person again there are extensions of kingdoms coalescent that assume that relaxes assumption and can can have non binary trees.",
                    "label": 0
                },
                {
                    "sent": "But in this one we always assume binary trees.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this one we pick a coalescent time from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Given the number of individuals we call us them.",
                    "label": 0
                },
                {
                    "sent": "At the peak time here, let's say and then.",
                    "label": 0
                },
                {
                    "sent": "We carry everyone to that time because they haven't coalesced yet, so we now have them all here and we sample from again an exponential distribution, this time with of course different end because we have one less individuals and we go on like this until we construct the full tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is another way to sample from the King's cousin prior.",
                    "label": 0
                },
                {
                    "sent": "And the probability of the of the sample from Kingless cousin has this probability.",
                    "label": 0
                },
                {
                    "sent": "So the normalizing constant of the exponential distribution cancels out with the uniform distribution over pairs of individuals.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now we know how.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the sample from the prior, let's see how we sample from the for the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so remember this was the Wright Fisher model and.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can again think of the data generation by first looking at the Wright, Wright, Fisher model, so individuals.",
                    "label": 0
                },
                {
                    "sent": "So before individuals they don't have identity's because they were from the prior right.",
                    "label": 0
                },
                {
                    "sent": "We were talking about the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "Now we're talking about the actual data generating process.",
                    "label": 0
                },
                {
                    "sent": "So we can assume that data the individuals may be of different types.",
                    "label": 1
                },
                {
                    "sent": "So here color represents a type.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And each child is basically a duplicate of the parent or.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It goes on to under mutation and it.",
                    "label": 0
                },
                {
                    "sent": "Mutates into a different type.",
                    "label": 0
                },
                {
                    "sent": "For example, this child is from this pink parent, but it is not yellow because it went under mutation, right?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we go on like this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we see that although this subpopulation originated from the same Adam and Eve, there is still some variability among the population because of limitation.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what we do in Kingman.",
                    "label": 0
                },
                {
                    "sent": "School isn't as well we have this binary tree.",
                    "label": 0
                },
                {
                    "sent": "We dropped mutations along the tree branches.",
                    "label": 0
                },
                {
                    "sent": "Dropping mutations mean we decide on where these mutations will happen by simply running opposing process down the tree, and then we go along the branches, changing the type of the node.",
                    "label": 0
                },
                {
                    "sent": "At every mutation point, according to the transition kernel OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we have a tree structured likelihood and we know how to generate.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "As I said, this is just a Markov process that goes down the tree.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we can generate different types of data, so the types of data that we generate will depend on the type of mutation process that we assume, for example.",
                    "label": 0
                },
                {
                    "sent": "A particular generative process can say there is a person process with uniform rate on the tree and at each spike it generates a completely new individual.",
                    "label": 1
                },
                {
                    "sent": "So this is a model from population genetics.",
                    "label": 0
                },
                {
                    "sent": "This called Infinite levels model.",
                    "label": 0
                },
                {
                    "sent": "So every time there is limitation it is something completely different from the rest of the population because it samples from my continuous distribution.",
                    "label": 0
                },
                {
                    "sent": "So one import 1 interesting.",
                    "label": 0
                },
                {
                    "sent": "Connection is that the marginal distribution induced on the leaves has a Chinese restaurant process partitioning structure.",
                    "label": 0
                },
                {
                    "sent": "If you don't know about this yet, you're going to learn about it later.",
                    "label": 0
                },
                {
                    "sent": "I think tomorrow Peter Robbins is going to talk about it, so you can.",
                    "label": 0
                },
                {
                    "sent": "Just for now, you can just keep in mind that kimmons calls and has some relation to the additional process or the Chinese restaurant process as well.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another mutation model you can think of is multinomial mutation.",
                    "label": 0
                },
                {
                    "sent": "Basically we have a vector representing the data and at each mutation event one of the dimensions of this data changes to something else and what it changes to is encoded by the Markov transition kernel, so it changes to one of the possible states with some probability.",
                    "label": 0
                },
                {
                    "sent": "It is encoded by the Markov transition kernel.",
                    "label": 0
                },
                {
                    "sent": "And at the leaves again we have the data.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is another example.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It doesn't all need to be discrete, so the data can be continuous as well.",
                    "label": 0
                },
                {
                    "sent": "The data generated by this process, for example, we can assume my Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "So let's say that the data starts at at this time 0.",
                    "label": 0
                },
                {
                    "sent": "So these are the values of the data points.",
                    "label": 0
                },
                {
                    "sent": "So the nodes go under Brownian motion and at every split point the branches will go.",
                    "label": 0
                },
                {
                    "sent": "Through Independent Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "OK, so the locations where these points end up are basically the.",
                    "label": 0
                },
                {
                    "sent": "Data identity's and of course you can think of multiple dimensional Brownian motion as well.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How about inference?",
                    "label": 0
                },
                {
                    "sent": "So as I said we have we have data at the leaves which is generated by picking.",
                    "label": 0
                },
                {
                    "sent": "Particulare parent, Particulare most recent common ancestor and dropping mutations along the branches of the tree and going along the branches and mutating this root note according to the macro transition kernel to get the data here.",
                    "label": 0
                },
                {
                    "sent": "Here these vectors represent.",
                    "label": 0
                },
                {
                    "sent": "Basically this is a 1 dimensional data with five possible states, like represents the state that.",
                    "label": 0
                },
                {
                    "sent": "Data takes at that point in time.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start with this.",
                    "label": 0
                },
                {
                    "sent": "This mutation caused it to transition to this state and then this mutation caused it.",
                    "label": 0
                },
                {
                    "sent": "Just transition it back to this state and so on until we get this state vector OK.",
                    "label": 0
                },
                {
                    "sent": "So this is still going forward.",
                    "label": 0
                },
                {
                    "sent": "This is still data generation.",
                    "label": 1
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in inference, what we want is we observe these data at the leaves and we want to go back and reconstruct the tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so when we do inference we don't observe this tree structure or the times of the mutations or anything like that, right?",
                    "label": 0
                },
                {
                    "sent": "We don't we don't observe anything but the data.",
                    "label": 0
                },
                {
                    "sent": "So given the data we want to go back and reconstruct the tree.",
                    "label": 0
                },
                {
                    "sent": "And we may.",
                    "label": 0
                },
                {
                    "sent": "We may do this by basically trying to reconstruct every detail about the tree, including the tree structure, the time of when things happen, and call us the mutation events and what these mutation events cause the data points to turn into an so on right?",
                    "label": 0
                },
                {
                    "sent": "But there will be lots of possibilities for that, especially because time is continues, there will be infinitely many.",
                    "label": 0
                },
                {
                    "sent": "Possible trees, so there's not very feasible to represent every possible 301 class of models that try to do inference on these kinds of problems is basically they don't represent these times, but they represent the coalescent and mutation events and what everything mutates into and so on, but.",
                    "label": 1
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about a different type of approach where we integrate out mutations and we represent things in terms of messages on the tree.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as we represent the state vectors as distribution over the nodes rather than these.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can't white vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, so we represent.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't know what went on in between these data points, for example, but we know what the data looked like so we know at the time that passed between the the observation of the data points and the coalescent events, right?",
                    "label": 0
                },
                {
                    "sent": "So we can have an idea about what are the possible States and with what probability for example.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is the notation we're going to use.",
                    "label": 0
                },
                {
                    "sent": "We have X data points at the leaves and we have these latent internal nodes which we do not buy.",
                    "label": 0
                },
                {
                    "sent": "Huawei and the subscript denotes who coalesced to construct that Y, and we have a state that is sampled from the equilibrium distribution.",
                    "label": 0
                },
                {
                    "sent": "Basically estate this in the distant past.",
                    "label": 0
                },
                {
                    "sent": "With Pi we represent the tree and Pi T is basically the partially constructed 3 until time T, so we go from time 0 towards the past times and Pi T tells us the partitioning structure that we get of these data points until a particular time.",
                    "label": 0
                },
                {
                    "sent": "For example, at this time we would have X1 and Y23 and Y-45 and so on.",
                    "label": 0
                },
                {
                    "sent": "And coalescent events happen at times, T0T1T2 and so on, and the time lapse between consecutive colors and events are shown by Delta.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here this denotes the transition kernel.",
                    "label": 0
                },
                {
                    "sent": "So in order to be able to compute the probability of.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This whole thing, the the data, the latent variables, and this set again, which is a latent variable which denotes the equilibrium state.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can simply use calculus, right?",
                    "label": 0
                },
                {
                    "sent": "We can, we can say, OK, the Markov transition kernels tell us how things turn into other things.",
                    "label": 0
                },
                {
                    "sent": "So this basically tells us what's the probability of going to a particular state if you're coming from a particular state.",
                    "label": 0
                },
                {
                    "sent": "OK, and there are two children per parent, so that's why we have two things here.",
                    "label": 0
                },
                {
                    "sent": "And this comes from the transition.",
                    "label": 0
                },
                {
                    "sent": "The equilibrium distribution and this is basically the probability of transition from the equilibrium distribution to the most recent common ancestor.",
                    "label": 0
                },
                {
                    "sent": "You don't need to get all the details about these equations is just I want to give.",
                    "label": 0
                },
                {
                    "sent": "A high level view of how we do what we do, but don't worry about the equation details.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically what we do is we use.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use this equation, but we know that we haven't observed Y or that we only observed X, so we need to somehow get rid of YN set.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we integrate.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come out by using message passing.",
                    "label": 0
                },
                {
                    "sent": "Again, if you're not familiar with message passing, don't worry about it.",
                    "label": 0
                },
                {
                    "sent": "The only take home message from this whole slide is the fact that the probability of the data at the leaves that we observed given the particular tree that we have, is given as a product of these functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so these functions that are encoded with set and.",
                    "label": 0
                },
                {
                    "sent": "If you want to know details, we can talk up after the lecture, but basically this is the form of the of the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Which we obtained by simply integrating out the hidden variables.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have all the ingredients we need right?",
                    "label": 0
                },
                {
                    "sent": "We have the likelihood which is of this form and we have the prior which is of this form.",
                    "label": 0
                },
                {
                    "sent": "So to get the posterior we combine the likelihood and prior so the posterior is proportional to this here.",
                    "label": 0
                },
                {
                    "sent": "So this comes from the prior and this comes from the likelihood and.",
                    "label": 0
                },
                {
                    "sent": "There is a, so both the prior and likelihood has this product over N -- 1 things because when we have an individuals at the start then we have N -- 1 coalescent events until we get to the most recent common ancestor.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why we have N -- 1 terms in the product.",
                    "label": 0
                },
                {
                    "sent": "So and in the posterior, of course, when we combine these, we will have an minus one products right?",
                    "label": 0
                },
                {
                    "sent": "And one interesting observation is that this comes from the likelihood prior this comes from the likelihood, so combining these will have the posterior.",
                    "label": 0
                },
                {
                    "sent": "So although it's not exactly true, we can view the ice term.",
                    "label": 0
                },
                {
                    "sent": "Here is like a local posterior OK. And we use this observation to construct our proposal distribution to do the inference basically.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do do is we'll start at the leaf nodes.",
                    "label": 0
                },
                {
                    "sent": "Ann will propose a agglomerative algorithm that will iteratively construct the tree by proposing coalescent times an pairs to call us OK, and for that we need proposal distribution, and we're going to use this observation that this is coming from the prior.",
                    "label": 0
                },
                {
                    "sent": "This is coming from the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Therefore this is somewhat like at iteration.",
                    "label": 0
                },
                {
                    "sent": "This ice product is somewhat like the local posterior.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just a very brief recap or not recap very brief crash course for sequential Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "So who is familiar with?",
                    "label": 0
                },
                {
                    "sent": "Monte Carlo techniques or Sequential Monte Carlo.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so you do have some background on it then so important sampling is basically a technique where we want to sample from a particular distribution, let's say P. But it's hard to sample from it.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to sample from it or it takes too much time.",
                    "label": 0
                },
                {
                    "sent": "Too much computation time to sample from it.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we construct proposal distribution Q, let's say, and we sample from Q rather than sampling from P. And we we have some weights that tell us how important each of these samples are so that we weight the samples with these importance weight and some of these samples up to get the.",
                    "label": 0
                },
                {
                    "sent": "Get this distribution that we are interested in.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sequential importance sampling is basically a sequential way of doing important sampling.",
                    "label": 1
                },
                {
                    "sent": "The proposal distribution is constructed sequentially, hence the name sequential.",
                    "label": 1
                },
                {
                    "sent": "And the important ways are also constructed sequentially.",
                    "label": 0
                },
                {
                    "sent": "OK, so because this is sequential, it gathers more information over iterations about the data or about the model.",
                    "label": 0
                },
                {
                    "sent": "Therefore, some early samples may not be as representative as in earlier iterations.",
                    "label": 0
                },
                {
                    "sent": "So if you started off with some samples that had high weight.",
                    "label": 0
                },
                {
                    "sent": "They are representative of the distribution that you want to represent later on at the during sampling.",
                    "label": 0
                },
                {
                    "sent": "These weights may shrink simply because you learn more things about the data and the data didn't tell you what your samples expected it to tell in the very beginning, so some of the weights may shrink.",
                    "label": 0
                },
                {
                    "sent": "So and this is called sample degeneracy.",
                    "label": 0
                },
                {
                    "sent": "So the way to combat this is basically.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Having sequential Monte Carlo or sequential importance sampling resampling So what you do is you have the sequential important sampling you put on top of that I re sampling step which basically means you look at the distribution of weights you sample.",
                    "label": 1
                },
                {
                    "sent": "You sample the samples or you sample the generated points.",
                    "label": 0
                },
                {
                    "sent": "Let's say according to their rates and you prune out the ones that didn't get picked and you keep the ones that did get picked so that you have a better representation of your current.",
                    "label": 0
                },
                {
                    "sent": "Data collection OK.",
                    "label": 0
                },
                {
                    "sent": "So that was the case.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Course to sequential Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "OK, so here what we're going to do is.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about two algorithms, one sequential Monte Carlo, and one greedy algorithm in detail.",
                    "label": 0
                },
                {
                    "sent": "And I'll also go over some other sequential Monte Carlo algorithms that are proposed for Kingman's coalescent.",
                    "label": 1
                },
                {
                    "sent": "One common aspect of all these algorithms that I'm going to talk about is that we start off with leaves.",
                    "label": 0
                },
                {
                    "sent": "As our data points, right so we start off with these data points and we iteratively construct this tree or construct sample trees multiple multiple of them.",
                    "label": 0
                },
                {
                    "sent": "If we are using sequential Monte Carlo that is representative of the posterior trees that generated the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so at each iteration we need to decide which pair to call us and when to call us.",
                    "label": 0
                },
                {
                    "sent": "That is going to be.",
                    "label": 0
                },
                {
                    "sent": "The question.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "As I said, we're going to use these.",
                    "label": 0
                },
                {
                    "sent": "The term that comes from the prior and the term that comes from the likelihood we're going to take this and use that as the proposal distribution for for our sampling algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so this this algorithm that was done by a sad can and tell them, and Roy assumes that this is the proposal distribution and at each iteration it samples a call sometime from the prior.",
                    "label": 0
                },
                {
                    "sent": "And then let's say these were our data points.",
                    "label": 0
                },
                {
                    "sent": "Let's say we decided that they're going to call us here, because that's the time that our prior told us to call us.",
                    "label": 0
                },
                {
                    "sent": "Now we evaluate this so because because now this is fixed, right?",
                    "label": 0
                },
                {
                    "sent": "Because now we have Delta already.",
                    "label": 0
                },
                {
                    "sent": "The time to call us so we evaluate this at that time and that gives us something that is like the probability of that particular pair to coalesce.",
                    "label": 0
                },
                {
                    "sent": "So we do that for valuate this for all possible pairs we have a distribution like this for all possible pairs in the population, so this takes tell you the probability the proposal probability of coalescing for a particular pair, another pair, another pair as well.",
                    "label": 0
                },
                {
                    "sent": "So from this distribution we choose which pair to coalesce and then call us that pair.",
                    "label": 0
                },
                {
                    "sent": "Let's say we chose this pair to call us.",
                    "label": 0
                },
                {
                    "sent": "So we call this them.",
                    "label": 0
                },
                {
                    "sent": "At this point that we already chose from the prior and now we have one less individual to worry about in the population, and we do the same thing iteratively until we get to the root of the tree, OK. Of course we need to compute weights for the sequential Monte Carlo and various sample particles.",
                    "label": 0
                },
                {
                    "sent": "If rates diverge, and if more than one point left we repeat this whole thing and so on.",
                    "label": 0
                },
                {
                    "sent": "But these are all common for all sequential Monte Carlo algorithm, so the important thing is how we propose things.",
                    "label": 0
                },
                {
                    "sent": "OK, so another.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rhythm that was developed by these guys again is again using the same type of local posterior as the proposal distribution.",
                    "label": 0
                },
                {
                    "sent": "But this time, rather than sampling times from the prior, what it does is it first chooses which pair to coalesce by.",
                    "label": 1
                },
                {
                    "sent": "Again looking at some distribution of probabilities for the pairs and the probabilities for each pair is given by this integral.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is the proposal distribution for this pair, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "And it's a distribution over time and over pairs, so if we integrate.",
                    "label": 1
                },
                {
                    "sent": "A time from this we have this right, so we have.",
                    "label": 0
                },
                {
                    "sent": "We have an expression that tells us again a proposal probability for a particular pair to coalesce.",
                    "label": 1
                },
                {
                    "sent": "OK, so we construct this one and we choose which pair to coalesce by sampling from this distribution.",
                    "label": 1
                },
                {
                    "sent": "And then we sample the coalescent time for that pair from this distribution.",
                    "label": 0
                },
                {
                    "sent": "It's basically going back to this what we took the integral off we.",
                    "label": 1
                },
                {
                    "sent": "This time we have we know which pair to do this.",
                    "label": 0
                },
                {
                    "sent": "For so we sample a time for that pair and then again do the same thing that we did before March.",
                    "label": 0
                },
                {
                    "sent": "The perrington endpoint remove merging pairs from the representation.",
                    "label": 0
                },
                {
                    "sent": "Compute weighs 3 sample gone OK, so again, the difference between this is this is sampling times from the prior and then sampling the pairs to coalesce given the time right?",
                    "label": 0
                },
                {
                    "sent": "And this is sampling appare.",
                    "label": 0
                },
                {
                    "sent": "By evaluating this integral for every possible pair.",
                    "label": 0
                },
                {
                    "sent": "And then sampling time.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are both.",
                    "label": 0
                },
                {
                    "sent": "Correct sampling algorithms.",
                    "label": 1
                },
                {
                    "sent": "Just a different take on the proposal distributions.",
                    "label": 0
                },
                {
                    "sent": "So one thing they have in common though, is that they are sampling a coalescent time for so sorry this is evaluating this integral for every possible pair of individuals at the current iteration, and because this expression changes at every iteration, it has to recompute everything.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a order of N squared pairs at each iteration, and there are N -- 1.",
                    "label": 0
                },
                {
                    "sent": "Patience so there is an order of N cubed computations for this.",
                    "label": 0
                },
                {
                    "sent": "Algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this algorithm also has a cubic scaling, because although it's, it's not doing that integral, which which is very costly, so this is more cost effective.",
                    "label": 0
                },
                {
                    "sent": "Still it is evaluating this.",
                    "label": 0
                },
                {
                    "sent": "Value for every pair of individuals that exist at the current time.",
                    "label": 0
                },
                {
                    "sent": "Right, so again, there are an escort individuals and their end on the order of N iterations, so the computations here is again CU.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One way to scale up.",
                    "label": 0
                },
                {
                    "sent": "Info.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is basically trying to avoid this cubic scaling and focusing only on a subset of pairs, and the idea is to focus on the similar points by using the similarity information between pairs of points.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We have we have these individuals right, and we're going to again use the second algorithm.",
                    "label": 0
                },
                {
                    "sent": "The proposal distribution, that is like the second algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is the local posterior.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And will integrate out the time so that we have the probability of coalescing four pairs of individuals.",
                    "label": 0
                },
                {
                    "sent": "But the difference is we're not going to do that for every possible pair of individuals.",
                    "label": 0
                },
                {
                    "sent": "We're going to do it only for a subset.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This would have been the distribution if we were to do it for every possible pair of individuals.",
                    "label": 0
                },
                {
                    "sent": "Rather than that.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to ignore this part.",
                    "label": 0
                },
                {
                    "sent": "We're only going to do this computation for this part only, and we're going to decide who to do computations for by looking at nearest neighbors of individuals and limiting the pairs to a subset of pairs.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If we want to use this in a sequential Monte Carlo scheme, then we need to make sure that these guys have posted probability of coalescing, so we're going to set their probability to coalesce to a constant that's not zero OK.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's say the length of this data pipeline is going to be the probability for the other guys to coalesce, and the probability of these are going to be given by this integral here.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do?",
                    "label": 0
                },
                {
                    "sent": "OK, and if we if we include more pairs than we are.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to.",
                    "label": 0
                },
                {
                    "sent": "Have a more efficient or better representation of our problem of our distribution that we actually want to sample from.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I said, this is.",
                    "label": 0
                },
                {
                    "sent": "This is how we obtained the speedup by ignoring some of the pairs, not really ignoring, but by setting them to a constant their probability to a constant and doing this cost of computation for only a subset.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in order to be able to do things really quickly, what we do is we maintain a priority cube using fast nearest neighbors so.",
                    "label": 0
                },
                {
                    "sent": "I think I'll see.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It doesn't go here, so let's say we have a bunch of individuals.",
                    "label": 0
                },
                {
                    "sent": "What we do is we search for the K nearest neighbor of each individual.",
                    "label": 0
                },
                {
                    "sent": "So K is perimetre of the algorithm.",
                    "label": 1
                },
                {
                    "sent": "So let's say two nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "We search for the two nearest neighbor of each of these individuals like this, for example like that and so on.",
                    "label": 0
                },
                {
                    "sent": "And then we put them.",
                    "label": 0
                },
                {
                    "sent": "On a table like this, right?",
                    "label": 0
                },
                {
                    "sent": "So we have the nearest neighbors here, so not every pair is represented in this nearest neighbor table because they don't appear in the nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "So there's this rest of the pairs.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "We have we would have had this distribution if we've already evaluates the probability of a repair coalescing.",
                    "label": 0
                },
                {
                    "sent": "But as I said, we're going to only focus on a subset of pairs, so not even on all the neighboring individuals.",
                    "label": 0
                },
                {
                    "sent": "But we're going to limit it to a finite number, which I do not here by R. So we will evaluate that integral for these are pairs, and for rest of the pairs, regardless whether they appear in the.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbor table or not, we're going to set their value, their cost and probability to constant vector constant constant value.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Will sample from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Let's say we sampled these guys, then we're going to coalesce them into.",
                    "label": 0
                },
                {
                    "sent": "Knew ancestor, we're going to get rid of the guys that coalesced from representation.",
                    "label": 0
                },
                {
                    "sent": "And also from the tree from the from the priority queue.",
                    "label": 1
                },
                {
                    "sent": "And then include the new guy in the representation.",
                    "label": 1
                },
                {
                    "sent": "Hope this is a little messed up.",
                    "label": 0
                },
                {
                    "sent": "Sorry for that, so we're going to include the new guy in the representation.",
                    "label": 0
                },
                {
                    "sent": "We're going to look for its two nearest neighbors in this case, or K nearest neighbors in general and we'll decide where to put them in the prior to Q.",
                    "label": 1
                },
                {
                    "sent": "Let's say here and there.",
                    "label": 0
                },
                {
                    "sent": "Will update the priority queue and will go on by computing the probability of causing for only only these guys and approximating these as constant and choosing who to call us this time.",
                    "label": 0
                },
                {
                    "sent": "For example, we chose these guys to call us into a new individual and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is how we maintain the priority queue.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just.",
                    "label": 0
                },
                {
                    "sent": "With the new substitute part with the constant probability.",
                    "label": 0
                },
                {
                    "sent": "No, we normalize the remaining parts well, these are all these probabilities that you get from that integral are unnormalized probabilities anyway.",
                    "label": 0
                },
                {
                    "sent": "But yeah, you're right.",
                    "label": 0
                },
                {
                    "sent": "I mean you do.",
                    "label": 0
                },
                {
                    "sent": "You do need to renormalize every.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, but again, as I said, when you do that integral that it doesn't give you a value between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Or a value that like these want sum up to one.",
                    "label": 0
                },
                {
                    "sent": "So you need to renormalize anyway.",
                    "label": 0
                },
                {
                    "sent": "In other questions.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just two.",
                    "label": 0
                },
                {
                    "sent": "Go over the algorithm as a whole.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we maintain a priority queue of pairs according to their distance or their similarity, and at each station what we do is we first for the first are pairs.",
                    "label": 0
                },
                {
                    "sent": "We compute the unnormalized probability this queue of the pair.",
                    "label": 0
                },
                {
                    "sent": "We set the probability of the rest of the pairs to some constant.",
                    "label": 0
                },
                {
                    "sent": "Sample appeared to coalesce from that distribution that sticks that I was showing you and then sample the colors on time for that pair.",
                    "label": 0
                },
                {
                    "sent": "We remove merging pairs from the representation and include new ancestor in the representation.",
                    "label": 0
                },
                {
                    "sent": "Update the priority queue computer SMC rates, resample particles.",
                    "label": 0
                },
                {
                    "sent": "If they diverge, and if more than one point left, reiterate OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the SMC algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The greedy algorithm is basically a really simple variance of this, so we do things deterministically and greedily in the greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "We picked the pair with maximum probability to coalesce rather than sampling OK, so rather than.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "This distribution to sample from we look at the one that has largest probability in this one.",
                    "label": 0
                },
                {
                    "sent": "In this case is the second one.",
                    "label": 0
                },
                {
                    "sent": "So we set that to be the pair to call us at that iteration.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a deterministic move given the probabilities and we set the coalescent time for that pair to the mean of the distribution rather than sampling from that distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so because this is going to be picking deterministically the one with the maximum probability, we don't need to worry about setting the probability of the rest of the things to a constant and so on, because that constant is going to be smaller than the maximum probability anyway.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, and the rest is basically OK. We get rid of these parts that have to do with SMC as well, so we get the greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it's really easy to go to the greedy algorithm.",
                    "label": 0
                },
                {
                    "sent": "Once you have the SMC algorithm or the other way around.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is what the computational costs breaks down to, so we have we have a cost of evaluating the integral I was talking about integrating over the coalescent times for the pairs.",
                    "label": 0
                },
                {
                    "sent": "That is something very costly, so there is the C, which is a very large constant and then there is this R which is the number of things that we want to we decide to do computations for.",
                    "label": 0
                },
                {
                    "sent": "The top are pairs and N is the number of individuals.",
                    "label": 0
                },
                {
                    "sent": "In the Origonal population that we started off with and we still have that end here because there are N -- 1 iterations until we get to the most recent common ancestor.",
                    "label": 0
                },
                {
                    "sent": "So this is, this part is coming from evaluating that integral for our pairs.",
                    "label": 0
                },
                {
                    "sent": "There is this part which comes from the nearest neighbor search so.",
                    "label": 0
                },
                {
                    "sent": "I wrote this for the KD tree for example, so there is the Alpha K factor, so for searching for Kay neighbors and it takes log N time to search for one neighbor in KD trees or in cover trees as well.",
                    "label": 0
                },
                {
                    "sent": "And again there are any iterations and then there is this are of North so it's the cost of re sampling and it's also a function of.",
                    "label": 0
                },
                {
                    "sent": "And it depends on the particular resampling algorithm that you choose and how often you you choose to do resampling.",
                    "label": 0
                },
                {
                    "sent": "And so on South.",
                    "label": 0
                },
                {
                    "sent": "And you may want to re balance your KD trees and so on.",
                    "label": 0
                },
                {
                    "sent": "South all that factors into this are often so.",
                    "label": 0
                },
                {
                    "sent": "The dominating factor here, as you can see, is the analogman factor, so this algorithm overall scales with analog.",
                    "label": 0
                },
                {
                    "sent": "The algorithm that I was talking about before this one scales with a similar.",
                    "label": 0
                },
                {
                    "sent": "See the cost here.",
                    "label": 0
                },
                {
                    "sent": "Technically I shouldn't have written constants in these or equations, but I just wrote them to emphasize that they are really big constants, so there is a C factor coming from evaluating those integrals.",
                    "label": 0
                },
                {
                    "sent": "And then N cube from having to do things over all iterations for every possible pairing.",
                    "label": 0
                },
                {
                    "sent": "And then there's another algorithm that we developed that I didn't talk about here, which has better scaling.",
                    "label": 0
                },
                {
                    "sent": "It is quadratic as opposed to cubic with a similar constant.",
                    "label": 0
                },
                {
                    "sent": "Its management manages to do things quadratically in this case because it saves some iterations by by keeping the coalescent time constant over all iterations.",
                    "label": 0
                },
                {
                    "sent": "So it does things for all possible pairs, but at every iteration it only needs to do things for the pairs that it hasn't done things before that it hasn't done computations for before.",
                    "label": 0
                },
                {
                    "sent": "Again, if you're interested, we can talk about that later, but just so just to emphasize that this is doing things by.",
                    "label": 0
                },
                {
                    "sent": "Keeping some computations that it has done before, so making being able to make use of those computations, whereas this is doing things more efficiently by avoiding doing some computations.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is like the comparison of overall costs.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go onto some applications.",
                    "label": 0
                },
                {
                    "sent": "Any guarantee about the pallative samples to generate?",
                    "label": 0
                },
                {
                    "sent": "Runtime guarantees producer in the end.",
                    "label": 0
                },
                {
                    "sent": "Q arrows into generate.",
                    "label": 0
                },
                {
                    "sent": "Better quality, yeah, that's a very good question, so I am going to show later comparison between the quality of samples.",
                    "label": 0
                },
                {
                    "sent": "The short answer is no, we don't have guarantees for for any SMC algorithm.",
                    "label": 0
                },
                {
                    "sent": "Basically, the more the rule of thumb is, the more particles you have, the better your approximation is going to get, and in theory only if you have infinitely many particles, you're going to get.",
                    "label": 0
                },
                {
                    "sent": "The correct distribution, but experiments showed that at least empirically.",
                    "label": 0
                },
                {
                    "sent": "The quality of samples is pretty good compared to the other algorithms.",
                    "label": 0
                },
                {
                    "sent": "So all the algorithms that I talked about here are generally like my personal experience is that they are generally better than the other algorithms that exist in population genetics that I haven't talked about, which actually represent the mutation events and the coalescent events and integrates out the times as opposed to us integrating out limitations and representing the times.",
                    "label": 0
                },
                {
                    "sent": "So these algorithms typically.",
                    "label": 0
                },
                {
                    "sent": "Produce better samples.",
                    "label": 0
                },
                {
                    "sent": "Plus, if you if you are interested in how they compare how these algorithms compare among each other, so the first algorithm I talked about, the one that samples times from the prior.",
                    "label": 0
                },
                {
                    "sent": "As you can imagine, it is.",
                    "label": 0
                },
                {
                    "sent": "It produces the least efficient samples simply because it's sampling times from the prior.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The last algorithm I talked about, if you look at only one sample then it is pretty inefficient as well, so it gives you quite bad samples if you only look one.",
                    "label": 0
                },
                {
                    "sent": "Look at 1 sample, but the fact that you can handle a lot more samples because of computation time using this last algorithm I talked about.",
                    "label": 0
                },
                {
                    "sent": "Basically answer.",
                    "label": 0
                },
                {
                    "sent": "Makes it makes it's the best performing algorithm simply because now you can get generate many more samples.",
                    "label": 0
                },
                {
                    "sent": "You can get rid of the bad samples by resampling and so you end up getting a better representation of your posterior but not serviceable guarantees.",
                    "label": 0
                },
                {
                    "sent": "OK, a short break.",
                    "label": 0
                }
            ]
        }
    }
}