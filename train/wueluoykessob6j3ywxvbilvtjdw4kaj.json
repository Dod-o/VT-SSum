{
    "id": "wueluoykessob6j3ywxvbilvtjdw4kaj",
    "title": "Large Scale Deep Learning with TensorFlow",
    "info": {
        "author": [
            "Jeffrey Dean, Google, Inc."
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_dean_deep_learning/",
    "segmentation": [
        [
            "Thank you and thank you for having me.",
            "So as usual said, I'm going to talk to you about sort of some of the research work that the drain team has been doing and with a bit of emphasis on the software tools that we've put together to kind of enable that research, and I'd like to point out this is work represented of many, many people here.",
            "There's not only my work clothes, literally hundreds of people at Google have been involved in various pieces of the work that I'll show."
        ],
        [
            "So by a bit of background, the brain project that started about five years ago and we had this hypothesis that time was sort of ripe for neural Nets, particularly if we could apply large amounts of computation and praying very large powerful models on large datasets that were starting to be available and just kind of understand what was possible initially, mostly in the field of perception, and gradually we've migrated to language understanding and other kinds of.",
            "Tasks as you'll see, but initially we were focused on speech and vision tasks."
        ],
        [
            "And I'm going to cover kind of some of our experience from the last five years, both on the research side in terms of the research work we've done in various different kinds of of various both sort of pure machine learning, research and also research in various domains.",
            "Like speech and images and robotics, and then also I'll talk about some of the production aspects of actually getting this kind of work deployed on real Google products, because deploying these kinds of models which are computationally intensive at scale where you have many 10s of thousands of search requests per second and you need to run many neural net inferences for every one of those that's somewhat challenging and offers different kinds of systems challenges than training robots.",
            "The focus of the talk will be mostly on what we've been doing in the area of neural Nets, but a lot of the software that we've built is actually.",
            "More generally, you can use it for all kinds of other machine learning algorithms.",
            "You can actually use it Tensorflow for sort of general purpose numerical computations.",
            "We've seen so much interest outside of Google in using it for what you might traditionally think of as high performance computing."
        ],
        [
            "Fications So 1 interesting thing about the brain team is we kind of bring together people with a mix of computer systems and machine learning, research expertise and kind of put them to work on interesting machine learning research problems and building software and distributed systems and hardware systems that really make sense in the context of accelerating that kind of research.",
            "So we have this kind of nice blend of different kinds of expertise that.",
            "Collectively, we can do things that none of us could do individually because we don't have one person with all these different skills.",
            "And so we do.",
            "Pure machine learning, research and we also do research in the context of emerging machine learning application areas, particularly things like robotics and language understanding.",
            "We're starting to do some work in the field of health care as well."
        ],
        [
            "So we disseminate this work in lots of different ways.",
            "We publish a fair number of papers.",
            "Recently we've released Tensorflow as the core machine learning system that we've been using internally as an open source project, but I'll talk about as part of that.",
            "We're starting to open source models associated with research papers that we published so that people can reproduce our work and build on it more readily than just reading an English description of some paper.",
            "And then we collaborate with product teams at Google to get our research.",
            "Into real products."
        ],
        [
            "OK, So what do we actually want?",
            "In terms of machine learning research, we want to be able to build really intelligent systems that can learn from experience and that experience can be sort of static datasets that exist on large storage systems and we just want to learn from the data that are there.",
            "It can mean agents operating in some simulated or real environment and collecting experience as they go, and then we want to use those intelligent systems to actually solve problems that people have in the world and so.",
            "Touch on some of."
        ],
        [
            "So what do I mean by understanding?",
            "Well, if you think about what humans are able to do looking at raw forms of data like this.",
            "If you look at this image, none of you would have any problem sort of telling me a descriptive sentence about what that image represents similarly."
        ],
        [
            "You don't have any trouble finding all the text in this image.",
            "'cause assuming you can read an identifying what kinds of stores those are from the context.",
            "But computers traditionally been pretty bad at this."
        ],
        [
            "And you don't have trouble understanding what I'm saying, hopefully, but computers are traditionally not been that great at that.",
            "They have much higher word error rates than people listening to other other people speaking, and particularly with more mobile devices having really good speech recognition is actually quite important for building the kinds of experiences we want to build in terms in terms of how those devices get used.",
            "'cause typing is a pain."
        ],
        [
            "Getting back to one of Google's initial product search.",
            "Traditional information retrieval techniques basically take the words in the query."
        ],
        [
            "And then they kind of look roughly for matching words in documents an.",
            "Consider things like proximity of where those words occur and so on.",
            "But really, as a human you have a much higher level of understanding of the language of documents and queries, and you know from reading these two snippets of documents that the second document is much closer match for what the user probably wants than the first one, even though the first one has all these highlighted red terms that seem to match the terms in the query.",
            "So knowing at a high level.",
            "But this is a good match for that document.",
            "The second document is much better match for the query is another thing we really want to get out of language understanding systems."
        ],
        [
            "So here is just some example needs of the future that we would like to be able to solve.",
            "The first few we can basically solve today the first 2 and then they get kind of progressively harder I think, but these are sort of the high level.",
            "This was taken from a talk I gave at a database conference where people were wondering what the future of database systems should look like and I sort of was trying to motivate them to say OK, you have all this data, but really what you want is actually understand the data at a level that you can answer these kinds of queries.",
            "Not that you can select this.",
            "Column and where the other values bigger than 20 years?",
            "I think so.",
            "I think these are kind of the aspirational things that we need to work towards in solving.",
            "I particularly like robot touch me a Cup of tea from the kitchen.",
            "Battle that will require solving lots of interesting problems."
        ],
        [
            "So.",
            "When the project started, there was basically not much use of deep learning at Google.",
            "About five years ago, an R group started and started dabbling with on sort of unsupervised learning research and also worked with the speech recognition team on speech recognition and actually get deployed.",
            "The first production use of neural Nets at Google in the context of our speech recognition system and overtime, we started working with a few more kind of computer vision problems and other teams would hear about the success that some team had.",
            "In solving some machine learning problem and one of the really nice properties that deep learning has is very widely applicable to all kinds of different problems, and so you see the breadth of different kinds of areas that people have applied this to, and the growing use of this within Google.",
            "This is kind of the number of unique directories of in our large shared source code repository that have configuration files for our two different machine learning systems checked in and that graph continues to go up."
        ],
        [
            "So I will, as part of this talk, discuss Tensorflow, an show real example of how we've used Tensorflow for different purposes and also explain what's happening underneath the covers.",
            "So if you ever use Tensorflow yourselves, you can kind of have a better mental model of what the system is actually doing under the underneath covers."
        ],
        [
            "So we've actually built two separate systems in the course of doing our machine learning research.",
            "The first was a system called this belief that we built and published a paper about it NIPS 2012 and had it had the properties that it was very scalable, so we ran some experiments as part of a different NIPS paper by CL paper on 16,000 CPU cores to train a single model, so it had the nice property that you could actually throw lots of.",
            "Lots of data at this problem and have fairly computationally intensive models, but it wasn't super flexible for research.",
            "It was very good at actually being sort of production ready, so you could take a model you've trained and push it out in their production, but it wasn't that flexible, like if you wanted to do something other than a feedforward net training.",
            "It was kind of kind of annoying and hard to specify if it didn't fit the programming model, so our second system sort of based on what we learned actually keeps what we think are the good properties of the first system, which is that it's scalable and good for production use.",
            "But generalizes things and simplifies it a lot actually, and makes it much more flexible for a variety of different kinds of research purposes.",
            "It also enhances the portability so we can actually run Tensorflow models on lots of different kinds of devices, as you'll see an we open sourced it with a fairly.",
            "Freewheeling license so you can kind of take it and do whatever you want with it."
        ],
        [
            "So one property of neural Nets is that the results tend to get better if you're able to train on more data.",
            "But when you do that, you also often need to train that system on using a bigger, more powerful model, either more capacity in the model or more computational complexity somehow in order to sort of by giving it more data, but not increasing the size of the model.",
            "At some point you saturate the amount of information that model can learn, and you really need to do both things, scale the model up and scale the data up, and then scale the model up so that you can pick up on the more subtle patterns that occur only rarely, but are now common enough.",
            "Across the larger data set that you actually see them and the model can generalize from, and the combination of those two things is actually quite painful from a computational perspective.",
            "'cause it's almost like the product of these two factors.",
            "That means that defines how much computation you need in order to actually get the model to train."
        ],
        [
            "This is taken from a paper published in 2015, but you can see similar kinds of trends in lots of different machine learning papers.",
            "This is comparing language modeling task.",
            "Looking at how the perplexity changes as you increase the size of the training set.",
            "So this first graph is billions of words and you can see that perplexity generally goes down for lots of different kinds of language models.",
            "Is a traditional ngram model, and this is.",
            "An RN with 2048 units.",
            "And you can see that making large having a larger data set actually helps you Fairmount.",
            "Similarly, if you hold the size of the data set fixed at 1 billion words, but increase the size of the model from 128 units here to 8192, here, you actually get substantially increases decreases in perplexity as well.",
            "And if you do both those things, presumably it's going to be even better.",
            "They didn't actually do that experiment in this paper."
        ],
        [
            "So this combination works really really well, but it poses pretty interesting systems problems.",
            "In particular, you need lots of computation in order to do this, and you also want to train and do experiments quickly, so the most efficient thing to do if you didn't care about training time, would be to do these computations on, say, a single GPU card, 'cause then you don't have any communication overhead or.",
            "Mutation bottlenecks, but in order to actually get the system to train in reasonable amounts of time, you actually have to generally go beyond that.",
            "The barrier of what fits on a single device for a single machine and use lots and lots of machines to train models quickly.",
            "A sort of related idea is the latency at which you can perform an experiment really influences the kind of research you can do if your experiments take a matter of a few hours.",
            "That's a very different feeling than than if they take a few weeks."
        ],
        [
            "So I'm going to skip."
        ],
        [
            "This."
        ],
        [
            "Anne."
        ],
        [
            "Just talk about some of the initial.",
            "The first thing that we that we worked on with our speech team was just replacing the acoustic model that they used in the speech recognition system with a fairly deep feedforward neural net that tried to predict which phoneme was being uttered in the middle.",
            "10 milliseconds of this this window.",
            "And that actually dramatically improved the word error rate in the speech recognition system.",
            "It actually required fairly intensive training, so we were using several 100 machines to train this model for several days."
        ],
        [
            "At."
        ],
        [
            "Several hundred I think it was maybe 400 or 500.",
            "Something like that.",
            "This was before we had GPS in our data center.",
            "So one way you can compensate for not having GPS deployed yet is used more CPUs.",
            "Anne."
        ],
        [
            "Overtime the speech team has sort of continued to evolve the kinds of models and they've been getting more complicated, so this is a model that they published last year that has some convolution at the bottom and some LCM layers at the top, and then some fully connected layers.",
            "Sorry LM layers in the middle and fully connected layers in the top.",
            "And.",
            "Not that this is exactly the end all be all model, but it's an example of something where you want flexibility to try lots of different kinds of models and understand what works, what doesn't."
        ],
        [
            "And the more recent trend has been to use sequence models of various kinds.",
            "Recurrent LCMS, typically with attention that.",
            "Can model the entire thing.",
            "So rather than separating the speech problem into an acoustic model where you try to predict parts of words and then you have a language model that takes parts of work, sequence of parts of words and stitches them together into.",
            "The actual phrase that the user uttered it's actually more efficient and allows you to optimize end to end to just train the whole thing jointly an allow you to have the model adapt to whatever makes sense and not necessarily have this artificial barrier of phonemes inserted in the middle of the speech pipeline.",
            "So our group and the speech group have continued to push on this this idea."
        ],
        [
            "Similarly, in the image recognition field, this was kind of a Seminole paper that really showed that.",
            "Deep convolutional Nets could be used for image recognition, and you know it's a somewhat complicated thing.",
            "It's got multiple parallel towers.",
            "Alex Chesky actually sort of hand paralyzed this computation across multiple GPU's to get more efficient."
        ],
        [
            "Computation."
        ],
        [
            "And then overtime, some Google researchers actually developed a little module with a more complex pattern of convolutions of different sizes and an sequences of convolutions, and then that basic module."
        ],
        [
            "Gets replicated a whole bunch of times in a very fairly deep model.",
            "I think this one's like 26 or 30 layers or something, but actually get substantially better results.",
            "Where each one of these boxes is maybe 10,000 neurons represents a fair amount of computation."
        ],
        [
            "And so that was published there."
        ],
        [
            "And you can see why people are excited about neural Nets for image recognition.",
            "So before people tried them in image net, the error rate was hovering around 2627% for the few years that they held the contest before then.",
            "And then Alex, an alien Jeffrey, came along an really dramatically drop that error rate and the next year my summer intern at dealers started a company and won the next year.",
            "And then there's been a just a progression of.",
            "Improvements since then.",
            "I particularly like this this result, which is Andre Karpathy, who was helped administer the image.",
            "Net contest as a PhD student at Stanford was really actually wondering what human error rate on this task would be an.",
            "So he tried to convince all his lab mates to actually subject themselves to test, but pretty much only he did a good, serviceable job of it.",
            "So he did 100 hours of training on a training set of images and then then set those aside and then subjected himself to the test set and.",
            "And he got five point 1% error.",
            "An another lab mate of his that only did like 10 hours of training and then gave up got 12% error.",
            "Image that is actually pretty hard 'cause it has like 40 kinds of dogs that you need to discern and.",
            "So another interesting trend to notice is that the number of parameters in these models is actually been going down generally, or it went down and then it went up a little bit, and that's actually important for some kinds of applications because the number of parameters really affects the size of the model that you're going to deploy on a same mobile phone and.",
            "So those are actually a pretty nice for that.",
            "From that respect, it also makes it harder to overfit in a lot of cases.",
            "But anyway, that's why computer vision is working so well.",
            "'cause the error rates gone from 26 to 3%."
        ],
        [
            "OK, So what kinds of properties do you actually want in a machine learning system?",
            "So one is you want this ease of expression that I've been talking about where you want to be able to take any kind of machine learning idea.",
            "Combine it with a reinforcement learning system that might interact with an agent and be able to express all kinds of exotic models.",
            "In a way that's easy so that you can try them out without too much effort.",
            "And then you want the system to be scalable so that you can take that model you've expressed and try it on not just small datasets, but on sort of realistic datasets.",
            "For real world tasks like speech and vision and language.",
            "Anne.",
            "Depending on your needs, you know if you're a researcher you may not care much about portability.",
            "You may just want to get the answers to your experimental questions and then go on to the next thing next experiment.",
            "But if you're going to deploy these things in production systems often you do care about portability where you want to be able to train the models on a cluster of machines or on a desktop machine with a bunch of GPU's in it.",
            "But then maybe diploid, the model on mobile mobile phone.",
            "And another thing you care about is reproducibility.",
            "So you want it to be easy to share results and to reproduce other peoples results in a way that actually you have confidence that you actually reproduce them correctly.",
            "So being able to share actual working implementations of the research that people have done is pretty important, rather than 'cause often you'll read a paper and it will say, well, we initialized with a low learning rate and you're like, well, I have no idea what that actually means in practice.",
            "And then for some kinds of applications, if you actually want to play this in real products, you want the system to be able to take that research idea and then express it in a way that can go into a real product without reimplementing it in a different kind of system.",
            "By the way, I'll take questions anytime people have them so."
        ],
        [
            "And so tensor flow is kind of the second system that we came up with, and I'm going to talk to you about some of the attributes it has."
        ],
        [
            "So I did say this would work well with a lot of people, so this was the white paper we put out with the initial Tensorflow release.",
            "For kind of describes some of the inner work systems aspects of tensor flow also describes the visualization system that it has a bit and some other, and the programming model a bit we act."
        ],
        [
            "We have taken that an adapter that content with more of the systems focus and there will be a paper that appears it's going to appear in this year's OSD.",
            "We just found out last Friday that it was accepted, so that's good.",
            "That's more about the systems aspects of.",
            "The machine learning system."
        ],
        [
            "And we've seen pretty pretty good adoption of tensor flow.",
            "So we had about 50,000 binary installs in 72 hours.",
            "After we released it in about half a million since November.",
            "Anne, it's unclear if GitHub stars in Forks or great measure of engagement, but it is 1 measure an by those metrics for doing reasonably well."
        ],
        [
            "Ones that I do like is that we were the most forked new repository in GitHub in 2015 despite only being available in November.",
            "So I guess that's good."
        ],
        [
            "And it's sufficient that Bloomberg has taken notice of deep learning, open source packages, and they published the article a week and a half ago discussing how many stars different different open source packages have.",
            "So the blue line is tensor flow, and there's a bunch of other ones represented there.",
            "I don't know.",
            "I don't know what it means.",
            "the Bloomberg is writing about deep learning packages, but.",
            "Seems interesting."
        ],
        [
            "So one of the things that I think went reasonably well with the release was that we put together some tutorials that describe how to use tensor flow to implement various kinds of interesting machine learning models, and people really like those tutorials 'cause they talked both about kind of what is happening from the mathematical machine learning perspective, but also what's happening in the expression of that algorithm or system in Tensorflow.",
            "And so we have things like convolutional neural net model for images vector.",
            "Like a word, two VEC implementation, Simple RNN, a sequence of sequence model for translation and people can kind of work through those an understand what's going on at a reasonable level."
        ],
        [
            "So as I said, the motivation for tensor flow was really that it wasn't as flexible as we are.",
            "First system wasn't as flexible as we wanted, and based on that experience we actually could simplify a lot of things that I'll show you in a minute, particularly around how parameters are managed in the machine learning system.",
            "Anne.",
            "Also, make it more flexible."
        ],
        [
            "So the core bits of Tensorflow are basically this graph execution engine, so the computational graph model of computation is you express a computational graph, and then this graph execution engine is responsible for executing that and it's.",
            "Responsible for dealing with different kinds of devices.",
            "It has different implementations for different kinds of.",
            "Operating systems and platforms."
        ],
        [
            "And then there's different frontends for specifying and driving this computation, so."
        ],
        [
            "So the most fully developed ones today are Python And C plus most people use the Python front end 'cause it's sort of the most fully developed.",
            "That's what all of our researchers use.",
            "And we've actually gotten some pull requests from external contributors to add other language support, so there's one to add Java I think, and there's one to add.",
            "Go support ironically from Facebook, which is kind of.",
            "Maybe we should've gotten to that first, but we didn't.",
            "I."
        ],
        [
            "So the basic computation model is a dataflow graph, where the nodes are different kinds of operations on tense."
        ],
        [
            "There's an the things that flow along edges in this system are tensors, not me, like actual mathematical tensor sense, but in the end dimensional array, kind of.",
            "Caveman's interpretation of a tensor.",
            "And the tensors have types.",
            "You can have like a 3 dimensional tensor of floats or two dimensional matrix events.",
            "And."
        ],
        [
            "Then you essentially build your graph.",
            "So for example, you can read the data set and then you can create some placeholders which I'll talk about in a minute and some variables that are going to parameters and then you can say I'd like to do a matrix multiply of these two things and add a bias term and do a softmax, and that's going to be the output in my model."
        ],
        [
            "And unlike pure dataflow graphs, we've actually augmented the dataflow graphs in Tensorflow with notion of variables.",
            "An operations update variables.",
            "So that's basically how you have state in the system for holding parameters.",
            "And so, for example, here we have biases, which is a tensor and it flows into this computer computational graph and eventually we compute some update.",
            "We want to make Tobias operate variable and we do a minus equals operation on it and that allows us to update the state there."
        ],
        [
            "Taking a page from Theano, one thing that people really like about the on our what is the symbolic differentiation support so you don't have to write your own derivatives.",
            "The system just has derivation.",
            "Like differentiation built in and so you can say, I'd like to minimize my model with respect to this cross entropy loss and it sort of does the right computation and I'll show you in later on what that actually reflects two in the computational graph."
        ],
        [
            "And then typically the way that people use this is they.",
            "Launch the graph and run the graph multiple times, once for each mini batch in deep learning application.",
            "And so here you might say, OK, I'm going to get from my training data.",
            "I'm going to get the bat, the inputs and the outputs.",
            "And then I'm going to run training step where I'm going to feed in the batch of examples I read an feed in the labels that correspond to the elements in that batch, and then the system will go ahead and run that computation."
        ],
        [
            "So this abstract model of a graph actually can't directly be executed 'cause we need to actually place where different pieces of this computation are going to be performed.",
            "If we have multiple devices in our system.",
            "And so a very simple setup is you often have a CPU and GPU card in your machine and you want to be able to execute this graph, and perhaps some of the operations maybe don't have GPU implementations, so maybe one of in an image model.",
            "For example, there might be an operation that decodes a JPEG encoded image into the unpacked.",
            "3 dimensional red, green, blue height and width tensor of the actual image image data, and that maybe that doesn't have a GPU imitation and it only runs on CPU's, so the system is responsible for understanding where different operations can be placed and to try to place them in a way that is most efficient for.",
            "Executing the graph as quickly as possible."
        ],
        [
            "And so it might decide to place things here.",
            "And when it does that, the system transparently inserts these things called send and receive nodes that are responsible for managing all the communication across device boundaries.",
            "And.",
            "Essentially it's a pull model, so you start executing your receive node and the receive node.",
            "Kind of implicitly knows which send node on this other device to contact and says, please give me, you know the biases variable.",
            "I'm waiting for an if it's available already, great.",
            "If not, then that's receive operation will kind of hang out until that data is available and then transferred, and whatever way is appropriate for the two different device device pair that is involved."
        ],
        [
            "And so there's kind of the rewritten graph that you never see as a user.",
            "But actually kind of is what it looks like."
        ],
        [
            "Covers.",
            "And one really nice property about this is that by having just send and receive nodes, be responsible for transferring data across device boundaries.",
            "You encapsulate all the communication in the system into just the send and receive node implementations and you can have different implementations for the different kinds of devices involved.",
            "So for example, if you have two GPU cards on the same machine, you can actually do a GPU to GPU memory copy without involving the CPU other than to start the copy.",
            "If you have CPUs on different machines, you can do across machine remote procedure call and if you have GPU's on different machines on your network supports it, you can do our DMA sort of remote direct memory access that allows one GPU on one machine to reach into the memory of a GPU card in another machine and pull out.",
            "You know this particular tensor directly without involving the host CPU or having to go through the.",
            "Per host, yeah, gotta question.",
            "So we don't have an MPI implementation, but essentially, if you wanted to have MPI support, you would add an MPI implementation of the send and receive goods, and that's relatively easy to do.",
            "You can have different implementations of these these nodes.",
            "We do, for example, within Google there's one RPC subsystem that we use that has some extra monitoring and then a variant of that is what's open sourced as the G RPC open source subsystem, but they're slightly different, so we have two implementations of the RPC subsystem that we can swap in and out.",
            "So."
        ],
        [
            "The we've been sort of continuously working on improving the tensor flow system, as have a bunch of external contributors.",
            "So in November that was the initial release."
        ],
        [
            "And in December, you know one of the things that we heard was that people really wanted Python 3.3 support.",
            "There were a bunch of improvements made to GPU performance or performance out of the gate was actually a little little sad.",
            "We've been measuring it with our own GPU compiler toolchain inside Google, so we weren't using MVC.",
            "We're using jakuta CC, which is a variant we've built internally for compiling CUDA code, and it turned out our internal compiler dealt much better with 64 bit tensor indices.",
            "And so the MVC C compiler actually was 20 or 30% slower for every every time you indexed into a tensor, which in Tensorflow you do fairly often.",
            "So we fixed that by moving to 32 bit indices in a lot of cases.",
            "To fix some of the performance issues."
        ],
        [
            "Um?",
            "You know in February we did another release and one of the things that I think is important to understand about tensor flow is the lowest level representation of the graph has very sort of low level fine grained operations, and that's great for researchers who want to control exactly what kinds of operations are being performed.",
            "But for some people they want a much higher level abstract view of neural net.",
            "They want to say I want you know, five layers of this web.",
            "And they want that to be concise and easy to express, and they don't necessarily want very fine control over every last aspect of how those things work, and so we think what will likely happen is there will be a whole bunch of different higher level APIs that people use to specify Tensorflow graphs.",
            "And you can also use kind of the native raw data flow operations, graphs and so we have one that we added that has sort of some higher level support.",
            "There is also curious which is a popular open source packages that have been ported to.",
            "Also be able to use Tensorflow back end.",
            "Some people like that as a way of specifying models, so I think it's not like we will necessarily standardize on one way of specifying computations, But what we hope is that many of those map down on Tensorflow graphs and the optimizations we're doing and will improve on will benefit all those higher level ways of specifying these different system."
        ],
        [
            "So our April release added a distributed runtime, so our initial open source release didn't have a distributed runtime and that was one of the things that people really wanted.",
            "And the reason it didn't have it, we just hadn't yet implemented the G RPC transport subsystem for sending receiving nodes.",
            "And so we implemented that and got that to work with Kubernetes containers as the basis for running many different processes in a distributed system and communicating between them."
        ],
        [
            "And then more recent release, we've added iOS support and GPU support on Mac OS and FP16 support, which is going to be important with upcoming upcoming Pascal cards from NVIDIA for example.",
            "Anyway, the main point of that is just that there's been pretty steady improvement in the kinds of things the system."
        ],
        [
            "I can do this is a graph of activity to the GitHub repository.",
            "You know, since November we've had about 6000 commits to various changes.",
            "Which I think is.",
            "Like 20 or 30 today or something like that.",
            "And actually, the majority of contributors are now outside Google, although the heaviest contributors are are within Google."
        ],
        [
            "So our colleagues at DeepMind were mostly doing a lot of their research and torch, but they've decided to move to Tensorflow to standardize sort of in Google what we use for sort of our deep learning research and also production deployment of deep learning systems."
        ],
        [
            "One of the things that we do is also release pre trained models in Tensorflow where we release not just the model description but also the parameters.",
            "So this is the state of the art inception model that we released.",
            "The pre trained parameters so we can use that in various ways to do diff."
        ],
        [
            "Things, and there's a tutorial that shows people how to take that model an run, run it on their platform.",
            "There's also another tutorial that shows people how to use those pre trained that pre trained model to train then use it as pre training for a different image classification task where you might have a small amount of image data on your own.",
            "And that's Grace Hopper.",
            "Although we identify it as military uniform."
        ],
        [
            "OK, so one of the.",
            "The main motivations for doing all this computer systems work for machine learning is really to make experimental turn around time much lower than it would be if we're using sort of non distributed approaches.",
            "And it's really just a very fundamentally different feeling.",
            "Doing research when the experiments take, you know you can do multiple experiments per day, let's say then if it's something where you have to launch it and then wait several days or even.",
            "Several weeks and then you get your results and then you're like, oh, why was I running this in the 1st place?",
            "I kind of forgotten at this point, so that really is a big focus of us is to make that turn around time as low as we can."
        ],
        [
            "And one of the ways we do that is with large amounts of raw computing hardware.",
            "But you also need to make it easy for people to use those computers.",
            "You can't just sort of throw raw computing hardware.",
            "And expect every machine learning researcher to also be a distributed systems expert and know how to use a rack of GPU's in raw form and so Tensorflow is this system that is fairly abstract in how you specify the model, but then the system will have map that down to a bunch of hardware that you say, OK, I have the ability to use 32 GPU GPU cards for this experiment."
        ],
        [
            "So one of the ways you can get faster training is through data parallelism.",
            "And the idea here is that you want to use multiple model replicas that all kind of collaborate to update the shared set of parameters in the model.",
            "And but they're all going to be training on different data.",
            "The speedups that you can get here depends pretty highly on different.",
            "What kind of model?",
            "So for dense models, we regularly see kind of 30 or 40 X speedup using 50 replicas, so it's not quite linear speedup, but it's quite good 'cause it means.",
            "With 50 replicas, you move the time from a month down to a day for an experiment, and that's pretty compelling for sparse models where you have like a bunch of embeddings for vocabulary items and a given example only touches maybe a handful out of millions of embeddings.",
            "You can support many more replicas, often in those kinds of environments, so we regularly train models with 1000 replicas for some kinds of applications that have these very large sparse embeddings."
        ],
        [
            "Yeah."
        ],
        [
            "Better in that case.",
            "It's just you get less interference with the gradients from the multiple replicas.",
            "So if you're only touching 5 words in the vocabulary, the odds that another mini batch on some other machine has touched those same words is pretty low.",
            "So you still have interference for the shared part of the model, but that tends to be updated.",
            "You know the change in those shared parameters tends to be slower.",
            "And so you end up just being able to support more replicas before you kind of fall off a Cliff."
        ],
        [
            "OK, so the way this works, this is kind of a diagram from our earlier system.",
            "This belief is we had this set of machines called parameter servers that are going to be responsible for maintaining the current set of parameters in the model, and then we're going to model replicas, which themselves might be multiple machines or multiple."
        ],
        [
            "Devices.",
            "And.",
            "Be forgiven model replica actually processes a mini batch."
        ],
        [
            "It's going to download the current set of parameters from the parameter servers and those parameters might come from 100 different parameter servers where the parameters are distributed evenly across those, so that you get more network bandwidth into the parameter servers than if you just store them on a single machine.",
            "And then the model replica."
        ],
        [
            "I'll read mini batch of examples from disk will do the computation necessary to computer gradient and then instead of applying that gradient locally, it will go ahead and."
        ],
        [
            "Send the gradient to the parameter servers in a distributed manner, so they might be again spread over 10 or 100 machines and then that parameter servers.",
            "The parameter service will actually be responsible for updating the parameters and so that's one round trip through a mini batch is you now download the parameters and send gradients and they get applied in the centralized service and then before the next example you do the same thing.",
            "So now we have prime."
        ],
        [
            "And change to be prime.",
            "We get P double prime."
        ],
        [
            "And really, all of these machines."
        ],
        [
            "They're going to be doing this at the same time.",
            "And there's a few variations of how you can orchestrate this kind of parallelism."
        ],
        [
            "So in disbelief, we actually had these Peru."
        ],
        [
            "Better servers are actually completely separate software subsystem and the communication mechanism used here was not the same as the communication mechanism used, say between different machines in the model itself, which actually caused kind of a bunch of complexity."
        ],
        [
            "It also meant that you only could select from a handful of parameter update rules.",
            "You could didn't have the full generality of the programming language for updating parameters.",
            "You could say, OK, I want to do.",
            "Apply a standard learning rate update or a learning rate with momentum or a few things like that, but not much more general thing and so that ended up being much more complicated and so we dispense with that separate system in tensor flow."
        ],
        [
            "So because the send and receive nodes sort of abstract away network communication in Tensorflow, we're actually able to model the parameter server just as other devices in the computational graph.",
            "And so, for example, these things might.",
            "You might say that this part of the model here is a parameter server device, but it's just part of the computation graph that you're trying to perform, and it happens to have send and receive nodes to get copies of the parameters onto other machines or other devices."
        ],
        [
            "So the two main choices you have for data parallelism are you can do this synchronously where you have like as.",
            "All kind of processing a mini batch and essentially that's equivalent to a single machine processing and end times larger batch and for some kinds of problems that's OK, but often a large, much larger batch size means if you make your batch size 10 times as big, you don't learn 10 times as fast.",
            "The other advantage of this approach is you don't get any gradient staleness.",
            "So when you download the copy parameters, you synchronized everything so everyone gets the same copy of the parameters at the beginning of the step, they compute their own individual gradients and then you combine them somehow, either by averaging or adding the gradients from the individual machines.",
            "And so there's no issue of applying gradients to stale copies of the parameters.",
            "Another drawback, eh?",
            "Drawback of this approach, though, is it's less fault tolerant, so if any one of the machines dies, you kind of have to do some recovery process to deal with that.",
            "Whereas in the other approaches, to do this asynchronously, so if you have 10 replicas, they each kind of independently do steps as fast as they can.",
            "Independent of what the other machines are doing, yeah.",
            "I swear some machines will produce faster for some reason.",
            "The synchronous case, more typically yes.",
            "So in the asynchronous case, let's say 9 machines or zippy fast and attempt one has some other job running on it, so it's slow.",
            "That doesn't hurt you as much in the asynchronous case, 'cause that one just will give you half the throughput, but everyone else kind of runs at full speed rise.",
            "In the synchronous case that actually causes trouble.",
            "I'll talk a minute about some mechanisms to alleviate this.",
            "And you can also do some hybrid in between this with M groups of ends in cronous replicas."
        ],
        [
            "So in pictorial form, what this looks like is you might have this is the asynchronous training.",
            "You might have three different threads driving 3 copies of the model, all sharing the parameter device, so they all get the parameters from the same place.",
            "But then they do their own independent computation on a replica of the computation graph.",
            "And these threads kind of don't interact at all other than saying OK, I'm done with my step.",
            "I'm going to start in the next step."
        ],
        [
            "Whereas in a synchronous model, there's one controller for the entire process and the computational graph conceptually is sort of 3 * 3 replicas of that computation in one graph.",
            "And you get the parameters you do the computations on the different mini batches and then you add the parameter gradients from the individual mini batches together to then compute what the full gradient is for the full batch, and then you update them.",
            "Return to the other side, yeah?"
        ],
        [
            "So these clients could either be different threads on the same process, or they could even be separate threads in separate processes.",
            "As you scale, if you have hundreds or thousands of replicas then you probably want these to be separate processes themselves, because otherwise this thing can become a bottleneck and your pile of hardware here is bottleneck because this guy can't even say start fast enough."
        ],
        [
            "In terms of how the gradients get, parameters get updated every every square.",
            "Here is an update to the parameters and so an asynchronous replication.",
            "Those just pile in whenever they finish and in synchronous replication they piloting this orderly synchronized manner.",
            "We can actually introduce backup workers, where in this case we're showing three workers, Anne, we take the first 2 that finish, so this is a technique we used in map reduce.",
            "Which is a general computational framework of backup tasks where you essentially do additional computation sometimes, and take whichever one finishes 1st, and so here we're actually doing slightly different computations.",
            "It's typically a different randomly selected mini batch.",
            "And then you take whichever two of these smaller things finish first, and then combine those to compute the gradient and drop the last one.",
            "And the real numbers are typically more like 100 replicas with five extra ones rather than two and three."
        ],
        [
            "So one factor in using data parallelism in this way is because you're sending the parameters of the model over over the network quite often, basically before every one of these, these mini batch steps you really want the model computation time to be large relative to the time it takes to send and receive the parameters over the network.",
            "The models that have fewer parameters but use those parameters in many different floating point operations actually tend to work better in this regime, so convolutional models are very good in this regard because you essentially you reuse the convolutional parameters for a layer at every different spatial position, and so that you might reuse a given floating point number you've copied over the network as a parameter 10,000 or 100, or.",
            "50,000 times.",
            "Recurrent models also have this property, although not quite to the same extent because you're typically unrolling through time and you might unroll 20 or 100 steps, so you get to reuse that floating point value that you've brought in over the Network 20 or 100 times in that case."
        ],
        [
            "So data parallelism actually is a really important tool that we use pretty heavily.",
            "So for example, training are one of our ranking models for search, we use 500 replicas.",
            "We use 50 GPU's to train some of our large image models for image.",
            "For image net typically get maybe a 40X speedup.",
            "We have a system called Smart Reply that uses 16 replicas, each with many different GPU's.",
            "So this is pretty pretty important for getting the turn around time low."
        ],
        [
            "And just to illustrate this in pictorial form.",
            "So what we have here is a image.",
            "This is an inception model trained on image net and what you see is the X axis is hours of training time in the Y axis is accuracy.",
            "And."
        ],
        [
            "If you look at the difference between one GPU versus 50, we're getting 30 and a half X speedup with 50 GPU cards would be better if it was 50, but it's not, but it's clearly much better."
        ],
        [
            "The other thing we've been investigating a bit is.",
            "Going back to using more synchronous training techniques just because the noise in the gradients.",
            "That you get in the asynchronous regime tends to mean that you can't quite get to the highest level of accuracy because of that noise.",
            "So this blue line here is an asynchronous training with 100 replicas.",
            "Be aware that the Y axis is quite compressed, so it's .77 point 795, so these are fairly subtle differences.",
            "But still, if you care about that last little bit of accuracy, you can see that the asynchronous training kind of tops out at like 785 here, and you get another.",
            "Half a percent or almost a whole percent of accuracy in the synchronous training case because the noise in the gradients actually just means you bounce around a lot rather than actually descending to the bottom of the Valley like you want to.",
            "Learning.",
            "Even if you decrease the learning rate, an yeah that's our experience.",
            "Or you have to decrease it so much that then the training time becomes sort of antenna."
        ],
        [
            "And so if you look at this, if you compare several different comparisons are interesting here.",
            "So one is asynchronous versus synchronous training and you see you get to the same level of accuracy for synchronous training in 40 hours instead of 52 with basically the same amount of hardware.",
            "And, uh."
        ],
        [
            "A good comparison is.",
            "The synchronous of 100, but we also have five extra workers, so we're not using 15 replicas instead of 100, so it's 5% more hardware, but that got you a pretty big win.",
            "'cause now instead of 40 hours is 30 hours.",
            "Versus 52 hours."
        ],
        [
            "And if we compare just the synchronous 100 versus the synchronous 105 extra workers.",
            "You know you throw 5% more hardware at the problem, but you got a 25% reduction in training time.",
            "So, and these are all relatively easy to express as tensor flow graphs in various ways, and there's an archive paper about these experiments essentially."
        ],
        [
            "Any questions about any of that, yeah.",
            "Extra workers"
        ],
        [
            "Yeah, so in part it's because these machines are in Google's datacenters and there may be other tasks running on the machines sometimes.",
            "Or there may be network traffic on the network that is interfering with some of the machines more than others, and So what that means is you have a fair amount of variability in how long it takes each machine to complete this step so you know the median might be, say, one second, but the 99 percentile might be 1.",
            "4 seconds.",
            "And if you're waiting for the last guy, but 99th percentile is what you actually care about.",
            "'cause you have 100 guys and you have to wait for the last one and so basically by doing 105 and taking the first 100, you're able to bring it in so that what you care about now is the 95th percentile latency rather than the 99th.",
            "And that just tends to be that in these large datacenters there's just a lot of stuff going on.",
            "There's like background processes waking up on the machines every so often to compress logs, and there's extra traffic on the network, and the machines are overheating, so some of them are like slightly underclocked because of that, and this just makes it more robust to that sort of thing, and you can't sort of eliminate all those sources of variance very easily, so it's best to just build the system to be more tolerant of them.",
            "Yeah.",
            "This is a dense models.",
            "This is an image net model with convolutional kind of set of parameters.",
            "So not super many parameters, but a lot of computation per step.",
            "Has big differences in terms of accuracy or.",
            "I.",
            "Probably not as much because the thing that's killing you here is the conflicting updates for the asynchronous updates, 'cause the parameters move in the meantime and in sparse models that's less likely to be the case because the interference pattern looks more like that rather than like that.",
            "Yeah, I guess that's the reason we need backup workers, mostly because you know, I'm still down to the machine, but without slowdowns, use the data itself.",
            "That system that is just harder for us to make sure that you will eventually.",
            "Presently factions, or you don't think that under Florida.",
            "Yeah, I mean the assumption here is that most trips through the model are roughly the same, so that's true for image models.",
            "At least these dense image models.",
            "If you start to have branching in the models then then you probably.",
            "Would need to add some extra mechanism to keep track of the probability of sampling a particular example and make sure you don't drop examples in unbiased manner.",
            "Or in any bias manner.",
            "Yeah, 30 point 5X slide was it with synchronous plus extra or asynchronous."
        ],
        [
            "This was synchronous I believe."
        ],
        [
            "Yeah, I don't know if you're going to talk about the model or not, but I just want to know how do you feel about the whole data partners in the story?",
            "Because this please, as far as I know, the only possible model was you know their model replication over many devices, but that has been changed in Tensorflow and Tensorflow.",
            "Its potential is capable of, like you know, find training of how data is being transferred week and everything.",
            "So is that because you know you think that data?"
        ],
        [
            "Where do things in it?",
            "And because it's based on this graph, it seems that it kind of scale is 100 machines, not more than that.",
            "Right, so actually this belief also supported model parallelism, so if you think about this in either Tensorflow or disbelieve, each one of these yellow rectangles could be a separate machine that has got model parallelism involved in the model itself.",
            "In fact, we regularly train for example language models or sequence to sequence models, where each one of these might use 16 GPU cards, 'cause it's a very deep model with 16 layers, and you get pipelining as I'll show you later in terms of model parallelism.",
            "But then you also have.",
            "Data parallelism where you have multiple replicas of those 16 GPU setups, so it's not like you have to pick one or the other.",
            "Often both of them are good to do.",
            "Depending on your.",
            "Tolerance for what you want.",
            "Or was I?"
        ],
        [
            "Other questions."
        ],
        [
            "OK, so.",
            "So we originally designed tensor flow to kind of generalize the kinds of computations we wanted to do in terms of the deep learning research our group was doing.",
            "But it's actually pretty flexible, so I think what we'll see overtime is that a lot of other kinds of computations can be expressed in Tensorflow, and the kinds of optimizations we're starting to do, and will do more of.",
            "We'll make lots of different kinds of computations, expressed.",
            "Potential graphs run efficiently on sort of very heterogeneous hardware.",
            "I mean it's a very nice model that you can express this abstract graph and then say I would like to use 16 GPU's please and have the system automatically place the computation.",
            "Or have hints about where the competition should be placed."
        ],
        [
            "So as I said, it runs on the variety of different platforms, which is important for some kinds of applications.",
            "And in particular, we're starting to see people building custom machine learning hardware.",
            "For that, make certain kinds of operations that you see in deep models much more efficient than on CPUs or GPU's.",
            "Yeah, there's a company called Movidius that is built this kind of very low power.",
            "Custom ASIC that doesn't have a huge number of raw flops but actually does a fair number of flops per Watt.",
            "And they have a little evaluation USB stick, so you can get one of these and plug it into your laptop and have an accelerator for different kinds of models.",
            "I think that a typical application they're imagining is something like a drone where you actually want on board image models to run efficiently and not drain your battery too much.",
            "And Google has actually built a custom ASIC that I'll talk about in a minute for mostly for accelerating inference in datacenters.",
            "And I'll talk about that in a month."
        ],
        [
            "But I think a really likely trend is are going to be much more heterogeneous hardware in data centers and also in mobile devices because essentially general purpose CPU performance scaling has stopped following Moore's law.",
            "The basically you can't make.",
            "In general purpose CPUs that give you the performance scaling we were getting, say a decade ago anymore.",
            "And so specialization of hardware for different kinds of workloads will be much more important, and because deep learning has this handful of primitives that are really important, things like matrix multiplies or elementwise vector operations, they'll be a really big opportunity to specialize hardware.",
            "And they're also very tolerant of reduced precision, an noise.",
            "And that is a pretty different design point than you know.",
            "Always correct logic in your 32 bit multiplier that you might have in a general purpose CPU, so I think that'll mean that people tend to look at what kinds of things can we do that are a bit more specialized, especially because, unlike other kinds of specialized hardware that might do encryption well or might do compression well.",
            "If you believe the deep learning is going to be in pretty much everything.",
            "Then you can build specialized hardware, but it's not really so specialized right and like actually does everything you want.",
            "Which is kind of nice."
        ],
        [
            "So Google's take on customized hardware.",
            "Is this custom machine learning ASIC that we are chip design team built an it's been in production use for almost a year and a half now.",
            "It's used on every search query it was used for the Alphago match in March against Lee Sedol in Korea that DeepMind arranged so you can see the rack of machines that was used for the Alphago match with a little commemorative go board on the side.",
            "And essentially it gives us an order of magnitude better top peak performance, but also performance per Watt over things like GPU's and CPU's.",
            "So we think that's pretty important.",
            "The other thing it gives you is because it has higher peak operations.",
            "It means you can employ fairly computationally expensive models.",
            "At lower latency and for a lot of interactive kinds of applications, you really care pretty deeply about.",
            "You know responding quickly to the user.",
            "So you said it's pretty general.",
            "It's doing tensor operations, but you also said it was only used for inference.",
            "Where what's what's missing for doing training?",
            "It just turns out that inference requires a bit less precision than training generally, and so if you want to build something specialized for inference, then you might want to build something with even lower precision than really can be accommodated for training.",
            "Kind of precision.",
            "Do you have?",
            "We mostly are doing 8 bit operations.",
            "It rain within.",
            "Dad, it's harder.",
            "But definitely 32 bit Floating Points are overkill for training, so there is a 16 bit.",
            "8 bits for activations is plenty.",
            "Yeah, maybe with some scaling and so on.",
            "Yeah, question up there.",
            "The the first part was is it expensive?",
            "Priceless.",
            "We are not currently selling them so.",
            "You know?",
            "Right?",
            "Well, I mean we look at performance per dollar in terms of the solutions that we have available and we think it's better than other solutions out there.",
            "So you can infer from that what you like.",
            "I guess we don't currently have any plans to announce to make them available, but.",
            "It might make sense down the road to.",
            "Maybe not sell them, but sells sell the ability to run computations on them.",
            "OK.",
            "I think machine learning hardware is going to be interesting because.",
            "I think hardware designers are just starting to really look at what you can do given the kind of relaxed constraints of lower precision and maybe noise.",
            "In terms of what that might mean for designing hardware rather than having to always get the right answer, say."
        ],
        [
            "So I'm excited about that.",
            "So tensor flow is also extensible, so the core system defines a bunch of standard operations which are kind of these abstract things like matrix multiply or elementwise vector ad.",
            "And then also kernels which are implementations of operations for particular devices.",
            "So we have this kind of two level model of the abstract operations and then different implementations of those that can be mapped onto different devices.",
            "And not every operation will have an implementation for every device.",
            "So if you have something that only runs on CPU, you just wouldn't have a GPU kernel for that, and the system would know well my available choices for running this have to be on a CPU, 'cause that's the only kernel that I have for that.",
            "And it's relatively easy to define new operators or kernels if you wanted something that's not covered by the standard thing, but that's relatively rare for our machine.",
            "Learning researchers generally to want to do that."
        ],
        [
            "OK.",
            "So let me take a bit of a lower level tour now through what's actually happening under the covers.",
            "When you're expressing different tensor flow things, I was going to put together some slides and then I discovered this person, Kevin Robinson at MIT had put together a great set of slides that were covering mostly what I wanted, so I asked him if I could use his slides and he said yes.",
            "So the slides with this background, our thanks to Kevin.",
            "OK, so some of this I kind of already talked about, But basically you have this computational graph."
        ],
        [
            "And each one of these things is an op an as I alluded to, it has."
        ],
        [
            "Kernels, so you express these OPS.",
            "This is similar to the example I showed you earlier, but all in one slide, and we're going to have this little running example to go through.",
            "These things were going to weight matrix some input bias term that gets added together in a relative unit and some cost function."
        ],
        [
            "So now if we look here at the matrix, multiply."
        ],
        [
            "What that really means is we're going to take the tensor flowing out of W and the tensor flowing out of X, apply the matrix, multiply operation, and produce an output that will flow into some other part of the graph."
        ],
        [
            "So in Python you would write this TF macmall W, X.",
            "We actually automatically generate Python wrappers for the lower level representations of these things, because in C++ we have a matrix multiply operation that's got a variety of different options, like is a transposed already or is be transposed already.",
            "What is the type of the underlying elements?",
            "And so this call here will generate a call to here saying.",
            "Basically it's not transposed.",
            "So this is Python code.",
            "This is code.",
            "The user wrote.",
            "This is code that's automatically generated by our wrappers generators from the operations that are defined in C. Source code down here."
        ],
        [
            "So."
        ],
        [
            "Now we're going to zoom along, and this is now where we actually are going to execute the graph multiple times."
        ],
        [
            "And so we built the graph.",
            "This shows you the graph."
        ],
        [
            "And we have a visual."
        ],
        [
            "For the lousy to see what the graph that gets constructed by your Tensorflow program looks like in this visualizer front end called tensor board.",
            "And so you can see here we have an assignment to variable W and we have a read of this variable."
        ],
        [
            "And then the optimizer functions extend the graph so this."
        ],
        [
            "Is where the automatic symbolic differentiation happens.",
            "So for in particular if I say minimize the cross entropy loss here, then it's."
        ],
        [
            "Going to automatically compute derivatives of the various variables in my parameters in my model with respect to this, this cross entropy loss Ann will do the right extension of the graph.",
            "I mean, it's sort of drawn in this slightly weird way, but really you can think of it as more graph.",
            "And this is very much how Theano operates.",
            "As well."
        ],
        [
            "And then one of the ways that the graph the actual computation gets communicated around is through a serialization mechanism called protocol buffers.",
            "So that's a way of expressing what the graph is in a way that can be sent across machine boundaries pretty easily."
        ],
        [
            "So here we say, please give me the graph as graph DEF, which is a protocol buffer.",
            "And so here's a node with the matrix multiply operation where we're having W&X and they're not transposed.",
            "And then there's an add operation an irrelevant.",
            "And so that's kind of the representation of those three graph nodes.",
            "In graph.",
            "There is."
        ],
        [
            "One is that."
        ],
        [
            "There's that one."
        ],
        [
            "OK, let's see where are we in terms of time.",
            "Let's keep going a little bit right.",
            "We break it for.",
            "Or one for me.",
            "OK, so now let's look at how we actually get these graphs distributed."
        ],
        [
            "Across multiple devices so.",
            "In a distributed setting, there are many different processes involved.",
            "There's a client process, and then there's typically a master process that is responsible for kind of coordinating all the different operations that are going to be performed and orchestrating the graph execution.",
            "And then there may be multiple worker processes involved in your.",
            "In your model.",
            "Which may have multiple devices.",
            "So when you create."
        ],
        [
            "A new session that says please.",
            "Give me a way to talk to this master coordinating process.",
            "So over an RPC layer like G RPC.",
            "That has, so here we say I'm going to create a session."
        ],
        [
            "And then it's going to send in RPC."
        ],
        [
            "The master service with the saying."
        ],
        [
            "Please create this session to execute this graph and that's where the graph that you've constructed in your client program actually gets converted into the serialized form and sent over to the Master where."
        ],
        [
            "So now we have the graph on the master."
        ],
        [
            "And later you would say please run this graph and I would like to feed in for the value of X in the graph.",
            "This particular input value."
        ],
        [
            "And so that gets sent over as well and gets converted into a run step called says.",
            "Please run the graph.",
            "Given this data that's being fed in for X and you can also say I'd like to fetch the value of the cost of the model or whatever."
        ],
        [
            "And so now that coordinator is going to."
        ],
        [
            "Talk to the various workers involved in the process."
        ],
        [
            "And it's going to say please run.",
            "Run this graph for."
        ],
        [
            "The part that you have an these things might have receive and send tensor operations receive tensor calls that communicate tensors amongst these different workers as necessary to execute the entire graph."
        ],
        [
            "So one of the things you can do in Tensorflow is you can say, here's my general graph, but actually on this run call I only want to execute and fetch.",
            "Node F, and by the way, I'm going to feed in this value for node C."
        ],
        [
            "And so conceptually that gets converted into this."
        ],
        [
            "And then we can prove that we can then detur."
        ],
        [
            "And because we only want to fetch node F. We can determine the translator dependencies that we need to execute in order to compute the output of node F. And when we do that."
        ],
        [
            "Means we don't need to execute these other nodes in the graph.",
            "In this particular case.",
            "So this is a pretty general technique intensive where you express perhaps many different kinds of things you want to do in the graph in a single graph, and then depending on what operation you want to do, you execute just a piece of that graph.",
            "So we often for example have some part of the graph sitting off on the side that has checkpointing operations for the parameters in the modeling, and normally we don't execute those, but then every 10 minutes or something will execute this other part of the graph.",
            "That is going to Check Point the state of the model onto disk, or restore it for example, but most of the time we might not execute those.",
            "And that's a pretty general technique for.",
            "Having a single static graph that can be a little bit dynamic."
        ],
        [
            "Um?",
            "So one of the things that we need to do once we get this graph in the master process is figure out where we're going to place the different bits of computation, and we allow users to specify hints about that.",
            "So you can say I would like this particular set of variables to be placed on this job task 0.",
            "This is how you might, for example, use a parameter server and distribute a few of the parameters and their model across different parameter servers.",
            "And then I would like this bits of computation to be done on task 7.",
            "This is sort of."
        ],
        [
            "Unideal, ideally we would just take the graph and figure out where we want to place things, but that's slightly more aspirational than actually works in practice today.",
            "So we allow people to specify hints you don't have to specify hints for everything if you specify a few hints that will kind of anchor bits of the graph, and then the rest of the stuff will generally happen correctly, and we have plans to work on this more seriously to actually do a good job of actually measuring.",
            "You know where we should be placing things?",
            "This is actually an interesting machine learning problem in terms of because it's actually very amenable to reinforcement learning.",
            "Where you want to place nodes here and then you can get a measurement of how long the graph takes to execute and then say Oh well, that was a bad idea.",
            "I should have placed it over here and then.",
            "The trick is you want to be able to generalize from experience with one graph, 2 similar kinds of graphs.",
            "So we think that that might be an interesting problem to work on."
        ],
        [
            "Right?",
            "So here."
        ],
        [
            "Where is the placement we might."
        ],
        [
            "Decide to place, for example, these nodes on this device and these nodes on this device.",
            "Or these could be devices in the same process, but are different GPU cards, for example.",
            "So you see the difference there.",
            "This is actually a separate process, maybe on a different machine.",
            "Or they might be different devices, but on the same machine.",
            "Two different cards."
        ],
        [
            "And so one of the first things that happens is we partition this graph into subgraphs after we've made placement decision."
        ],
        [
            "And then we insert the send and receive nodes."
        ],
        [
            "And then we have this thing called a rendezvous.",
            "That's how the send and receive nodes kind of coordinate their activity together."
        ],
        [
            "Right?"
        ],
        [
            "And."
        ],
        [
            "So let's see what happens when we do run."
        ],
        [
            "Graph, so we're going to execute."
        ],
        [
            "This graph."
        ],
        [
            "There's a Colonel."
        ],
        [
            "So if the operation here A is for matrix multiply, we're going to find the corresponding kernel, and we're going to."
        ],
        [
            "Cute it.",
            "If it's a conf 2D operation, then there's a different kernel that's defined for that, or an op for that."
        ],
        [
            "And there's different kernel specified, so this is the C++ code that gets registered mostly if you're using tensor flow from the Python level, you don't have to care about any of us.",
            "You just care that the right operations for your model are there."
        ],
        [
            "And so, for example, here is the matrix multiply operation."
        ],
        [
            "Every operation, every kernel has a compute method, which is what gets invoked on every step, and that allows you to access your input tensors and then perform operations on them.",
            "So in particular, this will launch a metric multiply operation on A&B."
        ],
        [
            "And do the right thing.",
            "This is a different way of doing."
        ],
        [
            "Yeah.",
            "That's so exciting.",
            "This allows you to get a stream context for GPU devices.",
            "We've abstracted away the Koblas interface a bit so that we can, for example, use a stream execution stream for GPU cards and have a different implementation of it for, say, TPU cards."
        ],
        [
            "Yeah, there we go."
        ],
        [
            "I think I'm going to skip."
        ],
        [
            "Most of the rest of this."
        ],
        [
            "It's not supported."
        ],
        [
            "Right, so one of the benefits of the session interface is that you can essentially specify this graph and then call run many times, and that's typically what users do.",
            "Is they set up their model and then you run thousands or 10s of thousands of.",
            "Steps through your model and so that provides a big opportunity to do significant optimization work, and we do some of that today, but we intend to do a lot more in terms of generating really good code for.",
            "Particular sizes of tensors that are flowing around in the model.",
            "Which are not necessarily known at compile time in our case."
        ],
        [
            "OK, so why don't we take a break now?",
            "What seemed like a plan an?",
            "Will come back in half an hour though right?",
            "So a little bit before.",
            "Are there questions before we take a break?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you and thank you for having me.",
                    "label": 0
                },
                {
                    "sent": "So as usual said, I'm going to talk to you about sort of some of the research work that the drain team has been doing and with a bit of emphasis on the software tools that we've put together to kind of enable that research, and I'd like to point out this is work represented of many, many people here.",
                    "label": 0
                },
                {
                    "sent": "There's not only my work clothes, literally hundreds of people at Google have been involved in various pieces of the work that I'll show.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So by a bit of background, the brain project that started about five years ago and we had this hypothesis that time was sort of ripe for neural Nets, particularly if we could apply large amounts of computation and praying very large powerful models on large datasets that were starting to be available and just kind of understand what was possible initially, mostly in the field of perception, and gradually we've migrated to language understanding and other kinds of.",
                    "label": 0
                },
                {
                    "sent": "Tasks as you'll see, but initially we were focused on speech and vision tasks.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'm going to cover kind of some of our experience from the last five years, both on the research side in terms of the research work we've done in various different kinds of of various both sort of pure machine learning, research and also research in various domains.",
                    "label": 1
                },
                {
                    "sent": "Like speech and images and robotics, and then also I'll talk about some of the production aspects of actually getting this kind of work deployed on real Google products, because deploying these kinds of models which are computationally intensive at scale where you have many 10s of thousands of search requests per second and you need to run many neural net inferences for every one of those that's somewhat challenging and offers different kinds of systems challenges than training robots.",
                    "label": 0
                },
                {
                    "sent": "The focus of the talk will be mostly on what we've been doing in the area of neural Nets, but a lot of the software that we've built is actually.",
                    "label": 1
                },
                {
                    "sent": "More generally, you can use it for all kinds of other machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "You can actually use it Tensorflow for sort of general purpose numerical computations.",
                    "label": 0
                },
                {
                    "sent": "We've seen so much interest outside of Google in using it for what you might traditionally think of as high performance computing.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fications So 1 interesting thing about the brain team is we kind of bring together people with a mix of computer systems and machine learning, research expertise and kind of put them to work on interesting machine learning research problems and building software and distributed systems and hardware systems that really make sense in the context of accelerating that kind of research.",
                    "label": 1
                },
                {
                    "sent": "So we have this kind of nice blend of different kinds of expertise that.",
                    "label": 0
                },
                {
                    "sent": "Collectively, we can do things that none of us could do individually because we don't have one person with all these different skills.",
                    "label": 0
                },
                {
                    "sent": "And so we do.",
                    "label": 1
                },
                {
                    "sent": "Pure machine learning, research and we also do research in the context of emerging machine learning application areas, particularly things like robotics and language understanding.",
                    "label": 0
                },
                {
                    "sent": "We're starting to do some work in the field of health care as well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we disseminate this work in lots of different ways.",
                    "label": 1
                },
                {
                    "sent": "We publish a fair number of papers.",
                    "label": 1
                },
                {
                    "sent": "Recently we've released Tensorflow as the core machine learning system that we've been using internally as an open source project, but I'll talk about as part of that.",
                    "label": 0
                },
                {
                    "sent": "We're starting to open source models associated with research papers that we published so that people can reproduce our work and build on it more readily than just reading an English description of some paper.",
                    "label": 0
                },
                {
                    "sent": "And then we collaborate with product teams at Google to get our research.",
                    "label": 1
                },
                {
                    "sent": "Into real products.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what do we actually want?",
                    "label": 1
                },
                {
                    "sent": "In terms of machine learning research, we want to be able to build really intelligent systems that can learn from experience and that experience can be sort of static datasets that exist on large storage systems and we just want to learn from the data that are there.",
                    "label": 0
                },
                {
                    "sent": "It can mean agents operating in some simulated or real environment and collecting experience as they go, and then we want to use those intelligent systems to actually solve problems that people have in the world and so.",
                    "label": 0
                },
                {
                    "sent": "Touch on some of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what do I mean by understanding?",
                    "label": 1
                },
                {
                    "sent": "Well, if you think about what humans are able to do looking at raw forms of data like this.",
                    "label": 0
                },
                {
                    "sent": "If you look at this image, none of you would have any problem sort of telling me a descriptive sentence about what that image represents similarly.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You don't have any trouble finding all the text in this image.",
                    "label": 0
                },
                {
                    "sent": "'cause assuming you can read an identifying what kinds of stores those are from the context.",
                    "label": 0
                },
                {
                    "sent": "But computers traditionally been pretty bad at this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you don't have trouble understanding what I'm saying, hopefully, but computers are traditionally not been that great at that.",
                    "label": 0
                },
                {
                    "sent": "They have much higher word error rates than people listening to other other people speaking, and particularly with more mobile devices having really good speech recognition is actually quite important for building the kinds of experiences we want to build in terms in terms of how those devices get used.",
                    "label": 0
                },
                {
                    "sent": "'cause typing is a pain.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Getting back to one of Google's initial product search.",
                    "label": 0
                },
                {
                    "sent": "Traditional information retrieval techniques basically take the words in the query.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then they kind of look roughly for matching words in documents an.",
                    "label": 0
                },
                {
                    "sent": "Consider things like proximity of where those words occur and so on.",
                    "label": 0
                },
                {
                    "sent": "But really, as a human you have a much higher level of understanding of the language of documents and queries, and you know from reading these two snippets of documents that the second document is much closer match for what the user probably wants than the first one, even though the first one has all these highlighted red terms that seem to match the terms in the query.",
                    "label": 0
                },
                {
                    "sent": "So knowing at a high level.",
                    "label": 0
                },
                {
                    "sent": "But this is a good match for that document.",
                    "label": 0
                },
                {
                    "sent": "The second document is much better match for the query is another thing we really want to get out of language understanding systems.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is just some example needs of the future that we would like to be able to solve.",
                    "label": 1
                },
                {
                    "sent": "The first few we can basically solve today the first 2 and then they get kind of progressively harder I think, but these are sort of the high level.",
                    "label": 0
                },
                {
                    "sent": "This was taken from a talk I gave at a database conference where people were wondering what the future of database systems should look like and I sort of was trying to motivate them to say OK, you have all this data, but really what you want is actually understand the data at a level that you can answer these kinds of queries.",
                    "label": 0
                },
                {
                    "sent": "Not that you can select this.",
                    "label": 0
                },
                {
                    "sent": "Column and where the other values bigger than 20 years?",
                    "label": 0
                },
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "I think these are kind of the aspirational things that we need to work towards in solving.",
                    "label": 0
                },
                {
                    "sent": "I particularly like robot touch me a Cup of tea from the kitchen.",
                    "label": 1
                },
                {
                    "sent": "Battle that will require solving lots of interesting problems.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "When the project started, there was basically not much use of deep learning at Google.",
                    "label": 1
                },
                {
                    "sent": "About five years ago, an R group started and started dabbling with on sort of unsupervised learning research and also worked with the speech recognition team on speech recognition and actually get deployed.",
                    "label": 0
                },
                {
                    "sent": "The first production use of neural Nets at Google in the context of our speech recognition system and overtime, we started working with a few more kind of computer vision problems and other teams would hear about the success that some team had.",
                    "label": 0
                },
                {
                    "sent": "In solving some machine learning problem and one of the really nice properties that deep learning has is very widely applicable to all kinds of different problems, and so you see the breadth of different kinds of areas that people have applied this to, and the growing use of this within Google.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the number of unique directories of in our large shared source code repository that have configuration files for our two different machine learning systems checked in and that graph continues to go up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will, as part of this talk, discuss Tensorflow, an show real example of how we've used Tensorflow for different purposes and also explain what's happening underneath the covers.",
                    "label": 0
                },
                {
                    "sent": "So if you ever use Tensorflow yourselves, you can kind of have a better mental model of what the system is actually doing under the underneath covers.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've actually built two separate systems in the course of doing our machine learning research.",
                    "label": 0
                },
                {
                    "sent": "The first was a system called this belief that we built and published a paper about it NIPS 2012 and had it had the properties that it was very scalable, so we ran some experiments as part of a different NIPS paper by CL paper on 16,000 CPU cores to train a single model, so it had the nice property that you could actually throw lots of.",
                    "label": 1
                },
                {
                    "sent": "Lots of data at this problem and have fairly computationally intensive models, but it wasn't super flexible for research.",
                    "label": 1
                },
                {
                    "sent": "It was very good at actually being sort of production ready, so you could take a model you've trained and push it out in their production, but it wasn't that flexible, like if you wanted to do something other than a feedforward net training.",
                    "label": 0
                },
                {
                    "sent": "It was kind of kind of annoying and hard to specify if it didn't fit the programming model, so our second system sort of based on what we learned actually keeps what we think are the good properties of the first system, which is that it's scalable and good for production use.",
                    "label": 1
                },
                {
                    "sent": "But generalizes things and simplifies it a lot actually, and makes it much more flexible for a variety of different kinds of research purposes.",
                    "label": 0
                },
                {
                    "sent": "It also enhances the portability so we can actually run Tensorflow models on lots of different kinds of devices, as you'll see an we open sourced it with a fairly.",
                    "label": 0
                },
                {
                    "sent": "Freewheeling license so you can kind of take it and do whatever you want with it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one property of neural Nets is that the results tend to get better if you're able to train on more data.",
                    "label": 1
                },
                {
                    "sent": "But when you do that, you also often need to train that system on using a bigger, more powerful model, either more capacity in the model or more computational complexity somehow in order to sort of by giving it more data, but not increasing the size of the model.",
                    "label": 0
                },
                {
                    "sent": "At some point you saturate the amount of information that model can learn, and you really need to do both things, scale the model up and scale the data up, and then scale the model up so that you can pick up on the more subtle patterns that occur only rarely, but are now common enough.",
                    "label": 0
                },
                {
                    "sent": "Across the larger data set that you actually see them and the model can generalize from, and the combination of those two things is actually quite painful from a computational perspective.",
                    "label": 0
                },
                {
                    "sent": "'cause it's almost like the product of these two factors.",
                    "label": 0
                },
                {
                    "sent": "That means that defines how much computation you need in order to actually get the model to train.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is taken from a paper published in 2015, but you can see similar kinds of trends in lots of different machine learning papers.",
                    "label": 0
                },
                {
                    "sent": "This is comparing language modeling task.",
                    "label": 0
                },
                {
                    "sent": "Looking at how the perplexity changes as you increase the size of the training set.",
                    "label": 0
                },
                {
                    "sent": "So this first graph is billions of words and you can see that perplexity generally goes down for lots of different kinds of language models.",
                    "label": 0
                },
                {
                    "sent": "Is a traditional ngram model, and this is.",
                    "label": 0
                },
                {
                    "sent": "An RN with 2048 units.",
                    "label": 0
                },
                {
                    "sent": "And you can see that making large having a larger data set actually helps you Fairmount.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if you hold the size of the data set fixed at 1 billion words, but increase the size of the model from 128 units here to 8192, here, you actually get substantially increases decreases in perplexity as well.",
                    "label": 0
                },
                {
                    "sent": "And if you do both those things, presumably it's going to be even better.",
                    "label": 0
                },
                {
                    "sent": "They didn't actually do that experiment in this paper.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this combination works really really well, but it poses pretty interesting systems problems.",
                    "label": 1
                },
                {
                    "sent": "In particular, you need lots of computation in order to do this, and you also want to train and do experiments quickly, so the most efficient thing to do if you didn't care about training time, would be to do these computations on, say, a single GPU card, 'cause then you don't have any communication overhead or.",
                    "label": 1
                },
                {
                    "sent": "Mutation bottlenecks, but in order to actually get the system to train in reasonable amounts of time, you actually have to generally go beyond that.",
                    "label": 0
                },
                {
                    "sent": "The barrier of what fits on a single device for a single machine and use lots and lots of machines to train models quickly.",
                    "label": 0
                },
                {
                    "sent": "A sort of related idea is the latency at which you can perform an experiment really influences the kind of research you can do if your experiments take a matter of a few hours.",
                    "label": 0
                },
                {
                    "sent": "That's a very different feeling than than if they take a few weeks.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to skip.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just talk about some of the initial.",
                    "label": 0
                },
                {
                    "sent": "The first thing that we that we worked on with our speech team was just replacing the acoustic model that they used in the speech recognition system with a fairly deep feedforward neural net that tried to predict which phoneme was being uttered in the middle.",
                    "label": 0
                },
                {
                    "sent": "10 milliseconds of this this window.",
                    "label": 0
                },
                {
                    "sent": "And that actually dramatically improved the word error rate in the speech recognition system.",
                    "label": 0
                },
                {
                    "sent": "It actually required fairly intensive training, so we were using several 100 machines to train this model for several days.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Several hundred I think it was maybe 400 or 500.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "This was before we had GPS in our data center.",
                    "label": 0
                },
                {
                    "sent": "So one way you can compensate for not having GPS deployed yet is used more CPUs.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overtime the speech team has sort of continued to evolve the kinds of models and they've been getting more complicated, so this is a model that they published last year that has some convolution at the bottom and some LCM layers at the top, and then some fully connected layers.",
                    "label": 0
                },
                {
                    "sent": "Sorry LM layers in the middle and fully connected layers in the top.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Not that this is exactly the end all be all model, but it's an example of something where you want flexibility to try lots of different kinds of models and understand what works, what doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the more recent trend has been to use sequence models of various kinds.",
                    "label": 0
                },
                {
                    "sent": "Recurrent LCMS, typically with attention that.",
                    "label": 0
                },
                {
                    "sent": "Can model the entire thing.",
                    "label": 0
                },
                {
                    "sent": "So rather than separating the speech problem into an acoustic model where you try to predict parts of words and then you have a language model that takes parts of work, sequence of parts of words and stitches them together into.",
                    "label": 0
                },
                {
                    "sent": "The actual phrase that the user uttered it's actually more efficient and allows you to optimize end to end to just train the whole thing jointly an allow you to have the model adapt to whatever makes sense and not necessarily have this artificial barrier of phonemes inserted in the middle of the speech pipeline.",
                    "label": 0
                },
                {
                    "sent": "So our group and the speech group have continued to push on this this idea.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similarly, in the image recognition field, this was kind of a Seminole paper that really showed that.",
                    "label": 0
                },
                {
                    "sent": "Deep convolutional Nets could be used for image recognition, and you know it's a somewhat complicated thing.",
                    "label": 1
                },
                {
                    "sent": "It's got multiple parallel towers.",
                    "label": 0
                },
                {
                    "sent": "Alex Chesky actually sort of hand paralyzed this computation across multiple GPU's to get more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Computation.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then overtime, some Google researchers actually developed a little module with a more complex pattern of convolutions of different sizes and an sequences of convolutions, and then that basic module.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gets replicated a whole bunch of times in a very fairly deep model.",
                    "label": 1
                },
                {
                    "sent": "I think this one's like 26 or 30 layers or something, but actually get substantially better results.",
                    "label": 0
                },
                {
                    "sent": "Where each one of these boxes is maybe 10,000 neurons represents a fair amount of computation.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that was published there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can see why people are excited about neural Nets for image recognition.",
                    "label": 0
                },
                {
                    "sent": "So before people tried them in image net, the error rate was hovering around 2627% for the few years that they held the contest before then.",
                    "label": 0
                },
                {
                    "sent": "And then Alex, an alien Jeffrey, came along an really dramatically drop that error rate and the next year my summer intern at dealers started a company and won the next year.",
                    "label": 0
                },
                {
                    "sent": "And then there's been a just a progression of.",
                    "label": 0
                },
                {
                    "sent": "Improvements since then.",
                    "label": 0
                },
                {
                    "sent": "I particularly like this this result, which is Andre Karpathy, who was helped administer the image.",
                    "label": 0
                },
                {
                    "sent": "Net contest as a PhD student at Stanford was really actually wondering what human error rate on this task would be an.",
                    "label": 0
                },
                {
                    "sent": "So he tried to convince all his lab mates to actually subject themselves to test, but pretty much only he did a good, serviceable job of it.",
                    "label": 0
                },
                {
                    "sent": "So he did 100 hours of training on a training set of images and then then set those aside and then subjected himself to the test set and.",
                    "label": 0
                },
                {
                    "sent": "And he got five point 1% error.",
                    "label": 0
                },
                {
                    "sent": "An another lab mate of his that only did like 10 hours of training and then gave up got 12% error.",
                    "label": 0
                },
                {
                    "sent": "Image that is actually pretty hard 'cause it has like 40 kinds of dogs that you need to discern and.",
                    "label": 0
                },
                {
                    "sent": "So another interesting trend to notice is that the number of parameters in these models is actually been going down generally, or it went down and then it went up a little bit, and that's actually important for some kinds of applications because the number of parameters really affects the size of the model that you're going to deploy on a same mobile phone and.",
                    "label": 0
                },
                {
                    "sent": "So those are actually a pretty nice for that.",
                    "label": 0
                },
                {
                    "sent": "From that respect, it also makes it harder to overfit in a lot of cases.",
                    "label": 0
                },
                {
                    "sent": "But anyway, that's why computer vision is working so well.",
                    "label": 0
                },
                {
                    "sent": "'cause the error rates gone from 26 to 3%.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what kinds of properties do you actually want in a machine learning system?",
                    "label": 0
                },
                {
                    "sent": "So one is you want this ease of expression that I've been talking about where you want to be able to take any kind of machine learning idea.",
                    "label": 0
                },
                {
                    "sent": "Combine it with a reinforcement learning system that might interact with an agent and be able to express all kinds of exotic models.",
                    "label": 0
                },
                {
                    "sent": "In a way that's easy so that you can try them out without too much effort.",
                    "label": 0
                },
                {
                    "sent": "And then you want the system to be scalable so that you can take that model you've expressed and try it on not just small datasets, but on sort of realistic datasets.",
                    "label": 0
                },
                {
                    "sent": "For real world tasks like speech and vision and language.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Depending on your needs, you know if you're a researcher you may not care much about portability.",
                    "label": 0
                },
                {
                    "sent": "You may just want to get the answers to your experimental questions and then go on to the next thing next experiment.",
                    "label": 0
                },
                {
                    "sent": "But if you're going to deploy these things in production systems often you do care about portability where you want to be able to train the models on a cluster of machines or on a desktop machine with a bunch of GPU's in it.",
                    "label": 0
                },
                {
                    "sent": "But then maybe diploid, the model on mobile mobile phone.",
                    "label": 0
                },
                {
                    "sent": "And another thing you care about is reproducibility.",
                    "label": 0
                },
                {
                    "sent": "So you want it to be easy to share results and to reproduce other peoples results in a way that actually you have confidence that you actually reproduce them correctly.",
                    "label": 0
                },
                {
                    "sent": "So being able to share actual working implementations of the research that people have done is pretty important, rather than 'cause often you'll read a paper and it will say, well, we initialized with a low learning rate and you're like, well, I have no idea what that actually means in practice.",
                    "label": 0
                },
                {
                    "sent": "And then for some kinds of applications, if you actually want to play this in real products, you want the system to be able to take that research idea and then express it in a way that can go into a real product without reimplementing it in a different kind of system.",
                    "label": 0
                },
                {
                    "sent": "By the way, I'll take questions anytime people have them so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so tensor flow is kind of the second system that we came up with, and I'm going to talk to you about some of the attributes it has.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I did say this would work well with a lot of people, so this was the white paper we put out with the initial Tensorflow release.",
                    "label": 0
                },
                {
                    "sent": "For kind of describes some of the inner work systems aspects of tensor flow also describes the visualization system that it has a bit and some other, and the programming model a bit we act.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have taken that an adapter that content with more of the systems focus and there will be a paper that appears it's going to appear in this year's OSD.",
                    "label": 0
                },
                {
                    "sent": "We just found out last Friday that it was accepted, so that's good.",
                    "label": 0
                },
                {
                    "sent": "That's more about the systems aspects of.",
                    "label": 0
                },
                {
                    "sent": "The machine learning system.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we've seen pretty pretty good adoption of tensor flow.",
                    "label": 0
                },
                {
                    "sent": "So we had about 50,000 binary installs in 72 hours.",
                    "label": 0
                },
                {
                    "sent": "After we released it in about half a million since November.",
                    "label": 0
                },
                {
                    "sent": "Anne, it's unclear if GitHub stars in Forks or great measure of engagement, but it is 1 measure an by those metrics for doing reasonably well.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ones that I do like is that we were the most forked new repository in GitHub in 2015 despite only being available in November.",
                    "label": 0
                },
                {
                    "sent": "So I guess that's good.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's sufficient that Bloomberg has taken notice of deep learning, open source packages, and they published the article a week and a half ago discussing how many stars different different open source packages have.",
                    "label": 0
                },
                {
                    "sent": "So the blue line is tensor flow, and there's a bunch of other ones represented there.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know what it means.",
                    "label": 0
                },
                {
                    "sent": "the Bloomberg is writing about deep learning packages, but.",
                    "label": 0
                },
                {
                    "sent": "Seems interesting.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the things that I think went reasonably well with the release was that we put together some tutorials that describe how to use tensor flow to implement various kinds of interesting machine learning models, and people really like those tutorials 'cause they talked both about kind of what is happening from the mathematical machine learning perspective, but also what's happening in the expression of that algorithm or system in Tensorflow.",
                    "label": 0
                },
                {
                    "sent": "And so we have things like convolutional neural net model for images vector.",
                    "label": 0
                },
                {
                    "sent": "Like a word, two VEC implementation, Simple RNN, a sequence of sequence model for translation and people can kind of work through those an understand what's going on at a reasonable level.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, the motivation for tensor flow was really that it wasn't as flexible as we are.",
                    "label": 0
                },
                {
                    "sent": "First system wasn't as flexible as we wanted, and based on that experience we actually could simplify a lot of things that I'll show you in a minute, particularly around how parameters are managed in the machine learning system.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Also, make it more flexible.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the core bits of Tensorflow are basically this graph execution engine, so the computational graph model of computation is you express a computational graph, and then this graph execution engine is responsible for executing that and it's.",
                    "label": 0
                },
                {
                    "sent": "Responsible for dealing with different kinds of devices.",
                    "label": 0
                },
                {
                    "sent": "It has different implementations for different kinds of.",
                    "label": 0
                },
                {
                    "sent": "Operating systems and platforms.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's different frontends for specifying and driving this computation, so.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the most fully developed ones today are Python And C plus most people use the Python front end 'cause it's sort of the most fully developed.",
                    "label": 0
                },
                {
                    "sent": "That's what all of our researchers use.",
                    "label": 0
                },
                {
                    "sent": "And we've actually gotten some pull requests from external contributors to add other language support, so there's one to add Java I think, and there's one to add.",
                    "label": 0
                },
                {
                    "sent": "Go support ironically from Facebook, which is kind of.",
                    "label": 0
                },
                {
                    "sent": "Maybe we should've gotten to that first, but we didn't.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the basic computation model is a dataflow graph, where the nodes are different kinds of operations on tense.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's an the things that flow along edges in this system are tensors, not me, like actual mathematical tensor sense, but in the end dimensional array, kind of.",
                    "label": 0
                },
                {
                    "sent": "Caveman's interpretation of a tensor.",
                    "label": 0
                },
                {
                    "sent": "And the tensors have types.",
                    "label": 0
                },
                {
                    "sent": "You can have like a 3 dimensional tensor of floats or two dimensional matrix events.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you essentially build your graph.",
                    "label": 0
                },
                {
                    "sent": "So for example, you can read the data set and then you can create some placeholders which I'll talk about in a minute and some variables that are going to parameters and then you can say I'd like to do a matrix multiply of these two things and add a bias term and do a softmax, and that's going to be the output in my model.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And unlike pure dataflow graphs, we've actually augmented the dataflow graphs in Tensorflow with notion of variables.",
                    "label": 1
                },
                {
                    "sent": "An operations update variables.",
                    "label": 0
                },
                {
                    "sent": "So that's basically how you have state in the system for holding parameters.",
                    "label": 0
                },
                {
                    "sent": "And so, for example, here we have biases, which is a tensor and it flows into this computer computational graph and eventually we compute some update.",
                    "label": 1
                },
                {
                    "sent": "We want to make Tobias operate variable and we do a minus equals operation on it and that allows us to update the state there.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taking a page from Theano, one thing that people really like about the on our what is the symbolic differentiation support so you don't have to write your own derivatives.",
                    "label": 0
                },
                {
                    "sent": "The system just has derivation.",
                    "label": 0
                },
                {
                    "sent": "Like differentiation built in and so you can say, I'd like to minimize my model with respect to this cross entropy loss and it sort of does the right computation and I'll show you in later on what that actually reflects two in the computational graph.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then typically the way that people use this is they.",
                    "label": 0
                },
                {
                    "sent": "Launch the graph and run the graph multiple times, once for each mini batch in deep learning application.",
                    "label": 0
                },
                {
                    "sent": "And so here you might say, OK, I'm going to get from my training data.",
                    "label": 0
                },
                {
                    "sent": "I'm going to get the bat, the inputs and the outputs.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to run training step where I'm going to feed in the batch of examples I read an feed in the labels that correspond to the elements in that batch, and then the system will go ahead and run that computation.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this abstract model of a graph actually can't directly be executed 'cause we need to actually place where different pieces of this computation are going to be performed.",
                    "label": 0
                },
                {
                    "sent": "If we have multiple devices in our system.",
                    "label": 0
                },
                {
                    "sent": "And so a very simple setup is you often have a CPU and GPU card in your machine and you want to be able to execute this graph, and perhaps some of the operations maybe don't have GPU implementations, so maybe one of in an image model.",
                    "label": 0
                },
                {
                    "sent": "For example, there might be an operation that decodes a JPEG encoded image into the unpacked.",
                    "label": 0
                },
                {
                    "sent": "3 dimensional red, green, blue height and width tensor of the actual image image data, and that maybe that doesn't have a GPU imitation and it only runs on CPU's, so the system is responsible for understanding where different operations can be placed and to try to place them in a way that is most efficient for.",
                    "label": 0
                },
                {
                    "sent": "Executing the graph as quickly as possible.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so it might decide to place things here.",
                    "label": 0
                },
                {
                    "sent": "And when it does that, the system transparently inserts these things called send and receive nodes that are responsible for managing all the communication across device boundaries.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Essentially it's a pull model, so you start executing your receive node and the receive node.",
                    "label": 0
                },
                {
                    "sent": "Kind of implicitly knows which send node on this other device to contact and says, please give me, you know the biases variable.",
                    "label": 0
                },
                {
                    "sent": "I'm waiting for an if it's available already, great.",
                    "label": 0
                },
                {
                    "sent": "If not, then that's receive operation will kind of hang out until that data is available and then transferred, and whatever way is appropriate for the two different device device pair that is involved.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so there's kind of the rewritten graph that you never see as a user.",
                    "label": 0
                },
                {
                    "sent": "But actually kind of is what it looks like.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Covers.",
                    "label": 0
                },
                {
                    "sent": "And one really nice property about this is that by having just send and receive nodes, be responsible for transferring data across device boundaries.",
                    "label": 0
                },
                {
                    "sent": "You encapsulate all the communication in the system into just the send and receive node implementations and you can have different implementations for the different kinds of devices involved.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have two GPU cards on the same machine, you can actually do a GPU to GPU memory copy without involving the CPU other than to start the copy.",
                    "label": 0
                },
                {
                    "sent": "If you have CPUs on different machines, you can do across machine remote procedure call and if you have GPU's on different machines on your network supports it, you can do our DMA sort of remote direct memory access that allows one GPU on one machine to reach into the memory of a GPU card in another machine and pull out.",
                    "label": 0
                },
                {
                    "sent": "You know this particular tensor directly without involving the host CPU or having to go through the.",
                    "label": 0
                },
                {
                    "sent": "Per host, yeah, gotta question.",
                    "label": 0
                },
                {
                    "sent": "So we don't have an MPI implementation, but essentially, if you wanted to have MPI support, you would add an MPI implementation of the send and receive goods, and that's relatively easy to do.",
                    "label": 0
                },
                {
                    "sent": "You can have different implementations of these these nodes.",
                    "label": 0
                },
                {
                    "sent": "We do, for example, within Google there's one RPC subsystem that we use that has some extra monitoring and then a variant of that is what's open sourced as the G RPC open source subsystem, but they're slightly different, so we have two implementations of the RPC subsystem that we can swap in and out.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The we've been sort of continuously working on improving the tensor flow system, as have a bunch of external contributors.",
                    "label": 0
                },
                {
                    "sent": "So in November that was the initial release.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in December, you know one of the things that we heard was that people really wanted Python 3.3 support.",
                    "label": 0
                },
                {
                    "sent": "There were a bunch of improvements made to GPU performance or performance out of the gate was actually a little little sad.",
                    "label": 0
                },
                {
                    "sent": "We've been measuring it with our own GPU compiler toolchain inside Google, so we weren't using MVC.",
                    "label": 0
                },
                {
                    "sent": "We're using jakuta CC, which is a variant we've built internally for compiling CUDA code, and it turned out our internal compiler dealt much better with 64 bit tensor indices.",
                    "label": 0
                },
                {
                    "sent": "And so the MVC C compiler actually was 20 or 30% slower for every every time you indexed into a tensor, which in Tensorflow you do fairly often.",
                    "label": 0
                },
                {
                    "sent": "So we fixed that by moving to 32 bit indices in a lot of cases.",
                    "label": 0
                },
                {
                    "sent": "To fix some of the performance issues.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You know in February we did another release and one of the things that I think is important to understand about tensor flow is the lowest level representation of the graph has very sort of low level fine grained operations, and that's great for researchers who want to control exactly what kinds of operations are being performed.",
                    "label": 0
                },
                {
                    "sent": "But for some people they want a much higher level abstract view of neural net.",
                    "label": 0
                },
                {
                    "sent": "They want to say I want you know, five layers of this web.",
                    "label": 0
                },
                {
                    "sent": "And they want that to be concise and easy to express, and they don't necessarily want very fine control over every last aspect of how those things work, and so we think what will likely happen is there will be a whole bunch of different higher level APIs that people use to specify Tensorflow graphs.",
                    "label": 0
                },
                {
                    "sent": "And you can also use kind of the native raw data flow operations, graphs and so we have one that we added that has sort of some higher level support.",
                    "label": 0
                },
                {
                    "sent": "There is also curious which is a popular open source packages that have been ported to.",
                    "label": 0
                },
                {
                    "sent": "Also be able to use Tensorflow back end.",
                    "label": 0
                },
                {
                    "sent": "Some people like that as a way of specifying models, so I think it's not like we will necessarily standardize on one way of specifying computations, But what we hope is that many of those map down on Tensorflow graphs and the optimizations we're doing and will improve on will benefit all those higher level ways of specifying these different system.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our April release added a distributed runtime, so our initial open source release didn't have a distributed runtime and that was one of the things that people really wanted.",
                    "label": 0
                },
                {
                    "sent": "And the reason it didn't have it, we just hadn't yet implemented the G RPC transport subsystem for sending receiving nodes.",
                    "label": 0
                },
                {
                    "sent": "And so we implemented that and got that to work with Kubernetes containers as the basis for running many different processes in a distributed system and communicating between them.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then more recent release, we've added iOS support and GPU support on Mac OS and FP16 support, which is going to be important with upcoming upcoming Pascal cards from NVIDIA for example.",
                    "label": 0
                },
                {
                    "sent": "Anyway, the main point of that is just that there's been pretty steady improvement in the kinds of things the system.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can do this is a graph of activity to the GitHub repository.",
                    "label": 0
                },
                {
                    "sent": "You know, since November we've had about 6000 commits to various changes.",
                    "label": 0
                },
                {
                    "sent": "Which I think is.",
                    "label": 0
                },
                {
                    "sent": "Like 20 or 30 today or something like that.",
                    "label": 0
                },
                {
                    "sent": "And actually, the majority of contributors are now outside Google, although the heaviest contributors are are within Google.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our colleagues at DeepMind were mostly doing a lot of their research and torch, but they've decided to move to Tensorflow to standardize sort of in Google what we use for sort of our deep learning research and also production deployment of deep learning systems.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the things that we do is also release pre trained models in Tensorflow where we release not just the model description but also the parameters.",
                    "label": 0
                },
                {
                    "sent": "So this is the state of the art inception model that we released.",
                    "label": 0
                },
                {
                    "sent": "The pre trained parameters so we can use that in various ways to do diff.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things, and there's a tutorial that shows people how to take that model an run, run it on their platform.",
                    "label": 0
                },
                {
                    "sent": "There's also another tutorial that shows people how to use those pre trained that pre trained model to train then use it as pre training for a different image classification task where you might have a small amount of image data on your own.",
                    "label": 0
                },
                {
                    "sent": "And that's Grace Hopper.",
                    "label": 0
                },
                {
                    "sent": "Although we identify it as military uniform.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so one of the.",
                    "label": 0
                },
                {
                    "sent": "The main motivations for doing all this computer systems work for machine learning is really to make experimental turn around time much lower than it would be if we're using sort of non distributed approaches.",
                    "label": 0
                },
                {
                    "sent": "And it's really just a very fundamentally different feeling.",
                    "label": 0
                },
                {
                    "sent": "Doing research when the experiments take, you know you can do multiple experiments per day, let's say then if it's something where you have to launch it and then wait several days or even.",
                    "label": 0
                },
                {
                    "sent": "Several weeks and then you get your results and then you're like, oh, why was I running this in the 1st place?",
                    "label": 0
                },
                {
                    "sent": "I kind of forgotten at this point, so that really is a big focus of us is to make that turn around time as low as we can.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one of the ways we do that is with large amounts of raw computing hardware.",
                    "label": 0
                },
                {
                    "sent": "But you also need to make it easy for people to use those computers.",
                    "label": 0
                },
                {
                    "sent": "You can't just sort of throw raw computing hardware.",
                    "label": 0
                },
                {
                    "sent": "And expect every machine learning researcher to also be a distributed systems expert and know how to use a rack of GPU's in raw form and so Tensorflow is this system that is fairly abstract in how you specify the model, but then the system will have map that down to a bunch of hardware that you say, OK, I have the ability to use 32 GPU GPU cards for this experiment.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the ways you can get faster training is through data parallelism.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is that you want to use multiple model replicas that all kind of collaborate to update the shared set of parameters in the model.",
                    "label": 0
                },
                {
                    "sent": "And but they're all going to be training on different data.",
                    "label": 0
                },
                {
                    "sent": "The speedups that you can get here depends pretty highly on different.",
                    "label": 0
                },
                {
                    "sent": "What kind of model?",
                    "label": 0
                },
                {
                    "sent": "So for dense models, we regularly see kind of 30 or 40 X speedup using 50 replicas, so it's not quite linear speedup, but it's quite good 'cause it means.",
                    "label": 0
                },
                {
                    "sent": "With 50 replicas, you move the time from a month down to a day for an experiment, and that's pretty compelling for sparse models where you have like a bunch of embeddings for vocabulary items and a given example only touches maybe a handful out of millions of embeddings.",
                    "label": 0
                },
                {
                    "sent": "You can support many more replicas, often in those kinds of environments, so we regularly train models with 1000 replicas for some kinds of applications that have these very large sparse embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Better in that case.",
                    "label": 0
                },
                {
                    "sent": "It's just you get less interference with the gradients from the multiple replicas.",
                    "label": 1
                },
                {
                    "sent": "So if you're only touching 5 words in the vocabulary, the odds that another mini batch on some other machine has touched those same words is pretty low.",
                    "label": 0
                },
                {
                    "sent": "So you still have interference for the shared part of the model, but that tends to be updated.",
                    "label": 0
                },
                {
                    "sent": "You know the change in those shared parameters tends to be slower.",
                    "label": 0
                },
                {
                    "sent": "And so you end up just being able to support more replicas before you kind of fall off a Cliff.",
                    "label": 1
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the way this works, this is kind of a diagram from our earlier system.",
                    "label": 0
                },
                {
                    "sent": "This belief is we had this set of machines called parameter servers that are going to be responsible for maintaining the current set of parameters in the model, and then we're going to model replicas, which themselves might be multiple machines or multiple.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Devices.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Be forgiven model replica actually processes a mini batch.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's going to download the current set of parameters from the parameter servers and those parameters might come from 100 different parameter servers where the parameters are distributed evenly across those, so that you get more network bandwidth into the parameter servers than if you just store them on a single machine.",
                    "label": 0
                },
                {
                    "sent": "And then the model replica.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll read mini batch of examples from disk will do the computation necessary to computer gradient and then instead of applying that gradient locally, it will go ahead and.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Send the gradient to the parameter servers in a distributed manner, so they might be again spread over 10 or 100 machines and then that parameter servers.",
                    "label": 0
                },
                {
                    "sent": "The parameter service will actually be responsible for updating the parameters and so that's one round trip through a mini batch is you now download the parameters and send gradients and they get applied in the centralized service and then before the next example you do the same thing.",
                    "label": 0
                },
                {
                    "sent": "So now we have prime.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And change to be prime.",
                    "label": 0
                },
                {
                    "sent": "We get P double prime.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And really, all of these machines.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They're going to be doing this at the same time.",
                    "label": 0
                },
                {
                    "sent": "And there's a few variations of how you can orchestrate this kind of parallelism.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in disbelief, we actually had these Peru.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better servers are actually completely separate software subsystem and the communication mechanism used here was not the same as the communication mechanism used, say between different machines in the model itself, which actually caused kind of a bunch of complexity.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It also meant that you only could select from a handful of parameter update rules.",
                    "label": 1
                },
                {
                    "sent": "You could didn't have the full generality of the programming language for updating parameters.",
                    "label": 0
                },
                {
                    "sent": "You could say, OK, I want to do.",
                    "label": 0
                },
                {
                    "sent": "Apply a standard learning rate update or a learning rate with momentum or a few things like that, but not much more general thing and so that ended up being much more complicated and so we dispense with that separate system in tensor flow.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So because the send and receive nodes sort of abstract away network communication in Tensorflow, we're actually able to model the parameter server just as other devices in the computational graph.",
                    "label": 0
                },
                {
                    "sent": "And so, for example, these things might.",
                    "label": 0
                },
                {
                    "sent": "You might say that this part of the model here is a parameter server device, but it's just part of the computation graph that you're trying to perform, and it happens to have send and receive nodes to get copies of the parameters onto other machines or other devices.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the two main choices you have for data parallelism are you can do this synchronously where you have like as.",
                    "label": 0
                },
                {
                    "sent": "All kind of processing a mini batch and essentially that's equivalent to a single machine processing and end times larger batch and for some kinds of problems that's OK, but often a large, much larger batch size means if you make your batch size 10 times as big, you don't learn 10 times as fast.",
                    "label": 0
                },
                {
                    "sent": "The other advantage of this approach is you don't get any gradient staleness.",
                    "label": 0
                },
                {
                    "sent": "So when you download the copy parameters, you synchronized everything so everyone gets the same copy of the parameters at the beginning of the step, they compute their own individual gradients and then you combine them somehow, either by averaging or adding the gradients from the individual machines.",
                    "label": 0
                },
                {
                    "sent": "And so there's no issue of applying gradients to stale copies of the parameters.",
                    "label": 0
                },
                {
                    "sent": "Another drawback, eh?",
                    "label": 0
                },
                {
                    "sent": "Drawback of this approach, though, is it's less fault tolerant, so if any one of the machines dies, you kind of have to do some recovery process to deal with that.",
                    "label": 0
                },
                {
                    "sent": "Whereas in the other approaches, to do this asynchronously, so if you have 10 replicas, they each kind of independently do steps as fast as they can.",
                    "label": 0
                },
                {
                    "sent": "Independent of what the other machines are doing, yeah.",
                    "label": 0
                },
                {
                    "sent": "I swear some machines will produce faster for some reason.",
                    "label": 0
                },
                {
                    "sent": "The synchronous case, more typically yes.",
                    "label": 0
                },
                {
                    "sent": "So in the asynchronous case, let's say 9 machines or zippy fast and attempt one has some other job running on it, so it's slow.",
                    "label": 0
                },
                {
                    "sent": "That doesn't hurt you as much in the asynchronous case, 'cause that one just will give you half the throughput, but everyone else kind of runs at full speed rise.",
                    "label": 0
                },
                {
                    "sent": "In the synchronous case that actually causes trouble.",
                    "label": 0
                },
                {
                    "sent": "I'll talk a minute about some mechanisms to alleviate this.",
                    "label": 0
                },
                {
                    "sent": "And you can also do some hybrid in between this with M groups of ends in cronous replicas.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in pictorial form, what this looks like is you might have this is the asynchronous training.",
                    "label": 0
                },
                {
                    "sent": "You might have three different threads driving 3 copies of the model, all sharing the parameter device, so they all get the parameters from the same place.",
                    "label": 0
                },
                {
                    "sent": "But then they do their own independent computation on a replica of the computation graph.",
                    "label": 0
                },
                {
                    "sent": "And these threads kind of don't interact at all other than saying OK, I'm done with my step.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start in the next step.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Whereas in a synchronous model, there's one controller for the entire process and the computational graph conceptually is sort of 3 * 3 replicas of that computation in one graph.",
                    "label": 1
                },
                {
                    "sent": "And you get the parameters you do the computations on the different mini batches and then you add the parameter gradients from the individual mini batches together to then compute what the full gradient is for the full batch, and then you update them.",
                    "label": 0
                },
                {
                    "sent": "Return to the other side, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these clients could either be different threads on the same process, or they could even be separate threads in separate processes.",
                    "label": 0
                },
                {
                    "sent": "As you scale, if you have hundreds or thousands of replicas then you probably want these to be separate processes themselves, because otherwise this thing can become a bottleneck and your pile of hardware here is bottleneck because this guy can't even say start fast enough.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of how the gradients get, parameters get updated every every square.",
                    "label": 0
                },
                {
                    "sent": "Here is an update to the parameters and so an asynchronous replication.",
                    "label": 1
                },
                {
                    "sent": "Those just pile in whenever they finish and in synchronous replication they piloting this orderly synchronized manner.",
                    "label": 0
                },
                {
                    "sent": "We can actually introduce backup workers, where in this case we're showing three workers, Anne, we take the first 2 that finish, so this is a technique we used in map reduce.",
                    "label": 0
                },
                {
                    "sent": "Which is a general computational framework of backup tasks where you essentially do additional computation sometimes, and take whichever one finishes 1st, and so here we're actually doing slightly different computations.",
                    "label": 0
                },
                {
                    "sent": "It's typically a different randomly selected mini batch.",
                    "label": 0
                },
                {
                    "sent": "And then you take whichever two of these smaller things finish first, and then combine those to compute the gradient and drop the last one.",
                    "label": 0
                },
                {
                    "sent": "And the real numbers are typically more like 100 replicas with five extra ones rather than two and three.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one factor in using data parallelism in this way is because you're sending the parameters of the model over over the network quite often, basically before every one of these, these mini batch steps you really want the model computation time to be large relative to the time it takes to send and receive the parameters over the network.",
                    "label": 0
                },
                {
                    "sent": "The models that have fewer parameters but use those parameters in many different floating point operations actually tend to work better in this regime, so convolutional models are very good in this regard because you essentially you reuse the convolutional parameters for a layer at every different spatial position, and so that you might reuse a given floating point number you've copied over the network as a parameter 10,000 or 100, or.",
                    "label": 0
                },
                {
                    "sent": "50,000 times.",
                    "label": 0
                },
                {
                    "sent": "Recurrent models also have this property, although not quite to the same extent because you're typically unrolling through time and you might unroll 20 or 100 steps, so you get to reuse that floating point value that you've brought in over the Network 20 or 100 times in that case.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So data parallelism actually is a really important tool that we use pretty heavily.",
                    "label": 0
                },
                {
                    "sent": "So for example, training are one of our ranking models for search, we use 500 replicas.",
                    "label": 0
                },
                {
                    "sent": "We use 50 GPU's to train some of our large image models for image.",
                    "label": 0
                },
                {
                    "sent": "For image net typically get maybe a 40X speedup.",
                    "label": 0
                },
                {
                    "sent": "We have a system called Smart Reply that uses 16 replicas, each with many different GPU's.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty pretty important for getting the turn around time low.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just to illustrate this in pictorial form.",
                    "label": 0
                },
                {
                    "sent": "So what we have here is a image.",
                    "label": 0
                },
                {
                    "sent": "This is an inception model trained on image net and what you see is the X axis is hours of training time in the Y axis is accuracy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at the difference between one GPU versus 50, we're getting 30 and a half X speedup with 50 GPU cards would be better if it was 50, but it's not, but it's clearly much better.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other thing we've been investigating a bit is.",
                    "label": 0
                },
                {
                    "sent": "Going back to using more synchronous training techniques just because the noise in the gradients.",
                    "label": 0
                },
                {
                    "sent": "That you get in the asynchronous regime tends to mean that you can't quite get to the highest level of accuracy because of that noise.",
                    "label": 0
                },
                {
                    "sent": "So this blue line here is an asynchronous training with 100 replicas.",
                    "label": 0
                },
                {
                    "sent": "Be aware that the Y axis is quite compressed, so it's .77 point 795, so these are fairly subtle differences.",
                    "label": 0
                },
                {
                    "sent": "But still, if you care about that last little bit of accuracy, you can see that the asynchronous training kind of tops out at like 785 here, and you get another.",
                    "label": 0
                },
                {
                    "sent": "Half a percent or almost a whole percent of accuracy in the synchronous training case because the noise in the gradients actually just means you bounce around a lot rather than actually descending to the bottom of the Valley like you want to.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "Even if you decrease the learning rate, an yeah that's our experience.",
                    "label": 0
                },
                {
                    "sent": "Or you have to decrease it so much that then the training time becomes sort of antenna.",
                    "label": 1
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if you look at this, if you compare several different comparisons are interesting here.",
                    "label": 0
                },
                {
                    "sent": "So one is asynchronous versus synchronous training and you see you get to the same level of accuracy for synchronous training in 40 hours instead of 52 with basically the same amount of hardware.",
                    "label": 0
                },
                {
                    "sent": "And, uh.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A good comparison is.",
                    "label": 0
                },
                {
                    "sent": "The synchronous of 100, but we also have five extra workers, so we're not using 15 replicas instead of 100, so it's 5% more hardware, but that got you a pretty big win.",
                    "label": 0
                },
                {
                    "sent": "'cause now instead of 40 hours is 30 hours.",
                    "label": 0
                },
                {
                    "sent": "Versus 52 hours.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we compare just the synchronous 100 versus the synchronous 105 extra workers.",
                    "label": 1
                },
                {
                    "sent": "You know you throw 5% more hardware at the problem, but you got a 25% reduction in training time.",
                    "label": 0
                },
                {
                    "sent": "So, and these are all relatively easy to express as tensor flow graphs in various ways, and there's an archive paper about these experiments essentially.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions about any of that, yeah.",
                    "label": 0
                },
                {
                    "sent": "Extra workers",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so in part it's because these machines are in Google's datacenters and there may be other tasks running on the machines sometimes.",
                    "label": 0
                },
                {
                    "sent": "Or there may be network traffic on the network that is interfering with some of the machines more than others, and So what that means is you have a fair amount of variability in how long it takes each machine to complete this step so you know the median might be, say, one second, but the 99 percentile might be 1.",
                    "label": 0
                },
                {
                    "sent": "4 seconds.",
                    "label": 0
                },
                {
                    "sent": "And if you're waiting for the last guy, but 99th percentile is what you actually care about.",
                    "label": 0
                },
                {
                    "sent": "'cause you have 100 guys and you have to wait for the last one and so basically by doing 105 and taking the first 100, you're able to bring it in so that what you care about now is the 95th percentile latency rather than the 99th.",
                    "label": 0
                },
                {
                    "sent": "And that just tends to be that in these large datacenters there's just a lot of stuff going on.",
                    "label": 0
                },
                {
                    "sent": "There's like background processes waking up on the machines every so often to compress logs, and there's extra traffic on the network, and the machines are overheating, so some of them are like slightly underclocked because of that, and this just makes it more robust to that sort of thing, and you can't sort of eliminate all those sources of variance very easily, so it's best to just build the system to be more tolerant of them.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "This is a dense models.",
                    "label": 0
                },
                {
                    "sent": "This is an image net model with convolutional kind of set of parameters.",
                    "label": 0
                },
                {
                    "sent": "So not super many parameters, but a lot of computation per step.",
                    "label": 0
                },
                {
                    "sent": "Has big differences in terms of accuracy or.",
                    "label": 1
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Probably not as much because the thing that's killing you here is the conflicting updates for the asynchronous updates, 'cause the parameters move in the meantime and in sparse models that's less likely to be the case because the interference pattern looks more like that rather than like that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess that's the reason we need backup workers, mostly because you know, I'm still down to the machine, but without slowdowns, use the data itself.",
                    "label": 0
                },
                {
                    "sent": "That system that is just harder for us to make sure that you will eventually.",
                    "label": 0
                },
                {
                    "sent": "Presently factions, or you don't think that under Florida.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean the assumption here is that most trips through the model are roughly the same, so that's true for image models.",
                    "label": 0
                },
                {
                    "sent": "At least these dense image models.",
                    "label": 0
                },
                {
                    "sent": "If you start to have branching in the models then then you probably.",
                    "label": 0
                },
                {
                    "sent": "Would need to add some extra mechanism to keep track of the probability of sampling a particular example and make sure you don't drop examples in unbiased manner.",
                    "label": 0
                },
                {
                    "sent": "Or in any bias manner.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 30 point 5X slide was it with synchronous plus extra or asynchronous.",
                    "label": 1
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was synchronous I believe.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, I don't know if you're going to talk about the model or not, but I just want to know how do you feel about the whole data partners in the story?",
                    "label": 0
                },
                {
                    "sent": "Because this please, as far as I know, the only possible model was you know their model replication over many devices, but that has been changed in Tensorflow and Tensorflow.",
                    "label": 0
                },
                {
                    "sent": "Its potential is capable of, like you know, find training of how data is being transferred week and everything.",
                    "label": 0
                },
                {
                    "sent": "So is that because you know you think that data?",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where do things in it?",
                    "label": 0
                },
                {
                    "sent": "And because it's based on this graph, it seems that it kind of scale is 100 machines, not more than that.",
                    "label": 0
                },
                {
                    "sent": "Right, so actually this belief also supported model parallelism, so if you think about this in either Tensorflow or disbelieve, each one of these yellow rectangles could be a separate machine that has got model parallelism involved in the model itself.",
                    "label": 0
                },
                {
                    "sent": "In fact, we regularly train for example language models or sequence to sequence models, where each one of these might use 16 GPU cards, 'cause it's a very deep model with 16 layers, and you get pipelining as I'll show you later in terms of model parallelism.",
                    "label": 0
                },
                {
                    "sent": "But then you also have.",
                    "label": 0
                },
                {
                    "sent": "Data parallelism where you have multiple replicas of those 16 GPU setups, so it's not like you have to pick one or the other.",
                    "label": 0
                },
                {
                    "sent": "Often both of them are good to do.",
                    "label": 0
                },
                {
                    "sent": "Depending on your.",
                    "label": 0
                },
                {
                    "sent": "Tolerance for what you want.",
                    "label": 0
                },
                {
                    "sent": "Or was I?",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other questions.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So we originally designed tensor flow to kind of generalize the kinds of computations we wanted to do in terms of the deep learning research our group was doing.",
                    "label": 0
                },
                {
                    "sent": "But it's actually pretty flexible, so I think what we'll see overtime is that a lot of other kinds of computations can be expressed in Tensorflow, and the kinds of optimizations we're starting to do, and will do more of.",
                    "label": 0
                },
                {
                    "sent": "We'll make lots of different kinds of computations, expressed.",
                    "label": 0
                },
                {
                    "sent": "Potential graphs run efficiently on sort of very heterogeneous hardware.",
                    "label": 0
                },
                {
                    "sent": "I mean it's a very nice model that you can express this abstract graph and then say I would like to use 16 GPU's please and have the system automatically place the computation.",
                    "label": 0
                },
                {
                    "sent": "Or have hints about where the competition should be placed.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, it runs on the variety of different platforms, which is important for some kinds of applications.",
                    "label": 0
                },
                {
                    "sent": "And in particular, we're starting to see people building custom machine learning hardware.",
                    "label": 0
                },
                {
                    "sent": "For that, make certain kinds of operations that you see in deep models much more efficient than on CPUs or GPU's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a company called Movidius that is built this kind of very low power.",
                    "label": 0
                },
                {
                    "sent": "Custom ASIC that doesn't have a huge number of raw flops but actually does a fair number of flops per Watt.",
                    "label": 0
                },
                {
                    "sent": "And they have a little evaluation USB stick, so you can get one of these and plug it into your laptop and have an accelerator for different kinds of models.",
                    "label": 0
                },
                {
                    "sent": "I think that a typical application they're imagining is something like a drone where you actually want on board image models to run efficiently and not drain your battery too much.",
                    "label": 0
                },
                {
                    "sent": "And Google has actually built a custom ASIC that I'll talk about in a minute for mostly for accelerating inference in datacenters.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about that in a month.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I think a really likely trend is are going to be much more heterogeneous hardware in data centers and also in mobile devices because essentially general purpose CPU performance scaling has stopped following Moore's law.",
                    "label": 0
                },
                {
                    "sent": "The basically you can't make.",
                    "label": 0
                },
                {
                    "sent": "In general purpose CPUs that give you the performance scaling we were getting, say a decade ago anymore.",
                    "label": 0
                },
                {
                    "sent": "And so specialization of hardware for different kinds of workloads will be much more important, and because deep learning has this handful of primitives that are really important, things like matrix multiplies or elementwise vector operations, they'll be a really big opportunity to specialize hardware.",
                    "label": 0
                },
                {
                    "sent": "And they're also very tolerant of reduced precision, an noise.",
                    "label": 0
                },
                {
                    "sent": "And that is a pretty different design point than you know.",
                    "label": 0
                },
                {
                    "sent": "Always correct logic in your 32 bit multiplier that you might have in a general purpose CPU, so I think that'll mean that people tend to look at what kinds of things can we do that are a bit more specialized, especially because, unlike other kinds of specialized hardware that might do encryption well or might do compression well.",
                    "label": 0
                },
                {
                    "sent": "If you believe the deep learning is going to be in pretty much everything.",
                    "label": 0
                },
                {
                    "sent": "Then you can build specialized hardware, but it's not really so specialized right and like actually does everything you want.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of nice.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Google's take on customized hardware.",
                    "label": 0
                },
                {
                    "sent": "Is this custom machine learning ASIC that we are chip design team built an it's been in production use for almost a year and a half now.",
                    "label": 0
                },
                {
                    "sent": "It's used on every search query it was used for the Alphago match in March against Lee Sedol in Korea that DeepMind arranged so you can see the rack of machines that was used for the Alphago match with a little commemorative go board on the side.",
                    "label": 0
                },
                {
                    "sent": "And essentially it gives us an order of magnitude better top peak performance, but also performance per Watt over things like GPU's and CPU's.",
                    "label": 0
                },
                {
                    "sent": "So we think that's pretty important.",
                    "label": 0
                },
                {
                    "sent": "The other thing it gives you is because it has higher peak operations.",
                    "label": 0
                },
                {
                    "sent": "It means you can employ fairly computationally expensive models.",
                    "label": 0
                },
                {
                    "sent": "At lower latency and for a lot of interactive kinds of applications, you really care pretty deeply about.",
                    "label": 0
                },
                {
                    "sent": "You know responding quickly to the user.",
                    "label": 0
                },
                {
                    "sent": "So you said it's pretty general.",
                    "label": 0
                },
                {
                    "sent": "It's doing tensor operations, but you also said it was only used for inference.",
                    "label": 0
                },
                {
                    "sent": "Where what's what's missing for doing training?",
                    "label": 0
                },
                {
                    "sent": "It just turns out that inference requires a bit less precision than training generally, and so if you want to build something specialized for inference, then you might want to build something with even lower precision than really can be accommodated for training.",
                    "label": 0
                },
                {
                    "sent": "Kind of precision.",
                    "label": 0
                },
                {
                    "sent": "Do you have?",
                    "label": 0
                },
                {
                    "sent": "We mostly are doing 8 bit operations.",
                    "label": 0
                },
                {
                    "sent": "It rain within.",
                    "label": 0
                },
                {
                    "sent": "Dad, it's harder.",
                    "label": 0
                },
                {
                    "sent": "But definitely 32 bit Floating Points are overkill for training, so there is a 16 bit.",
                    "label": 0
                },
                {
                    "sent": "8 bits for activations is plenty.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe with some scaling and so on.",
                    "label": 0
                },
                {
                    "sent": "Yeah, question up there.",
                    "label": 0
                },
                {
                    "sent": "The the first part was is it expensive?",
                    "label": 0
                },
                {
                    "sent": "Priceless.",
                    "label": 0
                },
                {
                    "sent": "We are not currently selling them so.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Well, I mean we look at performance per dollar in terms of the solutions that we have available and we think it's better than other solutions out there.",
                    "label": 0
                },
                {
                    "sent": "So you can infer from that what you like.",
                    "label": 0
                },
                {
                    "sent": "I guess we don't currently have any plans to announce to make them available, but.",
                    "label": 0
                },
                {
                    "sent": "It might make sense down the road to.",
                    "label": 0
                },
                {
                    "sent": "Maybe not sell them, but sells sell the ability to run computations on them.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I think machine learning hardware is going to be interesting because.",
                    "label": 0
                },
                {
                    "sent": "I think hardware designers are just starting to really look at what you can do given the kind of relaxed constraints of lower precision and maybe noise.",
                    "label": 0
                },
                {
                    "sent": "In terms of what that might mean for designing hardware rather than having to always get the right answer, say.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm excited about that.",
                    "label": 0
                },
                {
                    "sent": "So tensor flow is also extensible, so the core system defines a bunch of standard operations which are kind of these abstract things like matrix multiply or elementwise vector ad.",
                    "label": 0
                },
                {
                    "sent": "And then also kernels which are implementations of operations for particular devices.",
                    "label": 0
                },
                {
                    "sent": "So we have this kind of two level model of the abstract operations and then different implementations of those that can be mapped onto different devices.",
                    "label": 0
                },
                {
                    "sent": "And not every operation will have an implementation for every device.",
                    "label": 0
                },
                {
                    "sent": "So if you have something that only runs on CPU, you just wouldn't have a GPU kernel for that, and the system would know well my available choices for running this have to be on a CPU, 'cause that's the only kernel that I have for that.",
                    "label": 0
                },
                {
                    "sent": "And it's relatively easy to define new operators or kernels if you wanted something that's not covered by the standard thing, but that's relatively rare for our machine.",
                    "label": 0
                },
                {
                    "sent": "Learning researchers generally to want to do that.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me take a bit of a lower level tour now through what's actually happening under the covers.",
                    "label": 0
                },
                {
                    "sent": "When you're expressing different tensor flow things, I was going to put together some slides and then I discovered this person, Kevin Robinson at MIT had put together a great set of slides that were covering mostly what I wanted, so I asked him if I could use his slides and he said yes.",
                    "label": 0
                },
                {
                    "sent": "So the slides with this background, our thanks to Kevin.",
                    "label": 0
                },
                {
                    "sent": "OK, so some of this I kind of already talked about, But basically you have this computational graph.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And each one of these things is an op an as I alluded to, it has.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kernels, so you express these OPS.",
                    "label": 0
                },
                {
                    "sent": "This is similar to the example I showed you earlier, but all in one slide, and we're going to have this little running example to go through.",
                    "label": 0
                },
                {
                    "sent": "These things were going to weight matrix some input bias term that gets added together in a relative unit and some cost function.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now if we look here at the matrix, multiply.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What that really means is we're going to take the tensor flowing out of W and the tensor flowing out of X, apply the matrix, multiply operation, and produce an output that will flow into some other part of the graph.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in Python you would write this TF macmall W, X.",
                    "label": 0
                },
                {
                    "sent": "We actually automatically generate Python wrappers for the lower level representations of these things, because in C++ we have a matrix multiply operation that's got a variety of different options, like is a transposed already or is be transposed already.",
                    "label": 0
                },
                {
                    "sent": "What is the type of the underlying elements?",
                    "label": 0
                },
                {
                    "sent": "And so this call here will generate a call to here saying.",
                    "label": 0
                },
                {
                    "sent": "Basically it's not transposed.",
                    "label": 0
                },
                {
                    "sent": "So this is Python code.",
                    "label": 0
                },
                {
                    "sent": "This is code.",
                    "label": 0
                },
                {
                    "sent": "The user wrote.",
                    "label": 0
                },
                {
                    "sent": "This is code that's automatically generated by our wrappers generators from the operations that are defined in C. Source code down here.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we're going to zoom along, and this is now where we actually are going to execute the graph multiple times.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we built the graph.",
                    "label": 0
                },
                {
                    "sent": "This shows you the graph.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have a visual.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the lousy to see what the graph that gets constructed by your Tensorflow program looks like in this visualizer front end called tensor board.",
                    "label": 0
                },
                {
                    "sent": "And so you can see here we have an assignment to variable W and we have a read of this variable.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the optimizer functions extend the graph so this.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is where the automatic symbolic differentiation happens.",
                    "label": 0
                },
                {
                    "sent": "So for in particular if I say minimize the cross entropy loss here, then it's.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to automatically compute derivatives of the various variables in my parameters in my model with respect to this, this cross entropy loss Ann will do the right extension of the graph.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's sort of drawn in this slightly weird way, but really you can think of it as more graph.",
                    "label": 0
                },
                {
                    "sent": "And this is very much how Theano operates.",
                    "label": 0
                },
                {
                    "sent": "As well.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then one of the ways that the graph the actual computation gets communicated around is through a serialization mechanism called protocol buffers.",
                    "label": 0
                },
                {
                    "sent": "So that's a way of expressing what the graph is in a way that can be sent across machine boundaries pretty easily.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we say, please give me the graph as graph DEF, which is a protocol buffer.",
                    "label": 0
                },
                {
                    "sent": "And so here's a node with the matrix multiply operation where we're having W&X and they're not transposed.",
                    "label": 0
                },
                {
                    "sent": "And then there's an add operation an irrelevant.",
                    "label": 0
                },
                {
                    "sent": "And so that's kind of the representation of those three graph nodes.",
                    "label": 0
                },
                {
                    "sent": "In graph.",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is that.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's that one.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's see where are we in terms of time.",
                    "label": 0
                },
                {
                    "sent": "Let's keep going a little bit right.",
                    "label": 0
                },
                {
                    "sent": "We break it for.",
                    "label": 0
                },
                {
                    "sent": "Or one for me.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's look at how we actually get these graphs distributed.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Across multiple devices so.",
                    "label": 0
                },
                {
                    "sent": "In a distributed setting, there are many different processes involved.",
                    "label": 0
                },
                {
                    "sent": "There's a client process, and then there's typically a master process that is responsible for kind of coordinating all the different operations that are going to be performed and orchestrating the graph execution.",
                    "label": 0
                },
                {
                    "sent": "And then there may be multiple worker processes involved in your.",
                    "label": 0
                },
                {
                    "sent": "In your model.",
                    "label": 0
                },
                {
                    "sent": "Which may have multiple devices.",
                    "label": 0
                },
                {
                    "sent": "So when you create.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A new session that says please.",
                    "label": 0
                },
                {
                    "sent": "Give me a way to talk to this master coordinating process.",
                    "label": 0
                },
                {
                    "sent": "So over an RPC layer like G RPC.",
                    "label": 0
                },
                {
                    "sent": "That has, so here we say I'm going to create a session.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then it's going to send in RPC.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The master service with the saying.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please create this session to execute this graph and that's where the graph that you've constructed in your client program actually gets converted into the serialized form and sent over to the Master where.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we have the graph on the master.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And later you would say please run this graph and I would like to feed in for the value of X in the graph.",
                    "label": 0
                },
                {
                    "sent": "This particular input value.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so that gets sent over as well and gets converted into a run step called says.",
                    "label": 0
                },
                {
                    "sent": "Please run the graph.",
                    "label": 0
                },
                {
                    "sent": "Given this data that's being fed in for X and you can also say I'd like to fetch the value of the cost of the model or whatever.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so now that coordinator is going to.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk to the various workers involved in the process.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's going to say please run.",
                    "label": 0
                },
                {
                    "sent": "Run this graph for.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The part that you have an these things might have receive and send tensor operations receive tensor calls that communicate tensors amongst these different workers as necessary to execute the entire graph.",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_139": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the things you can do in Tensorflow is you can say, here's my general graph, but actually on this run call I only want to execute and fetch.",
                    "label": 0
                },
                {
                    "sent": "Node F, and by the way, I'm going to feed in this value for node C.",
                    "label": 0
                }
            ]
        },
        "clip_140": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so conceptually that gets converted into this.",
                    "label": 0
                }
            ]
        },
        "clip_141": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can prove that we can then detur.",
                    "label": 0
                }
            ]
        },
        "clip_142": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And because we only want to fetch node F. We can determine the translator dependencies that we need to execute in order to compute the output of node F. And when we do that.",
                    "label": 0
                }
            ]
        },
        "clip_143": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Means we don't need to execute these other nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "In this particular case.",
                    "label": 0
                },
                {
                    "sent": "So this is a pretty general technique intensive where you express perhaps many different kinds of things you want to do in the graph in a single graph, and then depending on what operation you want to do, you execute just a piece of that graph.",
                    "label": 0
                },
                {
                    "sent": "So we often for example have some part of the graph sitting off on the side that has checkpointing operations for the parameters in the modeling, and normally we don't execute those, but then every 10 minutes or something will execute this other part of the graph.",
                    "label": 0
                },
                {
                    "sent": "That is going to Check Point the state of the model onto disk, or restore it for example, but most of the time we might not execute those.",
                    "label": 0
                },
                {
                    "sent": "And that's a pretty general technique for.",
                    "label": 0
                },
                {
                    "sent": "Having a single static graph that can be a little bit dynamic.",
                    "label": 0
                }
            ]
        },
        "clip_144": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So one of the things that we need to do once we get this graph in the master process is figure out where we're going to place the different bits of computation, and we allow users to specify hints about that.",
                    "label": 0
                },
                {
                    "sent": "So you can say I would like this particular set of variables to be placed on this job task 0.",
                    "label": 0
                },
                {
                    "sent": "This is how you might, for example, use a parameter server and distribute a few of the parameters and their model across different parameter servers.",
                    "label": 0
                },
                {
                    "sent": "And then I would like this bits of computation to be done on task 7.",
                    "label": 0
                },
                {
                    "sent": "This is sort of.",
                    "label": 0
                }
            ]
        },
        "clip_145": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unideal, ideally we would just take the graph and figure out where we want to place things, but that's slightly more aspirational than actually works in practice today.",
                    "label": 0
                },
                {
                    "sent": "So we allow people to specify hints you don't have to specify hints for everything if you specify a few hints that will kind of anchor bits of the graph, and then the rest of the stuff will generally happen correctly, and we have plans to work on this more seriously to actually do a good job of actually measuring.",
                    "label": 0
                },
                {
                    "sent": "You know where we should be placing things?",
                    "label": 0
                },
                {
                    "sent": "This is actually an interesting machine learning problem in terms of because it's actually very amenable to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Where you want to place nodes here and then you can get a measurement of how long the graph takes to execute and then say Oh well, that was a bad idea.",
                    "label": 0
                },
                {
                    "sent": "I should have placed it over here and then.",
                    "label": 0
                },
                {
                    "sent": "The trick is you want to be able to generalize from experience with one graph, 2 similar kinds of graphs.",
                    "label": 0
                },
                {
                    "sent": "So we think that that might be an interesting problem to work on.",
                    "label": 0
                }
            ]
        },
        "clip_146": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_147": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where is the placement we might.",
                    "label": 0
                }
            ]
        },
        "clip_148": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Decide to place, for example, these nodes on this device and these nodes on this device.",
                    "label": 0
                },
                {
                    "sent": "Or these could be devices in the same process, but are different GPU cards, for example.",
                    "label": 0
                },
                {
                    "sent": "So you see the difference there.",
                    "label": 0
                },
                {
                    "sent": "This is actually a separate process, maybe on a different machine.",
                    "label": 0
                },
                {
                    "sent": "Or they might be different devices, but on the same machine.",
                    "label": 0
                },
                {
                    "sent": "Two different cards.",
                    "label": 0
                }
            ]
        },
        "clip_149": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so one of the first things that happens is we partition this graph into subgraphs after we've made placement decision.",
                    "label": 0
                }
            ]
        },
        "clip_150": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we insert the send and receive nodes.",
                    "label": 0
                }
            ]
        },
        "clip_151": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have this thing called a rendezvous.",
                    "label": 0
                },
                {
                    "sent": "That's how the send and receive nodes kind of coordinate their activity together.",
                    "label": 0
                }
            ]
        },
        "clip_152": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_153": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_154": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_155": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see what happens when we do run.",
                    "label": 0
                }
            ]
        },
        "clip_156": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph, so we're going to execute.",
                    "label": 0
                }
            ]
        },
        "clip_157": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This graph.",
                    "label": 0
                }
            ]
        },
        "clip_158": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a Colonel.",
                    "label": 0
                }
            ]
        },
        "clip_159": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if the operation here A is for matrix multiply, we're going to find the corresponding kernel, and we're going to.",
                    "label": 0
                }
            ]
        },
        "clip_160": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cute it.",
                    "label": 0
                },
                {
                    "sent": "If it's a conf 2D operation, then there's a different kernel that's defined for that, or an op for that.",
                    "label": 0
                }
            ]
        },
        "clip_161": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's different kernel specified, so this is the C++ code that gets registered mostly if you're using tensor flow from the Python level, you don't have to care about any of us.",
                    "label": 0
                },
                {
                    "sent": "You just care that the right operations for your model are there.",
                    "label": 0
                }
            ]
        },
        "clip_162": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_163": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, for example, here is the matrix multiply operation.",
                    "label": 0
                }
            ]
        },
        "clip_164": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Every operation, every kernel has a compute method, which is what gets invoked on every step, and that allows you to access your input tensors and then perform operations on them.",
                    "label": 0
                },
                {
                    "sent": "So in particular, this will launch a metric multiply operation on A&B.",
                    "label": 0
                }
            ]
        },
        "clip_165": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And do the right thing.",
                    "label": 0
                },
                {
                    "sent": "This is a different way of doing.",
                    "label": 0
                }
            ]
        },
        "clip_166": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's so exciting.",
                    "label": 0
                },
                {
                    "sent": "This allows you to get a stream context for GPU devices.",
                    "label": 0
                },
                {
                    "sent": "We've abstracted away the Koblas interface a bit so that we can, for example, use a stream execution stream for GPU cards and have a different implementation of it for, say, TPU cards.",
                    "label": 0
                }
            ]
        },
        "clip_167": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_168": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, there we go.",
                    "label": 0
                }
            ]
        },
        "clip_169": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I'm going to skip.",
                    "label": 0
                }
            ]
        },
        "clip_170": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most of the rest of this.",
                    "label": 0
                }
            ]
        },
        "clip_171": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not supported.",
                    "label": 0
                }
            ]
        },
        "clip_172": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so one of the benefits of the session interface is that you can essentially specify this graph and then call run many times, and that's typically what users do.",
                    "label": 0
                },
                {
                    "sent": "Is they set up their model and then you run thousands or 10s of thousands of.",
                    "label": 0
                },
                {
                    "sent": "Steps through your model and so that provides a big opportunity to do significant optimization work, and we do some of that today, but we intend to do a lot more in terms of generating really good code for.",
                    "label": 0
                },
                {
                    "sent": "Particular sizes of tensors that are flowing around in the model.",
                    "label": 0
                },
                {
                    "sent": "Which are not necessarily known at compile time in our case.",
                    "label": 0
                }
            ]
        },
        "clip_173": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so why don't we take a break now?",
                    "label": 0
                },
                {
                    "sent": "What seemed like a plan an?",
                    "label": 0
                },
                {
                    "sent": "Will come back in half an hour though right?",
                    "label": 0
                },
                {
                    "sent": "So a little bit before.",
                    "label": 0
                },
                {
                    "sent": "Are there questions before we take a break?",
                    "label": 0
                }
            ]
        }
    }
}