{
    "id": "k3fijoqol6dbi3lwei2hjorcyhx5imug",
    "title": "Practical RL: Representation, interaction, synthesis, and morality (PRISM)",
    "info": {
        "author": [
            "Peter Stone, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Medicine->Neuroscience",
            "Top->Social Sciences->Economics",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering",
            "Top->Social Sciences->Psychology"
        ]
    },
    "url": "http://videolectures.net/rldm2015_stone_practical_rl/",
    "segmentation": [
        [
            "I'm going to talk today about but practical reinforcement learning in college, and I have sort of my acronym, PRISM representation, Interaction, synthesis, immortality and I could have given a talk.",
            "I think this is founded on talk to they gave it the European Workshop on Reinforcement learning a couple years ago, where I talked about 8 different things for five minutes each.",
            "And I'm not going to do that today.",
            "I'm going to tell you what those 88 are now.",
            "There's nine or ten of them.",
            "Things were and then go deeply on two of them.",
            "Um, but it's still the structure is sort of what I've been doing in my in my lab over the past.",
            "Decade and a half or so."
        ],
        [
            "And is trying to think of how we can start using how we can use reinforcement learning as a tool.",
            "Usually historically when we've thought about reinforcement learning, we thought.",
            "Well what if we have a finite MDP?",
            "You know what's the?",
            "What's the optimal way to solve it?"
        ],
        [
            "And then often we've said, well, OK, in the real world, there's actually stayed aliasing, and there's actually generalization, but let's just pretend it's a finite MDP and apply the same methods and see what happens now.",
            "Of course, that's not the whole story, and we've done made a lot of progress, but but that's sort of often the way we think."
        ],
        [
            "Instead, we've started saying, let's just embrace all of the complexity of the world and and try to develop algorithms that do work so well."
        ],
        [
            "Are asking.",
            "At least the theme of this talk will be we're asking, not should reinforce."
        ],
        [
            "Learning, but does it?"
        ],
        [
            "Work and when not, how can we make it work now this is an oversimplification.",
            "There's been research in my lab starting from the algorithms in the theory that we've as well, but this is going to be emphasizing the more."
        ],
        [
            "The more practical side along four different dimensions.",
            "Um?"
        ],
        [
            "We've done work on representation, by which I mean, you know, trying to figure out how to select the algorithm to use for a given problem.",
            "In parameterized domains, that was with Shivram, Kalyanam Krishnan, or adaptive representations.",
            "Trying to figure out what the format of the function approximator should be on line."
        ],
        [
            "We've done a lot of thinking about interaction.",
            "What if there's adversaries in the mix?",
            "Some more sort of theoretical style work with Doran Chakraborty in repeated repeated game settings.",
            "We have an algorithm called seamless that we that we proved was able to to optimally exploit an adversary if it was in the target class while still converging to self to equilibrium in self play and never being vulnerable to doing worse than a safety level.",
            "In this sort of a pack sense.",
            "We've looked at interacting with teammates that you haven't that you haven't interacted with before we call that ad hoc teamwork.",
            "That's my most recent graduate.",
            "Sam Barrett is now achievable.",
            "He introduced an algorithm called plastic.",
            "We've looked at Brad Knox is thesis was looking at interaction from people.",
            "What if there's human input coming in as a reinforcement, learning as a signal, and Andrews talk?",
            "Yesterday was sort of in that in that theme, so that's what I mean by some some interaction that we have to take take into account."
        ],
        [
            "Look at synthesis of different algorithms.",
            "What if you have different reinforcement learning algorithms?",
            "How can we put them together and get the best of both worlds?",
            "My thesis was introduced layered learning and actually I'm going to talk more about that today.",
            "Nick Jong in his his thesis introduced an algorithm called fitted our Max Q which is you can maybe tell from the from the name was model based and and.",
            "A hierarchical an worked in continuous settings."
        ],
        [
            "And then finally mortality.",
            "It is the other, the sort of last part of this transfer, learning that Taylor's work on leveraging the past because you have a sort of I'm calling it mortality because you have you have to, embracing the idea that the agent has a limited amount of time to live in the environment.",
            "And that's really the motivation behind Todd Hesters work.",
            "Also, the text blur, acknowledging a finite future.",
            "And I'm going to really tell you what I mean by that."
        ],
        [
            "2nd, in fact, that's the first of all these things.",
            "That's the first deep dive I'm going to do is on this algorithm text floor then."
        ],
        [
            "I will talk about our recent advances in layered learning.",
            "I said introduce that in my thesis.",
            "We've actually made a big jump forward in that in the past year and then time permitting I'll have one."
        ],
        [
            "Side of this ad hoc teamwork aspect."
        ],
        [
            "So first I'm going to talk about text lore, real time, sample efficient reinforcement learning for robots.",
            "This is with my student at Todd Hester who's now at Nest.",
            "The main article on this appeared in Machine Learning a couple of years ago."
        ],
        [
            "And you know there's been talk here.",
            "There always is this sort of a meeting between model free and model based methods.",
            "The motivation for this is that.",
            "You know, at the most abstract level models, free methods.",
            "Can run in real time, but are generally not very sample efficient."
        ],
        [
            "And model based methods.",
            "Um?",
            "Can are much more efficient with data, but often take a lot more computation.",
            "And Todd came in saying, well, you know, for a robot we really need both of those.",
            "We need sample efficient to see an we need computational efficiency.",
            "There had been some.",
            "There had been some work on helicopter control was very, very impressive that Andrew Yang's group had done.",
            "And Drew bagnell's group.",
            "But it was unsatisfying to us that you had to learn the model from some data and then go off and compute for half an hour an hour before actually acting, and so the goal here was to try to create a algorithm that would allow learning online in real time."
        ],
        [
            "The robot, and so that's what to do that we embrace.",
            "What this concept that I'm calling mortality.",
            "The idea that robots lifetime is short compared to the size of the world.",
            "Um?",
            "We embrace this as people, right?",
            "We acknowledge that.",
            "There's no, there's no possible way to to explore all the possible cities we might live in, or even the example I often uses.",
            "There's a building I can see outside my office, and I've been in my office for years.",
            "I've never gone to the third floor of that building.",
            "In theory, my own personal Nirvana could be up there, right?",
            "Or my favorite restaurant I'll never find in in my lifetime, but I've I'm aggressively deciding never to explore there.",
            "And so by doing that I'm.",
            "I'm assuming the possibility of.",
            "Of find guaranteeing myself the optimal policy.",
            "But in exchange I'm I'm heuristically sort of exploring to more distant, more distant areas in the world, and so we're trying to text floor is trying."
        ],
        [
            "To embrace that.",
            "Well still acting in."
        ],
        [
            "All time, so this is what I just said.",
            "We can't explore everywhere.",
            "And so that the."
        ],
        [
            "Sort of a motivating domain for us.",
            "Um?",
            "Well, I'll show this working on a on a real robot, but sort of a testbed domain that we used was fuel world where most of the state space is very predictable.",
            "The interior white squares you move right move in one of the eight directions, but the fuel stations which are the green and the bottom and the green on the top have varying costs and you can't move directly from the start, which is the blue to the goal, which is the red without running out of fuel.",
            "So you need to go through one of those green fuel stations to refuel, but you don't know how they all have varying costs and so you mainly want to explore.",
            "Um?",
            "The."
        ],
        [
            "Those those fuel stations and we all know what it would look like if we were using like you learning or our Max, they'd sort of exhaustively explore all of the states.",
            "But here's what I'll just sort of show you.",
            "The kind of exploration that text floor is doing, and then I'll tell you how it's doing this, but it's it is doing some exploration around the environment to sort of build its model of the internal dynamics, and then it's pretty quickly starts just exploring the parts that are uncertain to it, and there are parts of the environment over here that it never that never visits, even once, and even in the interior, even though in theory that as far as it knows, those could be.",
            "Infinitely rewarding states.",
            "And.",
            "And so we find it's focusing its exploration, but it's still."
        ],
        [
            "Ending near optimal Policy's the we also, then, and like I said, the motivation here was on robots we.",
            "I implemented on a real autonomous car on a speed control task that I'll tell you about, but where it's where it's doing.",
            "It's learning well, actually driving.",
            "And in a in a very short amount of time.",
            "And the speed can be, I'll tell you."
        ],
        [
            "The task itself.",
            "The state was that they was the current velocity, the desired velocity, the accelerator pedal position, and this was an old autonomous car, was in the DARPA Urban Challenge.",
            "Our version of drive by Wire was there was actually a wire attached to the brake pedal that would get pulled by a motor and pull the brake in.",
            "So we had the accelerator pedal position in the brake pedal position and its actions were to do nothing.",
            "Increase or decrease the brake position by some percentage, or the accelerator position and the reward signal was just the.",
            "The distance from the desired velocity and at the outset it knew nothing about the about the what would happen if you pressed any.",
            "If you took any of the actions."
        ],
        [
            "And what we wanted here was that it needs to learn with very few actions.",
            "It has to be sample efficient, has to act continually in real time, deal with the continuous."
        ],
        [
            "Eight in the delayed actions, and this is what I yeah.",
            "This is what I brake pedal looked like."
        ],
        [
            "We've retired this autonomous car.",
            "There's people doing it much better than we are now, but but this was it was a great testbed, and so."
        ],
        [
            "Try to achieve these desiderata.",
            "We sort of did a survey.",
            "Are there any algorithms that can do all?"
        ],
        [
            "And there's many that did.",
            "Some of them that dealt with some."
        ],
        [
            "These different aspects, but we we believe text blurs the first algorithm that could handle all of these four different or satisfy all of these four."
        ],
        [
            "Desideratum.",
            "We did make it publicly available as a Ros package and so for those of you who use robots, you can go out and download it and just and there's the interface."
        ],
        [
            "To use it so.",
            "So how do we achieve all four of these of these things?",
            "Of course, there's many more details in the paper, and because I want to just in time management, I'm going to give just the overview of them, but.",
            "To deal with."
        ],
        [
            "Efficiency we do what you would think.",
            "We take a model based approach.",
            "Um?"
        ],
        [
            "Learning a factored model so a separate model to predict each next state feature and reward from the from the current state."
        ],
        [
            "Using decision trees."
        ],
        [
            "But actually using a random forest model, and this is one of the keys to allow us to to find to find some states that we will never explore.",
            "We're using the average predictions of M different decision trees where each one is built from a random subset of the experiences.",
            "So we've saved all of these state action reward tools.",
            "We've then taken some random partitions of those built a model from each of them.",
            "And then each tree represents one hypothesis of the true dynamics."
        ],
        [
            "Domains the domain and then the agent will act greedily with respect to the average model in by planning planning ahead in the in the average in the sort of average outcomes of the various models, and then act greedily with respect to that.",
            "And what this?",
            "It turns out that what this allows the agent to do is if there's a.",
            "If there are states that may have disastrous consequences, and even one of the models the agent will never, never plan to go there, or if there are states where there's really no no predicted benefit like that third floor of the building that I can see outside my outside my office, the model will predict that there's probably nothing interesting that's going to happen that happen there.",
            "The agent will never plan to explore there, but if there's some states with either high variance.",
            "Or that don't have with or with high variance.",
            "It will be more likely to plan there and.",
            "Part that I don't have in here.",
            "There are two exploration bonuses that are sort of in the Rmac style, one for for states that are regions of the state space that are completely unknown, so not individual states but, but there are exploration bonuses, both for novel novelty in for variance, and we define novelty instead of distance from distance in the in the.",
            "State space representation from places that the agent knows.",
            "In other words, in any case, this limits the agents exploration to the state actions that appear promising while avoiding those."
        ],
        [
            "Which have negative outcomes.",
            "With regards to the real time aspect in text lore, we took the sort of the standard model based approach which would.",
            "Update the model, then plan on the expected model and then return an action."
        ],
        [
            "And broke this into a multi threaded architecture.",
            "That has parallel threads for for acting, planning an model, learning where the action thread is happening at the whatever rate the robot requires and in the background the model learning is happening in the planning from the current state."
        ],
        [
            "Using a."
        ],
        [
            "Sample based planning Monte Carlo tree search.",
            "And then there are mutex law."
        ],
        [
            "Talks between these different threads so that you always have a consistent view of the data and are always planning and acting is on the the model and the plan that says as current as possible given the rate of the reaction and this allows we did some testing on this.",
            "It allows to exploit also parallel parallel computing arc."
        ],
        [
            "Textures.",
            "To deal with continuous state, we didn't do anything special.",
            "We just used a regression tree model rather than a."
        ],
        [
            "In the standard decision tree, we do discretize the state space for the value updates from UCT.",
            "But then we still plan over the continuously valued state."
        ],
        [
            "And then finally, for actually delays."
        ],
        [
            "We just augment the states in a fairly standard way to make it have the previous K actions that were taken.",
            "Action delay in our domain was very important because we decide to pull on the break.",
            "It takes several seconds, which is several time steps before the effect of that would happen.",
            "So you need to store what actions have been taken in the past and the trees consider learned."
        ],
        [
            "Each delayed actions are relevant.",
            "And UCT can plan over these augmented state action histories."
        ],
        [
            "So we then took our autonomous vehicle, which runs at 20 Hertz and allowed the agency to."
        ],
        [
            "Provide commands at this frequency, embedded it in a standard Ras interface node.",
            "Which as I said is is pub."
        ],
        [
            "We available and they did a bunch of experiments.",
            "First, in simulation, I'm not going to interest of time.",
            "I'm not going to go into detail on these experiments just to tell you for each of these different.",
            "Peter, out of sample efficiency and computational efficiency, we found sort of the best competitive."
        ],
        [
            "Competitor"
        ],
        [
            "Times and then tested text floor against each of these.",
            "The details are in the paper, and textbook competes favorably."
        ],
        [
            "Along each of these."
        ],
        [
            "I."
        ],
        [
            "Dimensions."
        ],
        [
            "But then we put it on the physical vehicle and asked you, does it, does it work in practice?"
        ],
        [
            "And the.",
            "The end result was that basically in two minutes of driving time.",
            "So these are episode 10 second episodes and these are showing the reward meaning the distance from the desired velocity that the agent achieves by the end of the episode and after about 2 minutes or so it was able to get to achieve the five meters per second at basically the same level of accuracy that the tuned PID controller was able to do, which for those of use PID control requires a fair amount of manual effort."
        ],
        [
            "And So what do you want?",
            "But it ended up looking like was, here's the.",
            "I have it in.",
            "Anne.",
            "I thought I had the video that would show.",
            "No, I'm not going to want to.",
            "Didn't want to show that whole video.",
            "I showed you already the car driving so.",
            "Basically it was learning the within those 2 1/2 minutes.",
            "Um?",
            "With without any pauses for computation, and that was one of that was one of the keys for us."
        ],
        [
            "OK, that's the whirlwind of text.",
            "Or of course there's a lot more details, but like I said, I want to sort of do a dive on two different aspects.",
            "The that was I think, but I do think this is very important.",
            "This idea of equipping our algorithms with the ability to.",
            "Aggressively generalize over the over the model so that they can actively decide where never to explore, and that's sort of the key motivation behind textbook.",
            "I'm now going to pop back up and change gears entirely to synthesis, and this is to talk about one of our more recent advance."
        ],
        [
            "Is in layered learning and this is joint work with especially Patrick Macalpine, a PhD student in my lab, also an undergrad, Mike definite contributed contributed to this work and we have a paper at triple AI this past year describing the detail."
        ],
        [
            "Cause of this?",
            "So layered learning was as I said, I introduced in my PhD thesis back in the late 90s.",
            "It's for domains that are too complex for attractive, attractively mapping state features directly to the output.",
            "So on a robot you know, think of the pixels in and the joints of the robot out.",
            "Although people are starting to get toward to the point of being able to deal with that, I think you going all the way to joints on a robot is still too much to do in a monolithic way.",
            "So layered learning assumes there's some hierarchical sub task decomposition given.",
            "There's also starting to be research on how can we automatically find this test decomposition, but let's assume for now that that's an input to."
        ],
        [
            "Algorithm.",
            "It uses then learning to exploit data training."
        ],
        [
            "Doepfer the key idea.",
            "The synthesis aspect of layered learning is that learning in one layer feeds directly."
        ],
        [
            "Into the next layer.",
            "So you could imagine an agent taking the IT sensors to build a world state, using that to learn individual behaviors, freezing those learning, using those to learn multi agent behaviors all the way up to the high level goals."
        ],
        [
            "And when we introduced this there in my in my thesis, we did it in a simulated robot soccer domain where there were three layers, learned a sort of ball interception behavior that used in neural networks.",
            "So just the ball was coming to an agent.",
            "What angle should it go to try to intercept it?",
            "Froze that and use that to to learn a passive valuation behavior.",
            "So given that my teammates are using that learned interception behavior, what's the likelihood they'll receive the pacify pasta them at a given time given the configuration of all the other agents that was using a decision tree and then we introduced a new reinforcement learning algorithm called it teapot IRL, it was a distributed multiagent reinforcement learning algorithm that did the past selection.",
            "Where to pass?",
            "We deployed this in the two dimensional simulated robot soccer team.",
            "Our agent team was here shown in red.",
            "This was the finals of the 1999 combat.",
            "Competition and they were using this layered learning and ended up winning the competition, scoring 110 goals and giving up none in eight games.",
            "And here you see, then the then setting up for for a set play.",
            "It almost scored one goal.",
            "Here it's here.",
            "It does succeed in scoring and the key here is that each of the agents was independent.",
            "Deciding what deciding what to do on its own and using these learned layers.",
            "And there were three learned layers."
        ],
        [
            "At that time we then took two learned layers on to some real robots, so these were the left robots using a policy gradient.",
            "RL method were learning to walk back and forth across the field autonomously.",
            "They were searching in a 12 dimensional continuous parameter space of what should be the height of their body and how fast they should move.",
            "Their legs timing themselves based on the size of those beacons, and they ended up with a walk that was about 15% faster than anybody had been able to generate on these robots.",
            "We then took that behavior and used it to learn a ball control behavior where given that walk they were trying to, the robot was trying to learn how to control the ball under its chin so they could turn with it and then pass it here.",
            "This is before learning.",
            "At the end it went from it, went from about a 40% success rate to a 70% success rate and being able to capture the ball and here they are again just two layers connected to one another.",
            "There's a successful trial."
        ],
        [
            "And.",
            "And these were using and some other people had had, I think put four layers together as well in various tasks.",
            "Had people who had picked up layered learning.",
            "And.",
            "The layered learning that we had looked at had two different types of paradigms.",
            "The one with sequential layered learning.",
            "The one I described, where you learn one layer, freeze that freeze all those parameters, and then use that to learn the second layer.",
            "That had and then another variant that we introduced shortly after we called concurrent layered Learning which I depict on the right where you would learn the first behavior, but then keep learning the first one while you're learning the second one."
        ],
        [
            "Both of those are limiting in some sense, though they can be both.",
            "Two limiting in the joint layer of the sequential layer learning could could be limiting in terms of what you learn at the lowest levels and the concurrent layered learning increases the dimensionality of learning and can make learning quite slow."
        ],
        [
            "And so we."
        ],
        [
            "My student Patrick was trying to use these in a new domain, which I'll show you.",
            "And found that they were that he needed to refine the concept significantly.",
            "He introduced overlapping layered learning.",
            "Which basically creates a tradeoff.",
            "It optimizes Asim or an overlap between the behave."
        ],
        [
            "In three different ways so."
        ],
        [
            "And they're sort of depicted schematically here.",
            "Overlap are combining independently learned behaviors, learns behaviors A&B and then freezes most of them, but learns just a few parameters at the at the scene."
        ],
        [
            "Partial concurrent, layered learning learns the first behavior, learns the second one while continuing to learn only part of some of the parameters of the of the first behavior."
        ],
        [
            "And finally, previously learned layer refinement will learn the first behavior, then learn the second behavior, then freeze the second one and relearn the first one.",
            "You can obviously do this multiple multiple times."
        ],
        [
            "And the domain that we were using that sort of pushed us to do this is now a new simulation or the more recent simulation in RoboCop, the 3D simulation domain where you have teams of 11 autonomous robots playing in a ody physics engine.",
            "They're modeled after the Elder on now Robot.",
            "We also have some of those real robots and the robots receiving noisy visual information about the environment so noisy distance and angle tell their agents and they can communicate this."
        ],
        [
            "Sort of what the domain looks like because it's in a physics engine.",
            "They do fall over.",
            "It's hard to even get them to stay on their feet and to find a walk that's that's robust and and stable and.",
            "And I'll tell you a lot more about about how the agents work as I move on now we.",
            "Created an agent that was able using layered learning with just a few learn layers we were able to create an agent that won the championship in 2011 and 2012."
        ],
        [
            "Using.",
            "A a policy policy gradient sort of method called Cmas by Hanson.",
            "That space was basically optimizing the parameters of a double inverted linear pendulum model for the walk.",
            "So the robot was basically using the equations of motion of a pendulum with pendulum, with the fulcrum being one of the one of the feet swinging over that to a double support phase, and then using the other foot is a fulcrum and again swinging like a pendulum and this left open a whole bunch of different parameters, such as how quickly should the center of mass or how much did the center of mass moves from side to side as you're doing this.",
            "How long should the steps be?",
            "How fast should the steps be?",
            "10s of different continuous parameters that needed to be optimized?"
        ],
        [
            "Within this model.",
            "And so we did this using CMAS, which is basically a stochastic derivative free numerical optimization method where the candidates are sampled from a multi."
        ],
        [
            "No Gaussian, it's sort of a step after cross entropy methods where it's not just adjusting the mean of the distribution for the next samples to test but also the covariance update controls the search step size and again this is not our methods from Hanson 09, but schematically.",
            "It's testing a bunch of of of individuals, then finding the most successful of them, updating the distribution to select for the next.",
            "The next set and basically honing in on a local optimum so it's not going to.",
            "It's not going to converge to a global optimum, but will will come to a local."
        ],
        [
            "Optimum.",
            "And using this method we create, we took an initial walk from the from a real robot, ported it into the simulator and it worked.",
            "Sort of like this sort of slow and lumbering.",
            "Um, but then we decided we tried to optimize it and actually, in 2010 we optimized it just for straight ahead speed and came up with the walk that was about twice as twice as fast as anybody else.",
            "We got very excited and then started to try to embed it in a behavior where it would use the very fast walk, but then have to slow down when it got to the ball.",
            "When it did that.",
            "This is sort of what happened.",
            "The transition between behaviors didn't workout very well, so this is where layered learning came in where we had the agents.",
            "Over its head is the different parameter sets it learned.",
            "the T is for going to target, in essence for sprinting where it learned one of them first, throws it, and then learn the other one.",
            "In the context of this sort of obstacle course where it's always trying to to chase the purple dot and come up with a set of parameters that not only work moves quickly, but also transitions well from one behavior to the other.",
            "And then added a third behavior for positioning near the ball.",
            "So we had it.",
            "The P is the positioning behavior.",
            "So again, this is through the three layered learning.",
            "It's just learning one at a time and we're testing it on how quickly it's able to dribble the ball forward, which is more than just the straight ahead walk.",
            "It's actually the behavior that's needed in the actual environment.",
            "And we came up with the with the final walk that.",
            "Wouldn't work on the on the real robot 'cause the physics simulator is not is not perfect.",
            "The legs can move nearly that quickly, but the goal was to actually work in simulation.",
            "This is the only work I know of that actually uses a real robot control policy to help us simulator rather than the other way around, but it ended up with something that was it was robust enough that we could put it into the.",
            "In 2011 we were able to put it into the competition.",
            "Agent scored 136 goals and gave up zero and never passed the ball in the competition.",
            "Basically just walked the ball into the goal.",
            "So a lot of our goals were just scored by by dribbling the ball into the into the goal.",
            "Here.",
            "This was a team that wasn't.",
            "This wasn't much of a competition in that set instead, but that was great.",
            "In 2011 we published this in 2012.",
            "The other agents started doing the same thing, and by 2013 we actually we lost in the finals."
        ],
        [
            "And so we sort of went back to the drawing board and said, OK, this was."
        ],
        [
            "We can't just dribble the ball into the goal.",
            "We're going to need to build up more skills, and this is where Patrick came up with."
        ],
        [
            "A set of 19 different learned behaviors that all interact with one another in a very complex way using layered learning that use these different types of overlapping, layered learning and over there there's some that are getting up and some that are kicking with different kicking, faster kicking, lower kicking high.",
            "There's a kickoff behavior you don't have to understand all of the numbers here, but they're the sort of the numbers of parameters that are optimized for each of the parameters, or for each of the behaviors are in parentheses.",
            "The numbers that are passed from one behavior to another are shown in red.",
            "The seated behaviors for the different types of overlapping layered learning are shown and look at their color coded based on weather, which flavor of overlapping layer learning?",
            "They're so this was not this 19 behavior hierarchy was not learned automatically, but given it we were."
        ],
        [
            "Able to learn a much more complex behavior and I'll just sort of give you some insight into how it looks.",
            "The approaching the ball to kick added a layer of this sort of a behavior where it had to get the robot into position where it would be able to kick, and then we combine that with a with a fixed kick where the robot was just practicing trying to kick the ball as far as it could, but from a from a fixed position where we were the robot was starting from exactly the same location, so this is an example of combining independently learned behaviors.",
            "It learn to approach separately, learn to kick separately.",
            "And then we had to try to combine them.",
            "And.",
            "And so we did that by having it now be beamed to a place that's not right in front of the ball and have it try to walk and approach.",
            "And the robot was able to to learn to kick the ball.",
            "Fairly far and.",
            "And then the final walk in kick.",
            "You can see the four different behaviors up here.",
            "The Sprint, the position that go to target, and then finally the approach when it gets to a position where it wants to kick the ball, which is here and then it was able to execute the kick finally.",
            "And."
        ],
        [
            "And then we also had this.",
            "There was this sort of off to the side kick off behavior.",
            "Where we had.",
            "It turns out that you can't score directly from a kick off, so we had the robot learn to initially just on the kickoff.",
            "Try to try to score and it would have been fine, but that wouldn't have counted because you're not allowed to score without another robot kicking it.",
            "So we had another robot learn to touch the ball just a just a little bit without moving it very far.",
            "This is actually we held this in.",
            "We held this in reserve.",
            "We didn't actually use this until the finals of the competition.",
            "But if you just put those together naively, they interfere with one another and we did not choreograph this.",
            "For a demonstration, we just put them together and saw what would happen and.",
            "Not only that, the first robots appears to have been very frustrated with the second one.",
            "Again, that just emerged, but um.",
            "But then by using layered learning we were able to put them together and you can see here in the in the inset here how they actually work together.",
            "The first one touches it just a little bit.",
            "The second one kicks it and in the finals we were winning one.",
            "Nothing after a second and sort of the other team sort of look that way with what just happened."
        ],
        [
            "Um?",
            "So in RoboCop we can then not only see what how the how the competition turns out, but we can.",
            "We can then, especially in the simulation Lee we can do much more control."
        ],
        [
            "The experiments, and so we were able to see.",
            "Well, you know what happens if we had just dribbled versus using the 19 different learn, learn layers against the top three teams from 2013.",
            "The number here is the gold different.",
            "The average goal difference over 1000 games and we were able to show that the real impact of putting adding the kick in the kickoff into the."
        ],
        [
            "To the team.",
            "But the real win here was that there are multiple different heterogeneous robot types in the.",
            "In the competition I showed you everything with the Type 01, but there's some that have longer legs, quicker moving legs.",
            "Other people were having the hand tuned walks and kicks for each of those we could just repeat this methodology."
        ],
        [
            "For each of them, and show that you know that we can affectively learn and perform well with any of those robot."
        ],
        [
            "Types.",
            "Using a lot of computation in this, I have to admit a lot of computation in this context in this setting, so this is not as nearly as sample efficient as text floor.",
            "There were 700,000 parameter sets evaluated for learning each of these types.",
            "It would have taken 1 1/2 years of compute time if we didn't have our cluster, which allowed us to do this in about 50 hours per type.",
            "But then we were able to look at the result in the competition.",
            "We beat the each of the other teams in the real competition.",
            "Winning by a combined score of 52 to nothing.",
            "We then played."
        ],
        [
            "1000 games against each of them and one all of them, but 67 which we which we tide and that sort of showed.",
            "That that at least as of last summer, we were well ahead of the of the competition.",
            "Here's some highlights from the finals against the team from University of Miami.",
            "Here's the initial kickoff where the robot scored right away.",
            "And then you'll see some of the other.",
            "The other kicks that were used.",
            "Here's a kick off by the other team.",
            "Our goalie dove, 'cause it thought the ball was going to make it, but you'll see here the learned kick out to the side and another robot coming in and scoring.",
            "Now in this last year it was really just still.",
            "Mainly straight ahead kicks.",
            "We think that this year's competition we're going to start getting back to the point of like the two D simulator where there's really interesting behaviors to learn of where to kick in, where to move, and we're starting to work on that.",
            "We're in the process of working to that now.",
            "This time here we're in red.",
            "You'll see the other team goalie saved it, but we have another robot ready to finish off the.",
            "The."
        ],
        [
            "So."
        ],
        [
            "A lot more information."
        ],
        [
            "Here on our on our website.",
            "OK so I am cutting into my questions already.",
            "I'm just going to say one last thing about interaction.",
            "I've said we've done a lot of work on learning from interaction.",
            "The most recent is Sam Barrett's thesis on ad hoc teamwork, where we."
        ],
        [
            "And Katie Genter is also working in this.",
            "In this space we have recent papers."
        ],
        [
            "The idea of ad hoc teamwork is trying to learn, sort of like a pickup soccer game or a pickup basketball game where you're given teammates that you've never seen before and you need to try to enter it.",
            "Learn how to it."
        ],
        [
            "React with them and people are very good at this."
        ],
        [
            "We've built up a much more conventional sort of reinforcement learning system where.",
            "Learning, uh, we have the model of our teammates is both.",
            "Seated from past experience with past teammates, an learned on line from the current set of teammates that you're given in a in a model based reinforcement learning kind of setting and then using a again a UCT kind of a Monte Carlo tree search planning approach to try to see what behaviors I could do that with interact well with the behaviors to the best of my knowledge of my of my current teammates."
        ],
        [
            "There's a lot of details that I'm not going to talk about, but we've tested this both in the RoboCop setting where we remove one player from a whole bunch of teams and we have to fill in as the as the green one on that yellow team and learn how to behave differently for different teams.",
            "We've also in this predator prey domain where a whole bunch of students created coherent predators, the red agents, and then we removed one of them from each of their teams.",
            "That's the one with the star and had our agent try to learn to quickly adapt to whatever team it was put on, again using a.",
            "Model based reinforcement learning approach."
        ],
        [
            "Um, I'm not.",
            "You know, there's a lot more details there that could be a full full hour talk, but I'm just going to give you that one."
        ],
        [
            "Slide, so I'll.",
            "But you know the the main.",
            "The main point is that we've been looking over the course of many years in my lab at many different tools or many different ways to make reinforcement learning practical that take into account representation, interaction, synthesis and more."
        ],
        [
            "Alaji I've especially talked about text floor and layered learning and with that, thank you for your attention and be more than happy to take questions.",
            "Thanks for the great talk, so I want to ask about layered learning and what we can learn from that to psychology and neuroscience.",
            "So specifically I'm thinking so my understanding of layered learning is you learn each one skill on top of other skills.",
            "So what was the trick to make them talk together without messing up with each other?",
            "Because that might help us understand how you know the brain through development and through life learns different skills, right?",
            "So in this case they're not messing with each other because they're they're different parameter sets that they're being learned, right?",
            "So we're not using the same the same.",
            "Neural net or function approximator or whatever.",
            "To learn the kicking behavior in The Walking behavior, there's more of a high level switch that says what are the parameters for walking in one of the behavior.",
            "The parameters for learning the key is how to make them so I don't.",
            "I'm not making a claim about, you know whether whether this is the way it would be done in the brain, whether there would be a separate sort of conceptually separate parameters, or whether the same parameters need to be shared across behaviors, but we're assuming in at least in this case.",
            "Which we can, which we can do in a computer science setting is that those those parameters are independent, so we don't.",
            "When we change the kick parameters, it doesn't affect the walk parameters or the different behaviors.",
            "The key is more.",
            "How do we make them interact?",
            "So when you switch from one to another, how do they not interfere?",
            "So what I'm taking home from this is kind of a super modularity.",
            "Which when you think of many more behaviors, what coming next at the expense may be over generalization between behaviors.",
            "And then maybe you'd want to start.",
            "Combining them again again, thinking of an architecture that can do a lot more than play soccer, right?",
            "And I should say it's not that there's no generalization.",
            "I mean there is for the for the different parameter set for the different walking.",
            "We have the same.",
            "Architecture the same set of parameters, but the actual parameter values differ for each of them.",
            "So once we learned once, we figured out that double inverted pendulum model, then that gets reused among all of them, and it's just the actual parameter values that get changed.",
            "So there is and I should say.",
            "We also don't start from scratch each time we take our currently learned behavior and try to augment from there.",
            "So transfer learning is one of the things we play a lot around with, but I think at the highest level you can think of them as being independent for now, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to talk today about but practical reinforcement learning in college, and I have sort of my acronym, PRISM representation, Interaction, synthesis, immortality and I could have given a talk.",
                    "label": 1
                },
                {
                    "sent": "I think this is founded on talk to they gave it the European Workshop on Reinforcement learning a couple years ago, where I talked about 8 different things for five minutes each.",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to do that today.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you what those 88 are now.",
                    "label": 0
                },
                {
                    "sent": "There's nine or ten of them.",
                    "label": 0
                },
                {
                    "sent": "Things were and then go deeply on two of them.",
                    "label": 0
                },
                {
                    "sent": "Um, but it's still the structure is sort of what I've been doing in my in my lab over the past.",
                    "label": 0
                },
                {
                    "sent": "Decade and a half or so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And is trying to think of how we can start using how we can use reinforcement learning as a tool.",
                    "label": 1
                },
                {
                    "sent": "Usually historically when we've thought about reinforcement learning, we thought.",
                    "label": 1
                },
                {
                    "sent": "Well what if we have a finite MDP?",
                    "label": 0
                },
                {
                    "sent": "You know what's the?",
                    "label": 0
                },
                {
                    "sent": "What's the optimal way to solve it?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then often we've said, well, OK, in the real world, there's actually stayed aliasing, and there's actually generalization, but let's just pretend it's a finite MDP and apply the same methods and see what happens now.",
                    "label": 0
                },
                {
                    "sent": "Of course, that's not the whole story, and we've done made a lot of progress, but but that's sort of often the way we think.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead, we've started saying, let's just embrace all of the complexity of the world and and try to develop algorithms that do work so well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are asking.",
                    "label": 0
                },
                {
                    "sent": "At least the theme of this talk will be we're asking, not should reinforce.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning, but does it?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work and when not, how can we make it work now this is an oversimplification.",
                    "label": 0
                },
                {
                    "sent": "There's been research in my lab starting from the algorithms in the theory that we've as well, but this is going to be emphasizing the more.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The more practical side along four different dimensions.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've done work on representation, by which I mean, you know, trying to figure out how to select the algorithm to use for a given problem.",
                    "label": 0
                },
                {
                    "sent": "In parameterized domains, that was with Shivram, Kalyanam Krishnan, or adaptive representations.",
                    "label": 0
                },
                {
                    "sent": "Trying to figure out what the format of the function approximator should be on line.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've done a lot of thinking about interaction.",
                    "label": 0
                },
                {
                    "sent": "What if there's adversaries in the mix?",
                    "label": 0
                },
                {
                    "sent": "Some more sort of theoretical style work with Doran Chakraborty in repeated repeated game settings.",
                    "label": 0
                },
                {
                    "sent": "We have an algorithm called seamless that we that we proved was able to to optimally exploit an adversary if it was in the target class while still converging to self to equilibrium in self play and never being vulnerable to doing worse than a safety level.",
                    "label": 0
                },
                {
                    "sent": "In this sort of a pack sense.",
                    "label": 0
                },
                {
                    "sent": "We've looked at interacting with teammates that you haven't that you haven't interacted with before we call that ad hoc teamwork.",
                    "label": 0
                },
                {
                    "sent": "That's my most recent graduate.",
                    "label": 0
                },
                {
                    "sent": "Sam Barrett is now achievable.",
                    "label": 0
                },
                {
                    "sent": "He introduced an algorithm called plastic.",
                    "label": 0
                },
                {
                    "sent": "We've looked at Brad Knox is thesis was looking at interaction from people.",
                    "label": 0
                },
                {
                    "sent": "What if there's human input coming in as a reinforcement, learning as a signal, and Andrews talk?",
                    "label": 0
                },
                {
                    "sent": "Yesterday was sort of in that in that theme, so that's what I mean by some some interaction that we have to take take into account.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at synthesis of different algorithms.",
                    "label": 0
                },
                {
                    "sent": "What if you have different reinforcement learning algorithms?",
                    "label": 0
                },
                {
                    "sent": "How can we put them together and get the best of both worlds?",
                    "label": 0
                },
                {
                    "sent": "My thesis was introduced layered learning and actually I'm going to talk more about that today.",
                    "label": 0
                },
                {
                    "sent": "Nick Jong in his his thesis introduced an algorithm called fitted our Max Q which is you can maybe tell from the from the name was model based and and.",
                    "label": 0
                },
                {
                    "sent": "A hierarchical an worked in continuous settings.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then finally mortality.",
                    "label": 0
                },
                {
                    "sent": "It is the other, the sort of last part of this transfer, learning that Taylor's work on leveraging the past because you have a sort of I'm calling it mortality because you have you have to, embracing the idea that the agent has a limited amount of time to live in the environment.",
                    "label": 1
                },
                {
                    "sent": "And that's really the motivation behind Todd Hesters work.",
                    "label": 0
                },
                {
                    "sent": "Also, the text blur, acknowledging a finite future.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to really tell you what I mean by that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2nd, in fact, that's the first of all these things.",
                    "label": 0
                },
                {
                    "sent": "That's the first deep dive I'm going to do is on this algorithm text floor then.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will talk about our recent advances in layered learning.",
                    "label": 0
                },
                {
                    "sent": "I said introduce that in my thesis.",
                    "label": 0
                },
                {
                    "sent": "We've actually made a big jump forward in that in the past year and then time permitting I'll have one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Side of this ad hoc teamwork aspect.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I'm going to talk about text lore, real time, sample efficient reinforcement learning for robots.",
                    "label": 1
                },
                {
                    "sent": "This is with my student at Todd Hester who's now at Nest.",
                    "label": 0
                },
                {
                    "sent": "The main article on this appeared in Machine Learning a couple of years ago.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you know there's been talk here.",
                    "label": 0
                },
                {
                    "sent": "There always is this sort of a meeting between model free and model based methods.",
                    "label": 0
                },
                {
                    "sent": "The motivation for this is that.",
                    "label": 0
                },
                {
                    "sent": "You know, at the most abstract level models, free methods.",
                    "label": 0
                },
                {
                    "sent": "Can run in real time, but are generally not very sample efficient.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And model based methods.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Can are much more efficient with data, but often take a lot more computation.",
                    "label": 0
                },
                {
                    "sent": "And Todd came in saying, well, you know, for a robot we really need both of those.",
                    "label": 0
                },
                {
                    "sent": "We need sample efficient to see an we need computational efficiency.",
                    "label": 0
                },
                {
                    "sent": "There had been some.",
                    "label": 0
                },
                {
                    "sent": "There had been some work on helicopter control was very, very impressive that Andrew Yang's group had done.",
                    "label": 0
                },
                {
                    "sent": "And Drew bagnell's group.",
                    "label": 0
                },
                {
                    "sent": "But it was unsatisfying to us that you had to learn the model from some data and then go off and compute for half an hour an hour before actually acting, and so the goal here was to try to create a algorithm that would allow learning online in real time.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The robot, and so that's what to do that we embrace.",
                    "label": 0
                },
                {
                    "sent": "What this concept that I'm calling mortality.",
                    "label": 0
                },
                {
                    "sent": "The idea that robots lifetime is short compared to the size of the world.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We embrace this as people, right?",
                    "label": 0
                },
                {
                    "sent": "We acknowledge that.",
                    "label": 0
                },
                {
                    "sent": "There's no, there's no possible way to to explore all the possible cities we might live in, or even the example I often uses.",
                    "label": 0
                },
                {
                    "sent": "There's a building I can see outside my office, and I've been in my office for years.",
                    "label": 0
                },
                {
                    "sent": "I've never gone to the third floor of that building.",
                    "label": 0
                },
                {
                    "sent": "In theory, my own personal Nirvana could be up there, right?",
                    "label": 0
                },
                {
                    "sent": "Or my favorite restaurant I'll never find in in my lifetime, but I've I'm aggressively deciding never to explore there.",
                    "label": 0
                },
                {
                    "sent": "And so by doing that I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm assuming the possibility of.",
                    "label": 0
                },
                {
                    "sent": "Of find guaranteeing myself the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "But in exchange I'm I'm heuristically sort of exploring to more distant, more distant areas in the world, and so we're trying to text floor is trying.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To embrace that.",
                    "label": 0
                },
                {
                    "sent": "Well still acting in.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All time, so this is what I just said.",
                    "label": 0
                },
                {
                    "sent": "We can't explore everywhere.",
                    "label": 0
                },
                {
                    "sent": "And so that the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of a motivating domain for us.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well, I'll show this working on a on a real robot, but sort of a testbed domain that we used was fuel world where most of the state space is very predictable.",
                    "label": 1
                },
                {
                    "sent": "The interior white squares you move right move in one of the eight directions, but the fuel stations which are the green and the bottom and the green on the top have varying costs and you can't move directly from the start, which is the blue to the goal, which is the red without running out of fuel.",
                    "label": 1
                },
                {
                    "sent": "So you need to go through one of those green fuel stations to refuel, but you don't know how they all have varying costs and so you mainly want to explore.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those those fuel stations and we all know what it would look like if we were using like you learning or our Max, they'd sort of exhaustively explore all of the states.",
                    "label": 0
                },
                {
                    "sent": "But here's what I'll just sort of show you.",
                    "label": 0
                },
                {
                    "sent": "The kind of exploration that text floor is doing, and then I'll tell you how it's doing this, but it's it is doing some exploration around the environment to sort of build its model of the internal dynamics, and then it's pretty quickly starts just exploring the parts that are uncertain to it, and there are parts of the environment over here that it never that never visits, even once, and even in the interior, even though in theory that as far as it knows, those could be.",
                    "label": 0
                },
                {
                    "sent": "Infinitely rewarding states.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And so we find it's focusing its exploration, but it's still.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ending near optimal Policy's the we also, then, and like I said, the motivation here was on robots we.",
                    "label": 0
                },
                {
                    "sent": "I implemented on a real autonomous car on a speed control task that I'll tell you about, but where it's where it's doing.",
                    "label": 0
                },
                {
                    "sent": "It's learning well, actually driving.",
                    "label": 0
                },
                {
                    "sent": "And in a in a very short amount of time.",
                    "label": 0
                },
                {
                    "sent": "And the speed can be, I'll tell you.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The task itself.",
                    "label": 0
                },
                {
                    "sent": "The state was that they was the current velocity, the desired velocity, the accelerator pedal position, and this was an old autonomous car, was in the DARPA Urban Challenge.",
                    "label": 0
                },
                {
                    "sent": "Our version of drive by Wire was there was actually a wire attached to the brake pedal that would get pulled by a motor and pull the brake in.",
                    "label": 0
                },
                {
                    "sent": "So we had the accelerator pedal position in the brake pedal position and its actions were to do nothing.",
                    "label": 1
                },
                {
                    "sent": "Increase or decrease the brake position by some percentage, or the accelerator position and the reward signal was just the.",
                    "label": 0
                },
                {
                    "sent": "The distance from the desired velocity and at the outset it knew nothing about the about the what would happen if you pressed any.",
                    "label": 0
                },
                {
                    "sent": "If you took any of the actions.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we wanted here was that it needs to learn with very few actions.",
                    "label": 0
                },
                {
                    "sent": "It has to be sample efficient, has to act continually in real time, deal with the continuous.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eight in the delayed actions, and this is what I yeah.",
                    "label": 0
                },
                {
                    "sent": "This is what I brake pedal looked like.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've retired this autonomous car.",
                    "label": 0
                },
                {
                    "sent": "There's people doing it much better than we are now, but but this was it was a great testbed, and so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Try to achieve these desiderata.",
                    "label": 0
                },
                {
                    "sent": "We sort of did a survey.",
                    "label": 0
                },
                {
                    "sent": "Are there any algorithms that can do all?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's many that did.",
                    "label": 0
                },
                {
                    "sent": "Some of them that dealt with some.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These different aspects, but we we believe text blurs the first algorithm that could handle all of these four different or satisfy all of these four.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Desideratum.",
                    "label": 0
                },
                {
                    "sent": "We did make it publicly available as a Ros package and so for those of you who use robots, you can go out and download it and just and there's the interface.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To use it so.",
                    "label": 0
                },
                {
                    "sent": "So how do we achieve all four of these of these things?",
                    "label": 0
                },
                {
                    "sent": "Of course, there's many more details in the paper, and because I want to just in time management, I'm going to give just the overview of them, but.",
                    "label": 0
                },
                {
                    "sent": "To deal with.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Efficiency we do what you would think.",
                    "label": 0
                },
                {
                    "sent": "We take a model based approach.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning a factored model so a separate model to predict each next state feature and reward from the from the current state.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using decision trees.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But actually using a random forest model, and this is one of the keys to allow us to to find to find some states that we will never explore.",
                    "label": 0
                },
                {
                    "sent": "We're using the average predictions of M different decision trees where each one is built from a random subset of the experiences.",
                    "label": 1
                },
                {
                    "sent": "So we've saved all of these state action reward tools.",
                    "label": 0
                },
                {
                    "sent": "We've then taken some random partitions of those built a model from each of them.",
                    "label": 0
                },
                {
                    "sent": "And then each tree represents one hypothesis of the true dynamics.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Domains the domain and then the agent will act greedily with respect to the average model in by planning planning ahead in the in the average in the sort of average outcomes of the various models, and then act greedily with respect to that.",
                    "label": 0
                },
                {
                    "sent": "And what this?",
                    "label": 0
                },
                {
                    "sent": "It turns out that what this allows the agent to do is if there's a.",
                    "label": 0
                },
                {
                    "sent": "If there are states that may have disastrous consequences, and even one of the models the agent will never, never plan to go there, or if there are states where there's really no no predicted benefit like that third floor of the building that I can see outside my outside my office, the model will predict that there's probably nothing interesting that's going to happen that happen there.",
                    "label": 0
                },
                {
                    "sent": "The agent will never plan to explore there, but if there's some states with either high variance.",
                    "label": 0
                },
                {
                    "sent": "Or that don't have with or with high variance.",
                    "label": 0
                },
                {
                    "sent": "It will be more likely to plan there and.",
                    "label": 0
                },
                {
                    "sent": "Part that I don't have in here.",
                    "label": 0
                },
                {
                    "sent": "There are two exploration bonuses that are sort of in the Rmac style, one for for states that are regions of the state space that are completely unknown, so not individual states but, but there are exploration bonuses, both for novel novelty in for variance, and we define novelty instead of distance from distance in the in the.",
                    "label": 0
                },
                {
                    "sent": "State space representation from places that the agent knows.",
                    "label": 0
                },
                {
                    "sent": "In other words, in any case, this limits the agents exploration to the state actions that appear promising while avoiding those.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which have negative outcomes.",
                    "label": 0
                },
                {
                    "sent": "With regards to the real time aspect in text lore, we took the sort of the standard model based approach which would.",
                    "label": 0
                },
                {
                    "sent": "Update the model, then plan on the expected model and then return an action.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And broke this into a multi threaded architecture.",
                    "label": 0
                },
                {
                    "sent": "That has parallel threads for for acting, planning an model, learning where the action thread is happening at the whatever rate the robot requires and in the background the model learning is happening in the planning from the current state.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using a.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample based planning Monte Carlo tree search.",
                    "label": 0
                },
                {
                    "sent": "And then there are mutex law.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talks between these different threads so that you always have a consistent view of the data and are always planning and acting is on the the model and the plan that says as current as possible given the rate of the reaction and this allows we did some testing on this.",
                    "label": 0
                },
                {
                    "sent": "It allows to exploit also parallel parallel computing arc.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Textures.",
                    "label": 0
                },
                {
                    "sent": "To deal with continuous state, we didn't do anything special.",
                    "label": 1
                },
                {
                    "sent": "We just used a regression tree model rather than a.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the standard decision tree, we do discretize the state space for the value updates from UCT.",
                    "label": 0
                },
                {
                    "sent": "But then we still plan over the continuously valued state.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then finally, for actually delays.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We just augment the states in a fairly standard way to make it have the previous K actions that were taken.",
                    "label": 1
                },
                {
                    "sent": "Action delay in our domain was very important because we decide to pull on the break.",
                    "label": 0
                },
                {
                    "sent": "It takes several seconds, which is several time steps before the effect of that would happen.",
                    "label": 0
                },
                {
                    "sent": "So you need to store what actions have been taken in the past and the trees consider learned.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each delayed actions are relevant.",
                    "label": 0
                },
                {
                    "sent": "And UCT can plan over these augmented state action histories.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we then took our autonomous vehicle, which runs at 20 Hertz and allowed the agency to.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Provide commands at this frequency, embedded it in a standard Ras interface node.",
                    "label": 0
                },
                {
                    "sent": "Which as I said is is pub.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We available and they did a bunch of experiments.",
                    "label": 0
                },
                {
                    "sent": "First, in simulation, I'm not going to interest of time.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into detail on these experiments just to tell you for each of these different.",
                    "label": 0
                },
                {
                    "sent": "Peter, out of sample efficiency and computational efficiency, we found sort of the best competitive.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Competitor",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times and then tested text floor against each of these.",
                    "label": 0
                },
                {
                    "sent": "The details are in the paper, and textbook competes favorably.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Along each of these.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then we put it on the physical vehicle and asked you, does it, does it work in practice?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "The end result was that basically in two minutes of driving time.",
                    "label": 1
                },
                {
                    "sent": "So these are episode 10 second episodes and these are showing the reward meaning the distance from the desired velocity that the agent achieves by the end of the episode and after about 2 minutes or so it was able to get to achieve the five meters per second at basically the same level of accuracy that the tuned PID controller was able to do, which for those of use PID control requires a fair amount of manual effort.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what do you want?",
                    "label": 0
                },
                {
                    "sent": "But it ended up looking like was, here's the.",
                    "label": 0
                },
                {
                    "sent": "I have it in.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I thought I had the video that would show.",
                    "label": 0
                },
                {
                    "sent": "No, I'm not going to want to.",
                    "label": 0
                },
                {
                    "sent": "Didn't want to show that whole video.",
                    "label": 0
                },
                {
                    "sent": "I showed you already the car driving so.",
                    "label": 0
                },
                {
                    "sent": "Basically it was learning the within those 2 1/2 minutes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "With without any pauses for computation, and that was one of that was one of the keys for us.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's the whirlwind of text.",
                    "label": 0
                },
                {
                    "sent": "Or of course there's a lot more details, but like I said, I want to sort of do a dive on two different aspects.",
                    "label": 0
                },
                {
                    "sent": "The that was I think, but I do think this is very important.",
                    "label": 0
                },
                {
                    "sent": "This idea of equipping our algorithms with the ability to.",
                    "label": 0
                },
                {
                    "sent": "Aggressively generalize over the over the model so that they can actively decide where never to explore, and that's sort of the key motivation behind textbook.",
                    "label": 0
                },
                {
                    "sent": "I'm now going to pop back up and change gears entirely to synthesis, and this is to talk about one of our more recent advance.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is in layered learning and this is joint work with especially Patrick Macalpine, a PhD student in my lab, also an undergrad, Mike definite contributed contributed to this work and we have a paper at triple AI this past year describing the detail.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cause of this?",
                    "label": 0
                },
                {
                    "sent": "So layered learning was as I said, I introduced in my PhD thesis back in the late 90s.",
                    "label": 0
                },
                {
                    "sent": "It's for domains that are too complex for attractive, attractively mapping state features directly to the output.",
                    "label": 1
                },
                {
                    "sent": "So on a robot you know, think of the pixels in and the joints of the robot out.",
                    "label": 0
                },
                {
                    "sent": "Although people are starting to get toward to the point of being able to deal with that, I think you going all the way to joints on a robot is still too much to do in a monolithic way.",
                    "label": 1
                },
                {
                    "sent": "So layered learning assumes there's some hierarchical sub task decomposition given.",
                    "label": 0
                },
                {
                    "sent": "There's also starting to be research on how can we automatically find this test decomposition, but let's assume for now that that's an input to.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "It uses then learning to exploit data training.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doepfer the key idea.",
                    "label": 0
                },
                {
                    "sent": "The synthesis aspect of layered learning is that learning in one layer feeds directly.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into the next layer.",
                    "label": 0
                },
                {
                    "sent": "So you could imagine an agent taking the IT sensors to build a world state, using that to learn individual behaviors, freezing those learning, using those to learn multi agent behaviors all the way up to the high level goals.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when we introduced this there in my in my thesis, we did it in a simulated robot soccer domain where there were three layers, learned a sort of ball interception behavior that used in neural networks.",
                    "label": 0
                },
                {
                    "sent": "So just the ball was coming to an agent.",
                    "label": 0
                },
                {
                    "sent": "What angle should it go to try to intercept it?",
                    "label": 0
                },
                {
                    "sent": "Froze that and use that to to learn a passive valuation behavior.",
                    "label": 0
                },
                {
                    "sent": "So given that my teammates are using that learned interception behavior, what's the likelihood they'll receive the pacify pasta them at a given time given the configuration of all the other agents that was using a decision tree and then we introduced a new reinforcement learning algorithm called it teapot IRL, it was a distributed multiagent reinforcement learning algorithm that did the past selection.",
                    "label": 0
                },
                {
                    "sent": "Where to pass?",
                    "label": 0
                },
                {
                    "sent": "We deployed this in the two dimensional simulated robot soccer team.",
                    "label": 1
                },
                {
                    "sent": "Our agent team was here shown in red.",
                    "label": 0
                },
                {
                    "sent": "This was the finals of the 1999 combat.",
                    "label": 0
                },
                {
                    "sent": "Competition and they were using this layered learning and ended up winning the competition, scoring 110 goals and giving up none in eight games.",
                    "label": 0
                },
                {
                    "sent": "And here you see, then the then setting up for for a set play.",
                    "label": 0
                },
                {
                    "sent": "It almost scored one goal.",
                    "label": 0
                },
                {
                    "sent": "Here it's here.",
                    "label": 0
                },
                {
                    "sent": "It does succeed in scoring and the key here is that each of the agents was independent.",
                    "label": 0
                },
                {
                    "sent": "Deciding what deciding what to do on its own and using these learned layers.",
                    "label": 0
                },
                {
                    "sent": "And there were three learned layers.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At that time we then took two learned layers on to some real robots, so these were the left robots using a policy gradient.",
                    "label": 0
                },
                {
                    "sent": "RL method were learning to walk back and forth across the field autonomously.",
                    "label": 0
                },
                {
                    "sent": "They were searching in a 12 dimensional continuous parameter space of what should be the height of their body and how fast they should move.",
                    "label": 0
                },
                {
                    "sent": "Their legs timing themselves based on the size of those beacons, and they ended up with a walk that was about 15% faster than anybody had been able to generate on these robots.",
                    "label": 0
                },
                {
                    "sent": "We then took that behavior and used it to learn a ball control behavior where given that walk they were trying to, the robot was trying to learn how to control the ball under its chin so they could turn with it and then pass it here.",
                    "label": 0
                },
                {
                    "sent": "This is before learning.",
                    "label": 0
                },
                {
                    "sent": "At the end it went from it, went from about a 40% success rate to a 70% success rate and being able to capture the ball and here they are again just two layers connected to one another.",
                    "label": 0
                },
                {
                    "sent": "There's a successful trial.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And these were using and some other people had had, I think put four layers together as well in various tasks.",
                    "label": 0
                },
                {
                    "sent": "Had people who had picked up layered learning.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "The layered learning that we had looked at had two different types of paradigms.",
                    "label": 0
                },
                {
                    "sent": "The one with sequential layered learning.",
                    "label": 1
                },
                {
                    "sent": "The one I described, where you learn one layer, freeze that freeze all those parameters, and then use that to learn the second layer.",
                    "label": 0
                },
                {
                    "sent": "That had and then another variant that we introduced shortly after we called concurrent layered Learning which I depict on the right where you would learn the first behavior, but then keep learning the first one while you're learning the second one.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Both of those are limiting in some sense, though they can be both.",
                    "label": 0
                },
                {
                    "sent": "Two limiting in the joint layer of the sequential layer learning could could be limiting in terms of what you learn at the lowest levels and the concurrent layered learning increases the dimensionality of learning and can make learning quite slow.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My student Patrick was trying to use these in a new domain, which I'll show you.",
                    "label": 0
                },
                {
                    "sent": "And found that they were that he needed to refine the concept significantly.",
                    "label": 0
                },
                {
                    "sent": "He introduced overlapping layered learning.",
                    "label": 1
                },
                {
                    "sent": "Which basically creates a tradeoff.",
                    "label": 0
                },
                {
                    "sent": "It optimizes Asim or an overlap between the behave.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In three different ways so.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they're sort of depicted schematically here.",
                    "label": 0
                },
                {
                    "sent": "Overlap are combining independently learned behaviors, learns behaviors A&B and then freezes most of them, but learns just a few parameters at the at the scene.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Partial concurrent, layered learning learns the first behavior, learns the second one while continuing to learn only part of some of the parameters of the of the first behavior.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, previously learned layer refinement will learn the first behavior, then learn the second behavior, then freeze the second one and relearn the first one.",
                    "label": 0
                },
                {
                    "sent": "You can obviously do this multiple multiple times.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the domain that we were using that sort of pushed us to do this is now a new simulation or the more recent simulation in RoboCop, the 3D simulation domain where you have teams of 11 autonomous robots playing in a ody physics engine.",
                    "label": 0
                },
                {
                    "sent": "They're modeled after the Elder on now Robot.",
                    "label": 0
                },
                {
                    "sent": "We also have some of those real robots and the robots receiving noisy visual information about the environment so noisy distance and angle tell their agents and they can communicate this.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of what the domain looks like because it's in a physics engine.",
                    "label": 0
                },
                {
                    "sent": "They do fall over.",
                    "label": 0
                },
                {
                    "sent": "It's hard to even get them to stay on their feet and to find a walk that's that's robust and and stable and.",
                    "label": 0
                },
                {
                    "sent": "And I'll tell you a lot more about about how the agents work as I move on now we.",
                    "label": 0
                },
                {
                    "sent": "Created an agent that was able using layered learning with just a few learn layers we were able to create an agent that won the championship in 2011 and 2012.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "A a policy policy gradient sort of method called Cmas by Hanson.",
                    "label": 0
                },
                {
                    "sent": "That space was basically optimizing the parameters of a double inverted linear pendulum model for the walk.",
                    "label": 0
                },
                {
                    "sent": "So the robot was basically using the equations of motion of a pendulum with pendulum, with the fulcrum being one of the one of the feet swinging over that to a double support phase, and then using the other foot is a fulcrum and again swinging like a pendulum and this left open a whole bunch of different parameters, such as how quickly should the center of mass or how much did the center of mass moves from side to side as you're doing this.",
                    "label": 0
                },
                {
                    "sent": "How long should the steps be?",
                    "label": 0
                },
                {
                    "sent": "How fast should the steps be?",
                    "label": 0
                },
                {
                    "sent": "10s of different continuous parameters that needed to be optimized?",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Within this model.",
                    "label": 0
                },
                {
                    "sent": "And so we did this using CMAS, which is basically a stochastic derivative free numerical optimization method where the candidates are sampled from a multi.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No Gaussian, it's sort of a step after cross entropy methods where it's not just adjusting the mean of the distribution for the next samples to test but also the covariance update controls the search step size and again this is not our methods from Hanson 09, but schematically.",
                    "label": 0
                },
                {
                    "sent": "It's testing a bunch of of of individuals, then finding the most successful of them, updating the distribution to select for the next.",
                    "label": 0
                },
                {
                    "sent": "The next set and basically honing in on a local optimum so it's not going to.",
                    "label": 0
                },
                {
                    "sent": "It's not going to converge to a global optimum, but will will come to a local.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimum.",
                    "label": 0
                },
                {
                    "sent": "And using this method we create, we took an initial walk from the from a real robot, ported it into the simulator and it worked.",
                    "label": 0
                },
                {
                    "sent": "Sort of like this sort of slow and lumbering.",
                    "label": 0
                },
                {
                    "sent": "Um, but then we decided we tried to optimize it and actually, in 2010 we optimized it just for straight ahead speed and came up with the walk that was about twice as twice as fast as anybody else.",
                    "label": 0
                },
                {
                    "sent": "We got very excited and then started to try to embed it in a behavior where it would use the very fast walk, but then have to slow down when it got to the ball.",
                    "label": 0
                },
                {
                    "sent": "When it did that.",
                    "label": 0
                },
                {
                    "sent": "This is sort of what happened.",
                    "label": 0
                },
                {
                    "sent": "The transition between behaviors didn't workout very well, so this is where layered learning came in where we had the agents.",
                    "label": 0
                },
                {
                    "sent": "Over its head is the different parameter sets it learned.",
                    "label": 0
                },
                {
                    "sent": "the T is for going to target, in essence for sprinting where it learned one of them first, throws it, and then learn the other one.",
                    "label": 0
                },
                {
                    "sent": "In the context of this sort of obstacle course where it's always trying to to chase the purple dot and come up with a set of parameters that not only work moves quickly, but also transitions well from one behavior to the other.",
                    "label": 0
                },
                {
                    "sent": "And then added a third behavior for positioning near the ball.",
                    "label": 1
                },
                {
                    "sent": "So we had it.",
                    "label": 0
                },
                {
                    "sent": "The P is the positioning behavior.",
                    "label": 0
                },
                {
                    "sent": "So again, this is through the three layered learning.",
                    "label": 1
                },
                {
                    "sent": "It's just learning one at a time and we're testing it on how quickly it's able to dribble the ball forward, which is more than just the straight ahead walk.",
                    "label": 0
                },
                {
                    "sent": "It's actually the behavior that's needed in the actual environment.",
                    "label": 0
                },
                {
                    "sent": "And we came up with the with the final walk that.",
                    "label": 0
                },
                {
                    "sent": "Wouldn't work on the on the real robot 'cause the physics simulator is not is not perfect.",
                    "label": 0
                },
                {
                    "sent": "The legs can move nearly that quickly, but the goal was to actually work in simulation.",
                    "label": 0
                },
                {
                    "sent": "This is the only work I know of that actually uses a real robot control policy to help us simulator rather than the other way around, but it ended up with something that was it was robust enough that we could put it into the.",
                    "label": 0
                },
                {
                    "sent": "In 2011 we were able to put it into the competition.",
                    "label": 0
                },
                {
                    "sent": "Agent scored 136 goals and gave up zero and never passed the ball in the competition.",
                    "label": 0
                },
                {
                    "sent": "Basically just walked the ball into the goal.",
                    "label": 0
                },
                {
                    "sent": "So a lot of our goals were just scored by by dribbling the ball into the into the goal.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "This was a team that wasn't.",
                    "label": 0
                },
                {
                    "sent": "This wasn't much of a competition in that set instead, but that was great.",
                    "label": 0
                },
                {
                    "sent": "In 2011 we published this in 2012.",
                    "label": 0
                },
                {
                    "sent": "The other agents started doing the same thing, and by 2013 we actually we lost in the finals.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we sort of went back to the drawing board and said, OK, this was.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can't just dribble the ball into the goal.",
                    "label": 0
                },
                {
                    "sent": "We're going to need to build up more skills, and this is where Patrick came up with.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A set of 19 different learned behaviors that all interact with one another in a very complex way using layered learning that use these different types of overlapping, layered learning and over there there's some that are getting up and some that are kicking with different kicking, faster kicking, lower kicking high.",
                    "label": 1
                },
                {
                    "sent": "There's a kickoff behavior you don't have to understand all of the numbers here, but they're the sort of the numbers of parameters that are optimized for each of the parameters, or for each of the behaviors are in parentheses.",
                    "label": 0
                },
                {
                    "sent": "The numbers that are passed from one behavior to another are shown in red.",
                    "label": 0
                },
                {
                    "sent": "The seated behaviors for the different types of overlapping layered learning are shown and look at their color coded based on weather, which flavor of overlapping layer learning?",
                    "label": 0
                },
                {
                    "sent": "They're so this was not this 19 behavior hierarchy was not learned automatically, but given it we were.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Able to learn a much more complex behavior and I'll just sort of give you some insight into how it looks.",
                    "label": 0
                },
                {
                    "sent": "The approaching the ball to kick added a layer of this sort of a behavior where it had to get the robot into position where it would be able to kick, and then we combine that with a with a fixed kick where the robot was just practicing trying to kick the ball as far as it could, but from a from a fixed position where we were the robot was starting from exactly the same location, so this is an example of combining independently learned behaviors.",
                    "label": 0
                },
                {
                    "sent": "It learn to approach separately, learn to kick separately.",
                    "label": 0
                },
                {
                    "sent": "And then we had to try to combine them.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And so we did that by having it now be beamed to a place that's not right in front of the ball and have it try to walk and approach.",
                    "label": 0
                },
                {
                    "sent": "And the robot was able to to learn to kick the ball.",
                    "label": 0
                },
                {
                    "sent": "Fairly far and.",
                    "label": 0
                },
                {
                    "sent": "And then the final walk in kick.",
                    "label": 0
                },
                {
                    "sent": "You can see the four different behaviors up here.",
                    "label": 0
                },
                {
                    "sent": "The Sprint, the position that go to target, and then finally the approach when it gets to a position where it wants to kick the ball, which is here and then it was able to execute the kick finally.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we also had this.",
                    "label": 0
                },
                {
                    "sent": "There was this sort of off to the side kick off behavior.",
                    "label": 0
                },
                {
                    "sent": "Where we had.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can't score directly from a kick off, so we had the robot learn to initially just on the kickoff.",
                    "label": 0
                },
                {
                    "sent": "Try to try to score and it would have been fine, but that wouldn't have counted because you're not allowed to score without another robot kicking it.",
                    "label": 0
                },
                {
                    "sent": "So we had another robot learn to touch the ball just a just a little bit without moving it very far.",
                    "label": 1
                },
                {
                    "sent": "This is actually we held this in.",
                    "label": 0
                },
                {
                    "sent": "We held this in reserve.",
                    "label": 0
                },
                {
                    "sent": "We didn't actually use this until the finals of the competition.",
                    "label": 0
                },
                {
                    "sent": "But if you just put those together naively, they interfere with one another and we did not choreograph this.",
                    "label": 0
                },
                {
                    "sent": "For a demonstration, we just put them together and saw what would happen and.",
                    "label": 0
                },
                {
                    "sent": "Not only that, the first robots appears to have been very frustrated with the second one.",
                    "label": 0
                },
                {
                    "sent": "Again, that just emerged, but um.",
                    "label": 0
                },
                {
                    "sent": "But then by using layered learning we were able to put them together and you can see here in the in the inset here how they actually work together.",
                    "label": 0
                },
                {
                    "sent": "The first one touches it just a little bit.",
                    "label": 0
                },
                {
                    "sent": "The second one kicks it and in the finals we were winning one.",
                    "label": 1
                },
                {
                    "sent": "Nothing after a second and sort of the other team sort of look that way with what just happened.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So in RoboCop we can then not only see what how the how the competition turns out, but we can.",
                    "label": 0
                },
                {
                    "sent": "We can then, especially in the simulation Lee we can do much more control.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The experiments, and so we were able to see.",
                    "label": 0
                },
                {
                    "sent": "Well, you know what happens if we had just dribbled versus using the 19 different learn, learn layers against the top three teams from 2013.",
                    "label": 1
                },
                {
                    "sent": "The number here is the gold different.",
                    "label": 1
                },
                {
                    "sent": "The average goal difference over 1000 games and we were able to show that the real impact of putting adding the kick in the kickoff into the.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the team.",
                    "label": 0
                },
                {
                    "sent": "But the real win here was that there are multiple different heterogeneous robot types in the.",
                    "label": 0
                },
                {
                    "sent": "In the competition I showed you everything with the Type 01, but there's some that have longer legs, quicker moving legs.",
                    "label": 0
                },
                {
                    "sent": "Other people were having the hand tuned walks and kicks for each of those we could just repeat this methodology.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For each of them, and show that you know that we can affectively learn and perform well with any of those robot.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Types.",
                    "label": 0
                },
                {
                    "sent": "Using a lot of computation in this, I have to admit a lot of computation in this context in this setting, so this is not as nearly as sample efficient as text floor.",
                    "label": 0
                },
                {
                    "sent": "There were 700,000 parameter sets evaluated for learning each of these types.",
                    "label": 0
                },
                {
                    "sent": "It would have taken 1 1/2 years of compute time if we didn't have our cluster, which allowed us to do this in about 50 hours per type.",
                    "label": 0
                },
                {
                    "sent": "But then we were able to look at the result in the competition.",
                    "label": 0
                },
                {
                    "sent": "We beat the each of the other teams in the real competition.",
                    "label": 0
                },
                {
                    "sent": "Winning by a combined score of 52 to nothing.",
                    "label": 0
                },
                {
                    "sent": "We then played.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1000 games against each of them and one all of them, but 67 which we which we tide and that sort of showed.",
                    "label": 0
                },
                {
                    "sent": "That that at least as of last summer, we were well ahead of the of the competition.",
                    "label": 0
                },
                {
                    "sent": "Here's some highlights from the finals against the team from University of Miami.",
                    "label": 0
                },
                {
                    "sent": "Here's the initial kickoff where the robot scored right away.",
                    "label": 0
                },
                {
                    "sent": "And then you'll see some of the other.",
                    "label": 0
                },
                {
                    "sent": "The other kicks that were used.",
                    "label": 0
                },
                {
                    "sent": "Here's a kick off by the other team.",
                    "label": 0
                },
                {
                    "sent": "Our goalie dove, 'cause it thought the ball was going to make it, but you'll see here the learned kick out to the side and another robot coming in and scoring.",
                    "label": 0
                },
                {
                    "sent": "Now in this last year it was really just still.",
                    "label": 0
                },
                {
                    "sent": "Mainly straight ahead kicks.",
                    "label": 0
                },
                {
                    "sent": "We think that this year's competition we're going to start getting back to the point of like the two D simulator where there's really interesting behaviors to learn of where to kick in, where to move, and we're starting to work on that.",
                    "label": 0
                },
                {
                    "sent": "We're in the process of working to that now.",
                    "label": 0
                },
                {
                    "sent": "This time here we're in red.",
                    "label": 0
                },
                {
                    "sent": "You'll see the other team goalie saved it, but we have another robot ready to finish off the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot more information.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here on our on our website.",
                    "label": 0
                },
                {
                    "sent": "OK so I am cutting into my questions already.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to say one last thing about interaction.",
                    "label": 0
                },
                {
                    "sent": "I've said we've done a lot of work on learning from interaction.",
                    "label": 0
                },
                {
                    "sent": "The most recent is Sam Barrett's thesis on ad hoc teamwork, where we.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Katie Genter is also working in this.",
                    "label": 0
                },
                {
                    "sent": "In this space we have recent papers.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea of ad hoc teamwork is trying to learn, sort of like a pickup soccer game or a pickup basketball game where you're given teammates that you've never seen before and you need to try to enter it.",
                    "label": 0
                },
                {
                    "sent": "Learn how to it.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "React with them and people are very good at this.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've built up a much more conventional sort of reinforcement learning system where.",
                    "label": 0
                },
                {
                    "sent": "Learning, uh, we have the model of our teammates is both.",
                    "label": 0
                },
                {
                    "sent": "Seated from past experience with past teammates, an learned on line from the current set of teammates that you're given in a in a model based reinforcement learning kind of setting and then using a again a UCT kind of a Monte Carlo tree search planning approach to try to see what behaviors I could do that with interact well with the behaviors to the best of my knowledge of my of my current teammates.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a lot of details that I'm not going to talk about, but we've tested this both in the RoboCop setting where we remove one player from a whole bunch of teams and we have to fill in as the as the green one on that yellow team and learn how to behave differently for different teams.",
                    "label": 0
                },
                {
                    "sent": "We've also in this predator prey domain where a whole bunch of students created coherent predators, the red agents, and then we removed one of them from each of their teams.",
                    "label": 0
                },
                {
                    "sent": "That's the one with the star and had our agent try to learn to quickly adapt to whatever team it was put on, again using a.",
                    "label": 0
                },
                {
                    "sent": "Model based reinforcement learning approach.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um, I'm not.",
                    "label": 0
                },
                {
                    "sent": "You know, there's a lot more details there that could be a full full hour talk, but I'm just going to give you that one.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slide, so I'll.",
                    "label": 0
                },
                {
                    "sent": "But you know the the main.",
                    "label": 0
                },
                {
                    "sent": "The main point is that we've been looking over the course of many years in my lab at many different tools or many different ways to make reinforcement learning practical that take into account representation, interaction, synthesis and more.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alaji I've especially talked about text floor and layered learning and with that, thank you for your attention and be more than happy to take questions.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the great talk, so I want to ask about layered learning and what we can learn from that to psychology and neuroscience.",
                    "label": 0
                },
                {
                    "sent": "So specifically I'm thinking so my understanding of layered learning is you learn each one skill on top of other skills.",
                    "label": 0
                },
                {
                    "sent": "So what was the trick to make them talk together without messing up with each other?",
                    "label": 0
                },
                {
                    "sent": "Because that might help us understand how you know the brain through development and through life learns different skills, right?",
                    "label": 0
                },
                {
                    "sent": "So in this case they're not messing with each other because they're they're different parameter sets that they're being learned, right?",
                    "label": 0
                },
                {
                    "sent": "So we're not using the same the same.",
                    "label": 0
                },
                {
                    "sent": "Neural net or function approximator or whatever.",
                    "label": 0
                },
                {
                    "sent": "To learn the kicking behavior in The Walking behavior, there's more of a high level switch that says what are the parameters for walking in one of the behavior.",
                    "label": 0
                },
                {
                    "sent": "The parameters for learning the key is how to make them so I don't.",
                    "label": 0
                },
                {
                    "sent": "I'm not making a claim about, you know whether whether this is the way it would be done in the brain, whether there would be a separate sort of conceptually separate parameters, or whether the same parameters need to be shared across behaviors, but we're assuming in at least in this case.",
                    "label": 0
                },
                {
                    "sent": "Which we can, which we can do in a computer science setting is that those those parameters are independent, so we don't.",
                    "label": 0
                },
                {
                    "sent": "When we change the kick parameters, it doesn't affect the walk parameters or the different behaviors.",
                    "label": 0
                },
                {
                    "sent": "The key is more.",
                    "label": 0
                },
                {
                    "sent": "How do we make them interact?",
                    "label": 0
                },
                {
                    "sent": "So when you switch from one to another, how do they not interfere?",
                    "label": 0
                },
                {
                    "sent": "So what I'm taking home from this is kind of a super modularity.",
                    "label": 0
                },
                {
                    "sent": "Which when you think of many more behaviors, what coming next at the expense may be over generalization between behaviors.",
                    "label": 0
                },
                {
                    "sent": "And then maybe you'd want to start.",
                    "label": 0
                },
                {
                    "sent": "Combining them again again, thinking of an architecture that can do a lot more than play soccer, right?",
                    "label": 0
                },
                {
                    "sent": "And I should say it's not that there's no generalization.",
                    "label": 0
                },
                {
                    "sent": "I mean there is for the for the different parameter set for the different walking.",
                    "label": 0
                },
                {
                    "sent": "We have the same.",
                    "label": 0
                },
                {
                    "sent": "Architecture the same set of parameters, but the actual parameter values differ for each of them.",
                    "label": 0
                },
                {
                    "sent": "So once we learned once, we figured out that double inverted pendulum model, then that gets reused among all of them, and it's just the actual parameter values that get changed.",
                    "label": 0
                },
                {
                    "sent": "So there is and I should say.",
                    "label": 0
                },
                {
                    "sent": "We also don't start from scratch each time we take our currently learned behavior and try to augment from there.",
                    "label": 0
                },
                {
                    "sent": "So transfer learning is one of the things we play a lot around with, but I think at the highest level you can think of them as being independent for now, thanks.",
                    "label": 0
                }
            ]
        }
    }
}