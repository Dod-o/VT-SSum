{
    "id": "o4xeejflg4ytdsthk5x5mp3v4ficy5ds",
    "title": "Parameter Estimation for the Latent Dirichlet Allocation",
    "info": {
        "author": [
            "Jaka \u0160peh, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Oct. 30, 2013",
        "recorded": "October 2013",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/sikdd2013_speh_dirichlet_allocation/",
    "segmentation": [
        [
            "We have a corpus corpus is set of documents.",
            "Documents have topics.",
            "We try to learn these topics.",
            "We have three algorithms how to do it and which one is the best.",
            "That's the question we will try to answer at our presentation or presentation.",
            "Is parameter estimation for the latent direct threat allocation and this is joint work with Andre Mortgage and the 100 Nick."
        ],
        [
            "So this talk is divided into 3 parts.",
            "We will talk about LDA model, what model is, what are the topics, how we mix everything together.",
            "Then we will talk about basic ideas about these two algorithms.",
            "And then we will compare them.",
            "OK, just let's start."
        ],
        [
            "LDA model."
        ],
        [
            "LDA is probabilistic graphical model and this is its presentation.",
            "LDA assumes that documents are built in some way.",
            "So how they are built?",
            "We have first here we have K topics.",
            "I call them corpus topics.",
            "And then we for each document.",
            "We first choose.",
            "Document topics and then for each word in a document.",
            "We first choose.",
            "A topic of word according to this probability.",
            "And then we choose sword.",
            "Based on the topic and the corpus topics.",
            "And what algorithms are trying to find this this thing?",
            "This is the corpus topics and this thing.",
            "This is the documents topics."
        ],
        [
            "If previous explanation was not so clear enough, we have K topics.",
            "And have the words.",
            "W1W2, WV and topics are probability distribution over words, so columns must sum to one.",
            "And word one and word 2 have high probability in first topic and for example word 5 and word 6 have local probability in first topic and hear word 6 in Word 7 have low probability in second topic."
        ],
        [
            "Now we look at documents.",
            "We have M documents.",
            "And this.",
            "Discolors are topics of these documents and this is again probabilistic distribution.",
            "So rows must sum to one and for example, last document has very low portion of red topic and six document has very high portion of Red topic.",
            "And now."
        ],
        [
            "How we generate the whole corpus?",
            "For example, we choose one document, let's say document one.",
            "And for the 1st place here will be the work.",
            "We randomly choose which topic will be in this place, so we here randomly chose the blue one.",
            "This is the error.",
            "So now from the blue topic.",
            "We randomly choose.",
            "Word from this topic.",
            "OK, and according to these probabilities.",
            "So."
        ],
        [
            "We choose word tree because it has high probability.",
            "For example, word four is unprovable in this.",
            "Probability and now we move forward."
        ],
        [
            "For another spot we randomly choose with respect to this probability.",
            "For example green topic.",
            "And now we do it again.",
            "We from.",
            "Topic distribution on this side we choose award."
        ],
        [
            "And for example here we choose the word 6.",
            "And what we're trying to do now?",
            "We assume that our corpus is built in such way.",
            "Well, and then we want to compute corpus topics and document topics.",
            "This is our goal."
        ],
        [
            "We have three algorithms.",
            "How we can do this?"
        ],
        [
            "There are collapsed Gibbs sampling, variational, Bayesian inference, and online variational Bayesian inference.",
            "And now we will present core ideas of this tree algorithms."
        ],
        [
            "First, collapse Gibson."
        ],
        [
            "And let's just start with an example.",
            "Let's say I have here 8 documents, the rozar documents and 1st document have 7 words.",
            "Second document has 11 words and so forth.",
            "What we first do is we're and."
        ],
        [
            "We assign topics for each word.",
            "We have here 6 topics from one till 2.",
            "And what not.",
            "What we do now."
        ],
        [
            "We compute probability.",
            "Off the topic of the first word probability distribution given everything else.",
            "So we have this probability distribution and we sample from it.",
            "And what we get we get a topic from one to six in our case.",
            "An we assign this topic.",
            "To this world."
        ],
        [
            "For example, we sample from this probability distribution tree and put tree here as a topic of first word.",
            "And what we do now we move forward."
        ],
        [
            "We look at the probability distribution of the topic of the second word given everything else.",
            "And sample from this distribution.",
            "And what we get?"
        ],
        [
            "This topic from one to six.",
            "Input topic.",
            "That we sampled.",
            "As the topic of the second word."
        ],
        [
            "And we do this until we go through whole corpus.",
            "And we do this multiple times until everything converges.",
            "Or after we reached a certain number of iterations."
        ],
        [
            "Now second algorithm is a bit different than the first one."
        ],
        [
            "If we look at our model.",
            "And if we look at this probability distribution, it's quite hard to compute so.",
            "We want to work with some simpler distribution.",
            "And this is the simpler distribution.",
            "So we want these two distributions to be.",
            "Estate is close as possible.",
            "An we measure closeness with Kullback Leibler divergent.",
            "And.",
            "Minimizing this diversions is equivalent as maximizing evidence lower bound.",
            "OK, these are some new stuff, OK?",
            "And.",
            "Our goal is to maximize this lower bound."
        ],
        [
            "How we do it?",
            "We have some documents para meters and we have some topics para meters topics means of corpus topics.",
            "So we assign everything randomly.",
            "Then we compute elbow of this para meters and this is elbow contributions from each document and each topics.",
            "And what we know that what we do now is we compute documents parameters.",
            "Holding topics parameters fixed.",
            "And we do it in a way that we maximize elbow as high as possible.",
            "So we."
        ],
        [
            "Recompute documents parameters so that we maximize elbow as high as possible.",
            "And what we do now, we do the same for topics.",
            "We compute topics para meters holding documents parameters fixed so that we maximize.",
            "Elbow as high as possible."
        ],
        [
            "An we interchange between between these two steps.",
            "So we do again for documents."
        ],
        [
            "And we do agree."
        ],
        [
            "For topics.",
            "And we doing for so long time until we see some elbow improvement until we get evidence, lower bound is getting higher and higher and higher when it stops getting higher, we stop doing it."
        ],
        [
            "And now we move at the last algorithm.",
            "And if we look at the last two algorithms as we just saw.",
            "They use a document multiple times.",
            "They do something with document and they do something with topics and then do again something with document but.",
            "This is not good for streaming, for example, because in streaming.",
            "You want to just look at the document, do something with it, and throw it away.",
            "And this is the idea of online variational inference by synonym."
        ],
        [
            "France.",
            "So we first have just some topics parameters.",
            "And we compute this evidence.",
            "Elbow elbow.",
            "According to these parameters.",
            "Then we get one document.",
            "And we compute parameter."
        ],
        [
            "Of this document to be elbow as high as possible just for first document.",
            "And then we recompute.",
            "Topics parameters because we change something in the documents."
        ],
        [
            "Ann Arbor Eyes is a bit more.",
            "And now we do the same for 2nd."
        ],
        [
            "Comment and we compute second documents parameters.",
            "Then we go.",
            "We, then we compute."
        ],
        [
            "Experimenter, so that we rise elbow as high as possible."
        ],
        [
            "So topics."
        ],
        [
            "No, we talked about basic idea of this tree algorithms and of course we want to compare them."
        ],
        [
            "And 1st, we compare them according to the time.",
            "So we here we have 10,000 documents, 20,000 documents still 80,000 documents.",
            "And here are the hours, 2 hours, 4 hours and 16 hours.",
            "And we see that Gibbs sampling and variational inference are pretty much the same.",
            "With respect to the time.",
            "And online variational inference.",
            "Is very low if we look at the time so, but this is quite reasonable because we here we look at one document just once and then throw them away.",
            "But here we look multiple times at the documents."
        ],
        [
            "And now.",
            "We will try to measure how good actually these algorithms are.",
            "So we use perplexity.",
            "If we have low perplexity, the better algorithm is.",
            "So if these two so lower, the better.",
            "And here we got some unfortunate results.",
            "With if we look at the article by Hoffman Blind Bar.",
            "This.",
            "Should be transposed like here.",
            "And we see that Gibbs sampling and variational bias are pretty much the same.",
            "But here it's getting worse and worse.",
            "Well, we have some.",
            "Explanations why this might be such as this, because this is only a bound because perplexity is hard to compute in general, so maybe this bound is just lying and we also have something like this for this one and the other is that they used vocabulary size.",
            "I think something 700 or 7000 words, but we use something around 35,000.",
            "So this is quite a huge difference.",
            "Maybe this inflects.",
            "So such.",
            "Difference.",
            "No we haven't."
        ],
        [
            "OK, everything that I said about algorithms, the code and some sample data.",
            "Some slides, more detailed slides than this.",
            "Sure.",
            "So.",
            "Some code.",
            "Slides report and also a demo.",
            "Is available on.",
            "This web page is called LDA dot IGS dot SI.",
            "And here we can put our document, press compute topics and.",
            "Topics are returned to this document.",
            "For example, if we.",
            "What is this?",
            "If we put some.",
            "Document here which talks about history of basketball press.",
            "Compute topics.",
            "We get topics like season, team, game and player game Bolkart and also if we click here show all topics.",
            "We get all topics that are available.",
            "We right now.",
            "So basically this document is somehow did you get topics from?",
            "Um, what's the question?",
            "Topics you got from some corpus, right?",
            "Yeah, from Corpus was Wikipedia I think about.",
            "I don't know some some portion of Wikipedia.",
            "I think it's here.",
            "Oh yeah, something so.",
            "So if I conclude everything first we look at LDA model.",
            "While this model represents what are topics?",
            "What are corpus topics, how everything interacts between between?",
            "Done, then we look at three algorithms.",
            "We showed the basic idea of these three algorithms.",
            "And at least we compare them.",
            "So thank you very much and visit this webpage."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have a corpus corpus is set of documents.",
                    "label": 0
                },
                {
                    "sent": "Documents have topics.",
                    "label": 0
                },
                {
                    "sent": "We try to learn these topics.",
                    "label": 0
                },
                {
                    "sent": "We have three algorithms how to do it and which one is the best.",
                    "label": 0
                },
                {
                    "sent": "That's the question we will try to answer at our presentation or presentation.",
                    "label": 0
                },
                {
                    "sent": "Is parameter estimation for the latent direct threat allocation and this is joint work with Andre Mortgage and the 100 Nick.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this talk is divided into 3 parts.",
                    "label": 0
                },
                {
                    "sent": "We will talk about LDA model, what model is, what are the topics, how we mix everything together.",
                    "label": 0
                },
                {
                    "sent": "Then we will talk about basic ideas about these two algorithms.",
                    "label": 0
                },
                {
                    "sent": "And then we will compare them.",
                    "label": 0
                },
                {
                    "sent": "OK, just let's start.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "LDA model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "LDA is probabilistic graphical model and this is its presentation.",
                    "label": 0
                },
                {
                    "sent": "LDA assumes that documents are built in some way.",
                    "label": 0
                },
                {
                    "sent": "So how they are built?",
                    "label": 0
                },
                {
                    "sent": "We have first here we have K topics.",
                    "label": 0
                },
                {
                    "sent": "I call them corpus topics.",
                    "label": 0
                },
                {
                    "sent": "And then we for each document.",
                    "label": 0
                },
                {
                    "sent": "We first choose.",
                    "label": 0
                },
                {
                    "sent": "Document topics and then for each word in a document.",
                    "label": 0
                },
                {
                    "sent": "We first choose.",
                    "label": 0
                },
                {
                    "sent": "A topic of word according to this probability.",
                    "label": 0
                },
                {
                    "sent": "And then we choose sword.",
                    "label": 0
                },
                {
                    "sent": "Based on the topic and the corpus topics.",
                    "label": 0
                },
                {
                    "sent": "And what algorithms are trying to find this this thing?",
                    "label": 0
                },
                {
                    "sent": "This is the corpus topics and this thing.",
                    "label": 0
                },
                {
                    "sent": "This is the documents topics.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If previous explanation was not so clear enough, we have K topics.",
                    "label": 0
                },
                {
                    "sent": "And have the words.",
                    "label": 0
                },
                {
                    "sent": "W1W2, WV and topics are probability distribution over words, so columns must sum to one.",
                    "label": 0
                },
                {
                    "sent": "And word one and word 2 have high probability in first topic and for example word 5 and word 6 have local probability in first topic and hear word 6 in Word 7 have low probability in second topic.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we look at documents.",
                    "label": 0
                },
                {
                    "sent": "We have M documents.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                },
                {
                    "sent": "Discolors are topics of these documents and this is again probabilistic distribution.",
                    "label": 0
                },
                {
                    "sent": "So rows must sum to one and for example, last document has very low portion of red topic and six document has very high portion of Red topic.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we generate the whole corpus?",
                    "label": 0
                },
                {
                    "sent": "For example, we choose one document, let's say document one.",
                    "label": 0
                },
                {
                    "sent": "And for the 1st place here will be the work.",
                    "label": 0
                },
                {
                    "sent": "We randomly choose which topic will be in this place, so we here randomly chose the blue one.",
                    "label": 0
                },
                {
                    "sent": "This is the error.",
                    "label": 0
                },
                {
                    "sent": "So now from the blue topic.",
                    "label": 0
                },
                {
                    "sent": "We randomly choose.",
                    "label": 0
                },
                {
                    "sent": "Word from this topic.",
                    "label": 0
                },
                {
                    "sent": "OK, and according to these probabilities.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We choose word tree because it has high probability.",
                    "label": 0
                },
                {
                    "sent": "For example, word four is unprovable in this.",
                    "label": 0
                },
                {
                    "sent": "Probability and now we move forward.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For another spot we randomly choose with respect to this probability.",
                    "label": 0
                },
                {
                    "sent": "For example green topic.",
                    "label": 0
                },
                {
                    "sent": "And now we do it again.",
                    "label": 0
                },
                {
                    "sent": "We from.",
                    "label": 0
                },
                {
                    "sent": "Topic distribution on this side we choose award.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for example here we choose the word 6.",
                    "label": 0
                },
                {
                    "sent": "And what we're trying to do now?",
                    "label": 0
                },
                {
                    "sent": "We assume that our corpus is built in such way.",
                    "label": 0
                },
                {
                    "sent": "Well, and then we want to compute corpus topics and document topics.",
                    "label": 0
                },
                {
                    "sent": "This is our goal.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have three algorithms.",
                    "label": 0
                },
                {
                    "sent": "How we can do this?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are collapsed Gibbs sampling, variational, Bayesian inference, and online variational Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "And now we will present core ideas of this tree algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First, collapse Gibson.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's just start with an example.",
                    "label": 0
                },
                {
                    "sent": "Let's say I have here 8 documents, the rozar documents and 1st document have 7 words.",
                    "label": 0
                },
                {
                    "sent": "Second document has 11 words and so forth.",
                    "label": 0
                },
                {
                    "sent": "What we first do is we're and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We assign topics for each word.",
                    "label": 0
                },
                {
                    "sent": "We have here 6 topics from one till 2.",
                    "label": 0
                },
                {
                    "sent": "And what not.",
                    "label": 0
                },
                {
                    "sent": "What we do now.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compute probability.",
                    "label": 0
                },
                {
                    "sent": "Off the topic of the first word probability distribution given everything else.",
                    "label": 0
                },
                {
                    "sent": "So we have this probability distribution and we sample from it.",
                    "label": 0
                },
                {
                    "sent": "And what we get we get a topic from one to six in our case.",
                    "label": 0
                },
                {
                    "sent": "An we assign this topic.",
                    "label": 0
                },
                {
                    "sent": "To this world.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, we sample from this probability distribution tree and put tree here as a topic of first word.",
                    "label": 0
                },
                {
                    "sent": "And what we do now we move forward.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We look at the probability distribution of the topic of the second word given everything else.",
                    "label": 0
                },
                {
                    "sent": "And sample from this distribution.",
                    "label": 0
                },
                {
                    "sent": "And what we get?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This topic from one to six.",
                    "label": 0
                },
                {
                    "sent": "Input topic.",
                    "label": 0
                },
                {
                    "sent": "That we sampled.",
                    "label": 0
                },
                {
                    "sent": "As the topic of the second word.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we do this until we go through whole corpus.",
                    "label": 0
                },
                {
                    "sent": "And we do this multiple times until everything converges.",
                    "label": 0
                },
                {
                    "sent": "Or after we reached a certain number of iterations.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now second algorithm is a bit different than the first one.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at our model.",
                    "label": 0
                },
                {
                    "sent": "And if we look at this probability distribution, it's quite hard to compute so.",
                    "label": 0
                },
                {
                    "sent": "We want to work with some simpler distribution.",
                    "label": 0
                },
                {
                    "sent": "And this is the simpler distribution.",
                    "label": 0
                },
                {
                    "sent": "So we want these two distributions to be.",
                    "label": 0
                },
                {
                    "sent": "Estate is close as possible.",
                    "label": 0
                },
                {
                    "sent": "An we measure closeness with Kullback Leibler divergent.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Minimizing this diversions is equivalent as maximizing evidence lower bound.",
                    "label": 0
                },
                {
                    "sent": "OK, these are some new stuff, OK?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Our goal is to maximize this lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we do it?",
                    "label": 0
                },
                {
                    "sent": "We have some documents para meters and we have some topics para meters topics means of corpus topics.",
                    "label": 0
                },
                {
                    "sent": "So we assign everything randomly.",
                    "label": 0
                },
                {
                    "sent": "Then we compute elbow of this para meters and this is elbow contributions from each document and each topics.",
                    "label": 0
                },
                {
                    "sent": "And what we know that what we do now is we compute documents parameters.",
                    "label": 0
                },
                {
                    "sent": "Holding topics parameters fixed.",
                    "label": 0
                },
                {
                    "sent": "And we do it in a way that we maximize elbow as high as possible.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recompute documents parameters so that we maximize elbow as high as possible.",
                    "label": 0
                },
                {
                    "sent": "And what we do now, we do the same for topics.",
                    "label": 0
                },
                {
                    "sent": "We compute topics para meters holding documents parameters fixed so that we maximize.",
                    "label": 0
                },
                {
                    "sent": "Elbow as high as possible.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An we interchange between between these two steps.",
                    "label": 0
                },
                {
                    "sent": "So we do again for documents.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we do agree.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For topics.",
                    "label": 0
                },
                {
                    "sent": "And we doing for so long time until we see some elbow improvement until we get evidence, lower bound is getting higher and higher and higher when it stops getting higher, we stop doing it.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we move at the last algorithm.",
                    "label": 0
                },
                {
                    "sent": "And if we look at the last two algorithms as we just saw.",
                    "label": 0
                },
                {
                    "sent": "They use a document multiple times.",
                    "label": 0
                },
                {
                    "sent": "They do something with document and they do something with topics and then do again something with document but.",
                    "label": 0
                },
                {
                    "sent": "This is not good for streaming, for example, because in streaming.",
                    "label": 0
                },
                {
                    "sent": "You want to just look at the document, do something with it, and throw it away.",
                    "label": 0
                },
                {
                    "sent": "And this is the idea of online variational inference by synonym.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "France.",
                    "label": 0
                },
                {
                    "sent": "So we first have just some topics parameters.",
                    "label": 0
                },
                {
                    "sent": "And we compute this evidence.",
                    "label": 0
                },
                {
                    "sent": "Elbow elbow.",
                    "label": 0
                },
                {
                    "sent": "According to these parameters.",
                    "label": 0
                },
                {
                    "sent": "Then we get one document.",
                    "label": 0
                },
                {
                    "sent": "And we compute parameter.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this document to be elbow as high as possible just for first document.",
                    "label": 0
                },
                {
                    "sent": "And then we recompute.",
                    "label": 0
                },
                {
                    "sent": "Topics parameters because we change something in the documents.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ann Arbor Eyes is a bit more.",
                    "label": 0
                },
                {
                    "sent": "And now we do the same for 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comment and we compute second documents parameters.",
                    "label": 0
                },
                {
                    "sent": "Then we go.",
                    "label": 0
                },
                {
                    "sent": "We, then we compute.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experimenter, so that we rise elbow as high as possible.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So topics.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, we talked about basic idea of this tree algorithms and of course we want to compare them.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And 1st, we compare them according to the time.",
                    "label": 0
                },
                {
                    "sent": "So we here we have 10,000 documents, 20,000 documents still 80,000 documents.",
                    "label": 0
                },
                {
                    "sent": "And here are the hours, 2 hours, 4 hours and 16 hours.",
                    "label": 0
                },
                {
                    "sent": "And we see that Gibbs sampling and variational inference are pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "With respect to the time.",
                    "label": 0
                },
                {
                    "sent": "And online variational inference.",
                    "label": 0
                },
                {
                    "sent": "Is very low if we look at the time so, but this is quite reasonable because we here we look at one document just once and then throw them away.",
                    "label": 0
                },
                {
                    "sent": "But here we look multiple times at the documents.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "We will try to measure how good actually these algorithms are.",
                    "label": 0
                },
                {
                    "sent": "So we use perplexity.",
                    "label": 0
                },
                {
                    "sent": "If we have low perplexity, the better algorithm is.",
                    "label": 0
                },
                {
                    "sent": "So if these two so lower, the better.",
                    "label": 0
                },
                {
                    "sent": "And here we got some unfortunate results.",
                    "label": 0
                },
                {
                    "sent": "With if we look at the article by Hoffman Blind Bar.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Should be transposed like here.",
                    "label": 0
                },
                {
                    "sent": "And we see that Gibbs sampling and variational bias are pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "But here it's getting worse and worse.",
                    "label": 0
                },
                {
                    "sent": "Well, we have some.",
                    "label": 0
                },
                {
                    "sent": "Explanations why this might be such as this, because this is only a bound because perplexity is hard to compute in general, so maybe this bound is just lying and we also have something like this for this one and the other is that they used vocabulary size.",
                    "label": 0
                },
                {
                    "sent": "I think something 700 or 7000 words, but we use something around 35,000.",
                    "label": 0
                },
                {
                    "sent": "So this is quite a huge difference.",
                    "label": 0
                },
                {
                    "sent": "Maybe this inflects.",
                    "label": 0
                },
                {
                    "sent": "So such.",
                    "label": 0
                },
                {
                    "sent": "Difference.",
                    "label": 0
                },
                {
                    "sent": "No we haven't.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, everything that I said about algorithms, the code and some sample data.",
                    "label": 0
                },
                {
                    "sent": "Some slides, more detailed slides than this.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Some code.",
                    "label": 0
                },
                {
                    "sent": "Slides report and also a demo.",
                    "label": 0
                },
                {
                    "sent": "Is available on.",
                    "label": 0
                },
                {
                    "sent": "This web page is called LDA dot IGS dot SI.",
                    "label": 0
                },
                {
                    "sent": "And here we can put our document, press compute topics and.",
                    "label": 0
                },
                {
                    "sent": "Topics are returned to this document.",
                    "label": 0
                },
                {
                    "sent": "For example, if we.",
                    "label": 0
                },
                {
                    "sent": "What is this?",
                    "label": 0
                },
                {
                    "sent": "If we put some.",
                    "label": 0
                },
                {
                    "sent": "Document here which talks about history of basketball press.",
                    "label": 0
                },
                {
                    "sent": "Compute topics.",
                    "label": 0
                },
                {
                    "sent": "We get topics like season, team, game and player game Bolkart and also if we click here show all topics.",
                    "label": 0
                },
                {
                    "sent": "We get all topics that are available.",
                    "label": 0
                },
                {
                    "sent": "We right now.",
                    "label": 0
                },
                {
                    "sent": "So basically this document is somehow did you get topics from?",
                    "label": 0
                },
                {
                    "sent": "Um, what's the question?",
                    "label": 0
                },
                {
                    "sent": "Topics you got from some corpus, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, from Corpus was Wikipedia I think about.",
                    "label": 0
                },
                {
                    "sent": "I don't know some some portion of Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "I think it's here.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, something so.",
                    "label": 0
                },
                {
                    "sent": "So if I conclude everything first we look at LDA model.",
                    "label": 0
                },
                {
                    "sent": "While this model represents what are topics?",
                    "label": 0
                },
                {
                    "sent": "What are corpus topics, how everything interacts between between?",
                    "label": 0
                },
                {
                    "sent": "Done, then we look at three algorithms.",
                    "label": 0
                },
                {
                    "sent": "We showed the basic idea of these three algorithms.",
                    "label": 0
                },
                {
                    "sent": "And at least we compare them.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much and visit this webpage.",
                    "label": 0
                }
            ]
        }
    }
}