{
    "id": "gwxyvy3x6v6ybtrtshco4mj245goeyal",
    "title": "Towards Structured Output Prediction of Enzyme Function",
    "info": {
        "coauthor": [
            "Juho Rousu, Department of Computer Science, University of Helsinki"
        ],
        "published": "Nov. 20, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Bioinformatics->Computational Systems Biology"
        ]
    },
    "url": "http://videolectures.net/mlsb07_rousu_tso/",
    "segmentation": [
        [
            "OK, so so this is the last talk talk of today, so I hope you're still awake for half an hour or so.",
            "So my talk is about structured prediction of enzyme functions.",
            "This is George work.",
            "We've got the astic owner who is also here at the conference leads a whole mess up.",
            "It connects under shared market."
        ],
        [
            "Myself so.",
            "So the task is the following.",
            "I guess it's only after all of you.",
            "So given protein sequence that is supposed to be representing an enzyme, the goal is to predict.",
            "Function.",
            "That is, the biochemical reaction that is catalyzed by this enzyme.",
            "So.",
            "So this is not very very legible, but so this is 1 one answer button reaction where some metabolise reactants are converted to some products and and there is a piece of the protein sequence that that.",
            "Off the inside catalyzes this reaction.",
            "So motivation for this this whole thing of course is kind of starting from genome annotation.",
            "Motivation really is metabolic.",
            "Network reconstruction went to want to reconstruct.",
            "Network metabolic networks that that are kind of accurate, accurate as accurate as possible.",
            "And the reason to construct this metabolic network is is that it's kind of the one of the.",
            "Pieces you need in order to understand what goes inside inside the cell, so it's kind of.",
            "What central central theme?",
            "OK, so if you don't have any fancy machine learning tools that kind of the standard approach for inferring this function, I guess is to.",
            "Go with some plus some version of Flash to run database of enzymes.",
            "Find a similar sequence and predict that.",
            "This new production sequence will have the same function.",
            "Of course this.",
            "This doesn't give you any functions that are not in the database.",
            "You don't have any any mechanism to propose new new functions, so this is something that's.",
            "Important and also very challenging how to how to propose new functions?",
            "And this is also in the context of metabolic network reconstruction.",
            "That is kind of one of the missing pieces, because when you use these databases to infer your network is just.",
            "Text this whatever you can find there, you will end up with gaps in the network, so something some pieces missing, so your network actually is not capable of working properly.",
            "So this is."
        ],
        [
            "Something something we want to address.",
            "OK, so we have taken this structured output prediction approach.",
            "Where?",
            "The idea is that."
        ],
        [
            "Instead of.",
            "Trying to somehow convert is this prediction problem.",
            "From sequence to this function to some kind of binary machine learning problem really want to.",
            "Want to predict this is complex OPS."
        ],
        [
            "Act object this is reaction.",
            "How?",
            "So of course, the structured output prediction can be done in many ways.",
            "This is kind of the outline of a family of.",
            "The approach is based on SVM type type learning methods and we are using this framework.",
            "So what you do there you take your inputs and outputs so pairs of sequences and reactions.",
            "Your device some feature map, so some feature vectors representing this, this this object the sequences and and reactions and you make some kind of a joint feature feature map.",
            "This is the simplest way to do it.",
            "You do this kind of product features.",
            "You take 1 feature from the sequence and multiplied with the one feature from the from the.",
            "Output or reaction an you have this kind of set of.",
            "Product features that gone by the composers.",
            "Your joint feature feeds on map.",
            "And with this disjoint feature map you do Martin based learning of linear score function you have.",
            "Function of this kind.",
            "So you have this potentially pretty huge joint feature map and you just wait the features somehow.",
            "Possibly put some bias term in there so you have very simple score function, but it lives in a huge dimension office space.",
            "Can you?",
            "Because of this high dimensionality, you would want to use kernels for inputs and and.",
            "All support for the outputs.",
            "For making this learning efficient.",
            "The prediction.",
            "Is why are so called the pre image computation?",
            "So you go to your score function.",
            "And you look what is the maximising.",
            "What, which in this case which reaction you need to put here in order to maximize this score?",
            "This typically pretty.",
            "Nasty problems to solve in structured outputs, But anyway, this is the."
        ],
        [
            "Approaching want to take?",
            "This is 1 particular method that we have been using in this study.",
            "This is so called Max Martin regression.",
            "It's a algorithm.",
            "Under said mark.",
            "The idea is that.",
            "OK, you have.",
            "You have prediction.",
            "OK actually I have written this in different form and ask the previous slide.",
            "But here is your prediction.",
            "So this is you have a weight matrix.",
            "Waiting this input feature of extra, so that's something you have computed from your sequence.",
            "You have weight matrix and you've tried to push it as close to.",
            "The truth is true reaction, so you have a feature vector for the reaction and you want to push them as close to each other as possible.",
            "So this is written here, so have inner product.",
            "Between these guys and you want to.",
            "Get it as large as possible and this is something familiar from the support vector machines you have normally minimisation of the weight, so you want to recognize the weight somehow so that they don't get too large and you put some slack because you.",
            "I don't have an kind of linearly separable problem, OK?",
            "If some of you are aware of more of this structured output literature, so this is resembles quite closely to the.",
            "Set up of used by Corina Curtis and Co workers.",
            "The difference is that here, or regress an Israeli inner product maximization.",
            "So it means that we are trying to minimize the cosine angles of the feature vectors.",
            "Feature vector and and wait wait vector space rather than doing kind of help to type the distance, meaning says so.",
            "This is a point for you guys that are more deeply involved in this this.",
            "Research.",
            "But think about this, this MMR framework that's there is very extremely efficient optimization algorithm for for this model.",
            "So, so we can do structured output learning.",
            "With similar efficiency to SVM learning, and that's kind of one of the motivations to use this algorithm, and so it's much faster than many other of this structured output.",
            "Type algorithms."
        ],
        [
            "Right, so in this type of learning, of course the features are the key, So what kind of features are you going to use?",
            "This kind of decides how are you, how you will manage.",
            "So we will use.",
            "Quite obviously.",
            "Um?",
            "Features coming from kind of string kernel type type of family so.",
            "Um?",
            "So we have couple of.",
            "Either the contiguous substring so features, or whether some substring occurs in the sequence or not.",
            "And we can also put some gaps in the sequence so we can allow some some time number of gaps with some some link.",
            "The third.",
            "Peter set is perhaps more interesting to most of you is feature set that picks up corn syrup residue, so something that has been constant by evolution of the sequence.",
            "And I will talk about that.",
            "More a little bit into following.",
            "OK, Ann.",
            "Above this we want to use on different combinations so.",
            "So sums of of this kernels.",
            "That means that we make.",
            "Concatenates this features feature vectors to do each other, so have this summer some kernels.",
            "And then also.",
            "Put polynomial kernel on top of it, so which means that.",
            "Instead of picking individual features from this feature feature sets we combination so features."
        ],
        [
            "So this is this is.",
            "Need to shut OK?",
            "Something about this GT features so the idea of of behind this distribution features is ready to pick up this.",
            "This construct residues in sequences.",
            "How they how they do it till this or this is something made by by by Lisa Holman and coworkers so to have made this kind of huge alignment kind of pairwise alignment of 400,000 sequences so.",
            "So they have kind of.",
            "For each representative of the sequence they have.",
            "Made kind of pairwise alignment.",
            "So they have 150 million kind of this pairwise alignment scores and what they do.",
            "They will then go and look at individual residues and look at how there are aligned so they have huge amount of amount of residents that they look at and they put them in a single graph.",
            "And look like so so this guy is aligned with this guy.",
            "So for example.",
            "OK, so maybe it's.",
            "WY yeah OK so.",
            "And something like huaian.",
            "OK, so I know anyway you can.",
            "You can have have the idea that this is a huge graph where each of these individual.",
            "Price of Juice is represented and you have an edge.",
            "Whenever you have an alignment.",
            "There OK and what they do with this.",
            "They do crop clustering and look at the tight clusters or residues that align its order.",
            "Kind of inconsistent manner.",
            "And each each cluster that that comes out of this.",
            "This setting is a feature, so this is something that they said, since it aligns consistently with within a group group of residues, it must be conserved.",
            "So this is, this is so we take take these clusters as as as our features.",
            "So this is.",
            "This is how this GT features.",
            "Look like so the idea is that this controversy implies some kind of import hands.",
            "Either from function or structure point of view.",
            "So so this is the reason we want to look at the concert."
        ],
        [
            "Messages.",
            "OK, so.",
            "OK, that was the input.",
            "Features are sequence features, but we only also need to think about the output side.",
            "So since we are representing our outputs as feature vectors as well.",
            "So.",
            "So this is kind of 1 recent representation coming from a standard EC hierarchy of of of.",
            "Enzyme function, so this hierarchy is is 4 levels deep.",
            "Bus routes so you have to eat.",
            "Enzyme is kind of a path or the function of an enzyme is support in the.",
            "In the hierarchy, and if you have multiple function, you will have a discussion over parts that deviate somewhere.",
            "Um?",
            "OK, so this is something that that biologist or entomologist used all the time, so it's kind of a.",
            "A reasonable starting point, but of course it's limited in expressive power, so there is no real support to credit completely new function.",
            "So since those this here keys.",
            "For our power point of view, it's fixed of course sometimes something occurs there, but there is no real kind of.",
            "Support for machine learning.",
            "A method to propose new functions.",
            "OK, and how do you encode this to a feature vector valued?",
            "Just take either tease notes, individual notes of the hierarchy and both aspire bits in the relevant cost systems.",
            "Or you can do something bit more interesting.",
            "You take take the edges.",
            "I look at the labelings possible labelings of the edges so so if there is no label in parent and child or only label in parent and.",
            "There's something like this."
        ],
        [
            "OK, so so you have this kind of choices.",
            "OK, so we tried this this this.",
            "Thing with this kind of features at this prediction on start with the good news, or how did we?",
            "Managed.",
            "So.",
            "The first lesson is that OK, this is not.",
            "Hopeless so we can actually get pretty good good.",
            "Fictive accuracy, so this is these figures, here are Mike Label F1 scores so.",
            "Geometric average of precision and recall of individual easynote predictions so they can take this whole easy hierarchy and look at the individual nodes.",
            "How they are predicted and compute this F1 score.",
            "Based on based on this.",
            "OK, so so.",
            "Overall we're doing OK. And also we can see that OK. Did she see features?",
            "Disconcert residues alone work pretty well, but if you put some string kernel type.",
            "Information in you.",
            "Get something more.",
            "Here the comparison is in hierarchical multi label classic classifier to.",
            "So this is our implementation of the Max margin Markov networks for hierarchies.",
            "So it knows a little bit more more about hierarchies than this MMR that is completely general general structured output measures so.",
            "Perhaps not surprisingly, it's little bit better than 10 M and more, but not.",
            "Not not not much.",
            "OK, so so this is kind of."
        ],
        [
            "Goodness first but then.",
            "They notice some some peculiarities.",
            "So here I have something that you shouldn't try to.",
            "Read too carefully because you cannot see and then probably you don't understand Phineas either, so, but anyway.",
            "Here is what happens when we raised.",
            "Dikri of the polynomial kernel.",
            "What happens to the F1 score?",
            "So actually this is recall, but it's actually the same as F1 in this case, so you have this kind of monotone honestly.",
            "Rising curve so so no overfitting.",
            "When the polynomial Dick recalls, often that's surprising because we are exploding our feature space enormously and still be on all the time improving our prediction.",
            "So this is a bit peculiar and then started to wonder what happens.",
            "Because what happens in this case is that you will have fewer and fewer Seminick Anthony non zero elements in the in the kernel, so it starts to pick up sparse kernel and also it comes means that if you think about distances in this place you have fewer and fewer close neighbors proceed sequence.",
            "So this kind of led to a question.",
            "OK if we put on nearest neighbor classifier to working in the same space, how would it do?"
        ],
        [
            "And of course it works unknowingly well.",
            "So.",
            "So here is.",
            "Here is what what our nearest neighbor classifier doesn't actually beats us in.",
            "Every combination.",
            "Those of you who have read our paper carefully.",
            "Notice the.",
            "Look at different figures for this hierarchical classifier, but there actually was a normalization error so.",
            "So this figures.",
            "A little bit.",
            "Lower here than in the paper.",
            "OK, so any better lesson is that actually the nearest neighbor does very well in this case, so we don't still."
        ],
        [
            "They see much benefit off of our.",
            "Fancy structured output algorithms here so.",
            "I guess the next question is what is going on here?",
            "And here is a.",
            "List of possible explanations that we have.",
            "But made up and you can go and pick your favorite or.",
            "Invent a new one.",
            "The first, of course is OK. Maybe this whole thing is exhausted by this nearest neighbor thing, so we have so good features that with this 2D2 features that actually there's nothing else to do but count your constant residues and that's that's the whole thing.",
            "OK, so this is 1 explanation.",
            "We don't quite believe on that, so we want to think about the other explanations.",
            "One other explanation is that maybe most of the true functions in the easy hierarchy have been derived by some kind of blast nearest neighbors.",
            "So people plus.",
            "Against the database, look what is there and then go and possibly wet lab.",
            "Make a wet lab verification of the function.",
            "So this would of course mean that if there is something else, then the nearest neighbor tells you that would be missed by the true.",
            "Function, so your true function doesn't contain anything else.",
            "So then it's pretty hard to beat the nearest neighbor classifier, so this is 1 explanation of that might be true or.",
            "This is kind of speculative speculation.",
            "OK, the next one.",
            "Is that OK, maybe this easy hierarchy is not suitable for the whole thing, so maybe it doesn't support our kind of idea of of.",
            "Trying to generalize about about the function, so maybe we should look for another hierarchy that would be more suitable for the task.",
            "Or the 4th one.",
            "Is that OK?",
            "Maybe maybe this hierarchies are not rich enough representation for this task.",
            "But also maybe you should.",
            "Switch to complete the other representation.",
            "OK, so we have started to address this this later points."
        ],
        [
            "Recently and this is we found out this gold standard data set.",
            "That's has a different different hierarchy.",
            "It's based on super family, family relation relationships.",
            "It's actually much smaller than easy hierarchy.",
            "And also it's supposed to be very, very curated so that the functions really are what?",
            "What is, what is being included there.",
            "Um?",
            "There would make this comparation, so this is a depiction where we have this our training set an what happens when we so only 25% of the training set.",
            "Food algorithm or 5075 or everything we have.",
            "What happens and you can see that the MMR.",
            "Venues.",
            "Initially it works.",
            "Better than the nearest neighbor, but then the kind of the.",
            "Then you so more and more data at the nearest neighbor starts work better and better.",
            "So OK, this kind of supports our idea that we are able to generalize becausw form from lesser amount of training data we get.",
            "Get something.",
            "Out of it.",
            "So.",
            "We take this some coin over.",
            "Positive indication that there is something something that they do.",
            "Cite this nearest neighbor neighbor effect.",
            "OK, so this is this is wanting.",
            "But it's not very huge, so it's kind of splitting hairs.",
            "Really, if you talk to violate biologists.",
            "So so maybe we should do something that is right?"
        ],
        [
            "We got a different from this.",
            "OK, so this is kind of our idea that we need to go to watch more fine grained representation so.",
            "Because we see that OK.",
            "Whenever we have this close sequence neighbors in the data set, the nearest neighbor is just as good, if not better than this structured output models.",
            "However, our initial motivation was to be able to predict new function that are not in the in the in the taxonomy.",
            "So.",
            "We want to have some kind of representation for the function where this Inter Inter or expiration between functions becomes possible.",
            "On these hierarchies clearly are not not suitable for this, because there are fixed you, you don't have any any capabilities to success anything new, so it's it's clear that."
        ],
        [
            "That you need to do something else.",
            "So this is a direction we are actually going at the moment we want to.",
            "Want to measure the similarity of and semantic reactions now in a smooth manner.",
            "And how we do it is, while so-called reaction kernels.",
            "That we have a kind of proposed.",
            "So what we do there we start from any craft kernel for small molecules, so it can be apart kernel also subcraft spectrum.",
            "And they have this kernel matrix that contest this this.",
            "This kernel values for all pairs of relevant molecule, so so all molecules are bound to work which we have this this kernel for it.",
            "And then for each reaction we test construct an indicator vector.",
            "Indicating which which which reactions and products are part of the reaction.",
            "On this reaction, kernel is really simple.",
            "To compute, we have this matrix and compute this kind of.",
            "Product with this indicator vectors and and.",
            "For both 4th reaction.",
            "So basically it picks up this relevant sub metrics out of the kernel and sums everything off.",
            "So in essence this is just the sum of all against all pairwise similarities of this reactant product model case.",
            "So, so this is something where we want to.",
            "Want to do and are actually doing at the moment.",
            "But I don't have any results.",
            "Two to four from this approach as yet, but we hope to have something in.",
            "Real near future."
        ],
        [
            "OK, so this is this is.",
            "Last slide.",
            "So we we have present that react research towards kind of fine grained inside function prediction where we really want to predict this.",
            "Um?",
            "Reaction.",
            "In fine detail.",
            "What we have found out that when, when when we are predicting close sequence, neighbors are simple nearest neighbor classifier does as well or better.",
            "In this hierarchical set setting.",
            "We propose that this reaction kernels might be means to.",
            "Medic novel reactions that are not part of the existing taxonomies or databases.",
            "And that was the final.",
            "Comment is that so this whole set of experiments I showed you was really made possible by this very very efficient MMR.",
            "Training.",
            "Uh, let's us.",
            "Related to this structured output learning in the same time complexity S as classical SVM's.",
            "OK, so this was everything I had to say, so thank you for your attention.",
            "Right, you talk about outputs or not OK, we haven't thought about putting an Gaussian kernel on the outputs.",
            "Yeah, but of course it gets hairy when when you put very complicated.",
            "Colonel the preimage.",
            "How do you get back to the?",
            "Twitter to the reaction so.",
            "Typically you need to do something crude in the premix phase, so so either you call your training set, look what it is there, or you do some kind of local search in the neighborhood of of of kind of objects that look.",
            "Close to the OR you're kind of feature vector so.",
            "You would ideally would like to have kind of nice combinatorial algorithm to do that, but.",
            "This is pretty much open research, Phil.",
            "I don't think many of.",
            "All Guardians of this kind exist at the moment, so.",
            "Yeah, that's that's a good point.",
            "Repeat itself.",
            "Send it.",
            "So my question.",
            "Sorry I missed explore, yeah.",
            "Yes.",
            "Yes.",
            "State.",
            "We haven't really thought about optimizing the the.",
            "Kind of the lambdas, no, no, but that's one thing we could to do.",
            "Yeah, that's one.",
            "Backspace right?",
            "Right?",
            "Yeah, that's yeah, that's true.",
            "That's yeah, this yeah we should.",
            "We should maybe maybe look at this kind of kind of approach.",
            "Joe.",
            "Some numbers at the federal level so.",
            "Violation.",
            "Yes, these are five or five fold cross validation.",
            "Old.",
            "Oh I.",
            "Everybody is right.",
            "Right, well that's that's good point.",
            "Yeah, so all yeah and in the end this results are pretty close to its error so I wouldn't claim that there is going to be big winner to any any direction.",
            "But on the other hand, when we have this kind of large sets of different.",
            "Um?",
            "Different experiments and we see some common consistent trend between two methods so.",
            "Open questions about five years later, yeah?",
            "Yeah, yeah, that's that's true, but you would hope to kind of pay.",
            "Very clear of nearest neighbor when you do something more complicated, but because it's conceptually so, if so much more easy to do, nearest neighbor and then some.",
            "That we are trying to do here.",
            "This is really good.",
            "Affected.",
            "So the idea is that in the in, the kind of if you have this kind of reaction space.",
            "Where your training points are kind of.",
            "Scattered so you can hope that you can do some interpolation between your training points and sometimes there is.",
            "You know maybe that your test.",
            "Or your true new function is is there so that you can capture it by interpolation?",
            "When you reproduce.",
            "Yep.",
            "Oh yes, said you wouldn't like to go back to your training set, so that's kind of the if you cannot workout anything else you do that.",
            "Yeah, that's yeah, then you're not generalizing.",
            "If you take something from your training set, yeah, that's not right."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so this is the last talk talk of today, so I hope you're still awake for half an hour or so.",
                    "label": 0
                },
                {
                    "sent": "So my talk is about structured prediction of enzyme functions.",
                    "label": 1
                },
                {
                    "sent": "This is George work.",
                    "label": 0
                },
                {
                    "sent": "We've got the astic owner who is also here at the conference leads a whole mess up.",
                    "label": 0
                },
                {
                    "sent": "It connects under shared market.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Myself so.",
                    "label": 0
                },
                {
                    "sent": "So the task is the following.",
                    "label": 0
                },
                {
                    "sent": "I guess it's only after all of you.",
                    "label": 0
                },
                {
                    "sent": "So given protein sequence that is supposed to be representing an enzyme, the goal is to predict.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "That is, the biochemical reaction that is catalyzed by this enzyme.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is not very very legible, but so this is 1 one answer button reaction where some metabolise reactants are converted to some products and and there is a piece of the protein sequence that that.",
                    "label": 0
                },
                {
                    "sent": "Off the inside catalyzes this reaction.",
                    "label": 0
                },
                {
                    "sent": "So motivation for this this whole thing of course is kind of starting from genome annotation.",
                    "label": 0
                },
                {
                    "sent": "Motivation really is metabolic.",
                    "label": 0
                },
                {
                    "sent": "Network reconstruction went to want to reconstruct.",
                    "label": 0
                },
                {
                    "sent": "Network metabolic networks that that are kind of accurate, accurate as accurate as possible.",
                    "label": 0
                },
                {
                    "sent": "And the reason to construct this metabolic network is is that it's kind of the one of the.",
                    "label": 0
                },
                {
                    "sent": "Pieces you need in order to understand what goes inside inside the cell, so it's kind of.",
                    "label": 0
                },
                {
                    "sent": "What central central theme?",
                    "label": 0
                },
                {
                    "sent": "OK, so if you don't have any fancy machine learning tools that kind of the standard approach for inferring this function, I guess is to.",
                    "label": 0
                },
                {
                    "sent": "Go with some plus some version of Flash to run database of enzymes.",
                    "label": 1
                },
                {
                    "sent": "Find a similar sequence and predict that.",
                    "label": 1
                },
                {
                    "sent": "This new production sequence will have the same function.",
                    "label": 0
                },
                {
                    "sent": "Of course this.",
                    "label": 0
                },
                {
                    "sent": "This doesn't give you any functions that are not in the database.",
                    "label": 1
                },
                {
                    "sent": "You don't have any any mechanism to propose new new functions, so this is something that's.",
                    "label": 1
                },
                {
                    "sent": "Important and also very challenging how to how to propose new functions?",
                    "label": 0
                },
                {
                    "sent": "And this is also in the context of metabolic network reconstruction.",
                    "label": 0
                },
                {
                    "sent": "That is kind of one of the missing pieces, because when you use these databases to infer your network is just.",
                    "label": 0
                },
                {
                    "sent": "Text this whatever you can find there, you will end up with gaps in the network, so something some pieces missing, so your network actually is not capable of working properly.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something something we want to address.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have taken this structured output prediction approach.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "The idea is that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of.",
                    "label": 0
                },
                {
                    "sent": "Trying to somehow convert is this prediction problem.",
                    "label": 0
                },
                {
                    "sent": "From sequence to this function to some kind of binary machine learning problem really want to.",
                    "label": 0
                },
                {
                    "sent": "Want to predict this is complex OPS.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Act object this is reaction.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "So of course, the structured output prediction can be done in many ways.",
                    "label": 1
                },
                {
                    "sent": "This is kind of the outline of a family of.",
                    "label": 0
                },
                {
                    "sent": "The approach is based on SVM type type learning methods and we are using this framework.",
                    "label": 0
                },
                {
                    "sent": "So what you do there you take your inputs and outputs so pairs of sequences and reactions.",
                    "label": 0
                },
                {
                    "sent": "Your device some feature map, so some feature vectors representing this, this this object the sequences and and reactions and you make some kind of a joint feature feature map.",
                    "label": 0
                },
                {
                    "sent": "This is the simplest way to do it.",
                    "label": 1
                },
                {
                    "sent": "You do this kind of product features.",
                    "label": 0
                },
                {
                    "sent": "You take 1 feature from the sequence and multiplied with the one feature from the from the.",
                    "label": 0
                },
                {
                    "sent": "Output or reaction an you have this kind of set of.",
                    "label": 0
                },
                {
                    "sent": "Product features that gone by the composers.",
                    "label": 1
                },
                {
                    "sent": "Your joint feature feeds on map.",
                    "label": 0
                },
                {
                    "sent": "And with this disjoint feature map you do Martin based learning of linear score function you have.",
                    "label": 1
                },
                {
                    "sent": "Function of this kind.",
                    "label": 0
                },
                {
                    "sent": "So you have this potentially pretty huge joint feature map and you just wait the features somehow.",
                    "label": 0
                },
                {
                    "sent": "Possibly put some bias term in there so you have very simple score function, but it lives in a huge dimension office space.",
                    "label": 1
                },
                {
                    "sent": "Can you?",
                    "label": 0
                },
                {
                    "sent": "Because of this high dimensionality, you would want to use kernels for inputs and and.",
                    "label": 0
                },
                {
                    "sent": "All support for the outputs.",
                    "label": 0
                },
                {
                    "sent": "For making this learning efficient.",
                    "label": 0
                },
                {
                    "sent": "The prediction.",
                    "label": 0
                },
                {
                    "sent": "Is why are so called the pre image computation?",
                    "label": 0
                },
                {
                    "sent": "So you go to your score function.",
                    "label": 0
                },
                {
                    "sent": "And you look what is the maximising.",
                    "label": 0
                },
                {
                    "sent": "What, which in this case which reaction you need to put here in order to maximize this score?",
                    "label": 0
                },
                {
                    "sent": "This typically pretty.",
                    "label": 0
                },
                {
                    "sent": "Nasty problems to solve in structured outputs, But anyway, this is the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approaching want to take?",
                    "label": 0
                },
                {
                    "sent": "This is 1 particular method that we have been using in this study.",
                    "label": 0
                },
                {
                    "sent": "This is so called Max Martin regression.",
                    "label": 0
                },
                {
                    "sent": "It's a algorithm.",
                    "label": 0
                },
                {
                    "sent": "Under said mark.",
                    "label": 0
                },
                {
                    "sent": "The idea is that.",
                    "label": 0
                },
                {
                    "sent": "OK, you have.",
                    "label": 0
                },
                {
                    "sent": "You have prediction.",
                    "label": 0
                },
                {
                    "sent": "OK actually I have written this in different form and ask the previous slide.",
                    "label": 0
                },
                {
                    "sent": "But here is your prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is you have a weight matrix.",
                    "label": 0
                },
                {
                    "sent": "Waiting this input feature of extra, so that's something you have computed from your sequence.",
                    "label": 0
                },
                {
                    "sent": "You have weight matrix and you've tried to push it as close to.",
                    "label": 0
                },
                {
                    "sent": "The truth is true reaction, so you have a feature vector for the reaction and you want to push them as close to each other as possible.",
                    "label": 1
                },
                {
                    "sent": "So this is written here, so have inner product.",
                    "label": 0
                },
                {
                    "sent": "Between these guys and you want to.",
                    "label": 0
                },
                {
                    "sent": "Get it as large as possible and this is something familiar from the support vector machines you have normally minimisation of the weight, so you want to recognize the weight somehow so that they don't get too large and you put some slack because you.",
                    "label": 0
                },
                {
                    "sent": "I don't have an kind of linearly separable problem, OK?",
                    "label": 0
                },
                {
                    "sent": "If some of you are aware of more of this structured output literature, so this is resembles quite closely to the.",
                    "label": 0
                },
                {
                    "sent": "Set up of used by Corina Curtis and Co workers.",
                    "label": 1
                },
                {
                    "sent": "The difference is that here, or regress an Israeli inner product maximization.",
                    "label": 0
                },
                {
                    "sent": "So it means that we are trying to minimize the cosine angles of the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Feature vector and and wait wait vector space rather than doing kind of help to type the distance, meaning says so.",
                    "label": 0
                },
                {
                    "sent": "This is a point for you guys that are more deeply involved in this this.",
                    "label": 1
                },
                {
                    "sent": "Research.",
                    "label": 0
                },
                {
                    "sent": "But think about this, this MMR framework that's there is very extremely efficient optimization algorithm for for this model.",
                    "label": 1
                },
                {
                    "sent": "So, so we can do structured output learning.",
                    "label": 0
                },
                {
                    "sent": "With similar efficiency to SVM learning, and that's kind of one of the motivations to use this algorithm, and so it's much faster than many other of this structured output.",
                    "label": 0
                },
                {
                    "sent": "Type algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so in this type of learning, of course the features are the key, So what kind of features are you going to use?",
                    "label": 0
                },
                {
                    "sent": "This kind of decides how are you, how you will manage.",
                    "label": 0
                },
                {
                    "sent": "So we will use.",
                    "label": 0
                },
                {
                    "sent": "Quite obviously.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Features coming from kind of string kernel type type of family so.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So we have couple of.",
                    "label": 0
                },
                {
                    "sent": "Either the contiguous substring so features, or whether some substring occurs in the sequence or not.",
                    "label": 0
                },
                {
                    "sent": "And we can also put some gaps in the sequence so we can allow some some time number of gaps with some some link.",
                    "label": 0
                },
                {
                    "sent": "The third.",
                    "label": 0
                },
                {
                    "sent": "Peter set is perhaps more interesting to most of you is feature set that picks up corn syrup residue, so something that has been constant by evolution of the sequence.",
                    "label": 1
                },
                {
                    "sent": "And I will talk about that.",
                    "label": 0
                },
                {
                    "sent": "More a little bit into following.",
                    "label": 0
                },
                {
                    "sent": "OK, Ann.",
                    "label": 0
                },
                {
                    "sent": "Above this we want to use on different combinations so.",
                    "label": 1
                },
                {
                    "sent": "So sums of of this kernels.",
                    "label": 0
                },
                {
                    "sent": "That means that we make.",
                    "label": 0
                },
                {
                    "sent": "Concatenates this features feature vectors to do each other, so have this summer some kernels.",
                    "label": 0
                },
                {
                    "sent": "And then also.",
                    "label": 1
                },
                {
                    "sent": "Put polynomial kernel on top of it, so which means that.",
                    "label": 0
                },
                {
                    "sent": "Instead of picking individual features from this feature feature sets we combination so features.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is this is.",
                    "label": 0
                },
                {
                    "sent": "Need to shut OK?",
                    "label": 0
                },
                {
                    "sent": "Something about this GT features so the idea of of behind this distribution features is ready to pick up this.",
                    "label": 0
                },
                {
                    "sent": "This construct residues in sequences.",
                    "label": 0
                },
                {
                    "sent": "How they how they do it till this or this is something made by by by Lisa Holman and coworkers so to have made this kind of huge alignment kind of pairwise alignment of 400,000 sequences so.",
                    "label": 0
                },
                {
                    "sent": "So they have kind of.",
                    "label": 0
                },
                {
                    "sent": "For each representative of the sequence they have.",
                    "label": 0
                },
                {
                    "sent": "Made kind of pairwise alignment.",
                    "label": 0
                },
                {
                    "sent": "So they have 150 million kind of this pairwise alignment scores and what they do.",
                    "label": 0
                },
                {
                    "sent": "They will then go and look at individual residues and look at how there are aligned so they have huge amount of amount of residents that they look at and they put them in a single graph.",
                    "label": 0
                },
                {
                    "sent": "And look like so so this guy is aligned with this guy.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe it's.",
                    "label": 0
                },
                {
                    "sent": "WY yeah OK so.",
                    "label": 0
                },
                {
                    "sent": "And something like huaian.",
                    "label": 0
                },
                {
                    "sent": "OK, so I know anyway you can.",
                    "label": 0
                },
                {
                    "sent": "You can have have the idea that this is a huge graph where each of these individual.",
                    "label": 0
                },
                {
                    "sent": "Price of Juice is represented and you have an edge.",
                    "label": 0
                },
                {
                    "sent": "Whenever you have an alignment.",
                    "label": 0
                },
                {
                    "sent": "There OK and what they do with this.",
                    "label": 0
                },
                {
                    "sent": "They do crop clustering and look at the tight clusters or residues that align its order.",
                    "label": 0
                },
                {
                    "sent": "Kind of inconsistent manner.",
                    "label": 0
                },
                {
                    "sent": "And each each cluster that that comes out of this.",
                    "label": 0
                },
                {
                    "sent": "This setting is a feature, so this is something that they said, since it aligns consistently with within a group group of residues, it must be conserved.",
                    "label": 0
                },
                {
                    "sent": "So this is, this is so we take take these clusters as as as our features.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is how this GT features.",
                    "label": 0
                },
                {
                    "sent": "Look like so the idea is that this controversy implies some kind of import hands.",
                    "label": 0
                },
                {
                    "sent": "Either from function or structure point of view.",
                    "label": 0
                },
                {
                    "sent": "So so this is the reason we want to look at the concert.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Messages.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "OK, that was the input.",
                    "label": 0
                },
                {
                    "sent": "Features are sequence features, but we only also need to think about the output side.",
                    "label": 0
                },
                {
                    "sent": "So since we are representing our outputs as feature vectors as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of 1 recent representation coming from a standard EC hierarchy of of of.",
                    "label": 0
                },
                {
                    "sent": "Enzyme function, so this hierarchy is is 4 levels deep.",
                    "label": 1
                },
                {
                    "sent": "Bus routes so you have to eat.",
                    "label": 0
                },
                {
                    "sent": "Enzyme is kind of a path or the function of an enzyme is support in the.",
                    "label": 0
                },
                {
                    "sent": "In the hierarchy, and if you have multiple function, you will have a discussion over parts that deviate somewhere.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so this is something that that biologist or entomologist used all the time, so it's kind of a.",
                    "label": 0
                },
                {
                    "sent": "A reasonable starting point, but of course it's limited in expressive power, so there is no real support to credit completely new function.",
                    "label": 1
                },
                {
                    "sent": "So since those this here keys.",
                    "label": 0
                },
                {
                    "sent": "For our power point of view, it's fixed of course sometimes something occurs there, but there is no real kind of.",
                    "label": 0
                },
                {
                    "sent": "Support for machine learning.",
                    "label": 1
                },
                {
                    "sent": "A method to propose new functions.",
                    "label": 0
                },
                {
                    "sent": "OK, and how do you encode this to a feature vector valued?",
                    "label": 0
                },
                {
                    "sent": "Just take either tease notes, individual notes of the hierarchy and both aspire bits in the relevant cost systems.",
                    "label": 0
                },
                {
                    "sent": "Or you can do something bit more interesting.",
                    "label": 0
                },
                {
                    "sent": "You take take the edges.",
                    "label": 0
                },
                {
                    "sent": "I look at the labelings possible labelings of the edges so so if there is no label in parent and child or only label in parent and.",
                    "label": 0
                },
                {
                    "sent": "There's something like this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so so you have this kind of choices.",
                    "label": 0
                },
                {
                    "sent": "OK, so we tried this this this.",
                    "label": 0
                },
                {
                    "sent": "Thing with this kind of features at this prediction on start with the good news, or how did we?",
                    "label": 0
                },
                {
                    "sent": "Managed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first lesson is that OK, this is not.",
                    "label": 0
                },
                {
                    "sent": "Hopeless so we can actually get pretty good good.",
                    "label": 0
                },
                {
                    "sent": "Fictive accuracy, so this is these figures, here are Mike Label F1 scores so.",
                    "label": 0
                },
                {
                    "sent": "Geometric average of precision and recall of individual easynote predictions so they can take this whole easy hierarchy and look at the individual nodes.",
                    "label": 1
                },
                {
                    "sent": "How they are predicted and compute this F1 score.",
                    "label": 0
                },
                {
                    "sent": "Based on based on this.",
                    "label": 0
                },
                {
                    "sent": "OK, so so.",
                    "label": 0
                },
                {
                    "sent": "Overall we're doing OK. And also we can see that OK. Did she see features?",
                    "label": 0
                },
                {
                    "sent": "Disconcert residues alone work pretty well, but if you put some string kernel type.",
                    "label": 0
                },
                {
                    "sent": "Information in you.",
                    "label": 0
                },
                {
                    "sent": "Get something more.",
                    "label": 0
                },
                {
                    "sent": "Here the comparison is in hierarchical multi label classic classifier to.",
                    "label": 0
                },
                {
                    "sent": "So this is our implementation of the Max margin Markov networks for hierarchies.",
                    "label": 0
                },
                {
                    "sent": "So it knows a little bit more more about hierarchies than this MMR that is completely general general structured output measures so.",
                    "label": 0
                },
                {
                    "sent": "Perhaps not surprisingly, it's little bit better than 10 M and more, but not.",
                    "label": 0
                },
                {
                    "sent": "Not not not much.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is kind of.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Goodness first but then.",
                    "label": 0
                },
                {
                    "sent": "They notice some some peculiarities.",
                    "label": 0
                },
                {
                    "sent": "So here I have something that you shouldn't try to.",
                    "label": 0
                },
                {
                    "sent": "Read too carefully because you cannot see and then probably you don't understand Phineas either, so, but anyway.",
                    "label": 0
                },
                {
                    "sent": "Here is what happens when we raised.",
                    "label": 0
                },
                {
                    "sent": "Dikri of the polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "What happens to the F1 score?",
                    "label": 1
                },
                {
                    "sent": "So actually this is recall, but it's actually the same as F1 in this case, so you have this kind of monotone honestly.",
                    "label": 0
                },
                {
                    "sent": "Rising curve so so no overfitting.",
                    "label": 0
                },
                {
                    "sent": "When the polynomial Dick recalls, often that's surprising because we are exploding our feature space enormously and still be on all the time improving our prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is a bit peculiar and then started to wonder what happens.",
                    "label": 0
                },
                {
                    "sent": "Because what happens in this case is that you will have fewer and fewer Seminick Anthony non zero elements in the in the kernel, so it starts to pick up sparse kernel and also it comes means that if you think about distances in this place you have fewer and fewer close neighbors proceed sequence.",
                    "label": 1
                },
                {
                    "sent": "So this kind of led to a question.",
                    "label": 0
                },
                {
                    "sent": "OK if we put on nearest neighbor classifier to working in the same space, how would it do?",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course it works unknowingly well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So here is.",
                    "label": 0
                },
                {
                    "sent": "Here is what what our nearest neighbor classifier doesn't actually beats us in.",
                    "label": 0
                },
                {
                    "sent": "Every combination.",
                    "label": 0
                },
                {
                    "sent": "Those of you who have read our paper carefully.",
                    "label": 0
                },
                {
                    "sent": "Notice the.",
                    "label": 0
                },
                {
                    "sent": "Look at different figures for this hierarchical classifier, but there actually was a normalization error so.",
                    "label": 0
                },
                {
                    "sent": "So this figures.",
                    "label": 0
                },
                {
                    "sent": "A little bit.",
                    "label": 0
                },
                {
                    "sent": "Lower here than in the paper.",
                    "label": 0
                },
                {
                    "sent": "OK, so any better lesson is that actually the nearest neighbor does very well in this case, so we don't still.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They see much benefit off of our.",
                    "label": 0
                },
                {
                    "sent": "Fancy structured output algorithms here so.",
                    "label": 1
                },
                {
                    "sent": "I guess the next question is what is going on here?",
                    "label": 1
                },
                {
                    "sent": "And here is a.",
                    "label": 0
                },
                {
                    "sent": "List of possible explanations that we have.",
                    "label": 0
                },
                {
                    "sent": "But made up and you can go and pick your favorite or.",
                    "label": 0
                },
                {
                    "sent": "Invent a new one.",
                    "label": 0
                },
                {
                    "sent": "The first, of course is OK. Maybe this whole thing is exhausted by this nearest neighbor thing, so we have so good features that with this 2D2 features that actually there's nothing else to do but count your constant residues and that's that's the whole thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is 1 explanation.",
                    "label": 0
                },
                {
                    "sent": "We don't quite believe on that, so we want to think about the other explanations.",
                    "label": 0
                },
                {
                    "sent": "One other explanation is that maybe most of the true functions in the easy hierarchy have been derived by some kind of blast nearest neighbors.",
                    "label": 1
                },
                {
                    "sent": "So people plus.",
                    "label": 0
                },
                {
                    "sent": "Against the database, look what is there and then go and possibly wet lab.",
                    "label": 0
                },
                {
                    "sent": "Make a wet lab verification of the function.",
                    "label": 0
                },
                {
                    "sent": "So this would of course mean that if there is something else, then the nearest neighbor tells you that would be missed by the true.",
                    "label": 0
                },
                {
                    "sent": "Function, so your true function doesn't contain anything else.",
                    "label": 0
                },
                {
                    "sent": "So then it's pretty hard to beat the nearest neighbor classifier, so this is 1 explanation of that might be true or.",
                    "label": 0
                },
                {
                    "sent": "This is kind of speculative speculation.",
                    "label": 0
                },
                {
                    "sent": "OK, the next one.",
                    "label": 1
                },
                {
                    "sent": "Is that OK, maybe this easy hierarchy is not suitable for the whole thing, so maybe it doesn't support our kind of idea of of.",
                    "label": 0
                },
                {
                    "sent": "Trying to generalize about about the function, so maybe we should look for another hierarchy that would be more suitable for the task.",
                    "label": 0
                },
                {
                    "sent": "Or the 4th one.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 1
                },
                {
                    "sent": "Maybe maybe this hierarchies are not rich enough representation for this task.",
                    "label": 0
                },
                {
                    "sent": "But also maybe you should.",
                    "label": 0
                },
                {
                    "sent": "Switch to complete the other representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have started to address this this later points.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recently and this is we found out this gold standard data set.",
                    "label": 0
                },
                {
                    "sent": "That's has a different different hierarchy.",
                    "label": 0
                },
                {
                    "sent": "It's based on super family, family relation relationships.",
                    "label": 0
                },
                {
                    "sent": "It's actually much smaller than easy hierarchy.",
                    "label": 0
                },
                {
                    "sent": "And also it's supposed to be very, very curated so that the functions really are what?",
                    "label": 0
                },
                {
                    "sent": "What is, what is being included there.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There would make this comparation, so this is a depiction where we have this our training set an what happens when we so only 25% of the training set.",
                    "label": 0
                },
                {
                    "sent": "Food algorithm or 5075 or everything we have.",
                    "label": 0
                },
                {
                    "sent": "What happens and you can see that the MMR.",
                    "label": 0
                },
                {
                    "sent": "Venues.",
                    "label": 0
                },
                {
                    "sent": "Initially it works.",
                    "label": 0
                },
                {
                    "sent": "Better than the nearest neighbor, but then the kind of the.",
                    "label": 0
                },
                {
                    "sent": "Then you so more and more data at the nearest neighbor starts work better and better.",
                    "label": 0
                },
                {
                    "sent": "So OK, this kind of supports our idea that we are able to generalize becausw form from lesser amount of training data we get.",
                    "label": 0
                },
                {
                    "sent": "Get something.",
                    "label": 0
                },
                {
                    "sent": "Out of it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We take this some coin over.",
                    "label": 0
                },
                {
                    "sent": "Positive indication that there is something something that they do.",
                    "label": 0
                },
                {
                    "sent": "Cite this nearest neighbor neighbor effect.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this is wanting.",
                    "label": 0
                },
                {
                    "sent": "But it's not very huge, so it's kind of splitting hairs.",
                    "label": 0
                },
                {
                    "sent": "Really, if you talk to violate biologists.",
                    "label": 0
                },
                {
                    "sent": "So so maybe we should do something that is right?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We got a different from this.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is kind of our idea that we need to go to watch more fine grained representation so.",
                    "label": 0
                },
                {
                    "sent": "Because we see that OK.",
                    "label": 0
                },
                {
                    "sent": "Whenever we have this close sequence neighbors in the data set, the nearest neighbor is just as good, if not better than this structured output models.",
                    "label": 1
                },
                {
                    "sent": "However, our initial motivation was to be able to predict new function that are not in the in the in the taxonomy.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We want to have some kind of representation for the function where this Inter Inter or expiration between functions becomes possible.",
                    "label": 0
                },
                {
                    "sent": "On these hierarchies clearly are not not suitable for this, because there are fixed you, you don't have any any capabilities to success anything new, so it's it's clear that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you need to do something else.",
                    "label": 0
                },
                {
                    "sent": "So this is a direction we are actually going at the moment we want to.",
                    "label": 0
                },
                {
                    "sent": "Want to measure the similarity of and semantic reactions now in a smooth manner.",
                    "label": 1
                },
                {
                    "sent": "And how we do it is, while so-called reaction kernels.",
                    "label": 0
                },
                {
                    "sent": "That we have a kind of proposed.",
                    "label": 1
                },
                {
                    "sent": "So what we do there we start from any craft kernel for small molecules, so it can be apart kernel also subcraft spectrum.",
                    "label": 0
                },
                {
                    "sent": "And they have this kernel matrix that contest this this.",
                    "label": 1
                },
                {
                    "sent": "This kernel values for all pairs of relevant molecule, so so all molecules are bound to work which we have this this kernel for it.",
                    "label": 1
                },
                {
                    "sent": "And then for each reaction we test construct an indicator vector.",
                    "label": 0
                },
                {
                    "sent": "Indicating which which which reactions and products are part of the reaction.",
                    "label": 0
                },
                {
                    "sent": "On this reaction, kernel is really simple.",
                    "label": 1
                },
                {
                    "sent": "To compute, we have this matrix and compute this kind of.",
                    "label": 1
                },
                {
                    "sent": "Product with this indicator vectors and and.",
                    "label": 0
                },
                {
                    "sent": "For both 4th reaction.",
                    "label": 0
                },
                {
                    "sent": "So basically it picks up this relevant sub metrics out of the kernel and sums everything off.",
                    "label": 0
                },
                {
                    "sent": "So in essence this is just the sum of all against all pairwise similarities of this reactant product model case.",
                    "label": 0
                },
                {
                    "sent": "So, so this is something where we want to.",
                    "label": 0
                },
                {
                    "sent": "Want to do and are actually doing at the moment.",
                    "label": 0
                },
                {
                    "sent": "But I don't have any results.",
                    "label": 0
                },
                {
                    "sent": "Two to four from this approach as yet, but we hope to have something in.",
                    "label": 0
                },
                {
                    "sent": "Real near future.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is this is.",
                    "label": 0
                },
                {
                    "sent": "Last slide.",
                    "label": 0
                },
                {
                    "sent": "So we we have present that react research towards kind of fine grained inside function prediction where we really want to predict this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Reaction.",
                    "label": 0
                },
                {
                    "sent": "In fine detail.",
                    "label": 0
                },
                {
                    "sent": "What we have found out that when, when when we are predicting close sequence, neighbors are simple nearest neighbor classifier does as well or better.",
                    "label": 1
                },
                {
                    "sent": "In this hierarchical set setting.",
                    "label": 1
                },
                {
                    "sent": "We propose that this reaction kernels might be means to.",
                    "label": 1
                },
                {
                    "sent": "Medic novel reactions that are not part of the existing taxonomies or databases.",
                    "label": 0
                },
                {
                    "sent": "And that was the final.",
                    "label": 0
                },
                {
                    "sent": "Comment is that so this whole set of experiments I showed you was really made possible by this very very efficient MMR.",
                    "label": 0
                },
                {
                    "sent": "Training.",
                    "label": 0
                },
                {
                    "sent": "Uh, let's us.",
                    "label": 1
                },
                {
                    "sent": "Related to this structured output learning in the same time complexity S as classical SVM's.",
                    "label": 0
                },
                {
                    "sent": "OK, so this was everything I had to say, so thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Right, you talk about outputs or not OK, we haven't thought about putting an Gaussian kernel on the outputs.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but of course it gets hairy when when you put very complicated.",
                    "label": 0
                },
                {
                    "sent": "Colonel the preimage.",
                    "label": 0
                },
                {
                    "sent": "How do you get back to the?",
                    "label": 0
                },
                {
                    "sent": "Twitter to the reaction so.",
                    "label": 0
                },
                {
                    "sent": "Typically you need to do something crude in the premix phase, so so either you call your training set, look what it is there, or you do some kind of local search in the neighborhood of of of kind of objects that look.",
                    "label": 0
                },
                {
                    "sent": "Close to the OR you're kind of feature vector so.",
                    "label": 0
                },
                {
                    "sent": "You would ideally would like to have kind of nice combinatorial algorithm to do that, but.",
                    "label": 0
                },
                {
                    "sent": "This is pretty much open research, Phil.",
                    "label": 0
                },
                {
                    "sent": "I don't think many of.",
                    "label": 0
                },
                {
                    "sent": "All Guardians of this kind exist at the moment, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Repeat itself.",
                    "label": 0
                },
                {
                    "sent": "Send it.",
                    "label": 0
                },
                {
                    "sent": "So my question.",
                    "label": 0
                },
                {
                    "sent": "Sorry I missed explore, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "State.",
                    "label": 0
                },
                {
                    "sent": "We haven't really thought about optimizing the the.",
                    "label": 0
                },
                {
                    "sent": "Kind of the lambdas, no, no, but that's one thing we could to do.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's one.",
                    "label": 0
                },
                {
                    "sent": "Backspace right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "That's yeah, this yeah we should.",
                    "label": 0
                },
                {
                    "sent": "We should maybe maybe look at this kind of kind of approach.",
                    "label": 0
                },
                {
                    "sent": "Joe.",
                    "label": 0
                },
                {
                    "sent": "Some numbers at the federal level so.",
                    "label": 0
                },
                {
                    "sent": "Violation.",
                    "label": 0
                },
                {
                    "sent": "Yes, these are five or five fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "Old.",
                    "label": 0
                },
                {
                    "sent": "Oh I.",
                    "label": 0
                },
                {
                    "sent": "Everybody is right.",
                    "label": 0
                },
                {
                    "sent": "Right, well that's that's good point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so all yeah and in the end this results are pretty close to its error so I wouldn't claim that there is going to be big winner to any any direction.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, when we have this kind of large sets of different.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Different experiments and we see some common consistent trend between two methods so.",
                    "label": 0
                },
                {
                    "sent": "Open questions about five years later, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's that's true, but you would hope to kind of pay.",
                    "label": 0
                },
                {
                    "sent": "Very clear of nearest neighbor when you do something more complicated, but because it's conceptually so, if so much more easy to do, nearest neighbor and then some.",
                    "label": 0
                },
                {
                    "sent": "That we are trying to do here.",
                    "label": 0
                },
                {
                    "sent": "This is really good.",
                    "label": 0
                },
                {
                    "sent": "Affected.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that in the in, the kind of if you have this kind of reaction space.",
                    "label": 0
                },
                {
                    "sent": "Where your training points are kind of.",
                    "label": 0
                },
                {
                    "sent": "Scattered so you can hope that you can do some interpolation between your training points and sometimes there is.",
                    "label": 0
                },
                {
                    "sent": "You know maybe that your test.",
                    "label": 0
                },
                {
                    "sent": "Or your true new function is is there so that you can capture it by interpolation?",
                    "label": 0
                },
                {
                    "sent": "When you reproduce.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, said you wouldn't like to go back to your training set, so that's kind of the if you cannot workout anything else you do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's yeah, then you're not generalizing.",
                    "label": 0
                },
                {
                    "sent": "If you take something from your training set, yeah, that's not right.",
                    "label": 0
                }
            ]
        }
    }
}