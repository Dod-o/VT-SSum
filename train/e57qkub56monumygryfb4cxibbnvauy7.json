{
    "id": "e57qkub56monumygryfb4cxibbnvauy7",
    "title": "A Flexible and Efficient Algorithm for Regularized Fisher Discriminant Analysis",
    "info": {
        "author": [
            "Zhihua Zhang, Zhejiang University"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_zhang_fearfda/",
    "segmentation": [
        [
            "Michael Jordan.",
            "Universe OK thank you good afternoon everyone becausw some problems the authors cannot attend the conference so I just make a presentation instead of them.",
            "So the paper title I present is flexible and efficient algorithm for recognized Fisher discriminant analysis.",
            "This is joint work by children and one day and."
        ],
        [
            "Jordan so let's continue.",
            "This is just the outline of our slides."
        ],
        [
            "First of all, give some background of the LDA and Katie so Fisher linear discriminant analysis just idea and it's kernel extension, kernel discriminant analysis, kDa or well knowing dimension reduction method which can performance dimension reduction and also classification jointly.",
            "So the solution of idea and KD are often reduced to the solution of a generalized decomposition problem that which will involve between class scatter and total scatter matrix.",
            "So.",
            "Even an idea is widely used in many applications.",
            "There's still some issues to resolve.",
            "The first one is just.",
            "The LDA cannot be very efficiently implementation and also the second one is.",
            "We don't know the relations with between the LDA and least mean square error procedures.",
            "So in this paper we want to address these two issues, sorry."
        ],
        [
            "Here so.",
            "Ayodya requires the within class scatter matrix to be nonsingular, but when the dimension is very high than the scatter matrix is not going to be singular, so this will become a problem for ayodya here so.",
            "In many literatures there are two main variants or idea to deal with this problem.",
            "The first one is just the pseudoinverse method and the second one is the regularization method.",
            "Another important family of methods for dealing with this problem is based on two stage process.",
            "So in each stage one symmetric eigen decomposition problem will solved.",
            "So the challenge of developing an efficient implementation for LDA also is the problem."
        ],
        [
            "So the issue too is just.",
            "It is well knowing LDA is equivalent to at least mean square error procedure in binary classification problem, but it also of great interest to obtain a similar relationship.",
            "In multi class problems.",
            "What are the existing results in the literature?",
            "Often depends on some restrict district if conditions.",
            "So the problem of finding a general.",
            "On theoretical link between Aoda analyst Means Square still open."
        ],
        [
            "So we will first introduce annotations used in our paper.",
            "So we assume the training data set consists of data points X12 X here and the X1 XI here.",
            "Nice in P dimension data space.",
            "We also assume that detonated asset consists of C disjoint classes and.",
            "The ice class denoted by VI here.",
            "So we assume that each data point XI just belongs to one and only one classes.",
            "Also, we assume in ICE NGS class we have energy Stella Point, so the submission over often J equals to the end here.",
            "Then we will denote the capital in feeding times and identity matrix.",
            "And one in here is in times one vector of ones and X here is just the data matrix is just N times P metrics.",
            "Tiled exit just in time D. Feature data matrix after dimension reduction and a train here is just in times in centering matrix.",
            "Capital Pi here is just a diagonal matrix.",
            "It says degree.",
            "Metrics so the ice diagonal element is in I. ERE here is just the class indicator metrics.",
            "So if we I J = 2 one, that means XI belongs to JS class, others the EI will equal to 0 here."
        ],
        [
            "So then we will introduce a recognized linear discriminant analysis.",
            "So the total scatter matrix and between class scatter metrics are denoted by ST&SB here.",
            "So after some calculation we can.",
            "We can represent St as this one and SB.",
            "Like this one, so for the RLD we just want to solve the following problem.",
            "Here Sigma Square here is just the recognized parameters and a here is just the vectors.",
            "And the capital Lambda here is just the diagonal matrix consists of the eigenvalues."
        ],
        [
            "So for recognized kernel discriminant analysis qada.",
            "Similarly, the total scatter matrix and between class scatter metrics are divided by the Child St and tiled SP here.",
            "And the recognized kernel K. KD here solved this problem.",
            "This second decomposition problem.",
            "This is also the regular parameters and this is adding vectors and this is.",
            "Use.",
            "So this is algorithm proposed."
        ],
        [
            "In this paper.",
            "This is for recognize the linear discriminant analysis.",
            "The input is just XX.",
            "Here is data matrix E here is causing the current metrics.",
            "On Sigma Square is just the recognized parameters.",
            "And then Lambda Pi here, Capital Pi here is just the degree matrix.",
            "So we first calculate the metrics G. Like this one and then depend on the G. Here we calculate the arc here.",
            "So are just cheap.",
            "Just submit subway questions in are here.",
            "So then we will perform SVD of R. Like this one, this is becausw I symmetric here.",
            "So finally, if you output the A. Oh P, as solutions of IODIOB, it depends on the constraint on your algorithm for IoD here."
        ],
        [
            "So this is just the algorithm for recognized qada.",
            "So here see here is just the centering, metrics.",
            "And E is just the degree metrics.",
            "Sigma of Square here is just regular parameters, KX here is just KX.",
            "Here is just a test point.",
            "And KX is just a vector of test kernel values of test point on the training data set.",
            "So similarly, we also calculate the R. A child out here and then we will perform the SVD going like this.",
            "So link for kernel extension which is just interested in the no dimension of X here.",
            "So this is just the no dimension representation for X.",
            "Here."
        ],
        [
            "Then this paper defined Definer Ridge regression for the recognized.",
            "K. Claudia and Acadia.",
            "First, she will define the target used in Ridge regression Y here.",
            "So why I?",
            "Here is just for ICE data point.",
            "So if one excise belongs to jest class then why I will define like this otherwise like this?",
            "So for richer regression, the linear function is like just like this one.",
            "WO is offset and W is just the transformation matrix.",
            "So this is just object function for regular regression.",
            "So this can be.",
            "The solution of region regression is like this one W here.",
            "So this will."
        ],
        [
            "Discuss the relationship between recognized LDA and richer regression or post in the last page.",
            "So the blue here is just solution of Ridge regression problem and a here is just a solution of RLD here.",
            "So they also will find the A&W satisfies this following.",
            "Nation ship so equals 2 W times some rotations metrics here, so we are in that all are here is just the value and egg and this is just a convector and Reagan value metrics defined in algorithm for IoD here.",
            "Then we will find.",
            "I mean, when we let B = 2 eight times that all Power Point 5, then we can find B * B, transpose equals 2 W times W transpose.",
            "So according to this we can find the.",
            "The Euclidean distance of any data point on the in subspace defined by B or W is.",
            "Identical so we can find the solutions of ayodya and region regression is just the same.",
            "So then we go introduce the experiments.",
            "So this is used for this.",
            "We will use for."
        ],
        [
            "Face recognition for best database and.",
            "An ordered five data database for our experiments.",
            "So here she is just the number of classes piece dimension K. Here is the size of data."
        ],
        [
            "Set.",
            "So we.",
            "So the model permit hyperparameters Sigma here are selected by cross validation and for the kernel we use RBF kernel and the second set out.",
            "Here we just use it as a mean.",
            "You can edit distance between the training data point.",
            "So.",
            "After we get the no dimensional representations are here, we can use linear nearest neighbor classifier to do the classification."
        ],
        [
            "So this is just the results form LDA, so from the results we can find in most cases of our method achieves the best performance and also the time we need is almost the noise.",
            "The time we need.",
            "We also only need varies.",
            "Very small time.",
            "Yeah.",
            "So this is for this is for real."
        ],
        [
            "Phone Katie A so the same of Salvations also conveyed so the performance is good and the time is very requires not very little.",
            "So I make."
        ],
        [
            "Gives conclusion.",
            "So we have proposed a novel on the algorithm for solving recognized LD and kDa.",
            "So this algorithm is more efficient than other compared algorithms, especially in the setting of small end button logic problems.",
            "So all algorithm needs to equivalence between RDA and Ridge regression models.",
            "So this equivalence is derived in a general setting and it's fully consistent with its tabled routes in binary classification problem.",
            "So we have complete solve the open problem concerning the relationship between multitask or multiclass FDA problem.",
            "An multivariate linear expecting problems.",
            "I'll"
        ],
        [
            "OK, so thank you.",
            "Thank you very much.",
            "We have time for one question and put the next presenter prepare her laptop.",
            "So I have maybe one question.",
            "What's the time complexity analysis?",
            "Scale your proposed approach.",
            "They also could be clean.",
            "I also could be in, but he's in the minimum of A&P here."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Michael Jordan.",
                    "label": 0
                },
                {
                    "sent": "Universe OK thank you good afternoon everyone becausw some problems the authors cannot attend the conference so I just make a presentation instead of them.",
                    "label": 0
                },
                {
                    "sent": "So the paper title I present is flexible and efficient algorithm for recognized Fisher discriminant analysis.",
                    "label": 1
                },
                {
                    "sent": "This is joint work by children and one day and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jordan so let's continue.",
                    "label": 0
                },
                {
                    "sent": "This is just the outline of our slides.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, give some background of the LDA and Katie so Fisher linear discriminant analysis just idea and it's kernel extension, kernel discriminant analysis, kDa or well knowing dimension reduction method which can performance dimension reduction and also classification jointly.",
                    "label": 1
                },
                {
                    "sent": "So the solution of idea and KD are often reduced to the solution of a generalized decomposition problem that which will involve between class scatter and total scatter matrix.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Even an idea is widely used in many applications.",
                    "label": 0
                },
                {
                    "sent": "There's still some issues to resolve.",
                    "label": 0
                },
                {
                    "sent": "The first one is just.",
                    "label": 1
                },
                {
                    "sent": "The LDA cannot be very efficiently implementation and also the second one is.",
                    "label": 0
                },
                {
                    "sent": "We don't know the relations with between the LDA and least mean square error procedures.",
                    "label": 0
                },
                {
                    "sent": "So in this paper we want to address these two issues, sorry.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here so.",
                    "label": 0
                },
                {
                    "sent": "Ayodya requires the within class scatter matrix to be nonsingular, but when the dimension is very high than the scatter matrix is not going to be singular, so this will become a problem for ayodya here so.",
                    "label": 1
                },
                {
                    "sent": "In many literatures there are two main variants or idea to deal with this problem.",
                    "label": 0
                },
                {
                    "sent": "The first one is just the pseudoinverse method and the second one is the regularization method.",
                    "label": 0
                },
                {
                    "sent": "Another important family of methods for dealing with this problem is based on two stage process.",
                    "label": 1
                },
                {
                    "sent": "So in each stage one symmetric eigen decomposition problem will solved.",
                    "label": 0
                },
                {
                    "sent": "So the challenge of developing an efficient implementation for LDA also is the problem.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the issue too is just.",
                    "label": 0
                },
                {
                    "sent": "It is well knowing LDA is equivalent to at least mean square error procedure in binary classification problem, but it also of great interest to obtain a similar relationship.",
                    "label": 1
                },
                {
                    "sent": "In multi class problems.",
                    "label": 1
                },
                {
                    "sent": "What are the existing results in the literature?",
                    "label": 1
                },
                {
                    "sent": "Often depends on some restrict district if conditions.",
                    "label": 1
                },
                {
                    "sent": "So the problem of finding a general.",
                    "label": 0
                },
                {
                    "sent": "On theoretical link between Aoda analyst Means Square still open.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we will first introduce annotations used in our paper.",
                    "label": 0
                },
                {
                    "sent": "So we assume the training data set consists of data points X12 X here and the X1 XI here.",
                    "label": 0
                },
                {
                    "sent": "Nice in P dimension data space.",
                    "label": 0
                },
                {
                    "sent": "We also assume that detonated asset consists of C disjoint classes and.",
                    "label": 1
                },
                {
                    "sent": "The ice class denoted by VI here.",
                    "label": 0
                },
                {
                    "sent": "So we assume that each data point XI just belongs to one and only one classes.",
                    "label": 1
                },
                {
                    "sent": "Also, we assume in ICE NGS class we have energy Stella Point, so the submission over often J equals to the end here.",
                    "label": 1
                },
                {
                    "sent": "Then we will denote the capital in feeding times and identity matrix.",
                    "label": 1
                },
                {
                    "sent": "And one in here is in times one vector of ones and X here is just the data matrix is just N times P metrics.",
                    "label": 1
                },
                {
                    "sent": "Tiled exit just in time D. Feature data matrix after dimension reduction and a train here is just in times in centering matrix.",
                    "label": 0
                },
                {
                    "sent": "Capital Pi here is just a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "It says degree.",
                    "label": 0
                },
                {
                    "sent": "Metrics so the ice diagonal element is in I. ERE here is just the class indicator metrics.",
                    "label": 0
                },
                {
                    "sent": "So if we I J = 2 one, that means XI belongs to JS class, others the EI will equal to 0 here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then we will introduce a recognized linear discriminant analysis.",
                    "label": 1
                },
                {
                    "sent": "So the total scatter matrix and between class scatter metrics are denoted by ST&SB here.",
                    "label": 1
                },
                {
                    "sent": "So after some calculation we can.",
                    "label": 0
                },
                {
                    "sent": "We can represent St as this one and SB.",
                    "label": 0
                },
                {
                    "sent": "Like this one, so for the RLD we just want to solve the following problem.",
                    "label": 0
                },
                {
                    "sent": "Here Sigma Square here is just the recognized parameters and a here is just the vectors.",
                    "label": 0
                },
                {
                    "sent": "And the capital Lambda here is just the diagonal matrix consists of the eigenvalues.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for recognized kernel discriminant analysis qada.",
                    "label": 1
                },
                {
                    "sent": "Similarly, the total scatter matrix and between class scatter metrics are divided by the Child St and tiled SP here.",
                    "label": 0
                },
                {
                    "sent": "And the recognized kernel K. KD here solved this problem.",
                    "label": 0
                },
                {
                    "sent": "This second decomposition problem.",
                    "label": 0
                },
                {
                    "sent": "This is also the regular parameters and this is adding vectors and this is.",
                    "label": 0
                },
                {
                    "sent": "Use.",
                    "label": 0
                },
                {
                    "sent": "So this is algorithm proposed.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this paper.",
                    "label": 0
                },
                {
                    "sent": "This is for recognize the linear discriminant analysis.",
                    "label": 1
                },
                {
                    "sent": "The input is just XX.",
                    "label": 0
                },
                {
                    "sent": "Here is data matrix E here is causing the current metrics.",
                    "label": 0
                },
                {
                    "sent": "On Sigma Square is just the recognized parameters.",
                    "label": 0
                },
                {
                    "sent": "And then Lambda Pi here, Capital Pi here is just the degree matrix.",
                    "label": 0
                },
                {
                    "sent": "So we first calculate the metrics G. Like this one and then depend on the G. Here we calculate the arc here.",
                    "label": 0
                },
                {
                    "sent": "So are just cheap.",
                    "label": 0
                },
                {
                    "sent": "Just submit subway questions in are here.",
                    "label": 1
                },
                {
                    "sent": "So then we will perform SVD of R. Like this one, this is becausw I symmetric here.",
                    "label": 0
                },
                {
                    "sent": "So finally, if you output the A. Oh P, as solutions of IODIOB, it depends on the constraint on your algorithm for IoD here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just the algorithm for recognized qada.",
                    "label": 0
                },
                {
                    "sent": "So here see here is just the centering, metrics.",
                    "label": 0
                },
                {
                    "sent": "And E is just the degree metrics.",
                    "label": 0
                },
                {
                    "sent": "Sigma of Square here is just regular parameters, KX here is just KX.",
                    "label": 0
                },
                {
                    "sent": "Here is just a test point.",
                    "label": 0
                },
                {
                    "sent": "And KX is just a vector of test kernel values of test point on the training data set.",
                    "label": 0
                },
                {
                    "sent": "So similarly, we also calculate the R. A child out here and then we will perform the SVD going like this.",
                    "label": 0
                },
                {
                    "sent": "So link for kernel extension which is just interested in the no dimension of X here.",
                    "label": 0
                },
                {
                    "sent": "So this is just the no dimension representation for X.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then this paper defined Definer Ridge regression for the recognized.",
                    "label": 0
                },
                {
                    "sent": "K. Claudia and Acadia.",
                    "label": 0
                },
                {
                    "sent": "First, she will define the target used in Ridge regression Y here.",
                    "label": 0
                },
                {
                    "sent": "So why I?",
                    "label": 0
                },
                {
                    "sent": "Here is just for ICE data point.",
                    "label": 0
                },
                {
                    "sent": "So if one excise belongs to jest class then why I will define like this otherwise like this?",
                    "label": 0
                },
                {
                    "sent": "So for richer regression, the linear function is like just like this one.",
                    "label": 0
                },
                {
                    "sent": "WO is offset and W is just the transformation matrix.",
                    "label": 0
                },
                {
                    "sent": "So this is just object function for regular regression.",
                    "label": 0
                },
                {
                    "sent": "So this can be.",
                    "label": 0
                },
                {
                    "sent": "The solution of region regression is like this one W here.",
                    "label": 0
                },
                {
                    "sent": "So this will.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discuss the relationship between recognized LDA and richer regression or post in the last page.",
                    "label": 1
                },
                {
                    "sent": "So the blue here is just solution of Ridge regression problem and a here is just a solution of RLD here.",
                    "label": 1
                },
                {
                    "sent": "So they also will find the A&W satisfies this following.",
                    "label": 0
                },
                {
                    "sent": "Nation ship so equals 2 W times some rotations metrics here, so we are in that all are here is just the value and egg and this is just a convector and Reagan value metrics defined in algorithm for IoD here.",
                    "label": 0
                },
                {
                    "sent": "Then we will find.",
                    "label": 0
                },
                {
                    "sent": "I mean, when we let B = 2 eight times that all Power Point 5, then we can find B * B, transpose equals 2 W times W transpose.",
                    "label": 0
                },
                {
                    "sent": "So according to this we can find the.",
                    "label": 1
                },
                {
                    "sent": "The Euclidean distance of any data point on the in subspace defined by B or W is.",
                    "label": 0
                },
                {
                    "sent": "Identical so we can find the solutions of ayodya and region regression is just the same.",
                    "label": 0
                },
                {
                    "sent": "So then we go introduce the experiments.",
                    "label": 0
                },
                {
                    "sent": "So this is used for this.",
                    "label": 0
                },
                {
                    "sent": "We will use for.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Face recognition for best database and.",
                    "label": 0
                },
                {
                    "sent": "An ordered five data database for our experiments.",
                    "label": 0
                },
                {
                    "sent": "So here she is just the number of classes piece dimension K. Here is the size of data.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                },
                {
                    "sent": "So the model permit hyperparameters Sigma here are selected by cross validation and for the kernel we use RBF kernel and the second set out.",
                    "label": 1
                },
                {
                    "sent": "Here we just use it as a mean.",
                    "label": 1
                },
                {
                    "sent": "You can edit distance between the training data point.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "After we get the no dimensional representations are here, we can use linear nearest neighbor classifier to do the classification.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just the results form LDA, so from the results we can find in most cases of our method achieves the best performance and also the time we need is almost the noise.",
                    "label": 0
                },
                {
                    "sent": "The time we need.",
                    "label": 0
                },
                {
                    "sent": "We also only need varies.",
                    "label": 0
                },
                {
                    "sent": "Very small time.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So this is for this is for real.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Phone Katie A so the same of Salvations also conveyed so the performance is good and the time is very requires not very little.",
                    "label": 0
                },
                {
                    "sent": "So I make.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gives conclusion.",
                    "label": 0
                },
                {
                    "sent": "So we have proposed a novel on the algorithm for solving recognized LD and kDa.",
                    "label": 1
                },
                {
                    "sent": "So this algorithm is more efficient than other compared algorithms, especially in the setting of small end button logic problems.",
                    "label": 1
                },
                {
                    "sent": "So all algorithm needs to equivalence between RDA and Ridge regression models.",
                    "label": 1
                },
                {
                    "sent": "So this equivalence is derived in a general setting and it's fully consistent with its tabled routes in binary classification problem.",
                    "label": 0
                },
                {
                    "sent": "So we have complete solve the open problem concerning the relationship between multitask or multiclass FDA problem.",
                    "label": 0
                },
                {
                    "sent": "An multivariate linear expecting problems.",
                    "label": 0
                },
                {
                    "sent": "I'll",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "We have time for one question and put the next presenter prepare her laptop.",
                    "label": 0
                },
                {
                    "sent": "So I have maybe one question.",
                    "label": 0
                },
                {
                    "sent": "What's the time complexity analysis?",
                    "label": 0
                },
                {
                    "sent": "Scale your proposed approach.",
                    "label": 0
                },
                {
                    "sent": "They also could be clean.",
                    "label": 0
                },
                {
                    "sent": "I also could be in, but he's in the minimum of A&P here.",
                    "label": 0
                }
            ]
        }
    }
}