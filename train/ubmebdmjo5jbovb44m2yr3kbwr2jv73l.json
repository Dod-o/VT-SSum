{
    "id": "ubmebdmjo5jbovb44m2yr3kbwr2jv73l",
    "title": "High-Dimensional Graphical Model Selection",
    "info": {
        "author": [
            "Animashree Anandkumar, University of California, Irvine"
        ],
        "published": "Jan. 25, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/nips2011_anandkumar_conditions/",
    "segmentation": [
        [
            "Hi, good morning so today I'll be talking about high dimensional graphical model selection.",
            "So what we have you know in a big picture here are some GNU characterization of tractable graph families that can be learned efficiently through some simple algorithms based on conditional independence testing."
        ],
        [
            "So in the previous talk you know you saw the detailed analysis of causal models, but I'll be focusing on the non causal models.",
            "The usual graphical models with undirected graphs as you are all aware there is the global Markov condition that says that you know given two disjoint sets condition on this separator set on the graph, we have this notion of conditional independence."
        ],
        [
            "There's of course the equivalent notion of factorization according to the edges of this graph.",
            "For a pairwise model.",
            "So this is the usual graphical model."
        ],
        [
            "Formulation."
        ],
        [
            "And of course, you are also aware that for three models we have a simpler characterization in terms of pairwise edge."
        ],
        [
            "Nationals, so the problem I'm interested in is high dimensional structural estimation.",
            "So if I have an IID samples from this graphical model on P, note, how can I efficiently estimate the underlying graph?",
            "And of course, we want to think about useful theoretical guarantees for an algorithm, the usual one being structural consistent."
        ],
        [
            "Save and since we are interested in the high dimensional regime we want so characterize algorithms with low sample complexity, and indeed in high dimensions, computational complexity is also very crucial, so that will be the goal of."
        ],
        [
            "This talk and just to give a brief recap of the classical Charlie algorithm that was, you know, derived for tree models."
        ],
        [
            "And again, this well known algorithm.",
            "Now we have that for three models, the maximum likelihood graph estimate turns out to be a Max weight spanning tree where the edge weights are empirical mutual information estimate."
        ],
        [
            "So I."
        ],
        [
            "Just wanted to point out this algorithm to show that you know this is a simple, efficient algorithm for trees and only low order statistics are needed for structure estimation and this has good sample complexity, meaning that the number of nodes, the number of samples only needs to be log rhythmic in the number of nodes so."
        ],
        [
            "These are some nice properties for three models, so the question we wanted to address was how to relax this assumption and characterize families of graphs that have desirable properties such as you know, seen for three more."
        ],
        [
            "Notes.",
            "So the challenge is when we go to loopy graphs, as I'm sure all of you are aware are the fact that the partition function is not computable.",
            "So it's Sharpie complete and so you know if you want to think about maximum likelihood estimation, we can no longer show even compute the likelihood function."
        ],
        [
            "And another challenge for structure estimation is the presence of high degree nodes.",
            "Again 9.",
            "I'll briefly recap the conditional independence testing, and if we want to think about exact conditional independence testing, the computational complexity is exponential in the maximum degree.",
            "So if the degree of the graph grows with the number of nodes, again, we won't have a polynomial time algorithm for structure."
        ],
        [
            "Estimation, so the question we wanted to ask was to consider graphs that have these challenges, so we have loopy graphs with degrees possibly growing with the number of nodes and still can we think about some simple algorithms and regimes where we can efficiently estimate?"
        ],
        [
            "The underlying graph and of course we also want to, you know, think about graphs that are relevant for applications in many different domains and will see that many of the graphs we characterize are relevant for social network models."
        ],
        [
            "So just again this field of graphical model selection is indeed West, and there's a lot of new works on this, and even in this conference you will see there are many interesting approaches for graphical model selection I've just mentioned, you know a few example works just to point out that the approaches can be mainly classified into three categories.",
            "The EM based approaches which.",
            "Work well in practice, but theoretical guarantees are lacking.",
            "Some nonconvex approaches, and more recently the convex relaxation approaches.",
            "And today what I'll be talking about is again a non convex approach."
        ],
        [
            "So I will introduce, you know, our algorithm by first giving some intuitions.",
            "So you are all aware of the notion of separation in graphical models.",
            "So if I takes 2 non neighboring nodes I&J and I condition on the separator set as in the graph, we have conditional independence.",
            "We can you know think of it as being equivalent to the conditional mutual information being 0."
        ],
        [
            "So again, if we wanna do this conditional independence test to estimate the graph, we can undertake a brute force search.",
            "So I take any two nodes I&J and search overall sets for possible separation and we know the size of the set is bounded by the maximum degree and hence we can think of running such a test.",
            "Of course with samples will have an empirical estimate for the conditional mutual information.",
            "But the of course the problem with this approach is you know the computational complexity is growing exponentially in the maximum degree, and this is something that we want to."
        ],
        [
            "Relax so instead what we want to think about is to do an approximate condition."
        ],
        [
            "Independence test.",
            "So instead of searching for the exact separator, we can now think about a smaller."
        ],
        [
            "That approximately separates non neighboring nodes and again, of course this notion needs to be made precise that this conditional mutual information you know becomes small under this approximate conditioning."
        ],
        [
            "And to make this notion precise, we have the notion of graphs with sparse local separators.",
            "So the notion of a local separator are is.",
            "So if I take again two nodes I&J and I take the set S and if the set as separates all the paths between inj of length less than some parameter.",
            "So here the parameter is gamma, then we call it a local separator.",
            "So the notion is instead of having an exact separator on the graph, we only know separate based on short paths between non neighboring nodes and you know we consider the class of graphs that have sparse local separators.",
            "So here the size of all local separators is bounded by this parameter ETA.",
            "And here I've just listed some graphs.",
            "Where shall you know this property is satisfied?",
            "Indeed, you know, if we can think about locally tree like crafts, they have sparse local separators.",
            "So you know the in the simplest case, if the graph has.",
            "Good G and a limit to the length of parts being less than G. Then you know I just need a OneNote to separate all non neighboring nodes.",
            "And of course even with these Eldar Shreni another random graph so random regular graphs and again scale free graphs in certain regimes.",
            "We can have sparse local separators, so this is one class of models that you know has an efficient local separator.",
            "The, but then indeed even short loops are not a problem, so we can think about small world graph.",
            "So the simplest cases the watched Rogest model.",
            "So in this model there is the grid graph and then their long range links given by the adults for any model.",
            "And more generally you can think about what is the so called hybrid or the augmented graphs where there is a local graph and a global graph.",
            "The local graph typically has lots of short loops, but it has a low degree.",
            "While the global growth graph has large degrees, but it has locally tree like property and this model is 1 special case of it.",
            "So again, if you think about it, even in this case the size of the local separator is small because all the short loops are kind of modeled by this local graph and because of the local tree like property, the global graph also has a look sparse local separator.",
            "So we see here that a large family of graphs have the property of local sparse separator, and of course, if indeed if we think about the exact separation on these graphs, these are large, so that's where we see that local separation property can be an efficient way to characterize."
        ],
        [
            "Are these my?"
        ],
        [
            "Models."
        ],
        [
            "So what?"
        ],
        [
            "Again, I'm just."
        ],
        [
            "Formally stating the problem, we consider both the easing models and the Gaussian graphical models, and we want to characterize the you know now the set of parameters where this approximate conditional independence testing is efficient, so that's why we want to characterize the bound, the minimum and maximum bounds on the edge potentials.",
            "Where are you know this approximate conditional independent?",
            "Testing"
        ],
        [
            "Is efficient."
        ],
        [
            "So on that I mean to do this week, refer to the statistical physical properties of the model.",
            "So there is the notion that if I separate using only the approximate separator, there is a residual graph, so there are still some paths between non neighboring nodes, and so there's some residual dependence between these nodes.",
            "And if I have the maximum potential being sufficiently bounded, there is the decay of this.",
            "Effect so meaning this residual graph.",
            "There is not much conditional mutual information between non neighboring nodes.",
            "An for Gaussian models.",
            "Again this can be correct characterized through a notion of walks, immobility, and again walked.",
            "Some ability has been used before for characterizing the performance of belief propagation.",
            "Another inference algorithms.",
            "So we also see a connection between the same set of properties being efficient for both learning."
        ],
        [
            "And in France, so So what we have is characterization of the rate parimeter regime, where we can provide guarantees.",
            "And again, this threshold for the edge potentials can also be explicitly characterized for different graph families.",
            "I won't go into details, but there are available."
        ],
        [
            "The paper"
        ],
        [
            "So."
        ],
        [
            "So the main conditions are that we want so sparse local separator, so the size of the separator should be constant or not growing with the size of the graph to have a polynomial term algorithm.",
            "And as I said, the parameter regime should be, you know in this to these edge potentials should be sufficiently small and at the same time the minimum edge potential should be sufficiently large compared to the residual.",
            "You know dependence between non neighboring nodes and that is this technical condition I think about generic edge potentials because we want to rule out the cases where you know there's marginal independence between the neighboring."
        ],
        [
            "Notes.",
            "And so here I've just given an example if you think about the family of God bounded graph, so have the girth of the graph being G and the maximum degree being Delta.",
            "These set of conditions becomes an explicit condition of tradeoff between the maximum degree and the girth.",
            "So you can think about the extreme cases in one, and there are that remodels, so the girth is in finite, and there's no constraint on the degree, so.",
            "Trees of all degrees can be learned efficiently.",
            "On the other extreme, are these bounded degree graphs, and again, this Kino exact conditional independence testing gives a polynomial time algorithm and what we have is an entire spectrum of gertsen degrees where approximate conditional independence testing is efficient and that is this characterization."
        ],
        [
            "So to give the exact guarantees for our algorithm.",
            "So here are formally stated this, you know the approximate conditional testing conditional independence testing algorithm.",
            "So the idea is we compute the empirical mutual information and test it against a threshold and now this threshold depends on.",
            "Of course the number of samples.",
            "This is always the case here.",
            "This is the usual regularization.",
            "But it now also depends on the number of nodes, because we are thinking about the decay of conditional mutual information between non neighbors.",
            "As the graph becomes large and that's where these other regimes for this threshold come coming."
        ],
        [
            "And."
        ],
        [
            "So the exact result is if we have the number of samples scaling as this quantity.",
            "So again there's the minimum match potential and it's log rhythmic in the number of nodes.",
            "We have structural consistency and again in the long version of the paper they also non asymptotics sample complexity results as well as explicit characterization of the sample complexity."
        ],
        [
            "I won't go into details here.",
            "They are also necessary."
        ],
        [
            "Nations we can derive again using the.",
            "We know the information theoretic arguments I want to go into the deep."
        ],
        [
            "Well, since I'm old"
        ],
        [
            "Most out of."
        ],
        [
            "Time and coming to the proof details that mean analysis is involved in characterizing the regimes where this approximate separation results in decay of conditional mutual information between non neighbors, and that's where you know the different mathematical tools required for discrete and continuous models.",
            "And again, the details are in the long version of the."
        ],
        [
            "We are."
        ],
        [
            "Paper or so.",
            "In conclusion, what we have is.",
            "A simple algorithm based on approximate conditional independence testing that is shown to be efficient for a large family of graphs based on local separation and in parameter regimes that are related to the decay of long range correlations in the residual graph, and again there.",
            "Of course you know many interesting problems beyond this work, including the ones of thinking about in latent variables and connections with.",
            "You know convex relaxation methods, and so these are the long versions of the paper.",
            "If you are interested in more details, alright, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, good morning so today I'll be talking about high dimensional graphical model selection.",
                    "label": 0
                },
                {
                    "sent": "So what we have you know in a big picture here are some GNU characterization of tractable graph families that can be learned efficiently through some simple algorithms based on conditional independence testing.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the previous talk you know you saw the detailed analysis of causal models, but I'll be focusing on the non causal models.",
                    "label": 0
                },
                {
                    "sent": "The usual graphical models with undirected graphs as you are all aware there is the global Markov condition that says that you know given two disjoint sets condition on this separator set on the graph, we have this notion of conditional independence.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's of course the equivalent notion of factorization according to the edges of this graph.",
                    "label": 0
                },
                {
                    "sent": "For a pairwise model.",
                    "label": 0
                },
                {
                    "sent": "So this is the usual graphical model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formulation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, you are also aware that for three models we have a simpler characterization in terms of pairwise edge.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nationals, so the problem I'm interested in is high dimensional structural estimation.",
                    "label": 0
                },
                {
                    "sent": "So if I have an IID samples from this graphical model on P, note, how can I efficiently estimate the underlying graph?",
                    "label": 1
                },
                {
                    "sent": "And of course, we want to think about useful theoretical guarantees for an algorithm, the usual one being structural consistent.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Save and since we are interested in the high dimensional regime we want so characterize algorithms with low sample complexity, and indeed in high dimensions, computational complexity is also very crucial, so that will be the goal of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This talk and just to give a brief recap of the classical Charlie algorithm that was, you know, derived for tree models.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, this well known algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now we have that for three models, the maximum likelihood graph estimate turns out to be a Max weight spanning tree where the edge weights are empirical mutual information estimate.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just wanted to point out this algorithm to show that you know this is a simple, efficient algorithm for trees and only low order statistics are needed for structure estimation and this has good sample complexity, meaning that the number of nodes, the number of samples only needs to be log rhythmic in the number of nodes so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are some nice properties for three models, so the question we wanted to address was how to relax this assumption and characterize families of graphs that have desirable properties such as you know, seen for three more.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Notes.",
                    "label": 0
                },
                {
                    "sent": "So the challenge is when we go to loopy graphs, as I'm sure all of you are aware are the fact that the partition function is not computable.",
                    "label": 0
                },
                {
                    "sent": "So it's Sharpie complete and so you know if you want to think about maximum likelihood estimation, we can no longer show even compute the likelihood function.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another challenge for structure estimation is the presence of high degree nodes.",
                    "label": 0
                },
                {
                    "sent": "Again 9.",
                    "label": 0
                },
                {
                    "sent": "I'll briefly recap the conditional independence testing, and if we want to think about exact conditional independence testing, the computational complexity is exponential in the maximum degree.",
                    "label": 0
                },
                {
                    "sent": "So if the degree of the graph grows with the number of nodes, again, we won't have a polynomial time algorithm for structure.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Estimation, so the question we wanted to ask was to consider graphs that have these challenges, so we have loopy graphs with degrees possibly growing with the number of nodes and still can we think about some simple algorithms and regimes where we can efficiently estimate?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The underlying graph and of course we also want to, you know, think about graphs that are relevant for applications in many different domains and will see that many of the graphs we characterize are relevant for social network models.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just again this field of graphical model selection is indeed West, and there's a lot of new works on this, and even in this conference you will see there are many interesting approaches for graphical model selection I've just mentioned, you know a few example works just to point out that the approaches can be mainly classified into three categories.",
                    "label": 0
                },
                {
                    "sent": "The EM based approaches which.",
                    "label": 0
                },
                {
                    "sent": "Work well in practice, but theoretical guarantees are lacking.",
                    "label": 0
                },
                {
                    "sent": "Some nonconvex approaches, and more recently the convex relaxation approaches.",
                    "label": 0
                },
                {
                    "sent": "And today what I'll be talking about is again a non convex approach.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I will introduce, you know, our algorithm by first giving some intuitions.",
                    "label": 0
                },
                {
                    "sent": "So you are all aware of the notion of separation in graphical models.",
                    "label": 1
                },
                {
                    "sent": "So if I takes 2 non neighboring nodes I&J and I condition on the separator set as in the graph, we have conditional independence.",
                    "label": 1
                },
                {
                    "sent": "We can you know think of it as being equivalent to the conditional mutual information being 0.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, if we wanna do this conditional independence test to estimate the graph, we can undertake a brute force search.",
                    "label": 0
                },
                {
                    "sent": "So I take any two nodes I&J and search overall sets for possible separation and we know the size of the set is bounded by the maximum degree and hence we can think of running such a test.",
                    "label": 0
                },
                {
                    "sent": "Of course with samples will have an empirical estimate for the conditional mutual information.",
                    "label": 1
                },
                {
                    "sent": "But the of course the problem with this approach is you know the computational complexity is growing exponentially in the maximum degree, and this is something that we want to.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relax so instead what we want to think about is to do an approximate condition.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Independence test.",
                    "label": 0
                },
                {
                    "sent": "So instead of searching for the exact separator, we can now think about a smaller.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That approximately separates non neighboring nodes and again, of course this notion needs to be made precise that this conditional mutual information you know becomes small under this approximate conditioning.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to make this notion precise, we have the notion of graphs with sparse local separators.",
                    "label": 0
                },
                {
                    "sent": "So the notion of a local separator are is.",
                    "label": 0
                },
                {
                    "sent": "So if I take again two nodes I&J and I take the set S and if the set as separates all the paths between inj of length less than some parameter.",
                    "label": 1
                },
                {
                    "sent": "So here the parameter is gamma, then we call it a local separator.",
                    "label": 0
                },
                {
                    "sent": "So the notion is instead of having an exact separator on the graph, we only know separate based on short paths between non neighboring nodes and you know we consider the class of graphs that have sparse local separators.",
                    "label": 0
                },
                {
                    "sent": "So here the size of all local separators is bounded by this parameter ETA.",
                    "label": 0
                },
                {
                    "sent": "And here I've just listed some graphs.",
                    "label": 0
                },
                {
                    "sent": "Where shall you know this property is satisfied?",
                    "label": 0
                },
                {
                    "sent": "Indeed, you know, if we can think about locally tree like crafts, they have sparse local separators.",
                    "label": 0
                },
                {
                    "sent": "So you know the in the simplest case, if the graph has.",
                    "label": 0
                },
                {
                    "sent": "Good G and a limit to the length of parts being less than G. Then you know I just need a OneNote to separate all non neighboring nodes.",
                    "label": 0
                },
                {
                    "sent": "And of course even with these Eldar Shreni another random graph so random regular graphs and again scale free graphs in certain regimes.",
                    "label": 0
                },
                {
                    "sent": "We can have sparse local separators, so this is one class of models that you know has an efficient local separator.",
                    "label": 0
                },
                {
                    "sent": "The, but then indeed even short loops are not a problem, so we can think about small world graph.",
                    "label": 0
                },
                {
                    "sent": "So the simplest cases the watched Rogest model.",
                    "label": 0
                },
                {
                    "sent": "So in this model there is the grid graph and then their long range links given by the adults for any model.",
                    "label": 0
                },
                {
                    "sent": "And more generally you can think about what is the so called hybrid or the augmented graphs where there is a local graph and a global graph.",
                    "label": 0
                },
                {
                    "sent": "The local graph typically has lots of short loops, but it has a low degree.",
                    "label": 0
                },
                {
                    "sent": "While the global growth graph has large degrees, but it has locally tree like property and this model is 1 special case of it.",
                    "label": 0
                },
                {
                    "sent": "So again, if you think about it, even in this case the size of the local separator is small because all the short loops are kind of modeled by this local graph and because of the local tree like property, the global graph also has a look sparse local separator.",
                    "label": 1
                },
                {
                    "sent": "So we see here that a large family of graphs have the property of local sparse separator, and of course, if indeed if we think about the exact separation on these graphs, these are large, so that's where we see that local separation property can be an efficient way to characterize.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are these my?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, I'm just.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formally stating the problem, we consider both the easing models and the Gaussian graphical models, and we want to characterize the you know now the set of parameters where this approximate conditional independence testing is efficient, so that's why we want to characterize the bound, the minimum and maximum bounds on the edge potentials.",
                    "label": 0
                },
                {
                    "sent": "Where are you know this approximate conditional independent?",
                    "label": 0
                },
                {
                    "sent": "Testing",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is efficient.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on that I mean to do this week, refer to the statistical physical properties of the model.",
                    "label": 0
                },
                {
                    "sent": "So there is the notion that if I separate using only the approximate separator, there is a residual graph, so there are still some paths between non neighboring nodes, and so there's some residual dependence between these nodes.",
                    "label": 0
                },
                {
                    "sent": "And if I have the maximum potential being sufficiently bounded, there is the decay of this.",
                    "label": 0
                },
                {
                    "sent": "Effect so meaning this residual graph.",
                    "label": 0
                },
                {
                    "sent": "There is not much conditional mutual information between non neighboring nodes.",
                    "label": 0
                },
                {
                    "sent": "An for Gaussian models.",
                    "label": 0
                },
                {
                    "sent": "Again this can be correct characterized through a notion of walks, immobility, and again walked.",
                    "label": 0
                },
                {
                    "sent": "Some ability has been used before for characterizing the performance of belief propagation.",
                    "label": 0
                },
                {
                    "sent": "Another inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "So we also see a connection between the same set of properties being efficient for both learning.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in France, so So what we have is characterization of the rate parimeter regime, where we can provide guarantees.",
                    "label": 0
                },
                {
                    "sent": "And again, this threshold for the edge potentials can also be explicitly characterized for different graph families.",
                    "label": 0
                },
                {
                    "sent": "I won't go into details, but there are available.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The paper",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the main conditions are that we want so sparse local separator, so the size of the separator should be constant or not growing with the size of the graph to have a polynomial term algorithm.",
                    "label": 0
                },
                {
                    "sent": "And as I said, the parameter regime should be, you know in this to these edge potentials should be sufficiently small and at the same time the minimum edge potential should be sufficiently large compared to the residual.",
                    "label": 0
                },
                {
                    "sent": "You know dependence between non neighboring nodes and that is this technical condition I think about generic edge potentials because we want to rule out the cases where you know there's marginal independence between the neighboring.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notes.",
                    "label": 0
                },
                {
                    "sent": "And so here I've just given an example if you think about the family of God bounded graph, so have the girth of the graph being G and the maximum degree being Delta.",
                    "label": 0
                },
                {
                    "sent": "These set of conditions becomes an explicit condition of tradeoff between the maximum degree and the girth.",
                    "label": 0
                },
                {
                    "sent": "So you can think about the extreme cases in one, and there are that remodels, so the girth is in finite, and there's no constraint on the degree, so.",
                    "label": 0
                },
                {
                    "sent": "Trees of all degrees can be learned efficiently.",
                    "label": 0
                },
                {
                    "sent": "On the other extreme, are these bounded degree graphs, and again, this Kino exact conditional independence testing gives a polynomial time algorithm and what we have is an entire spectrum of gertsen degrees where approximate conditional independence testing is efficient and that is this characterization.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to give the exact guarantees for our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So here are formally stated this, you know the approximate conditional testing conditional independence testing algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we compute the empirical mutual information and test it against a threshold and now this threshold depends on.",
                    "label": 0
                },
                {
                    "sent": "Of course the number of samples.",
                    "label": 0
                },
                {
                    "sent": "This is always the case here.",
                    "label": 0
                },
                {
                    "sent": "This is the usual regularization.",
                    "label": 0
                },
                {
                    "sent": "But it now also depends on the number of nodes, because we are thinking about the decay of conditional mutual information between non neighbors.",
                    "label": 0
                },
                {
                    "sent": "As the graph becomes large and that's where these other regimes for this threshold come coming.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the exact result is if we have the number of samples scaling as this quantity.",
                    "label": 0
                },
                {
                    "sent": "So again there's the minimum match potential and it's log rhythmic in the number of nodes.",
                    "label": 0
                },
                {
                    "sent": "We have structural consistency and again in the long version of the paper they also non asymptotics sample complexity results as well as explicit characterization of the sample complexity.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I won't go into details here.",
                    "label": 0
                },
                {
                    "sent": "They are also necessary.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nations we can derive again using the.",
                    "label": 0
                },
                {
                    "sent": "We know the information theoretic arguments I want to go into the deep.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, since I'm old",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most out of.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time and coming to the proof details that mean analysis is involved in characterizing the regimes where this approximate separation results in decay of conditional mutual information between non neighbors, and that's where you know the different mathematical tools required for discrete and continuous models.",
                    "label": 0
                },
                {
                    "sent": "And again, the details are in the long version of the.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper or so.",
                    "label": 0
                },
                {
                    "sent": "In conclusion, what we have is.",
                    "label": 0
                },
                {
                    "sent": "A simple algorithm based on approximate conditional independence testing that is shown to be efficient for a large family of graphs based on local separation and in parameter regimes that are related to the decay of long range correlations in the residual graph, and again there.",
                    "label": 0
                },
                {
                    "sent": "Of course you know many interesting problems beyond this work, including the ones of thinking about in latent variables and connections with.",
                    "label": 0
                },
                {
                    "sent": "You know convex relaxation methods, and so these are the long versions of the paper.",
                    "label": 0
                },
                {
                    "sent": "If you are interested in more details, alright, thank you.",
                    "label": 0
                }
            ]
        }
    }
}